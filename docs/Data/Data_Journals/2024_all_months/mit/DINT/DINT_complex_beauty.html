<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>DINT_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="dint---20">DINT - 20</h2>
<ul>
<li><details>
<summary>
(2024). FAIR enough: Develop and assess a FAIR-compliant dataset for
large language model training? <em>DINT</em>, <em>6</em>(2), 559–585.
(<a href="https://doi.org/10.1162/dint_a_00255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid evolution of Large Language Models (LLMs) highlights the necessity for ethical considerations and data integrity in AI development, particularly emphasizing the role of FAIR (Findable, Accessible, Interoperable, Reusable) data principles. While these principles are crucial for ethical data stewardship, their specific application in the context of LLM training data remains an under-explored area. This research gap is the focus of our study, which begins with an examination of existing literature to underline the importance of FAIR principles in managing data for LLM training. Building upon this, we propose a novel frame-work designed to integrate FAIR principles into the LLM development lifecycle. A contribution of our work is the development of a comprehensive checklist intended to guide researchers and developers in applying FAIR data principles consistently across the model development process. The utility and effectiveness of our frame-work are validated through a case study on creating a FAIR-compliant dataset aimed at detecting and mitigating biases in LLMs. We present this framework to the community as a tool to foster the creation of technologically advanced, ethically grounded, and socially responsible AI models.},
  archive      = {J_DINT},
  author       = {Raza, Shaina and Ghuge, Shardul and Ding, Chen and Dolatabadi, Elham and Pandya, Deval},
  doi          = {10.1162/dint_a_00255},
  journal      = {Data Intelligence},
  month        = {5},
  number       = {2},
  pages        = {559-585},
  shortjournal = {Data Intell.},
  title        = {FAIR enough: Develop and assess a FAIR-compliant dataset for large language model training?},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BIKAS: Bio-inspired knowledge acquisition and simulacrum—a
knowledge database to support multifunctional design concept generation.
<em>DINT</em>, <em>6</em>(2), 531–558. (<a
href="https://doi.org/10.1162/dint_a_00240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A detailed acquisition, analysis, and representation of biological systems exhibiting different functions is required to develop unique bio-inspired multifunctional conceptual designs and methods. This paper presents BIKAS: Bio-inspired Knowledge Acquisition and Simulacrum, a knowledge database of biological systems exhibiting various functionalities, developed based on case-based bio-inspired examples from literature. The knowledge database represents the biological features, their characteristics, and the function exhibited by the biological feature as a combination of its integrated structure and structural strategy. Furthermore, this knowledge database is utilized by the Expandable Domain Integrated Design (xDID) model that works on classifying, mapping, and representing biological features into their respective geometric designations called Domains. The combination of features from the Domains results in the generation of multifunctional conceptual designs. In addition, Meta-level design factors are proposed to aid designers in filtering the biological features and their respective functions having a similar structural strategy, thus aiding designers in rapidly selecting and emulating biological functions.},
  archive      = {J_DINT},
  author       = {Velivela, Pavan Tejaswi and Zhao, Yaoyao Fiona},
  doi          = {10.1162/dint_a_00240},
  journal      = {Data Intelligence},
  month        = {5},
  number       = {2},
  pages        = {531-558},
  shortjournal = {Data Intell.},
  title        = {BIKAS: Bio-inspired knowledge acquisition and Simulacrum—A knowledge database to support multifunctional design concept generation},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting an optimal virtual data model for uniform access
to large heterogeneous data. <em>DINT</em>, <em>6</em>(2), 504–530. (<a
href="https://doi.org/10.1162/dint_a_00216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growth of generated data in the industry requires new efficient big data integration approaches for uniform data access by end-users to perform better business operations. Data virtualization systems, including Ontology-Based Data Access (ODBA) query data on-the-fly against the original data sources without any prior data materialization. Existing approaches by design use a fixed model e.g., TABULAR as the only Virtual Data Model — a uniform schema built on-the-fly to load, transform, and join relevant data. While other data models, such as GRAPH or DOCUMENT, are more flexible and, thus, can be more suitable for some common types of queries, such as join or nested queries. Those queries are hard to predict because they depend on many criteria, such as query plan, data model, data size, and operations. To address the problem of selecting the optimal virtual data model for queries on large datasets, we present a new approach that (1) builds on the principal of OBDA to query and join large heterogeneous data in a distributed manner and (2) calls a deep learning method to predict the optimal virtual data model using features extracted from SPARQL queries. OPTIMA — implementation of our approach currently leverages state-of-the-art Big Data technologies, Apache-Spark and Graphx, and implements two virtual data models, GRAPH and TABULAR, and supports out-of-the-box five data sources models: property graph, document-based, e.g., wide-columnar, relational, and tabular, stored in Neo4j, MongoDB, Cassandra, MySQL, and CSV respectively. Extensive experiments show that our approach is returning the optimal virtual model with an accuracy of 0.831, thus, a reduction in query execution time of over 40% for the tabular model selection and over 30% for the graph model selection.},
  archive      = {J_DINT},
  author       = {Belmehdi, Chahrazed B. Bachir and Khiat, Abderrahmane and Keskes, Nabil},
  doi          = {10.1162/dint_a_00216},
  journal      = {Data Intelligence},
  month        = {5},
  number       = {2},
  pages        = {504-530},
  shortjournal = {Data Intell.},
  title        = {Predicting an optimal virtual data model for uniform access to large heterogeneous data},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring attentive siamese LSTM for low-resource text
plagiarism detection. <em>DINT</em>, <em>6</em>(2), 488–503. (<a
href="https://doi.org/10.1162/dint_a_00242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-resource text plagiarism detection faces a significant challenge due to the limited availability of labeled data for training. This task requires the development of sophisticated algorithms capable of identifying similarities and differences in texts, particularly in the realm of semantic rewriting and translation-based plagiarism detection. In this paper, we present an enhanced attentive Siamese Long Short-Term Memory (LSTM) network designed for Tibetan-Chinese plagiarism detection. Our approach begins with the introduction of translation-based data augmentation, aimed at expanding the bilingual training dataset. Subsequently, we propose a pre-detection method leveraging abstract document vectors to enhance detection efficiency. Finally, we introduce an improved attentive Siamese LSTM network tailored for Tibetan-Chinese plagiarism detection. We conduct comprehensive experiments to showcase the effectiveness of our proposed plagiarism detection framework.},
  archive      = {J_DINT},
  author       = {Bao, Wei and Dong, Jian and Xu, Yang and Yang, Yuanyuan and Qi, Xiaoke},
  doi          = {10.1162/dint_a_00242},
  journal      = {Data Intelligence},
  month        = {5},
  number       = {2},
  pages        = {488-503},
  shortjournal = {Data Intell.},
  title        = {Exploring attentive siamese LSTM for low-resource text plagiarism detection},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The W3C data catalog vocabulary, version 2: Rationale,
design principles, and uptake. <em>DINT</em>, <em>6</em>(2), 457–487.
(<a href="https://doi.org/10.1162/dint_a_00241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DCAT is an RDF vocabulary designed to facilitate interoperability between data catalogs published on the Web. Since its first release in 2014 as a W3C Recommendation, DCAT has seen a wide adoption across communities and domains, particularly in conjunction with implementing the FAIR data principles (for findable, accessible, interoperable and reusable data). These implementation experiences, besides demonstrating the fitness of DCAT to meet its intended purpose, helped identify existing issues and gaps. Moreover, over the last few years, additional requirements emerged in data catalogs, given the increasing practice of documenting not only datasets but also data services and APIs. This paper illustrates the new version of DCAT, explaining the rationale behind its main revisions and extensions, based on the collected use cases and requirements, and outlines the issues yet to be addressed in future versions of DCAT.},
  archive      = {J_DINT},
  author       = {Albertoni, Riccardo and Browning, David and Cox, Simon and Gonzalez-Beltran, Alejandra N. and Perego, Andrea and Winstanley, Peter},
  doi          = {10.1162/dint_a_00241},
  journal      = {Data Intelligence},
  month        = {5},
  number       = {2},
  pages        = {457-487},
  shortjournal = {Data Intell.},
  title        = {The W3C data catalog vocabulary, version 2: Rationale, design principles, and uptake},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Building expertise on FAIR through evolving bring your own
data (BYOD) workshops: Describing the data, software, and
management-focused approaches and their evolution. <em>DINT</em>,
<em>6</em>(2), 429–456. (<a
href="https://doi.org/10.1162/dint_a_00236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since 2014, “Bring Your Own Data” workshops (BYODs) have been organised to inform people about the process and benefits of making resources Findable, Accessible, Interoperable, and Reusable (FAIR, and the FAIRification process). The BYOD workshops’ content and format differ depending on their goal, context, and the background and needs of participants. Data-focused BYODs educate domain experts on how to make their data FAIR to find new answers to research questions. Management-focused BYODs promote the benefits of making data FAIR and instruct project managers and policy-makers on the characteristics of FAIRification projects. Software-focused BYODs gather software developers and experts on FAIR to implement or improve software resources that are used to support FAIRification. Overall, these BYODs intend to foster collaboration between different types of stakeholders involved in data management, curation, and reuse (e.g. domain experts, trainers, developers, data owners, data analysts, FAIR experts). The BYODs also serve as an opportunity to learn what kind of support for FAIRification is needed from different communities and to develop teaching materials based on practical examples and experience. In this paper, we detail the three different structures of the BYODs and describe examples of early BYODs related to plant breeding data, and rare disease registries and biobanks, which have shaped the structure of the workshops. We discuss the latest insights into making BYODs more productive by leveraging our almost ten years of training experience in these workshops, including successes and encountered challenges. Finally, we examine how the participants’ feedback has motivated the research on FAIR, including the development of workflows and software.},
  archive      = {J_DINT},
  author       = {Bernabé, César H. and Thielemans, Lieze and Kaliyaperumal, Rajaram and Carta, Claudio and Zhang, Shuxin and van Gelder, Celia W.G. and Benis, Nirupama and da Silva Santos, Luiz Olavo Bonino and Cornet, Ronald and dos Santos Vieira, Bruna and Lalout, Nawel and Henriques, Ines and Ballesteros, Alberto Cámara and Burger, Kees and Kersloot, Martijn G. and Ehrhart, Friederike and van Enckevort, Esther and Evelo, Chris T. and Gray, Alasdair J. G. and Hanauer, Marc and Hettne, Kristina and de Ligt, Joep and Pereira, Arnaldo and Queralt-Rosinach, Núria and Schultes, Erik and Taruscio, Domenica and Waagmeester, Andra and Wilkinson, Mark D. and Willighagen, Egon L. and Jansen, Mascha and Mons, Barend and Roos, Marco and Jacobsen, Annika},
  doi          = {10.1162/dint_a_00236},
  journal      = {Data Intelligence},
  month        = {5},
  number       = {2},
  pages        = {429-456},
  shortjournal = {Data Intell.},
  title        = {Building expertise on FAIR through evolving bring your own data (BYOD) workshops: Describing the data, software, and management-focused approaches and their evolution},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sustainable connectivity in a community repository.
<em>DINT</em>, <em>6</em>(2), 409–428. (<a
href="https://doi.org/10.1162/dint_a_00252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Persistent identifiers for research objects, researchers, organizations, and funders are the key to creating unambiguous and persistent connections across the global research infrastructure (GRI). Many repositories are implementing mechanisms to collect and integrate these identifiers into their submission and record curation processes. This bodes well for a well-connected future, but metadata for existing resources submitted in the past are missing these identifiers, thus missing the connections required for inclusion in the connected infrastructure. Re-curation of these metadata is required to make these connections. This paper introduces the global research infrastructure and demonstrates how repositories, and their user communities, can contribute to and benefit from connections to the global research infrastructure. The Dryad Data Repository has existed since 2008 and has successfully re-curated the repository metadata several times, adding identifiers for research organizations, funders, and researchers. Understanding and quantifying these successes depends on measuring repository and identifier connectivity. Metrics are described and applied to the entire repository here. Identifiers (Digital Object Identifiers, DOIs) for papers connected to datasets in Dryad have long been a critical part of the Dryad metadata creation and curation processes. Since 2019, the portion of datasets with connected papers has decreased from 100% to less than 40%. This decrease has significant ramifications for the re-curation efforts described above as connected papers have been an important source of metadata. In addition, missing connections to papers make understanding and re-using datasets more difficult. Connections between datasets and papers can be difficult to make because of time lags between submission and publication, lack of clear mechanisms for citing datasets and other research objects from papers, changing focus of researchers, and other obstacles. The Dryad community of members, i.e. users, research institutions, publishers, and funders have vested interests in identifying these connections and critical roles in the curation and re-curation efforts. Their engagement will be critical in building on the successes Dryad has already achieved and ensuring sustainable connectivity in the future.},
  archive      = {J_DINT},
  author       = {Habermann, Ted},
  doi          = {10.1162/dint_a_00252},
  journal      = {Data Intelligence},
  month        = {5},
  number       = {2},
  pages        = {409-428},
  shortjournal = {Data Intell.},
  title        = {Sustainable connectivity in a community repository},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LLaMA-LoRA neural prompt engineering: A deep tuning
framework for automatically generating chinese text logical reasoning
thinking chains. <em>DINT</em>, <em>6</em>(2), 375–408. (<a
href="https://doi.org/10.1162/dint_a_00251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exption of Chinese natural language processing (NLP) has stimulated research in the broader NLP domain. However, existing large language models have limitations in comprehending and reasoning in Chinese. This paper addresses these limitations by enhancing Chinese language models comprehension and reasoning capabilities while minimizing resource requirements. We propose LLaMA-LoRA, a neural prompt engineering framework that builds upon the LLaMA-13B model and incorporates the Low-Rank Adaptation (LoRA) of Large Language Models technique for refinement. Chain-of-Thought (CoT) are crucial for generating intermediate reasoning chains in language models, but their effectiveness can be limited by isolated language patterns. Erroneous reasoning resulting from conventional prompts negatively impacts model performance. Automatic prompts are introduced to encourage reasoning chain generation and accurate answer inference. Training the model with an extensive corpus of Chinese CoT data enhances its comprehension and reasoning abilities. The LLaMA-LoRA model demonstrates exceptional performance across numerous Chinese language tasks, surpassing benchmark performance achieved by related language models such as GPT-3.5, Chat-GLM, and OpenAssistant, delivering accurate, comprehensive, and professional answers. The availability of our open-source model code facilitates further research in the field of Chinese text logical reasoning thinking chains.},
  archive      = {J_DINT},
  author       = {Chen, Songlin and Wang, Weicheng and Chen, Xiaoliang and lu, Peng and Yang, Zaiyan and Du, Yajun},
  doi          = {10.1162/dint_a_00251},
  journal      = {Data Intelligence},
  month        = {5},
  number       = {2},
  pages        = {375-408},
  shortjournal = {Data Intell.},
  title        = {LLaMA-LoRA neural prompt engineering: A deep tuning framework for automatically generating chinese text logical reasoning thinking chains},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Public opinions on ChatGPT: An analysis of reddit
discussions by using sentiment analysis, topic modeling, and SWOT
analysis. <em>DINT</em>, <em>6</em>(2), 344–374. (<a
href="https://doi.org/10.1162/dint_a_00250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sudden arrival of AI (Artificial Intelligence) into people&#39;s daily lives all around the world was marked by the introduction of ChatGPT, which was officially released on November 30, 2022. This AI invasion in our lives drew the attention of not only tech enthusiasts but also scholars from diverse fields, as its capacity extends across various fields. Consequently, numerous articles and journals have been discussing ChatGPT, making it a headline for several topics. However, it does not reflect most public opinion about the product. Therefore, this paper investigated the public&#39;s opinions on ChatGPT through topic modelling, Vader-based sentiment analysis and SWOT analysis. To gather data for this study, 202905 comments from the Reddit platform were collected between December 2022 and December 2023. The findings reveal that the Reddit community engaged in discussions related to ChatGPT, covering a range of topics including comparisons with traditional search engines, the impacts on software development, job market, and education industry, exploring ChatGPT&#39;s responses on entertainment and politics, the responses from Dan, the alter ego of ChatGPT, the ethical usage of user data as well as queries related to the AI-generated images. The sentiment analysis indicates that most people hold positive views towards this innovative technology across these several aspects. However, concerns also arise regarding the potential negative impacts associated with this product. The SWOT analysis of these results highlights both the strengths and pain points, market opportunities and threats associated with ChatGPT. This analysis also serves as a foundation for providing recommendations aimed at the product development and policy implementation in this paper.},
  archive      = {J_DINT},
  author       = {Naing, Shwe Zin Su and Udomwong, Piyachat},
  doi          = {10.1162/dint_a_00250},
  journal      = {Data Intelligence},
  month        = {5},
  number       = {2},
  pages        = {344-374},
  shortjournal = {Data Intell.},
  title        = {Public opinions on ChatGPT: An analysis of reddit discussions by using sentiment analysis, topic modeling, and SWOT analysis},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing ASReview simulations: A generic multiprocessing
solution for “light-data” and “heavy-data” users. <em>DINT</em>,
<em>6</em>(2), 320–343. (<a
href="https://doi.org/10.1162/dint_a_00244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning can be used for optimizing and speeding up the screening phase of systematic reviews. Running simulation studies mimicking the screening process can be used to test the performance of different machine-learning models or to study the impact of different training data. This paper presents an architecture design with a multiprocessing computational strategy for running many such simulation studies in parallel, using the ASReview Makita workflow generator and Kubernetes software for deployment with cloud technologies. We provide a technical explanation of the proposed cloud architecture and its usage. In addition to that, we conducted 1140 simulations investigating the computational time using various numbers of CPUs and RAM settings. Our analysis demonstrates the degree to which simulations can be accelerated with multiprocessing computing usage. The parallel computation strategy and the architecture design that was developed in the present paper can contribute to future research with more optimal simulation time and, at the same time, ensure the safe completion of the needed processes.},
  archive      = {J_DINT},
  author       = {Romanov, Sergei and Siqueira, Abel Soares and de Bruin, Jonathan and Teijema, Jelle and Hofstee, Laura and van de Schoot, Rens},
  doi          = {10.1162/dint_a_00244},
  journal      = {Data Intelligence},
  month        = {5},
  number       = {2},
  pages        = {320-343},
  shortjournal = {Data Intell.},
  title        = {Optimizing ASReview simulations: A generic multiprocessing solution for ‘Light-data’ and ‘Heavy-data’ users},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The four most basic elements in machine cognition.
<em>DINT</em>, <em>6</em>(2), 297–319. (<a
href="https://doi.org/10.1162/dint_a_00254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread use of ChatGPT has normalized the dialogue Turing test. To meet this challenge, China&#39;s major national development strategy suggests that for a new generation of artificial intelligence, it is first necessary to answer the big questions raised by Turing in 1950 from the perspective of cognitive physics: Can machines think? How do machines think? How do machines cognize? Whether it is carbon-based human cognition or silicon-based machine cognition, it is an interaction between complex constructs composed of the four most basic elements: matter, energy, structure, and time. Both humans and machines depend on negative entropy for living, and time is the cornerstone of cognition. Structure and time are parasitic on matter and energy in physical space, forming hard-structured ware. The soft-structured ware in cognitive space is mind, which is parasitic on the hard-structured ware or other existing soft-structured ware, and constitutes a rich hierarchy of multi-scale feelings, concepts, information, and knowledge. Extending “abstraction” from the symbolic school of artificial intelligence, “association” from the connectionist school, and “interaction” from the behaviorist school, the core of cognition is established on the shoulders of such scientific giants such as Schrödinger, Turing and Wiener. Soft and hard-structured ware interact. Cognitive machine can comprise heterogeneous hard-structured ware, such as field programmable gate arrays (FPGAs), data processing units (DPUs), central processing units (CPUs), graphics processing units (GPUs), tensor processing units (TPUs), and memory. It can also be implanted with the “Baby Cognitive Nucleus” which is hard-structured ware genetically inherited and naturally evolved to form the embodied machine. Then the hard-structured ware is parasitized by rich, multi-scale soft-structured ware. By regulating matter and energy through soft-structured ware, machines produce orderly events, form coordinated and orderly thinking activities. The heterogeneous sensors configured by the machine and the speed of thinking will no longer be trapped by the extreme values of biochemical parameters of carbon-based organisms but will be able to perceive through multi-channel cross-modal means, carry out intense thinking, and maintain cognitive continuity with memory. To generate computational and memory intelligence in cognitive space which can bootstrap, self-reuse and self-replicate, imagination and creativity are improved through memory-constrained computing. The new generation of artificial intelligence will leap beyond mechanized mathematics to automation in thinking and self-driven growth of cognition, and the thinking in the cognitive space and the behavior in the physical space verify each other, from the dialogue Turing test to the embodied Turing test. Humans have entered the intelligent era of human-machine co-creation with cognitive machines iteratively inventing, discovering, and creating alongside scientists, engineers, and skilled craftsmen, each wise in its way, improving thinking ability, and amplifying human energy.},
  archive      = {J_DINT},
  author       = {Li, Deyi and Yin, Jialun and Zhang, Tianlei and Han, Wei and Bao, Hong},
  doi          = {10.1162/dint_a_00254},
  journal      = {Data Intelligence},
  month        = {5},
  number       = {2},
  pages        = {297-319},
  shortjournal = {Data Intell.},
  title        = {The four most basic elements in machine cognition},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ChatGPT is a remarkable tool—for experts. <em>DINT</em>,
<em>6</em>(1), 240–296. (<a
href="https://doi.org/10.1162/dint_a_00235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the capabilities of ChatGPT as an automated assistant in diverse domains, including scientific writing, mathematics, education, programming, and healthcare. We explore the potential of ChatGPT to enhance productivity, streamline problem-solving processes, and improve writing style. Furthermore, we highlight the potential risks associated with excessive reliance on ChatGPT in these fields. These limitations encompass factors like incorrect and fictitious responses, inaccuracies in code, limited logical reasoning abilities, overconfidence, and critical ethical concerns of copyright and privacy violation. We outline areas and objectives where ChatGPT proves beneficial, applications where it should be used judiciously, and scenarios where its reliability may be limited. In light of observed limitations, and given that the tool&#39;s fundamental errors may pose a special challenge for non-experts, ChatGPT should be used with a strategic methodology. By drawing from comprehensive experimental studies, we offer methods and flowcharts for effectively using ChatGPT. Our recommendations emphasize iterative interaction with ChatGPT and independent verification of its outputs. Considering the importance of utilizing ChatGPT judiciously and with expertise, we recommend its usage for experts who are well-versed in the respective domains.},
  archive      = {J_DINT},
  author       = {Azaria, Amos and Azoulay, Rina and Reches, Shulamit},
  doi          = {10.1162/dint_a_00235},
  journal      = {Data Intelligence},
  month        = {2},
  number       = {1},
  pages        = {240-296},
  shortjournal = {Data Intell.},
  title        = {ChatGPT is a remarkable Tool—For experts},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The limitations and ethical considerations of ChatGPT.
<em>DINT</em>, <em>6</em>(1), 201–239. (<a
href="https://doi.org/10.1162/dint_a_00243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancements of artificial intelligence technology, ChatGPT, a new practice of artificial intelligence, holds immense potential across multiple fields. Its user-friendly human-machine interface, rapid response capabilities, and delivery of high-quality answers have attracted considerable attention and widespread usage. Regarded by many as a groundbreaking advancement in AI, ChatGPT represents a new milestone in the field. However, as with any technological evolution, the emergence of ChatGPT brings not only benefits, but also inevitable security risks and ethical issues. This paper provides specific information about ChatGPT, including its technology, limitations, ethical issues, governance paths and future directions. Specifically, we firstly offered a thorough exploration of the technical implementation details of GPT series models. Next, we provided an intricate analysis elucidating the reasons for limitations and scrutinized the consequential impacts, such as malicious misuse, privacy violation, and so on. Finally, we explore diverse governance paths to mitigate the impacts of ChatGPT and present future directions. This review aims to equip users with crucial knowledge, facilitating well-informed decision-making, effectively handling of potential challenges in employing ChatGPT, and staying abreast with the rapidly evolving landscape of this technology.},
  archive      = {J_DINT},
  author       = {Hua, Shangying and Jin, Shuangci and Jiang, Shengyi},
  doi          = {10.1162/dint_a_00243},
  journal      = {Data Intelligence},
  month        = {2},
  number       = {1},
  pages        = {201-239},
  shortjournal = {Data Intell.},
  title        = {The limitations and ethical considerations of ChatGPT},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-view feature learning for the over-penalty in
adversarial domain adaptation. <em>DINT</em>, <em>6</em>(1), 183–200.
(<a href="https://doi.org/10.1162/dint_a_00199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation aims to transfer knowledge from the labeled source domain to an unlabeled target domain that follows a similar but different distribution. Recently, adversarial-based methods have achieved remarkable success due to the excellent performance of domain-invariant feature presentation learning. However, the adversarial methods learn the transferability at the expense of the discriminability in feature representation, leading to low generalization to the target domain. To this end, we propose a Multi-view Feature Learning method for the Over-penalty in Adversarial Domain Adaptation. Specifically, multi-view representation learning is proposed to enrich the discriminative information contained in domain-invariant feature representation, which will counter the over-penalty for discriminability in adversarial training. Besides, the class distribution in the intra-domain is proposed to replace that in the inter-domain to capture more discriminative information in the learning of transferrable features. Extensive experiments show that our method can improve the discriminability while maintaining transferability and exceeds the most advanced methods in the domain adaptation benchmark datasets.},
  archive      = {J_DINT},
  author       = {Zhang, Yuhong and Wu, Jianqing and Zhang, Qi and Hu, Xuegang},
  doi          = {10.1162/dint_a_00199},
  journal      = {Data Intelligence},
  month        = {2},
  number       = {1},
  pages        = {183-200},
  shortjournal = {Data Intell.},
  title        = {Multi-view feature learning for the over-penalty in adversarial domain adaptation},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The stance and factors of international organizations
towards countries from a chinese perspective. <em>DINT</em>,
<em>6</em>(1), 158–182. (<a
href="https://doi.org/10.1162/dint_a_00248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The degree and scope of constraints imposed by International Organizations (IOs) on States are increasing, and identifying the factors affecting the IOs’ stances on States is helpful to enhance the state&#39;s discourse power and influence in the international community. First, by coding the records of Regular Press Conferences of the Speaking Office of the Chinese Ministry of Foreign Affairs during the period of 20182022, we obtained a dataset of IOs’ stances on China-related events. Second, we constructed political relation, economic relation, and humanistic relation indicators to complement the influence factors, adopted the Bayesian logit model, and applied the Monte Carlo Markov chain algorithm and Gibbs sampling to analyze the probability of IOs’ positive stances towards China. The result shows that IOs’ category, length of establishment, functional position, and relationship with China are all related to their tendency of making a statement about China. In terms of the heterogeneity of event types, forum-type IOs are significantly inclined to give positive assessment compared to service-type IOs on events focusing on China&#39;s own development. Further analysis reveals that the model for analyzing and predicting the attitudes of IOs is more effective when the international situation is in a stable period.},
  archive      = {J_DINT},
  author       = {Zhang, Qin and Xue, Haili and Wang, Yaotian and Wang, Yao and Zhang, Ziqin and Qin, Qinghua and Liang, Haoguang},
  doi          = {10.1162/dint_a_00248},
  journal      = {Data Intelligence},
  month        = {2},
  number       = {1},
  pages        = {158-182},
  shortjournal = {Data Intell.},
  title        = {The stance and factors of international organizations towards countries from a chinese perspective},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Training generative adversarial networks with adaptive
composite gradient. <em>DINT</em>, <em>6</em>(1), 120–157. (<a
href="https://doi.org/10.1162/dint_a_00246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The wide applications of Generative adversarial networks benefit from the successful training methods, guaranteeing that an object function converges to the local minimum. Nevertheless, designing an efficient and competitive training method is still a challenging task due to the cyclic behaviors of some gradient-based ways and the expensive computational cost of acquiring the Hessian matrix. To address this problem, we proposed the Adaptive Composite Gradients(ACG) method, linearly convergent in bilinear games under suitable settings. Theory analysis and toy-function experiments both suggest that our approach alleviates the cyclic behaviors and converges faster than recently proposed SOTA algorithms. The convergence speed of the ACG is improved by 33% than other methods. Our ACG method is a novel Semi-Gradient-Free algorithm that can reduce the computational cost of gradient and Hessian by utilizing the predictive information in future iterations. The mixture of Gaussians experiments and real-world digital image generative experiments show that our ACG method outperforms several existing technologies, illustrating the superiority and efficacy of our method.},
  archive      = {J_DINT},
  author       = {Qi, Huiqing and Li, Fang and Tan, Shengli and Zhang, Xiangyun},
  doi          = {10.1162/dint_a_00246},
  journal      = {Data Intelligence},
  month        = {2},
  number       = {1},
  pages        = {120-157},
  shortjournal = {Data Intell.},
  title        = {Training generative adversarial networks with adaptive composite gradient},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Resampling approaches for the quantitative analysis of
spatially distributed cells. <em>DINT</em>, <em>6</em>(1), 104–119. (<a
href="https://doi.org/10.1162/dint_a_00249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation is a crucial step in various image analysis pipelines and constitutes one of the cutting-edge areas of digital pathology. The advent of quantitative analysis has enabled the evaluation of millions of individual cells in tissues, allowing for the combined assessment of morphological features, biomarker expression, and spatial context. The recorded cells can be described as a point pattern process. However, the classical statistical approaches to point pattern processes prove unreliable in this context due to the presence of multiple irregularly-shaped interstitial cell-devoid spaces in the domain, which correspond to anatomical features (e.g. vessels, lipid vacuoles, glandular lumina) or tissue artefacts (e.g. tissue fractures), and whose coordinates are unknown. These interstitial spaces impede the accurate calculation of the domain area, resulting in biased clustering measurements. Moreover, the mistaken inclusion of empty regions of the domain can directly impact the results of hypothesis testing. The literature currently lacks any introduced bias correction method to address interstitial cell-devoid spaces. To address this gap, we propose novel resampling methods for testing spatial randomness and evaluating relationships among different cell populations. Our methods obviate the need for domain area estimation and provide non-biased clustering measurements. We created the SpaceR software ( https://github.com/GBertolazzi/SpaceR ) to enhance the accessibility of our methodologies.},
  archive      = {J_DINT},
  author       = {Bertolazzi, Giorgio and Tumminello, Michele and Morello, Gaia and Belmonte, Beatrice and Tripodo, Claudio},
  doi          = {10.1162/dint_a_00249},
  journal      = {Data Intelligence},
  month        = {2},
  number       = {1},
  pages        = {104-119},
  shortjournal = {Data Intell.},
  title        = {Resampling approaches for the quantitative analysis of spatially distributed cells},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Applying a context-based method to build a knowledge graph
for the blue amazon. <em>DINT</em>, <em>6</em>(1), 64–103. (<a
href="https://doi.org/10.1162/dint_a_00223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs are employed in several tasks, such as question answering and recommendation systems, due to their ability to represent relationships between concepts. Automatically constructing such a graphs, however, remains an unresolved challenge within knowledge representation. To tackle this challenge, we propose CtxKG, a method specifically aimed at extracting knowledge graphs in a context of limited resources in which the only input is a set of unstructured text documents. CtxKG is based on OpenIE (a relationship triple extraction method) and BERT (a language model) and contains four stages: the extraction of relationship triples directly from text; the identification of synonyms across triples; the merging of similar entities; and the building of bridges between knowledge graphs of different documents. Our method distinguishes itself from those in the current literature (i) through its use of the parse tree to avoid the overlapping entities produced by base implementations of OpenIE; and (ii) through its bridges, which create a connected network of graphs, overcoming a limitation similar methods have of one isolated graph per document. We compare our method to two others by generating graphs for movie articles from Wikipedia and contrasting them with benchmark graphs built from the OMDb movie database. Our results suggest that our method is able to improve multiple aspects of knowledge graph construction. They also highlight the critical role that triple identification and named-entity recognition have in improving the quality of automatically generated graphs, suggesting future paths for investigation. Finally, we apply CtxKG to build BlabKG, a knowledge graph for the Blue Amazon, and discuss possible improvements.},
  archive      = {J_DINT},
  author       = {Ligabue, Pedro de Moraes and Brandão, Anarosa Alves Franco and Peres, Sarajane Marques and Cozman, Fabio Gagliardi and Pirozelli, Paulo},
  doi          = {10.1162/dint_a_00223},
  journal      = {Data Intelligence},
  month        = {2},
  number       = {1},
  pages        = {64-103},
  shortjournal = {Data Intell.},
  title        = {Applying a context-based method to build a knowledge graph for the blue amazon},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Benchmarks for pirá 2.0, a reading comprehension dataset
about the ocean, the brazilian coast, and climate change. <em>DINT</em>,
<em>6</em>(1), 29–63. (<a
href="https://doi.org/10.1162/dint_a_00245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pirá is a reading comprehension dataset focused on the ocean, the Brazilian coast, and climate change, built from a collection of scientific abstracts and reports on these topics. This dataset represents a versatile language resource, particularly useful for testing the ability of current machine learning models to acquire expert scientific knowledge. Despite its potential, a detailed set of baselines has not yet been developed for Pirá. By creating these baselines, researchers can more easily utilize Pirá as a resource for testing machine learning models across a wide range of question answering tasks. In this paper, we define six benchmarks over the Pirá dataset, covering closed generative question answering, machine reading comprehension, information retrieval, open question answering, answer triggering, and multiple choice question answering. As part of this effort, we have also produced a curated version of the original dataset, where we fixed a number of grammar issues, repetitions, and other shortcomings. Furthermore, the dataset has been extended in several new directions, so as to face the aforementioned benchmarks: translation of supporting texts from English into Portuguese, classification labels for answerability, automatic paraphrases of questions and answers, and multiple choice candidates. The results described in this paper provide several points of reference for researchers interested in exploring the challenges provided by the Pirá dataset.},
  archive      = {J_DINT},
  author       = {Pirozelli, Paulo and José, Marcos M. and Silveira, Igor and Nakasato, Flávio and Peres, Sarajane M. and Brandão, Anarosa A. F. and Costa, Anna H. R. and Cozman, Fabio G.},
  doi          = {10.1162/dint_a_00245},
  journal      = {Data Intelligence},
  month        = {2},
  number       = {1},
  pages        = {29-63},
  shortjournal = {Data Intell.},
  title        = {Benchmarks for pirá 2.0, a reading comprehension dataset about the ocean, the brazilian coast, and climate change},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A theoretically grounded question answering data set for
evaluating machine common sense. <em>DINT</em>, <em>6</em>(1), 1–28. (<a
href="https://doi.org/10.1162/dint_a_00234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving machine common sense has been a longstanding problem within Artificial Intelligence. Thus far, benchmark data sets that are grounded in a theory of common sense and can be used to conduct rigorous, semantic evaluations of common sense reasoning (CSR) systems have been lacking. One expectation of the AI community is that neuro-symbolic reasoners can help bridge this gap towards more dependable systems with common sense. We propose a novel benchmark, called Theoretically Grounded common sense Reasoning (TG-CSR) , modeled as a set of question answering instances, with each instance grounded in a semantic category of common sense, such as space, time, and emotions. The benchmark is few-shot i.e., only a few training and validation examples are provided in the public release to avoid the possibility of overfitting. Results from recent evaluations suggest that TG-CSR is challenging even for state-of-the-art statistical models. Due to its semantic rigor, this benchmark can be used to evaluate the common sense reasoning capabilities of neuro-symbolic systems.},
  archive      = {J_DINT},
  author       = {Santos, Henrique and Shen, Ke and Mulvehill, Alice M. and Kejriwal, Mayank and McGuinness, Deborah L.},
  doi          = {10.1162/dint_a_00234},
  journal      = {Data Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Data Intell.},
  title        = {A theoretically grounded question answering data set for evaluating machine common sense},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
