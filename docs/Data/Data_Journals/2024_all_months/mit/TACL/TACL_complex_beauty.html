<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TACL_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tacl---95">TACL - 95</h2>
<ul>
<li><details>
<summary>
(2024). Robust pronoun fidelity with english LLMs: Are they
reasoning, repeating, or just biased? <em>TACL</em>, <em>12</em>,
1755–1779. (<a href="https://doi.org/10.1162/tacl_a_00719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust, faithful, and harm-free pronoun use for individuals is an important goal for language model development as their use increases, but prior work tends to study only one or two of these characteristics at a time. To measure progress towards the combined goal, we introduce the task of pronoun fidelity : Given a context introducing a co-referring entity and pronoun, the task is to reuse the correct pronoun later. We present RUFF , a carefully designed dataset of over 5 million instances to measure robust pronoun fidelity in English, and we evaluate 37 model variants from nine popular families, across architectures (encoder-only, decoder-only, and encoder-decoder) and scales (11M-70B parameters). When an individual is introduced with a pronoun, models can mostly faithfully reuse this pronoun in the next sentence, but they are significantly worse with she/her/her , singular they , and neopronouns. Moreover, models are easily distracted by non-adversarial sentences discussing other people; even one sentence with a distractor pronoun causes accuracy to drop on average by 34 percentage points. Our results show that pronoun fidelity is not robust, in a simple, naturalistic setting where humans achieve nearly 100% accuracy. We encourage researchers to bridge the gaps we find and to carefully evaluate reasoning in settings where superficial repetition might inflate perceptions of model performance.},
  archive      = {J_TACL},
  author       = {Gautam, Vagrant and Bingert, Eileen and Zhu, Dawei and Lauscher, Anne and Klakow, Dietrich},
  doi          = {10.1162/tacl_a_00719},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1755-1779},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Robust pronoun fidelity with english LLMs: Are they reasoning, repeating, or just biased?},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deuce: Dual-diversity enhancement and uncertainty-awareness
for cold-start active learning. <em>TACL</em>, <em>12</em>, 1736–1754.
(<a href="https://doi.org/10.1162/tacl_a_00731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cold-start active learning (CSAL) selects valuable instances from an unlabeled dataset for manual annotation. It provides high-quality data at a low annotation cost for label-scarce text classification. However, existing CSAL methods overlook weak classes and hard representative examples, resulting in biased learning. To address these issues, this paper proposes a novel dual-diversity enhancing and uncertainty-aware ( Deuce ) framework for CSAL. Specifically, Deuce leverages a pretrained language model (PLM) to efficiently extract textual representations, class predictions, and predictive uncertainty. Then, it constructs a Dual-Neighbor Graph (DNG) to combine information on both textual diversity and class diversity, ensuring a balanced data distribution. It further propagates uncertainty information via density-based clustering to select hard representative instances. Deuce performs well in selecting class-balanced and hard representative data by dual-diversity and informativeness. Experiments on six NLP datasets demonstrate the superiority and efficiency of Deuce .},
  archive      = {J_TACL},
  author       = {Guo, Jiaxin and Philip Chen, C. L. and Li, Shuzhen and Zhang, Tong},
  doi          = {10.1162/tacl_a_00731},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1736-1754},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Deuce: Dual-diversity enhancement and uncertainty-awareness for cold-start active learning},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SCL: Selective contrastive learning for data-driven
zero-shot relation extraction. <em>TACL</em>, <em>12</em>, 1720–1735.
(<a href="https://doi.org/10.1162/tacl_a_00721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relation extraction has evolved from supervised relation extraction to zero-shot setting due to the continuous emergence of newly generated relations. Some pioneering works handle zero-shot relation extraction by reformulating it into proxy tasks, such as reading comprehension and textual entailment. Nonetheless, the divergence in proxy task formulations from relation extraction hinders the acquisition of informative semantic representations, leading to subpar performance. Therefore, in this paper, we take a data-driven view to handle zero-shot relation extraction under a three-step paradigm, including encoder training, relation clustering, and summarization. Specifically, to train a discriminative relational encoder, we propose a novel selective contrastive learning framework, namely, SCL , where selective importance scores are assigned to distinguish the importance of different negative contrastive instances. During testing, the prompt-based encoder is employed to map test samples into representation vectors, which are then clustered into several groups. Typical samples closest to the cluster centroid are selected for summarization to generate the predicted relation for all samples in the cluster. Moreover, we design a simple non-parametric threshold plugin to reduce false-positive errors in inference on unseen relation representations. Our experiments demonstrate that SCL outperforms the current state-of-the-art method by over 3% across all metrics.},
  archive      = {J_TACL},
  author       = {Pang, Ning and Zhao, Xiang and Zeng, Weixin and Tan, Zhen and Xiao, Weidong},
  doi          = {10.1162/tacl_a_00721},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1720-1735},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {SCL: Selective contrastive learning for data-driven zero-shot relation extraction},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IndoCulture: Exploring geographically influenced cultural
commonsense reasoning across eleven indonesian provinces. <em>TACL</em>,
<em>12</em>, 1703–1719. (<a
href="https://doi.org/10.1162/tacl_a_00726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although commonsense reasoning is greatly shaped by cultural and geographical factors, previous studies have predominantly centered on cultures grounded in the English language, potentially resulting in an Anglocentric bias. In this paper, we introduce IndoCulture , aimed at understanding the influence of geographical factors on language model reasoning ability, with a specific emphasis on the diverse cultures found within eleven Indonesian provinces. In contrast to prior work that has relied on templates (Yin et al., 2022 ) and online scrapping (Fung et al., 2024 ), we create IndoCulture by asking local people to manually develop a cultural context and plausible options, across a set of predefined topics. Evaluation of 27 language models reveals several insights: (1) the open-weight Llama–3 is competitive with GPT–4, while other open-weight models struggle, with accuracies below 50%; (2) there is a general pattern of models generally performing better for some provinces, such as Bali and West Java, and less well for others; and (3) the inclusion of location context enhances performance, especially for larger models like GPT–4, emphasizing the significance of geographical context in commonsense reasoning. 1},
  archive      = {J_TACL},
  author       = {Koto, Fajri and Mahendra, Rahmad and Aisyah, Nurul and Baldwin, Timothy},
  doi          = {10.1162/tacl_a_00726},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1703-1719},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {IndoCulture: Exploring geographically influenced cultural commonsense reasoning across eleven indonesian provinces},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward robust RALMs: Revealing the impact of imperfect
retrieval on retrieval-augmented language models. <em>TACL</em>,
<em>12</em>, 1686–1702. (<a
href="https://doi.org/10.1162/tacl_a_00724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retrieval Augmented Language Models (RALMs) have gained significant attention for their ability to generate accurate answers and improve efficiency. However, RALMs are inherently vulnerable to imperfect information due to their reliance on the imperfect retriever or knowledge source. We identify three common scenarios—unanswerable, adversarial, conflicting—where retrieved document sets can confuse RALMs with plausible real-world examples. We present the first comprehensive investigation to assess how well RALMs detect and handle such problematic scenarios. Among these scenarios, to systematically examine adversarial robustness we propose a new adversarial attack method, Gen erative model-based ADV ersarial attack ( GenADV ) and a novel metric R obustness under A dditional D ocument ( RAD ). Our findings reveal that RALMs often fail to identify the unanswerability or contradiction of a document set, which frequently leads to hallucinations. Moreover, we show that the addition of an adversary significantly degrades RALM’s performance, with the model becoming even more vulnerable when the two scenarios overlap (adversarial+ unanswerable). Our research identifies critical areas for assessing and enhancing the robustness of RALMs, laying the foundation for the development of more robust models. 1},
  archive      = {J_TACL},
  author       = {Park, Seong-Il and Lee, Jay-Yoon},
  doi          = {10.1162/tacl_a_00724},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1686-1702},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Toward robust RALMs: Revealing the impact of imperfect retrieval on retrieval-augmented language models},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The causal influence of grammatical gender on distributional
semantics. <em>TACL</em>, <em>12</em>, 1672–1685. (<a
href="https://doi.org/10.1162/tacl_a_00723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How much meaning influences gender assignment across languages is an active area of research in linguistics and cognitive science. We can view current approaches as aiming to determine where gender assignment falls on a spectrum, from being fully arbitrarily determined to being largely semantically determined. For the latter case, there is a formulation of the neo-Whorfian hypothesis, which claims that even inanimate noun gender influences how people conceive of and talk about objects (using the choice of adjective used to modify inanimate nouns as a proxy for meaning). We offer a novel, causal graphical model that jointly represents the interactions between a noun’s grammatical gender, its meaning, and adjective choice. In accordance with past results, we find a significant relationship between the gender of nouns and the adjectives that modify them. However, when we control for the meaning of the noun, the relationship between grammatical gender and adjective choice is near zero and insignificant.},
  archive      = {J_TACL},
  author       = {Stańczak, Karolina and Du, Kevin and Williams, Adina and Augenstein, Isabelle and Cotterell, Ryan},
  doi          = {10.1162/tacl_a_00723},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1672-1685},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {The causal influence of grammatical gender on distributional semantics},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TabVer: Tabular fact verification with natural logic.
<em>TACL</em>, <em>12</em>, 1648–1671. (<a
href="https://doi.org/10.1162/tacl_a_00722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fact verification on tabular evidence incentivizes the use of symbolic reasoning models where a logical form is constructed (e.g., a LISP-style program), providing greater verifiability than fully neural approaches. However, these logical forms typically rely on well-formed tables, restricting their use in many scenarios. An emerging symbolic reasoning paradigm for textual evidence focuses on natural logic inference, which constructs proofs by modeling set-theoretic relations between a claim and its evidence in natural language. This approach provides flexibility and transparency but is less compatible with tabular evidence since the relations do not extend to arithmetic functions. We propose a set-theoretic interpretation of numerals and arithmetic functions in the context of natural logic, enabling the integration of arithmetic expressions in deterministic proofs. We leverage large language models to generate arithmetic expressions by generating questions about salient parts of a claim which are answered by executing appropriate functions on tables. In a few-shot setting on FEVEROUS, we achieve an accuracy of 71.4, outperforming both fully neural and symbolic reasoning models by 3.4 points. When evaluated on TabFact without any further training, our method remains competitive with an accuracy lead of 0.5 points.},
  archive      = {J_TACL},
  author       = {Aly, Rami and Vlachos, Andreas},
  doi          = {10.1162/tacl_a_00722},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1648-1671},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {TabVer: Tabular fact verification with natural logic},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Holmes ⌕ a benchmark to assess the linguistic competence of
language models. <em>TACL</em>, <em>12</em>, 1616–1647. (<a
href="https://doi.org/10.1162/tacl_a_00718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Holmes , a new benchmark designed to assess language models’ (LMs’) linguistic competence —their unconscious understanding of linguistic phenomena. Specifically, we use classifier-based probing to examine LMs’ internal representations regarding distinct linguistic phenomena (e.g., part-of-speech tagging). As a result, we meet recent calls to disentangle LMs’ linguistic competence from other cognitive abilities, such as following instructions in prompting-based evaluations. Composing Holmes , we review over 270 probing studies and include more than 200 datasets to assess syntax, morphology, semantics, reasoning, and discourse phenomena. Analyzing over 50 LMs reveals that, aligned with known trends, their linguistic competence correlates with model size. However, surprisingly, model architecture and instruction tuning also significantly influence performance, particularly in morphology and syntax . Finally, we propose FlashHolmes , a streamlined version that reduces the computation load while maintaining high-ranking precision.},
  archive      = {J_TACL},
  author       = {Waldis, Andreas and Perlitz, Yotam and Choshen, Leshem and Hou, Yufang and Gurevych, Iryna},
  doi          = {10.1162/tacl_a_00718},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1616-1647},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Holmes ⌕ a benchmark to assess the linguistic competence of language models},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Filtered corpus training (FiCT) shows that language models
can generalize from indirect evidence. <em>TACL</em>, <em>12</em>,
1597–1615. (<a href="https://doi.org/10.1162/tacl_a_00720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces Fi ltered C orpus T raining, a method that trains language models (LMs) on corpora with certain linguistic constructions filtered out from the training data, and uses it to measure the ability of LMs to perform linguistic generalization on the basis of indirect evidence. We apply the method to both LSTM and Transformer LMs (of roughly comparable size), developing filtered corpora that target a wide range of linguistic phenomena. Our results show that while transformers are better qua LMs (as measured by perplexity), both models perform equally and surprisingly well on linguistic generalization measures, suggesting that they are capable of generalizing from indirect evidence.},
  archive      = {J_TACL},
  author       = {Patil, Abhinav and Jumelet, Jaap and Chiu, Yu Ying and Lapastora, Andy and Shen, Peter and Wang, Lexie and Willrich, Clevis and Steinert-Threlkeld, Shane},
  doi          = {10.1162/tacl_a_00720},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1597-1615},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Filtered corpus training (FiCT) shows that language models can generalize from indirect evidence},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rescue conversations from dead-ends: Efficient exploration
for task-oriented dialogue policy optimization. <em>TACL</em>,
<em>12</em>, 1578–1596. (<a
href="https://doi.org/10.1162/tacl_a_00717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training a task-oriented dialogue policy using deep reinforcement learning is promising but requires extensive environment exploration. The amount of wasted invalid exploration makes policy learning inefficient. In this paper, we define and argue that dead-end states are important reasons for invalid exploration. When a conversation enters a dead-end state, regardless of the actions taken afterward, it will continue in a dead-end trajectory until the agent reaches a termination state or maximum turn. We propose a D ead-end D etection and R esurrection (DDR) method that detects dead-end states in an efficient manner and provides a rescue action to guide and correct the exploration direction. To prevent dialogue policies from repeating errors, DDR also performs dialogue data augmentation by adding relevant experiences that include dead-end states and penalties into the experience pool. We first validate the dead-end detection reliability and then demonstrate the effectiveness and generality of the method across various domains through experiments on four public dialogue datasets.},
  archive      = {J_TACL},
  author       = {Zhao, Yangyang and Dastani, Mehdi and Long, Jinchuan and Wang, Zhenyu and Wang, Shihan},
  doi          = {10.1162/tacl_a_00717},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1578-1596},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Rescue conversations from dead-ends: Efficient exploration for task-oriented dialogue policy optimization},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on model compression for large language models.
<em>TACL</em>, <em>12</em>, 1556–1577. (<a
href="https://doi.org/10.1162/tacl_a_00704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) have transformed natural language processing tasks successfully. Yet, their large size and high computational needs pose challenges for practical use, especially in resource-limited settings. Model compression has emerged as a key research area to address these challenges. This paper presents a survey of model compression techniques for LLMs. We cover methods like quantization, pruning, and knowledge distillation, highlighting recent advancements. We also discuss benchmarking strategies and evaluation metrics crucial for assessing compressed LLMs. This survey offers valuable insights for researchers and practitioners, aiming to enhance efficiency and real-world applicability of LLMs while laying a foundation for future advancements.},
  archive      = {J_TACL},
  author       = {Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang, Weiping},
  doi          = {10.1162/tacl_a_00704},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1556-1577},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {A survey on model compression for large language models},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical indexing for retrieval-augmented opinion
summarization. <em>TACL</em>, <em>12</em>, 1533–1555. (<a
href="https://doi.org/10.1162/tacl_a_00703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method for unsupervised abstractive opinion summarization, that combines the attributability and scalability of extractive approaches with the coherence and fluency of Large Language Models (LLMs). Our method, HIRO, learns an index structure that maps sentences to a path through a semantically organized discrete hierarchy. At inference time, we populate the index and use it to identify and retrieve clusters of sentences containing popular opinions from input reviews. Then, we use a pretrained LLM to generate a readable summary that is grounded in these extracted evidential clusters. The modularity of our approach allows us to evaluate its efficacy at each stage. We show that HIRO learns an encoding space that is more semantically structured than prior work, and generates summaries that are more representative of the opinions in the input reviews. Human evaluation confirms that HIRO generates significantly more coherent, detailed, and accurate summaries.},
  archive      = {J_TACL},
  author       = {Hosking, Tom and Tang, Hao and Lapata, Mirella},
  doi          = {10.1162/tacl_a_00703},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1533-1555},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Hierarchical indexing for retrieval-augmented opinion summarization},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FINCH: Prompt-guided key-value cache compression for large
language models. <em>TACL</em>, <em>12</em>, 1517–1532. (<a
href="https://doi.org/10.1162/tacl_a_00716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent large language model applications, such as Retrieval-Augmented Generation and chatbots, have led to an increased need to process longer input contexts. However, this requirement is hampered by inherent limitations. Architecturally, models are constrained by a context window defined during training. Additionally, processing extensive texts requires substantial GPU memory. We propose a novel approach, Finch , to compress the input context by leveraging the pre-trained model weights of the self-attention. Given a prompt and a long text, Finch iteratively identifies the most relevant Key (K) and Value (V) pairs over chunks of the text conditioned on the prompt. Only such pairs are stored in the KV cache, which, within the space constrained by the context window, ultimately contains a compressed version of the long text. Our proposal enables models to consume large inputs even with high compression (up to 93x) while preserving semantic integrity without the need for fine-tuning.},
  archive      = {J_TACL},
  author       = {Corallo, Giulio and Papotti, Paolo},
  doi          = {10.1162/tacl_a_00716},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1517-1532},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {FINCH: Prompt-guided key-value cache compression for large language models},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conformal prediction for natural language processing: A
survey. <em>TACL</em>, <em>12</em>, 1497–1516. (<a
href="https://doi.org/10.1162/tacl_a_00715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid proliferation of large language models and natural language processing (NLP) applications creates a crucial need for uncertainty quantification to mitigate risks such as Hallucinations and to enhance decision-making reliability in critical applications. Conformal prediction is emerging as a theoretically sound and practically useful framework, combining flexibility with strong statistical guarantees. Its model-agnostic and distribution-free nature makes it particularly promising to address the current shortcomings of NLP systems that stem from the absence of uncertainty quantification. This paper provides a comprehensive survey of conformal prediction techniques, their guarantees, and existing applications in NLP, pointing to directions for future research and open challenges.},
  archive      = {J_TACL},
  author       = {Campos, Margarida and Farinhas, António and Zerva, Chrysoula and Figueiredo, Mário A. T. and Martins, André F. T.},
  doi          = {10.1162/tacl_a_00715},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1497-1516},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Conformal prediction for natural language processing: A survey},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting human translation difficulty with neural machine
translation. <em>TACL</em>, <em>12</em>, 1479–1496. (<a
href="https://doi.org/10.1162/tacl_a_00714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human translators linger on some words and phrases more than others, and predicting this variation is a step towards explaining the underlying cognitive processes. Using data from the CRITT Translation Process Research Database, we evaluate the extent to which surprisal and attentional features derived from a Neural Machine Translation (NMT) model account for reading and production times of human translators. We find that surprisal and attention are complementary predictors of translation difficulty, and that surprisal derived from a NMT model is the single most successful predictor of production duration. Our analyses draw on data from hundreds of translators operating across 13 language pairs, and represent the most comprehensive investigation of human translation difficulty to date.},
  archive      = {J_TACL},
  author       = {Lim, Zheng Wei and Vylomova, Ekaterina and Kemp, Charles and Cohn, Trevor},
  doi          = {10.1162/tacl_a_00714},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1479-1496},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Predicting human translation difficulty with neural machine translation},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conformalizing machine translation evaluation.
<em>TACL</em>, <em>12</em>, 1460–1478. (<a
href="https://doi.org/10.1162/tacl_a_00711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several uncertainty estimation methods have been recently proposed for machine translation evaluation. While these methods can provide a useful indication of when not to trust model predictions, we show in this paper that the majority of them tend to underestimate model uncertainty, and as a result, they often produce misleading confidence intervals that do not cover the ground truth. We propose as an alternative the use of conformal prediction , a distribution-free method to obtain confidence intervals with a theoretically established guarantee on coverage. First, we demonstrate that split conformal prediction can “correct” the confidence intervals of previous methods to yield a desired coverage level, and we demonstrate these findings across multiple machine translation evaluation metrics and uncertainty quantification methods. Further, we highlight biases in estimated confidence intervals, reflected in imbalanced coverage for different attributes, such as the language and the quality of translations. We address this by applying conditional conformal prediction techniques to obtain calibration subsets for each data subgroup, leading to equalized coverage. Overall, we show that, provided access to a calibration set, conformal prediction can help identify the most suitable uncertainty quantification methods and adapt the predicted confidence intervals to ensure fairness with respect to different attributes. 1},
  archive      = {J_TACL},
  author       = {Zerva, Chrysoula and Martins, André F. T.},
  doi          = {10.1162/tacl_a_00711},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1460-1478},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Conformalizing machine translation evaluation},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactive machine teaching by labeling rules and
instances. <em>TACL</em>, <em>12</em>, 1441–1459. (<a
href="https://doi.org/10.1162/tacl_a_00707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised learning aims to reduce the cost of labeling data by using expert-designed labeling rules. However, existing methods require experts to design effective rules in a single shot, which is difficult in the absence of proper guidance and tooling. Therefore, it is still an open question whether experts should spend their limited time writing rules or instead providing instance labels via active learning. In this paper, we investigate how to exploit an expert’s limited time to create effective supervision. First, to develop practical guidelines for rule creation, we conduct an exploratory analysis of diverse collections of existing expert-designed rules and find that rule precision is more important than coverage across datasets. Second, we compare rule creation to individual instance labeling via active learning and demonstrate the importance of both across 6 datasets. Third, we propose an interactive learning framework, INTERVAL, that achieves efficiency by automatically extracting candidate rules based on rich patterns (e.g., by prompting a language model), and effectiveness by soliciting expert feedback on both candidate rules and individual instances. Across 6 datasets, INTERVAL outperforms state-of-the-art weakly supervised approaches by 7% in F1. Furthermore, it requires as few as 10 queries for expert feedback to reach F1 values that existing active learning methods cannot match even with 100 queries.},
  archive      = {J_TACL},
  author       = {Karamanolakis, Giannis and Hsu, Daniel and Gravaano, Luis},
  doi          = {10.1162/tacl_a_00707},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1441-1459},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Interactive machine teaching by labeling rules and instances},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). When can LLMs actually correct their own mistakes? A
critical survey of self-correction of LLMs. <em>TACL</em>, <em>12</em>,
1417–1440. (<a href="https://doi.org/10.1162/tacl_a_00713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-correction is an approach to improving responses from large language models (LLMs) by refining the responses using LLMs during inference. Prior work has proposed various self-correction frameworks using different sources of feedback, including self-evaluation and external feedback. However, there is still no consensus on the question of when LLMs can correct their own mistakes , as recent studies also report negative results. In this work, we critically survey broad papers and discuss the conditions required for successful self-correction. We first find that prior studies often do not define their research questions in detail and involve impractical frameworks or unfair evaluations that over-evaluate self-correction. To tackle these issues, we categorize research questions in self-correction research and provide a checklist for designing appropriate experiments. Our critical survey based on the newly categorized research questions shows that (1) no prior work demonstrates successful self-correction with feedback from prompted LLMs, except for studies in tasks that are exceptionally suited for self-correction, (2) self-correction works well in tasks that can use reliable external feedback, and (3) large-scale fine-tuning enables self-correction.},
  archive      = {J_TACL},
  author       = {Kamoi, Ryo and Zhang, Yusen and Zhang, Nan and Han, Jiawei and Zhang, Rui},
  doi          = {10.1162/tacl_a_00713},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1417-1440},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {When can LLMs actually correct their own mistakes? a critical survey of self-correction of LLMs},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised topic taxonomy discovery in the box
embedding space. <em>TACL</em>, <em>12</em>, 1401–1416. (<a
href="https://doi.org/10.1162/tacl_a_00712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topic taxonomy discovery aims at uncovering topics of different abstraction levels and constructing hierarchical relations between them. Unfortunately, most prior work can hardly model semantic scopes of words and topics by holding the Euclidean embedding space assumption. What’s worse, they infer asymmetric hierarchical relations by symmetric distances between topic embeddings. As a result, existing methods suffer from problems of low-quality topics at high abstraction levels and inaccurate hierarchical relations. To alleviate these problems, this paper develops a Box embedding-based Topic Model (BoxTM) that maps words and topics into the box embedding space, where the asymmetric metric is defined to properly infer hierarchical relations among topics. Additionally, our BoxTM explicitly infers upper-level topics based on correlation between specific topics through recursive clustering on topic boxes. Finally, extensive experiments validate high-quality of the topic taxonomy learned by BoxTM.},
  archive      = {J_TACL},
  author       = {Lu, Yuyin and Chen, Hegang and Mao, Pengbo and Rao, Yanghui and Xie, Haoran and Wang, Fu Lee and Li, Qing},
  doi          = {10.1162/tacl_a_00712},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1401-1416},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Self-supervised topic taxonomy discovery in the box embedding space},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beyond prompt brittleness: Evaluating the reliability and
consistency of political worldviews in LLMs. <em>TACL</em>, <em>12</em>,
1378–1400. (<a href="https://doi.org/10.1162/tacl_a_00710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the widespread use of large language models (LLMs), we need to understand whether they embed a specific “worldview” and what these views reflect. Recent studies report that, prompted with political questionnaires, LLMs show left-liberal leanings (Feng et al., 2023 ; Motoki et al., 2024 ). However, it is as yet unclear whether these leanings are reliable (robust to prompt variations) and whether the leaning is consistent across policies and political leaning. We propose a series of tests which assess the reliability and consistency of LLMs’ stances on political statements based on a dataset of voting-advice questionnaires collected from seven EU countries and annotated for policy issues. We study LLMs ranging in size from 7B to 70B parameters and find that their reliability increases with parameter count. Larger models show overall stronger alignment with left-leaning parties but differ among policy programs: They show a (left-wing) positive stance towards environment protection, social welfare state, and liberal society but also (right-wing) law and order, with no consistent preferences in the areas of foreign policy and migration.},
  archive      = {J_TACL},
  author       = {Ceron, Tanise and Falk, Neele and Barić, Ana and Nikolaev, Dmitry and Padó, Sebastian},
  doi          = {10.1162/tacl_a_00710},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1378-1400},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Addressing topic leakage in cross-topic evaluation for
authorship verification. <em>TACL</em>, <em>12</em>, 1363–1377. (<a
href="https://doi.org/10.1162/tacl_a_00709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Authorship verification (AV) aims to identify whether a pair of texts has the same author. We address the challenge of evaluating AV models’ robustness against topic shifts. The conventional evaluation assumes minimal topic overlap between training and test data. However, we argue that there can still be topic leakage in test data, causing misleading model performance and unstable rankings. To address this, we propose an evaluation method called Heterogeneity-Informed Topic Sampling (HITS), which creates a smaller dataset with a heterogeneously distributed topic set. Our experimental results demonstrate that HITS-sampled datasets yield a more stable ranking of models across random seeds and evaluation splits. Our contributions include: 1. An analysis of causes and effects of topic leakage; 2. A demonstration of the HITS in reducing the effects of topic leakage; and 3. The Robust Authorship Verification bENchmark (RAVEN) that allows topic shortcut test to uncover AV models’ reliance on topic-specific features.},
  archive      = {J_TACL},
  author       = {Sawatphol, Jitkapat and Udomcharoenchaikit, Can and Nutanong, Sarana},
  doi          = {10.1162/tacl_a_00709},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1363-1377},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Addressing topic leakage in cross-topic evaluation for authorship verification},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Characterizing learning curves during language model
pre-training: Learning, forgetting, and stability. <em>TACL</em>,
<em>12</em>, 1346–1362. (<a
href="https://doi.org/10.1162/tacl_a_00708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How do language models learn to make predictions during pre-training? To study this, we extract learning curves from five autoregressive English language model pre-training runs, for 1M unseen tokens in context. We observe that the language models generate short repetitive phrases before learning to generate longer and more coherent text. We also find that individual tokens often exhibit sudden increases or decreases in loss that are surprisingly consistent across pre-training runs. To better understand these fluctuations, we quantify the final surprisal, within-run variability, age of acquisition, forgettability, and cross-run variability of learning curves for individual tokens in context. More frequent tokens reach lower final surprisals, exhibit less variability within and across pre-training runs, are learned earlier, and are less likely to be “forgotten” during pre-training. Higher n -gram probabilities further accentuate these effects. Independent of the target token, shorter and more frequent contexts correlate with marginally more stable and quickly acquired predictions. Based on our results, we argue for the existence of sequential learning dependencies between different model capabilities, and we characterize language model learning as early n -gram learning before gradual refinement of tail n -gram predictions.},
  archive      = {J_TACL},
  author       = {Chang, Tyler A. and Tu, Zhuowen and Bergen, Benjamin K.},
  doi          = {10.1162/tacl_a_00708},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1346-1362},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Characterizing learning curves during language model pre-training: Learning, forgetting, and stability},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NoviCode: Generating programs from natural language
utterances by novices. <em>TACL</em>, <em>12</em>, 1330–1345. (<a
href="https://doi.org/10.1162/tacl_a_00694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current Text-to-Code models demonstrate impressive capabilities in generating executable code from natural language snippets. However, current studies focus on technical instructions and programmer-oriented language, and it is an open question whether these models can effectively translate natural language descriptions given by non-technical users and express complex goals, to an executable program that contains an intricate flow—composed of API access and control structures as loops, conditions, and sequences. To unlock the challenge of generating a complete program from a plain non-technical description we present NoviCode, a novel NL Programming task, which takes as input an API and a natural language description by a novice non-programmer, and provides an executable program as output. To assess the efficacy of models on this task, we provide a novel benchmark accompanied by test suites wherein the generated program code is assessed not according to their form, but according to their functional execution. Our experiments show that, first, NoviCode is indeed a challenging task in the code synthesis domain, and that generating complex code from non-technical instructions goes beyond the current Text-to-Code paradigm. Second, we show that a novel approach wherein we align the NL utterances with the compositional hierarchical structure of the code, greatly enhances the performance of LLMs on this task, compared with the end-to-end Text-to-Code counterparts.},
  archive      = {J_TACL},
  author       = {Mordechai, Asaf Achi and Goldberg, Yoav and Tsarfaty, Reut},
  doi          = {10.1162/tacl_a_00694},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1330-1345},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {NoviCode: Generating programs from natural language utterances by novices},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). L2CEval: Evaluating language-to-code generation capabilities
of large language models. <em>TACL</em>, <em>12</em>, 1311–1329. (<a
href="https://doi.org/10.1162/tacl_a_00705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, large language models (LLMs), especially those that are pretrained on code, have demonstrated strong capabilities in generating programs from natural language inputs. Despite promising results, there is a notable lack of a comprehensive evaluation of these models’ language-to-code generation capabilities. Existing studies often focus on specific tasks, model architectures, or learning paradigms, leading to a fragmented understanding of the overall landscape. In this work, we present L2CEval , a systematic evaluation of the language-to-code generation capabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing, math reasoning, and Python programming, analyzing the factors that potentially affect their performance, such as model size, pretraining data, instruction tuning, and different prompting methods. In addition, we assess confidence calibration, and conduct human evaluations to identify typical failures across different tasks and models. L2CEval offers a comprehensive understanding of the capabilities and limitations of LLMs in language-to-code generation. We release the evaluation framework 1 and all model outputs, hoping to lay the groundwork for further future research. All future evaluations ( e.g., LLaMA-3, StarCoder2, etc) will be updated on the project website: https://l2c-eval.github.io/ .},
  archive      = {J_TACL},
  author       = {Ni, Ansong and Yin, Pengcheng and Zhao, Yilun and Riddell, Martin and Feng, Troy and Shen, Rui and Yin, Stephen and Liu, Ye and Yavuz, Semih and Xiong, Caiming and Joty, Shafiq and Zhou, Yingbo and Radev, Dragomir and Cohan, Arman},
  doi          = {10.1162/tacl_a_00705},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1311-1329},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {L2CEval: Evaluating language-to-code generation capabilities of large language models},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reading subtext: Evaluating large language models on short
story summarization with writers. <em>TACL</em>, <em>12</em>, 1290–1310.
(<a href="https://doi.org/10.1162/tacl_a_00702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We evaluate recent Large Language Models (LLMs) on the challenging task of summarizing short stories, which can be lengthy, and include nuanced subtext or scrambled timelines. Importantly, we work directly with authors to ensure that the stories have not been shared online (and therefore are unseen by the models), and to obtain informed evaluations of summary quality using judgments from the authors themselves. Through quantitative and qualitative analysis grounded in narrative theory, we compare GPT-4, Claude-2.1, and LLama-2-70B. We find that all three models make faithfulness mistakes in over 50% of summaries and struggle with specificity and interpretation of difficult subtext. We additionally demonstrate that LLM ratings and other automatic metrics for summary quality do not correlate well with the quality ratings from the writers.},
  archive      = {J_TACL},
  author       = {Subbiah, Melanie and Zhang, Sean and Chilton, Lydia B. and McKeown, Kathleen},
  doi          = {10.1162/tacl_a_00702},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1290-1310},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Reading subtext: Evaluating large language models on short story summarization with writers},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Not eliminate but aggregate: Post-hoc control over
mixture-of-experts to address shortcut shifts in natural language
understanding. <em>TACL</em>, <em>12</em>, 1268–1289. (<a
href="https://doi.org/10.1162/tacl_a_00701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent models for natural language understanding are inclined to exploit simple patterns in datasets, commonly known as shortcuts . These shortcuts hinge on spurious correlations between labels and latent features existing in the training data. At inference time, shortcut-dependent models are likely to generate erroneous predictions under distribution shifts, particularly when some latent features are no longer correlated with the labels. To avoid this, previous studies have trained models to eliminate the reliance on shortcuts. In this study, we explore a different direction: pessimistically aggregating the predictions of a mixture-of-experts, assuming each expert captures relatively different latent features. The experimental results demonstrate that our post-hoc control over the experts significantly enhances the model’s robustness to the distribution shift in shortcuts. Additionally, we show that our approach has some practical advantages. We also analyze our model and provide results to support the assumption. 1},
  archive      = {J_TACL},
  author       = {Honda, Ukyo and Oka, Tatsushi and Zhang, Peinan and Mita, Masato},
  doi          = {10.1162/tacl_a_00701},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1268-1289},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Not eliminate but aggregate: Post-hoc control over mixture-of-experts to address shortcut shifts in natural language understanding},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessing the role of context in chat translation
evaluation: Is context helpful and under what conditions? <em>TACL</em>,
<em>12</em>, 1250–1267. (<a
href="https://doi.org/10.1162/tacl_a_00700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the recent success of automatic metrics for assessing translation quality, their application in evaluating the quality of machine-translated chats has been limited. Unlike more structured texts like news, chat conversations are often unstructured, short, and heavily reliant on contextual information. This poses questions about the reliability of existing sentence-level metrics in this domain as well as the role of context in assessing the translation quality. Motivated by this, we conduct a meta-evaluation of existing automatic metrics, primarily designed for structured domains such as news, to assess the quality of machine-translated chats. We find that reference-free metrics lag behind reference-based ones, especially when evaluating translation quality in out-of-English settings. We then investigate how incorporating conversational contextual information in these metrics for sentence-level evaluation affects their performance. Our findings show that augmenting neural learned metrics with contextual information helps improve correlation with human judgments in the reference-free scenario and when evaluating translations in out-of-English settings. Finally, we propose a new evaluation metric, Context-MQM , that utilizes bilingual context with a large language model (LLM) and further validate that adding context helps even for LLM-based evaluation metrics.},
  archive      = {J_TACL},
  author       = {Agrawal, Sweta and Farajian, Amin and Fernandes, Patrick and Rei, Ricardo and Martins, André F. T.},
  doi          = {10.1162/tacl_a_00700},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1250-1267},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Assessing the role of context in chat translation evaluation: Is context helpful and under what conditions?},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Do vision and language models share concepts? A vector space
alignment study. <em>TACL</em>, <em>12</em>, 1232–1249. (<a
href="https://doi.org/10.1162/tacl_a_00698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale pretrained language models (LMs) are said to “lack the ability to connect utterances to the world” (Bender and Koller, 2020 ), because they do not have “mental models of the world” (Mitchell and Krakauer, 2023 ). If so, one would expect LM representations to be unrelated to representations induced by vision models. We present an empirical evaluation across four families of LMs (BERT, GPT-2, OPT, and LLaMA-2) and three vision model architectures (ResNet, SegFormer, and MAE). Our experiments show that LMs partially converge towards representations isomorphic to those of vision models, subject to dispersion, polysemy, and frequency. This has important implications for both multi-modal processing and the LM understanding debate (Mitchell and Krakauer, 2023 ). 1},
  archive      = {J_TACL},
  author       = {Li, Jiaang and Kementchedjhieva, Yova and Fierro, Constanza and Søgaard, Anders},
  doi          = {10.1162/tacl_a_00698},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1232-1249},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Do vision and language models share concepts? a vector space alignment study},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Retrieval-style in-context learning for few-shot
hierarchical text classification. <em>TACL</em>, <em>12</em>, 1214–1231.
(<a href="https://doi.org/10.1162/tacl_a_00697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical text classification (HTC) is an important task with broad applications, and few-shot HTC has gained increasing interest recently. While in-context learning (ICL) with large language models (LLMs) has achieved significant success in few-shot learning, it is not as effective for HTC because of the expansive hierarchical label sets and extremely ambiguous labels. In this work, we introduce the first ICL-based framework with LLM for few-shot HTC. We exploit a retrieval database to identify relevant demonstrations, and an iterative policy to manage multi-layer hierarchical labels. Particularly, we equip the retrieval database with HTC label-aware representations for the input texts, which is achieved by continual training on a pretrained language model with masked language modeling (MLM), layer-wise classification (CLS, specifically for HTC), and a novel divergent contrastive learning (DCL, mainly for adjacent semantically similar labels) objective. Experimental results on three benchmark datasets demonstrate superior performance of our method, and we can achieve state-of-the-art results in few-shot HTC.},
  archive      = {J_TACL},
  author       = {Chen, Huiyao and Zhao, Yu and Chen, Zulong and Wang, Mengjia and Li, Liangyue and Zhang, Meishan and Zhang, Min},
  doi          = {10.1162/tacl_a_00697},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1214-1231},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Retrieval-style in-context learning for few-shot hierarchical text classification},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Retrieval-pretrained transformer: Long-range language
modeling with self-retrieval. <em>TACL</em>, <em>12</em>, 1197–1213. (<a
href="https://doi.org/10.1162/tacl_a_00693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retrieval-augmented language models (LMs) have received much attention recently. However, typically the retriever is not trained jointly as a native component of the LM, but added post-hoc to an already-pretrained LM, which limits the ability of the LM and the retriever to adapt to one another. In this work, we propose the Retrieval-Pretrained Transformer (RPT), an architecture and training procedure for jointly training a retrieval-augmented LM from scratch and applying it to the task of modeling long texts. Given a recently generated text chunk in a long document, the LM computes query representations, which are then used to retrieve earlier chunks in the document, located potentially tens of thousands of tokens before. Information from retrieved chunks is fused into the LM representations to predict the next target chunk. We train the retriever component with a semantic objective, where the goal is to retrieve chunks that increase the probability of the next chunk, according to a reference LM. We evaluate RPT on four long-range language modeling tasks, spanning books, code, and mathematical writing, and demonstrate that RPT improves retrieval quality and subsequently perplexity across the board compared to strong baselines.},
  archive      = {J_TACL},
  author       = {Rubin, Ohad and Berant, Jonathan},
  doi          = {10.1162/tacl_a_00693},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1197-1213},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Retrieval-pretrained transformer: Long-range language modeling with self-retrieval},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hypernetworks for personalizing ASR to atypical speech.
<em>TACL</em>, <em>12</em>, 1182–1196. (<a
href="https://doi.org/10.1162/tacl_a_00696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameter-efficient fine-tuning (PEFT) for personalizing automatic speech recognition (ASR) has recently shown promise for adapting general population models to atypical speech. However, these approaches assume a priori knowledge of the atypical speech disorder being adapted for—the diagnosis of which requires expert knowledge that is not always available. Even given this knowledge, data scarcity and high inter-/intra-speaker variability further limit the effectiveness of traditional fine-tuning. To circumvent these challenges, we first identify the minimal set of model parameters required for ASR adaptation. Our analysis of each individual parameter’s effect on adaptation performance allows us to reduce Word Error Rate (WER) by half while adapting 0.03% of all weights. Alleviating the need for cohort-specific models, we next propose the novel use of a meta-learned hypernetwork to generate highly individualized, utterance-level adaptations on-the-fly for a diverse set of atypical speech characteristics. Evaluating adaptation at the global, cohort, and individual-level, we show that hypernetworks generalize better to out-of-distribution speakers, while maintaining an overall relative WER reduction of 75.2% using 0.1% of the full parameter budget.},
  archive      = {J_TACL},
  author       = {Müller-Eberstein, Max and Yee, Dianna and Yang, Karren and Mantena, Gautam Varma and Lea, Colin},
  doi          = {10.1162/tacl_a_00696},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1182-1196},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Hypernetworks for personalizing ASR to atypical speech},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Investigating hallucinations in pruned large language models
for abstractive summarization. <em>TACL</em>, <em>12</em>, 1163–1181.
(<a href="https://doi.org/10.1162/tacl_a_00695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the remarkable performance of generative large language models (LLMs) on abstractive summarization, they face two significant challenges: their considerable size and tendency to hallucinate. Hallucinations are concerning because they erode reliability and raise safety issues. Pruning is a technique that reduces model size by removing redundant weights, enabling more efficient sparse inference. Pruned models yield downstream task performance comparable to the original, making them ideal alternatives when operating on a limited budget. However, the effect that pruning has upon hallucinations in abstractive summarization with LLMs has yet to be explored. In this paper, we provide an extensive empirical study across five summarization datasets, two state-of-the-art pruning methods, and five instruction-tuned LLMs. Surprisingly, we find that hallucinations are less prevalent from pruned LLMs than the original models. Our analysis suggests that pruned models tend to depend more on the source document for summary generation. This leads to a higher lexical overlap between the generated summary and the source document, which could be a reason for the reduction in hallucination risk. 1},
  archive      = {J_TACL},
  author       = {Chrysostomou, George and Zhao, Zhixue and Williams, Miles and Aletras, Nikolaos},
  doi          = {10.1162/tacl_a_00695},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1163-1181},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Investigating hallucinations in pruned large language models for abstractive summarization},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How often are errors in natural language reasoning due to
paraphrastic variability? <em>TACL</em>, <em>12</em>, 1143–1162. (<a
href="https://doi.org/10.1162/tacl_a_00692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models have been shown to behave inconsistently in response to meaning-preserving paraphrastic inputs. At the same time, researchers evaluate the knowledge and reasoning abilities of these models with test evaluations that do not disaggregate the effect of paraphrastic variability on performance. We propose a metric, P C , for evaluating the paraphrastic consistency of natural language reasoning models based on the probability of a model achieving the same correctness on two paraphrases of the same problem. We mathematically connect this metric to the proportion of a model’s variance in correctness attributable to paraphrasing. To estimate P C , we collect ParaNlu , a dataset of 7,782 human-written and validated paraphrased reasoning problems constructed on top of existing benchmark datasets for defeasible and abductive natural language inference. 1 Using ParaNlu , we measure the paraphrastic consistency of several model classes and show that consistency dramatically increases with pretraining but not fine-tuning. All models tested exhibited room for improvement in paraphrastic consistency.},
  archive      = {J_TACL},
  author       = {Srikanth, Neha and Carpuat, Marine and Rudinger, Rachel},
  doi          = {10.1162/tacl_a_00692},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1143-1162},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {How often are errors in natural language reasoning due to paraphrastic variability?},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Do language models enjoy their own stories? Prompting large
language models for automatic story evaluation. <em>TACL</em>,
<em>12</em>, 1122–1142. (<a
href="https://doi.org/10.1162/tacl_a_00689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Storytelling is an integral part of human experience and plays a crucial role in social interactions. Thus, Automatic Story Evaluation (ASE) and Generation (ASG) could benefit society in multiple ways, but they are challenging tasks which require high-level human abilities such as creativity, reasoning, and deep understanding. Meanwhile, Large Language Models (LLMs) now achieve state-of-the-art performance on many NLP tasks. In this paper, we study whether LLMs can be used as substitutes for human annotators for ASE. We perform an extensive analysis of the correlations between LLM ratings, other automatic measures, and human annotations, and we explore the influence of prompting on the results and the explainability of LLM behaviour. Most notably, we find that LLMs outperform current automatic measures for system-level evaluation but still struggle at providing satisfactory explanations for their answers.},
  archive      = {J_TACL},
  author       = {Chhun, Cyril and Suchanek, Fabian M. and Clavel, Chloé},
  doi          = {10.1162/tacl_a_00689},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1122-1142},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Do language models enjoy their own stories? prompting large language models for automatic story evaluation},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Segmentation-free streaming machine translation.
<em>TACL</em>, <em>12</em>, 1104–1121. (<a
href="https://doi.org/10.1162/tacl_a_00691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Streaming Machine Translation (MT) is the task of translating an unbounded input text stream in real-time. The traditional cascade approach, which combines an Automatic Speech Recognition (ASR) and an MT system, relies on an intermediate segmentation step which splits the transcription stream into sentence-like units. However, the incorporation of a hard segmentation constrains the MT system and is a source of errors. This paper proposes a Segmentation-Free framework that enables the model to translate an unsegmented source stream by delaying the segmentation decision until after the translation has been generated. Extensive experiments show how the proposed Segmentation-Free framework has better quality-latency trade-off than competing approaches that use an independent segmentation model. 1},
  archive      = {J_TACL},
  author       = {Iranzo-Sánchez, Javier and Iranzo-Sánchez, Jorge and Giménez, Adrià and Civera, Jorge and Juan, Alfons},
  doi          = {10.1162/tacl_a_00691},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1104-1121},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Segmentation-free streaming machine translation},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Are language models more like libraries or like librarians?
Bibliotechnism, the novel reference problem, and the attitudes of LLMs.
<em>TACL</em>, <em>12</em>, 1087–1103. (<a
href="https://doi.org/10.1162/tacl_a_00690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Are LLMs cultural technologies like photocopiers or printing presses, which transmit information but cannot create new content? A challenge for this idea, which we call bibliotechnism , is that LLMs generate novel text. We begin with a defense of bibliotechnism, showing how even novel text may inherit its meaning from original human-generated text. We then argue that bibliotechnism faces an independent challenge from examples in which LLMs generate novel reference , using new names to refer to new entities. Such examples could be explained if LLMs were not cultural technologies but had beliefs, desires, and intentions. According to interpretationism in the philosophy of mind, a system has such attitudes if and only if its behavior is well explained by the hypothesis that it does. Interpretationists may hold that LLMs have attitudes, and thus have a simple solution to the novel reference problem. We emphasize, however, that interpretationism is compatible with very simple creatures having attitudes and differs sharply from views that presuppose these attitudes require consciousness, sentience, or intelligence (topics about which we make no claims).},
  archive      = {J_TACL},
  author       = {Lederman, Harvey and Mahowald, Kyle},
  doi          = {10.1162/tacl_a_00690},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1087-1103},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Are language models more like libraries or like librarians? bibliotechnism, the novel reference problem, and the attitudes of LLMs},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ARN: Analogical reasoning on narratives. <em>TACL</em>,
<em>12</em>, 1063–1086. (<a
href="https://doi.org/10.1162/tacl_a_00688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a core cognitive skill that enables the transferability of information across domains, analogical reasoning has been extensively studied for both humans and computational models. However, while cognitive theories of analogy often focus on narratives and study the distinction between surface, relational, and system similarities, existing work in natural language processing has a narrower focus as far as relational analogies between word pairs. This gap brings a natural question: can state-of-the-art large language models (LLMs) detect system analogies between narratives? To gain insight into this question and extend word-based relational analogies to relational system analogies, we devise a comprehensive computational framework that operationalizes dominant theories of analogy, using narrative elements to create surface and system mappings. Leveraging the interplay between these mappings, we create a binary task and benchmark for Analogical Reasoning on Narratives ( ARN ), covering four categories of far (cross-domain)/near (within-domain) analogies and disanalogies. We show that while all LLMs can largely recognize near analogies, even the largest ones struggle with far analogies in a zero-shot setting, with GPT4.0 scoring below random. Guiding the models through solved examples and Chain-of-Thought reasoning enhances their analogical reasoning ability. Yet, since even in the few-shot setting, the best model only performs halfway between random and humans, ARN opens exciting directions for computational analogical reasoners.},
  archive      = {J_TACL},
  author       = {Sourati, Zhivar and Ilievski, Filip and Sommerauer, Pia and Jiang, Yifan},
  doi          = {10.1162/tacl_a_00688},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1063-1086},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {ARN: Analogical reasoning on narratives},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Do multi-document summarization models synthesize?
<em>TACL</em>, <em>12</em>, 1043–1062. (<a
href="https://doi.org/10.1162/tacl_a_00687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-document summarization entails producing concise synopses of collections of inputs. For some applications, the synopsis should accurately synthesize inputs with respect to a key aspect, e.g., a synopsis of film reviews written about a particular movie should reflect the average critic consensus. As a more consequential example, narrative summaries that accompany biomedical systematic reviews of clinical trial results should accurately summarize the potentially conflicting results from individual trials. In this paper we ask: To what extent do modern multi-document summarization models implicitly perform this sort of synthesis? We run experiments over opinion and evidence synthesis datasets using a suite of summarization models, from fine-tuned transformers to GPT-4. We find that existing models partially perform synthesis, but imperfectly: Even the best performing models are over-sensitive to changes in input ordering and under-sensitive to changes in input compositions (e.g., ratio of positive to negative reviews). We propose a simple, general, effective method for improving model synthesis capabilities by generating an explicitly diverse set of candidate outputs, and then selecting from these the string best aligned with the expected aggregate measure for the inputs, or abstaining when the model produces no good candidate.},
  archive      = {J_TACL},
  author       = {DeYoung, Jay and Martinez, Stephanie C. and Marshall, Iain J. and Wallace, Byron C.},
  doi          = {10.1162/tacl_a_00687},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1043-1062},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Do multi-document summarization models synthesize?},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-level shared knowledge guided learning for knowledge
graph completion. <em>TACL</em>, <em>12</em>, 1027–1042. (<a
href="https://doi.org/10.1162/tacl_a_00686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the task of Knowledge Graph Completion (KGC), the existing datasets and their inherent subtasks carry a wealth of shared knowledge that can be utilized to enhance the representation of knowledge triplets and overall performance. However, no current studies specifically address the shared knowledge within KGC. To bridge this gap, we introduce a multi-level S hared K nowledge G uided learning method (SKG) that operates at both the dataset and task levels. On the dataset level, SKG-KGC broadens the original dataset by identifying shared features within entity sets via text summarization. On the task level, for the three typical KGC subtasks—head entity prediction, relation prediction, and tail entity prediction—we present an innovative multi-task learning architecture with dynamically adjusted loss weights. This approach allows the model to focus on more challenging and underperforming tasks, effectively mitigating the imbalance of knowledge sharing among subtasks. Experimental results demonstrate that SKG-KGC outperforms existing text-based methods significantly on three well-known datasets, with the most notable improvement on WN18RR (MRR: 66.6% → 72.2%, Hit@1: 58.7% → 67.0%).},
  archive      = {J_TACL},
  author       = {Shan, Yongxue and Zhou, Jie and Peng, Jie and Zhou, Xin and Yin, Jiaqian and Wang, Xiaodong},
  doi          = {10.1162/tacl_a_00686},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1027-1042},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Multi-level shared knowledge guided learning for knowledge graph completion},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Do LLMs exhibit human-like response biases? A case study in
survey design. <em>TACL</em>, <em>12</em>, 1011–1026. (<a
href="https://doi.org/10.1162/tacl_a_00685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One widely cited barrier to the adoption of LLMs as proxies for humans in subjective tasks is their sensitivity to prompt wording—but interestingly, humans also display sensitivities to instruction changes in the form of response biases . We investigate the extent to which LLMs reflect human response biases, if at all. We look to survey design, where human response biases caused by changes in the wordings of “prompts” have been extensively explored in social psychology literature. Drawing from these works, we design a dataset and framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior, particularly in models that have undergone RLHF. Furthermore, even if a model shows a significant change in the same direction as humans, we find that they are sensitive to perturbations that do not elicit significant changes in humans. These results highlight the pitfalls of using LLMs as human proxies, and underscore the need for finer-grained characterizations of model behavior. 1},
  archive      = {J_TACL},
  author       = {Tjuatja, Lindia and Chen, Valerie and Wu, Tongshuang and Talwalkwar, Ameet and Neubig, Graham},
  doi          = {10.1162/tacl_a_00685},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {1011-1026},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Do LLMs exhibit human-like response biases? a case study in survey design},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SEEP: Training dynamics grounds latent representation search
for mitigating backdoor poisoning attacks. <em>TACL</em>, <em>12</em>,
996–1010. (<a href="https://doi.org/10.1162/tacl_a_00684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern NLP models are often trained on public datasets drawn from diverse sources, rendering them vulnerable to data poisoning attacks. These attacks can manipulate the model’s behavior in ways engineered by the attacker. One such tactic involves the implantation of backdoors, achieved by poisoning specific training instances with a textual trigger and a target class label. Several strategies have been proposed to mitigate the risks associated with backdoor attacks by identifying and removing suspected poisoned examples. However, we observe that these strategies fail to offer effective protection against several advanced backdoor attacks. To remedy this deficiency, we propose a novel defensive mechanism that first exploits training dynamics to identify poisoned samples with high precision, followed by a label propagation step to improve recall and thus remove the majority of poisoned instances. Compared with recent advanced defense methods, our method considerably reduces the success rates of several backdoor attacks while maintaining high classification accuracy on clean test sets.},
  archive      = {J_TACL},
  author       = {He, Xuanli and Xu, Qiongkai and Wang, Jun and Rubinstein, Benjamin I. P. and Cohn, Trevor},
  doi          = {10.1162/tacl_a_00684},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {996-1010},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {SEEP: Training dynamics grounds latent representation search for mitigating backdoor poisoning attacks},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Xcomet: Transparent machine translation evaluation through
fine-grained error detection. <em>TACL</em>, <em>12</em>, 979–995. (<a
href="https://doi.org/10.1162/tacl_a_00683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Widely used learned metrics for machine translation evaluation, such as Comet and Bleurt , estimate the quality of a translation hypothesis by providing a single sentence-level score. As such, they offer little insight into translation errors (e.g., what are the errors and what is their severity). On the other hand, generative large language models (LLMs) are amplifying the adoption of more granular strategies to evaluation, attempting to detail and categorize translation errors. In this work, we introduce x comet , an open-source learned metric designed to bridge the gap between these approaches. x comet integrates both sentence-level evaluation and error span detection capabilities, exhibiting state-of-the-art performance across all types of evaluation (sentence-level, system-level, and error span detection). Moreover, it does so while highlighting and categorizing error spans, thus enriching the quality assessment. We also provide a robustness analysis with stress tests, and show that x comet is largely capable of identifying localized critical errors and hallucinations.},
  archive      = {J_TACL},
  author       = {Guerreiro, Nuno M. and Rei, Ricardo and Stigt, Daan van and Coheur, Luisa and Colombo, Pierre and Martins, André F. T.},
  doi          = {10.1162/tacl_a_00683},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {979-995},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Xcomet: Transparent machine translation evaluation through fine-grained error detection},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CreoleVal: Multilingual multitask benchmarks for creoles.
<em>TACL</em>, <em>12</em>, 950–978. (<a
href="https://doi.org/10.1162/tacl_a_00682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creoles represent an under-explored and marginalized group of languages, with few available resources for NLP research. While the genealogical ties between Creoles and a number of highly resourced languages imply a significant potential for transfer learning, this potential is hampered due to this lack of annotated data. In this work we present CreoleVal , a collection of benchmark datasets spanning 8 different NLP tasks, covering up to 28 Creole languages; it is an aggregate of novel development datasets for reading comprehension relation classification, and machine translation for Creoles, in addition to a practical gateway to a handful of preexisting benchmarks. For each benchmark, we conduct baseline experiments in a zero-shot setting in order to further ascertain the capabilities and limitations of transfer learning for Creoles. Ultimately, we see CreoleVal as an opportunity to empower research on Creoles in NLP and computational linguistics, and in general, a step towards more equitable language technology around the globe.},
  archive      = {J_TACL},
  author       = {Lent, Heather and Tatariya, Kushal and Dabre, Raj and Chen, Yiyi and Fekete, Marcell and Ploeger, Esther and Zhou, Li and Armstrong, Ruth-Ann and Eijansantos, Abee and Malau, Catriona and Heje, Hans Erik and Lavrinovics, Ernests and Kanojia, Diptesh and Belony, Paul and Bollmann, Marcel and Grobol, Loïc and Lhoneux, Miryam de and Hershcovich, Daniel and DeGraff, Michel and Søgaard, Anders and Bjerva, Johannes},
  doi          = {10.1162/tacl_a_00682},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {12},
  pages        = {950-978},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {CreoleVal: Multilingual multitask benchmarks for creoles},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). State of what art? A call for multi-prompt LLM evaluation.
<em>TACL</em>, <em>12</em>, 933–949. (<a
href="https://doi.org/10.1162/tacl_a_00681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in LLMs have led to an abundance of evaluation benchmarks, which typically rely on a single instruction template per task. We create a large-scale collection of instruction paraphrases and comprehensively analyze the brittleness introduced by single-prompt evaluations across 6.5M instances, involving 20 different LLMs and 39 tasks from 3 benchmarks. We find that different instruction templates lead to very different performance, both absolute and relative. Instead, we propose a set of diverse metrics on multiple instruction paraphrases , specifically tailored for different use cases (e.g., LLM vs. downstream development), ensuring a more reliable and meaningful assessment of LLM capabilities. We show that our metrics provide new insights into the strengths and limitations of current LLMs.},
  archive      = {J_TACL},
  author       = {Mizrahi, Moran and Kaplan, Guy and Malkin, Dan and Dror, Rotem and Shahaf, Dafna and Stanovsky, Gabriel},
  doi          = {10.1162/tacl_a_00681},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {8},
  pages        = {933-949},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {State of what art? a call for multi-prompt LLM evaluation},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring continual learning of compositional generalization
in NLI. <em>TACL</em>, <em>12</em>, 912–932. (<a
href="https://doi.org/10.1162/tacl_a_00680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional Natural Language Inference (NLI) has been explored to assess the true abilities of neural models to perform NLI. Yet, current evaluations assume models to have full access to all primitive inferences in advance, in contrast to humans that continuously acquire inference knowledge. In this paper, we introduce the C ontinual C ompositional Gen eralization in Inference (C 2 Gen NLI) challenge, where a model continuously acquires knowledge of constituting primitive inference tasks as a basis for compositional inferences. We explore how continual learning affects compositional generalization in NLI, by designing a continual learning setup for compositional NLI inference tasks. Our experiments demonstrate that models fail to compositionally generalize in a continual scenario. To address this problem, we first benchmark various continual learning algorithms and verify their efficacy. We then further analyze C 2 Gen, focusing on how to order primitives and compositional inference types, and examining correlations between subtasks. Our analyses show that by learning subtasks continuously while observing their dependencies and increasing degrees of difficulty, continual learning can enhance composition generalization ability. 1},
  archive      = {J_TACL},
  author       = {Fu, Xiyan and Frank, Anette},
  doi          = {10.1162/tacl_a_00680},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {8},
  pages        = {912-932},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Exploring continual learning of compositional generalization in NLI},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decision-oriented dialogue for human-AI collaboration.
<em>TACL</em>, <em>12</em>, 892–911. (<a
href="https://doi.org/10.1162/tacl_a_00679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a class of tasks called decision-oriented dialogues , in which AI assistants such as large language models (LMs) must collaborate with one or more humans via natural language to help them make complex decisions. We formalize three domains in which users face everyday decisions: (1) choosing an assignment of reviewers to conference papers, (2) planning a multi-step itinerary in a city, and (3) negotiating travel plans for a group of friends. In each of these settings, AI assistants and users have disparate abilities that they must combine to arrive at the best decision: Assistants can access and process large amounts of information, while users have preferences and constraints external to the system. For each task, we build a dialogue environment where agents receive a reward based on the quality of the final decision they reach. We evaluate LMs in self-play and in collaboration with humans and find that they fall short compared to human assistants, achieving much lower rewards despite engaging in longer dialogues. We highlight a number of challenges models face in decision-oriented dialogues, ranging from goal-directed behavior to reasoning and optimization, and release our environments as a testbed for future work.},
  archive      = {J_TACL},
  author       = {Lin, Jessy and Tomlin, Nicholas and Andreas, Jacob and Eisner, Jason},
  doi          = {10.1162/tacl_a_00679},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {8},
  pages        = {892-911},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Decision-oriented dialogue for human-AI collaboration},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Can authorship attribution models distinguish speakers in
speech transcripts? <em>TACL</em>, <em>12</em>, 875–891. (<a
href="https://doi.org/10.1162/tacl_a_00678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Authorship verification is the task of determining if two distinct writing samples share the same author and is typically concerned with the attribution of written text. In this paper, we explore the attribution of transcribed speech , which poses novel challenges. The main challenge is that many stylistic features, such as punctuation and capitalization, are not informative in this setting. On the other hand, transcribed speech exhibits other patterns, such as filler words and backchannels (e.g., um , uh-huh ), which may be characteristic of different speakers. We propose a new benchmark for speaker attribution focused on human-transcribed conversational speech transcripts. To limit spurious associations of speakers with topic, we employ both conversation prompts and speakers participating in the same conversation to construct verification trials of varying difficulties. We establish the state of the art on this new benchmark by comparing a suite of neural and non-neural baselines, finding that although written text attribution models achieve surprisingly good performance in certain settings, they perform markedly worse as conversational topic is increasingly controlled. We present analyses of the impact of transcription style on performance as well as the ability of fine-tuning on speech transcripts to improve performance. 1},
  archive      = {J_TACL},
  author       = {Aggazzotti, Cristina and Andrews, Nicholas and Smith, Elizabeth Allyn},
  doi          = {10.1162/tacl_a_00678},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {8},
  pages        = {875-891},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Can authorship attribution models distinguish speakers in speech transcripts?},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Context-aware machine translation with source coreference
explanation. <em>TACL</em>, <em>12</em>, 856–874. (<a
href="https://doi.org/10.1162/tacl_a_00677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite significant improvements in enhancing the quality of translation, context-aware machine translation (MT) models underperform in many cases. One of the main reasons is that they fail to utilize the correct features from context when the context is too long or their models are overly complex. This can lead to the explain-away effect, wherein the models only consider features easier to explain predictions, resulting in inaccurate translations. To address this issue, we propose a model that explains the decisions made for translation by predicting coreference features in the input. We construct a model for input coreference by exploiting contextual features from both the input and translation output representations on top of an existing MT model. We evaluate and analyze our method in the WMT document-level translation task of English-German dataset, the English-Russian dataset, and the multilingual TED talk dataset, demonstrating an improvement of over 1.0 BLEU score when compared with other context-aware models.},
  archive      = {J_TACL},
  author       = {Vu, Huy Hien and Kamigaito, Hidetaka and Watanabe, Taro},
  doi          = {10.1162/tacl_a_00677},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {8},
  pages        = {856-874},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Context-aware machine translation with source coreference explanation},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revisiting meta-evaluation for grammatical error correction.
<em>TACL</em>, <em>12</em>, 837–855. (<a
href="https://doi.org/10.1162/tacl_a_00676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metrics are the foundation for automatic evaluation in grammatical error correction (GEC), with their evaluation of the metrics (meta-evaluation) relying on their correlation with human judgments. However, conventional meta-evaluations in English GEC encounter several challenges, including biases caused by inconsistencies in evaluation granularity and an outdated setup using classical systems. These problems can lead to misinterpretation of metrics and potentially hinder the applicability of GEC techniques. To address these issues, this paper proposes SEEDA, a new dataset for GEC meta-evaluation. SEEDA consists of corrections with human ratings along two different granularities: edit-based and sentence-based , covering 12 state-of-the-art systems including large language models, and two human corrections with different focuses. The results of improved correlations by aligning the granularity in the sentence-level meta-evaluation suggest that edit-based metrics may have been underestimated in existing studies. Furthermore, correlations of most metrics decrease when changing from classical to neural systems, indicating that traditional metrics are relatively poor at evaluating fluently corrected sentences with many edits.},
  archive      = {J_TACL},
  author       = {Kobayashi, Masamune and Mita, Masato and Komachi, Mamoru},
  doi          = {10.1162/tacl_a_00676},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {8},
  pages        = {837-855},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Revisiting meta-evaluation for grammatical error correction},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A closer look at classification evaluation metrics and a
critical reflection of common evaluation practice. <em>TACL</em>,
<em>12</em>, 820–836. (<a
href="https://doi.org/10.1162/tacl_a_00675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification systems are evaluated in a countless number of papers. However, we find that evaluation practice is often nebulous. Frequently, metrics are selected without arguments, and blurry terminology invites misconceptions. For instance, many works use so-called ‘macro’ metrics to rank systems (e.g., ‘macro F1’) but do not clearly specify what they would expect from such a ‘macro’ metric. This is problematic, since picking a metric can affect research findings and thus any clarity in the process should be maximized. Starting from the intuitive concepts of bias and prevalence , we perform an analysis of common evaluation metrics. The analysis helps us understand the metrics’ underlying properties, and how they align with expectations as found expressed in papers. Then we reflect on the practical situation in the field, and survey evaluation practice in recent shared tasks. We find that metric selection is often not supported with convincing arguments, an issue that can make a system ranking seem arbitrary. Our work aims at providing overview and guidance for more informed and transparent metric selection, fostering meaningful evaluation.},
  archive      = {J_TACL},
  author       = {Opitz, Juri},
  doi          = {10.1162/tacl_a_00675},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {6},
  pages        = {820-836},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {A closer look at classification evaluation metrics and a critical reflection of common evaluation practice},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparing humans and large language models on an
experimental protocol inventory for theory of mind evaluation (EPITOME).
<em>TACL</em>, <em>12</em>, 803–819. (<a
href="https://doi.org/10.1162/tacl_a_00674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address a growing debate about the extent to which large language models (LLMs) produce behavior consistent with Theory of Mind (ToM) in humans. We present EPITOME: a battery of six experiments that tap diverse ToM capacities, including belief attribution, emotional inference, and pragmatic reasoning. We elicit a performance baseline from human participants for each task. We use the dataset to ask whether distributional linguistic information learned by LLMs is sufficient to explain ToM in humans. We compare performance of five LLMs to a baseline of responses from human comprehenders. Results are mixed. LLMs display considerable sensitivity to mental states and match human performance in several tasks. Yet, they commit systematic errors in others, especially those requiring pragmatic reasoning on the basis of mental state information. Such uneven performance indicates that human-level ToM may require resources beyond distributional information.},
  archive      = {J_TACL},
  author       = {Jones, Cameron R. and Trott, Sean and Bergen, Benjamin},
  doi          = {10.1162/tacl_a_00674},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {6},
  pages        = {803-819},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Comparing humans and large language models on an experimental protocol inventory for theory of mind evaluation (EPITOME)},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beyond boundaries: A human-like approach for question
answering over structured and unstructured information sources.
<em>TACL</em>, <em>12</em>, 786–802. (<a
href="https://doi.org/10.1162/tacl_a_00671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Answering factual questions from heterogenous sources, such as graphs and text, is a key capacity of intelligent systems. Current approaches either (i) perform question answering over text and structured sources as separate pipelines followed by a merge step or (ii) provide an early integration, giving up the strengths of particular information sources. To solve this problem, we present “HumanIQ”, a method that teaches language models to dynamically combine retrieved information by imitating how humans use retrieval tools. Our approach couples a generic method for gathering human demonstrations of tool use with adaptive few-shot learning for tool augmented models. We show that HumanIQ confers significant benefits, including i) reducing the error rate of our strongest baseline (GPT-4) by over 50% across 3 benchmarks, (ii) improving human preference over responses from vanilla GPT-4 (45.3% wins, 46.7% ties, 8.0% loss), and (iii) outperforming numerous task-specific baselines.},
  archive      = {J_TACL},
  author       = {Lehmann, Jens and Bhandiwad, Dhananjay and Gattogi, Preetam and Vahdati, Sahar},
  doi          = {10.1162/tacl_a_00671},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {6},
  pages        = {786-802},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Beyond boundaries: A human-like approach for question answering over structured and unstructured information sources},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Instructed to bias: Instruction-tuned language models
exhibit emergent cognitive bias. <em>TACL</em>, <em>12</em>, 771–785.
(<a href="https://doi.org/10.1162/tacl_a_00673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies show that instruction tuning (IT) and reinforcement learning from human feedback (RLHF) improve the abilities of large language models (LMs) dramatically. While these tuning methods can help align models with human objectives and generate high-quality text, not much is known about their potential adverse effects. In this work, we investigate the effect of IT and RLHF on decision making and reasoning in LMs, focusing on three cognitive biases—the decoy effect, the certainty effect, and the belief bias—all of which are known to influence human decision-making and reasoning. Our findings highlight the presence of these biases in various models from the GPT-3, Mistral, and T5 families. Notably, we find a stronger presence of biases in models that have undergone instruction tuning, such as Flan-T5, Mistral-Instruct, GPT3.5, and GPT4. Our work constitutes a step toward comprehending cognitive biases in instruction-tuned LMs, which is crucial for the development of more reliable and unbiased language models. 1},
  archive      = {J_TACL},
  author       = {Itzhak, Itay and Stanovsky, Gabriel and Rosenfeld, Nir and Belinkov, Yonatan},
  doi          = {10.1162/tacl_a_00673},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {6},
  pages        = {771-785},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Instructed to bias: Instruction-tuned language models exhibit emergent cognitive bias},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visually grounded speech models have a mutual exclusivity
bias. <em>TACL</em>, <em>12</em>, 755–770. (<a
href="https://doi.org/10.1162/tacl_a_00672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When children learn new words, they employ constraints such as the mutual exclusivity (ME) bias: A novel word is mapped to a novel object rather than a familiar one. This bias has been studied computationally, but only in models that use discrete word representations as input, ignoring the high variability of spoken words. We investigate the ME bias in the context of visually grounded speech models that learn from natural images and continuous speech audio. Concretely, we train a model on familiar words and test its ME bias by asking it to select between a novel and a familiar object when queried with a novel word. To simulate prior acoustic and visual knowledge, we experiment with several initialization strategies using pretrained speech and vision networks. Our findings reveal the ME bias across the different initialization approaches, with a stronger bias in models with more prior (in particular, visual) knowledge. Additional tests confirm the robustness of our results, even when different loss functions are considered. Based on detailed analyses to piece out the model’s representation space, we attribute the ME bias to how familiar and novel classes are distinctly separated in the resulting space.},
  archive      = {J_TACL},
  author       = {Nortje, Leanne and Oneaţă, Dan and Matusevych, Yevgen and Kamper, Herman},
  doi          = {10.1162/tacl_a_00672},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {6},
  pages        = {755-770},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Visually grounded speech models have a mutual exclusivity bias},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scope ambiguities in large language models. <em>TACL</em>,
<em>12</em>, 738–754. (<a
href="https://doi.org/10.1162/tacl_a_00670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentences containing multiple semantic operators with overlapping scope often create ambiguities in interpretation, known as scope ambiguities . These ambiguities offer rich insights into the interaction between semantic structure and world knowledge in language processing. Despite this, there has been little research into how modern large language models treat them. In this paper, we investigate how different versions of certain autoregressive language models—GPT-2, GPT-3/3.5, Llama 2, and GPT-4—treat scope ambiguous sentences, and compare this with human judgments. We introduce novel datasets that contain a joint total of almost 1,000 unique scope-ambiguous sentences, containing interactions between a range of semantic operators, and annotated for human judgments. Using these datasets, we find evidence that several models (i) are sensitive to the meaning ambiguity in these sentences, in a way that patterns well with human judgments, and (ii) can successfully identify human-preferred readings at a high level of accuracy (over 90% in some cases). 1},
  archive      = {J_TACL},
  author       = {Kamath, Gaurav and Schuster, Sebastian and Vajjala, Sowmya and Reddy, Siva},
  doi          = {10.1162/tacl_a_00670},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {6},
  pages        = {738-754},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Scope ambiguities in large language models},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Source-free domain adaptation for question answering with
masked self-training. <em>TACL</em>, <em>12</em>, 721–737. (<a
href="https://doi.org/10.1162/tacl_a_00669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous unsupervised domain adaptation (UDA) methods for question answering (QA) require access to source domain data while fine-tuning the model for the target domain. Source domain data may, however, contain sensitive information and should be protected. In this study, we investigate a more challenging setting, source-free UDA, in which we have only the pretrained source model and target domain data, without access to source domain data. We propose a novel self-training approach to QA models that integrates a specially designed mask module for domain adaptation. The mask is auto-adjusted to extract key domain knowledge when trained on the source domain. To maintain previously learned domain knowledge, certain mask weights are frozen during adaptation, while other weights are adjusted to mitigate domain shifts with pseudo-labeled samples generated in the target domain. Our empirical results on four benchmark datasets suggest that our approach significantly enhances the performance of pretrained QA models on the target domain, and even outperforms models that have access to the source data during adaptation.},
  archive      = {J_TACL},
  author       = {Yin, Maxwell J. and Wang, Boyu and Dong, Yue and Ling, Charles},
  doi          = {10.1162/tacl_a_00669},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {6},
  pages        = {721-737},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Source-free domain adaptation for question answering with masked self-training},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The ethics of automating legal actors. <em>TACL</em>,
<em>12</em>, 700–720. (<a
href="https://doi.org/10.1162/tacl_a_00668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The introduction of large public legal datasets has brought about a renaissance in legal NLP. Many of these datasets are composed of legal judgments—the product of judges deciding cases. Since ML algorithms learn to model the data they are trained on, several legal NLP models are models of judges. While some have argued for the automation of judges, in this position piece, we argue that automating the role of the judge raises difficult ethical challenges, in particular for common law legal systems. Our argument follows from the social role of the judge in actively shaping the law, rather than merely applying it. Since current NLP models are too far away from having the facilities necessary for this task, they should not be used to automate judges. Furthermore, even in the case that the models could achieve human-level capabilities, there would still be remaining ethical concerns inherent in the automation of the legal process.},
  archive      = {J_TACL},
  author       = {Valvoda, Josef and Thompson, Alec and Cotterell, Ryan and Teufel, Simone},
  doi          = {10.1162/tacl_a_00668},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {6},
  pages        = {700-720},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {The ethics of automating legal actors},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating correctness and faithfulness of
instruction-following models for question answering. <em>TACL</em>,
<em>12</em>, 775–793. (<a
href="https://doi.org/10.1162/tacl_a_00667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instruction-following models are attractive alternatives to fine-tuned approaches for question answering (QA). By simply prepending relevant documents and an instruction to their input, these models can be adapted to various information domains and tasks without additional training. However, these models tend to produce verbose responses with supplementary information, which makes traditional QA metrics like exact match (EM) and F1 unreliable for accurately quantifying model performance. In this work, we evaluate instruction-following models along two fronts: 1) how well they satisfy user’s information need (correctness), and 2) whether they disseminate information supported by the provided knowledge (faithfulness). Guided by human evaluation and analysis, we highlight the shortcomings of traditional metrics for both correctness and faithfulness and propose simple token-overlap metrics that correlate highly with human judgments. Our analysis reveals that for correctness, instruction-following models perform comparably to models specifically fine-tuned for that task. However, they struggle to accurately judge the relevance of the provided knowledge and often hallucinate in their responses. We hope our work encourages more holistic evaluation of instruction-following models for QA. Our code and human annotation data is available at https://github.com/McGill-NLP/instruct-qa .},
  archive      = {J_TACL},
  author       = {Adlakha, Vaibhav and BehnamGhader, Parishad and Lu, Xing Han and Meade, Nicholas and Reddy, Siva},
  doi          = {10.1162/tacl_a_00667},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {775-793},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Evaluating correctness and faithfulness of instruction-following models for question answering},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving probability-based prompt selection through unified
evaluation and analysis. <em>TACL</em>, <em>12</em>, 758–774. (<a
href="https://doi.org/10.1162/tacl_a_00666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous work in prompt engineering for large language models has introduced different gradient-free probability-based prompt selection methods that aim to choose the optimal prompt among the candidates for a given task but have failed to provide a comprehensive and fair comparison between each other. In this paper, we propose a unified framework to interpret and evaluate the existing probability-based prompt selection methods by performing extensive experiments on 13 common and diverse NLP tasks. We find that each of the existing methods can be interpreted as some variant of the method that maximizes mutual information between the input and the predicted output (MI). Utilizing this finding, we develop several other combinatorial variants of MI and increase the effectiveness of the oracle prompt selection method from 87.79% to 94.98%, measured as the ratio of the performance of the selected prompt to that of the optimal oracle prompt. Furthermore, considering that all the methods rely on the output probability distribution of the model that might be biased, we propose a novel calibration method called Calibration by Marginalization (CBM) that is orthogonal to the existing methods and helps increase the prompt selection effectiveness of the best method to 96.85%, achieving 99.44% of the oracle prompt F1 without calibration. 1},
  archive      = {J_TACL},
  author       = {Yang, Sohee and Kim, Jonghyeon and Jang, Joel and Ye, Seonghyeon and Lee, Hyunji and Seo, Minjoon},
  doi          = {10.1162/tacl_a_00666},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {758-774},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Improving probability-based prompt selection through unified evaluation and analysis},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computational complexity of natural morphology revisited.
<em>TACL</em>, <em>12</em>, 743–757. (<a
href="https://doi.org/10.1162/tacl_a_00665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper revisits a classical, yet fundamental, discussion of theoretical computational linguistics: the computational complexity of natural languages. Past studies have revealed that syntax, as observed in Swiss-German, is not weakly context-free. Concerning morphology, Culy ( 1985 ) employed a construction in Bambara to show that morphology is not weakly context-free; however, Manaster-Ramer ( 1988 ) pointed out that the Bambara case can be problematic because the wordhood of the construction is reliant on special tonal behaviors, and it is ambiguous whether the behaviors belong to the morphological domain. This raises doubts about whether the case can be considered a genuine morphological phenomenon. In this paper, we argue that Classical Ainu, a language we examine, also defies weak context-freeness at the morphological level. The construction we introduce is unambiguously morphological because this language’s valency-sensitive structure and valency-changing operations, such as noun incorporation, preclude its grammatical interpretation as syntactic.},
  archive      = {J_TACL},
  author       = {Senuma, Hajime and Aizawa, Akiko},
  doi          = {10.1162/tacl_a_00665},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {743-757},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Computational complexity of natural morphology revisited},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated learning for exploiting annotators’ disagreements
in natural language processing. <em>TACL</em>, <em>12</em>, 724–742. (<a
href="https://doi.org/10.1162/tacl_a_00664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The annotation of ambiguous or subjective NLP tasks is usually addressed by various annotators. In most datasets, these annotations are aggregated into a single ground truth. However, this omits divergent opinions of annotators, hence missing individual perspectives. We propose FLEAD (Federated Learning for Exploiting Annotators’ Disagreements), a methodology built upon federated learning to independently learn from the opinions of all the annotators, thereby leveraging all their underlying information without relying on a single ground truth. We conduct an extensive experimental study and analysis in diverse text classification tasks to show the contribution of our approach with respect to mainstream approaches based on majority voting and other recent methodologies that also learn from annotator disagreements.},
  archive      = {J_TACL},
  author       = {Rodríguez-Barroso, Nuria and Cámara, Eugenio Martínez and Collados, Jose Camacho and Luzón, M. Victoria and Herrera, Francisco},
  doi          = {10.1162/tacl_a_00664},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {724-742},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Federated learning for exploiting annotators’ disagreements in natural language processing},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The thai discourse treebank: Annotating and classifying thai
discourse connectives. <em>TACL</em>, <em>12</em>, 613–629. (<a
href="https://doi.org/10.1162/tacl_a_00650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discourse analysis is a highly applicable area of natural language processing. In English and other languages, resources for discourse-based tasks are widely available. Thai, however, has hitherto lacked such resources. We present the Thai Discourse Treebank, the first, large Thai corpus annotated in the style of the Penn Discourse Treebank. The resulting corpus has over 10,000 sentences and 18,000 instances of connectives in 33 different relations. We release the corpus alongside our list of 148 potentially polysemous discourse connectives with a total of 340 form-sense pairs and their classification criteria to facilitate future research. We also develop models for connective identification and classification tasks. Our best models achieve an F 1 of 0.96 in the identification task and 0.46 on the sense classification task. Our results serve as benchmarks for future models for Thai discourse tasks.},
  archive      = {J_TACL},
  author       = {Prasertsom, Ponrawee and Jaroonpol, Apiwat and Rutherford, Attapol T.},
  doi          = {10.1162/tacl_a_00650},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {613-629},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {The thai discourse treebank: Annotating and classifying thai discourse connectives},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantics of multiword expressions in transformer-based
models: A survey. <em>TACL</em>, <em>12</em>, 593–612. (<a
href="https://doi.org/10.1162/tacl_a_00657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiword expressions (MWEs) are composed of multiple words and exhibit variable degrees of compositionality. As such, their meanings are notoriously difficult to model, and it is unclear to what extent this issue affects transformer architectures. Addressing this gap, we provide the first in-depth survey of MWE processing with transformer models. We overall find that they capture MWE semantics inconsistently, as shown by reliance on surface patterns and memorized information. MWE meaning is also strongly localized, predominantly in early layers of the architecture. Representations benefit from specific linguistic properties, such as lower semantic idiosyncrasy and ambiguity of target expressions. Our findings overall question the ability of transformer models to robustly capture fine-grained semantics. Furthermore, we highlight the need for more directly comparable evaluation setups.},
  archive      = {J_TACL},
  author       = {Miletić, Filip and Walde, Sabine Schulte im},
  doi          = {10.1162/tacl_a_00657},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {593-612},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Semantics of multiword expressions in transformer-based models: A survey},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Eliciting the translation ability of large language models
via multilingual finetuning with translation instructions.
<em>TACL</em>, <em>12</em>, 576–592. (<a
href="https://doi.org/10.1162/tacl_a_00655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale pretrained language models (LLMs), such as ChatGPT and GPT4, have shown strong abilities in multilingual translation, without being explicitly trained on parallel corpora. It is intriguing how the LLMs obtain their ability to carry out translation instructions for different languages. In this paper, we present a detailed analysis by finetuning a multilingual pretrained language model, XGLM-7.5B, to perform multilingual translation following given instructions. Firstly, we show that multilingual LLMs have stronger translation abilities than previously demonstrated. For a certain language, the translation performance depends on its similarity to English and the amount of data used in the pretraining phase. Secondly, we find that LLMs’ ability to carry out translation instructions relies on the understanding of translation instructions and the alignment among different languages. With multilingual finetuning with translation instructions, LLMs could learn to perform the translation task well even for those language pairs unseen during the instruction tuning phase.},
  archive      = {J_TACL},
  author       = {Li, Jiahuan and Zhou, Hao and Huang, Shujian and Cheng, Shanbo and Chen, Jiajun},
  doi          = {10.1162/tacl_a_00655},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {576-592},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Eliciting the translation ability of large language models via multilingual finetuning with translation instructions},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Text-to-OverpassQL: A natural language interface for complex
geodata querying of OpenStreetMap. <em>TACL</em>, <em>12</em>, 562–575.
(<a href="https://doi.org/10.1162/tacl_a_00654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Text-to-OverpassQL, a task designed to facilitate a natural language interface for querying geodata from OpenStreetMap (OSM). The Overpass Query Language (OverpassQL) allows users to formulate complex database queries and is widely adopted in the OSM ecosystem. Generating Overpass queries from natural language input serves multiple use-cases. It enables novice users to utilize OverpassQL without prior knowledge, assists experienced users with crafting advanced queries, and enables tool-augmented large language models to access information stored in the OSM database. In order to assess the performance of current sequence generation models on this task, we propose OverpassNL, 1 a dataset of 8,352 queries with corresponding natural language inputs. We further introduce task specific evaluation metrics and ground the evaluation of the Text-to-OverpassQL task by executing the queries against the OSM database. We establish strong baselines by finetuning sequence-to-sequence models and adapting large language models with in-context examples. The detailed evaluation reveals strengths and weaknesses of the considered learning strategies, laying the foundations for further research into the Text-to-OverpassQL task.},
  archive      = {J_TACL},
  author       = {Staniek, Michael and Schumann, Raphael and Züfle, Maike and Riezler, Stefan},
  doi          = {10.1162/tacl_a_00654},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {562-575},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Text-to-OverpassQL: A natural language interface for complex geodata querying of OpenStreetMap},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). What formal languages can transformers express? A survey.
<em>TACL</em>, <em>12</em>, 543–561. (<a
href="https://doi.org/10.1162/tacl_a_00663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As transformers have gained prominence in natural language processing, some researchers have investigated theoretically what problems they can and cannot solve, by treating problems as formal languages . Exploring such questions can help clarify the power of transformers relative to other models of computation, their fundamental capabilities and limits, and the impact of architectural choices. Work in this subarea has made considerable progress in recent years. Here, we undertake a comprehensive survey of this work, documenting the diverse assumptions that underlie different results and providing a unified framework for harmonizing seemingly contradictory findings.},
  archive      = {J_TACL},
  author       = {Strobl, Lena and Merrill, William and Weiss, Gail and Chiang, David and Angluin, Dana},
  doi          = {10.1162/tacl_a_00663},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {543-561},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {What formal languages can transformers express? a survey},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AutoPEFT: Automatic configuration search for
parameter-efficient fine-tuning. <em>TACL</em>, <em>12</em>, 525–542.
(<a href="https://doi.org/10.1162/tacl_a_00662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large pretrained language models are widely used in downstream NLP tasks via task- specific fine-tuning, but such procedures can be costly. Recently, Parameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task performance while updating much fewer parameters than full model fine-tuning (FFT). However, it is non-trivial to make informed design choices on the PEFT configurations , such as their architecture, the number of tunable parameters, and even the layers in which the PEFT modules are inserted. Consequently, it is highly likely that the current, manually designed configurations are suboptimal in terms of their performance-efficiency trade-off. Inspired by advances in neural architecture search, we propose AutoPEFT for automatic PEFT configuration selection: We first design an expressive configuration search space with multiple representative PEFT modules as building blocks. Using multi-objective Bayesian optimization in a low-cost setup, we then discover a Pareto-optimal set of configurations with strong performance-cost trade-offs across different numbers of parameters that are also highly transferable across different tasks. Empirically, on GLUE and SuperGLUE tasks, we show that AutoPEFT -discovered configurations significantly outperform existing PEFT methods and are on par or better than FFT without incurring substantial training efficiency costs.},
  archive      = {J_TACL},
  author       = {Zhou, Han and Wan, Xingchen and Vulić, Ivan and Korhonen, Anna},
  doi          = {10.1162/tacl_a_00662},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {525-542},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {AutoPEFT: Automatic configuration search for parameter-efficient fine-tuning},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). KoBBQ: Korean bias benchmark for question answering.
<em>TACL</em>, <em>12</em>, 507–524. (<a
href="https://doi.org/10.1162/tacl_a_00661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Warning: This paper contains examples of stereotypes and biases. The Bias Benchmark for Question Answering (BBQ) is designed to evaluate social biases of language models (LMs), but it is not simple to adapt this benchmark to cultural contexts other than the US because social biases depend heavily on the cultural context. In this paper, we present KoBBQ, a Korean bias benchmark dataset, and we propose a general framework that addresses considerations for cultural adaptation of a dataset. Our framework includes partitioning the BBQ dataset into three classes—Simply-Transferred (can be used directly after cultural translation), Target-Modified (requires localization in target groups), and Sample-Removed (does not fit Korean culture)—and adding four new categories of bias specific to Korean culture. We conduct a large-scale survey to collect and validate the social biases and the targets of the biases that reflect the stereotypes in Korean culture. The resulting KoBBQ dataset comprises 268 templates and 76,048 samples across 12 categories of social bias. We use KoBBQ to measure the accuracy and bias scores of several state-of-the-art multilingual LMs. The results clearly show differences in the bias of LMs as measured by KoBBQ and a machine-translated version of BBQ, demonstrating the need for and utility of a well-constructed, culturally aware social bias benchmark.},
  archive      = {J_TACL},
  author       = {Jin, Jiho and Kim, Jiseon and Lee, Nayeon and Yoo, Haneul and Oh, Alice and Lee, Hwaran},
  doi          = {10.1162/tacl_a_00661},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {507-524},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {KoBBQ: Korean bias benchmark for question answering},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatically correcting large language models: Surveying
the landscape of diverse automated correction strategies. <em>TACL</em>,
<em>12</em>, 484–506. (<a
href="https://doi.org/10.1162/tacl_a_00660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While large language models (LLMs) have shown remarkable effectiveness in various NLP tasks, they are still prone to issues such as hallucination, unfaithful reasoning, and toxicity. A promising approach to rectify these flaws is correcting LLMs with feedback , where the LLM itself is prompted or guided with feedback to fix problems in its own output. Techniques leveraging automated feedback —either produced by the LLM itself (self-correction) or some external system—are of particular interest as they make LLM-based solutions more practical and deployable with minimal human intervention. This paper provides an exhaustive review of the recent advances in correcting LLMs with automated feedback, categorizing them into training-time, generation-time, and post-hoc approaches. We also identify potential challenges and future directions in this emerging field.},
  archive      = {J_TACL},
  author       = {Pan, Liangming and Saxon, Michael and Xu, Wenda and Nathani, Deepak and Wang, Xinyi and Wang, William Yang},
  doi          = {10.1162/tacl_a_00660},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {484-506},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Automatically correcting large language models: Surveying the landscape of diverse automated correction strategies},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ConvoSense: Overcoming monotonous commonsense inferences for
conversational AI. <em>TACL</em>, <em>12</em>, 467–483. (<a
href="https://doi.org/10.1162/tacl_a_00659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mastering commonsense understanding and reasoning is a pivotal skill essential for conducting engaging conversations. While there have been several attempts to create datasets that facilitate commonsense inferences in dialogue contexts, existing datasets tend to lack in-depth details, restate information already present in the conversation, and often fail to capture the multifaceted nature of commonsense reasoning. In response to these limitations, we compile a new synthetic dataset for commonsense reasoning in dialogue contexts using GPT, ℂonvo S ense, that boasts greater contextual novelty, offers a higher volume of inferences per example, and substantially enriches the detail conveyed by the inferences. Our dataset contains over 500,000 inferences across 12,000 dialogues with 10 popular inference types, which empowers the training of generative commonsense models for dialogue that are superior in producing plausible inferences with high novelty when compared to models trained on the previous datasets. To the best of our knowledge, ℂonvo S ense is the first of its kind to provide such a multitude of novel inferences at such a large scale.},
  archive      = {J_TACL},
  author       = {Finch, Sarah E. and Choi, Jinho D.},
  doi          = {10.1162/tacl_a_00659},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {467-483},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {ConvoSense: Overcoming monotonous commonsense inferences for conversational AI},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simultaneous selection and adaptation of source data via
four-level optimization. <em>TACL</em>, <em>12</em>, 449–466. (<a
href="https://doi.org/10.1162/tacl_a_00658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TACL},
  author       = {Xie, Pengtao and Zhao, Xingchen and He, Xuehai},
  doi          = {10.1162/tacl_a_00658},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {449-466},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Simultaneous selection and adaptation of source data via four-level optimization},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Do text simplification systems preserve meaning? A human
evaluation via reading comprehension. <em>TACL</em>, <em>12</em>,
432–448. (<a href="https://doi.org/10.1162/tacl_a_00653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic text simplification (TS) aims to automate the process of rewriting text to make it easier for people to read. A pre-requisite for TS to be useful is that it should convey information that is consistent with the meaning of the original text. However, current TS evaluation protocols assess system outputs for simplicity and meaning preservation without regard for the document context in which output sentences occur and for how people understand them. In this work, we introduce a human evaluation framework to assess whether simplified texts preserve meaning using reading comprehension questions. With this framework, we conduct a thorough human evaluation of texts by humans and by nine automatic systems. Supervised systems that leverage pre-training knowledge achieve the highest scores on the reading comprehension tasks among the automatic controllable TS systems. However, even the best-performing supervised system struggles with at least 14% of the questions, marking them as “unanswerable” based on simplified content. We further investigate how existing TS evaluation metrics and automatic question-answering systems approximate the human judgments we obtained.},
  archive      = {J_TACL},
  author       = {Agrawal, Sweta and Carpuat, Marine},
  doi          = {10.1162/tacl_a_00653},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {432-448},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Do text simplification systems preserve meaning? a human evaluation via reading comprehension},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Geographic adaptation of pretrained language models.
<em>TACL</em>, <em>12</em>, 411–431. (<a
href="https://doi.org/10.1162/tacl_a_00652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While pretrained language models (PLMs) have been shown to possess a plethora of linguistic knowledge, the existing body of research has largely neglected extralinguistic knowledge, which is generally difficult to obtain by pretraining on text alone. Here, we contribute to closing this gap by examining geolinguistic knowledge, i.e., knowledge about geographic variation in language. We introduce geoadaptation , an intermediate training step that couples language modeling with geolocation prediction in a multi-task learning setup. We geoadapt four PLMs, covering language groups from three geographic areas, and evaluate them on five different tasks: fine-tuned (i.e., supervised) geolocation prediction, zero-shot (i.e., unsupervised) geolocation prediction, fine-tuned language identification, zero-shot language identification, and zero-shot prediction of dialect features. Geoadaptation is very successful at injecting geolinguistic knowledge into the PLMs: The geoadapted PLMs consistently outperform PLMs adapted using only language modeling (by especially wide margins on zero-shot prediction tasks), and we obtain new state-of-the-art results on two benchmarks for geolocation prediction and language identification. Furthermore, we show that the effectiveness of geoadaptation stems from its ability to geographically retrofit the representation space of the PLMs.},
  archive      = {J_TACL},
  author       = {Hofmann, Valentin and Glavaš, Goran and Ljubešić, Nikola and Pierrehumbert, Janet B. and Schütze, Hinrich},
  doi          = {10.1162/tacl_a_00652},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {411-431},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Geographic adaptation of pretrained language models},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Are character-level translations worth the wait? Comparing
ByT5 and mT5 for machine translation. <em>TACL</em>, <em>12</em>,
392–410. (<a href="https://doi.org/10.1162/tacl_a_00651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pretrained character-level and byte-level language models have been shown to be competitive with popular subword models across a range of Natural Language Processing tasks. However, there has been little research on their effectiveness for neural machine translation (NMT), particularly within the popular pretrain-then-finetune paradigm. This work performs an extensive comparison across multiple languages and experimental conditions of character- and subword-level pretrained models (ByT5 and mT5, respectively) on NMT. We show the effectiveness of character-level modeling in translation, particularly in cases where fine-tuning data is limited. In our analysis, we show how character models’ gains in translation quality are reflected in better translations of orthographically similar words and rare words. While evaluating the importance of source texts in driving model predictions, we highlight word-level patterns within ByT5, suggesting an ability to modulate word-level and character-level information during generation. We conclude by assessing the efficiency tradeoff of byte models, suggesting their usage in non-time-critical scenarios to boost translation quality.},
  archive      = {J_TACL},
  author       = {Edman, Lukas and Sarti, Gabriele and Toral, Antonio and Noord, Gertjan van and Bisazza, Arianna},
  doi          = {10.1162/tacl_a_00651},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {392-410},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Are character-level translations worth the wait? comparing ByT5 and mT5 for machine translation},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). What do self-supervised speech models know about words?
<em>TACL</em>, <em>12</em>, 372–391. (<a
href="https://doi.org/10.1162/tacl_a_00656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many self-supervised speech models (S3Ms) have been introduced over the last few years, improving performance and data efficiency on various speech tasks. However, these empirical successes alone do not give a complete picture of what is learned during pre-training. Recent work has begun analyzing how S3Ms encode certain properties, such as phonetic and speaker information, but we still lack a proper understanding of knowledge encoded at the word level and beyond. In this work, we use lightweight analysis methods to study segment-level linguistic properties—word identity, boundaries, pronunciation, syntactic features, and semantic features—encoded in S3Ms. We present a comparative study of layer-wise representations from ten S3Ms and find that (i) the frame-level representations within each word segment are not all equally informative, and (ii) the pre-training objective and model size heavily influence the accessibility and distribution of linguistic information across layers. We also find that on several tasks—word discrimination, word segmentation, and semantic sentence similarity—S3Ms trained with visual grounding outperform their speech-only counterparts. Finally, our task-based analyses demonstrate improved performance on word segmentation and acoustic word discrimination while using simpler methods than prior work. 1},
  archive      = {J_TACL},
  author       = {Pasad, Ankita and Chien, Chung-Ming and Settle, Shane and Livescu, Karen},
  doi          = {10.1162/tacl_a_00656},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {372-391},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {What do self-supervised speech models know about words?},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). To diverge or not to diverge: A morphosyntactic perspective
on machine translation vs human translation. <em>TACL</em>, <em>12</em>,
355–371. (<a href="https://doi.org/10.1162/tacl_a_00645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We conduct a large-scale fine-grained comparative analysis of machine translations (MTs) against human translations (HTs) through the lens of morphosyntactic divergence. Across three language pairs and two types of divergence defined as the structural difference between the source and the target, MT is consistently more conservative than HT, with less morphosyntactic diversity, more convergent patterns, and more one-to-one alignments. Through analysis on different decoding algorithms, we attribute this discrepancy to the use of beam search that biases MT towards more convergent patterns. This bias is most amplified when the convergent pattern appears around 50% of the time in training data. Lastly, we show that for a majority of morphosyntactic divergences, their presence in HT is correlated with decreased MT performance, presenting a greater challenge for MT systems.},
  archive      = {J_TACL},
  author       = {Luo, Jiaming and Cherry, Colin and Foster, George},
  doi          = {10.1162/tacl_a_00645},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {355-371},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {To diverge or not to diverge: A morphosyntactic perspective on machine translation vs human translation},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). JustiLM: Few-shot justification generation for explainable
fact-checking of real-world claims. <em>TACL</em>, <em>12</em>, 334–354.
(<a href="https://doi.org/10.1162/tacl_a_00649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Justification is an explanation that supports the veracity assigned to a claim in fact-checking. However, the task of justification generation has been previously oversimplified as summarization of a fact-check article authored by fact-checkers. Therefore, we propose a realistic approach to generate justification based on retrieved evidence. We present a new benchmark dataset called ExClaim (for Ex plainable fact-checking of real-world Claim s), and introduce JustiLM, a novel few-shot Justi fication generation based on retrieval-augmented L anguage M odel by using fact-check articles as an auxiliary resource during training only. Experiments show that JustiLM achieves promising performance in justification generation compared to strong baselines, and can also enhance veracity classification with a straightforward extension. 1 Code and dataset are released at https://github.com/znhy1024/JustiLM .},
  archive      = {J_TACL},
  author       = {Zeng, Fengzhu and Gao, Wei},
  doi          = {10.1162/tacl_a_00649},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {334-354},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {JustiLM: Few-shot justification generation for explainable fact-checking of real-world claims},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large language models enable few-shot clustering.
<em>TACL</em>, <em>12</em>, 321–333. (<a
href="https://doi.org/10.1162/tacl_a_00648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike traditional unsupervised clustering, semi-supervised clustering allows users to provide meaningful structure to the data, which helps the clustering algorithm to match the user’s intent. Existing approaches to semi-supervised clustering require a significant amount of feedback from an expert to improve the clusters. In this paper, we ask whether a large language model (LLM) can amplify an expert’s guidance to enable query-efficient, few-shot semi-supervised text clustering. We show that LLMs are surprisingly effective at improving clustering. We explore three stages where LLMs can be incorporated into clustering: before clustering (improving input features), during clustering (by providing constraints to the clusterer), and after clustering (using LLMs post-correction). We find that incorporating LLMs in the first two stages routinely provides significant improvements in cluster quality, and that LLMs enable a user to make trade-offs between cost and accuracy to produce desired clusters. We release our code and LLM prompts for the public to use. 1},
  archive      = {J_TACL},
  author       = {Viswanathan, Vijay and Gashteovski, Kiril and Lawrence, Carolin and Wu, Tongshuang and Neubig, Graham},
  doi          = {10.1162/tacl_a_00648},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {321-333},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Large language models enable few-shot clustering},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The impact of word splitting on the semantic content of
contextualized word representations. <em>TACL</em>, <em>12</em>,
299–320. (<a href="https://doi.org/10.1162/tacl_a_00647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When deriving contextualized word representations from language models, a decision needs to be made on how to obtain one for out-of-vocabulary (OOV) words that are segmented into subwords. What is the best way to represent these words with a single vector, and are these representations of worse quality than those of in-vocabulary words? We carry out an intrinsic evaluation of embeddings from different models on semantic similarity tasks involving OOV words. Our analysis reveals, among other interesting findings, that the quality of representations of words that are split is often, but not always, worse than that of the embeddings of known words. Their similarity values, however, must be interpreted with caution.},
  archive      = {J_TACL},
  author       = {Soler, Aina Garí and Labeau, Matthieu and Clavel, Chloé},
  doi          = {10.1162/tacl_a_00647},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {299-320},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {The impact of word splitting on the semantic content of contextualized word representations},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating the ripple effects of knowledge editing in
language models. <em>TACL</em>, <em>12</em>, 283–298. (<a
href="https://doi.org/10.1162/tacl_a_00644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern language models capture a large body of factual knowledge. However, some facts can be incorrectly induced or become obsolete over time, resulting in factually incorrect generations. This has led to the development of various editing methods that allow updating facts encoded by the model. Evaluation of these methods has primarily focused on testing whether an individual fact has been successfully injected, and if similar predictions for other subjects have not changed. Here we argue that such evaluation is limited, since injecting one fact (e.g., “Jack Depp is the son of Johnny Depp” ) introduces a “ripple effect” in the form of additional facts that the model needs to update (e.g., “Jack Depp is the sibling of Lily-Rose Depp” ). To address this, we propose novel evaluation criteria that consider the implications of an edit on related facts. Using these criteria, we then construct RippleEdits , a diagnostic benchmark of 5K factual edits, capturing various types of ripple effects. We evaluate prominent editing methods on RippleEdits , showing that they fail to introduce consistent changes in the model’s knowledge. In addition, we find that a simple in-context editing baseline obtains the best scores on our benchmark, suggesting a promising research direction for model editing. 1},
  archive      = {J_TACL},
  author       = {Cohen, Roi and Biran, Eden and Yoran, Ori and Globerson, Amir and Geva, Mor},
  doi          = {10.1162/tacl_a_00644},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {283-298},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Evaluating the ripple effects of knowledge editing in language models},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explicitly representing syntax improves sentence-to-layout
prediction of unexpected situations. <em>TACL</em>, <em>12</em>,
264–282. (<a href="https://doi.org/10.1162/tacl_a_00643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing visual entities in a natural language sentence and arranging them in a 2D spatial layout require a compositional understanding of language and space. This task of layout prediction is valuable in text-to-image synthesis as it allows localized and controlled in-painting of the image. In this comparative study it is shown that we can predict layouts from language representations that implicitly or explicitly encode sentence syntax, if the sentences mention similar entity-relationships to the ones seen during training. To test compositional understanding, we collect a test set of grammatically correct sentences and layouts describing compositions of entities and relations that unlikely have been seen during training. Performance on this test set substantially drops, showing that current models rely on correlations in the training data and have difficulties in understanding the structure of the input sentences. We propose a novel structural loss function that better enforces the syntactic structure of the input sentence and show large performance gains in the task of 2D spatial layout prediction conditioned on text. The loss has the potential to be used in other generation tasks where a tree-like structure underlies the conditioning modality. Code, trained models, and the USCOCO evaluation set are available via Github. 1},
  archive      = {J_TACL},
  author       = {Nuyts, Wolf and Cartuyvels, Ruben and Moens, Marie-Francine},
  doi          = {10.1162/tacl_a_00643},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {264-282},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Explicitly representing syntax improves sentence-to-layout prediction of unexpected situations},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Retrieve what you need: A mutual learning framework for
open-domain question answering. <em>TACL</em>, <em>12</em>, 247–263. (<a
href="https://doi.org/10.1162/tacl_a_00646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An open-domain question answering (QA) system usually follows a retrieve-then-read paradigm, in which a retriever is used to retrieve relevant passages from a large corpus, and then a reader generates answers based on the retrieved passages and the original question. In this paper, we propose a simple and novel mutual learning framework to improve the performance of retrieve-then-read -style models via an intermediate module named the knowledge selector , which we train with reinforcement learning. The key benefits of our proposed intermediate module are: 1) no requirement for additional annotated question-passage pairs; 2) improvements in both retrieval and QA performance, as well as computational efficiency, compared to prior competitive retrieve-then-read models; 3) with no finetuning, improvement in the zero-shot performance of large-scale pre-trained language models, e.g., ChatGPT, by encapsulating the input with relevant knowledge without violating the input length constraint.},
  archive      = {J_TACL},
  author       = {Wang, Dingmin and Huang, Qiuyuan and Jackson, Matthew and Gao, Jianfeng},
  doi          = {10.1162/tacl_a_00646},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {247-263},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Retrieve what you need: A mutual learning framework for open-domain question answering},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring human-like translation strategy with large
language models. <em>TACL</em>, <em>12</em>, 229–246. (<a
href="https://doi.org/10.1162/tacl_a_00642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have demonstrated impressive capabilities in general scenarios, exhibiting a level of aptitude that approaches, in some aspects even surpasses, human-level intelligence. Among their numerous skills, the translation abilities of LLMs have received considerable attention. Compared to typical machine translation that focuses solely on source-to-target mapping, LLM-based translation can potentially mimic the human translation process, which might take preparatory steps to ensure high-quality translation. This work explores this possibility by proposing the MAPS framework, which stands for M ulti- A spect P rompting and S election. Specifically, we enable LLMs first to analyze the given source sentence and induce three aspects of translation-related knowledge (keywords, topics, and relevant demonstrations) to guide the final translation process. Moreover, we employ a selection mechanism based on quality estimation to filter out noisy and unhelpful knowledge. Both automatic (3 LLMs × 11 directions × 2 automatic metrics) and human evaluation (preference study and MQM) demonstrate the effectiveness of MAPS. Further analysis shows that by mimicking the human translation process, MAPS reduces various translation errors such as hallucination, ambiguity, mistranslation, awkward style, untranslated text, and omission. Source code is available at https://github.com/zwhe99/MAPS-mt .},
  archive      = {J_TACL},
  author       = {He, Zhiwei and Liang, Tian and Jiao, Wenxiang and Zhang, Zhuosheng and Yang, Yujiu and Wang, Rui and Tu, Zhaopeng and Shi, Shuming and Wang, Xing},
  doi          = {10.1162/tacl_a_00642},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {229-246},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Exploring human-like translation strategy with large language models},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unifying structured data as graph for data-to-text
pre-training. <em>TACL</em>, <em>12</em>, 210–228. (<a
href="https://doi.org/10.1162/tacl_a_00641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-to-text (D2T) generation aims to transform structured data into natural language text. Data-to-text pre-training has proved to be powerful in enhancing D2T generation and yields impressive performance. However, previous pre-training methods either oversimplified structured data into a sequence without considering input structures or designed training objectives tailored for a specific data structure (e.g., table or knowledge graph). In this paper, we unify different types of structured data (i.e., table, key-value data, knowledge graph) into the graph format and cast different D2T generation tasks as graph-to-text generation. To effectively exploit the structural information of the input graph, we propose a structure-enhanced pre-training method for D2T generation by designing a structure-enhanced Transformer. Concretely, we devise a position matrix for the Transformer, encoding relative positional information of connected nodes in the input graph. In addition, we propose a new attention matrix to incorporate graph structures into the original Transformer by taking the available explicit connectivity structure into account. Extensive experiments on six benchmark datasets show the effectiveness of our model. Our source codes are available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/unid2t .},
  archive      = {J_TACL},
  author       = {Li, Shujie and Li, Liang and Geng, Ruiying and Yang, Min and Li, Binhua and Yuan, Guanghu and He, Wanwei and Yuan, Shao and Ma, Can and Huang, Fei and Li, Yongbin},
  doi          = {10.1162/tacl_a_00641},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {210-228},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Unifying structured data as graph for data-to-text pre-training},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Text attribute control via closed-loop disentanglement.
<em>TACL</em>, <em>12</em>, 190–209. (<a
href="https://doi.org/10.1162/tacl_a_00640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Changing an attribute of a text without changing the content usually requires first disentangling the text into irrelevant attributes and content representations. After that, in the inference phase, the representation of one attribute is tuned to a different value, expecting that the corresponding attribute of the text can also be changed accordingly. The usual way of disentanglement is to add some constraints on the latent space of an encoder-decoder architecture, including adversarial-based constraints and mutual-information-based constraints. However, previous semi-supervised processes of attribute change are usually not enough to guarantee the success of attribute change and content preservation. In this paper, we propose a novel approach to achieve a robust control of attributes while enhancing content preservation. In this approach, we use a semi-supervised contrastive learning method to encourage the disentanglement of attributes in latent spaces. Differently from previous works, we re-disentangle the reconstructed sentence and compare the re-disentangled latent space with the original latent space, which makes a closed-loop disentanglement process. This also helps content preservation. In addition, the contrastive learning method is also able to replace the role of minimizing mutual information and adversarial training in the disentanglement process, which alleviates the computation cost. We conducted experiments on three text datasets, including the Yelp Service review dataset, the Amazon Product review dataset, and the GoEmotions dataset. The experimental results show the effectiveness of our model.},
  archive      = {J_TACL},
  author       = {Sha, Lei and Lukasiewicz, Thomas},
  doi          = {10.1162/tacl_a_00640},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {190-209},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Text attribute control via closed-loop disentanglement},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Red teaming language model detectors with language models.
<em>TACL</em>, <em>12</em>, 174–189. (<a
href="https://doi.org/10.1162/tacl_a_00639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users. To prevent the potentially deceptive usage of LLMs, recent work has proposed algorithms to detect LLM-generated text and protect LLMs. In this paper, we investigate the robustness and reliability of these LLM detectors under adversarial attacks. We study two types of attack strategies: 1) replacing certain words in an LLM’s output with their synonyms given the context; 2) automatically searching for an instructional prompt to alter the writing style of the generation. In both strategies, we leverage an auxiliary LLM to generate the word replacements or the instructional prompt. Different from previous works, we consider a challenging setting where the auxiliary LLM can also be protected by a detector. Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems. Code is available at https://github.com/shizhouxing/LLM-Detector-Robustness .},
  archive      = {J_TACL},
  author       = {Shi, Zhouxing and Wang, Yihan and Yin, Fan and Chen, Xiangning and Chang, Kai-Wei and Hsieh, Cho-Jui},
  doi          = {10.1162/tacl_a_00639},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {174-189},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Red teaming language model detectors with language models},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lost in the middle: How language models use long contexts.
<em>TACL</em>, <em>12</em>, 157–173. (<a
href="https://doi.org/10.1162/tacl_a_00638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.},
  archive      = {J_TACL},
  author       = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  doi          = {10.1162/tacl_a_00638},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {157-173},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Lost in the middle: How language models use long contexts},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An energy-based model for word-level AutoCompletion in
computer-aided translation. <em>TACL</em>, <em>12</em>, 137–156. (<a
href="https://doi.org/10.1162/tacl_a_00637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Word-level AutoCompletion (WLAC) is a rewarding yet challenging task in Computer-aided Translation. Existing work addresses this task through a classification model based on a neural network that maps the hidden vector of the input context into its corresponding label (i.e., the candidate target word is treated as a label). Since the context hidden vector itself does not take the label into account and it is projected to the label through a linear classifier, the model cannot sufficiently leverage valuable information from the source sentence as verified in our experiments, which eventually hinders its overall performance. To alleviate this issue, this work proposes an energy-based model for WLAC, which enables the context hidden vector to capture crucial information from the source sentence. Unfortunately, training and inference suffer from efficiency and effectiveness challenges, therefore we employ three simple yet effective strategies to put our model into practice. Experiments on four standard benchmarks demonstrate that our reranking-based approach achieves substantial improvements (about 6.07%) over the previous state-of-the-art model. Further analyses show that each strategy of our approach contributes to the final performance. 1},
  archive      = {J_TACL},
  author       = {Yang, Cheng and Huang, Guoping and Yu, Mo and Zhang, Zhirui and Li, Siheng and Yang, Mingming and Shi, Shuming and Yang, Yujiu and Liu, Lemao},
  doi          = {10.1162/tacl_a_00637},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {137-156},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {An energy-based model for word-level AutoCompletion in computer-aided translation},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Addressing the binning problem in calibration assessment
through scalar annotations. <em>TACL</em>, <em>12</em>, 120–136. (<a
href="https://doi.org/10.1162/tacl_a_00636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational linguistics models commonly target the prediction of discrete— categorical —labels. When assessing how well-calibrated these model predictions are, popular evaluation schemes require practitioners to manually determine a binning scheme: grouping labels into bins to approximate true label posterior. The problem is that these metrics are sensitive to binning decisions. We consider two solutions to the binning problem that apply at the stage of data annotation: collecting either distributed (redundant) labels or direct scalar value assignment. In this paper, we show that although both approaches address the binning problem by evaluating instance-level calibration, direct scalar assignment is significantly more cost-effective. We provide theoretical analysis and empirical evidence to support our proposal for dataset creators to adopt scalar annotation protocols to enable a higher-quality assessment of model calibration.},
  archive      = {J_TACL},
  author       = {Jiang, Zhengping and Liu, Anqi and Durme, Benjamnin Van},
  doi          = {10.1162/tacl_a_00636},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {120-136},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Addressing the binning problem in calibration assessment through scalar annotations},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Metric-free learning network with dual relations propagation
for few-shot aspect category sentiment analysis. <em>TACL</em>,
<em>12</em>, 100–119. (<a
href="https://doi.org/10.1162/tacl_a_00635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot Aspect Category Sentiment Analysis (ACSA) is a crucial task for aspect-based sentiment analysis, which aims to detect sentiment polarity for a given aspect category in a sentence with limited data. However, few-shot learning methods focus on distance metrics between the query and support sets to classify queries, heavily relying on aspect distributions in the embedding space. Thus, they suffer from overlapping distributions of aspect embeddings caused by irrelevant sentiment noise among sentences with multiple sentiment aspects, leading to misclassifications. To solve the above issues, we propose a metric-free method for few-shot ACSA, which models the associated relations among the aspects of support and query sentences by Dual Relations Propagation (DRP), addressing the passive effect of overlapping distributions. Specifically, DRP uses the dual relations (similarity and diversity) among the aspects of support and query sentences to explore intra-cluster commonality and inter-cluster uniqueness for alleviating sentiment noise and enhancing aspect features. Additionally, the dual relations are transformed from support-query to class-query to promote query inference by learning class knowledge. Experiments show that we achieve convincing performance on few-shot ACSA, especially an average improvement of 2.93% accuracy and 2.10% F1 score in the 3-way 1-shot setting.},
  archive      = {J_TACL},
  author       = {Zhao, Shiman and Xie, Yutao and Chen, Wei and Wang, Tengjiao and Yao, Jiahui and Zheng, Jiabin},
  doi          = {10.1162/tacl_a_00635},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {100-119},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Metric-free learning network with dual relations propagation for few-shot aspect category sentiment analysis},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cultural adaptation of recipes. <em>TACL</em>, <em>12</em>,
80–99. (<a href="https://doi.org/10.1162/tacl_a_00634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building upon the considerable advances in Large Language Models (LLMs), we are now equipped to address more sophisticated tasks demanding a nuanced understanding of cross-cultural contexts. A key example is recipe adaptation, which goes beyond simple translation to include a grasp of ingredients, culinary techniques, and dietary preferences specific to a given culture. We introduce a new task involving the translation and cultural adaptation of recipes between Chinese- and English-speaking cuisines. To support this investigation, we present CulturalRecipes, a unique dataset composed of automatically paired recipes written in Mandarin Chinese and English. This dataset is further enriched with a human-written and curated test set. In this intricate task of cross-cultural recipe adaptation, we evaluate the performance of various methods, including GPT-4 and other LLMs, traditional machine translation, and information retrieval techniques. Our comprehensive analysis includes both automatic and human evaluation metrics. While GPT-4 exhibits impressive abilities in adapting Chinese recipes into English, it still lags behind human expertise when translating English recipes into Chinese. This underscores the multifaceted nature of cultural adaptations. We anticipate that these insights will significantly contribute to future research on culturally aware language models and their practical application in culturally diverse contexts.},
  archive      = {J_TACL},
  author       = {Cao, Yong and Kementchedjhieva, Yova and Cui, Ruixiang and Karamolegkou, Antonia and Zhou, Li and Dare, Megan and Donatelli, Lucia and Hershcovich, Daniel},
  doi          = {10.1162/tacl_a_00634},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {80-99},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Cultural adaptation of recipes},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MGPT: Few-shot learners go multilingual. <em>TACL</em>,
<em>12</em>, 58–79. (<a
href="https://doi.org/10.1162/tacl_a_00633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces mGPT, a multilingual variant of GPT-3, pretrained on 61 languages from 25 linguistically diverse language families using Wikipedia and the C4 Corpus. We detail the design and pretraining procedure. The models undergo an intrinsic and extrinsic evaluation: language modeling in all languages, downstream evaluation on cross-lingual NLU datasets and benchmarks in 33 languages, and world knowledge probing in 23 languages. The in-context learning abilities are on par with the contemporaneous language models while covering a larger number of languages, including underrepresented and low-resource languages of the Commonwealth of Independent States and the indigenous peoples in Russia. The source code and the language models are publicly available under the MIT license.},
  archive      = {J_TACL},
  author       = {Shliazhko, Oleh and Fenogenova, Alena and Tikhonova, Maria and Kozlova, Anastasia and Mikhailov, Vladislav and Shavrina, Tatiana},
  doi          = {10.1162/tacl_a_00633},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {58-79},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {MGPT: Few-shot learners go multilingual},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Benchmarking large language models for news summarization.
<em>TACL</em>, <em>12</em>, 39–57. (<a
href="https://doi.org/10.1162/tacl_a_00632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have shown promise for automatic summarization but the reasons behind their successes are poorly understood. By conducting a human evaluation on ten LLMs across different pretraining methods, prompts, and model scales, we make two important observations. First, we find instruction tuning, not model size, is the key to the LLM’s zero-shot summarization capability. Second, existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance. To better evaluate LLMs, we perform human evaluation over high-quality summaries we collect from freelance writers. Despite major stylistic differences such as the amount of paraphrasing, we find that LLM summaries are judged to be on par with human written summaries.},
  archive      = {J_TACL},
  author       = {Zhang, Tianyi and Ladhak, Faisal and Durmus, Esin and Liang, Percy and McKeown, Kathleen and Hashimoto, Tatsunori B.},
  doi          = {10.1162/tacl_a_00632},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {39-57},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Benchmarking large language models for news summarization},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Language varieties of italy: Technology challenges and
opportunities. <em>TACL</em>, <em>12</em>, 19–38. (<a
href="https://doi.org/10.1162/tacl_a_00631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Italy is characterized by a one-of-a-kind linguistic diversity landscape in Europe, which implicitly encodes local knowledge, cultural traditions, artistic expressions, and history of its speakers. However, most local languages and dialects in Italy are at risk of disappearing within a few generations. The NLP community has recently begun to engage with endangered languages, including those of Italy. Yet, most efforts assume that these varieties are under-resourced language monoliths with an established written form and homogeneous functions and needs, and thus highly interchangeable with each other and with high-resource , standardized languages. In this paper, we introduce the linguistic context of Italy and challenge the default machine-centric assumptions of NLP for Italy’s language varieties. We advocate for a shift in the paradigm from machine-centric to speaker-centric NLP, and provide recommendations and opportunities for work that prioritizes languages and their speakers over technological advances. To facilitate the process, we finally propose building a local community towards responsible, participatory efforts aimed at supporting vitality of languages and dialects of Italy.},
  archive      = {J_TACL},
  author       = {Ramponi, Alan},
  doi          = {10.1162/tacl_a_00631},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {19-38},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Language varieties of italy: Technology challenges and opportunities},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AmbiFC: Fact-checking ambiguous claims with evidence.
<em>TACL</em>, <em>12</em>, 1–18. (<a
href="https://doi.org/10.1162/tacl_a_00629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated fact-checking systems verify claims against evidence to predict their veracity. In real-world scenarios, the retrieved evidence may not unambiguously support or refute the claim and yield conflicting but valid interpretations. Existing fact-checking datasets assume that the models developed with them predict a single veracity label for each claim, thus discouraging the handling of such ambiguity. To address this issue we present AmbiFC , 1 a fact-checking dataset with 10k claims derived from real-world information needs. It contains fine-grained evidence annotations of 50k passages from 5k Wikipedia pages. We analyze the disagreements arising from ambiguity when comparing claims against evidence in AmbiFC , observing a strong correlation of annotator disagreement with linguistic phenomena such as underspecification and probabilistic reasoning. We develop models for predicting veracity handling this ambiguity via soft labels, and find that a pipeline that learns the label distribution for sentence-level evidence selection and veracity prediction yields the best performance. We compare models trained on different subsets of AmbiFC and show that models trained on the ambiguous instances perform better when faced with the identified linguistic phenomena.},
  archive      = {J_TACL},
  author       = {Glockner, Max and Staliūnaitė, Ieva and Thorne, James and Vallejo, Gisela and Vlachos, Andreas and Gurevych, Iryna},
  doi          = {10.1162/tacl_a_00629},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {5},
  pages        = {1-18},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {AmbiFC: Fact-checking ambiguous claims with evidence},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
