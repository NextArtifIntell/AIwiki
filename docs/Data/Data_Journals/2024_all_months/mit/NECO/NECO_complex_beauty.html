<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>NECO_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="neco---84">NECO - 84</h2>
<ul>
<li><details>
<summary>
(2024). Optimizing attention and cognitive control costs using
temporally layered architectures. <em>NECO</em>, <em>36</em>(12),
2734–2763. (<a href="https://doi.org/10.1162/neco_a_01718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current reinforcement learning framework focuses exclusively on performance, often at the expense of efficiency. In contrast, biological control achieves remarkable performance while also optimizing computational energy expenditure and decision frequency. We propose a decision-bounded Markov decision process (DB-MDP) that constrains the number of decisions and computational energy available to agents in reinforcement learning environments. Our experiments demonstrate that existing reinforcement learning algorithms struggle within this framework, leading to either failure or suboptimal performance. To address this, we introduce a biologically inspired, temporally layered architecture (TLA), enabling agents to manage computational costs through two layers with distinct timescales and energy requirements. TLA achieves optimal performance in decision-bounded environments and in continuous control environments, matching state-of-the-art performance while using a fraction of the computing cost. Compared to current reinforcement learning algorithms that solely prioritize performance, our approach significantly lowers computational energy expenditure while maintaining performance. These findings establish a benchmark and pave the way for future research on energy and time-aware control.},
  archive      = {J_NECO},
  author       = {Patel, Devdhar and Sejnowski, Terrence and Siegelmann, Hava},
  doi          = {10.1162/neco_a_01718},
  journal      = {Neural Computation},
  month        = {11},
  number       = {12},
  pages        = {2734-2763},
  shortjournal = {Neural Comput.},
  title        = {Optimizing attention and cognitive control costs using temporally layered architectures},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fast algorithm for all-pairs-shortest-paths suitable for
neural networks. <em>NECO</em>, <em>36</em>(12), 2710–2733. (<a
href="https://doi.org/10.1162/neco_a_01716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Given a directed graph of nodes and edges connecting them, a common problem is to find the shortest path between any two nodes. Here we show that the shortest path distances can be found by a simple matrix inversion: if the edges are given by the adjacency matrix Aij, then with a suitably small value of γ, the shortest path distances are Dij=ceil(logγ[(I-γA)-1]ij).We derive several graph-theoretic bounds on the value of γ and explore its useful range with numerics on different graph types. Even when the distance function is not globally accurate across the entire graph, it still works locally to instruct pursuit of the shortest path. In this mode, it also extends to weighted graphs with positive edge weights. For a wide range of dense graphs, this distance function is computationally faster than the best available alternative. Finally, we show that this method leads naturally to a neural network solution of the all-pairs-shortest-path problem.},
  archive      = {J_NECO},
  author       = {Jing, Zeyu and Meister, Markus},
  doi          = {10.1162/neco_a_01716},
  journal      = {Neural Computation},
  month        = {11},
  number       = {12},
  pages        = {2710-2733},
  shortjournal = {Neural Comput.},
  title        = {A fast algorithm for all-pairs-shortest-paths suitable for neural networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fine granularity is critical for intelligent neural network
pruning. <em>NECO</em>, <em>36</em>(12), 2677–2709. (<a
href="https://doi.org/10.1162/neco_a_01717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural network pruning is a popular approach to reducing the computational costs of training and/or deploying a network and aims to do so while minimizing accuracy loss. Pruning methods that remove individual weights (fine granularity) can remove more total network parameters before reaching a given degree of accuracy loss, while methods that preserve some or all of a network’s structure (coarser granularity, such as pruning channels from a CNN) take better advantage of hardware and software optimized for dense matrix computations. We compare intelligent iterative pruning using several different criteria sampled from the literature against random pruning at initialization across multiple granularities on two different architectures and three image classification tasks. Our work is the first direct and comprehensive investigation of the relationship between granularity and the efficacy of intelligent pruning relative to a random-pruning baseline. We find that the accuracy advantage of intelligent over random pruning decreases dramatically as granularity becomes coarser, with minimal advantage for intelligent pruning at granularity coarse enough to fully preserve network structure. For instance, at pruning rates where random pruning leaves ResNet-20 at 85.0% test accuracy on CIFAR-10 after 30,000 training iterations, intelligent weight pruning with the best-in-context criterion leaves it at about 90.0% accuracy (on par with the unpruned network), kernel pruning leaves it at about 86.5%, and channel pruning leaves it at about 85.5%. Our results suggest that compared to coarse pruning, fine pruning combined with efficient implementation of the resulting networks is a more promising direction for easing the trade-off between high accuracy and low computational cost.},
  archive      = {J_NECO},
  author       = {Heyman, Alex and Zylberberg, Joel},
  doi          = {10.1162/neco_a_01717},
  journal      = {Neural Computation},
  month        = {11},
  number       = {12},
  pages        = {2677-2709},
  shortjournal = {Neural Comput.},
  title        = {Fine granularity is critical for intelligent neural network pruning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Orthogonal gated recurrent unit with neumann-cayley
transformation. <em>NECO</em>, <em>36</em>(12), 2651–2676. (<a
href="https://doi.org/10.1162/neco_a_01710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, using orthogonal matrices has been shown to be a promising approach to improving recurrent neural networks (RNNs) with training, stability, and convergence, particularly to control gradients. While gated recurrent unit (GRU) and long short-term memory (LSTM) architectures address the vanishing gradient problem by using a variety of gates and memory cells, they are still prone to the exploding gradient problem. In this work, we analyze the gradients in GRU and propose the use of orthogonal matrices to prevent exploding gradient problems and enhance long-term memory. We study where to use orthogonal matrices and propose a Neumann series–based scaled Cayley transformation for training orthogonal matrices in GRU, which we call Neumann-Cayley orthogonal GRU (NC-GRU). We present detailed experiments of our model on several synthetic and real-world tasks, which show that NC-GRU significantly outperforms GRU and several other RNNs.},
  archive      = {J_NECO},
  author       = {Zadorozhnyy, Vasily and Mucllari, Edison and Pospisil, Cole and Nguyen, Duc and Ye, Qiang},
  doi          = {10.1162/neco_a_01710},
  journal      = {Neural Computation},
  month        = {11},
  number       = {12},
  pages        = {2651-2676},
  shortjournal = {Neural Comput.},
  title        = {Orthogonal gated recurrent unit with neumann-cayley transformation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). KLIF: An optimized spiking neuron unit for tuning surrogate
gradient function. <em>NECO</em>, <em>36</em>(12), 2636–2650. (<a
href="https://doi.org/10.1162/neco_a_01712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) have garnered significant attention owing to their adeptness in processing temporal information, low power consumption, and enhanced biological plausibility. Despite these advantages, the development of efficient and high-performing learning algorithms for SNNs remains a formidable challenge. Techniques such as artificial neural network (ANN)-to-SNN conversion can convert ANNs to SNNs with minimal performance loss, but they necessitate prolonged simulations to approximate rate coding accurately. Conversely, the direct training of SNNs using spike-based backpropagation (BP), such as surrogate gradient approximation, is more flexible and widely adopted. Nevertheless, our research revealed that the shape of the surrogate gradient function profoundly influences the training and inference accuracy of SNNs. Importantly, we identified that the shape of the surrogate gradient function significantly affects the final training accuracy. The shape of the surrogate gradient function is typically manually selected before training and remains static throughout the training process. In this article, we introduce a novel k-based leaky integrate-and-fire (KLIF) spiking neural model. KLIF, featuring a learnable parameter, enables the dynamic adjustment of the height and width of the effective surrogate gradient near threshold during training. Our proposed model undergoes evaluation on static CIFAR-10 and CIFAR-100 data sets, as well as neuromorphic CIFAR10-DVS and DVS128-Gesture data sets. Experimental results demonstrate that KLIF outperforms the leaky Integrate-and-Fire (LIF) model across multiple data sets and network architectures. The superior performance of KLIF positions it as a viable replacement for the essential role of LIF in SNNs across diverse tasks.},
  archive      = {J_NECO},
  author       = {Jiang, Chunming and Zhang, Yilei},
  doi          = {10.1162/neco_a_01712},
  journal      = {Neural Computation},
  month        = {11},
  number       = {12},
  pages        = {2636-2650},
  shortjournal = {Neural Comput.},
  title        = {KLIF: An optimized spiking neuron unit for tuning surrogate gradient function},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Associative learning and active inference. <em>NECO</em>,
<em>36</em>(12), 2602–2635. (<a
href="https://doi.org/10.1162/neco_a_01711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Associative learning is a behavioral phenomenon in which individuals develop connections between stimuli or events based on their co-occurrence. Initially studied by Pavlov in his conditioning experiments, the fundamental principles of learning have been expanded on through the discovery of a wide range of learning phenomena. Computational models have been developed based on the concept of minimizing reward prediction errors. The Rescorla-Wagner model, in particular, is a well-known model that has greatly influenced the field of reinforcement learning. However, the simplicity of these models restricts their ability to fully explain the diverse range of behavioral phenomena associated with learning. In this study, we adopt the free energy principle, which suggests that living systems strive to minimize surprise or uncertainty under their internal models of the world. We consider the learning process as the minimization of free energy and investigate its relationship with the Rescorla-Wagner model, focusing on the informational aspects of learning, different types of surprise, and prediction errors based on beliefs and values. Furthermore, we explore how well-known behavioral phenomena such as blocking, overshadowing, and latent inhibition can be modeled within the active inference framework. We accomplish this by using the informational and novelty aspects of attention, which share similar ideas proposed by seemingly contradictory models such as Mackintosh and Pearce-Hall models. Thus, we demonstrate that the free energy principle, as a theoretical framework derived from first principles, can integrate the ideas and models of associative learning proposed based on empirical experiments and serve as a framework for a better understanding of the computational processes behind associative learning in the brain.},
  archive      = {J_NECO},
  author       = {Anokhin, Petr and Sorokin, Artyom and Burtsev, Mikhail and Friston, Karl},
  doi          = {10.1162/neco_a_01711},
  journal      = {Neural Computation},
  month        = {11},
  number       = {12},
  pages        = {2602-2635},
  shortjournal = {Neural Comput.},
  title        = {Associative learning and active inference},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse-coding variational autoencoders. <em>NECO</em>,
<em>36</em>(12), 2571–2601. (<a
href="https://doi.org/10.1162/neco_a_01715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sparse coding model posits that the visual system has evolved to efficiently code natural stimuli using a sparse set of features from an overcomplete dictionary. The original sparse coding model suffered from two key limitations; however: (1) computing the neural response to an image patch required minimizing a nonlinear objective function via recurrent dynamics and (2) fitting relied on approximate inference methods that ignored uncertainty. Although subsequent work has developed several methods to overcome these obstacles, we propose a novel solution inspired by the variational autoencoder (VAE) framework. We introduce the sparse coding variational autoencoder (SVAE), which augments the sparse coding model with a probabilistic recognition model parameterized by a deep neural network. This recognition model provides a neurally plausible feedforward implementation for the mapping from image patches to neural activities and enables a principled method for fitting the sparse coding model to data via maximization of the evidence lower bound (ELBO). The SVAE differs from standard VAEs in three key respects: the latent representation is overcomplete (there are more latent dimensions than image pixels), the prior is sparse or heavy-tailed instead of gaussian, and the decoder network is a linear projection instead of a deep network. We fit the SVAE to natural image data under different assumed prior distributions and show that it obtains higher test performance than previous fitting methods. Finally, we examine the response properties of the recognition network and show that it captures important nonlinear properties of neurons in the early visual pathway.},
  archive      = {J_NECO},
  author       = {Geadah, Victor and Barello, Gabriel and Greenidge, Daniel and Charles, Adam S. and Pillow, Jonathan W.},
  doi          = {10.1162/neco_a_01715},
  journal      = {Neural Computation},
  month        = {11},
  number       = {12},
  pages        = {2571-2601},
  shortjournal = {Neural Comput.},
  title        = {Sparse-coding variational autoencoders},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). ℓ1-regularized ICA: A novel method for analysis of
task-related fMRI data. <em>NECO</em>, <em>36</em>(11), 2540–2570. (<a
href="https://doi.org/10.1162/neco_a_01709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new method of independent component analysis (ICA) in order to extract appropriate features from high-dimensional data. In general, matrix factorization methods including ICA have a problem regarding the interpretability of extracted features. For the improvement of interpretability, sparse constraint on a factorized matrix is helpful. With this background, we construct a new ICA method with sparsity. In our method, the ℓ 1 -regularization term is added to the cost function of ICA, and minimization of the cost function is performed by a difference of convex functions algorithm. For the validity of our proposed method, we apply it to synthetic data and real functional magnetic resonance imaging data.},
  archive      = {J_NECO},
  author       = {Endo, Yusuke and Takeda, Koujin},
  doi          = {10.1162/neco_a_01709},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {2540-2570},
  shortjournal = {Neural Comput.},
  title        = {ℓ1-regularized ICA: A novel method for analysis of task-related fMRI data},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning internal representations of 3D transformations from
2D projected inputs. <em>NECO</em>, <em>36</em>(11), 2505–2539. (<a
href="https://doi.org/10.1162/neco_a_01695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a computational model for inferring 3D structure from the motion of projected 2D points in an image, with the aim of understanding how biological vision systems learn and internally represent 3D transformations from the statistics of their input. The model uses manifold transport operators to describe the action of 3D points in a scene as they undergo transformation. We show that the model can learn the generator of the Lie group for these transformations from purely 2D input, providing a proof-of-concept demonstration for how biological systems could adapt their internal representations based on sensory input. Focusing on a rotational model, we evaluate the ability of the model to infer depth from moving 2D projected points and to learn rotational transformations from 2D training stimuli. Finally, we compare the model performance to psychophysical performance on structure-from-motion tasks.},
  archive      = {J_NECO},
  author       = {Connor, Marissa and Olshausen, Bruno and Rozell, Christopher},
  doi          = {10.1162/neco_a_01695},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {2505-2539},
  shortjournal = {Neural Comput.},
  title        = {Learning internal representations of 3D transformations from 2D projected inputs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal and multifactor branching time active inference.
<em>NECO</em>, <em>36</em>(11), 2479–2504. (<a
href="https://doi.org/10.1162/neco_a_01703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active inference is a state-of-the-art framework for modeling the brain that explains a wide range of mechanisms. Recently, two versions of branching time active inference (BTAI) have been developed to handle the exponential (space and time) complexity class that occurs when computing the prior over all possible policies up to the time horizon. However, those two versions of BTAI still suffer from an exponential complexity class with regard to the number of observed and latent variables being modeled. We resolve this limitation by allowing each observation to have its own likelihood mapping and each latent variable to have its own transition mapping. The implicit mean field approximation was tested in terms of its efficiency and computational cost using a dSprites environment in which the metadata of the dSprites data set was used as input to the model. In this setting, earlier implementations of branching time active inference (namely, BTAI VMP and BTAI BF ) underperformed in relation to the mean field approximation ( BTAI 3MF ) in terms of performance and computational efficiency. Specifically, BTAI VMP was able to solve 96.9% of the task in 5.1 seconds, and BTAI BF was able to solve 98.6% of the task in 17.5 seconds. Our new approach outperformed both of its predecessors by solving the task completely (100%) in only 2.559 seconds.},
  archive      = {J_NECO},
  author       = {Champion, Théophile and Grześ, Marek and Bowman, Howard},
  doi          = {10.1162/neco_a_01703},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {2479-2504},
  shortjournal = {Neural Comput.},
  title        = {Multimodal and multifactor branching time active inference},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Latent space bayesian optimization with latent data
augmentation for enhanced exploration. <em>NECO</em>, <em>36</em>(11),
2446–2478. (<a href="https://doi.org/10.1162/neco_a_01708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent space Bayesian optimization (LSBO) combines generative models, typically variational autoencoders (VAE), with Bayesian optimization (BO), to generate de novo objects of interest. However, LSBO faces challenges due to the mismatch between the objectives of BO and VAE, resulting in poor exploration capabilities. In this article, we propose novel contributions to enhance LSBO efficiency and overcome this challenge. We first introduce the concept of latent consistency/inconsistency as a crucial problem in LSBO, arising from the VAE-BO mismatch. To address this, we propose the latent consistent aware-acquisition function (LCA-AF) that leverages consistent points in LSBO. Additionally, we present LCA-VAE, a novel VAE method that creates a latent space with increased consistent points through data augmentation in latent space and penalization of latent inconsistencies. Combining LCA-VAE and LCA-AF, we develop LCA-LSBO. Our approach achieves high sample efficiency and effective exploration, emphasizing the significance of addressing latent consistency through the novel incorporation of data augmentation in latent space within LCA-VAE in LSBO. We showcase the performance of our proposal via de novo image generation and de novo chemical design tasks.},
  archive      = {J_NECO},
  author       = {Boyar, Onur and Takeuchi, Ichiro},
  doi          = {10.1162/neco_a_01708},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {2446-2478},
  shortjournal = {Neural Comput.},
  title        = {Latent space bayesian optimization with latent data augmentation for enhanced exploration},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deconstructing deep active inference: A contrarian
information gatherer. <em>NECO</em>, <em>36</em>(11), 2403–2445. (<a
href="https://doi.org/10.1162/neco_a_01697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active inference is a theory of perception, learning, and decision making that can be applied to neuroscience, robotics, psychology, and machine learning. Recently, intensive research has been taking place to scale up this framework using Monte Carlo tree search and deep learning. The goal of this activity is to solve more complicated tasks using deep active inference. First, we review the existing literature and then progressively build a deep active inference agent as follows: we (1) implement a variational autoencoder (VAE), (2) implement a deep hidden Markov model (HMM), and (3) implement a deep critical hidden Markov model (CHMM). For the CHMM, we implemented two versions, one minimizing expected free energy, CHMM[EFE] and one maximizing rewards, CHMM[reward]. Then we experimented with three different action selection strategies: the ε -greedy algorithm as well as softmax and best action selection. According to our experiments, the models able to solve the dSprites environment are the ones that maximize rewards. On further inspection, we found that the CHMM minimizing expected free energy almost always picks the same action, which makes it unable to solve the dSprites environment. In contrast, the CHMM maximizing reward keeps on selecting all the actions, enabling it to successfully solve the task. The only difference between those two CHMMs is the epistemic value, which aims to make the outputs of the transition and encoder networks as close as possible. Thus, the CHMM minimizing expected free energy repeatedly picks a single action and becomes an expert at predicting the future when selecting this action. This effectively makes the KL divergence between the output of the transition and encoder networks small. Additionally, when selecting the action down the average reward is zero, while for all the other actions, the expected reward will be negative. Therefore, if the CHMM has to stick to a single action to keep the KL divergence small, then the action down is the most rewarding. We also show in simulation that the epistemic value used in deep active inference can behave degenerately and in certain circumstances effectively lose, rather than gain, information. As the agent minimizing EFE is not able to explore its environment, the appropriate formulation of the epistemic value in deep active inference remains an open question.},
  archive      = {J_NECO},
  author       = {Champion, Théophile and Grześ, Marek and Bonheme, Lisa and Bowman, Howard},
  doi          = {10.1162/neco_a_01697},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {2403-2445},
  shortjournal = {Neural Comput.},
  title        = {Deconstructing deep active inference: A contrarian information gatherer},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep nonnegative matrix factorization with beta divergences.
<em>NECO</em>, <em>36</em>(11), 2365–2402. (<a
href="https://doi.org/10.1162/neco_a_01679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep nonnegative matrix factorization (deep NMF) has recently emerged as a valuable technique for extracting multiple layers of features across different scales. However, all existing deep NMF models and algorithms have primarily centered their evaluation on the least squares error, which may not be the most appropriate metric for assessing the quality of approximations on diverse data sets. For instance, when dealing with data types such as audio signals and documents, it is widely acknowledged that ß-divergences offer a more suitable alternative. In this article, we develop new models and algorithms for deep NMF using some ß-divergences, with a focus on the Kullback-Leibler divergence. Subsequently, we apply these techniques to the extraction of facial features, the identification of topics within document collections, and the identification of materials within hyperspectral images.},
  archive      = {J_NECO},
  author       = {Leplat, Valentin and Hien, Le T. K. and Onwunta, Akwum and Gillis, Nicolas},
  doi          = {10.1162/neco_a_01679},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {2365-2402},
  shortjournal = {Neural Comput.},
  title        = {Deep nonnegative matrix factorization with beta divergences},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prototype analysis in hopfield networks with hebbian
learning. <em>NECO</em>, <em>36</em>(11), 2322–2364. (<a
href="https://doi.org/10.1162/neco_a_01704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We discuss prototype formation in the Hopfield network. Typically, Hebbian learning with highly correlated states leads to degraded memory performance. We show that this type of learning can lead to prototype formation, where unlearned states emerge as representatives of large correlated subsets of states, alleviating capacity woes. This process has similarities to prototype learning in human cognition. We provide a substantial literature review of prototype learning in associative memories, covering contributions from psychology, statistical physics, and computer science. We analyze prototype formation from a theoretical perspective and derive a stability condition for these states based on the number of examples of the prototype presented for learning, the noise in those examples, and the number of nonexample states presented. The stability condition is used to construct a probability of stability for a prototype state as the factors of stability change. We also note similarities to traditional network analysis, allowing us to find a prototype capacity. We corroborate these expectations of prototype formation with experiments using a simple Hopfield network with standard Hebbian learning. We extend our experiments to a Hopfield network trained on data with multiple prototypes and find the network is capable of stabilizing multiple prototypes concurrently. We measure the basins of attraction of the multiple prototype states, finding attractor strength grows with the number of examples and the agreement of examples. We link the stability and dominance of prototype states to the energy profile of these states, particularly when comparing the profile shape to target states or other spurious states.},
  archive      = {J_NECO},
  author       = {McAlister, Hayden and Robins, Anthony and Szymanski, Lech},
  doi          = {10.1162/neco_a_01704},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {2322-2364},
  shortjournal = {Neural Comput.},
  title        = {Prototype analysis in hopfield networks with hebbian learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spiking neural network pressure sensor. <em>NECO</em>,
<em>36</em>(11), 2299–2321. (<a
href="https://doi.org/10.1162/neco_a_01706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Von Neumann architecture requires information to be encoded as numerical values. For that reason, artificial neural networks running on computers require the data coming from sensors to be discretized. Other network architectures that more closely mimic biological neural networks (e.g., spiking neural networks) can be simulated on von Neumann architecture, but more important, they can also be executed on dedicated electrical circuits having orders of magnitude less power consumption. Unfortunately, input signal conditioning and encoding are usually not supported by such circuits, so a separate module consisting of an analog-to-digital converter, encoder, and transmitter is required. The aim of this article is to propose a sensor architecture, the output signal of which can be directly connected to the input of a spiking neural network. We demonstrate that the output signal is a valid spike source for the Izhikevich model neurons, ensuring the proper operation of a number of neurocomputational features. The advantages are clear: much lower power consumption, smaller area, and a less complex electronic circuit. The main disadvantage is that sensor characteristics somehow limit the parameters of applicable spiking neurons. The proposed architecture is illustrated by a case study involving a capacitive pressure sensor circuit, which is compatible with most of the neurocomputational properties of the Izhikevich neuron model. The sensor itself is characterized by very low power consumption: it draws only 3.49 μ A at 3.3 V.},
  archive      = {J_NECO},
  author       = {Markiewicz, Michał and Brzozowski, Ireneusz and Janusz, Szymon},
  doi          = {10.1162/neco_a_01706},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {2299-2321},
  shortjournal = {Neural Comput.},
  title        = {Spiking neural network pressure sensor},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predictive representations: Building blocks of intelligence.
<em>NECO</em>, <em>36</em>(11), 2225–2298. (<a
href="https://doi.org/10.1162/neco_a_01705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive behavior often requires predicting future events. The theory of reinforcement learning prescribes what kinds of predictive representations are useful and how to compute them. This review integrates these theoretical ideas with work on cognition and neuroscience. We pay special attention to the successor representation and its generalizations, which have been widely applied as both engineering tools and models of brain function. This convergence suggests that particular kinds of predictive representations may function as versatile building blocks of intelligence.},
  archive      = {J_NECO},
  author       = {Carvalho, Wilka and Tomov, Momchil S. and de Cothi, William and Barry, Caswell and Gershman, Samuel J.},
  doi          = {10.1162/neco_a_01705},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {2225-2298},
  shortjournal = {Neural Comput.},
  title        = {Predictive representations: Building blocks of intelligence},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Top-down priors disambiguate target and distractor features
in simulated covert visual search. <em>NECO</em>, <em>36</em>(10),
2201–2224. (<a href="https://doi.org/10.1162/neco_a_01700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several models of visual search consider visual attention as part of a perceptual inference process, in which top-down priors disambiguate bottom-up sensory information. Many of these models have focused on gaze behavior, but there are relatively fewer models of covert spatial attention, in which attention is directed to a peripheral location in visual space without a shift in gaze direction. Here, we propose a biologically plausible model of covert attention during visual search that helps to bridge the gap between Bayesian modeling and neurophysiological modeling by using (1) top-down priors over target features that are acquired through Hebbian learning, and (2) spatial resampling of modeled cortical receptive fields to enhance local spatial resolution of image representations for downstream target classification. By training a simple generative model using a Hebbian update rule, top-down priors for target features naturally emerge without the need for hand-tuned or predetermined priors. Furthermore, the implementation of covert spatial attention in our model is based on a known neurobiological mechanism, providing a plausible process through which Bayesian priors could locally enhance the spatial resolution of image representations. We validate this model during simulated visual search for handwritten digits among nondigit distractors, demonstrating that top-down priors improve accuracy for estimation of target location and classification, relative to bottom-up signals alone. Our results support previous reports in the literature that demonstrated beneficial effects of top-down priors on visual search performance, while extending this literature to incorporate known neural mechanisms of covert spatial attention.},
  archive      = {J_NECO},
  author       = {Theiss, Justin D. and Silver, Michael A.},
  doi          = {10.1162/neco_a_01700},
  journal      = {Neural Computation},
  month        = {9},
  number       = {10},
  pages        = {2201-2224},
  shortjournal = {Neural Comput.},
  title        = {Top-down priors disambiguate target and distractor features in simulated covert visual search},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mechanism of duration perception in artificial brains
suggests new model of attentional entrainment. <em>NECO</em>,
<em>36</em>(10), 2170–2200. (<a
href="https://doi.org/10.1162/neco_a_01699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While cognitive theory has advanced several candidate frameworks to explain attentional entrainment, the neural basis for the temporal allocation of attention is unknown. Here we present a new model of attentional entrainment guided by empirical evidence obtained using a cohort of 50 artificial brains. These brains were evolved in silico to perform a duration judgment task similar to one where human subjects perform duration judgments in auditory oddball paradigms. We found that the artificial brains display psychometric characteristics remarkably similar to those of human listeners and exhibit similar patterns of distortions of perception when presented with out-of-rhythm oddballs. A detailed analysis of mechanisms behind the duration distortion suggests that attention peaks at the end of the tone, which is inconsistent with previous attentional entrainment models. Instead, the new model of entrainment emphasizes increased attention to those aspects of the stimulus that the brain expects to be highly informative.},
  archive      = {J_NECO},
  author       = {Tehrani-Saleh, Ali and McAuley, J. Devin and Adami, Christoph},
  doi          = {10.1162/neco_a_01699},
  journal      = {Neural Computation},
  month        = {9},
  number       = {10},
  pages        = {2170-2200},
  shortjournal = {Neural Comput.},
  title        = {Mechanism of duration perception in artificial brains suggests new model of attentional entrainment},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trainable reference spikes improve temporal information
processing of SNNs with supervised learning. <em>NECO</em>,
<em>36</em>(10), 2136–2169. (<a
href="https://doi.org/10.1162/neco_a_01702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) are the next-generation neural networks composed of biologically plausible neurons that communicate through trains of spikes. By modifying the plastic parameters of SNNs, including weights and time delays, SNNs can be trained to perform various AI tasks, although in general not at the same level of performance as typical artificial neural networks (ANNs). One possible solution to improve the performance of SNNs is to consider plastic parameters other than just weights and time delays drawn from the inherent complexity of the neural system of the brain, which may help SNNs improve their information processing ability and achieve brainlike functions. Here, we propose reference spikes as a new type of plastic parameters in a supervised learning scheme in SNNs. A neuron receives reference spikes through synapses providing reference information independent of input to help during learning, whose number of spikes and timings are trainable by error backpropagation. Theoretically, reference spikes improve the temporal information processing of SNNs by modulating the integration of incoming spikes at a detailed level. Through comparative computational experiments, we demonstrate using supervised learning that reference spikes improve the memory capacity of SNNs to map input spike patterns to target output spike patterns and increase classification accuracy on the MNIST, Fashion-MNIST, and SHD data sets, where both input and target output are temporally encoded. Our results demonstrate that applying reference spikes improves the performance of SNNs by enhancing their temporal information processing ability.},
  archive      = {J_NECO},
  author       = {Wang, Zeyuan and Cruz, Luis},
  doi          = {10.1162/neco_a_01702},
  journal      = {Neural Computation},
  month        = {9},
  number       = {10},
  pages        = {2136-2169},
  shortjournal = {Neural Comput.},
  title        = {Trainable reference spikes improve temporal information processing of SNNs with supervised learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active inference and reinforcement learning: A unified
inference on continuous state and action spaces under partial
observability. <em>NECO</em>, <em>36</em>(10), 2073–2135. (<a
href="https://doi.org/10.1162/neco_a_01698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) has garnered significant attention for developing decision-making agents that aim to maximize rewards, specified by an external supervisor, within fully observable environments. However, many real-world problems involve partial or noisy observations, where agents cannot access complete and accurate information about the environment. These problems are commonly formulated as partially observable Markov decision processes (POMDPs). Previous studies have tackled RL in POMDPs by either incorporating the memory of past actions and observations or by inferring the true state of the environment from observed data. Nevertheless, aggregating observations and actions over time becomes impractical in problems with large decision-making time horizons and high-dimensional spaces. Furthermore, inference-based RL approaches often require many environmental samples to perform well, as they focus solely on reward maximization and neglect uncertainty in the inferred state. Active inference (AIF) is a framework naturally formulated in POMDPs and directs agents to select actions by minimizing a function called expected free energy (EFE). This supplies reward-maximizing (or exploitative) behavior, as in RL, with information-seeking (or exploratory) behavior. Despite this exploratory behavior of AIF, its use is limited to problems with small time horizons and discrete spaces due to the computational challenges associated with EFE. In this article, we propose a unified principle that establishes a theoretical connection between AIF and RL, enabling seamless integration of these two approaches and overcoming their limitations in continuous space POMDP settings. We substantiate our findings with rigorous theoretical analysis, providing novel perspectives for using AIF in designing and implementing artificial agents. Experimental results demonstrate the superior learning capabilities of our method compared to other alternative RL approaches in solving partially observable tasks with continuous spaces. Notably, our approach harnesses information-seeking exploration, enabling it to effectively solve reward-free problems and rendering explicit task reward design by an external supervisor optional.},
  archive      = {J_NECO},
  author       = {Malekzadeh, Parvin and Plataniotis, Konstantinos N.},
  doi          = {10.1162/neco_a_01698},
  journal      = {Neural Computation},
  month        = {9},
  number       = {10},
  pages        = {2073-2135},
  shortjournal = {Neural Comput.},
  title        = {Active inference and reinforcement learning: A unified inference on continuous state and action spaces under partial observability},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inference on the macroscopic dynamics of spiking neurons.
<em>NECO</em>, <em>36</em>(10), 2030–2072. (<a
href="https://doi.org/10.1162/neco_a_01701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The process of inference on networks of spiking neurons is essential to decipher the underlying mechanisms of brain computation and function. In this study, we conduct inference on parameters and dynamics of a mean-field approximation, simplifying the interactions of neurons. Estimating parameters of this class of generative model allows one to predict the system’s dynamics and responses under changing inputs and, indeed, changing parameters. We first assume a set of known state-space equations and address the problem of inferring the lumped parameters from observed time series. Crucially, we consider this problem in the setting of bistability, random fluctuations in system dynamics, and partial observations, in which some states are hidden. To identify the most efficient estimation or inversion scheme in this particular system identification, we benchmark against state-of-the-art optimization and Bayesian estimation algorithms, highlighting their strengths and weaknesses. Additionally, we explore how well the statistical relationships between parameters are maintained across different scales. We found that deep neural density estimators outperform other algorithms in the inversion scheme, despite potentially resulting in overestimated uncertainty and correlation between parameters. Nevertheless, this issue can be improved by incorporating time-delay embedding. We then eschew the mean-field approximation and employ deep neural ODEs on spiking neurons, illustrating prediction of system dynamics and vector fields from microscopic states. Overall, this study affords an opportunity to predict brain dynamics and responses to various perturbations or pharmacological interventions using deep neural networks.},
  archive      = {J_NECO},
  author       = {Baldy, Nina and Breyton, Martin and Woodman, Marmaduke M. and Jirsa, Viktor K. and Hashemi, Meysam},
  doi          = {10.1162/neco_a_01701},
  journal      = {Neural Computation},
  month        = {9},
  number       = {10},
  pages        = {2030-2072},
  shortjournal = {Neural Comput.},
  title        = {Inference on the macroscopic dynamics of spiking neurons},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Electrical signaling beyond neurons. <em>NECO</em>,
<em>36</em>(10), 1939–2029. (<a
href="https://doi.org/10.1162/neco_a_01696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural action potentials (APs) are difficult to interpret as signal encoders and/or computational primitives. Their relationships with stimuli and behaviors are obscured by the staggering complexity of nervous systems themselves. We can reduce this complexity by observing that “simpler” neuron-less organisms also transduce stimuli into transient electrical pulses that affect their behaviors. Without a complicated nervous system, APs are often easier to understand as signal/response mechanisms. We review examples of nonneural stimulus transductions in domains of life largely neglected by theoretical neuroscience: bacteria, protozoans, plants, fungi, and neuron-less animals. We report properties of those electrical signals—for example, amplitudes, durations, ionic bases, refractory periods, and particularly their ecological purposes. We compare those properties with those of neurons to infer the tasks and selection pressures that neurons satisfy. Throughout the tree of life, nonneural stimulus transductions time behavioral responses to environmental changes. Nonneural organisms represent the presence or absence of a stimulus with the presence or absence of an electrical signal. Their transductions usually exhibit high sensitivity and specificity to a stimulus, but are often slow compared to neurons. Neurons appear to be sacrificing the specificity of their stimulus transductions for sensitivity and speed. We interpret cellular stimulus transductions as a cell’s assertion that it detected something important at that moment in time. In particular, we consider neural APs as fast but noisy detection assertions. We infer that a principal goal of nervous systems is to detect extremely weak signals from noisy sensory spikes under enormous time pressure. We discuss neural computation proposals that address this goal by casting neurons as devices that implement online, analog, probabilistic computations with their membrane potentials. Those proposals imply a measurable relationship between afferent neural spiking statistics and efferent neural membrane electrophysiology.},
  archive      = {J_NECO},
  author       = {Monk, Travis and Dennler, Nik and Ralph, Nicholas and Rastogi, Shavika and Afshar, Saeed and Urbizagastegui, Pablo and Jarvis, Russell and van Schaik, André and Adamatzky, Andrew},
  doi          = {10.1162/neco_a_01696},
  journal      = {Neural Computation},
  month        = {9},
  number       = {10},
  pages        = {1939-2029},
  shortjournal = {Neural Comput.},
  title        = {Electrical signaling beyond neurons},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UAdam: Unified adam-type algorithmic framework for nonconvex
optimization. <em>NECO</em>, <em>36</em>(9), 1912–1938. (<a
href="https://doi.org/10.1162/neco_a_01692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adam-type algorithms have become a preferred choice for optimization in the deep learning setting; however, despite their success, their convergence is still not well understood. To this end, we introduce a unified framework for Adam-type algorithms, termed UAdam. It is equipped with a general form of the second-order moment, which makes it possible to include Adam and its existing and future variants as special cases, such as NAdam, AMSGrad, AdaBound, AdaFom, and Adan. The approach is supported by a rigorous convergence analysis of UAdam in the general nonconvex stochastic setting, showing that UAdam converges to the neighborhood of stationary points with a rate of O(1/ T ⁠ ). Furthermore, the size of the neighborhood decreases as the parameter β 1 increases. Importantly, our analysis only requires the first-order momentum factor to be close enough to 1, without any restrictions on the second-order momentum factor. Theoretical results also reveal the convergence conditions of vanilla Adam, together with the selection of appropriate hyperparameters. This provides a theoretical guarantee for the analysis, applications, and further developments of the whole general class of Adam-type algorithms. Finally, several numerical experiments are provided to support our theoretical findings.},
  archive      = {J_NECO},
  author       = {Jiang, Yiming and Liu, Jinlan and Xu, Dongpo and Mandic, Danilo P.},
  doi          = {10.1162/neco_a_01692},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1912-1938},
  shortjournal = {Neural Comput.},
  title        = {UAdam: Unified adam-type algorithmic framework for nonconvex optimization},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient hyperdimensional computing with spiking phasors.
<em>NECO</em>, <em>36</em>(9), 1886–1911. (<a
href="https://doi.org/10.1162/neco_a_01693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperdimensional (HD) computing (also referred to as vector symbolic architectures, VSAs) offers a method for encoding symbols into vectors, allowing for those symbols to be combined in different ways to form other vectors in the same vector space. The vectors and operators form a compositional algebra, such that composite vectors can be decomposed back to their constituent vectors. Many useful algorithms have implementations in HD computing, such as classification, spatial navigation, language modeling, and logic. In this letter, we propose a spiking implementation of Fourier holographic reduced representation (FHRR), one of the most versatile VSAs. The phase of each complex number of an FHRR vector is encoded as a spike time within a cycle. Neuron models derived from these spiking phasors can perform the requisite vector operations to implement an FHRR. We demonstrate the power and versatility of our spiking networks in a number of foundational problem domains, including symbol binding and unbinding, spatial representation, function representation, function integration, and memory (i.e., signal delay).},
  archive      = {J_NECO},
  author       = {Orchard, Jeff and Furlong, P. Michael and Simone, Kathryn},
  doi          = {10.1162/neco_a_01693},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1886-1911},
  shortjournal = {Neural Comput.},
  title        = {Efficient hyperdimensional computing with spiking phasors},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intrinsic rewards for exploration without harm from
observational noise: A simulation study based on the free energy
principle. <em>NECO</em>, <em>36</em>(9), 1854–1885. (<a
href="https://doi.org/10.1162/neco_a_01690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In reinforcement learning (RL), artificial agents are trained to maximize numerical rewards by performing tasks. Exploration is essential in RL because agents must discover information before exploiting it. Two rewards encouraging efficient exploration are the entropy of action policy and curiosity for information gain. Entropy is well established in the literature, promoting randomized action selection. Curiosity is defined in a broad variety of ways in literature, promoting discovery of novel experiences. One example, prediction error curiosity, rewards agents for discovering observations they cannot accurately predict. However, such agents may be distracted by unpredictable observational noises known as curiosity traps. Based on the free energy principle (FEP), this letter proposes hidden state curiosity, which rewards agents by the KL divergence between the predictive prior and posterior probabilities of latent variables. We trained six types of agents to navigate mazes: baseline agents without rewards for entropy or curiosity and agents rewarded for entropy and/or either prediction error curiosity or hidden state curiosity. We find that entropy and curiosity result in efficient exploration, especially both employed together. Notably, agents with hidden state curiosity demonstrate resilience against curiosity traps, which hinder agents with prediction error curiosity. This suggests implementing the FEP that may enhance the robustness and generalization of RL models, potentially aligning the learning processes of artificial and biological agents.},
  archive      = {J_NECO},
  author       = {Tinker, Theodore Jerome and Doya, Kenji and Tani, Jun},
  doi          = {10.1162/neco_a_01690},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1854-1885},
  shortjournal = {Neural Comput.},
  title        = {Intrinsic rewards for exploration without harm from observational noise: A simulation study based on the free energy principle},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spontaneous emergence of robustness to light variation in
CNNs with a precortically inspired module. <em>NECO</em>,
<em>36</em>(9), 1832–1853. (<a
href="https://doi.org/10.1162/neco_a_01691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analogies between the mammalian primary visual cortex and the structure of CNNs used for image classification tasks suggest that the introduction of an additional preliminary convolutional module inspired by the mathematical modeling of the precortical neuronal circuits can improve robustness with respect to global light intensity and contrast variations in the input images. We validate this hypothesis using the popular databases MNIST, FashionMNIST, and SVHN for these variations once an extra module is added.},
  archive      = {J_NECO},
  author       = {Petkovic, J. and Fioresi, R.},
  doi          = {10.1162/neco_a_01691},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1832-1853},
  shortjournal = {Neural Comput.},
  title        = {Spontaneous emergence of robustness to light variation in CNNs with a precortically inspired module},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the search for data-driven and reproducible schizophrenia
subtypes using resting state fMRI data from multiple sites.
<em>NECO</em>, <em>36</em>(9), 1799–1831. (<a
href="https://doi.org/10.1162/neco_a_01689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For decades, fMRI data have been used to search for biomarkers for patients with schizophrenia. Still, firm conclusions are yet to be made, which is often attributed to the high internal heterogeneity of the disorder. A promising way to disentangle the heterogeneity is to search for subgroups of patients with more homogeneous biological profiles. We applied an unsupervised multiple co-clustering (MCC) method to identify subtypes using functional connectivity data from a multisite resting-state data set. We merged data from two publicly available databases and split the data into a discovery data set (143 patients and 143 healthy controls (HC)) and an external test data set (63 patients and 63 HC) from independent sites. On the discovery data, we investigated the stability of the clustering toward data splits and initializations. Subsequently we searched for cluster solutions, also called “views,” with a significant diagnosis association and evaluated these based on their subject and feature cluster separability, and correlation to clinical manifestations as measured with the positive and negative syndrome scale (PANSS). Finally, we validated our findings by testing the diagnosis association on the external test data. A major finding of our study was that the stability of the clustering was highly dependent on variations in the data set, and even across initializations, we found only a moderate subject clustering stability. Nevertheless, we still discovered one view with a significant diagnosis association. This view reproducibly showed an overrepresentation of schizophrenia patients in three subject clusters, and one feature cluster showed a continuous trend, ranging from positive to negative connectivity values, when sorted according to the proportions of patients with schizophrenia. When investigating all patients, none of the feature clusters in the view were associated with severity of positive, negative, and generalized symptoms, indicating that the cluster solutions reflect other disease related mechanisms.},
  archive      = {J_NECO},
  author       = {Krohne, Lærke Gebser and Hansen, Ingeborg Helbech and Madsen, Kristoffer H.},
  doi          = {10.1162/neco_a_01689},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1799-1831},
  shortjournal = {Neural Comput.},
  title        = {On the search for data-driven and reproducible schizophrenia subtypes using resting state fMRI data from multiple sites},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Manifold gaussian variational bayes on the precision matrix.
<em>NECO</em>, <em>36</em>(9), 1744–1798. (<a
href="https://doi.org/10.1162/neco_a_01686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an optimization algorithm for variational inference (VI) in complex models. Our approach relies on natural gradient updates where the variational space is a Riemann manifold. We develop an efficient algorithm for gaussian variational inference whose updates satisfy the positive definite constraint on the variational covariance matrix. Our manifold gaussian variational Bayes on the precision matrix (MGVBP) solution provides simple update rules, is straightforward to implement, and the use of the precision matrix parameterization has a significant computational advantage. Due to its black-box nature, MGVBP stands as a ready-to-use solution for VI in complex models. Over five data sets, we empirically validate our feasible approach on different statistical and econometric models, discussing its performance with respect to baseline methods.},
  archive      = {J_NECO},
  author       = {Magris, Martin and Shabani, Mostafa and Iosifidis, Alexandros},
  doi          = {10.1162/neco_a_01686},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1744-1798},
  shortjournal = {Neural Comput.},
  title        = {Manifold gaussian variational bayes on the precision matrix},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human eyes–inspired recurrent neural networks are more
robust against adversarial noises. <em>NECO</em>, <em>36</em>(9),
1713–1743. (<a href="https://doi.org/10.1162/neco_a_01688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans actively observe the visual surroundings by focusing on salient objects and ignoring trivial details. However, computer vision models based on convolutional neural networks (CNN) often analyze visual input all at once through a single feedforward pass. In this study, we designed a dual-stream vision model inspired by the human brain. This model features retina-like input layers and includes two streams: one determining the next point of focus (the fixation), while the other interprets the visuals surrounding the fixation. Trained on image recognition, this model examines an image through a sequence of fixations, each time focusing on different parts, thereby progressively building a representation of the image. We evaluated this model against various benchmarks in terms of object recognition, gaze behavior, and adversarial robustness. Our findings suggest that the model can attend and gaze in ways similar to humans without being explicitly trained to mimic human attention and that the model can enhance robustness against adversarial attacks due to its retinal sampling and recurrent processing. In particular, the model can correct its perceptual errors by taking more glances, setting itself apart from all feedforward-only models. In conclusion, the interactions of retinal sampling, eye movement, and recurrent dynamics are important to human-like visual exploration and inference.},
  archive      = {J_NECO},
  author       = {Choi, Minkyu and Zhang, Yizhen and Han, Kuan and Wang, Xiaokai and Liu, Zhongming},
  doi          = {10.1162/neco_a_01688},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1713-1743},
  shortjournal = {Neural Comput.},
  title        = {Human Eyes–Inspired recurrent neural networks are more robust against adversarial noises},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hebbian descent: A unified view on log-likelihood learning.
<em>NECO</em>, <em>36</em>(9), 1669–1712. (<a
href="https://doi.org/10.1162/neco_a_01684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study discusses the negative impact of the derivative of the activation functions in the output layer of artificial neural networks, in particular in continual learning. We propose Hebbian descent as a theoretical framework to overcome this limitation, which is implemented through an alternative loss function for gradient descent we refer to as Hebbian descent loss. This loss is effectively the generalized log-likelihood loss and corresponds to an alternative weight update rule for the output layer wherein the derivative of the activation function is disregarded. We show how this update avoids vanishing error signals during backpropagation in saturated regions of the activation functions, which is particularly helpful in training shallow neural networks and deep neural networks where saturating activation functions are only used in the output layer. In combination with centering, Hebbian descent leads to better continual learning capabilities. It provides a unifying perspective on Hebbian learning, gradient descent, and generalized linear models, for all of which we discuss the advantages and disadvantages. Given activation functions with strictly positive derivative (as often the case in practice), Hebbian descent inherits the convergence properties of regular gradient descent. While established pairings of loss and output layer activation function (e.g., mean squared error with linear or cross-entropy with sigmoid/softmax) are subsumed by Hebbian descent, we provide general insights for designing arbitrary loss activation function combinations that benefit from Hebbian descent. For shallow networks, we show that Hebbian descent outperforms Hebbian learning, has a performance similar to regular gradient descent, and has a much better performance than all other tested update rules in continual learning. In combination with centering, Hebbian descent implements a forgetting mechanism that prevents catastrophic interference notably better than the other tested update rules. When training deep neural networks, our experimental results suggest that Hebbian descent has better or similar performance as gradient descent.},
  archive      = {J_NECO},
  author       = {Melchior, Jan and Schiewer, Robin and Wiskott, Laurenz},
  doi          = {10.1162/neco_a_01684},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1669-1712},
  shortjournal = {Neural Comput.},
  title        = {Hebbian descent: A unified view on log-likelihood learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A general, noise-driven mechanism for the 1/f-like behavior
of neural field spectra. <em>NECO</em>, <em>36</em>(8), 1643–1668. (<a
href="https://doi.org/10.1162/neco_a_01682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consistent observations across recording modalities, experiments, and neural systems find neural field spectra with 1/f-like scaling, eliciting many alternative theories to explain this universal phenomenon. We show that a general dynamical system with stochastic drive and minimal assumptions generates 1/f-like spectra consistent with the range of values observed in vivo without requiring a specific biological mechanism or collective critical behavior.},
  archive      = {J_NECO},
  author       = {Kramer, Mark A. and Chu, Catherine J.},
  doi          = {10.1162/neco_a_01682},
  journal      = {Neural Computation},
  month        = {7},
  number       = {8},
  pages        = {1643-1668},
  shortjournal = {Neural Comput.},
  title        = {A general, noise-driven mechanism for the 1/f-like behavior of neural field spectra},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Promoting the shift from pixel-level correlations to object
semantics learning by rethinking computer vision benchmark data sets.
<em>NECO</em>, <em>36</em>(8), 1626–1642. (<a
href="https://doi.org/10.1162/neco_a_01677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In computer vision research, convolutional neural networks (CNNs) have demonstrated remarkable capabilities at extracting patterns from raw pixel data, achieving state-of-the-art recognition accuracy. However, they significantly differ from human visual perception, prioritizing pixel-level correlations and statistical patterns, often overlooking object semantics. To explore this difference, we propose an approach that isolates core visual features crucial for human perception and object recognition: color, texture, and shape. In experiments on three benchmarks—Fruits 360, CIFAR-10, and Fashion MNIST—each visual feature is individually input into a neural network. Results reveal data set–dependent variations in classification accuracy, highlighting that deep learning models tend to learn pixel-level correlations instead of fundamental visual features. To validate this observation, we used various combinations of concatenated visual features as input for a neural network on the CIFAR-10 data set. CNNs excel at learning statistical patterns in images, achieving exceptional performance when training and test data share similar distributions. To substantiate this point, we trained a CNN on CIFAR-10 data set and evaluated its performance on the “dog” class from CIFAR-10 and on an equivalent number of examples from the Stanford Dogs data set. The CNN poor performance on Stanford Dogs images underlines the disparity between deep learning and human visual perception, highlighting the need for models that learn object semantics. Specialized benchmark data sets with controlled variations hold promise for aligning learned representations with human cognition in computer vision research.},
  archive      = {J_NECO},
  author       = {Osório, Maria and Wichert, Andreas},
  doi          = {10.1162/neco_a_01677},
  journal      = {Neural Computation},
  month        = {7},
  number       = {8},
  pages        = {1626-1642},
  shortjournal = {Neural Comput.},
  title        = {Promoting the shift from pixel-level correlations to object semantics learning by rethinking computer vision benchmark data sets},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy complexity of convolutional neural networks.
<em>NECO</em>, <em>36</em>(8), 1601–1625. (<a
href="https://doi.org/10.1162/neco_a_01676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The energy efficiency of hardware implementations of convolutional neural networks (CNNs) is critical to their widespread deployment in low-power mobile devices. Recently, a number of methods have been proposed for providing energy-optimal mappings of CNNs onto diverse hardware accelerators. Their estimated energy consumption is related to specific implementation details and hardware parameters, which does not allow for machine-independent exploration of CNN energy measures. In this letter, we introduce a simplified theoretical energy complexity model for CNNs, based on only a two-level memory hierarchy that captures asymptotically all important sources of energy consumption for different CNN hardware implementations. In this model, we derive a simple energy lower bound and calculate the energy complexity of evaluating a CNN layer for two common data flows, providing corresponding upper bounds. According to statistical tests, the theoretical energy upper and lower bounds we present fit asymptotically very well with the real energy consumption of CNN implementations on the Simba and Eyeriss hardware platforms, estimated by the Timeloop/Accelergy program, which validates the proposed energy complexity model for CNNs.},
  archive      = {J_NECO},
  author       = {Šíma, Jiří and Vidnerová, Petra and Mrázek, Vojtěch},
  doi          = {10.1162/neco_a_01676},
  journal      = {Neural Computation},
  month        = {7},
  number       = {8},
  pages        = {1601-1625},
  shortjournal = {Neural Comput.},
  title        = {Energy complexity of convolutional neural networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning fixed points of recurrent neural networks by
reparameterizing the network model. <em>NECO</em>, <em>36</em>(8),
1568–1600. (<a href="https://doi.org/10.1162/neco_a_01681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In computational neuroscience, recurrent neural networks are widely used to model neural activity and learning. In many studies, fixed points of recurrent neural networks are used to model neural responses to static or slowly changing stimuli, such as visual cortical responses to static visual stimuli. These applications raise the question of how to train the weights in a recurrent neural network to minimize a loss function evaluated on fixed points. In parallel, training fixed points is a central topic in the study of deep equilibrium models in machine learning. A natural approach is to use gradient descent on the Euclidean space of weights. We show that this approach can lead to poor learning performance due in part to singularities that arise in the loss surface. We use a reparameterization of the recurrent network model to derive two alternative learning rules that produce more robust learning dynamics. We demonstrate that these learning rules avoid singularities and learn more effectively than standard gradient descent. The new learning rules can be interpreted as steepest descent and gradient descent, respectively, under a non-Euclidean metric on the space of recurrent weights. Our results question the common, implicit assumption that learning in the brain should be expected to follow the negative Euclidean gradient of synaptic weights.},
  archive      = {J_NECO},
  author       = {Zhu, Vicky and Rosenbaum, Robert},
  doi          = {10.1162/neco_a_01681},
  journal      = {Neural Computation},
  month        = {7},
  number       = {8},
  pages        = {1568-1600},
  shortjournal = {Neural Comput.},
  title        = {Learning fixed points of recurrent neural networks by reparameterizing the network model},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trade-offs between energy and depth of neural networks.
<em>NECO</em>, <em>36</em>(8), 1541–1567. (<a
href="https://doi.org/10.1162/neco_a_01683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an investigation on threshold circuits and other discretized neural networks in terms of the following four computational resources—size (the number of gates), depth (the number of layers), weight (weight resolution), and energy—where the energy is a complexity measure inspired by sparse coding and is defined as the maximum number of gates outputting nonzero values, taken over all the input assignments. As our main result, we prove that if a threshold circuit C of size s ⁠ , depth d ⁠ , energy e ⁠ , and weight w computes a Boolean function f (i.e., a classification task) of n variables, it holds that log ( rk ( f ) ) ≤ e d ( log s + log w + log n ) regardless of the algorithm employed by C to compute f ⁠ , where rk ( f ) is a parameter solely determined by a scale of f and defined as the maximum rank of a communication matrix with regard to f taken over all the possible partitions of the n input variables. For example, given a Boolean function CD n ( ξ ⁠ ) = ⋁ i = 1 n / 2 ξ i ∧ ξ n / 2 + i ⁠ , we can prove that n / 2 ≤ e d ( log s + log w + log n ) holds for any circuit C computing CD n ⁠ . While its left-hand side is linear in n ⁠ , its right-hand side is bounded by the product of the logarithmic factors of s , w , n and the linear factors of d , e ⁠ . If we view the logarithmic terms as having a negligible impact on the bound, our result implies a trade-off between depth and energy: n / 2 needs to be smaller than the product of e and d ⁠ . For other neural network models, such as discretized ReLU circuits and discretized sigmoid circuits, we also prove that a similar trade-off holds. Thus, our results indicate that increasing depth linearly enhances the capability of neural networks to acquire sparse representations when there are hardware constraints on the number of neurons and weight resolution.},
  archive      = {J_NECO},
  author       = {Uchizawa, Kei and Abe, Haruki},
  doi          = {10.1162/neco_a_01683},
  journal      = {Neural Computation},
  month        = {7},
  number       = {8},
  pages        = {1541-1567},
  shortjournal = {Neural Comput.},
  title        = {Trade-offs between energy and depth of neural networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pulse shape and voltage-dependent synchronization in spiking
neuron networks. <em>NECO</em>, <em>36</em>(8), 1476–1540. (<a
href="https://doi.org/10.1162/neco_a_01680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pulse-coupled spiking neural networks are a powerful tool to gain mechanistic insights into how neurons self-organize to produce coherent collective behavior. These networks use simple spiking neuron models, such as the θ -neuron or the quadratic integrate-and-fire (QIF) neuron, that replicate the essential features of real neural dynamics. Interactions between neurons are modeled with infinitely narrow pulses, or spikes, rather than the more complex dynamics of real synapses. To make these networks biologically more plausible, it has been proposed that they must also account for the finite width of the pulses, which can have a significant impact on the network dynamics. However, the derivation and interpretation of these pulses are contradictory, and the impact of the pulse shape on the network dynamics is largely unexplored. Here, I take a comprehensive approach to pulse coupling in networks of QIF and θ -neurons. I argue that narrow pulses activate voltage-dependent synaptic conductances and show how to implement them in QIF neurons such that their effect can last through the phase after the spike. Using an exact low-dimensional description for networks of globally coupled spiking neurons, I prove for instantaneous interactions that collective oscillations emerge due to an effective coupling through the mean voltage. I analyze the impact of the pulse shape by means of a family of smooth pulse functions with arbitrary finite width and symmetric or asymmetric shapes. For symmetric pulses, the resulting voltage coupling is not very effective in synchronizing neurons, but pulses that are slightly skewed to the phase after the spike readily generate collective oscillations. The results unveil a voltage-dependent spike synchronization mechanism at the heart of emergent collective behavior, which is facilitated by pulses of finite width and complementary to traditional synaptic transmission in spiking neuron networks.},
  archive      = {J_NECO},
  author       = {Pietras, Bastian},
  doi          = {10.1162/neco_a_01680},
  journal      = {Neural Computation},
  month        = {7},
  number       = {8},
  pages        = {1476-1540},
  shortjournal = {Neural Comput.},
  title        = {Pulse shape and voltage-dependent synchronization in spiking neuron networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extended poisson gaussian-process latent variable model for
unsupervised neural decoding. <em>NECO</em>, <em>36</em>(8), 1449–1475.
(<a href="https://doi.org/10.1162/neco_a_01685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimension reduction on neural activity paves a way for unsupervised neural decoding by dissociating the measurement of internal neural pattern reactivation from the measurement of external variable tuning. With assumptions only on the smoothness of latent dynamics and of internal tuning curves, the Poisson gaussian-process latent variable model (P-GPLVM; Wu et al., 2017 ) is a powerful tool to discover the low-dimensional latent structure for high-dimensional spike trains. However, when given novel neural data, the original model lacks a method to infer their latent trajectories in the learned latent space, limiting its ability for estimating the neural reactivation. Here, we extend the P-GPLVM to enable the latent variable inference of new data constrained by previously learned smoothness and mapping information. We also describe a principled approach for the constrained latent variable inference for temporally compressed patterns of activity, such as those found in population burst events during hippocampal sharp-wave ripples, as well as metrics for assessing the validity of neural pattern reactivation and inferring the encoded experience. Applying these approaches to hippocampal ensemble recordings during active maze exploration, we replicate the result that P-GPLVM learns a latent space encoding the animal’s position. We further demonstrate that this latent space can differentiate one maze context from another. By inferring the latent variables of new neural data during running, certain neural patterns are observed to reactivate, in accordance with the similarity of experiences encoded by its nearby neural trajectories in the training data manifold. Finally, reactivation of neural patterns can be estimated for neural activity during population burst events as well, allowing the identification for replay events of versatile behaviors and more general experiences. Thus, our extension of the P-GPLVM framework for unsupervised analysis of neural activity can be used to answer critical questions related to scientific discovery.},
  archive      = {J_NECO},
  author       = {Luo, Della Daiyi and Giri, Bapun and Diba, Kamran and Kemere, Caleb},
  doi          = {10.1162/neco_a_01685},
  journal      = {Neural Computation},
  month        = {7},
  number       = {8},
  pages        = {1449-1475},
  shortjournal = {Neural Comput.},
  title        = {Extended poisson gaussian-process latent variable model for unsupervised neural decoding},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A mean field to capture asynchronous irregular dynamics of
conductance-based networks of adaptive quadratic integrate-and-fire
neuron models. <em>NECO</em>, <em>36</em>(7), 1433–1448. (<a
href="https://doi.org/10.1162/neco_a_01670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mean-field models are a class of models used in computational neuroscience to study the behavior of large populations of neurons. These models are based on the idea of representing the activity of a large number of neurons as the average behavior of mean-field variables. This abstraction allows the study of large-scale neural dynamics in a computationally efficient and mathematically tractable manner. One of these methods, based on a semianalytical approach, has previously been applied to different types of single-neuron models, but never to models based on a quadratic form. In this work, we adapted this method to quadratic integrate-and-fire neuron models with adaptation and conductance-based synaptic interactions. We validated the mean-field model by comparing it to the spiking network model. This mean-field model should be useful to model large-scale activity based on quadratic neurons interacting with conductance-based synapses.},
  archive      = {J_NECO},
  author       = {Alexandersen, Christoffer G. and Duprat, Chloé and Ezzati, Aitakin and Houzelstein, Pierre and Ledoux, Ambre and Liu, Yuhong and Saghir, Sandra and Destexhe, Alain and Tesler, Federico and Depannemaecker, Damien},
  doi          = {10.1162/neco_a_01670},
  journal      = {Neural Computation},
  month        = {6},
  number       = {7},
  pages        = {1433-1448},
  shortjournal = {Neural Comput.},
  title        = {A mean field to capture asynchronous irregular dynamics of conductance-based networks of adaptive quadratic integrate-and-fire neuron models},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Is learning in biological neural networks based on
stochastic gradient descent? An analysis using stochastic processes.
<em>NECO</em>, <em>36</em>(7), 1424–1432. (<a
href="https://doi.org/10.1162/neco_a_01668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been an intense debate about how learning in biological neural networks (BNNs) differs from learning in artificial neural networks. It is often argued that the updating of connections in the brain relies only on local information, and therefore a stochastic gradient-descent type optimization method cannot be used. In this note, we study a stochastic model for supervised learning in BNNs. We show that a (continuous) gradient step occurs approximately when each learning opportunity is processed by many local updates. This result suggests that stochastic gradient descent may indeed play a role in optimizing BNNs.},
  archive      = {J_NECO},
  author       = {Christensen, Sören and Kallsen, Jan},
  doi          = {10.1162/neco_a_01668},
  journal      = {Neural Computation},
  month        = {6},
  number       = {7},
  pages        = {1424-1432},
  shortjournal = {Neural Comput.},
  title        = {Is learning in biological neural networks based on stochastic gradient descent? an analysis using stochastic processes},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Associative learning of an unnormalized successor
representation. <em>NECO</em>, <em>36</em>(7), 1410–1423. (<a
href="https://doi.org/10.1162/neco_a_01675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The successor representation is known to relate to temporal associations learned in the temporal context model (Gershman et al., 2012), and subsequent work suggests a wide relevance of the successor representation across spatial, visual, and abstract relational tasks. I demonstrate that the successor representation and purely associative learning have an even deeper relationship than initially indicated: Hebbian temporal associations are an unnormalized form of the successor representation, such that the two converge on an identical representation whenever all states are equally frequent and can correlate highly in practice even when the state distribution is nonuniform.},
  archive      = {J_NECO},
  author       = {Verosky, Niels J.},
  doi          = {10.1162/neco_a_01675},
  journal      = {Neural Computation},
  month        = {6},
  number       = {7},
  pages        = {1410-1423},
  shortjournal = {Neural Comput.},
  title        = {Associative learning of an unnormalized successor representation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse generalized canonical correlation analysis:
Distributed alternating iteration-based approach. <em>NECO</em>,
<em>36</em>(7), 1380–1409. (<a
href="https://doi.org/10.1162/neco_a_01673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse canonical correlation analysis (CCA) is a useful statistical tool to detect latent information with sparse structures. However, sparse CCA, where the sparsity could be considered as a Laplace prior on the canonical variates, works only for two data sets, that is, there are only two views or two distinct objects. To overcome this limitation, we propose a sparse generalized canonical correlation analysis (GCCA), which could detect the latent relations of multiview data with sparse structures. Specifically, we convert the GCCA into a linear system of equations and impose ℓ 1 minimization penalty to pursue sparsity. This results in a nonconvex problem on the Stiefel manifold. Based on consensus optimization, a distributed alternating iteration approach is developed, and consistency is investigated elaborately under mild conditions. Experiments on several synthetic and real-world data sets demonstrate the effectiveness of the proposed algorithm.},
  archive      = {J_NECO},
  author       = {Lv, Kexin and Cai, Jia and Huo, Junyi and Shang, Chao and Huang, Xiaolin and Yang, Jie},
  doi          = {10.1162/neco_a_01673},
  journal      = {Neural Computation},
  month        = {6},
  number       = {7},
  pages        = {1380-1409},
  shortjournal = {Neural Comput.},
  title        = {Sparse generalized canonical correlation analysis: Distributed alternating iteration-based approach},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data efficiency, dimensionality reduction, and the
generalized symmetric information bottleneck. <em>NECO</em>,
<em>36</em>(7), 1353–1379. (<a
href="https://doi.org/10.1162/neco_a_01667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The symmetric information bottleneck (SIB), an extension of the more familiar information bottleneck, is a dimensionality-reduction technique that simultaneously compresses two random variables to preserve information between their compressed versions. We introduce the generalized symmetric information bottleneck (GSIB), which explores different functional forms of the cost of such simultaneous reduction. We then explore the data set size requirements of such simultaneous compression. We do this by deriving bounds and root-mean-squared estimates of statistical fluctuations of the involved loss functions. We show that in typical situations, the simultaneous GSIB compression requires qualitatively less data to achieve the same errors compared to compressing variables one at a time. We suggest that this is an example of a more general principle that simultaneous compression is more data efficient than independent compression of each of the input variables.},
  archive      = {J_NECO},
  author       = {Martini, K. Michael and Nemenman, Ilya},
  doi          = {10.1162/neco_a_01667},
  journal      = {Neural Computation},
  month        = {6},
  number       = {7},
  pages        = {1353-1379},
  shortjournal = {Neural Comput.},
  title        = {Data efficiency, dimensionality reduction, and the generalized symmetric information bottleneck},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bioplausible unsupervised delay learning for extracting
spatiotemporal features in spiking neural networks. <em>NECO</em>,
<em>36</em>(7), 1332–1352. (<a
href="https://doi.org/10.1162/neco_a_01674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The plasticity of the conduction delay between neurons plays a fundamental role in learning temporal features that are essential for processing videos, speech, and many high-level functions. However, the exact underlying mechanisms in the brain for this modulation are still under investigation. Devising a rule for precisely adjusting the synaptic delays could eventually help in developing more efficient and powerful brain-inspired computational models. In this article, we propose an unsupervised bioplausible learning rule for adjusting the synaptic delays in spiking neural networks. We also provide the mathematical proofs to show the convergence of our rule in learning spatiotemporal patterns. Furthermore, to show the effectiveness of our learning rule, we conducted several experiments on random dot kinematogram and a subset of DVS128 Gesture data sets. The experimental results indicate the efficiency of applying our proposed delay learning rule in extracting spatiotemporal features in an STDP-based spiking neural network.},
  archive      = {J_NECO},
  author       = {Nadafian, Alireza and Ganjtabesh, Mohammad},
  doi          = {10.1162/neco_a_01674},
  journal      = {Neural Computation},
  month        = {6},
  number       = {7},
  pages        = {1332-1352},
  shortjournal = {Neural Comput.},
  title        = {Bioplausible unsupervised delay learning for extracting spatiotemporal features in spiking neural networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multimodal fitting approach to construct single-neuron
models with patch clamp and high-density microelectrode arrays.
<em>NECO</em>, <em>36</em>(7), 1286–1331. (<a
href="https://doi.org/10.1162/neco_a_01672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In computational neuroscience, multicompartment models are among the most biophysically realistic representations of single neurons. Constructing such models usually involves the use of the patch-clamp technique to record somatic voltage signals under different experimental conditions. The experimental data are then used to fit the many parameters of the model. While patching of the soma is currently the gold-standard approach to build multicompartment models, several studies have also evidenced a richness of dynamics in dendritic and axonal sections. Recording from the soma alone makes it hard to observe and correctly parameterize the activity of nonsomatic compartments. In order to provide a richer set of data as input to multicompartment models, we here investigate the combination of somatic patch-clamp recordings with recordings of high-density microelectrode arrays (HD-MEAs). HD-MEAs enable the observation of extracellular potentials and neural activity of neuronal compartments at subcellular resolution. In this work, we introduce a novel framework to combine patch-clamp and HD-MEA data to construct multicompartment models. We first validate our method on a ground-truth model with known parameters and show that the use of features extracted from extracellular signals, in addition to intracellular ones, yields models enabling better fits than using intracellular features alone. We also demonstrate our procedure using experimental data by constructing cell models from in vitro cell cultures. The proposed multimodal fitting procedure has the potential to augment the modeling efforts of the computational neuroscience community and provide the field with neuronal models that are more realistic and can be better validated.},
  archive      = {J_NECO},
  author       = {Buccino, Alessio Paolo and Damart, Tanguy and Bartram, Julian and Mandge, Darshan and Xue, Xiaohan and Zbili, Mickael and Gänswein, Tobias and Jaquier, Aurélien and Emmenegger, Vishalini and Markram, Henry and Hierlemann, Andreas and Van Geit, Werner},
  doi          = {10.1162/neco_a_01672},
  journal      = {Neural Computation},
  month        = {6},
  number       = {7},
  pages        = {1286-1331},
  shortjournal = {Neural Comput.},
  title        = {A multimodal fitting approach to construct single-neuron models with patch clamp and high-density microelectrode arrays},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Desiderata for normative models of synaptic plasticity.
<em>NECO</em>, <em>36</em>(7), 1245–1285. (<a
href="https://doi.org/10.1162/neco_a_01671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Normative models of synaptic plasticity use computational rationales to arrive at predictions of behavioral and network-level adaptive phenomena. In recent years, there has been an explosion of theoretical work in this realm, but experimental confirmation remains limited. In this review, we organize work on normative plasticity models in terms of a set of desiderata that, when satisfied, are designed to ensure that a given model demonstrates a clear link between plasticity and adaptive behavior, is consistent with known biological evidence about neural plasticity and yields specific testable predictions. As a prototype, we include a detailed analysis of the REINFORCE algorithm. We also discuss how new models have begun to improve on the identified criteria and suggest avenues for further development. Overall, we provide a conceptual guide to help develop neural learning theories that are precise, powerful, and experimentally testable.},
  archive      = {J_NECO},
  author       = {Bredenberg, Colin and Savin, Cristina},
  doi          = {10.1162/neco_a_01671},
  journal      = {Neural Computation},
  month        = {6},
  number       = {7},
  pages        = {1245-1285},
  shortjournal = {Neural Comput.},
  title        = {Desiderata for normative models of synaptic plasticity},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dense sample deep learning. <em>NECO</em>, <em>36</em>(6),
1228–1244. (<a href="https://doi.org/10.1162/neco_a_01666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL), a variant of the neural network algorithms originally proposed in the 1980s (Rumelhart et al., 1986 ), has made surprising progress in artificial intelligence (AI), ranging from language translation, protein folding (Jumper et al., 2021 ), autonomous cars, and, more recently, human-like language models (chatbots). All that seemed intractable until very recently. Despite the growing use of DL networks, little is understood about the learning mechanisms and representations that make these networks effective across such a diverse range of applications. Part of the answer must be the huge scale of the architecture and, of course, the large scale of the data, since not much has changed since 1986. But the nature of deep learned representations remains largely unknown. Unfortunately, training sets with millions or billions of tokens have unknown combinatorics, and networks with millions or billions of hidden units can&#39;t easily be visualized and their mechanisms can&#39;t be easily revealed. In this letter, we explore these challenges with a large (1.24 million weights VGG) DL in a novel high-density sample task (five unique tokens with more than 500 exemplars per token), which allows us to more carefully follow the emergence of category structure and feature construction. We use various visualization methods for following the emergence of the classification and the development of the coupling of feature detectors and structures that provide a type of graphical bootstrapping. From these results, we harvest some basic observations of the learning dynamics of DL and propose a new theory of complex feature construction based on our results.},
  archive      = {J_NECO},
  author       = {Hanson, Stephen José and Yadav, Vivek and Hanson, Catherine},
  doi          = {10.1162/neco_a_01666},
  journal      = {Neural Computation},
  month        = {5},
  number       = {6},
  pages        = {1228-1244},
  shortjournal = {Neural Comput.},
  title        = {Dense sample deep learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gauge-optimal approximate learning for small data
classification. <em>NECO</em>, <em>36</em>(6), 1198–1227. (<a
href="https://doi.org/10.1162/neco_a_01664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small data learning problems are characterized by a significant discrepancy between the limited number of response variable observations and the large feature space dimension. In this setting, the common learning tools struggle to identify the features important for the classification task from those that bear no relevant information and cannot derive an appropriate learning rule that allows discriminating among different classes. As a potential solution to this problem, here we exploit the idea of reducing and rotating the feature space in a lower-dimensional gauge and propose the gauge-optimal approximate learning (GOAL) algorithm, which provides an analytically tractable joint solution to the dimension reduction, feature segmentation, and classification problems for small data learning problems. We prove that the optimal solution of the GOAL algorithm consists in piecewise-linear functions in the Euclidean space and that it can be approximated through a monotonically convergent algorithm that presents—under the assumption of a discrete segmentation of the feature space—a closed-form solution for each optimization substep and an overall linear iteration cost scaling. The GOAL algorithm has been compared to other state-of-the-art machine learning tools on both synthetic data and challenging real-world applications from climate science and bioinformatics (i.e., prediction of the El Niño Southern Oscillation and inference of epigenetically induced gene-activity networks from limited experimental data). The experimental results show that the proposed algorithm outperforms the reported best competitors for these problems in both learning performance and computational cost.},
  archive      = {J_NECO},
  author       = {Vecchi, Edoardo and Bassetti, Davide and Graziato, Fabio and Pospíšil, Lukáš and Horenko, Illia},
  doi          = {10.1162/neco_a_01664},
  journal      = {Neural Computation},
  month        = {5},
  number       = {6},
  pages        = {1198-1227},
  shortjournal = {Neural Comput.},
  title        = {Gauge-optimal approximate learning for small data classification},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Positive competitive networks for sparse reconstruction.
<em>NECO</em>, <em>36</em>(6), 1163–1197. (<a
href="https://doi.org/10.1162/neco_a_01657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and analyze a continuous-time firing-rate neural network, the positive firing-rate competitive network (PFCN), to tackle sparse reconstruction problems with non-negativity constraints. These problems, which involve approximating a given input stimulus from a dictionary using a set of sparse (active) neurons, play a key role in a wide range of domains, including, for example, neuroscience, signal processing, and machine learning. First, by leveraging the theory of proximal operators, we relate the equilibria of a family of continuous-time firing-rate neural networks to the optimal solutions of sparse reconstruction problems. Then we prove that the PFCN is a positive system and give rigorous conditions for the convergence to the equilibrium. Specifically, we show that the convergence depends only on a property of the dictionary and is linear-exponential in the sense that initially, the convergence rate is at worst linear and then, after a transient, becomes exponential. We also prove a number of technical results to assess the contractivity properties of the neural dynamics of interest. Our analysis leverages contraction theory to characterize the behavior of a family of firing-rate competitive networks for sparse reconstruction with and without non-negativity constraints. Finally, we validate the effectiveness of our approach via a numerical example.},
  archive      = {J_NECO},
  author       = {Centorrino, Veronica and Gokhale, Anand and Davydov, Alexander and Russo, Giovanni and Bullo, Francesco},
  doi          = {10.1162/neco_a_01657},
  journal      = {Neural Computation},
  month        = {5},
  number       = {6},
  pages        = {1163-1197},
  shortjournal = {Neural Comput.},
  title        = {Positive competitive networks for sparse reconstruction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The determining role of covariances in large networks of
stochastic neurons. <em>NECO</em>, <em>36</em>(6), 1121–1162. (<a
href="https://doi.org/10.1162/neco_a_01656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biological neural networks are notoriously hard to model due to their stochastic behavior and high dimensionality. We tackle this problem by constructing a dynamical model of both the expectations and covariances of the fractions of active and refractory neurons in the network’s populations. We do so by describing the evolution of the states of individual neurons with a continuous-time Markov chain, from which we formally derive a low-dimensional dynamical system. This is done by solving a moment closure problem in a way that is compatible with the nonlinearity and boundedness of the activation function. Our dynamical system captures the behavior of the high-dimensional stochastic model even in cases where the mean-field approximation fails to do so. Taking into account the second-order moments modifies the solutions that would be obtained with the mean-field approximation and can lead to the appearance or disappearance of fixed points and limit cycles. We moreover perform numerical experiments where the mean-field approximation leads to periodically oscillating solutions, while the solutions of the second-order model can be interpreted as an average taken over many realizations of the stochastic model. Altogether, our results highlight the importance of including higher moments when studying stochastic networks and deepen our understanding of correlated neuronal activity.},
  archive      = {J_NECO},
  author       = {Painchaud, Vincent and Desrosiers, Patrick and Doyon, Nicolas},
  doi          = {10.1162/neco_a_01656},
  journal      = {Neural Computation},
  month        = {5},
  number       = {6},
  pages        = {1121-1162},
  shortjournal = {Neural Comput.},
  title        = {The determining role of covariances in large networks of stochastic neurons},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Linear codes for hyperdimensional computing. <em>NECO</em>,
<em>36</em>(6), 1084–1120. (<a
href="https://doi.org/10.1162/neco_a_01665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperdimensional computing (HDC) is an emerging computational paradigm for representing compositional information as high-dimensional vectors and has a promising potential in applications ranging from machine learning to neuromorphic computing. One of the long-standing challenges in HDC is factoring a compositional representation to its constituent factors, also known as the recovery problem . In this article, we take a novel approach to solve the recovery problem and propose the use of random linear codes. These codes are subspaces over the Boolean field and are a well-studied topic in information theory with various applications in digital communication. We begin by showing that hyperdimensional encoding using random linear codes retains favorable properties of the prevalent (ordinary) random codes; hence, HD representations using the two methods have comparable information storage capabilities. We proceed to show that random linear codes offer a rich subcode structure that can be used to form key-value stores, which encapsulate the most used cases of HDC. Most important, we show that under the framework we develop, random linear codes admit simple recovery algorithms to factor (either bundled or bound) compositional representations. The former relies on constructing certain linear equation systems over the Boolean field, the solution to which reduces the search space dramatically and strictly outperforms exhaustive search in many cases. The latter employs the subspace structure of these codes to achieve provably correct factorization. Both methods are strictly faster than the state-of-the-art resonator networks, often by an order of magnitude. We implemented our techniques in Python using a benchmark software library and demonstrated promising experimental results.},
  archive      = {J_NECO},
  author       = {Raviv, Netanel},
  doi          = {10.1162/neco_a_01665},
  journal      = {Neural Computation},
  month        = {5},
  number       = {6},
  pages        = {1084-1120},
  shortjournal = {Neural Comput.},
  title        = {Linear codes for hyperdimensional computing},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How does the inner retinal network shape the ganglion cells
receptive field? A computational study. <em>NECO</em>, <em>36</em>(6),
1041–1083. (<a href="https://doi.org/10.1162/neco_a_01663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a model of basic inner retinal connectivity where bipolar and amacrine cells interconnect and both cell types project onto ganglion cells, modulating their response output to the brain visual areas. We derive an analytical formula for the spatiotemporal response of retinal ganglion cells to stimuli, taking into account the effects of amacrine cells inhibition. This analysis reveals two important functional parameters of the network: (1) the intensity of the interactions between bipolar and amacrine cells and (2) the characteristic timescale of these responses. Both parameters have a profound combined impact on the spatiotemporal features of retinal ganglion cells’ responses to light. The validity of the model is confirmed by faithfully reproducing pharmacogenetic experimental results obtained by stimulating excitatory DREADDs (Designer Receptors Exclusively Activated by Designer Drugs) expressed on ganglion cells and amacrine cells’ subclasses, thereby modifying the inner retinal network activity to visual stimuli in a complex, entangled manner. Our mathematical model allows us to explore and decipher these complex effects in a manner that would not be feasible experimentally and provides novel insights in retinal dynamics.},
  archive      = {J_NECO},
  author       = {Kartsaki, Evgenia and Hilgen, Gerrit and Sernagor, Evelyne and Cessac, Bruno},
  doi          = {10.1162/neco_a_01663},
  journal      = {Neural Computation},
  month        = {5},
  number       = {6},
  pages        = {1041-1083},
  shortjournal = {Neural Comput.},
  title        = {How does the inner retinal network shape the ganglion cells receptive field? a computational study},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous forgetting rates and greedy allocation in
slot-based memory networks promotes signal retention. <em>NECO</em>,
<em>36</em>(5), 1022–1040. (<a
href="https://doi.org/10.1162/neco_a_01655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key question in the neuroscience of memory encoding pertains to the mechanisms by which afferent stimuli are allocated within memory networks. This issue is especially pronounced in the domain of working memory, where capacity is finite. Presumably the brain must embed some “policy” by which to allocate these mnemonic resources in an online manner in order to maximally represent and store afferent information for as long as possible and without interference from subsequent stimuli. Here, we engage this question through a top-down theoretical modeling framework. We formally optimize a gating mechanism that projects afferent stimuli onto a finite number of memory slots within a recurrent network architecture. In the absence of external input, the activity in each slot attenuates over time (i.e., a process of gradual forgetting). It turns out that the optimal gating policy consists of a direct projection from sensory activity to memory slots, alongside an activity-dependent lateral inhibition. Interestingly, allocating resources myopically (greedily with respect to the current stimulus) leads to efficient utilization of slots over time. In other words, later-arriving stimuli are distributed across slots in such a way that the network state is minimally shifted and so prior signals are minimally “overwritten.” Further, networks with heterogeneity in the timescales of their forgetting rates retain stimuli better than those that are more homogeneous. Our results suggest how online, recurrent networks working on temporally localized objectives without high-level supervision can nonetheless implement efficient allocation of memory resources over time.},
  archive      = {J_NECO},
  author       = {Jones, BethAnna and Snyder, Lawrence and Ching, ShiNung},
  doi          = {10.1162/neco_a_01655},
  journal      = {Neural Computation},
  month        = {4},
  number       = {5},
  pages        = {1022-1040},
  shortjournal = {Neural Comput.},
  title        = {Heterogeneous forgetting rates and greedy allocation in slot-based memory networks promotes signal retention},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An overview of the free energy principle and related
research. <em>NECO</em>, <em>36</em>(5), 963–1021. (<a
href="https://doi.org/10.1162/neco_a_01642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The free energy principle and its corollary, the active inference framework, serve as theoretical foundations in the domain of neuroscience, explaining the genesis of intelligent behavior. This principle states that the processes of perception, learning, and decision making—within an agent—are all driven by the objective of “minimizing free energy,” evincing the following behaviors: learning and employing a generative model of the environment to interpret observations, thereby achieving perception, and selecting actions to maintain a stable preferred state and minimize the uncertainty about the environment, thereby achieving decision making. This fundamental principle can be used to explain how the brain processes perceptual information, learns about the environment, and selects actions. Two pivotal tenets are that the agent employs a generative model for perception and planning and that interaction with the world (and other agents) enhances the performance of the generative model and augments perception. With the evolution of control theory and deep learning tools, agents based on the FEP have been instantiated in various ways across different domains, guiding the design of a multitude of generative models and decision-making algorithms. This letter first introduces the basic concepts of the FEP, followed by its historical development and connections with other theories of intelligence, and then delves into the specific application of the FEP to perception and decision making, encompassing both low-dimensional simple situations and high-dimensional complex situations. It compares the FEP with model-based reinforcement learning to show that the FEP provides a better objective function. We illustrate this using numerical studies of Dreamer3 by adding expected information gain into the standard objective function. In a complementary fashion, existing reinforcement learning, and deep learning algorithms can also help implement the FEP-based agents. Finally, we discuss the various capabilities that agents need to possess in complex environments and state that the FEP can aid agents in acquiring these capabilities.},
  archive      = {J_NECO},
  author       = {Zhang, Zhengquan and Xu, Feng},
  doi          = {10.1162/neco_a_01642},
  journal      = {Neural Computation},
  month        = {4},
  number       = {5},
  pages        = {963-1021},
  shortjournal = {Neural Comput.},
  title        = {An overview of the free energy principle and related research},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Instance-specific model perturbation improves generalized
zero-shot learning. <em>NECO</em>, <em>36</em>(5), 936–962. (<a
href="https://doi.org/10.1162/neco_a_01639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) refers to the design of predictive functions on new classes (unseen classes) of data that have never been seen during training. In a more practical scenario, generalized zero-shot learning (GZSL) requires predicting both seen and unseen classes accurately. In the absence of target samples, many GZSL models may overfit training data and are inclined to predict individuals as categories that have been seen in training. To alleviate this problem, we develop a parameter-wise adversarial training process that promotes robust recognition of seen classes while designing during the test a novel model perturbation mechanism to ensure sufficient sensitivity to unseen classes. Concretely, adversarial perturbation is conducted on the model to obtain instance-specific parameters so that predictions can be biased to unseen classes in the test. Meanwhile, the robust training encourages the model robustness, leading to nearly unaffected prediction for seen classes. Moreover, perturbations in the parameter space, computed from multiple individuals simultaneously, can be used to avoid the effect of perturbations that are too extreme and ruin the predictions. Comparison results on four benchmark ZSL data sets show the effective improvement that the proposed framework made on zero-shot methods with learned metrics.},
  archive      = {J_NECO},
  author       = {Yang, Guanyu and Huang, Kaizhu and Zhang, Rui and Yang, Xi},
  doi          = {10.1162/neco_a_01639},
  journal      = {Neural Computation},
  month        = {4},
  number       = {5},
  pages        = {936-962},
  shortjournal = {Neural Comput.},
  title        = {Instance-specific model perturbation improves generalized zero-shot learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Obtaining lower query complexities through lightweight
zeroth-order proximal gradient algorithms. <em>NECO</em>,
<em>36</em>(5), 897–935. (<a
href="https://doi.org/10.1162/neco_a_01636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zeroth-order (ZO) optimization is one key technique for machine learning problems where gradient calculation is expensive or impossible. Several variance, reduced ZO proximal algorithms have been proposed to speed up ZO optimization for nonsmooth problems, and all of them opted for the coordinated ZO estimator against the random ZO estimator when approximating the true gradient, since the former is more accurate. While the random ZO estimator introduces a larger error and makes convergence analysis more challenging compared to coordinated ZO estimator, it requires only O ( 1 ) computation, which is significantly less than O ( d ) computation of the coordinated ZO estimator, with d being dimension of the problem space. To take advantage of the computationally efficient nature of the random ZO estimator, we first propose a ZO objective decrease (ZOOD) property that can incorporate two different types of errors in the upper bound of convergence rate. Next, we propose two generic reduction frameworks for ZO optimization, which can automatically derive the convergence results for convex and nonconvex problems, respectively, as long as the convergence rate for the inner solver satisfies the ZOOD property. With the application of two reduction frameworks on our proposed ZOR-ProxSVRG and ZOR-ProxSAGA, two variance-reduced ZO proximal algorithms with fully random ZO estimators, we improve the state-of-the-art function query complexities from O min d n 1 / 2 ε 2 , d ε 3 to O ˜ n + d ε 2 under d &gt; n 1 2 for nonconvex problems, and from O d ε 2 to O ˜ n log 1 ε + d ε for convex problems. Finally, we conduct experiments to verify the superiority of our proposed methods.},
  archive      = {J_NECO},
  author       = {Gu, Bin and Wei, Xiyuan and Zhang, Hualin and Chang, Yi and Huang, Heng},
  doi          = {10.1162/neco_a_01636},
  journal      = {Neural Computation},
  month        = {4},
  number       = {5},
  pages        = {897-935},
  shortjournal = {Neural Comput.},
  title        = {Obtaining lower query complexities through lightweight zeroth-order proximal gradient algorithms},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward improving the generation quality of autoregressive
slot VAEs. <em>NECO</em>, <em>36</em>(5), 858–896. (<a
href="https://doi.org/10.1162/neco_a_01635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unconditional scene inference and generation are challenging to learn jointly with a single compositional model. Despite encouraging progress on models that extract object-centric representations (“slots”) from images, unconditional generation of scenes from slots has received less attention. This is primarily because learning the multiobject relations necessary to imagine coherent scenes is difficult. We hypothesize that most existing slot-based models have a limited ability to learn object correlations. We propose two improvements that strengthen object correlation learning. The first is to condition the slots on a global, scene-level variable that captures higher-order correlations between slots. Second, we address the fundamental lack of a canonical order for objects in images by proposing to learn a consistent order to use for the autoregressive generation of scene objects. Specifically, we train an autoregressive slot prior to sequentially generate scene objects following a learned order. Ordered slot inference entails first estimating a randomly ordered set of slots using existing approaches for extracting slots from images, then aligning those slots to ordered slots generated autoregressively with the slot prior. Our experiments across three multiobject environments demonstrate clear gains in unconditional scene generation quality. Detailed ablation studies are also provided that validate the two proposed improvements.},
  archive      = {J_NECO},
  author       = {Emami, Patrick and He, Pan and Ranka, Sanjay and Rangarajan, Anand},
  doi          = {10.1162/neco_a_01635},
  journal      = {Neural Computation},
  month        = {4},
  number       = {5},
  pages        = {858-896},
  shortjournal = {Neural Comput.},
  title        = {Toward improving the generation quality of autoregressive slot VAEs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Approximating nonlinear functions with latent boundaries in
low-rank excitatory-inhibitory spiking networks. <em>NECO</em>,
<em>36</em>(5), 803–857. (<a
href="https://doi.org/10.1162/neco_a_01658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep feedforward and recurrent neural networks have become successful functional models of the brain, but they neglect obvious biological details such as spikes and Dale’s law. Here we argue that these details are crucial in order to understand how real neural circuits operate. Towards this aim, we put forth a new framework for spike-based computation in low-rank excitatory-inhibitory spiking networks. By considering populations with rank-1 connectivity, we cast each neuron’s spiking threshold as a boundary in a low-dimensional input-output space. We then show how the combined thresholds of a population of inhibitory neurons form a stable boundary in this space, and those of a population of excitatory neurons form an unstable boundary. Combining the two boundaries results in a rank-2 excitatory-inhibitory (EI) network with inhibition-stabilized dynamics at the intersection of the two boundaries. The computation of the resulting networks can be understood as the difference of two convex functions and is thereby capable of approximating arbitrary non-linear input-output mappings. We demonstrate several properties of these networks, including noise suppression and amplification, irregular activity and synaptic balance, as well as how they relate to rate network dynamics in the limit that the boundary becomes soft. Finally, while our work focuses on small networks (5-50 neurons), we discuss potential avenues for scaling up to much larger networks. Overall, our work proposes a new perspective on spiking networks that may serve as a starting point for a mechanistic understanding of biological spike-based computation.},
  archive      = {J_NECO},
  author       = {Podlaski, William F. and Machens, Christian K.},
  doi          = {10.1162/neco_a_01658},
  journal      = {Neural Computation},
  month        = {4},
  number       = {5},
  pages        = {803-857},
  shortjournal = {Neural Comput.},
  title        = {Approximating nonlinear functions with latent boundaries in low-rank excitatory-inhibitory spiking networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synaptic information storage capacity measured with
information theory. <em>NECO</em>, <em>36</em>(5), 781–802. (<a
href="https://doi.org/10.1162/neco_a_01659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variation in the strength of synapses can be quantified by measuring the anatomical properties of synapses. Quantifying precision of synaptic plasticity is fundamental to understanding information storage and retrieval in neural circuits. Synapses from the same axon onto the same dendrite have a common history of coactivation, making them ideal candidates for determining the precision of synaptic plasticity based on the similarity of their physical dimensions. Here, the precision and amount of information stored in synapse dimensions were quantified with Shannon information theory, expanding prior analysis that used signal detection theory (Bartol et al., 2015 ). The two methods were compared using dendritic spine head volumes in the middle of the stratum radiatum of hippocampal area CA1 as well-defined measures of synaptic strength. Information theory delineated the number of distinguishable synaptic strengths based on nonoverlapping bins of dendritic spine head volumes. Shannon entropy was applied to measure synaptic information storage capacity (SISC) and resulted in a lower bound of 4.1 bits and upper bound of 4.59 bits of information based on 24 distinguishable sizes. We further compared the distribution of distinguishable sizes and a uniform distribution using Kullback-Leibler divergence and discovered that there was a nearly uniform distribution of spine head volumes across the sizes, suggesting optimal use of the distinguishable values. Thus, SISC provides a new analytical measure that can be generalized to probe synaptic strengths and capacity for plasticity in different brain regions of different species and among animals raised in different conditions or during learning. How brain diseases and disorders affect the precision of synaptic plasticity can also be probed.},
  archive      = {J_NECO},
  author       = {Samavat, Mohammad and Bartol, Thomas M. and Harris, Kristen M. and Sejnowski, Terrence J.},
  doi          = {10.1162/neco_a_01659},
  journal      = {Neural Computation},
  month        = {4},
  number       = {5},
  pages        = {781-802},
  shortjournal = {Neural Comput.},
  title        = {Synaptic information storage capacity measured with information theory},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse firing in a hybrid central pattern generator for
spinal motor circuits. <em>NECO</em>, <em>36</em>(5), 759–780. (<a
href="https://doi.org/10.1162/neco_a_01660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Central pattern generators are circuits generating rhythmic movements, such as walking. The majority of existing computational models of these circuits produce antagonistic output where all neurons within a population spike with a broad burst at about the same neuronal phase with respect to network output. However, experimental recordings reveal that many neurons within these circuits fire sparsely, sometimes as rarely as once within a cycle. Here we address the sparse neuronal firing and develop a model to replicate the behavior of individual neurons within rhythm-generating populations to increase biological plausibility and facilitate new insights into the underlying mechanisms of rhythm generation. The developed network architecture is able to produce sparse firing of individual neurons, creating a novel implementation for exploring the contribution of network architecture on rhythmic output. Furthermore, the introduction of sparse firing of individual neurons within the rhythm-generating circuits is one of the factors that allows for a broad neuronal phase representation of firing at the population level. This moves the model toward recent experimental findings of evenly distributed neuronal firing across phases among individual spinal neurons. The network is tested by methodically iterating select parameters to gain an understanding of how connectivity and the interplay of excitation and inhibition influence the output. This knowledge can be applied in future studies to implement a biologically plausible rhythm-generating circuit for testing biological hypotheses.},
  archive      = {J_NECO},
  author       = {Strohmer, Beck and Najarro, Elias and Ausborn, Jessica and Berg, Rune W. and Tolu, Silvia},
  doi          = {10.1162/neco_a_01660},
  journal      = {Neural Computation},
  month        = {4},
  number       = {5},
  pages        = {759-780},
  shortjournal = {Neural Comput.},
  title        = {Sparse firing in a hybrid central pattern generator for spinal motor circuits},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Column row convolutional neural network: Reducing parameters
for efficient image processing. <em>NECO</em>, <em>36</em>(4), 744–758.
(<a href="https://doi.org/10.1162/neco_a_01653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in deep learning have achieved significant progress by increasing the number of parameters in a given model. However, this comes at the cost of computing resources, prompting researchers to explore model compression techniques that reduce the number of parameters while maintaining or even improving performance. Convolutional neural networks (CNN) have been recognized as more efficient and effective than fully connected (FC) networks. We propose a column row convolutional neural network (CRCNN) in this letter that applies 1D convolution to image data, significantly reducing the number of learning parameters and operational steps. The CRCNN uses column and row local receptive fields to perform data abstraction, concatenating each direction&#39;s feature before connecting it to an FC layer. Experimental results demonstrate that the CRCNN maintains comparable accuracy while reducing the number of parameters and compared to prior work. Moreover, the CRCNN is employed for one-class anomaly detection, demonstrating its feasibility for various applications.},
  archive      = {J_NECO},
  author       = {Im, Seongil and Jeong, Jae-Seung and Lee, Junseo and Shin, Changhwan and Cho, Jeong Ho and Ju, Hyunsu},
  doi          = {10.1162/neco_a_01653},
  journal      = {Neural Computation},
  month        = {3},
  number       = {4},
  pages        = {744-758},
  shortjournal = {Neural Comput.},
  title        = {Column row convolutional neural network: Reducing parameters for efficient image processing},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning korobov functions by correntropy and convolutional
neural networks. <em>NECO</em>, <em>36</em>(4), 718–743. (<a
href="https://doi.org/10.1162/neco_a_01650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combining information-theoretic learning with deep learning has gained significant attention in recent years, as it offers a promising approach to tackle the challenges posed by big data. However, the theoretical understanding of convolutional structures, which are vital to many structured deep learning models, remains incomplete. To partially bridge this gap, this letter aims to develop generalization analysis for deep convolutional neural network (CNN) algorithms using learning theory. Specifically, we focus on investigating robust regression using correntropy-induced loss functions derived from information-theoretic learning. Our analysis demonstrates an explicit convergence rate for deep CNN-based robust regression algorithms when the target function resides in the Korobov space. This study sheds light on the theoretical underpinnings of CNNs and provides a framework for understanding their performance and limitations.},
  archive      = {J_NECO},
  author       = {Fang, Zhiying and Mao, Tong and Fan, Jun},
  doi          = {10.1162/neco_a_01650},
  journal      = {Neural Computation},
  month        = {3},
  number       = {4},
  pages        = {718-743},
  shortjournal = {Neural Comput.},
  title        = {Learning korobov functions by correntropy and convolutional neural networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lateral connections improve generalizability of learning in
a simple neural network. <em>NECO</em>, <em>36</em>(4), 705–717. (<a
href="https://doi.org/10.1162/neco_a_01640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To navigate the world around us, neural circuits rapidly adapt to their environment learning generalizable strategies to decode information. When modeling these learning strategies, network models find the optimal solution to satisfy one task condition but fail when introduced to a novel task or even a different stimulus in the same space. In the experiments described in this letter, I investigate the role of lateral gap junctions in learning generalizable strategies to process information. Lateral gap junctions are formed by connexin proteins creating an open pore that allows for direct electrical signaling between two neurons. During neural development, the rate of gap junctions is high, and daughter cells that share similar tuning properties are more likely to be connected by these junctions. Gap junctions are highly plastic and get heavily pruned throughout development. I hypothesize that they mediate generalized learning by imprinting the weighting structure within a layer to avoid overfitting to one task condition. To test this hypothesis, I implemented a feedforward probabilistic neural network mimicking a cortical fast spiking neuron circuit that is heavily involved in movement. Many of these cells are tuned to speeds that I used as the input stimulus for the network to estimate. When training this network using a delta learning rule, both a laterally connected network and an unconnected network can estimate a single speed. However, when asking the network to estimate two or more speeds, alternated in training, an unconnected network either cannot learn speed or optimizes to a singular speed, while the laterally connected network learns the generalizable strategy and can estimate both speeds. These results suggest that lateral gap junctions between neurons enable generalized learning, which may help explain learning differences across life span.},
  archive      = {J_NECO},
  author       = {Crutcher, Garrett},
  doi          = {10.1162/neco_a_01640},
  journal      = {Neural Computation},
  month        = {3},
  number       = {4},
  pages        = {705-717},
  shortjournal = {Neural Comput.},
  title        = {Lateral connections improve generalizability of learning in a simple neural network},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Object-centric scene representations using active inference.
<em>NECO</em>, <em>36</em>(4), 677–704. (<a
href="https://doi.org/10.1162/neco_a_01637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representing a scene and its constituent objects from raw sensory data is a core ability for enabling robots to interact with their environment. In this letter, we propose a novel approach for scene understanding, leveraging an object-centric generative model that enables an agent to infer object category and pose in an allocentric reference frame using active inference, a neuro-inspired framework for action and perception. For evaluating the behavior of an active vision agent, we also propose a new benchmark where, given a target viewpoint of a particular object, the agent needs to find the best matching viewpoint given a workspace with randomly positioned objects in 3D. We demonstrate that our active inference agent is able to balance epistemic foraging and goal-driven behavior, and quantitatively outperforms both supervised and reinforcement learning baselines by more than a factor of two in terms of success rate.},
  archive      = {J_NECO},
  author       = {Van de Maele, Toon and Verbelen, Tim and Mazzaglia, Pietro and Ferraro, Stefano and Dhoedt, Bart},
  doi          = {10.1162/neco_a_01637},
  journal      = {Neural Computation},
  month        = {3},
  number       = {4},
  pages        = {677-704},
  shortjournal = {Neural Comput.},
  title        = {Object-centric scene representations using active inference},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mathematical modeling of PI3K/akt pathway in microglia.
<em>NECO</em>, <em>36</em>(4), 645–676. (<a
href="https://doi.org/10.1162/neco_a_01643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The motility of microglia involves intracellular signaling pathways that are predominantly controlled by changes in cytosolic Ca 2+ and activation of PI3K/Akt (phosphoinositide-3-kinase/protein kinase B). In this letter, we develop a novel biophysical model for cytosolic Ca 2+ activation of the PI3K/Akt pathway in microglia where Ca 2+ influx is mediated by both P2Y purinergic receptors (P2YR) and P2X purinergic receptors (P2XR). The model parameters are estimated by employing optimization techniques to fit the model to phosphorylated Akt (pAkt) experimental modeling/in vitro data. The integrated model supports the hypothesis that Ca 2+ influx via P2YR and P2XR can explain the experimentally reported biphasic transient responses in measuring pAkt levels. Our predictions reveal new quantitative insights into P2Rs on how they regulate Ca 2+ and Akt in terms of physiological interactions and transient responses. It is shown that the upregulation of P2X receptors through a repetitive application of agonist results in a continual increase in the baseline [Ca 2+ ], which causes the biphasic response to become a monophasic response which prolongs elevated levels of pAkt.},
  archive      = {J_NECO},
  author       = {Poshtkohi, Alireza and Wade, John and McDaid, Liam and Liu, Junxiu and Dallas, Mark L. and Bithell, Angela},
  doi          = {10.1162/neco_a_01643},
  journal      = {Neural Computation},
  month        = {3},
  number       = {4},
  pages        = {645-676},
  shortjournal = {Neural Comput.},
  title        = {Mathematical modeling of PI3K/Akt pathway in microglia},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probing the structure and functional properties of the
dropout-induced correlated variability in convolutional neural networks.
<em>NECO</em>, <em>36</em>(4), 621–644. (<a
href="https://doi.org/10.1162/neco_a_01652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational neuroscience studies have shown that the structure of neural variability to an unchanged stimulus affects the amount of information encoded. Some artificial deep neural networks, such as those with Monte Carlo dropout layers, also have variable responses when the input is fixed. However, the structure of the trial-by-trial neural covariance in neural networks with dropout has not been studied, and its role in decoding accuracy is unknown. We studied the above questions in a convolutional neural network model with dropout in both the training and testing phases. We found that trial-by-trial correlation between neurons (i.e., noise correlation) is positive and low dimensional. Neurons that are close in a feature map have larger noise correlation. These properties are surprisingly similar to the findings in the visual cortex. We further analyzed the alignment of the main axes of the covariance matrix. We found that different images share a common trial-by-trial noise covariance subspace, and they are aligned with the global signal covariance. This evidence that the noise covariance is aligned with signal covariance suggests that noise covariance in dropout neural networks reduces network accuracy, which we further verified directly with a trial-shuffling procedure commonly used in neuroscience. These findings highlight a previously overlooked aspect of dropout layers that can affect network performance. Such dropout networks could also potentially be a computational model of neural variability.},
  archive      = {J_NECO},
  author       = {Pan, Xu and Coen-Cagli, Ruben and Schwartz, Odelia},
  doi          = {10.1162/neco_a_01652},
  journal      = {Neural Computation},
  month        = {3},
  number       = {4},
  pages        = {621-644},
  shortjournal = {Neural Comput.},
  title        = {Probing the structure and functional properties of the dropout-induced correlated variability in convolutional neural networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Frequency propagation: Multimechanism learning in nonlinear
physical networks. <em>NECO</em>, <em>36</em>(4), 596–620. (<a
href="https://doi.org/10.1162/neco_a_01648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce frequency propagation, a learning algorithm for nonlinear physical networks. In a resistive electrical circuit with variable resistors, an activation current is applied at a set of input nodes at one frequency and an error current is applied at a set of output nodes at another frequency. The voltage response of the circuit to these boundary currents is the superposition of an activation signal and an error signal whose coefficients can be read in different frequencies of the frequency domain. Each conductance is updated proportionally to the product of the two coefficients. The learning rule is local and proved to perform gradient descent on a loss function. We argue that frequency propagation is an instance of a multimechanism learning strategy for physical networks, be it resistive, elastic, or flow networks. Multimechanism learning strategies incorporate at least two physical quantities, potentially governed by independent physical mechanisms, to act as activation and error signals in the training process. Locally available information about these two signals is then used to update the trainable parameters to perform gradient descent. We demonstrate how earlier work implementing learning via chemical signaling in flow networks (Anisetti, Scellier, et al., 2023 ) also falls under the rubric of multimechanism learning.},
  archive      = {J_NECO},
  author       = {Anisetti, Vidyesh Rao and Kandala, Ananth and Scellier, Benjamin and Schwarz, J. M.},
  doi          = {10.1162/neco_a_01648},
  journal      = {Neural Computation},
  month        = {3},
  number       = {4},
  pages        = {596-620},
  shortjournal = {Neural Comput.},
  title        = {Frequency propagation: Multimechanism learning in nonlinear physical networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vector symbolic finite state machines in attractor neural
networks. <em>NECO</em>, <em>36</em>(4), 549–595. (<a
href="https://doi.org/10.1162/neco_a_01638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hopfield attractor networks are robust distributed models of human memory, but they lack a general mechanism for effecting state-dependent attractor transitions in response to input. We propose construction rules such that an attractor network may implement an arbitrary finite state machine (FSM), where states and stimuli are represented by high-dimensional random vectors and all state transitions are enacted by the attractor network’s dynamics. Numerical simulations show the capacity of the model, in terms of the maximum size of implementable FSM, to be linear in the size of the attractor network for dense bipolar state vectors and approximately quadratic for sparse binary state vectors. We show that the model is robust to imprecise and noisy weights, and so a prime candidate for implementation with high-density but unreliable devices. By endowing attractor networks with the ability to emulate arbitrary FSMs, we propose a plausible path by which FSMs could exist as a distributed computational primitive in biological neural networks.},
  archive      = {J_NECO},
  author       = {Cotteret, Madison and Greatorex, Hugh and Ziegler, Martin and Chicca, Elisabetta},
  doi          = {10.1162/neco_a_01638},
  journal      = {Neural Computation},
  month        = {3},
  number       = {4},
  pages        = {549-595},
  shortjournal = {Neural Comput.},
  title        = {Vector symbolic finite state machines in attractor neural networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CA3 circuit model compressing sequential information in
theta oscillation and replay. <em>NECO</em>, <em>36</em>(4), 501–548.
(<a href="https://doi.org/10.1162/neco_a_01641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hippocampus plays a critical role in the compression and retrieval of sequential information. During wakefulness, it achieves this through theta phase precession and theta sequences. Subsequently, during periods of sleep or rest, the compressed information reactivates through sharp-wave ripple events, manifesting as memory replay. However, how these sequential neuronal activities are generated and how they store information about the external environment remain unknown. We developed a hippocampal cornu ammonis 3 (CA3) computational model based on anatomical and electrophysiological evidence from the biological CA3 circuit to address these questions. The model comprises theta rhythm inhibition, place input, and CA3-CA3 plastic recurrent connection. The model can compress the sequence of the external inputs, reproduce theta phase precession and replay, learn additional sequences, and reorganize previously learned sequences. A gradual increase in synaptic inputs, controlled by interactions between theta-paced inhibition and place inputs, explained the mechanism of sequence acquisition. This model highlights the crucial role of plasticity in the CA3 recurrent connection and theta oscillational dynamics and hypothesizes how the CA3 circuit acquires, compresses, and replays sequential information.},
  archive      = {J_NECO},
  author       = {Kuroki, Satoshi and Mizuseki, Kenji},
  doi          = {10.1162/neco_a_01641},
  journal      = {Neural Computation},
  month        = {3},
  number       = {4},
  pages        = {501-548},
  shortjournal = {Neural Comput.},
  title        = {CA3 circuit model compressing sequential information in theta oscillation and replay},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Errata to “a tutorial on the spectral theory of markov
chains” by eddie seabrook and laurenz wiskott (neural computation,
november 2023, vol. 35, no. 11, pp. 1713–1796, https:
//doi.org/10.1162/neco_a_01611). <em>NECO</em>, <em>36</em>(3), 499–500.
(<a href="https://doi.org/10.1162/neco_e_01662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {1) Due to a coding error in the original manuscript, the equation label A.11 appears on page 1786 without a corresponding equation. There is no equation A.11 so readers should disregard the equation label.2) Due to a technical error during production, a number of equations in the published version contained an error. The affected equations contained references to other equations that did not reflect the numbering used in the final published article. The table below reproduces the erroneous equation followed by the corrected version.The original article at https://doi.org/10.1162/neco_a_01611 has now been corrected.},
  archive      = {J_NECO},
  doi          = {10.1162/neco_e_01662},
  journal      = {Neural Computation},
  month        = {2},
  number       = {3},
  pages        = {499-500},
  shortjournal = {Neural Comput.},
  title        = {Errata to “A tutorial on the spectral theory of markov chains” by eddie seabrook and laurenz wiskott (Neural computation, november 2023, vol. 35, no. 11, pp. 1713–1796, https: //doi.org/10.1162/neco_a_01611)},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning only on boundaries: A physics-informed neural
operator for solving parametric partial differential equations in
complex geometries. <em>NECO</em>, <em>36</em>(3), 475–498. (<a
href="https://doi.org/10.1162/neco_a_01647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning surrogates and neural operators have shown promise in solving partial differential equations  (PDEs). However, they often require a large amount of training data and are limited to bounded domains. In this work, we present a novel physics-informed neural operator method to solve parameterized boundary value problems without labeled data. By reformulating the PDEs into boundary integral equations (BIEs), we can train the operator network solely on the boundary of the domain. This approach reduces the number of required sample points from O ( N d ) to O ( N d - 1 ) ⁠ , where d is the domain’s dimension, leading to a significant acceleration of the training process. Additionally, our method can handle unbounded problems, which are unattainable for existing physics-informed neural networks (PINNs) and neural operators. Our numerical experiments show the effectiveness of parameterized complex geometries and unbounded problems.},
  archive      = {J_NECO},
  author       = {Fang, Zhiwei and Wang, Sifan and Perdikaris, Paris},
  doi          = {10.1162/neco_a_01647},
  journal      = {Neural Computation},
  month        = {2},
  number       = {3},
  pages        = {475-498},
  shortjournal = {Neural Comput.},
  title        = {Learning only on boundaries: A physics-informed neural operator for solving parametric partial differential equations in complex geometries},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active learning for discrete latent variable models.
<em>NECO</em>, <em>36</em>(3), 437–474. (<a
href="https://doi.org/10.1162/neco_a_01646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning seeks to reduce the amount of data required to fit the parameters of a model, thus forming an important class of techniques in modern machine learning. However, past work on active learning has largely overlooked latent variable models, which play a vital role in neuroscience, psychology, and a variety of other engineering and scientific disciplines. Here we address this gap by proposing a novel framework for maximum-mutual-information input selection for discrete latent variable regression models. We first apply our method to a class of models known as mixtures of linear regressions (MLR). While it is well known that active learning confers no advantage for linear-gaussian regression models, we use Fisher information to show analytically that active learning can nevertheless achieve large gains for mixtures of such models, and we validate this improvement using both simulations and real-world data. We then consider a powerful class of temporally structured latent variable models given by a hidden Markov model (HMM) with generalized linear model (GLM) observations, which has recently been used to identify discrete states from animal decision-making data. We show that our method substantially reduces the amount of data needed to fit GLM-HMMs and outperforms a variety of approximate methods based on variational and amortized inference. Infomax learning for latent variable models thus offers a powerful approach for characterizing temporally structured latent states, with a wide variety of applications in neuroscience and beyond.},
  archive      = {J_NECO},
  author       = {Jha, Aditi and Ashwood, Zoe C. and Pillow, Jonathan W.},
  doi          = {10.1162/neco_a_01646},
  journal      = {Neural Computation},
  month        = {2},
  number       = {3},
  pages        = {437-474},
  shortjournal = {Neural Comput.},
  title        = {Active learning for discrete latent variable models},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evidence for multiscale multiplexed representation of visual
features in EEG. <em>NECO</em>, <em>36</em>(3), 412–436. (<a
href="https://doi.org/10.1162/neco_a_01649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distinct neural processes such as sensory and memory processes are often encoded over distinct timescales of neural activations. Animal studies have shown that this multiscale coding strategy is also implemented for individual components of a single process, such as individual features of a multifeature stimulus in sensory coding. However, the generalizability of this encoding strategy to the human brain has remained unclear. We asked if individual features of visual stimuli were encoded over distinct timescales. We applied a multiscale time-resolved decoding method to electroencephalography (EEG) collected from human subjects presented with grating visual stimuli to estimate the timescale of individual stimulus features. We observed that the orientation and color of the stimuli were encoded in shorter timescales, whereas spatial frequency and the contrast of the same stimuli were encoded in longer timescales. The stimulus features appeared in temporally overlapping windows along the trial supporting a multiplexed coding strategy. These results provide evidence for a multiplexed, multiscale coding strategy in the human visual system.},
  archive      = {J_NECO},
  author       = {Karimi-Rouzbahani, Hamid},
  doi          = {10.1162/neco_a_01649},
  journal      = {Neural Computation},
  month        = {2},
  number       = {3},
  pages        = {412-436},
  shortjournal = {Neural Comput.},
  title        = {Evidence for multiscale multiplexed representation of visual features in EEG},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advantages of persistent cohomology in estimating animal
location from grid cell population activity. <em>NECO</em>,
<em>36</em>(3), 385–411. (<a
href="https://doi.org/10.1162/neco_a_01645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many cognitive functions are represented as cell assemblies. In the case of spatial navigation, the population activity of place cells in the hippocampus and grid cells in the entorhinal cortex represents self-location in the environment. The brain cannot directly observe self-location information in the environment. Instead, it relies on sensory information and memory to estimate self-location. Therefore, estimating low-dimensional dynamics, such as the movement trajectory of an animal exploring its environment, from only the high-dimensional neural activity is important in deciphering the information represented in the brain. Most previous studies have estimated the low-dimensional dynamics (i.e., latent variables) behind neural activity by unsupervised learning with Bayesian population decoding using artificial neural networks or gaussian processes. Recently, persistent cohomology has been used to estimate latent variables from the phase information (i.e., circular coordinates) of manifolds created by neural activity. However, the advantages of persistent cohomology over Bayesian population decoding are not well understood. We compared persistent cohomology and Bayesian population decoding in estimating the animal location from simulated and actual grid cell population activity. We found that persistent cohomology can estimate the animal location with fewer neurons than Bayesian population decoding and robustly estimate the animal location from actual noisy data.},
  archive      = {J_NECO},
  author       = {Kawahara, Daisuke and Fujisawa, Shigeyoshi},
  doi          = {10.1162/neco_a_01645},
  journal      = {Neural Computation},
  month        = {2},
  number       = {3},
  pages        = {385-411},
  shortjournal = {Neural Comput.},
  title        = {Advantages of persistent cohomology in estimating animal location from grid cell population activity},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantifying and maximizing the information flux in recurrent
neural networks. <em>NECO</em>, <em>36</em>(3), 351–384. (<a
href="https://doi.org/10.1162/neco_a_01651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Free-running recurrent neural networks (RNNs), especially probabilistic models, generate an ongoing information flux that can be quantified with the mutual information I [ x → ( t ) , x → ( t + 1 ) ] between subsequent system states x → ⁠ . Although previous studies have shown that I depends on the statistics of the network’s connection weights, it is unclear how to maximize I systematically and how to quantify the flux in large systems where computing the mutual information becomes intractable. Here, we address these questions using Boltzmann machines as model systems. We find that in networks with moderately strong connections, the mutual information I is approximately a monotonic transformation of the root-mean-square averaged Pearson correlations between neuron pairs, a quantity that can be efficiently computed even in large systems. Furthermore, evolutionary maximization of I [ x → ( t ) , x → ( t + 1 ) ] reveals a general design principle for the weight matrices enabling the systematic construction of systems with a high spontaneous information flux. Finally, we simultaneously maximize information flux and the mean period length of cyclic attractors in the state-space of these dynamical networks. Our results are potentially useful for the construction of RNNs that serve as short-time memories or pattern generators.},
  archive      = {J_NECO},
  author       = {Metzner, Claus and Yamakou, Marius E. and Voelkl, Dennis and Schilling, Achim and Krauss, Patrick},
  doi          = {10.1162/neco_a_01651},
  journal      = {Neural Computation},
  month        = {2},
  number       = {3},
  pages        = {351-384},
  shortjournal = {Neural Comput.},
  title        = {Quantifying and maximizing the information flux in recurrent neural networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Q&amp;a label learning. <em>NECO</em>, <em>36</em>(2),
312–349. (<a href="https://doi.org/10.1162/neco_a_01633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assigning labels to instances is crucial for supervised machine learning. In this letter, we propose a novel annotation method, Q&amp;A labeling, which involves a question generator that asks questions about the labels of the instances to be assigned and an annotator that answers the questions and assigns the corresponding labels to the instances. We derived a generative model of labels assigned according to two Q&amp;A labeling procedures that differ in the way questions are asked and answered. We showed that in both procedures, the derived model is partially consistent with that assumed in previous studies. The main distinction of this study from previous ones lies in the fact that the label generative model was not assumed but, rather, derived based on the definition of a specific annotation method, Q&amp;A labeling. We also derived a loss function to evaluate the classification risk of ordinary supervised machine learning using instances assigned Q&amp;A labels and evaluated the upper bound of the classification error. The results indicate statistical consistency in learning with Q&amp;A labels.},
  archive      = {J_NECO},
  author       = {Kawamoto, Kota and Uchida, Masato},
  doi          = {10.1162/neco_a_01633},
  journal      = {Neural Computation},
  month        = {1},
  number       = {2},
  pages        = {312-349},
  shortjournal = {Neural Comput.},
  title        = {Q&amp;A label learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cooperativity, information gain, and energy cost during
early LTP in dendritic spines. <em>NECO</em>, <em>36</em>(2), 271–311.
(<a href="https://doi.org/10.1162/neco_a_01632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate a mutual relationship between information and energy during the early phase of LTP induction and maintenance in a large-scale system of mutually coupled dendritic spines, with discrete internal states and probabilistic dynamics, within the framework of nonequilibrium stochastic thermodynamics. In order to analyze this computationally intractable stochastic multidimensional system, we introduce a pair approximation, which allows us to reduce the spine dynamics into a lower-dimensional manageable system of closed equations. We found that the rates of information gain and energy attain their maximal values during an initial period of LTP (i.e., during stimulation), and after that, they recover to their baseline low values, as opposed to a memory trace that lasts much longer. This suggests that the learning phase is much more energy demanding than the memory phase. We show that positive correlations between neighboring spines increase both a duration of memory trace and energy cost during LTP, but the memory time per invested energy increases dramatically for very strong, positive synaptic cooperativity, suggesting a beneficial role of synaptic clustering on memory duration. In contrast, information gain after LTP is the largest for negative correlations, and energy efficiency of that information generally declines with increasing synaptic cooperativity. We also find that dendritic spines can use sparse representations for encoding long-term information, as both energetic and structural efficiencies of retained information and its lifetime exhibit maxima for low fractions of stimulated synapses during LTP. Moreover, we find that such efficiencies drop significantly with increasing the number of spines. In general, our stochastic thermodynamics approach provides a unifying framework for studying, from first principles, information encoding, and its energy cost during learning and memory in stochastic systems of interacting synapses.},
  archive      = {J_NECO},
  author       = {Karbowski, Jan and Urban, Paulina},
  doi          = {10.1162/neco_a_01632},
  journal      = {Neural Computation},
  month        = {1},
  number       = {2},
  pages        = {271-311},
  shortjournal = {Neural Comput.},
  title        = {Cooperativity, information gain, and energy cost during early LTP in dendritic spines},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Emergence of universal computations through neural manifold
dynamics. <em>NECO</em>, <em>36</em>(2), 227–270. (<a
href="https://doi.org/10.1162/neco_a_01631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is growing evidence that many forms of neural computation may be implemented by low-dimensional dynamics unfolding at the population scale. However, neither the connectivity structure nor the general capabilities of these embedded dynamical processes are currently understood. In this work, the two most common formalisms of firing-rate models are evaluated using tools from analysis, topology, and nonlinear dynamics in order to provide plausible explanations for these problems. It is shown that low-rank structured connectivities predict the formation of invariant and globally attracting manifolds in all these models. Regarding the dynamics arising in these manifolds, it is proved they are topologically equivalent across the considered formalisms. This letter also shows that under the low-rank hypothesis, the flows emerging in neural manifolds, including input-driven systems, are universal, which broadens previous findings. It explores how low-dimensional orbits can bear the production of continuous sets of muscular trajectories, the implementation of central pattern generators, and the storage of memory states. These dynamics can robustly simulate any Turing machine over arbitrary bounded memory strings, virtually endowing rate models with the power of universal computation. In addition, the letter shows how the low-rank hypothesis predicts the parsimonious correlation structure observed in cortical activity. Finally, it discusses how this theory could provide a useful tool from which to study neuropsychological phenomena using mathematical methods.},
  archive      = {J_NECO},
  author       = {Gort, Joan},
  doi          = {10.1162/neco_a_01631},
  journal      = {Neural Computation},
  month        = {1},
  number       = {2},
  pages        = {227-270},
  shortjournal = {Neural Comput.},
  title        = {Emergence of universal computations through neural manifold dynamics},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient decoding of large-scale neural population
responses with gaussian-process multiclass regression. <em>NECO</em>,
<em>36</em>(2), 175–226. (<a
href="https://doi.org/10.1162/neco_a_01630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural decoding methods provide a powerful tool for quantifying the information content of neural population codes and the limits imposed by correlations in neural activity. However, standard decoding methods are prone to overfitting and scale poorly to high-dimensional settings. Here, we introduce a novel decoding method to overcome these limitations. Our approach, the gaussian process multiclass decoder (GPMD), is well suited to decoding a continuous low-dimensional variable from high-dimensional population activity and provides a platform for assessing the importance of correlations in neural population codes. The GPMD is a multinomial logistic regression model with a gaussian process prior over the decoding weights. The prior includes hyperparameters that govern the smoothness of each neuron’s decoding weights, allowing automatic pruning of uninformative neurons during inference. We provide a variational inference method for fitting the GPMD to data, which scales to hundreds or thousands of neurons and performs well even in data sets with more neurons than trials. We apply the GPMD to recordings from primary visual cortex in three species: monkey, ferret, and mouse. Our decoder achieves state-of-the-art accuracy on all three data sets and substantially outperforms independent Bayesian decoding, showing that knowledge of the correlation structure is essential for optimal decoding in all three species.},
  archive      = {J_NECO},
  author       = {Greenidge, C. Daniel and Scholl, Benjamin and Yates, Jacob L. and Pillow, Jonathan W.},
  doi          = {10.1162/neco_a_01630},
  journal      = {Neural Computation},
  month        = {1},
  number       = {2},
  pages        = {175-226},
  shortjournal = {Neural Comput.},
  title        = {Efficient decoding of large-scale neural population responses with gaussian-process multiclass regression},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The limiting dynamics of SGD: Modified loss, phase-space
oscillations, and anomalous diffusion. <em>NECO</em>, <em>36</em>(1),
151–174. (<a href="https://doi.org/10.1162/neco_a_01626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we explore the limiting dynamics of deep neural networks trained with stochastic gradient descent (SGD). As observed previously, long after performance has converged, networks continue to move through parameter space by a process of anomalous diffusion in which distance traveled grows as a power law in the number of gradient updates with a nontrivial exponent. We reveal an intricate interaction among the hyperparameters of optimization, the structure in the gradient noise, and the Hessian matrix at the end of training that explains this anomalous diffusion. To build this understanding, we first derive a continuous-time model for SGD with finite learning rates and batch sizes as an underdamped Langevin equation. We study this equation in the setting of linear regression, where we can derive exact, analytic expressions for the phase-space dynamics of the parameters and their instantaneous velocities from initialization to stationarity. Using the Fokker-Planck equation, we show that the key ingredient driving these dynamics is not the original training loss but rather the combination of a modified loss, which implicitly regularizes the velocity, and probability currents that cause oscillations in phase space. We identify qualitative and quantitative predictions of this theory in the dynamics of a ResNet-18 model trained on ImageNet. Through the lens of statistical physics, we uncover a mechanistic origin for the anomalous limiting dynamics of deep neural networks trained with SGD. Understanding the limiting dynamics of SGD, and its dependence on various important hyperparameters like batch size, learning rate, and momentum, can serve as a basis for future work that can turn these insights into algorithmic gains.},
  archive      = {J_NECO},
  author       = {Kunin, Daniel and Sagastuy-Brena, Javier and Gillespie, Lauren and Margalit, Eshed and Tanaka, Hidenori and Ganguli, Surya and Yamins, Daniel L. K.},
  doi          = {10.1162/neco_a_01626},
  journal      = {Neural Computation},
  month        = {1},
  number       = {1},
  pages        = {151-174},
  shortjournal = {Neural Comput.},
  title        = {The limiting dynamics of SGD: Modified loss, phase-space oscillations, and anomalous diffusion},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Performance evaluation of matrix factorization for fMRI
data. <em>NECO</em>, <em>36</em>(1), 128–150. (<a
href="https://doi.org/10.1162/neco_a_01628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A hypothesis in the study of the brain is that sparse coding is realized in information representation of external stimuli, which has been experimentally confirmed for visual stimulus recently. However, unlike the specific functional region in the brain, sparse coding in information processing in the whole brain has not been clarified sufficiently. In this study, we investigate the validity of sparse coding in the whole human brain by applying various matrix factorization methods to functional magnetic resonance imaging data of neural activities in the brain. The result suggests the sparse coding hypothesis in information representation in the whole human brain, because extracted features from the sparse matrix factorization (MF) method, sparse principal component analysis (SparsePCA), or method of optimal directions (MOD) under a high sparsity setting or an approximate sparse MF method, fast independent component analysis (FastICA), can classify external visual stimuli more accurately than the nonsparse MF method or sparse MF method under a low sparsity setting.},
  archive      = {J_NECO},
  author       = {Endo, Yusuke and Takeda, Koujin},
  doi          = {10.1162/neco_a_01628},
  journal      = {Neural Computation},
  month        = {1},
  number       = {1},
  pages        = {128-150},
  shortjournal = {Neural Comput.},
  title        = {Performance evaluation of matrix factorization for fMRI data},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cocaine use prediction with tensor-based machine learning on
multimodal MRI connectome data. <em>NECO</em>, <em>36</em>(1), 107–127.
(<a href="https://doi.org/10.1162/neco_a_01623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This letter considers the use of machine learning algorithms for predicting cocaine use based on magnetic resonance imaging (MRI) connectomic data. The study used functional MRI (fMRI) and diffusion MRI (dMRI) data collected from 275 individuals, which was then parcellated into 246 regions of interest (ROIs) using the Brainnetome atlas. After data preprocessing, the data sets were transformed into tensor form. We developed a tensor-based unsupervised machine learning algorithm to reduce the size of the data tensor from 275 (individuals) × 2 (fMRI and dMRI) × 246 (ROIs) × 246 (ROIs) to 275 (individuals) × 2 (fMRI and dMRI) × 6 (clusters) × 6 (clusters). This was achieved by applying the high-order Lloyd algorithm to group the ROI data into six clusters. Features were extracted from the reduced tensor and combined with demographic features (age, gender, race, and HIV status). The resulting data set was used to train a Catboost model using subsampling and nested cross-validation techniques, which achieved a prediction accuracy of 0.857 for identifying cocaine users. The model was also compared with other models, and the feature importance of the model was presented. Overall, this study highlights the potential for using tensor-based machine learning algorithms to predict cocaine use based on MRI connectomic data and presents a promising approach for identifying individuals at risk of substance abuse.},
  archive      = {J_NECO},
  author       = {Zhang, Anru R. and Bell, Ryan P. and An, Chen and Tang, Runshi and Hall, Shana A. and Chan, Cliburn and Al-Khalil, Kareem and Meade, Christina S.},
  doi          = {10.1162/neco_a_01623},
  journal      = {Neural Computation},
  month        = {1},
  number       = {1},
  pages        = {107-127},
  shortjournal = {Neural Comput.},
  title        = {Cocaine use prediction with tensor-based machine learning on multimodal MRI connectome data},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synchronization and clustering in complex quadratic
networks. <em>NECO</em>, <em>36</em>(1), 75–106. (<a
href="https://doi.org/10.1162/neco_a_01624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synchronization and clustering are well studied in the context of networks of oscillators, such as neuronal networks. However, this relationship is notoriously difficult to approach mathematically in natural, complex networks. Here, we aim to understand it in a canonical framework, using complex quadratic node dynamics, coupled in networks that we call complex quadratic networks (CQNs). We review previously defined extensions of the Mandelbrot and Julia sets for networks, focusing on the behavior of the node-wise projections of these sets and on describing the phenomena of node clustering and synchronization. One aspect of our work consists of exploring ties between a network’s connectivity and its ensemble dynamics by identifying mechanisms that lead to clusters of nodes exhibiting identical or different Mandelbrot sets. Based on our preliminary analytical results (obtained primarily in two-dimensional networks), we propose that clustering is strongly determined by the network connectivity patterns, with the geometry of these clusters further controlled by the connection weights. Here, we first explore this relationship further, using examples of synthetic networks, increasing in size (from 3, to 5, to 20 nodes). We then illustrate the potential practical implications of synchronization in an existing set of whole brain, tractography-based networks obtained from 197 human subjects using diffusion tensor imaging. Understanding the similarities to how these concepts apply to CQNs contributes to our understanding of universal principles in dynamic networks and may help extend theoretical results to natural, complex systems.},
  archive      = {J_NECO},
  author       = {Rǎdulescu, Anca and Evans, Danae and Augustin, Amani-Dasia and Cooper, Anthony and Nakuci, Johan and Muldoon, Sarah},
  doi          = {10.1162/neco_a_01624},
  journal      = {Neural Computation},
  month        = {1},
  number       = {1},
  pages        = {75-106},
  shortjournal = {Neural Comput.},
  title        = {Synchronization and clustering in complex quadratic networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling the role of contour integration in visual
inference. <em>NECO</em>, <em>36</em>(1), 33–74. (<a
href="https://doi.org/10.1162/neco_a_01625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under difficult viewing conditions, the brain’s visual system uses a variety of recurrent modulatory mechanisms to augment feedforward processing. One resulting phenomenon is contour integration, which occurs in the primary visual (V1) cortex and strengthens neural responses to edges if they belong to a larger smooth contour. Computational models have contributed to an understanding of the circuit mechanisms of contour integration, but less is known about its role in visual perception. To address this gap, we embedded a biologically grounded model of contour integration in a task-driven artificial neural network and trained it using a gradient-descent variant. We used this model to explore how brain-like contour integration may be optimized for high-level visual objectives as well as its potential roles in perception. When the model was trained to detect contours in a background of random edges, a task commonly used to examine contour integration in the brain, it closely mirrored the brain in terms of behavior, neural responses, and lateral connection patterns. When trained on natural images, the model enhanced weaker contours and distinguished whether two points lay on the same versus different contours. The model learned robust features that generalized well to out-of-training-distribution stimuli. Surprisingly, and in contrast with the synthetic task, a parameter-matched control network without recurrence performed the same as or better than the model on the natural-image tasks. Thus, a contour integration mechanism is not essential to perform these more naturalistic contour-related tasks. Finally, the best performance in all tasks was achieved by a modified contour integration model that did not distinguish between excitatory and inhibitory neurons.},
  archive      = {J_NECO},
  author       = {Khan, Salman and Wong, Alexander and Tripp, Bryan},
  doi          = {10.1162/neco_a_01625},
  journal      = {Neural Computation},
  month        = {1},
  number       = {1},
  pages        = {33-74},
  shortjournal = {Neural Comput.},
  title        = {Modeling the role of contour integration in visual inference},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active predictive coding: A unifying neural model for active
perception, compositional learning, and hierarchical planning.
<em>NECO</em>, <em>36</em>(1), 1–32. (<a
href="https://doi.org/10.1162/neco_a_01627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is growing interest in predictive coding as a model of how the brain learns through predictions and prediction errors. Predictive coding models have traditionally focused on sensory coding and perception. Here we introduce active predictive coding (APC) as a unifying model for perception, action, and cognition. The APC model addresses important open problems in cognitive science and AI, including (1) how we learn compositional representations (e.g., part-whole hierarchies for equivariant vision) and (2) how we solve large-scale planning problems, which are hard for traditional reinforcement learning, by composing complex state dynamics and abstract actions from simpler dynamics and primitive actions. By using hypernetworks, self-supervised learning, and reinforcement learning, APC learns hierarchical world models by combining task-invariant state transition networks and task-dependent policy networks at multiple abstraction levels. We illustrate the applicability of the APC model to active visual perception and hierarchical planning. Our results represent, to our knowledge, the first proof-of-concept demonstration of a unified approach to addressing the part-whole learning problem in vision, the nested reference frames learning problem in cognition, and the integrated state-action hierarchy learning problem in reinforcement learning.},
  archive      = {J_NECO},
  author       = {Rao, Rajesh P. N. and Gklezakos, Dimitrios C. and Sathish, Vishwas},
  doi          = {10.1162/neco_a_01627},
  journal      = {Neural Computation},
  month        = {1},
  number       = {1},
  pages        = {1-32},
  shortjournal = {Neural Comput.},
  title        = {Active predictive coding: A unifying neural model for active perception, compositional learning, and hierarchical planning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
