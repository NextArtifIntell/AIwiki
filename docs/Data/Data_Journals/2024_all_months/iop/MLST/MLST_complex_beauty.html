<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MLST_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mlst---317">MLST - 317</h2>
<ul>
<li><details>
<summary>
(2024). Machine learning-based compression of quantum many body
physics: PCA and autoencoder representation of the vertex function.
<em>MLST</em>, <em>5</em>(4), 045076. (<a
href="https://doi.org/10.1088/2632-2153/ad9f20">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Theoretical approaches to quantum many-body physics require developing compact representations of the complexity of generic quantum states. This paper explores an interpretable data-driven approach utilizing principal component analysis (PCA) and autoencoder neural networks to compress the two-particle vertex, a key element in Feynman diagram approaches. We show that the linear PCA offers more physical insight and better out-of-distribution generalization than the nominally more expressive autoencoders. Even with ∼10–20 principal components, we find excellent reconstruction across the phase diagram suggesting the existence of heretofore unrealized structures in the diagrammatic theory. We show that the principal components needed to describe the ferromagnetic state are not contained in the low rank description of the Fermi liquid (FL) state, unlike those for antiferromagnetic and superconducting states, suggesting that the latter two states emerge from pre-existing fluctuations in the FL while ferromagnetism is driven by a different process.},
  archive      = {J_MLST},
  author       = {Jiawei Zang and Matija Medvidović and Dominik Kiese and Domenico Di Sante and Anirvan M Sengupta and Andrew J Millis},
  doi          = {10.1088/2632-2153/ad9f20},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045076},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning-based compression of quantum many body physics: PCA and autoencoder representation of the vertex function},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). QR-DeepONet: Resolve abnormal convergence issue in deep
operator network. <em>MLST</em>, <em>5</em>(4), 045075. (<a
href="https://doi.org/10.1088/2632-2153/ada0a5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep operator network (DeepONet) has been proven to be highly successful in operator learning tasks. Theoretical analysis indicates that the generation error of DeepONet should decrease as the basis dimension increases, thus providing a systematic way to reduce its generalization errors (GEs) by varying the network hyperparameters. However, in practice, we found that, depending on the problem being solved and the activation function used, the GEs fluctuate unpredictably, contrary to theoretical expectations. Upon analyzing the output matrix of the trunk net, we determined that this behavior stems from the learned basis functions being highly linearly dependent, which limits the expressivity of the vanilla DeepONet. To address these limitations, we propose QR decomposition enhanced DeepONet (QR-DeepONet), an enhanced version of DeepONet using QR decomposition. These modifications ensured that the learned basis functions were linearly independent and orthogonal to each other. The numerical results demonstrate that the GEs of QR-DeepONet follow theoretical predictions that decrease monotonically as the basis dimension increases and outperform vanilla DeepONet. Consequently, the proposed method successfully fills the gap between the theory and practice.},
  archive      = {J_MLST},
  author       = {Jie Zhao and Biwei Xie and Xingquan Li},
  doi          = {10.1088/2632-2153/ada0a5},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045075},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {QR-DeepONet: Resolve abnormal convergence issue in deep operator network},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine-learning-assisted dual harmonic generation FROG for
enhanced ultrafast pulse recovery. <em>MLST</em>, <em>5</em>(4), 045074.
(<a href="https://doi.org/10.1088/2632-2153/ad9f21">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultrafast pulse characterisation is crucial for studying processes that occur at femtosecond timescales and below. Because of this, various methods have been developed to recover a pulse&#39;s electric field profile at these durations, with the frequency-resolved optical gating (FROG) technique being the most common. However, this approach is computationally expensive and suffers from limitations in terms of robustness and reliability. In this regard, recent publications have demonstrated that applying machine learning towards ultrafast pulse recovery can alleviate these issues, providing more accurate retrievals. Inspired by these works, we propose an encoder–decoder scheme for a FROG system which exploits dual harmonic generation in low-index thin films. Specifically, we demonstrate enhanced reliability and accuracy of ultrafast pulse recovery when compared to machine learning approaches using second or third harmonic signals independently. As the amount of information used to train each neural network is kept constant, this study demonstrates and benchmarks the technological advantages of contextual information analysis involving multiple nonlinear processes.},
  archive      = {J_MLST},
  author       = {Wallace Jaffray and Ziheng Guo and Andrea Di Falco and Marcello Ferrera},
  doi          = {10.1088/2632-2153/ad9f21},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045074},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine-learning-assisted dual harmonic generation FROG for enhanced ultrafast pulse recovery},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Implicit quantile networks for emulation in jet physics.
<em>MLST</em>, <em>5</em>(4), 045073. (<a
href="https://doi.org/10.1088/2632-2153/ad9884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to model and sample from conditional densities is important in many physics applications. Implicit quantile networks (IQN) have been successfully applied to this task in domains outside physics. In this work, we illustrate the potential of IQNs as components of emulators using the simulation of jets as an example. Specifically, we use an IQN to map jets described by their 4-momenta at the generation level to jets at the event reconstruction level. The conditional densities emulated by our model closely match those generated by Delphes , while also enabling faster jet simulation.},
  archive      = {J_MLST},
  author       = {Braden Kronheim and Ali Al Kadhim and Michelle P Kuchera and Harrison B Prosper and Raghuram Ramanujan},
  doi          = {10.1088/2632-2153/ad9884},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045073},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Implicit quantile networks for emulation in jet physics},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SICGNN: Structurally informed convolutional graph neural
networks for protein classification. <em>MLST</em>, <em>5</em>(4),
045072. (<a href="https://doi.org/10.1088/2632-2153/ad979b">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, graph neural networks (GNNs) have been widely used in various domains, including social networks, recommender systems, protein classification, molecular property prediction, and genetic networks. In bioinformatics and chemical engineering, considerable research is being actively conducted to represent molecules or proteins on graphs by conceptualizing atoms or amino acids as nodes and the relationships between nodes as edges. The overall structures of proteins and their interconnections are crucial for predicting and classifying their properties. However, as GNNs stack more layers to create deeper networks, the embeddings between nodes may become excessively similar, causing an oversmoothing problem that reduces the performance for downstream tasks. To avoid this, GNNs typically use a limited number of layers, which leads to the problem of reflecting only the local structure and neighborhood information rather than the global structure of the graph. Therefore, we propose a structurally informed convolutional GNN (SICGNN) that utilizes information that can express the overall topological structure of a protein graph during GNN training and prediction. By explicitly including information of the entire graph topology, the proposed model can utilize both local neighborhood and global structural information. We applied the SICGNN to representative GNNs such as GraphSAGE, graph isomorphism network, and graph attention network, and confirmed performance improvements across various datasets. We also demonstrate the robustness of SICGNN using multiple stratified 10-fold cross-validations and various hyperparameter settings, and demonstrate that its accuracy is comparable or better than those of existing GNN models.},
  archive      = {J_MLST},
  author       = {YongHyun Lee and Eunchan Kim and Jiwoong Choi and Changhyun Lee},
  doi          = {10.1088/2632-2153/ad979b},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045072},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {SICGNN: Structurally informed convolutional graph neural networks for protein classification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Process tomography of structured optical gates with
convolutional neural networks. <em>MLST</em>, <em>5</em>(4), 045071. (<a
href="https://doi.org/10.1088/2632-2153/ad9ba8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient and accurate characterization of an experimental setup is a critical requirement in any physical setting. In the quantum realm, the characterization of an unknown operator is experimentally accomplished via Quantum Process Tomography (QPT). This technique combines the outcomes of different projective measurements to reconstruct the underlying process matrix, typically extracted from maximum-likelihood estimation. Here, we exploit the logical correspondence between optical polarization and two-level quantum systems to retrieve the complex action of structured metasurfaces within a QPT-inspired context. In particular, we investigate a deep-learning approach that allows for fast and accurate reconstructions of space-dependent SU(2) operators by only processing a minimal set of measurements. We train a convolutional neural network based on a scalable U-Net architecture to process entire experimental images in parallel. Synthetic processes are reconstructed with average fidelity above 90%. The performance of our routine is experimentally validated in the case of space-dependent polarization transformations acting on a classical laser beam. Our approach further expands the toolbox of data-driven approaches to QPT and shows promise in the real-time characterization of complex optical gates.},
  archive      = {J_MLST},
  author       = {Tareq Jaouni and Francesco Di Colandrea and Lorenzo Amato and Filippo Cardano and Ebrahim Karimi},
  doi          = {10.1088/2632-2153/ad9ba8},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045071},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Process tomography of structured optical gates with convolutional neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Latent space dynamics learning for stiff
collisional-radiative models. <em>MLST</em>, <em>5</em>(4), 045070. (<a
href="https://doi.org/10.1088/2632-2153/ad9ce7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a data-driven method to discover the latent space and learn the corresponding latent dynamics for a collisional-radiative (CR) model in radiative plasma simulations. The CR model, consisting of high-dimensional stiff ordinary differential equations, must be solved at each grid point in the configuration space, leading to significant computational costs in plasma simulations. Our method employs a physics-assisted autoencoder to extract a low-dimensional latent representation of the original CR system. A flow map neural network is then used to learn the latent dynamics. Once trained, the reduced surrogate model predicts the entire latent dynamics given only the initial condition by iteratively applying the flow map. The radiative power loss (RPL) is then reconstructed using a decoder. Numerical experiments demonstrate that the proposed architecture can accurately predict both the full-order CR dynamics and the RPL rate.},
  archive      = {J_MLST},
  author       = {Xuping Xie and Qi Tang and Xianzhu Tang},
  doi          = {10.1088/2632-2153/ad9ce7},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045070},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Latent space dynamics learning for stiff collisional-radiative models},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Maven: A multimodal foundation model for supernova science.
<em>MLST</em>, <em>5</em>(4), 045069. (<a
href="https://doi.org/10.1088/2632-2153/ad990d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common setting in astronomy is the availability of a small number of high-quality observations, and larger amounts of either lower-quality observations or synthetic data from simplified models. Time-domain astrophysics is a canonical example of this imbalance, with the number of supernovae observed photometrically outpacing the number observed spectroscopically by multiple orders of magnitude. At the same time, no data-driven models exist to understand these photometric and spectroscopic observables in a common context. Contrastive learning objectives, which have grown in popularity for aligning distinct data modalities in a shared embedding space, provide a potential solution to extract information from these modalities. We present Maven, the first foundation model for supernova science. To construct Maven, we first pre-train our model to align photometry and spectroscopy from 0.5 M synthetic supernovae using a contrastive objective. We then fine-tune the model on 4702 observed supernovae from the Zwicky transient facility. Maven reaches state-of-the-art performance on both classification and redshift estimation, despite the embeddings not being explicitly optimized for these tasks. Through ablation studies, we show that pre-training with synthetic data improves overall performance. In the upcoming era of the Vera C. Rubin observatory, Maven will serve as a valuable tool for leveraging large, unlabeled and multimodal time-domain datasets.},
  archive      = {J_MLST},
  author       = {Gemma Zhang and Thomas Helfer and Alexander T Gagliano and Siddharth Mishra-Sharma and V Ashley Villar},
  doi          = {10.1088/2632-2153/ad990d},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045069},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Maven: A multimodal foundation model for supernova science},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). <span class="math inline">ABℂMB</span>: Deep delensing
assisted likelihood-free inference from CMB polarization maps.
<em>MLST</em>, <em>5</em>(4), 045068. (<a
href="https://doi.org/10.1088/2632-2153/ad9af9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existence of a cosmic background of primordial gravitational waves is a robust prediction of inflationary cosmology, but it has so far evaded discovery. The most promising avenue of its detection is via measurements of cosmic microwave background (CMB) B -polarization. However, this is not straightforward due to (a) the fact that CMB maps are distorted by gravitational lensing and (b) the high-dimensional nature of CMB data, which renders likelihood-based analysis methods computationally extremely expensive. In this paper, we introduce an efficient likelihood-free, end-to-end inference method to directly infer the posterior distribution of the tensor-to-scalar ratio r from lensed maps of the Stokes Q and U polarization parameters. Our method employs a generative model to delense the maps and utilizes the approximate Bayesian computation algorithm to sample r . We demonstrate that our method yields unbiased estimates of r with well-calibrated uncertainty quantification.},
  archive      = {J_MLST},
  author       = {Kai Yi and Yanan Fan and Jan Hamann and Pietro Liò and Yu Guang Wang},
  doi          = {10.1088/2632-2153/ad9af9},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045068},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {$\text{AB}\mathbb{C}\text{MB}$: Deep delensing assisted likelihood-free inference from CMB polarization maps},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Forecasting high-dimensional spatio-temporal systems from
sparse measurements. <em>MLST</em>, <em>5</em>(4), 045067. (<a
href="https://doi.org/10.1088/2632-2153/ad9883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new neural network architecture designed to forecast high-dimensional spatio-temporal data using only sparse measurements. The architecture uses a two-stage end-to-end framework that combines neural ordinary differential equations (NODEs) with vision transformers. Initially, our approach models the underlying dynamics of complex systems within a low-dimensional space; and then it reconstructs the corresponding high-dimensional spatial fields. Many traditional methods involve decoding high-dimensional spatial fields before modeling the dynamics, while some other methods use an encoder to transition from high-dimensional observations to a latent space for dynamic modeling. In contrast, our approach directly uses sparse measurements to model the dynamics, bypassing the need for an encoder. This direct approach simplifies the modeling process, reduces computational complexity, and enhances the efficiency and scalability of the method for large datasets. We demonstrate the effectiveness of our framework through applications to various spatio-temporal systems, including fluid flows and global weather patterns. Although sparse measurements have limitations, our experiments reveal that they are sufficient to forecast system dynamics accurately over long time horizons. Our results also indicate that the performance of our proposed method remains robust across different sensor placement strategies, with further improvements as the number of sensors increases. This robustness underscores the flexibility of our architecture, particularly in real-world scenarios where sensor data is often sparse and unevenly distributed.},
  archive      = {J_MLST},
  author       = {Jialin Song and Zezheng Song and Pu Ren and N Benjamin Erichson and Michael W Mahoney and Xiaoye S Li},
  doi          = {10.1088/2632-2153/ad9883},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045067},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Forecasting high-dimensional spatio-temporal systems from sparse measurements},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stabilizing training of affine coupling layers for
high-dimensional variational inference. <em>MLST</em>, <em>5</em>(4),
045066. (<a href="https://doi.org/10.1088/2632-2153/ad9a39">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational inference with normalizing flows is an increasingly popular alternative to MCMC methods. In particular, normalizing flows based on affine coupling layers (Real NVPs) are frequently used due to their good empirical performance. In theory, increasing the depth of normalizing flows should lead to more accurate posterior approximations. However, in practice, training deep normalizing flows for approximating high-dimensional posterior distributions is often infeasible due to the high variance of the stochastic gradients. In this work, we show that previous methods for stabilizing the variance of stochastic gradient descent can be insufficient to achieve stable training of Real NVPs. As the source of the problem, we identify that, during training, samples often exhibit unusual high values. As a remedy, we propose a combination of two methods: (1) soft-thresholding of the scale in Real NVPs, and (2) a bijective soft log transformation of the samples. We evaluate these and other previously proposed modification on several challenging target distributions, including a high-dimensional horseshoe logistic regression model. Our experiments show that with our modifications, stable training of Real NVPs for posteriors with several thousand dimensions and heavy tails is possible, allowing for more accurate marginal likelihood estimation via importance sampling. Moreover, we evaluate several common training techniques and architecture choices and provide practical advise for training Real NVPs for high-dimensional variational inference. Finally, we also provide new empirical and theoretical justification that optimizing the evidence lower bound of normalizing flows leads to good posterior distribution coverage.},
  archive      = {J_MLST},
  author       = {Daniel Andrade},
  doi          = {10.1088/2632-2153/ad9a39},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045066},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Stabilizing training of affine coupling layers for high-dimensional variational inference},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning continuous scattering length density profiles from
neutron reflectivities using convolutional neural networks*.
<em>MLST</em>, <em>5</em>(4), 045065. (<a
href="https://doi.org/10.1088/2632-2153/ad9809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interpreting neutron reflectivity (NR) data using ad hoc multi-layer models and physics-based models provides information about spatially resolved neutron scattering length density (NSLD) profiles. Recent improvements in data acquisition systems have allowed acquiring thousands of NR curves in a couple of hours, which has led to a need for automated data analysis tools to interpret NR measurements in real-time. Here, we present a machine learning analysis workflow that uses a series of models, based on a convolutional neural network (CNN), to learn the relation between the NSLDs and the NRs, and subsequently produce continuous NSLD profiles directly from NRs. The usefulness of our CNN-based models is demonstrated by constructing NSLDs from NRs of several films containing homopolymer polyzwitterions and diblock copolymers mixed with different types of salts. Comparisons of the NSLDs with those constructed using ad hoc multi-layer models reveal a very good agreement, suggesting the potential of CNN-based models for real-time automated data analysis of NRs.},
  archive      = {J_MLST},
  author       = {Brian Qu and Panagiotis Christakopoulos and Hanyu Wang and Jong Keum and Polyxeni P Angelopoulou and Peter V Bonnesen and Kunlun Hong and Mathieu Doucet and James F Browning and Miguel Fuentes-Cabrera and Rajeev Kumar},
  doi          = {10.1088/2632-2153/ad9809},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045065},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Learning continuous scattering length density profiles from neutron reflectivities using convolutional neural networks*},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CSS-net: A collaborative framework for synthesis and
segmentation of missing contrast-enhanced image with error-prediction
consistency. <em>MLST</em>, <em>5</em>(4), 045064. (<a
href="https://doi.org/10.1088/2632-2153/ad8e2c">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated tumor segmentation plays a critical role in facilitating the diagnosis and assessment of disease progression. Within the realm of tumor segmentation, Contrast-Enhanced (CE) scans are an effective imaging tool that allows for more intuitive observation of tumor characteristics and generally provide better segmentation results compared with Non-CE scans alone. However, CE images are not available in most cases due to the time-consuming and costly need for contrast and repeat scans. To solve this issue, this paper proposes a Collaborative framework for the Synthesis and Segmentation of missing CE images in medical imaging with error-prediction consistency (CSS-Net). CSS-Net simultaneously addresses synthesis and segmentation tasks, generating both the synthesized CE-like images and coarse segmentation results. Subsequently, a multi-layer adaptive feature fusion strategy is utilized to effectively leverage the correlation between these tasks, resulting in refined segmentation results. Additionally, the proposed method incorporates a multi-layer feature fusion block, which adaptively selects features pertinent to segmentation. Furthermore, error-prediction consistency is also introduced between coarse and refined segmentation for regularization, leading to high-performance segmentation results. What&#39;s more, we constructed a multimodal esophageal tumor segmentation dataset with 902 patients and validated it on this dataset and two publicly available multimodal brain tumor datasets. The results indicate that our method achieved Dice scores of 89.04% in esophageal tumor segmentation, 77.01% in whole glioma segmentation, and 91.14% in Vestibular Schwannoma segmentation. This performance surpasses that of segmentation using only available modalities and other image synthesis-based segmentation methods, demonstrating the superior robustness of CSS-Net.},
  archive      = {J_MLST},
  author       = {Xiaoyu Huang and Feixiang Zhang and Yong Huang and Kai Xu},
  doi          = {10.1088/2632-2153/ad8e2c},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045064},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {CSS-net: A collaborative framework for synthesis and segmentation of missing contrast-enhanced image with error-prediction consistency},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine-learned closure of URANS for stably stratified
turbulence: Connecting physical timescales &amp; data hyperparameters of
deep time-series models. <em>MLST</em>, <em>5</em>(4), 045063. (<a
href="https://doi.org/10.1088/2632-2153/ad9704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stably stratified turbulence (SST), a model that is representative of the turbulence found in the oceans and atmosphere, is strongly affected by fine balances between forces and becomes more anisotropic in time for decaying scenarios. Moreover, there is a limited understanding of the physical phenomena described by some of the terms in the Unsteady Reynolds-Averaged Navier–Stokes (URANS) equations—used to numerically simulate approximate solutions for such turbulent flows. Rather than attempting to model each term in URANS separately, it is attractive to explore the capability of machine learning (ML) to model groups of terms, i.e. to directly model the force balances. We develop deep time-series ML for closure modeling of the URANS equations applied to SST. We consider decaying SST which are homogeneous and stably stratified by a uniform density gradient, enabling dimensionality reduction. We consider two time-series ML models: long short-term memory and neural ordinary differential equation. Both models perform accurately and are numerically stable in a posteriori (online) tests. Furthermore, we explore the data requirements of the time-series ML models by extracting physically relevant timescales of the complex system. We find that the ratio of the timescales of the minimum information required by the ML models to accurately capture the dynamics of the SST corresponds to the Reynolds number of the flow. The current framework provides the backbone to explore the capability of such models to capture the dynamics of high-dimensional complex dynamical system like SST flows 6 .},
  archive      = {J_MLST},
  author       = {Muralikrishnan Gopalakrishnan Meena and Demetri Liousas and Andrew D Simin and Aditya Kashi and Wesley H Brewer and James J Riley and Stephen M de Bruyn Kops},
  doi          = {10.1088/2632-2153/ad9704},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045063},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine-learned closure of URANS for stably stratified turbulence: Connecting physical timescales &amp; data hyperparameters of deep time-series models},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal neural network-based predictive modeling of
nanoparticle properties from pure compounds. <em>MLST</em>,
<em>5</em>(4), 045062. (<a
href="https://doi.org/10.1088/2632-2153/ad9708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulating complex and large materials is a challenging task that requires extensive domain knowledge and computational expertise. This study introduces Pure2DopeNet, an innovative multimodal neural network that tackles these challenges by integrating image and text data to accurately predict the physical properties of doped compounds, specifically Carbon (C)-doped TiO 2 and Sulfur (S)-doped ZnO nanoparticles. The model achieves quantum mechanical level accuracy, comparable to density functional tight binding (DFTB), across various doping levels, demonstrating its capability to determine the properties from a single simulation of the pure compound. Pure2DopeNet outperforms traditional deep learning architectures such as ResNet, ViT, and CoAtNet, delivering superior accuracy, faster performance, and reduced dependence on domain expertise. This approach highlights the potential of multimodal machine learning to revolutionize materials science by making high-fidelity simulations more accessible and efficient, opening paving the way for material discovery and the exploration of novel properties.},
  archive      = {J_MLST},
  author       = {Can Polat and Mustafa Kurban and Hasan Kurban},
  doi          = {10.1088/2632-2153/ad9708},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045062},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Multimodal neural network-based predictive modeling of nanoparticle properties from pure compounds},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Illuminating new and known relations between knot
invariants. <em>MLST</em>, <em>5</em>(4), 045061. (<a
href="https://doi.org/10.1088/2632-2153/ad95d9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We automate the process of machine learning correlations between knot invariants. For nearly 200 000 distinct sets of input knot invariants together with an output invariant, we attempt to learn the output invariant by training a neural network on the input invariants. Correlation between invariants is measured by the accuracy of the neural network prediction, and bipartite or tripartite correlations are sequentially filtered from the input invariant sets so that experiments with larger input sets are checking for true multipartite correlation. We rediscover several known relationships between polynomial, homological, and hyperbolic knot invariants, while also finding novel correlations which are not explained by known results in knot theory. These unexplained correlations strengthen previous observations concerning links between Khovanov and knot Floer homology. Our results also point to a new connection between quantum algebraic and hyperbolic invariants, similar to the generalized volume conjecture.},
  archive      = {J_MLST},
  author       = {Jessica Craven and Mark Hughes and Vishnu Jejjala and Arjun Kar},
  doi          = {10.1088/2632-2153/ad95d9},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045061},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Illuminating new and known relations between knot invariants},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inverse prediction of al alloy post-processing conditions
using classification with guided oversampling. <em>MLST</em>,
<em>5</em>(4), 045060. (<a
href="https://doi.org/10.1088/2632-2153/ad95dc">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning is proving to be an ideal tool for materials design, capable of predicting forward structure-property relationships, and inverse property-structure relationships. However, it has yet to be used extensively for materials engineering challenges, predicting post-processing/structure relationships, and has yet to be used for to predict structure/post-processing relationships for inverse engineering. This is often due to the lack of sufficient metadata, and the overall scarcity and imbalance of processing data in many domains. This topic is explored in the current study using binary and multi-class classification to predict the appropriate post-synthesis processing conditions for aluminium alloys, based entirely on the alloying composition. The data imbalance was addressed using a new guided oversampling strategy that improves model performance by simultaneously balancing the classes and avoiding noise that contributes to over-fitting. This is achieved by through the deliberate but strategic introduction of not-a-numbers (NaNs) and the use of algorithms that naturally avoid them during learning. The outcome is the successful training of highly accurate binary classifiers, with significant reductions in false negatives and/or false positives with respect to the classifiers trained on the original data alone. Superior results were obtained for models predicting whether alloys should be solutionised or aged, post-synthesis, by guiding the re-balancing of the classes based on features (metals) that are highly ranked by the classifier, and then doubling the size of the data set via interpolation. Overall, this strategy has the greatest impact on tasks with a Shannon Diversity Index greater than 1 or less than 0.5, but can be applied to any prediction of post-processing conditions as part of an inverse engineering workflow.},
  archive      = {J_MLST},
  author       = {A S Barnard},
  doi          = {10.1088/2632-2153/ad95dc},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045060},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Inverse prediction of al alloy post-processing conditions using classification with guided oversampling},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transferability of atom-based neural networks.
<em>MLST</em>, <em>5</em>(4), 045059. (<a
href="https://doi.org/10.1088/2632-2153/ad9709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine-learning models in chemistry—when based on descriptors of atoms embedded within molecules—face essential challenges in transferring the quality of predictions of local electronic structures and their associated properties across chemical compound space. In the present work, we make use of adversarial validation to elucidate certain intrinsic complications related to machine inferences of unseen chemistry. On this basis, we employ invariant and equivariant neural networks—both trained either exclusively on total molecular energies or a combination of these and data from atomic partitioning schemes—to evaluate how such models scale performance-wise between datasets of fundamentally different functionality and composition. We find the inference of local electronic properties to improve significantly when training models on augmented data that appropriately expose local functional features. However, molecular datasets for training purposes must themselves be sufficiently comprehensive and rich in composition to warrant any generalizations to larger systems, and even then, transferability can still only genuinely manifest if the body of atomic energies available for training purposes exposes the uniqueness of different functional moieties within molecules. We demonstrate this point by comparing machine models trained on atomic partitioning schemes based on the spatial locality of either native atomic or molecular orbitals.},
  archive      = {J_MLST},
  author       = {Frederik Ø Kjeldal and Janus J Eriksen},
  doi          = {10.1088/2632-2153/ad9709},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045059},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Transferability of atom-based neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid data-driven and physics-informed regularized learning
of cyclic plasticity with neural networks. <em>MLST</em>, <em>5</em>(4),
045058. (<a href="https://doi.org/10.1088/2632-2153/ad95da">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An extendable, efficient and explainable Machine Learning approach is proposed to represent cyclic plasticity and replace conventional material models based on the Radial Return Mapping algorithm. High accuracy and stability by means of a limited amount of training data is achieved by implementing physics-informed regularizations and the back stress information. The off-loading of the neural network (NN) is applied to the maximal extent. The proposed model architecture is simpler and more efficient compared to existing solutions from the literature using approximately only half the amount of NN parameters, while representing a complete three-dimensional material model. The validation of the approach is carried out by means of results obtained with the Armstrong–Frederick kinematic hardening model. The mean squared error is assumed as the loss function which stipulates several restrictions: deviatoric character of internal variables, compliance with the flow rule, the differentiation of elastic and plastic steps and the associativity of the flow rule. The latter, however, has a minor impact on the accuracy, which implies the generalizability of the model for a broad spectrum of evolution laws for internal variables. Numerical tests simulating several load cases are presented in detail. The validation shows cyclic stability and deviations in normal directions of less than 2% at peak values which is comparable to the order of measurement inaccuracies.},
  archive      = {J_MLST},
  author       = {Stefan Hildebrand and Sandra Klinge},
  doi          = {10.1088/2632-2153/ad95da},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045058},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Hybrid data-driven and physics-informed regularized learning of cyclic plasticity with neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-target quantum compilation algorithm. <em>MLST</em>,
<em>5</em>(4), 045057. (<a
href="https://doi.org/10.1088/2632-2153/ad9705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum compilation is the process of converting a target unitary operation into a trainable unitary represented by a quantum circuit. It has a wide range of applications, including gate optimization, quantum-assisted compiling, quantum state preparation, and quantum dynamic simulation. Traditional quantum compilation usually optimizes circuits for a single target. However, many quantum systems require simultaneous optimization of multiple targets, such as thermal state preparation, time-dependent dynamic simulation, and others. To address this, we develop a multi-target quantum compilation algorithm to improve the performance and flexibility of simulating multiple quantum systems. Our benchmarks and case studies demonstrate the effectiveness of the algorithm, highlighting the importance of multi-target optimization in advancing quantum computing. This work lays the groundwork for further development and evaluation of multi-target quantum compilation algorithms.},
  archive      = {J_MLST},
  author       = {Vu Tuan Hai and Nguyen Tan Viet and Jesus Urbaneja and Nguyen Vu Linh and Lan Nguyen Tran and Le Bin Ho},
  doi          = {10.1088/2632-2153/ad9705},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045057},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Multi-target quantum compilation algorithm},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring spatial reasoning performances of CNN on linear
layout dataset. <em>MLST</em>, <em>5</em>(4), 045056. (<a
href="https://doi.org/10.1088/2632-2153/ad9706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial reasoning, a fundamental aspect of human intelligence, is essential for machine learning models to understand and interpret object relationships. It is crucial for numerous real-world applications, ranging from autonomous navigation to urban planning. The lack of comprehensive datasets limits the development and evaluation of models that can effectively handle spatial reasoning tasks. Existing datasets often contain complex spatial reasoning problems with overlapping spatial relationships, making it challenging to diagnose specific aspects that a model struggles with. We address this gap by introducing a new dataset of linear layouts. This dataset is systematically designed to exhibit a range of spatial relations and complexity levels. Analyzing spatial reasoning through linear layout generation offers a more structured and manageable approach to understanding how models learn and interpret spatial relationships. Linear layout generation has broad applicability and is of fundamental importance in design and optimization. To benchmark dataset, we develop LinLayCNN, a generic data-driven method that applies shallow, one-dimensional convolutional neural network (CNN), to generate linear layouts in an iterative process. Experimental results reveal that LinLayCNN can effectively solve fundamental spatial challenges even with the relatively small size of the training set. It is capable of precise object placement, making it a robust tool for linear layout generation. Current layout generation methods focus on domain-specific solutions and often fail to maintain the precision needed for technical domains, such as accurate sizing, and object counting. They also require a substantial amount of data to function effectively. LinLayCNN overcame these issues. This study further clarifies CNNs&#39; capabilities in spatial reasoning, highlight their potential to advance the field of layout generation. As a result, our approach establishes a clear benchmark for evaluating spatial reasoning and aids in development of models that can more effectively understand and reason about space.},
  archive      = {J_MLST},
  author       = {Jelena Pejic and Marko Petkovic and Sandra Klinge},
  doi          = {10.1088/2632-2153/ad9706},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045056},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Exploring spatial reasoning performances of CNN on linear layout dataset},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Paired autoencoders for likelihood-free estimation in
inverse problems. <em>MLST</em>, <em>5</em>(4), 045055. (<a
href="https://doi.org/10.1088/2632-2153/ad95dd">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the solution of nonlinear inverse problems where the forward problem is a discretization of a partial differential equation. Such problems are notoriously difficult to solve in practice and require minimizing a combination of a data-fit term and a regularization term. The main computational bottleneck of typical algorithms is the direct estimation of the data misfit. Therefore, likelihood-free approaches have become appealing alternatives. Nonetheless, difficulties in generalization and limitations in accuracy have hindered their broader utility and applicability. In this work, we use a paired autoencoder framework as a likelihood-free estimator (LFE) for inverse problems. We show that the use of such an architecture allows us to construct a solution efficiently and to overcome some known open problems when using LFEs. In particular, our framework can assess the quality of the solution and improve on it if needed. We demonstrate the viability of our approach using examples from full waveform inversion and inverse electromagnetic imaging.},
  archive      = {J_MLST},
  author       = {Matthias Chung and Emma Hart and Julianne Chung and Bas Peters and Eldad Haber},
  doi          = {10.1088/2632-2153/ad95dd},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045055},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Paired autoencoders for likelihood-free estimation in inverse problems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ZeROf-offload: Forward-gradient scheme for efficient full
parameter fine-tuning of billion-scale language models. <em>MLST</em>,
<em>5</em>(4), 045054. (<a
href="https://doi.org/10.1088/2632-2153/ad9667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In large language models (LLMs), full-parameter fine-tuning is crucial for task-specific adaptation. Traditionally, this relies on deep learning training frameworks utilizing the back-propagation scheme. However, this scheme presents inherent issues, e.g. activation memory bottlenecks and backward locking, which limit the efficient computational resource usage. In this work, we propose the design and analysis of ZeROf-Offload, an innovative fine-tuning framework that adapts the forward-gradient scheme. This framework adopts a unique forward-gradient-oriented CPU offload strategy, enabling fine-tuning of billion-scale LLMs solely in the forward phase and enhancing computational efficiency. Empirical evaluations reveal the advantage of eliminating the backward phase in fine-tuning. ZeROf-Offload achieves134 TFlops/GPU for models with over 130 billion parameters on a single DGX-A100 node, outperforming DeepSpeed&#39;s ZeRO-Offload, which achieves 102 TFlops/GPU for models with up to 53.7 billion parameters, the largest size manageable within GPU memory limitations. Furthermore, we have expanded ZeROf-Offload for multi-DGX-A100 environments with integrated 3D parallelism, achieving near-linear speedup across up to 128 GPUs and the token throughput by 1.4x and 1.5x, respectively. The experimental results demonstrate that the proposed ZeROf-Offload has achieved the highest throughput performance compared to all examined state-of-the-art frameworks.},
  archive      = {J_MLST},
  author       = {Jian Zhu and Peicheng Feng and Jiawei Lu and Bowei Fang and Hesong Yang},
  doi          = {10.1088/2632-2153/ad9667},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045054},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {ZeROf-offload: Forward-gradient scheme for efficient full parameter fine-tuning of billion-scale language models},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-lattice sampling of quantum field theories via neural
operator-based flows. <em>MLST</em>, <em>5</em>(4), 045053. (<a
href="https://doi.org/10.1088/2632-2153/ad9707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of sampling lattice field configurations on a lattice from the Boltzmann distribution corresponding to some action. Since such densities arise as approximationw of an underlying functional density, we frame the task as an instance of operator learning. We propose to approximate a time-dependent neural operator whose time integral provides a mapping between the functional distributions of the free and target theories. Once a particular lattice is chosen, the neural operator can be discretized to a finite-dimensional, time-dependent vector field which in turn induces a continuous normalizing flow between finite dimensional distributions over the chosen lattice. This flow can then be trained to be a diffeormorphism between the discretized free and target theories on the chosen lattice, and, by construction, can be evaluated on different discretizations of spacetime. We experimentally validate the proposal on the 2-dimensional φ 4 -theory to explore to what extent such operator-based flow architectures generalize to lattice sizes they were not trained on, and show that pretraining on smaller lattices can lead to a speedup over training directly on the target lattice size.},
  archive      = {J_MLST},
  author       = {Bálint Máté and François Fleuret},
  doi          = {10.1088/2632-2153/ad9707},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045053},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Multi-lattice sampling of quantum field theories via neural operator-based flows},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient wasserstein-distance approach for
reconstructing jump-diffusion processes using parameterized neural
networks. <em>MLST</em>, <em>5</em>(4), 045052. (<a
href="https://doi.org/10.1088/2632-2153/ad9379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze the Wasserstein distance ( W -distance) between two probability distributions associated with two multidimensional jump-diffusion processes. Specifically, we analyze a temporally decoupled squared W 2 -distance, which provides both upper and lower bounds associated with the discrepancies in the drift, diffusion, and jump amplitude functions between the two jump-diffusion processes. Then, we propose a temporally decoupled squared W 2 -distance method for efficiently reconstructing unknown jump-diffusion processes from data using parameterized neural networks. We further show its performance can be enhanced by utilizing prior information on the drift function of the jump-diffusion process. The effectiveness of our proposed reconstruction method is demonstrated across several examples and applications.},
  archive      = {J_MLST},
  author       = {Mingtao Xia and Xiangting Li and Qijing Shen and Tom Chou},
  doi          = {10.1088/2632-2153/ad9379},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045052},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {An efficient wasserstein-distance approach for reconstructing jump-diffusion processes using parameterized neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning materials properties with accurate
predictions, uncertainty estimates, domain guidance, and persistent
online accessibility. <em>MLST</em>, <em>5</em>(4), 045051. (<a
href="https://doi.org/10.1088/2632-2153/ad95db">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One compelling vision of the future of materials discovery and design involves the use of machine learning (ML) models to predict materials properties and then rapidly find materials tailored for specific applications. However, realizing this vision requires both providing detailed uncertainty quantification (model prediction errors and domain of applicability) and making models readily usable. At present, it is common practice in the community to assess ML model performance only in terms of prediction accuracy (e.g. mean absolute error), while neglecting detailed uncertainty quantification and robust model accessibility and usability. Here, we demonstrate a practical method for realizing both uncertainty and accessibility features with a large set of models. We develop random forest ML models for 33 materials properties spanning an array of data sources (computational and experimental) and property types (electrical, mechanical, thermodynamic, etc). All models have calibrated ensemble error bars to quantify prediction uncertainty and domain of applicability guidance enabled by kernel-density-estimate-based feature distance measures. All data and models are publicly hosted on the Garden-AI infrastructure, which provides an easy-to-use, persistent interface for model dissemination that permits models to be invoked with only a few lines of Python code. We demonstrate the power of this approach by using our models to conduct a fully ML-based materials discovery exercise to search for new stable, highly active perovskite oxide catalyst materials.},
  archive      = {J_MLST},
  author       = {Ryan Jacobs and Lane E Schultz and Aristana Scourtas and KJ Schmidt and Owen Price-Skelly and Will Engler and Ian Foster and Ben Blaiszik and Paul M Voyles and Dane Morgan},
  doi          = {10.1088/2632-2153/ad95db},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {4},
  pages        = {045051},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning materials properties with accurate predictions, uncertainty estimates, domain guidance, and persistent online accessibility},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From STEM-EDXS data to phase separation and quantification
using physics-guided NMF. <em>MLST</em>, <em>5</em>(4), 045050. (<a
href="https://doi.org/10.1088/2632-2153/ad9192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the development of a new algorithm which combines state-of-the-art energy-dispersive x-ray (EDX) spectroscopy theory and a suitable machine learning formulation for the hyperspectral unmixing of scanning transmission electron microscope EDX spectrum images. The algorithm is based on non-negative matrix factorization (NMF) incorporating a physics-guided factorization model. It optimizes a Poisson likelihood, under additional simplex constraint together with user-chosen sparsity-inducing and smoothing regularizations, and is based on iterative multiplicative updates. The fluorescence of x-rays is fully modeled thanks to state-of-the-art theoretical work. It is shown that the output of the algorithm can be used for a direct chemical quantification. With this approach, it is straightforward to include a priori knowledge on the specimen such as the presence or absence of certain chemical elements in some of its phases. This work is implemented within two open-source Python packages, espm and emtables , which are used here for data simulation, data analysis and quantification. Using simulated data, we demonstrate that incorporating physical modeling in the decomposition helps retrieve meaningful components from spatially and spectrally mixed phases, even when the data are very noisy. For synthetic data with a higher signal, the regularizations yield a tenfold increase in the quality of the reconstructed abundance maps compared to standard NMF. Our approach is further validated on experimental data with a known ground truth, where state-of-the art results are achieved by using prior knowledge about the sample. Our model can be generalized to any other scanning spectroscopy techniques where underlying physical modeling can be linearized.},
  archive      = {J_MLST},
  author       = {Adrien Teurtrie and Nathanaël Perraudin and Thomas Holvoet and Hui Chen and Duncan T L Alexander and Guillaume Obozinski and Cécile Hébert},
  doi          = {10.1088/2632-2153/ad9192},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {4},
  pages        = {045050},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {From STEM-EDXS data to phase separation and quantification using physics-guided NMF},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Noise classification in three-level quantum networks by
machine learning. <em>MLST</em>, <em>5</em>(4), 045049. (<a
href="https://doi.org/10.1088/2632-2153/ad9193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate a machine learning based classification of noise acting on a small quantum network with the aim of detecting spatial or multilevel correlations, and the interplay with Markovianity. We control a three-level system by inducing coherent population transfer exploiting different pulse amplitude combinations as inputs to train a feedforward neural network. We show that supervised learning can classify different types of classical dephasing noise affecting the system. Three non-Markovian (quasi-static correlated, anti-correlated and uncorrelated) and Markovian noises are classified with more than 99% accuracy. On the contrary, correlations of Markovian noise cannot be discriminated with our method. Our approach is robust to statistical measurement errors and retains its effectiveness for physical measurements where only a limited number of samples is available making it very experimental-friendly. Our result paves the way for classifying spatial correlations of noise in quantum architectures.},
  archive      = {J_MLST},
  author       = {Shreyasi Mukherjee and Dario Penna and Fabio Cirinnà and Mauro Paternostro and Elisabetta Paladino and Giuseppe Falci and Luigi Giannelli},
  doi          = {10.1088/2632-2153/ad9193},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {4},
  pages        = {045049},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Noise classification in three-level quantum networks by machine learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning visualization tool for exploring
parameterized hydrodynamics*. <em>MLST</em>, <em>5</em>(4), 045048. (<a
href="https://doi.org/10.1088/2632-2153/ad8daa">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are interested in the computational study of shock hydrodynamics, i.e. problems involving compressible solids, liquids, and gases that undergo large deformation. These problems are dynamic and nonlinear and can exhibit complex instabilities. Due to advances in high performance computing it is possible to parameterize a hydrodynamic problem and perform a computational study yielding \mathcal{O}\left(\textrm{TB}\right) of simulation state data. We present an interactive machine learning tool that can be used to compress, browse, and interpolate these large simulation datasets. This tool allows computational scientists and researchers to quickly visualize &#39;what-if&#39; situations, perform sensitivity analyses, and optimize complex hydrodynamic experiments.},
  archive      = {J_MLST},
  author       = {C F Jekel and D M Sterbentz and T M Stitt and P Mocz and R N Rieben and D A White and J L Belof},
  doi          = {10.1088/2632-2153/ad8daa},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {4},
  pages        = {045048},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning visualization tool for exploring parameterized hydrodynamics*},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MSMGE-CNN: A multi-scale multi-graph embedding convolutional
neural network for motor related EEG decoding. <em>MLST</em>,
<em>5</em>(4), 045047. (<a
href="https://doi.org/10.1088/2632-2153/ad9135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning technique has been widely used for decoding motor related electroencephalography (EEG) signals, which has considerably driven the development of motor related brain–computer interfaces (BCIs). However, traditional convolutional neural networks (CNNs) cannot fully represent spatial topology information and dynamic temporal characteristics of multi-channel EEG signals, resulting in limited decoding accuracy. To address such challenges, a novel multi-scale multi-graph embedding CNN (MSMGE-CNN) is proposed in this study. The proposed MSMGE-CNN contains two crucial components: multi-scale time convolution and multi-graph embedding. Specifically, we design a multi-branch CNN architecture with mixed-scale time convolutions based on EEGNet to sufficiently extract robust time domain features. Afterward, we embed multi-graph information obtained based on physical distance proximity and functional connectivity of multi-channel EEG signals into the time-domain features to capture rich spatial topological dependencies via multi-graph convolution operation. We extensively evaluated the proposed method on three benchmark EEG datasets commonly used for motor imagery/execution (MI/ME) classification and obtained accuracies of 79.59% (BCICIV-2a Dataset), 69.77% (OpenBMI Dataset) and 96.34% (High Gamma Dataset), respectively. These results powerfully demonstrate that MSMGE-CNN outperforms several state-of-the-art algorithms. In addition, we further conducted a series of ablation experiments to validate the rationality of our network architecture. Overall, the proposed MSMGE-CNN method dramatically improves the accuracy and robustness of MI/ME-EEG decoding, which can effectively enhance the performance of motor related BCI system.},
  archive      = {J_MLST},
  author       = {Binren Wang and Minmin Miao and Ke Zhang and Wenzhe Liu and Zhenzhen Sheng and Baoguo Xu and Wenjun Hu},
  doi          = {10.1088/2632-2153/ad9135},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {4},
  pages        = {045047},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {MSMGE-CNN: A multi-scale multi-graph embedding convolutional neural network for motor related EEG decoding},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physics-inspired machine learning detects “unknown unknowns”
in networks: Discovering network boundaries from observable dynamics.
<em>MLST</em>, <em>5</em>(4), 045046. (<a
href="https://doi.org/10.1088/2632-2153/ad9194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamics on networks is often only partially observable in experiment, with many nodes being inaccessible or indeed the existence and properties of a larger unobserved network being unknown. This limits our ability to reconstruct the topology of the network and the strength of the interactions among even the observed nodes. Here, we show how machine learning inspired by physics can be utilized on noisy time series of such partially observed networks to determine which nodes of the observed part of a network form its boundary, i.e. have significant interactions with the unobserved part. This opens a route to reliable network reconstruction. We develop the method for arbitrary network dynamics and topologies and demonstrate it on a broad range of dynamics including non-linear coupled oscillators and chaotic attractors. Beyond these we focus in particular on biochemical reaction networks, where we apply the approach to the dynamics of the epidermal growth factor receptor (EGFR) network and show that it works even for substantial noise levels.},
  archive      = {J_MLST},
  author       = {Moshir Harsh and Leonhard Götz Vulpius and Peter Sollich},
  doi          = {10.1088/2632-2153/ad9194},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {4},
  pages        = {045046},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Physics-inspired machine learning detects ‘unknown unknowns’ in networks: Discovering network boundaries from observable dynamics},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient mapping of phase diagrams with conditional
boltzmann generators. <em>MLST</em>, <em>5</em>(4), 045045. (<a
href="https://doi.org/10.1088/2632-2153/ad849d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate prediction of phase diagrams is of central importance for both the fundamental understanding of materials as well as for technological applications in material sciences. However, the computational prediction of the relative stability between phases based on their free energy is a daunting task, as traditional free energy estimators require a large amount of simulation data to obtain uncorrelated equilibrium samples over a grid of thermodynamic states. In this work, we develop deep generative machine learning models based on the Boltzmann Generator approach for entire phase diagrams, employing normalizing flows conditioned on the thermodynamic states, e.g. temperature and pressure, that they map to. By training a single normalizing flow to transform the equilibrium distribution sampled at only one reference thermodynamic state to a wide range of target temperatures and pressures, we can efficiently generate equilibrium samples across the entire phase diagram. Using a permutation-equivariant architecture allows us, thereby, to treat solid and liquid phases on the same footing. We demonstrate our approach by predicting the solid–liquid coexistence line for a Lennard-Jones system in excellent agreement with state-of-the-art free energy methods while significantly reducing the number of energy evaluations needed.},
  archive      = {J_MLST},
  author       = {Maximilian Schebek and Michele Invernizzi and Frank Noé and Jutta Rogal},
  doi          = {10.1088/2632-2153/ad849d},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {4},
  pages        = {045045},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Efficient mapping of phase diagrams with conditional boltzmann generators},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Calibrating bayesian generative machine learning for
bayesiamplification. <em>MLST</em>, <em>5</em>(4), 045044. (<a
href="https://doi.org/10.1088/2632-2153/ad9136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, combinations of generative and Bayesian deep learning have been introduced in particle physics for both fast detector simulation and inference tasks. These neural networks aim to quantify the uncertainty on the generated distribution originating from limited training statistics. The interpretation of a distribution-wide uncertainty however remains ill-defined. We show a clear scheme for quantifying the calibration of Bayesian generative machine learning models. For a Continuous Normalizing Flow applied to a low-dimensional toy example, we evaluate the calibration of Bayesian uncertainties from either a mean-field Gaussian weight posterior, or Monte Carlo sampling network weights, to gauge their behaviour on unsteady distribution edges. Well calibrated uncertainties can then be used to roughly estimate the number of uncorrelated truth samples that are equivalent to the generated sample and clearly indicate data amplification for smooth features of the distribution.},
  archive      = {J_MLST},
  author       = {S Bieringer and S Diefenbacher and G Kasieczka and M Trabs},
  doi          = {10.1088/2632-2153/ad9136},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {4},
  pages        = {045044},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Calibrating bayesian generative machine learning for bayesiamplification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated design of digital filters using convolutional
neural networks for extracting ringdown gravitational waves.
<em>MLST</em>, <em>5</em>(4), 045043. (<a
href="https://doi.org/10.1088/2632-2153/ad8b94">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The observation of gravitational waves is expected to allow new tests of general relativity to be performed. As the gravitational wave signal is hidden by detector noise in observed data, a method to reduce noise is required to analyze the ringdown phase of gravitational wave signals. Recently, some noise reduction methods based on a neural network have been proposed; however, the results of these methods must be considered with caution because the output can contain spurious components. To overcome this limitation, in this study, we developed a neural network–based method to design optimal digital filters for extracting ringdown gravitational wave signals. In this method, no spurious components appear in the output because the digital filters reduce the noise. We conducted simulations with waveforms of gravitational waves from binary black hole coalescence and confirmed that the proposed method designs appropriate filters that reduce detector noise.},
  archive      = {J_MLST},
  author       = {Kazuki Sakai and Sodtavilan Odonchimed and Mitsuki Takano and Hirotaka Takahashi},
  doi          = {10.1088/2632-2153/ad8b94},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {4},
  pages        = {045043},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Automated design of digital filters using convolutional neural networks for extracting ringdown gravitational waves},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accelerating graph-based tracking tasks with symbolic
regression. <em>MLST</em>, <em>5</em>(4), 045042. (<a
href="https://doi.org/10.1088/2632-2153/ad8f12">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reconstruction of particle tracks from hits in tracking detectors is a computationally intensive task due to the large combinatorics of detector signals. Recent efforts have proven that ML techniques can be successfully applied to the tracking problem, extending and improving the conventional methods based on feature engineering. However, complex models can be challenging to implement on heterogeneous trigger systems, integrating architectures such as field programmable gate arrays (FPGAs). Deploying the network on an FPGA is feasible but challenging and limited by its resources. An efficient alternative can employ symbolic regression (SR). We propose a novel approach that uses SR to replace a graph-based neural network. Substituting each network block with a symbolic function preserves the graph structure of the data and enables message passing. The technique is perfectly suitable for heterogeneous hardware, as it can be implemented more easily on FPGAs and grants faster execution times on CPU with respect to conventional methods. While the tracking problem is the target for this work, it also provides a proof-of-principle for the method that can be applied to many use cases.},
  archive      = {J_MLST},
  author       = {Nathalie Soybelman and Carlo Schiavi and Francesco A Di Bello and Eilam Gross},
  doi          = {10.1088/2632-2153/ad8f12},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {4},
  pages        = {045042},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Accelerating graph-based tracking tasks with symbolic regression},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accelerating data acquisition with FPGA-based edge machine
learning: A case study with LCLS-II. <em>MLST</em>, <em>5</em>(4),
045041. (<a href="https://doi.org/10.1088/2632-2153/ad8ea8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New scientific experiments and instruments generate vast amounts of data that need to be transferred for storage or further processing, often overwhelming traditional systems. Edge machine learning (EdgeML) addresses this challenge by integrating machine learning (ML) algorithms with edge computing, enabling real-time data processing directly at the point of data generation. EdgeML is particularly beneficial for environments where immediate decisions are required, or where bandwidth and storage are limited. In this paper, we demonstrate a high-speed configurable ML model in a fully customizable EdgeML system using a field programmable gate array (FPGA). Our demonstration focuses on an angular array of electron spectrometers, referred to as the &#39;CookieBox,&#39; developed for the Linac Coherent Light Source II project. The EdgeML system captures 51.2 Gbps from a 6.4 GS s −1 analog to digital converter and is designed to integrate data pre-processing and ML inside an FPGA. Our implementation achieves an inference latency of 0.2 µ s for the ML model, and a total latency of 0.4 µ s for the complete EdgeML system, which includes pre-processing, data transmission, digitization, and ML inference. The modular design of the system allows it to be adapted for other instrumentation applications requiring low-latency data processing.},
  archive      = {J_MLST},
  author       = {Mohammad Mehdi Rahimifar and Quentin Wingering and Berthié Gouin-Ferland and Ryan Coffee and Audrey C Therrien},
  doi          = {10.1088/2632-2153/ad8ea8},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {4},
  pages        = {045041},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Accelerating data acquisition with FPGA-based edge machine learning: A case study with LCLS-II},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rapid parameter estimation for merging massive black hole
binaries using continuous normalizing flows. <em>MLST</em>,
<em>5</em>(4), 045040. (<a
href="https://doi.org/10.1088/2632-2153/ad8da9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting the coalescences of massive black hole binaries (MBHBs) is one of the primary targets for space-based gravitational wave observatories such as laser interferometer space antenna, Taiji, and Tianqin. The fast and accurate parameter estimation of merging MBHBs is of great significance for the global fitting of all resolvable sources, as well as the astrophysical interpretation of gravitational wave signals. However, such analyses usually entail significant computational costs. To address these challenges, inspired by the latest progress in generative models, we explore the application of continuous normalizing flows (CNFs) on the parameter estimation of MBHBs. Specifically, we employ linear interpolation and trig interpolation methods to construct transport paths for training CNFs. Additionally, we creatively introduce a parameter transformation method based on the symmetry in the detector&#39;s response function. This transformation is integrated within CNFs, allowing us to train the model using a simplified dataset, and then perform parameter estimation on more general data, hence also acting as a crucial factor in improving the training speed. In conclusion, for the first time, within a comprehensive and reasonable parameter range, we have achieved a complete and unbiased 11-dimensional rapid inference for MBHBs in the presence of astrophysical confusion noise using CNFs. In the experiments based on simulated data, our model produces posterior distributions comparable to those obtained by nested sampling.},
  archive      = {J_MLST},
  author       = {Bo Liang and Minghui Du and He Wang and Yuxiang Xu and Chang Liu and Xiaotong Wei and Peng Xu and Li-e Qiang and Ziren Luo},
  doi          = {10.1088/2632-2153/ad8da9},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {4},
  pages        = {045040},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Rapid parameter estimation for merging massive black hole binaries using continuous normalizing flows},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural network representation of quantum systems.
<em>MLST</em>, <em>5</em>(4), 045039. (<a
href="https://doi.org/10.1088/2632-2153/ad81ac">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been proposed that random wide neural networks near Gaussian process are quantum field theories around Gaussian fixed points. In this paper, we provide a novel map with which a wide class of quantum mechanical systems can be cast into the form of a neural network with a statistical summation over network parameters. Our simple idea is to use the universal approximation theorem of neural networks to generate arbitrary paths in the Feynman&#39;s path integral. The map can be applied to interacting quantum systems / field theories, even away from the Gaussian limit. Our findings bring machine learning closer to the quantum world.},
  archive      = {J_MLST},
  author       = {Koji Hashimoto and Yuji Hirono and Jun Maeda and Jojiro Totsuka-Yoshinaka},
  doi          = {10.1088/2632-2153/ad81ac},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {4},
  pages        = {045039},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Neural network representation of quantum systems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards harmonization of SO(3)-equivariance and
expressiveness: A hybrid deep learning framework for
electronic-structure hamiltonian prediction. <em>MLST</em>,
<em>5</em>(4), 045038. (<a
href="https://doi.org/10.1088/2632-2153/ad8d30">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning for predicting the electronic-structure Hamiltonian of quantum systems necessitates satisfying the covariance laws, among which achieving SO(3)-equivariance without sacrificing the non-linear expressive capability of networks remains unsolved. To navigate the harmonization between SO(3)-equivariance and expressiveness, we propose HarmoSE, a deep learning method synergizing two distinct categories of neural mechanisms as a two-stage encoding and regression framework. The first stage corresponds to group theory-based neural mechanisms with inherent SO(3)-equivariant properties prior to the parameter learning process, while the second stage is characterized by a non-linear 3D graph Transformer network we propose, featuring high capability on non-linear expressiveness. Their combination lies in the point that, the first stage predicts baseline Hamiltonians with abundant SO(3)-equivariant features extracted, assisting the second stage in empirical learning of equivariance; and in turn, the second stage refines the first stage&#39;s output as a fine-grained prediction of Hamiltonians using powerful non-linear neural mappings, compensating for the intrinsic weakness on non-linear expressiveness capability of mechanisms in the first stage. Our method enables precise, generalizable predictions while capturing SO(3)-equivariance under rotational transformations, and achieves state-of-the-art performance in Hamiltonian prediction tasks under multiple mean absolute error (MAE) metrics, such as the average MAE across all samples and matrix elements, the MAE for challenging samples, the MAE for different Hamiltonian blocks, and the MAE for the challenging blocks. It also demonstrates significant improvements in accuracy for downstream quantities, such as occupied orbital energy and the electronic wavefunction, as measured by MAE and cosine similarity, respectively.},
  archive      = {J_MLST},
  author       = {Shi Yin and Xinyang Pan and Xudong Zhu and Tianyu Gao and Haochong Zhang and Feng Wu and Lixin He},
  doi          = {10.1088/2632-2153/ad8d30},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {4},
  pages        = {045038},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Towards harmonization of SO(3)-equivariance and expressiveness: A hybrid deep learning framework for electronic-structure hamiltonian prediction},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spectroscopy-guided discovery of three-dimensional
structures of disordered materials with diffusion models. <em>MLST</em>,
<em>5</em>(4), 045037. (<a
href="https://doi.org/10.1088/2632-2153/ad8c10">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectroscopy techniques such as x-ray absorption near edge structure (XANES) provide valuable insights into the atomic structures of materials, yet the inverse prediction of precise structures from spectroscopic data remains a formidable challenge. In this study, we introduce a framework that combines generative artificial intelligence models with XANES spectroscopy to predict three-dimensional atomic structures of disordered systems, using amorphous carbon ( a -C) as a model system. In this work, we introduce a new framework based on the diffusion model, a recent generative machine learning method, to predict 3D structures of disordered materials from a target property. For demonstration, we apply the model to identify the atomic structures of a -C as a representative material system from the target XANES spectra. We show that conditional generation guided by XANES spectra reproduces key features of the target structures. Furthermore, we show that our model can steer the generative process to tailor atomic arrangements for a specific XANES spectrum. Finally, our generative model exhibits a remarkable scale-agnostic property, thereby enabling generation of realistic, large-scale structures through learning from a small-scale dataset (i.e. with small unit cells). Our work represents a significant stride in bridging the gap between materials characterization and atomic structure determination; in addition, it can be leveraged for materials discovery in exploring various material properties as targeted.},
  archive      = {J_MLST},
  author       = {Hyuna Kwon and Tim Hsu and Wenyu Sun and Wonseok Jeong and Fikret Aydin and James Chapman and Xiao Chen and Vincenzo Lordi and Matthew R Carbone and Deyu Lu and Fei Zhou and Tuan Anh Pham},
  doi          = {10.1088/2632-2153/ad8c10},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {4},
  pages        = {045037},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Spectroscopy-guided discovery of three-dimensional structures of disordered materials with diffusion models},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Observation impact explanation in atmospheric state
estimation using hierarchical message-passing graph neural networks*.
<em>MLST</em>, <em>5</em>(4), 045036. (<a
href="https://doi.org/10.1088/2632-2153/ad8981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The impact of meteorological observations on weather forecasting varies with the sensor type, location, time, and other environmental factors. Thus, the quantitative analysis of observation impacts is crucial for the effective and efficient development of weather forecasting systems. However, existing impact analysis methods are dependent on specific forecast systems, because system-specific adjoint models are used and the sensitivity of the observation to the forecast is measured. This study investigates the impact of observations on atmospheric state estimation in weather forecasting systems by developing a novel graph neural network (GNN) model specialized for analyzing the heterogeneous relations between observations and atmospheric states. The observation impact can then be assessed by applying explainable methods to the proposed GNN model, which is independent of forecasting systems. Further, we develop a novel application called &#39;CloudNine,&#39; a system that provides impact analysis for individual observations with visualization. Our GNN model comprises hierarchical message-passing modules that separately analyze spatial correlations between observations at close locations and atmospheric states at close locations and then examine correlations between observations and atmospheric states. To consider the different factors influencing these correlations, we utilized geo-coordinates and types of observations in the attention mechanism of the modules with their feature vectors. We then applied gradient-based explainability methods to quantify the significance of the different observations in the estimation. Evaluated using data from 11 satellites and land-based observations, the results highlight the effectiveness of the proposed model and the visualization of observation impacts, enhancing the understanding and optimization of observational data in weather forecasting.},
  archive      = {J_MLST},
  author       = {Hyeon-Ju Jeon and Jeon-ho Kang and In-Hyuk Kwon and O-Joun Lee},
  doi          = {10.1088/2632-2153/ad8981},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {4},
  pages        = {045036},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Observation impact explanation in atmospheric state estimation using hierarchical message-passing graph neural networks*},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated learning with tensor networks: A quantum AI
framework for healthcare. <em>MLST</em>, <em>5</em>(4), 045035. (<a
href="https://doi.org/10.1088/2632-2153/ad8c11">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The healthcare industry frequently handles sensitive and proprietary data, and due to strict privacy regulations, it is often reluctant to share it directly. In today&#39;s context, Federated Learning (FL) stands out as a crucial remedy, facilitating the rapid advancement of distributed machine learning while effectively managing critical concerns regarding data privacy and governance. The fusion of federated learning and quantum computing represents a groundbreaking interdisciplinary approach with immense potential to revolutionize various industries, from healthcare to finance. In this work, we propose a federated learning framework based on quantum tensor networks (QTNs) that takes advantage of the principles of many-body quantum physics. Currently, there are no known classical tensor networks (TNs) implemented in federated settings. Furthermore, we investigated the effectiveness and feasibility of the proposed framework by conducting a differential privacy analysis to ensure the security of sensitive data across healthcare institutions. Experiments on popular medical image datasets show that the federated quantum tensor network (FedQTNs) model achieved a mean receiver-operator characteristic area under the curve of 91%–98%, outperforming several state-of-the-art federated learning methods. Moreover, QTN models require fewer parameters in FL settings compared to traditional classical models, which often suffer from over-parameterization. This reduction in parameters not only improves the efficiency of the communication process but also significantly decreases data consumption during training. As a result, QTN models facilitate a more effective and resource-efficient approach to training in decentralized environments with limited communication bandwidth. The FedQTN models demonstrate a smaller performance drop even when using strong differential privacy settings, maintaining higher accuracy compared to classical models under similar privacy constraints. Experimental results demonstrate that the quantum federated global model, consisting of highly entangled TN structures, showed better generalization and robustness and achieved higher testing accuracy, surpassing the performance of locally trained clients under unbalanced data distributions among healthcare institutions.},
  archive      = {J_MLST},
  author       = {Amandeep Singh Bhatia and David E Bernal Neira},
  doi          = {10.1088/2632-2153/ad8c11},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {4},
  pages        = {045035},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Federated learning with tensor networks: A quantum AI framework for healthcare},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust quantum dots charge autotuning using neural network
uncertainty. <em>MLST</em>, <em>5</em>(4), 045034. (<a
href="https://doi.org/10.1088/2632-2153/ad88d5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a machine learning-based procedure to automate the charge tuning of semiconductor spin qubits with minimal human intervention, addressing one of the significant challenges in scaling up quantum dot technologies. This method exploits artificial neural networks to identify noisy transition lines in stability diagrams, guiding a robust exploration strategy leveraging neural network uncertainty estimations. Tested across three distinct offline experimental datasets representing different single-quantum-dot technologies, this approach achieves a tuning success rate of over 99% in optimal cases, where more than 10% of the success is directly attributable to uncertainty exploitation. The challenging constraints of small training sets containing high diagram-to-diagram variability allowed us to evaluate the capabilities and limits of the proposed procedure.},
  archive      = {J_MLST},
  author       = {Victor Yon and Bastien Galaup and Claude Rohrbacher and Joffrey Rivard and Clément Godfrin and Ruoyu Li and Stefan Kubicek and Kristiaan De Greve and Louis Gaudreau and Eva Dupont-Ferrier and Yann Beilliard and Roger G Melko and Dominique Drouin},
  doi          = {10.1088/2632-2153/ad88d5},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {4},
  pages        = {045034},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Robust quantum dots charge autotuning using neural network uncertainty},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Geometric neural operators (gnps) for data-driven deep
learning in non-euclidean settings. <em>MLST</em>, <em>5</em>(4),
045033. (<a href="https://doi.org/10.1088/2632-2153/ad8980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Geometric Neural Operators (GNPs) for data-driven deep learning of geometric features for tasks in non-euclidean settings. We present a formulation for accounting for geometric contributions along with practical neural network architectures and factorizations for training. We then demonstrate how GNPs can be used (i) to estimate geometric properties, such as the metric and curvatures of surfaces, (ii) to approximate solutions of geometric partial differential equations on manifolds, and (iii) to solve Bayesian inverse problems for identifying manifold shapes. These results show a few ways GNPs can be used for incorporating the roles of geometry in the data-driven learning of operators.},
  archive      = {J_MLST},
  author       = {B Quackenbush and P J Atzberger},
  doi          = {10.1088/2632-2153/ad8980},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {4},
  pages        = {045033},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Geometric neural operators (gnps) for data-driven deep learning in non-euclidean settings},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large-scale gene expression data clustering through
incremental ensemble approach. <em>MLST</em>, <em>5</em>(4), 045032. (<a
href="https://doi.org/10.1088/2632-2153/ad81ca">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DNA microarray technology monitors gene activity in real-time in living organisms. It creates a large amount of data that helps scientists learn about how genes work. Clustering this data helps understand gene interactions and uncover important biological processes. However, the traditional clustering techniques have difficulties due to the enormous dimensionality of gene expression data and the intricacy of biological networks. Although ensemble clustering is a viable strategy, such high-dimensional data may not lend itself well to traditional approaches. This study introduces a novel technique for gene expression data clustering called incremental ensemble clustering for gene expression data (IECG). There are two steps in the IECG. A technique for grouping gene expression data into windows is presented in the first step, producing a tree of clusters. This procedure is carried out again for succeeding windows that have distinct feature sets. The base clusterings of two consecutive windows are ensembled using a new goal function to form a new clustering solution. By repeating this step-by-step method for further windows, reliable patterns that are beneficial for medical applications can be extracted. The results from both biological and non-biological data demonstrate that the proposed algorithm outperformed the state-of-the-art algorithms. Additionally, the running time of the proposed algorithm has been examined.},
  archive      = {J_MLST},
  author       = {Imran Khan and Abdul Khalique Shaikh and Naresh Adhikari},
  doi          = {10.1088/2632-2153/ad81ca},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {4},
  pages        = {045032},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Large-scale gene expression data clustering through incremental ensemble approach},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Designing the next generation of polymers with machine
learning and physics-based models. <em>MLST</em>, <em>5</em>(4), 045031.
(<a href="https://doi.org/10.1088/2632-2153/ad88d7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of next-generation polymers necessitates optimizing several key properties simultaneously, a task that is expensive and infeasible using traditional trial-and-error experimental approaches. A promising alternative is employing a combination of machine learning and physics-based tools to rapidly screen the polymer design space and provide suggestions of new polymers that meet the critical properties required for industrial applications. In this study, we introduce a comprehensive workflow that utilizes machine learning and molecular modeling approaches to design new polymers with the focus on improving five polymer properties: (1) glass transition temperature, (2) dielectric constant, (3) refractive index, (4) stress optic coefficient, and (5) linear coefficient of thermal expansion. Using a small dataset ( \lt 200 unique polymers), we developed quantitative structure-property relationships (QSPRs) models to accurately predict the experimental polymer properties for both homo- and co-polymer systems. We tested several ML algorithms and identified the best models for predicting these polymer properties, achieving test set R 2 greater than 0.77 across all properties. We then explored new polymers by creating a library of over ∼10 000 homopolymers using R-group enumeration tools and applied the trained QSPR models to rapidly predict the five polymer properties. The predictions of QSPR models were used to create a multi-parameter optimization score, which helped downselect the large polymer space to ∼10 promising candidates. The properties of these selected polymer candidates were subsequently validated with classical molecular dynamics simulations and density functional theory, revealing a strong correlation with the QSPR model predictions. Finally, one of the top candidates was validated by experiments, which showed good agreement against QSPR and physics-based models. Our workflow underscores the power of combining data-driven and theoretical methods in the polymer design process given a small dataset size, offering a valuable resource for experimentalists looking to leverage computer-aided strategies in materials innovation.},
  archive      = {J_MLST},
  author       = {Alex K Chew and Mohammad Atif Faiz Afzal and Anand Chandrasekaran and Jan Henk Kamps and Vaidya Ramakrishnan},
  doi          = {10.1088/2632-2153/ad88d7},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {4},
  pages        = {045031},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Designing the next generation of polymers with machine learning and physics-based models},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Molecular quantum chemical data sets and databases for
machine learning potentials. <em>MLST</em>, <em>5</em>(4), 041001. (<a
href="https://doi.org/10.1088/2632-2153/ad8f13">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of computational chemistry is increasingly leveraging machine learning (ML) potentials to predict molecular properties with high accuracy and efficiency, providing a viable alternative to traditional quantum mechanical (QM) methods, which are often computationally intensive. Central to the success of ML models is the quality and comprehensiveness of the data sets on which they are trained. Quantum chemistry data sets and databases, comprising extensive information on molecular structures, energies, forces, and other properties derived from QM calculations, are crucial for developing robust and generalizable ML potentials. In this review, we provide an overview of the current landscape of quantum chemical data sets and databases. We examine key characteristics and functionalities of prominent resources, including the types of information they store, the level of electronic structure theory employed, the diversity of chemical space covered, and the methodologies used for data creation. Additionally, an updatable resource is provided to track new data sets and databases at https://github.com/Arif-PhyChem/datasets_and_databases_4_MLPs . This resource also has the overview in a machine-readable database format with the Jupyter notebook example for analysis. Looking forward, we discuss the challenges associated with the rapid growth of quantum chemical data sets and databases, emphasizing the need for updatable and accessible resources to ensure the long-term utility of them. We also address the importance of data format standardization and the ongoing efforts to align with the FAIR principles to enhance data interoperability and reusability. Drawing inspiration from established materials databases, we advocate for the development of user-friendly and sustainable platforms for these data sets and databases.},
  archive      = {J_MLST},
  author       = {Arif Ullah and Yuxinxin Chen and Pavlo O Dral},
  doi          = {10.1088/2632-2153/ad8f13},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {4},
  pages        = {041001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Molecular quantum chemical data sets and databases for machine learning potentials},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Impact of data bias on machine learning for crystal compound
synthesizability predictions. <em>MLST</em>, <em>5</em>(4), 040501. (<a
href="https://doi.org/10.1088/2632-2153/ad9378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning models are susceptible to being misled by biases in training data that emphasize incidental correlations over the intended learning task. In this study, we demonstrate the impact of data bias on the performance of a machine learning model designed to predict the likelihood of synthesizability of crystal compounds. The model performs a binary classification on labeled crystal samples. Despite using the same architecture for the machine learning model, we showcase how the model&#39;s learning and prediction behavior differs once trained on distinct data. We use two data sets for illustration: a mixed-source data set that integrates experimental and computational crystal samples and a single-source data set consisting of data exclusively from one computational database. We present simple procedures to detect data bias and to evaluate its effect on the model&#39;s performance and generalization. This study reveals how inconsistent, unbalanced data can propagate bias, undermining real-world applicability even for advanced machine learning techniques.},
  archive      = {J_MLST},
  author       = {Ali Davariashtiyani and Busheng Wang and Samad Hajinazar and Eva Zurek and Sara Kadkhodaei},
  doi          = {10.1088/2632-2153/ad9378},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {4},
  pages        = {040501},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Impact of data bias on machine learning for crystal compound synthesizability predictions},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rapid likelihood free inference of compact binary
coalescences using accelerated hardware. <em>MLST</em>, <em>5</em>(4),
045030. (<a href="https://doi.org/10.1088/2632-2153/ad8982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We report a gravitational-wave parameter estimation algorithm, AMPLFI , based on likelihood-free inference using normalizing flows. The focus of AMPLFI is to perform real-time parameter estimation for candidates detected by machine-learning based compact binary coalescence search, Aframe . We present details of our algorithm and optimizations done related to data-loading and pre-processing on accelerated hardware. We train our model using binary black-hole (BBH) simulations on real LIGO-Virgo detector noise. Our model has {\sim}6 million trainable parameters with training times {\lesssim}24 h. Based on online deployment on a mock data stream of LIGO-Virgo data, Aframe + AMPLFI is able to pick up BBH candidates and infer parameters for real-time alerts from data acquisition with a net latency of {\sim}6 s.},
  archive      = {J_MLST},
  author       = {D Chatterjee and E Marx and W Benoit and R Kumar and M Desai and E Govorkova and A Gunny and E Moreno and R Omer and R Raikman and M Saleem and S Aggarwal and M W Coughlin and P Harris and E Katsavounidis},
  doi          = {10.1088/2632-2153/ad8982},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045030},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Rapid likelihood free inference of compact binary coalescences using accelerated hardware},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient ensemble uncertainty estimation in gaussian
processes regression. <em>MLST</em>, <em>5</em>(4), 045029. (<a
href="https://doi.org/10.1088/2632-2153/ad8984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable uncertainty measures are required when using data-based machine learning interatomic potentials (MLIPs) for atomistic simulations. In this work, we propose for sparse Gaussian process regression (GPR) type MLIPs a stochastic uncertainty measure akin to the query-by-committee approach often used in conjunction with neural network based MLIPs. The uncertainty measure is coined &#39;label noise&#39; ensemble uncertainty as it emerges from adding noise to the energy labels in the training data. We find that this method of calculating an ensemble uncertainty is as well calibrated as the one obtained from the closed-form expression for the posterior variance when the sparse GPR is treated as a projected process. Comparing the two methods, our proposed ensemble uncertainty is, however, faster to evaluate than the closed-form expression. Finally, we demonstrate that the proposed uncertainty measure acts better to support a Bayesian search for optimal structure of Au 20 clusters.},
  archive      = {J_MLST},
  author       = {Mads-Peter Verner Christiansen and Nikolaj Rønne and Bjørk Hammer},
  doi          = {10.1088/2632-2153/ad8984},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045029},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Efficient ensemble uncertainty estimation in gaussian processes regression},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PASCL: Supervised contrastive learning with perturbative
augmentation for particle decay reconstruction. <em>MLST</em>,
<em>5</em>(4), 045028. (<a
href="https://doi.org/10.1088/2632-2153/ad8060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high-energy physics, particles produced in collision events decay in a format of a hierarchical tree structure, where only the final decay products can be observed using detectors. However, the large combinatorial space of possible tree structures makes it challenging to recover the actual decay process given a set of final particles. To better analyse the hierarchical tree structure, we propose a graph-based deep learning model to infer the tree structure to reconstruct collision events. In particular, we use a compact matrix representation termed as lowest common ancestor generations matrix , to encode the particle decay tree structure. Then, we introduce a perturbative augmentation technique applied to node features, aiming to mimic experimental uncertainties and increase data diversity. We further propose a supervised graph contrastive learning algorithm to utilize the information of inter-particle relations from multiple decay processes. Extensive experiments show that our proposed supervised graph contrastive learning with perturbative augmentation method outperforms state-of-the-art baseline models on an existing physics-based dataset, significantly improving the reconstruction accuracy. This method provides a more effective training strategy for models with the same parameters and makes way for more accurate and efficient high-energy particle physics data analysis.},
  archive      = {J_MLST},
  author       = {Junjian Lu and Siwei Liu and Dmitrii Kobylianskii and Etienne Dreyer and Eilam Gross and Shangsong Liang},
  doi          = {10.1088/2632-2153/ad8060},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045028},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {PASCL: Supervised contrastive learning with perturbative augmentation for particle decay reconstruction},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring the potential of contemporary deep learning
methods in purifying polluted information. <em>MLST</em>, <em>5</em>(4),
045026. (<a href="https://doi.org/10.1088/2632-2153/ad8983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately predicting chaotic dynamical systems is a crucial task in various fields, and recent advancements have leveraged deep learning for this purpose. However, in the era of big data, the inevitable challenge of data contamination caused by invalid information from other interfering systems becomes increasingly prominent and complicates accurate predictions. Although contemporary deep learning methods have shown their potential, very few studies have focused on developing algorithms specifically designed to address the problem posed by such data contamination. Thus, exploring the ability of contemporary deep learning methods to purify polluted information fills an important gap in the current body of research. This study explores the performance and stability of several modern deep learning techniques for predicting chaotic systems using datasets polluted by invalid information. Our findings reveal that while most of the state-of-the-art deep learning methods exhibit reduced and unstable predictive performance owing to such contamination, the dynamical system deep learning (DSDL) method stands out, remaining unaffected by any interference. This breakthrough illustrates DSDL&#39;s unique ability to purify invalid data and uncover the inherent rules of chaotic systems. As we move forward, DSDL paves the way for a more reliable and interpretable model, ensuring that we can confidently predict chaotic systems with precision even in the most challenging environments.},
  archive      = {J_MLST},
  author       = {Mingyu Wang and Jianping Li},
  doi          = {10.1088/2632-2153/ad8983},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045026},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Exploring the potential of contemporary deep learning methods in purifying polluted information},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced deep learning-based water area segmentation for
flood detection and monitoring. <em>MLST</em>, <em>5</em>(4), 045025.
(<a href="https://doi.org/10.1088/2632-2153/ad8985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a hybrid architecture tailored for semantic segmentation challenges, mainly targeting the water area extraction for flood detection and monitoring. The model integrates an efficient transformer-based encoder, utilizing an efficient multi-head self-attention module for capturing hierarchical feature maps through a &#39;downsample-upsample&#39; strategy. The proposed decoder architecture comprises one feature refinement head block and three CNN-based dual-branch context blocks. The convolutional block attention module is employed within the feature refinement head block to refine feature representation. The depth-wise separable atrous spatial pyramid pooling module is central to this architecture, facilitating efficient multi-scale contextual information capture. Compared to the state-of-the-art models, our model and the PSPNet model obtained the highest precision, recall, and F1-scores of above 80%, and mIoU surpassing 70%. The proposed method outperformed PSPNet in recall, F1-score, mIoU, and pixel accuracy, albeit with a slight deficit in precision. In terms of scale and efficiency, compared to the PSPNet model, our model has lower complexity and slightly higher inference speed, highlighting its effectiveness and efficiency in the water area segmentation for flood detection. The source code is available at https://github.com/manhhv87/mmsegmentation.git .},
  archive      = {J_MLST},
  author       = {Thang M Pham and Nam Do and Hanh T Bui and Manh V Hoang},
  doi          = {10.1088/2632-2153/ad8985},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045025},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Enhanced deep learning-based water area segmentation for flood detection and monitoring},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine-learning-based diabetes classification method using
blood flow oscillations and pearson correlation analysis of feature
importance. <em>MLST</em>, <em>5</em>(4), 045024. (<a
href="https://doi.org/10.1088/2632-2153/ad861d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetes is a global health issue affecting millions of people and is related to high morbidity and mortality rates. Current diagnostic methods are primarily invasive, involving blood sampling, which can lead to infection and increased patient stress. As a result, there is a growing need for noninvasive diabetes diagnostic methods that are both accurate and fast. High measurement accuracy and fast measurement time are essential for effective noninvasive diabetes diagnosis; these can be achieved using diffuse speckle contrast analysis (DSCA) systems and artificial intelligence algorithms. In this study, we use a machine learning algorithm to analyze rat blood flow signals measured using a DSCA system with simple operation, easy fabrication, and fast measurement for helping diagnose diabetes. The results confirmed that the machine learning algorithm for analyzing blood flow oscillation data shows good potential for diabetes classification. Furthermore, analyzing the blood flow reactivity test revealed that blood flow signals can be quickly measured for diabetes classification. Finally, we evaluated the influence of each blood flow oscillation data on diabetes classification through feature importance and Pearson correlation analysis. The results of this study should provide a basis for the future development of hemodynamic-based disease diagnostic methods.},
  archive      = {J_MLST},
  author       = {Hanbeen Jung and Chaebeom Yeo and Eunsil Jang and Yeonhee Chang and Cheol Song},
  doi          = {10.1088/2632-2153/ad861d},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045024},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine-learning-based diabetes classification method using blood flow oscillations and pearson correlation analysis of feature importance},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Chemical space-informed machine learning models for rapid
predictions of x-ray photoelectron spectra of organic molecules.
<em>MLST</em>, <em>5</em>(4), 045023. (<a
href="https://doi.org/10.1088/2632-2153/ad871d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present machine learning models based on kernel-ridge regression for predicting x-ray photoelectron spectra of organic molecules originating from the K -shell ionization energies of carbon (C), nitrogen (N), oxygen (O), and fluorine (F) atoms. We constructed the training dataset through high-throughput calculations of K -shell core-electron binding energies (CEBEs) for 12 880 small organic molecules in the bigQM7 ω dataset, employing the Δ-SCF formalism coupled with meta-GGA-DFT and a variationally converged basis set. The models are cost-effective, as they require the atomic coordinates of a molecule generated using universal force fields while estimating the target-level CEBEs corresponding to DFT-level equilibrium geometry. We explore transfer learning by utilizing the atomic environment feature vectors learned using a graph neural network framework in kernel-ridge regression. Additionally, we enhance accuracy within the Δ-machine learning framework by leveraging inexpensive baseline spectra derived from Kohn–Sham eigenvalues. When applied to 208 combinatorially substituted uracil molecules larger than those in the training set, our analyses suggest that the models may not provide quantitatively accurate predictions of CEBEs but offer a strong linear correlation relevant for virtual high-throughput screening. We present the dataset and models as the Python module, cebeconf , to facilitate further explorations.},
  archive      = {J_MLST},
  author       = {Susmita Tripathy and Surajit Das and Shweta Jindal and Raghunathan Ramakrishnan},
  doi          = {10.1088/2632-2153/ad871d},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045023},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Chemical space-informed machine learning models for rapid predictions of x-ray photoelectron spectra of organic molecules},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning phase retrieval in x-ray single-particle
imaging for biological macromolecules. <em>MLST</em>, <em>5</em>(4),
045022. (<a href="https://doi.org/10.1088/2632-2153/ad7f22">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phase retrieval is an important optimization problem that occurs, for example, in the analysis of coherent diffraction patterns from isolated proteins. All iterative algorithms employed for phase retrieval in this context require some a priori knowledge of the object, usually in the form of a support that describes the extent of the particle. Phase retrieval is a time-consuming task that can often fail, particularly if the support is too loose or of bad quality. This paper presents a neural network that can produce low-resolution estimates of the phased object in a fraction of the time it takes for a full phase retrieval. It can also successfully be used as support for further analysis. Our network is trained on simulated data from biological macromolecules and is thus tailored to the type of data seen in a typical CDI experiment. Other approaches to support finding require very accurate data without missing regions or the full phase-retrieval algorithm to be run for a long time. Our network could speed up offline analysis and provide real-time feedback during data collection.},
  archive      = {J_MLST},
  author       = {Alfredo Bellisario and Tomas Ekeberg},
  doi          = {10.1088/2632-2153/ad7f22},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045022},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deep learning phase retrieval in x-ray single-particle imaging for biological macromolecules},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unpaired image translation to mitigate domain shift in
liquid argon time projection chamber detector responses. <em>MLST</em>,
<em>5</em>(4), 045021. (<a
href="https://doi.org/10.1088/2632-2153/ad849c">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning algorithms often are developed and trained on a training dataset and deployed on test datasets. Any systematic difference between the training and a test dataset may severely degrade the final algorithm performance on the test dataset—what is known as the domain shift problem . This issue is prevalent in many scientific domains where algorithms are trained on simulated data but applied to real-world datasets. Typically, the domain shift problem is solved through various domain adaptation (DA) methods. However, these methods are often tailored for a specific downstream task, such as classification or semantic segmentation, and may not easily generalize to different tasks. This work explores the feasibility of using an alternative way to solve the domain shift problem that is not specific to any downstream algorithm. The proposed approach relies on modern Unpaired Image-to-Image (UI2I) translation techniques, designed to find translations between different image domains in a fully unsupervised fashion. In this study, the approach is applied to a domain shift problem commonly encountered in Liquid Argon Time Projection Chamber (LArTPC) detector research when seeking a way to translate samples between two differently distributed LArTPC detector datasets deterministically. This translation allows for mapping real-world data into the simulated data domain where the downstream algorithms can be run with much less domain-shift-related performance degradation. Conversely, using the translation from the simulated data to a real-world domain can increase the realism of the simulated dataset and reduce the magnitude of any systematic uncertainties. To evaluate the quality of the translations, we use both pixel-wise metrics and a downstream task to measure the effectiveness of UI2I methods for mitigating the domain shift problem. We adapted several popular UI2I translation algorithms to work on scientific data and demonstrated the viability of these techniques for solving the domain shift problem with LArTPC detector data. To facilitate further development of DA techniques for scientific datasets, the &#39;Simple Liquid-Argon Track Samples&#39; dataset used in this study is also published.},
  archive      = {J_MLST},
  author       = {Yi Huang and Dmitrii Torbunov and Brett Viren and Haiwang Yu and Jin Huang and Meifeng Lin and Yihui Ren},
  doi          = {10.1088/2632-2153/ad849c},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045021},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Unpaired image translation to mitigate domain shift in liquid argon time projection chamber detector responses},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Metric flows with neural networks. <em>MLST</em>,
<em>5</em>(4), 045020. (<a
href="https://doi.org/10.1088/2632-2153/ad8533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a general theory of flows in the space of Riemannian metrics induced by neural network (NN) gradient descent. This is motivated in part by recent advances in approximating Calabi–Yau metrics with NNs and is enabled by recent advances in understanding flows in the space of NNs. We derive the corresponding metric flow equations, which are governed by a metric neural tangent kernel (NTK), a complicated, non-local object that evolves in time. However, many architectures admit an infinite-width limit in which the kernel becomes fixed and the dynamics simplify. Additional assumptions can induce locality in the flow, which allows for the realization of Perelman&#39;s formulation of Ricci flow that was used to resolve the 3d Poincaré conjecture. We demonstrate that such fixed kernel regimes lead to poor learning of numerical Calabi–Yau metrics, as is expected since the associated NNs do not learn features. Conversely, we demonstrate that well-learned numerical metrics at finite-width exhibit an evolving metric-NTK, associated with feature learning. Our theory of NN metric flows therefore explains why NNs are better at learning Calabi–Yau metrics than fixed kernel methods, such as the Ricci flow.},
  archive      = {J_MLST},
  author       = {James Halverson and Fabian Ruehle},
  doi          = {10.1088/2632-2153/ad8533},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045020},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Metric flows with neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Training machine learning interatomic potentials for
accurate phonon properties. <em>MLST</em>, <em>5</em>(4), 045019. (<a
href="https://doi.org/10.1088/2632-2153/ad86a1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the major challenges in the development of universal machine learning interatomic potentials is accurately reproducing phonon properties. This issue appears to arise from the limitations of available datasets rather than the models themselves. To address this, we develop an extensive dataset of phonon calculations using density-functional perturbation theory (DFPT). We then show how this dataset can be used to train neural-network force fields, by implementing the training and the prediction of force constants in periodic crystals. This approach improves the quality of phonon properties prediction while reducing the number of structures needed for neural network training. We demonstrate the efficiency of this method using two examples of ternary phase diagrams: Ti–Nb–Ta and Li–B–C. In both cases, neural network predictions for the energy and forces show a considerable improvement, while phonon properties are predicted with high precision for all structures across the entire phase diagrams.},
  archive      = {J_MLST},
  author       = {Antoine Loew and Hai-Chen Wang and Tiago F T Cerqueira and Miguel A L Marques},
  doi          = {10.1088/2632-2153/ad86a1},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045019},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Training machine learning interatomic potentials for accurate phonon properties},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A prediction rigidity formalism for low-cost uncertainties
in trained neural networks. <em>MLST</em>, <em>5</em>(4), 045018. (<a
href="https://doi.org/10.1088/2632-2153/ad805f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantifying the uncertainty of regression models is essential to ensure their reliability, particularly since their application often extends beyond their training domain. Based on the solution of a constrained optimization problem, this work proposes &#39;prediction rigidities&#39; as a formalism to obtain uncertainties of arbitrary pre-trained regressors. A clear connection between the suggested framework and Bayesian inference is established, and a last-layer approximation is developed and rigorously justified to enable the application of the method to neural networks. This extension affords cheap uncertainties without any modification to the neural network itself or its training procedure. The effectiveness of this approach is shown for a wide range of regression tasks, ranging from simple toy models to applications in chemistry and meteorology.},
  archive      = {J_MLST},
  author       = {Filippo Bigi and Sanggyu Chong and Michele Ceriotti and Federico Grasselli},
  doi          = {10.1088/2632-2153/ad805f},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045018},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A prediction rigidity formalism for low-cost uncertainties in trained neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extreme time extrapolation capabilities and thermodynamic
consistency of physics-inspired neural networks for the 3D
microstructure evolution of materials via cahn–hilliard flow.
<em>MLST</em>, <em>5</em>(4), 045017. (<a
href="https://doi.org/10.1088/2632-2153/ad8532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Convolutional Recurrent Neural Network (CRNN) is trained to reproduce the evolution of the spinodal decomposition process in three dimensions as described by the Cahn–Hilliard equation. A specialized, physics-inspired architecture is proven to provide close accordance between the predicted evolutions and the ground truth ones obtained via conventional integration schemes. The method can accurately reproduce the evolution of microstructures not represented in the training set at a fraction of the computational costs. Extremely long-time extrapolation capabilities are achieved, up to reaching the theoretically expected equilibrium state of the system, consisting of a layered, phase-separated morphology, despite the training set containing only relatively-short, initial phases of the evolution. Quantitative accordance with the decay rate of the free energy is also demonstrated up to the late coarsening stages, proving that this class of machine learning approaches can become a new and powerful tool for the long timescale and high throughput simulation of materials, while retaining thermodynamic consistency and high-accuracy.},
  archive      = {J_MLST},
  author       = {Daniele Lanzoni and Andrea Fantasia and Roberto Bergamaschini and Olivier Pierre-Louis and Francesco Montalenti},
  doi          = {10.1088/2632-2153/ad8532},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045017},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Extreme time extrapolation capabilities and thermodynamic consistency of physics-inspired neural networks for the 3D microstructure evolution of materials via Cahn–Hilliard flow},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IDEEA: Information diffusion model for integrating gene
expression and EEG data in identifying alzheimer’s disease markers.
<em>MLST</em>, <em>5</em>(4), 045016. (<a
href="https://doi.org/10.1088/2632-2153/ad829d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the genetic components of Alzheimer&#39;s disease (AD) via transcriptome analysis often necessitates the use of invasive methods. This work focuses on overcoming the difficulties associated with the invasive process of collecting brain tissue samples in order to measure and investigate the transcriptome behavior of AD. Our approach called IDEEA ( I nformation D iffusion model for integrating gene E xpression and E EG data in identifying A lzheimer&#39;s disease markers) involves systematically linking two different but complementary modalities: transcriptomics and electroencephalogram (EEG) data. We preprocess these two data types by calculating the spectral and transcriptional sample distances, over 11 brain regions encompassing 6 distinct frequency bands. Subsequently, we employ a genetic algorithm approach to integrate the distinct features of the preprocessed data. Our experimental results show that IDEEA converges rapidly to local optima gene subsets, in fewer than 250 iterations. Our algorithm identifies novel genes along with genes that have previously been linked to AD. It is also capable of detecting genes with transcription patterns specific to individual EEG bands as well as those with common patterns among bands. In particular, the alpha2 (10–13 Hz) frequency band yielded 8 AD-associated genes out of the top 100 most frequently selected genes by our algorithm, with a p -value of 0.05. Our method not only identifies AD-related genes but also genes that interact with AD genes in terms of transcription regulation. We evaluated various aspects of our approach, including the genetic algorithm performance, band-pair association and gene interaction topology. Our approach reveals AD-relevant genes with transcription patterns inferred from EEG alone, across various frequency bands, avoiding the risky brain tissue collection process. This is a significant advancement toward the early identification of AD using non-invasive EEG recordings.},
  archive      = {J_MLST},
  author       = {Enes Ozelbas and Tuba Sevimoglu and Tamer Kahveci},
  doi          = {10.1088/2632-2153/ad829d},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045016},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {IDEEA: Information diffusion model for integrating gene expression and EEG data in identifying alzheimer’s disease markers},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-fidelity gaussian process surrogate modeling for
regression problems in physics. <em>MLST</em>, <em>5</em>(4), 045015.
(<a href="https://doi.org/10.1088/2632-2153/ad7ad5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main challenges in surrogate modeling is the limited availability of data due to resource constraints associated with computationally expensive simulations. Multi-fidelity methods provide a solution by chaining models in a hierarchy with increasing fidelity, associated with lower error, but increasing cost. In this paper, we compare different multi-fidelity methods employed in constructing Gaussian process surrogates for regression. Non-linear autoregressive methods in the existing literature are primarily confined to two-fidelity models, and we extend these methods to handle more than two levels of fidelity. Additionally, we propose enhancements for an existing method incorporating delay terms by introducing a structured kernel. We demonstrate the performance of these methods across various academic and real-world scenarios. Our findings reveal that multi-fidelity methods generally have a smaller prediction error for the same computational cost as compared to the single-fidelity method, although their effectiveness varies across different scenarios.},
  archive      = {J_MLST},
  author       = {Kislaya Ravi and Vladyslav Fediukov and Felix Dietrich and Tobias Neckel and Fabian Buse and Michael Bergmann and Hans-Joachim Bungartz},
  doi          = {10.1088/2632-2153/ad7ad5},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045015},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Multi-fidelity gaussian process surrogate modeling for regression problems in physics},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mapping causal pathways with structural modes fingerprint
for perovskite oxides. <em>MLST</em>, <em>5</em>(4), 045014. (<a
href="https://doi.org/10.1088/2632-2153/ad7d5e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causality is innate to the determination of the fundamental mechanism controlling any physical phenomena. However, combining causality within the standard practices of computational modelling to understand structure-functionality connections is extremely rare. This work proposes a fingerprint based on key structural modes for ABO 3 -type perovskite oxides and its derivatives, combined with causal models, for predicting Kohn–Sham energies. Our study of causal models captures the inherent coupling between structural modes such as rotation, tilt and antiferroelectric displacements, responsible for phase transition, polarization, magnetization and metal–insulator transition, exhibited by these materials. Although developed for modelling specific functionality, this method is universally applicable to derive other functionalities and even different material classes while tracking hidden causal mechanisms via structural distortions.},
  archive      = {J_MLST},
  author       = {Ayana Ghosh and Saurabh Ghosh},
  doi          = {10.1088/2632-2153/ad7d5e},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045014},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Mapping causal pathways with structural modes fingerprint for perovskite oxides},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quality assurance for online adaptive radiotherapy: A
secondary dose verification model with geometry-encoded u-net.
<em>MLST</em>, <em>5</em>(4), 045013. (<a
href="https://doi.org/10.1088/2632-2153/ad829e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In online adaptive radiotherapy (ART), quick computation-based secondary dose verification is crucial for ensuring the quality of ART plans while the patient is positioned on the treatment couch. However, traditional dose verification algorithms are generally time-consuming, reducing the efficiency of ART workflow. This study aims to develop an ultra-fast deep-learning (DL) based secondary dose verification algorithm to accurately estimate dose distributions using computed tomography (CT) and fluence maps (FMs). We integrated FMs into the CT image domain by explicitly resolving the geometry of treatment delivery. For each gantry angle, an FM was constructed based on the optimized multi-leaf collimator apertures and corresponding monitoring units. To effectively encode treatment beam configuration, the constructed FMs were back-projected to 30 cm away from the isocenter with respect to the exact geometry of the treatment machines. Then, a 3D U-Net was utilized to take the integrated CT and FM volume as input to estimate dose. Training and validation were performed on 381 prostate cancer cases, with an additional 40 testing cases for independent evaluation of model performance. The proposed model can estimate dose in ∼ 15 ms for each patient. The average γ passing rate ( 3\% /2\,{\text{mm}} , 10\% threshold) for the estimated dose was 99.9% ± 0.15% on testing patients. The mean dose differences for the planning target volume and organs at risk were 0.07\% \pm 0.34\% and 0.48\% \pm 0.72\% , respectively. We have developed a geometry-resolved DL framework for accurate dose estimation and demonstrated its potential in real-time online ART doses verification.},
  archive      = {J_MLST},
  author       = {Shunyu Yan and Austen Maniscalco and Biling Wang and Dan Nguyen and Steve Jiang and Chenyang Shen},
  doi          = {10.1088/2632-2153/ad829e},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045013},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Quality assurance for online adaptive radiotherapy: A secondary dose verification model with geometry-encoded U-net},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Planning with tensor networks based on active inference.
<em>MLST</em>, <em>5</em>(4), 045012. (<a
href="https://doi.org/10.1088/2632-2153/ad7571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor networks (TNs) have seen an increase in applications in recent years. While they were originally developed to model many-body quantum systems, their usage has expanded into the field of machine learning. This work adds to the growing range of applications by focusing on planning by combining the generative modeling capabilities of matrix product states and the action selection algorithm provided by active inference. Their ability to deal with the curse of dimensionality, to represent probability distributions, and to dynamically discover hidden variables make matrix product states specifically an interesting choice to use as the generative model in active inference, which relies on &#39;beliefs&#39; about hidden states within an environment. We evaluate our method on the T-maze and Frozen Lake environments, and show that the TN-based agent acts Bayes optimally as expected under active inference.},
  archive      = {J_MLST},
  author       = {Samuel T Wauthier and Tim Verbelen and Bart Dhoedt and Bram Vanhecke},
  doi          = {10.1088/2632-2153/ad7571},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045012},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Planning with tensor networks based on active inference},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-driven acceleration of multi-physics simulations.
<em>MLST</em>, <em>5</em>(4), 045011. (<a
href="https://doi.org/10.1088/2632-2153/ad7572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-physics simulations play a crucial role in understanding complex systems. However, their computational demands are often prohibitive due to high dimensionality and complex interactions, such that actual calculations often rely on approximations. To address this, we introduce a data-driven approach to approximate interactions among degrees of freedom of no direct interest and thus significantly reduce computational costs. Focusing on a semiconductor laser as a case study, we demonstrate the superiority of this method over traditional analytical approximations in both accuracy and efficiency. Our approach streamlines simulations, offering promise for complex multi-physics systems, especially for scenarios requiring a large number of individual simulations.},
  archive      = {J_MLST},
  author       = {Stefan Meinecke and Malte Selig and Felix Köster and Andreas Knorr and Kathy Lüdge},
  doi          = {10.1088/2632-2153/ad7572},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045011},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Data-driven acceleration of multi-physics simulations},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Application of deep learning-based fuzzy systems to analyze
the overall risk of mortality in glioblastoma multiforme. <em>MLST</em>,
<em>5</em>(4), 045010. (<a
href="https://doi.org/10.1088/2632-2153/ad67a9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glioblastoma multiforme (GBM) is the most aggressive brain cancer in adults, with 3.2–3.4 cases per 100 thousand. In the US, brain cancer does not rank in the top 10 causes of death, but it remains in the top 15. Therefore, this research proposes a fuzzy-based GRUCoxPH model to identify missense variants associated with a high risk of all-cause mortality in GBM. The study combines various models, including fuzzy logic, gated recurrent units (GRUs), and Cox proportional hazards regression (CoxPh), to identify potential risk factors. The dataset is derived from TCGA-GBM clinicopathological information and mutations to create four risk score models: GRU, CoxPH, GRUCoxPH Addition , and GRUCoxPH Multiplication , analyzing nine risk factors of the dataset. The Fuzzy-based GRUCoxPH model achieves an average accuracy of 87.64%, outperforming other models. This model demonstrates its ability to classify and identify missense variants associated with mortality in GBM, potentially advancing cancer research.},
  archive      = {J_MLST},
  author       = {Cheng-Hong Yang and Tin-Ho Cheung and Li-Yeh Chuang},
  doi          = {10.1088/2632-2153/ad67a9},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045010},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Application of deep learning-based fuzzy systems to analyze the overall risk of mortality in glioblastoma multiforme},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distance preserving machine learning for uncertainty aware
accelerator capacitance predictions. <em>MLST</em>, <em>5</em>(4),
045009. (<a href="https://doi.org/10.1088/2632-2153/ad7cbf">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate uncertainty estimations are essential for producing reliable machine learning models, especially in safety-critical applications such as accelerator systems. Gaussian process models are generally regarded as the gold standard for this task; however, they can struggle with large, high-dimensional datasets. Combining deep neural networks with Gaussian process approximation techniques has shown promising results, but dimensionality reduction through standard deep neural network layers is not guaranteed to maintain the distance information necessary for Gaussian process models. We build on previous work by comparing the use of the singular value decomposition against a spectral-normalized dense layer as a feature extractor for a deep neural Gaussian process approximation model and apply it to a capacitance prediction problem for the High Voltage Converter Modulators in the Oak Ridge Spallation Neutron Source. Our model shows improved distance preservation and predicts in-distribution capacitance values with less than 1% error.},
  archive      = {J_MLST},
  author       = {Steven Goldenberg and Malachi Schram and Kishansingh Rajput and Thomas Britton and Chris Pappas and Dan Lu and Jared Walden and Majdi I Radaideh and Sarah Cousineau and Sudarshan Harave},
  doi          = {10.1088/2632-2153/ad7cbf},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045009},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Distance preserving machine learning for uncertainty aware accelerator capacitance predictions},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comparison of deep learning models for proton background
rejection with the AMS electromagnetic calorimeter. <em>MLST</em>,
<em>5</em>(4), 045008. (<a
href="https://doi.org/10.1088/2632-2153/ad7cc0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The alpha magnetic spectrometer (AMS) is a high-precision particle detector onboard the International Space Station containing six different subdetectors. The transition radiation detector and electromagnetic calorimeter (ECAL) are used to separate electrons/positrons from the abundant cosmic-ray proton background. The positron flux measured in space by AMS falls with a power law which unexpectedly softens above 25 GeV and then hardens above 280 GeV. Several theoretical models try to explain these phenomena, and a more accurate measurement of positrons at higher energies is needed to help test them. The currently used methods to reject the proton background at high energies involve extrapolating shower features from the ECAL to use as inputs for boosted decision tree and likelihood classifiers. We present a new approach for particle identification with the AMS ECAL using deep learning (DL). By taking the energy deposition within all the ECAL cells as an input and treating them as pixels in an image-like format, we train an MLP, a CNN, and multiple ResNets and convolutional vision transformers (CvTs) as shower classifiers. Proton rejection performance is evaluated using Monte Carlo (MC) events and ISS data separately. For MC, using events with a reconstructed energy between 0.2–2 TeV, at 90% electron accuracy, the proton rejection power of our CvT model is more than five times that of the other DL models. Similarly, for ISS data with a reconstructed energy between 50–70 GeV, the proton rejection power of our CvT model is more than 2.5 times that of the other DL models.},
  archive      = {J_MLST},
  author       = {R K Hashmani and E Akbas and M B Demirköz},
  doi          = {10.1088/2632-2153/ad7cc0},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045008},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A comparison of deep learning models for proton background rejection with the AMS electromagnetic calorimeter},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prediction of flow stress of ta–w alloys using machine
learning. <em>MLST</em>, <em>5</em>(4), 045007. (<a
href="https://doi.org/10.1088/2632-2153/ad8061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary aim of this article was to predict the flow stress of Ta–W alloys using the eXtreme Gradient Boosting (XGBoost) machine learning model and to explain the outcome using SHapley Additive exPlanations (SHAP). The article details the effect of temperature, strain rate, and alloying content on the deformation behavior. Though grain size, dislocation density, texture and impurities are also important factors affecting the deformation behavior, these have not been considered in this work. Data and constitutive models from the literature were used to find and compare the predictiveness of the flow stress in Ta–W alloys. XGBoost predicted flow stress with a root mean square error of 12 MPa during training and 40 MPa during testing, while constitutive models such as Johnson–Cook (JC), Zerilli–Armstrong (ZA) and mechanical threshold stress (MTS) models showed a root mean square error of 208, 131 and 149 MPa respectively. The linear correlation between the predicted and experimental flow stress at 10% strain was calculated using the Pearson correlation coefficient and found to be 0.64, 0.93, and 0.70 for JC, ZA and MTS models respectively, while XGBoost showed 0.99 during training and 0.98 during testing. The optimized XGBoost model was validated using five-fold and leave-one-group-out cross-validations. The flow stress at 10% strain was predicted using XGBoost at various temperatures, strain rates, and alloying content. The flow stress was low at temperatures above 1000 K and strain rates below 10 −2 s −1 . From SHAP analysis, it was found that the base flow stress value (at which the SHAP value is zero) was 477 MPa. For temperatures less than 275 K, strain rates greater than 1 s −1 , and alloying content greater than 2.5 wt.% W, the flow stress showed an increase from its base value.},
  archive      = {J_MLST},
  author       = {A Kedharnath and Rajeev Kapoor and Apu Sarkar},
  doi          = {10.1088/2632-2153/ad8061},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045007},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Prediction of flow stress of Ta–W alloys using machine learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Local classification of crystalline structures in complex
plasmas using a PointNet. <em>MLST</em>, <em>5</em>(4), 045006. (<a
href="https://doi.org/10.1088/2632-2153/ad8062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In complex plasmas, microparticles can form ordered crystalline structures under specific conditions. Accurately identifying these structures, such as face-centered cubic, hexagonal close-packed, and body-centered cubic, is a common task in physics. Previous methods rely on detecting symmetries in the spatial arrangement of particles, often requiring extensive calculations. This study presents a novel approach by utilizing a PointNet-based deep learning algorithm, called WignerNet, to classify these structures directly from three-dimensional reconstructions of their Voronoi cells. The model was trained exclusively on artificial and labeled data, incorporating various noise levels, and subsequently tested on real experimental data. The results demonstrate that our method effectively classifies structures, reducing computational complexity and improving accuracy compared to conventional techniques. This advancement opens up new possibilities for real-time analysis of complex plasma systems in various research.},
  archive      = {J_MLST},
  author       = {N Dormagen and M Klein and A S Schmitz and L Wimmer and M H Thoma and M Schwarz},
  doi          = {10.1088/2632-2153/ad8062},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045006},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Local classification of crystalline structures in complex plasmas using a PointNet},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessing non-nested configurations of multifidelity machine
learning for quantum-chemical properties. <em>MLST</em>, <em>5</em>(4),
045005. (<a href="https://doi.org/10.1088/2632-2153/ad7f25">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multifidelity machine learning (MFML) for quantum chemical properties has seen strong development in the recent years. The method has been shown to reduce the cost of generating training data for high-accuracy low-cost ML models. In such a set-up, the ML models are trained on molecular geometries and some property of interest computed at various computational chemistry accuracies, or fidelities. These are then combined in training the MFML models. In some multifidelity models, the training data is required to be nested, that is the same molecular geometries are included to calculate the property across all the fidelities. In these multifidelity models, the requirement of a nested configuration restricts the kind of sampling that can be performed while selection training samples at different fidelities. This work assesses the use of non-nested training data for two of these multifidelity methods, namely MFML and optimized MFML (o-MFML). The assessment is carried out for the prediction of ground state energies and first vertical excitation energies of a diverse collection of molecules of the CheMFi dataset. Results indicate that the MFML method still requires a nested structure of training data across the fidelities. However, the o-MFML method shows promising results for non-nested multifidelity training data with model errors comparable to the nested configurations.},
  archive      = {J_MLST},
  author       = {Vivin Vinod and Peter Zaspel},
  doi          = {10.1088/2632-2153/ad7f25},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045005},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Assessing non-nested configurations of multifidelity machine learning for quantum-chemical properties},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decision tree insights analytics (DTIA) tool: An analytic
framework to identify insights from large data records across fields of
science. <em>MLST</em>, <em>5</em>(4), 045004. (<a
href="https://doi.org/10.1088/2632-2153/ad7f23">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised machine learning (SML) techniques have been developed since the 1960s. Most of their applications were oriented towards developing models capable of predicting numerical values or categorical output based on a set of input variables (input features). Recently, SML models&#39; interpretability and explainability were extensively studied to have confidence in the models&#39; decisions. In this work, we propose a new deployment method named Decision Tree Insights Analytics (DTIA) that shifts the purpose of using decision tree classification from having a model capable of differentiating the different categorical outputs based on the input features to systematically finding the associations between inputs and outputs. DTIA can reveal interesting areas in the feature space, leading to the development of research questions and the discovery of new associations that might have been overlooked earlier. We applied the method to three case studies: (1) nuclear reactor accident propagation, (2) single-cell RNA sequencing of Niemann-Pick disease type C1 in mice, and (3) bulk RNA sequencing for breast cancer staging in humans. The developed method provided insights into the first two. On the other hand, it showed some of the method&#39;s limitations in the third case study. Finally, we presented how the DTIA&#39;s insights are more agreeable with the abstract information gain calculations and provide more in-depth information that can help derive more profound physical meaning compared to the random forest&#39;s feature importance attribute and K-means clustering for feature ranking.},
  archive      = {J_MLST},
  author       = {Karim Hossny and Mohammed Hossny and Antony Cougnoux and Loay Mahmoud and Walter Villanueva},
  doi          = {10.1088/2632-2153/ad7f23},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045004},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Decision tree insights analytics (DTIA) tool: An analytic framework to identify insights from large data records across fields of science},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advancements in prostate zone segmentation: Integrating
attention mechanisms into the nnU-net framework. <em>MLST</em>,
<em>5</em>(4), 045003. (<a
href="https://doi.org/10.1088/2632-2153/ad7f24">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prostate cancer is one of the most lethal cancers in the world. Early diagnosis is essential for successful treatment of prostate cancer. Segmentation of prostate zones in magnetic resonance images is an important task in the diagnosis of prostate cancer. Currently, the state-of-the-art method for this task is no-new U-Net. In this paper, a method to incorporate the attention U-Net architecture into no-new U-Net is proposed and compared with a classical U-net architecture as research. The experimental results indicate that there is no significant statistical difference between the proposed modification of no-new U-Net with the generalizability of the attention mechanism or the ability to achieve more accurate results. Moreover, two novel workflows are proposed for prostate segmentation, transitional zone segmentation and peripheral zone calculation workflow, and separate models for peripheral zone and transitional zone segmentation workflow. These workflows are compared with a baseline single peripheral zone and transitional zone segmentation model workflow. The experimental results indicate that separate models for peripheral zone and transitional zone segmentation workflow generalizes better than the baseline between data sets of different sources. In peripheral zone segmentation separate models for peripheral zone and transitional zone segmentation workflow achieves 1.9% higher median Dice score coefficient than the baseline workflow when using the attention U-Net architecture and 5.6% higher median Dice score coefficient when using U-Net architecture. Moreover, in transitional zone segmentation separate models for peripheral zone and transitional zone segmentation workflow achieves 0.4% higher median Dice score coefficient than the baseline workflow when using the attention U-Net architecture and 0.7% higher median Dice score coefficient when using U-Net architecture. Meanwhile, prostate segmentation, transitional zone segmentation and peripheral zone calculation workflow generalizes worse than the baseline. In peripheral zone segmentation prostate segmentation, transitional zone segmentation and peripheral zone calculation workflow achieves 4.6% lower median Dice score coefficient than the baseline workflow when using the attention U-Net architecture and 3.6% lower median Dice score coefficient when using U-Net architecture. In transitional zone segmentation prostate segmentation, transitional zone segmentation and peripheral zone calculation workflow achieves a similar median Dice score coefficient to the baseline workflow.},
  archive      = {J_MLST},
  author       = {Aleksas Vaitulevičius and Jolita Bernatavičienė and Jurgita Markevičiutė and Ieva Naruševičiūtė and Mantas Trakymas and Povilas Treigys},
  doi          = {10.1088/2632-2153/ad7f24},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045003},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Advancements in prostate zone segmentation: Integrating attention mechanisms into the nnU-net framework},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning stochastic differential equations for the
evolution of order parameters of classical many-body systems in and out
of equilibrium. <em>MLST</em>, <em>5</em>(4), 045002. (<a
href="https://doi.org/10.1088/2632-2153/ad7ad7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a machine learning algorithm to infer the emergent stochastic equation governing the evolution of an order parameter of a many-body system. We train our neural network to independently learn the directed force acting on the order parameter as well as an effective diffusive noise. We illustrate our approach using the classical Ising model endowed with Glauber dynamics, and the contact process as test cases. For both models, which represent paradigmatic equilibrium and nonequilibrium scenarios, the directed force and noise can be efficiently inferred. The directed force term of the Ising model allows us to reconstruct an effective potential for the order parameter which develops the characteristic double-well shape below the critical temperature. Despite its genuine nonequilibrium nature, such an effective potential can also be obtained for the contact process and its shape signals a phase transition into an absorbing state. Also, in contrast to the equilibrium Ising model, the presence of an absorbing state renders the noise term dependent on the value of the order parameter itself.},
  archive      = {J_MLST},
  author       = {Francesco Carnazza and Federico Carollo and Sabine Andergassen and Georg Martius and Miriam Klopotek and Igor Lesanovsky},
  doi          = {10.1088/2632-2153/ad7ad7},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045002},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning stochastic differential equations for the evolution of order parameters of classical many-body systems in and out of equilibrium},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A PNP ion channel deep learning solver with local neural
network and finite element input data. <em>MLST</em>, <em>5</em>(4),
045001. (<a href="https://doi.org/10.1088/2632-2153/ad7e7a">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a deep learning method for solving an improved one-dimensional Poisson–Nernst–Planck ion channel (PNPic) model, called the PNPic deep learning solver. The solver combines a novel local neural network, adapted from the neural network with local converging inputs, with an efficient PNPic finite element solver, developed in this work. In particular, the local neural network is extended to handle the complexities of the PNPic model—a system of nonlinear convection–diffusion and elliptic equations with multiple subdomains connected by interface conditions. The PNPic finite element solver efficiently generates input and reference datasets for fast training the local neural network, as well as input datasets for quickly predicting PNPic solutions with high accuracy for a family of PNPic models. Initial numerical tests, involving perturbations of model parameters and interface locations, demonstrate that the PNPic deep learning solver can generate highly accurate numerical solutions.},
  archive      = {J_MLST},
  author       = {Hwi Lee and Zhen Chao and Harris Cobb and Yingjie Liu and Dexuan Xie},
  doi          = {10.1088/2632-2153/ad7e7a},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A PNP ion channel deep learning solver with local neural network and finite element input data},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probing the effects of broken symmetries in machine
learning. <em>MLST</em>, <em>5</em>(4), 04LT01. (<a
href="https://doi.org/10.1088/2632-2153/ad86a0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symmetry is one of the most central concepts in physics, and it is no surprise that it has also been widely adopted as an inductive bias for machine-learning models applied to the physical sciences. This is especially true for models targeting the properties of matter at the atomic scale. Both established and state-of-the-art approaches, with almost no exceptions, are built to be exactly equivariant to translations, permutations, and rotations of the atoms. Incorporating symmetries—rotations in particular—constrains the model design space and implies more complicated architectures that are often also computationally demanding. There are indications that unconstrained models can easily learn symmetries from data, and that doing so can even be beneficial for the accuracy of the model. We demonstrate that an unconstrained architecture can be trained to achieve a high degree of rotational invariance, testing the impacts of the small symmetry breaking in realistic scenarios involving simulations of gas-phase, liquid, and solid water. We focus specifically on physical observables that are likely to be affected—directly or indirectly—by non-invariant behavior under rotations, finding negligible consequences when the model is used in an interpolative, bulk, regime. Even for extrapolative gas-phase predictions, the model remains very stable, even though symmetry artifacts are noticeable. We also discuss strategies that can be used to systematically reduce the magnitude of symmetry breaking when it occurs, and assess their impact on the convergence of observables.},
  archive      = {J_MLST},
  author       = {Marcel F Langer and Sergey N Pozdnyakov and Michele Ceriotti},
  doi          = {10.1088/2632-2153/ad86a0},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {04LT01},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Probing the effects of broken symmetries in machine learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stable attractors for neural networks classification via
ordinary differential equations (SA-nODE). <em>MLST</em>, <em>5</em>(3),
035087. (<a href="https://doi.org/10.1088/2632-2153/ad7f26">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel approach for supervised classification is presented which sits at the intersection of machine learning and dynamical systems theory. At variance with other methodologies that employ ordinary differential equations for classification purposes, the untrained model is a priori constructed to accommodate for a set of pre-assigned stationary stable attractors. Classifying amounts to steer the dynamics towards one of the planted attractors, depending on the specificity of the processed item supplied as an input. Asymptotically the system will hence converge on a specific point of the explored multi-dimensional space, flagging the category of the object to be eventually classified. Working in this context, the inherent ability to perform classification, as acquired ex post by the trained model, is ultimately reflected in the shaped basin of attractions associated to each of the target stable attractors. The performance of the proposed method is here challenged against simple toy models crafted for the purpose, as well as by resorting to well established reference standards. More precisely, we achieved an accuracy of 98.06 \% on the MNIST test set and 88.21% on the Fashion MNIST test set. Although this method does not reach the performance of state-of-the-art deep learning algorithms, it illustrates that continuous dynamical systems with closed analytical interaction terms can serve as high-performance classifiers.},
  archive      = {J_MLST},
  author       = {Raffaele Marino and Lorenzo Buffoni and Lorenzo Chicchi and Lorenzo Giambagli and Duccio Fanelli},
  doi          = {10.1088/2632-2153/ad7f26},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {3},
  pages        = {035087},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Stable attractors for neural networks classification via ordinary differential equations (SA-nODE)},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WBC-KICNet: Knowledge-infused convolutional neural network
for white blood cell classification. <em>MLST</em>, <em>5</em>(3),
035086. (<a href="https://doi.org/10.1088/2632-2153/ad7a4e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {White blood cells (WBCs) are useful for diagnosing infectious diseases and infections. Machine learning and deep learning have been used to classify WBCs from blood smear images. Despite advances in machine learning, there has been little research on applying medical domain knowledge to convolutional neural networks (CNNs) to improve WBC classification. The existing models are often inaccurate, rely on manual input, and fail to incorporate external medical knowledge into decision-making. This study used the blood cell count and detection dataset which contains images of monocytes, lymphocytes, neutrophils, and eosinophils for WBC classification. In this paper, we propose a CNN model for WBC classification called WBC-KICNet (knowledge-infused convolutional neural network). The present work uses two CNN models: the first model generates the knowledge vector from input images and the domain expert (hematologist); the second model extracts deep features from the input image. A feature fusion mechanism is then used to combine these two features to classify the WBCs. Several metrics have been used to evaluate the performance of the WBC-KICNet model. These measures yielded impressive results. Accuracy, precision, recall, specificity, and F1-score were rated 99.22%, 99.25%, 99%, 99.77%, and 99.25%, respectively. In each of the WBC classes, accuracy rates are: 98.7% for eosinophils, 99.83% for lymphocytes, 100% for monocytes, and 98.32% for neutrophils. As a result, the proposed WBC-KICNet classifies WBCs accurately and without much misclassification, and the results have been confirmed by a statistical hypothesis test ( t -test).},
  archive      = {J_MLST},
  author       = {Jeneessha P and Vinoth Kumar Balasubramanian and M Murugappan},
  doi          = {10.1088/2632-2153/ad7a4e},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {3},
  pages        = {035086},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {WBC-KICNet: Knowledge-infused convolutional neural network for white blood cell classification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development and deployment of data-driven turbulence model
for three-dimensional complex configurations. <em>MLST</em>,
<em>5</em>(3), 035085. (<a
href="https://doi.org/10.1088/2632-2153/ad7d60">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the synergy between artificial intelligence and turbulence big data has given rise to a new data-driven paradigm in turbulence research. Data-driven turbulence modeling has emerged as one of the forefront directions in fluid mechanics. Most existing studies focus on feature construction, selection, and the development of modeling frameworks, often overlooking the practical deployment and application of trained models. This paper examines the entire process from model construction to real-world deployment, using data-driven turbulence modeling for high Reynolds number flows over complex three-dimensional configurations as a case study. Key stages include data generation, input-output feature construction, model training, model compilation and optimization, deployment, and validation. We successfully implemented the entire workflow in a heterogeneous supercomputing environment and, through mixed programming techniques, integrated the resulting turbulence model into the Platform for Hybrid Engineering Simulation of Flows (PHengLEI) open-source software framework. This allowed for mixed-precision simulations, with the main equations solved in double precision and the turbulence model in half precision. The new computational framework was validated through large-scale parallel numerical simulations on grids with tens of millions of elements for three-dimensional complex configurations. The results highlight the efficiency of our model deployment, with overall computational efficiency improving by 13.35% and the turbulence model&#39;s solution speed increasing by approximately 3.9 times. The accuracy of the computations was also confirmed, with the average relative error in the lift and drag coefficients calculated by the data-driven turbulence model within 3%. Across various computing nodes, the relative error in the computed aerodynamic coefficients remained within 1%, demonstrating the framework&#39;s scalability. Notably, our contributions have been incorporated as a case study in the latest PHengLEI open-source project 5 .},
  archive      = {J_MLST},
  author       = {Xuxiang Sun and Yilang Liu and Weiwei Zhang and Yongzhong Wang and Jingyuan Zou and Zhengrong Han and Yun Su},
  doi          = {10.1088/2632-2153/ad7d60},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {3},
  pages        = {035085},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Development and deployment of data-driven turbulence model for three-dimensional complex configurations},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Portable network resolving huge-graph isomorphism problem.
<em>MLST</em>, <em>5</em>(3), 035084. (<a
href="https://doi.org/10.1088/2632-2153/ad7d5f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The graph isomorphism, as a key task in graph data analysis, is of great significance for the understanding, feature extraction, and pattern recognition of graph data. The best performance of traditional methods is the quasi-polynomial time complexity, which is infeasible for huge graphs. This paper aims to propose a lightweight graph network to resolve the problem of isomorphism in huge graphs. We propose a partitioning algorithm with linear time complexity based on isomorphic necessary condition to handle network graphs that cannot be fully computed by a single computer. We use anti-weight convolution analysis and skip connections to obtain a more stable representation with fewer layers and parameters. We implement simulations on personal computer (PC) with graphs consisting of hundred millions of edges, demonstrating the identification performance ( \gt\!\!98\% ) and time efficiency.},
  archive      = {J_MLST},
  author       = {Xin An and Ling-Fang Li and Xue Yang and Ming-Xing Luo},
  doi          = {10.1088/2632-2153/ad7d5f},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035084},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Portable network resolving huge-graph isomorphism problem},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accelerating scientific discovery with generative knowledge
extraction, graph-based representation, and multimodal intelligent graph
reasoning. <em>MLST</em>, <em>5</em>(3), 035083. (<a
href="https://doi.org/10.1088/2632-2153/ad7228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging generative Artificial Intelligence (AI), we have transformed a dataset comprising 1000 scientific papers focused on biological materials into a comprehensive ontological knowledge graph. Through an in-depth structural analysis of this graph, we have calculated node degrees, identified communities along with their connectivities, and evaluated clustering coefficients and betweenness centrality of pivotal nodes, uncovering fascinating knowledge architectures. We find that the graph has an inherently scale-free nature, shows a high level of connectedness, and can be used as a rich source for downstream graph reasoning by taking advantage of transitive and isomorphic properties to reveal insights into unprecedented interdisciplinary relationships that can be used to answer queries, identify gaps in knowledge, propose never-before-seen material designs, and predict material behaviors. Using a large language embedding model we compute deep node representations and use combinatorial node similarity ranking to develop a path sampling strategy that allows us to link dissimilar concepts that have previously not been related. One comparison revealed detailed structural parallels between biological materials and Beethoven&#39;s 9th Symphony, highlighting shared patterns of complexity through isomorphic mapping. In another example, the algorithm proposed an innovative hierarchical mycelium-based composite based on integrating path sampling with principles extracted from Kandinsky&#39;s &#39;Composition VII&#39; painting. The resulting material integrates an innovative set of concepts that include a balance of chaos and order, adjustable porosity, mechanical strength, and complex patterned chemical functionalization. We uncover other isomorphisms across science, technology and art, revealing a nuanced ontology of immanence that reveal a context-dependent heterarchical interplay of constituents. Because our method transcends established disciplinary boundaries through diverse data modalities (graphs, images, text, numerical data, etc), graph-based generative AI achieves a far higher degree of novelty, explorative capacity, and technical detail, than conventional approaches and establishes a widely useful framework for innovation by revealing hidden connections.},
  archive      = {J_MLST},
  author       = {Markus J Buehler},
  doi          = {10.1088/2632-2153/ad7228},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035083},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Accelerating scientific discovery with generative knowledge extraction, graph-based representation, and multimodal intelligent graph reasoning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust anomaly detection for particle physics using
multi-background representation learning. <em>MLST</em>, <em>5</em>(3),
035082. (<a href="https://doi.org/10.1088/2632-2153/ad780c">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly, or out-of-distribution, detection is a promising tool for aiding discoveries of new particles or processes in particle physics. In this work, we identify and address two overlooked opportunities to improve anomaly detection (AD) for high-energy physics. First, rather than train a generative model on the single most dominant background process, we build detection algorithms using representation learning from multiple background types, thus taking advantage of more information to improve estimation of what is relevant for detection. Second, we generalize decorrelation to the multi-background setting, thus directly enforcing a more complete definition of robustness for AD. We demonstrate the benefit of the proposed robust multi-background AD algorithms on a high-dimensional dataset of particle decays at the Large Hadron Collider.},
  archive      = {J_MLST},
  author       = {Abhijith Gandrakota and Lily H Zhang and Aahlad Puli and Kyle Cranmer and Jennifer Ngadiuba and Rajesh Ranganath and Nhan Tran},
  doi          = {10.1088/2632-2153/ad780c},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035082},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Robust anomaly detection for particle physics using multi-background representation learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Benchmarking of quantum fidelity kernels for gaussian
process regression. <em>MLST</em>, <em>5</em>(3), 035081. (<a
href="https://doi.org/10.1088/2632-2153/ad7cc1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum computing algorithms have been shown to produce performant quantum kernels for machine-learning classification problems. Here, we examine the performance of quantum kernels for regression problems of practical interest. For an unbiased benchmarking of quantum kernels, it is necessary to construct the most optimal functional form of the classical kernels and the most optimal quantum kernels for each given data set. We develop an algorithm that uses an analog of the Bayesian information criterion to optimize the sequence of quantum gates used to estimate quantum kernels for Gaussian process models. The algorithm increases the complexity of the quantum circuits incrementally, while improving the performance of the resulting kernels, and is shown to yield much higher model accuracy with fewer quantum gates than a fixed quantum circuit ansatz. We demonstrate that quantum kernels thus obtained can be used to build accurate models of global potential energy surfaces (PES) for polyatomic molecules. The average interpolation error of the six-dimensional PES obtained with a random distribution of 2000 energy points is 16 cm −1 for H 3 O + , 15 cm −1 for H 2 CO and 88 cm −1 for HNO 2 . We show that a compositional optimization of classical kernels for Gaussian process regression converges to the same errors. This indicates that quantum kernels can achieve the same, though not better, expressivity as classical kernels for regression problems.},
  archive      = {J_MLST},
  author       = {Xuyang Guo and Jun Dai and Roman V Krems},
  doi          = {10.1088/2632-2153/ad7cc1},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035081},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Benchmarking of quantum fidelity kernels for gaussian process regression},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncovering obscured phonon dynamics from powder inelastic
neutron scattering using machine learning. <em>MLST</em>, <em>5</em>(3),
035080. (<a href="https://doi.org/10.1088/2632-2153/ad79b6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of phonon dynamics is pivotal for understanding material properties, yet it faces challenges due to the irreversible information loss inherent in powder inelastic neutron scattering spectra and the limitations of traditional analysis methods. In this study, we present a machine learning framework designed to reveal obscured phonon dynamics from powder spectra. Using a variational autoencoder, we obtain a disentangled latent representation of spectra and successfully extract force constants for reconstructing phonon dispersions. Notably, our model demonstrates effective applicability to experimental data even when trained exclusively on physics-based simulations. The fine-tuning with experimental spectra further mitigates issues arising from domain shift. Analysis of latent space underscores the model&#39;s versatility and generalizability, affirming its suitability for complex system applications. Furthermore, our framework&#39;s two-stage design is promising for developing a universal pre-trained feature extractor. This approach has the potential to revolutionize neutron measurements of phonon dynamics, offering researchers a potent tool to decipher intricate spectra and gain valuable insights into the intrinsic physics of materials.},
  archive      = {J_MLST},
  author       = {Yaokun Su and Chen Li},
  doi          = {10.1088/2632-2153/ad79b6},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035080},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Uncovering obscured phonon dynamics from powder inelastic neutron scattering using machine learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LoGAN: Local generative adversarial network for novel
structure prediction. <em>MLST</em>, <em>5</em>(3), 035079. (<a
href="https://doi.org/10.1088/2632-2153/ad7a4d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficient generation and filtering of candidate structures for new materials is becoming increasingly important as starting points for computational studies. In this work, we introduce an approach to Wasserstein generative adversarial networks for predicting unique crystal and molecular structures. Leveraging translation- and rotation-invariant atom-centered local descriptors addresses some of the major challenges faced by similar methods. Our models require only small sets of known structures as training data. Furthermore, the approach is able to generate both non-periodic and periodic structures based on local coordination. We showcase the data efficiency and versatility of the approach by recovering all stable C 5 H 12 O isomers using only 39 C 4 H 10 O and C 6 H 14 O training examples, as well as a few randomly selected known low-energy SiO 2 crystal structures utilizing only 167 training examples of other SiO 2 crystal structures. We also introduce a filtration technique to reduce the computational cost of subsequent characterization steps by selecting samples from unique basins on the potential energy surface, which allows to minimize the number of geometry relaxations needed after structure generation. The present method thus represents a new, versatile approach to generative modeling of crystal and molecular structures in the low-data regime, and is available as open-source.},
  archive      = {J_MLST},
  author       = {Péter Kovács and Esther Heid and Jasper De Landsheere and Georg K H Madsen},
  doi          = {10.1088/2632-2153/ad7a4d},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035079},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {LoGAN: Local generative adversarial network for novel structure prediction},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accelerating cavity fault prediction using deep learning at
jefferson laboratory. <em>MLST</em>, <em>5</em>(3), 035078. (<a
href="https://doi.org/10.1088/2632-2153/ad7ad6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accelerating cavities are an integral part of the continuous electron beam accelerator facility (CEBAF) at Jefferson Laboratory. When any of the over 400 cavities in CEBAF experiences a fault, it disrupts beam delivery to experimental user halls. In this study, we propose the use of a deep learning model to predict slowly developing cavity faults. By utilizing pre-fault signals, we train a long short-term memory-convolutional neural network binary classifier to distinguish between radio-frequency (RF) signals during normal operation and RF signals indicative of impending faults. We optimize the model by adjusting the fault confidence threshold and implementing a multiple consecutive window criterion to identify fault events, ensuring a low false positive rate. Results obtained from analysis of a real dataset collected from the accelerating cavities simulating a deployed scenario demonstrate the model&#39;s ability to identify normal signals with 99.99% accuracy and correctly predict 80% of slowly developing faults. Notably, these achievements were achieved in the context of a highly imbalanced dataset, and fault predictions were made several hundred milliseconds before the onset of the fault. Anticipating faults enables preemptive measures to improve operational efficiency by preventing or mitigating their occurrence.},
  archive      = {J_MLST},
  author       = {Md M Rahman and A Carpenter and K Iftekharuddin and C Tennant},
  doi          = {10.1088/2632-2153/ad7ad6},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035078},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Accelerating cavity fault prediction using deep learning at jefferson laboratory},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing ZX-diagrams with deep reinforcement learning.
<em>MLST</em>, <em>5</em>(3), 035077. (<a
href="https://doi.org/10.1088/2632-2153/ad76f7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ZX-diagrams are a powerful graphical language for the description of quantum processes with applications in fundamental quantum mechanics, quantum circuit optimization, tensor network simulation, and many more. The utility of ZX-diagrams relies on a set of local transformation rules that can be applied to them without changing the underlying quantum process they describe. These rules can be exploited to optimize the structure of ZX-diagrams for a range of applications. However, finding an optimal sequence of transformation rules is generally an open problem. In this work, we bring together ZX-diagrams with reinforcement learning, a machine learning technique designed to discover an optimal sequence of actions in a decision-making problem and show that a trained reinforcement learning agent can significantly outperform other optimization techniques like a greedy strategy, simulated annealing, and state-of-the-art hand-crafted algorithms. The use of graph neural networks to encode the policy of the agent enables generalization to diagrams much bigger than seen during the training phase.},
  archive      = {J_MLST},
  author       = {Maximilian Nägele and Florian Marquardt},
  doi          = {10.1088/2632-2153/ad76f7},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035077},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Optimizing ZX-diagrams with deep reinforcement learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DiffLense: A conditional diffusion model for
super-resolution of gravitational lensing data. <em>MLST</em>,
<em>5</em>(3), 035076. (<a
href="https://doi.org/10.1088/2632-2153/ad76f8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gravitational lensing data is frequently collected at low resolution due to instrumental limitations and observing conditions. Machine learning-based super-resolution techniques offer a method to enhance the resolution of these images, enabling more precise measurements of lensing effects and a better understanding of the matter distribution in the lensing system. This enhancement can significantly improve our knowledge of the distribution of mass within the lensing galaxy and its environment, as well as the properties of the background source being lensed. Traditional super-resolution techniques typically learn a mapping function from lower-resolution to higher-resolution samples. However, these methods are often constrained by their dependence on optimizing a fixed distance function, which can result in the loss of intricate details crucial for astrophysical analysis. In this work, we introduce DiffLense , a novel super-resolution pipeline based on a conditional diffusion model specifically designed to enhance the resolution of gravitational lensing images obtained from the Hyper Suprime-Cam Subaru Strategic Program (HSC-SSP). Our approach adopts a generative model, leveraging the detailed structural information present in Hubble space telescope (HST) counterparts. The diffusion model, trained to generate HST data, is conditioned on HSC data pre-processed with denoising techniques and thresholding to significantly reduce noise and background interference. This process leads to a more distinct and less overlapping conditional distribution during the model&#39;s training phase. We demonstrate that DiffLense outperforms existing state-of-the-art single-image super-resolution techniques, particularly in retaining the fine details necessary for astrophysical analyses.},
  archive      = {J_MLST},
  author       = {Pranath Reddy and Michael W Toomey and Hanna Parul and Sergei Gleyzer},
  doi          = {10.1088/2632-2153/ad76f8},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035076},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {DiffLense: A conditional diffusion model for super-resolution of gravitational lensing data},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Equivariant tensor network potentials. <em>MLST</em>,
<em>5</em>(3), 035075. (<a
href="https://doi.org/10.1088/2632-2153/ad79b5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine-learning interatomic potentials (MLIPs) have made a significant contribution to the recent progress in the fields of computational materials and chemistry due to the MLIPs&#39; ability of accurately approximating energy landscapes of quantum-mechanical models while being orders of magnitude more computationally efficient. However, the computational cost and number of parameters of many state-of-the-art MLIPs increases exponentially with the number of atomic features. Tensor (non-neural) networks, based on low-rank representations of high-dimensional tensors, have been a way to reduce the number of parameters in approximating multidimensional functions, however, it is often not easy to encode the model symmetries into them. In this work we develop a formalism for rank-efficient equivariant tensor networks (ETNs), i.e. tensor networks that remain invariant under actions of SO(3) upon contraction. All the key algorithms of tensor networks like orthogonalization of cores and DMRG-based algorithms carry over to our equivariant case. Moreover, we show that many elements of modern neural network architectures like message passing, pulling, or attention mechanisms, can in some form be implemented into the ETNs. Based on ETNs, we develop a new class of polynomial-based MLIPs that demonstrate superior performance over existing MLIPs for multicomponent systems.},
  archive      = {J_MLST},
  author       = {M Hodapp and A Shapeev},
  doi          = {10.1088/2632-2153/ad79b5},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035075},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Equivariant tensor network potentials},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Masked particle modeling on sets: Towards self-supervised
high energy physics foundation models. <em>MLST</em>, <em>5</em>(3),
035074. (<a href="https://doi.org/10.1088/2632-2153/ad64a8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose masked particle modeling (MPM) as a self-supervised method for learning generic, transferable, and reusable representations on unordered sets of inputs for use in high energy physics (HEP) scientific data. This work provides a novel scheme to perform masked modeling based pre-training to learn permutation invariant functions on sets. More generally, this work provides a step towards building large foundation models for HEP that can be generically pre-trained with self-supervised learning and later fine-tuned for a variety of down-stream tasks. In MPM, particles in a set are masked and the training objective is to recover their identity, as defined by a discretized token representation of a pre-trained vector quantized variational autoencoder. We study the efficacy of the method in samples of high energy jets at collider physics experiments, including studies on the impact of discretization, permutation invariance, and ordering. We also study the fine-tuning capability of the model, showing that it can be adapted to tasks such as supervised and weakly supervised jet classification, and that the model can transfer efficiently with small fine-tuning data sets to new classes and new data domains.},
  archive      = {J_MLST},
  author       = {Tobias Golling and Lukas Heinrich and Michael Kagan and Samuel Klein and Matthew Leigh and Margarita Osadchy and John Andrew Raine},
  doi          = {10.1088/2632-2153/ad64a8},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035074},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Masked particle modeling on sets: Towards self-supervised high energy physics foundation models},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transforming the bootstrap: Using transformers to compute
scattering amplitudes in planar <span class="math inline">𝒩 = 4</span>
super yang–mills theory. <em>MLST</em>, <em>5</em>(3), 035073. (<a
href="https://doi.org/10.1088/2632-2153/ad743e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We pursue the use of deep learning methods to improve state-of-the-art computations in theoretical high-energy physics. Planar \mathcal{N} = 4 Super Yang–Mills theory is a close cousin to the theory that describes Higgs boson production at the Large Hadron Collider; its scattering amplitudes are large mathematical expressions containing integer coefficients. In this paper, we apply transformers to predict these coefficients. The problem can be formulated in a language-like representation amenable to standard cross-entropy training objectives. We design two related experiments and show that the model achieves high accuracy ( {\gt}{98\%}) on both tasks. Our work shows that transformers can be applied successfully to problems in theoretical physics that require exact solutions.},
  archive      = {J_MLST},
  author       = {Tianji Cai and Garrett W Merz and François Charton and Niklas Nolte and Matthias Wilhelm and Kyle Cranmer and Lance J Dixon},
  doi          = {10.1088/2632-2153/ad743e},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035073},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Transforming the bootstrap: Using transformers to compute scattering amplitudes in planar $\mathcal{N} = 4$ super Yang–Mills theory},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning on the correctness class for domain inverse
problems of gravimetry. <em>MLST</em>, <em>5</em>(3), 035072. (<a
href="https://doi.org/10.1088/2632-2153/ad72cc">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider end-to-end learning approaches for inverse problems of gravimetry. Due to ill-posedness of the inverse gravimetry, the reliability of learning approaches is questionable. To deal with this problem, we propose the strategy of learning on the correctness class. The well-posedness theorems are employed when designing the neural-network architecture and constructing the training set. Given the density-contrast function as a priori information, the domain of mass can be uniquely determined under certain constrains, and the domain inverse problem is a correctness class of the inverse gravimetry. Under this correctness class, we design the neural network for learning by mimicking the level-set formulation for the inverse gravimetry. Numerical examples illustrate that the method is able to recover mass models with non-constant density contrast.},
  archive      = {J_MLST},
  author       = {Yihang Chen and Wenbin Li},
  doi          = {10.1088/2632-2153/ad72cc},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035072},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Learning on the correctness class for domain inverse problems of gravimetry},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A combined modeling method for complex multi-fidelity data
fusion. <em>MLST</em>, <em>5</em>(3), 035071. (<a
href="https://doi.org/10.1088/2632-2153/ad718f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, mainstream methods for multi-fidelity data fusion have achieved great success in many fields, but they generally suffer from poor scalability. Therefore, this paper proposes a C_3^2 combination modeling method for complex multi-fidelity data fusion, devoted to solving the modeling problems with three types of multi-fidelity data fusion, and explores a general solution for any n types of multi-fidelity data fusion. Different from the traditional direct modeling method—Multi-Fidelity Deep Neural Network (MFDNN)—the C_3^2 method is an indirect modeling method. The experimental results on three representative benchmark functions and the prediction tasks of SG6043 airfoil aerodynamic performance show that C_3^2 combination modeling has the following advantages: (1) It can quickly establish the mapping relationship between high, medium, and low fidelity data. (2) It can effectively solve the data imbalance problem in multi-fidelity modeling. (3) Compared with MFDNN, it has stronger noise resistance and higher prediction accuracy. Additionally, this paper discusses the scalability problem of the C_n^2 method when n = 4 and n = 5, providing a reference for further research on the combined modeling method.},
  archive      = {J_MLST},
  author       = {Lei Tang and Feng Liu and Anping Wu and Yubo Li and Wanqiu Jiang and Qingfeng Wang and Jun Huang},
  doi          = {10.1088/2632-2153/ad718f},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035071},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A combined modeling method for complex multi-fidelity data fusion},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Designing quantum multi-category classifier from the
perspective of brain processing information. <em>MLST</em>,
<em>5</em>(3), 035070. (<a
href="https://doi.org/10.1088/2632-2153/ad7570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of machine learning, the multi-category classification problem plays a crucial role. Solving the problem has a profound impact on driving the innovation and development of machine learning techniques and addressing complex problems in the real world. In recent years, researchers have begun to focus on utilizing quantum computing to solve the multi-category classification problem. Some studies have shown that the process of processing information in the brain may be related to quantum phenomena, with different brain regions having neurons with different structures. Inspired by this, we design a quantum multi-category classifier model from this perspective for the first time. The model employs a heterogeneous population of quantum neural networks (QNNs) to simulate the cooperative work of multiple different brain regions. When processing information, these heterogeneous clusters of QNNs allow for simultaneous execution on different quantum computers, thus simulating the brain&#39;s ability to utilize multiple brain regions working in concert to maintain the robustness of the model. By setting the number of heterogeneous QNN clusters and parameterizing the number of stacks of unit layers in the quantum circuit, the model demonstrates excellent scalability in dealing with different types of data and different numbers of classes in the classification problem. Based on the attention mechanism of the brain, we integrate the processing results of heterogeneous QNN clusters to achieve high accuracy in classification. Finally, we conducted classification simulation experiments on different datasets. The results show that our method exhibits strong robustness and scalability. Among them, on different subsets of the MNIST dataset, its classification accuracy improves by up to about 5% compared to other quantum multiclassification algorithms. This result becomes the state-of-the-art simulation result for quantum classification models and exceeds the performance of classical classifiers with a considerable number of trainable parameters on some subsets of the MNIST dataset.},
  archive      = {J_MLST},
  author       = {Xiaodong Ding and Jinchen Xu and Zhihui Song and Yifan Hou and Zheng Shan},
  doi          = {10.1088/2632-2153/ad7570},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035070},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Designing quantum multi-category classifier from the perspective of brain processing information},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Photonic modes prediction via multi-modal diffusion model.
<em>MLST</em>, <em>5</em>(3), 035069. (<a
href="https://doi.org/10.1088/2632-2153/ad743f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of photonic modes is the cornerstone in optics and photonics, which can describe the propagation of the light. The Maxwell&#39;s equations play the role in calculating the mode field based on the structure information, while this process needs a great deal of computations, especially in the handle with a three-dimensional model. To overcome this obstacle, we introduce the multi-modal diffusion model to predict the photonic modes in one certain structure. The Contrastive Language–Image Pre-training (CLIP) model is used to build the connections between photonic structures and the corresponding modes. Then we exemplify Stable Diffusion (SD) model to realize the function of optical fields generation from structure information. Our work introduces multi-modal deep learning to construct complex mapping between structural information and optical field as high-dimensional vectors, and generates optical field images based on this mapping.},
  archive      = {J_MLST},
  author       = {Jinyang Sun and Xi Chen and Xiumei Wang and Dandan Zhu and Xingping Zhou},
  doi          = {10.1088/2632-2153/ad743f},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035069},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Photonic modes prediction via multi-modal diffusion model},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An exponential reduction in training data sizes for machine
learning derived entanglement witnesses. <em>MLST</em>, <em>5</em>(3),
035068. (<a href="https://doi.org/10.1088/2632-2153/ad7457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a support vector machine (SVM) based approach for generating an entanglement witness that requires exponentially less training data than previously proposed methods. SVMs generate hyperplanes represented by a weighted sum of expectation values of local observables whose coefficients are optimized to sum to a positive number for all separable states and a negative number for as many entangled states as possible near a specific target state. Previous SVM-based approaches for entanglement witness generation used large amounts of randomly generated separable states to perform training, a task with considerable computational overhead. Here, we propose a method for orienting the witness hyperplane using only the significantly smaller set of states consisting of the eigenstates of the generalized Pauli matrices and a set of entangled states near the target entangled states. With the orientation of the witness hyperplane set by the SVM, we tune the plane&#39;s placement using a differential program that ensures perfect classification accuracy on a limited test set as well as maximal noise tolerance. For N qubits, the SVM portion of this approach requires only O(6^N) training states, whereas an existing method needs O(2^{4^N}) . We use this method to construct witnesses of 4 and 5 qubit GHZ states with coefficients agreeing with stabilizer formalism witnesses to within 3.7 percent and 1 percent, respectively. We also use the same training states to generate novel 4 and 5 qubit W state witnesses. Finally, we computationally verify these witnesses on small test sets and propose methods for further verification.},
  archive      = {J_MLST},
  author       = {Aiden R Rosebush and Alexander C B Greenwood and Brian T Kirby and Li Qian},
  doi          = {10.1088/2632-2153/ad7457},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035068},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {An exponential reduction in training data sizes for machine learning derived entanglement witnesses},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predictive models for inorganic materials thermoelectric
properties with machine learning. <em>MLST</em>, <em>5</em>(3), 035067.
(<a href="https://doi.org/10.1088/2632-2153/ad6831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high computational demand of the Density Functional Theory (DFT) based method for screening new materials properties remains a strong limitation to the development of clean and renewable energy technologies essential to transition to a carbon-neutral environment in the coming decades. Machine Learning comes into play with its innate capacity to handle huge amounts of data and high-dimensional statistical analysis. In this paper, supervised Machine Learning models together with data analysis on existing datasets obtained from a high-throughput calculation using Density Functional Theory are used to predict the Seebeck coefficient, electrical conductivity, and power factor of inorganic compounds. The analysis revealed a strong dependence of the thermoelectric properties on the effective masses, we also proposed a machine learning model for the prediction of highly performing thermoelectric materials which reached an efficiency of 95 percent. The analyzed data and developed model can significantly contribute to innovation by providing a faster and more accurate prediction of thermoelectric properties, thereby, facilitating the discovery of highly efficient thermoelectric materials.},
  archive      = {J_MLST},
  author       = {Delchere Don-tsa and Messanh Agbeko Mohou and Kossi Amouzouvi and Malik Maaza and Katawoura Beltako},
  doi          = {10.1088/2632-2153/ad6831},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035067},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Predictive models for inorganic materials thermoelectric properties with machine learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regulating the development of accurate data-driven
physics-informed deformation models. <em>MLST</em>, <em>5</em>(3),
035066. (<a href="https://doi.org/10.1088/2632-2153/ad7192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge posed by the inverse problem associated with ultrasonic elasticity imaging is well matched to the capabilities of data-driven solutions. This report describes how data properties and the time sequence by which the data are introduced during training influence deformation-model accuracy and training times. Our goal is to image the elastic modulus of soft linear-elastic media as accurately as possible within a limited volume. To monitor progress during training, we introduce metrics describing convergence rate and stress entropy to guide data acquisition and other timing features. For example, a regularization term in the loss function may be introduced and later removed to speed and stabilize developing deformation models as well as establishing stopping rules for neural-network convergence. Images of a 14.4 cm 3 volume within 3D software phantom visually indicate the quality of modulus images resulting over a range of training variables. The results show that a data-driven method constrained by the physics of a deformed solid will lead to quantitively accurate 3D elastic modulus images with minimum artifacts.},
  archive      = {J_MLST},
  author       = {Will Newman and Jamshid Ghaboussi and Michael Insana},
  doi          = {10.1088/2632-2153/ad7192},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035066},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Regulating the development of accurate data-driven physics-informed deformation models},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving model robustness to weight noise via consistency
regularization. <em>MLST</em>, <em>5</em>(3), 035065. (<a
href="https://doi.org/10.1088/2632-2153/ad734a">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an emerging computing architecture, the computing-in-memory (CIM) exhibits significant potential for energy efficiency and computing power in artificial intelligence applications. However, the intrinsic non-idealities of CIM devices, manifesting as random interference on the weights of neural network, may significantly impact the inference accuracy. In this paper, we propose a novel training algorithm designed to mitigate the impact of weight noise. The algorithm strategically minimizes cross-entropy loss while concurrently refining the feature representations in intermediate layers to emulate those of an ideal, noise-free network. This dual-objective approach not only preserves the accuracy of the neural network but also enhances its robustness against noise-induced degradation. Empirical validation across several benchmark datasets confirms that our algorithm sets a new benchmark for accuracy in CIM-enabled neural network applications. Compared to the most commonly used forward noise training methods, our approach yields approximately a 2% accuracy boost on the ResNet32 model with the CIFAR-10 dataset and a weight noise scale of 0.2, and achieves a minimum performance gain of 1% on ResNet18 with the ImageNet dataset under the same noise quantization conditions.},
  archive      = {J_MLST},
  author       = {Yaoqi Hou and Qingtian Zhang and Namin Wang and Huaqiang Wu},
  doi          = {10.1088/2632-2153/ad734a},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035065},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Improving model robustness to weight noise via consistency regularization},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Triggering dark showers with conditional dual auto-encoders.
<em>MLST</em>, <em>5</em>(3), 035064. (<a
href="https://doi.org/10.1088/2632-2153/ad652b">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a family of conditional dual auto-encoders (CoDAEs) for generic and model-independent new physics searches at colliders. New physics signals, which arise from new types of particles and interactions, are considered in our study as anomalies causing deviations in data with respect to expected background events. In this work, we perform a normal-only anomaly detection, which employs only background samples, to search for manifestations of a dark version of strong force applying (variational) auto-encoders on raw detector images, which are large and highly sparse, without leveraging any physics-based pre-processing or strong assumption on the signals. The proposed CoDAE has a dual-encoder design, which is general and can learn an auxiliary yet compact latent space through spatial conditioning, showing a neat improvement over competitive physics-based baselines and related approaches, therefore also reducing the gap with fully supervised models. It is the first time an unsupervised model is shown to exhibit excellent discrimination against multiple dark shower models, illustrating the suitability of this method as an accurate, fast, model-independent algorithm to deploy, e.g. in the real-time event triggering systems of large hadron collider experiments such as ATLAS and CMS.},
  archive      = {J_MLST},
  author       = {Luca Anzalone and Simranjit Singh Chhibra and Benedikt Maier and Nadezda Chernyavskaya and Maurizio Pierini},
  doi          = {10.1088/2632-2153/ad652b},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035064},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Triggering dark showers with conditional dual auto-encoders},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating cell growth and hypoxic regions of 3D spheroids
via a machine learning approach. <em>MLST</em>, <em>5</em>(3), 035063.
(<a href="https://doi.org/10.1088/2632-2153/ad718e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigated the applicability of the area of spheroids and hypoxic regions for efficient evaluation of drug efficacy using machine learning (ML). We initially developed a high-throughput detection method to obtain the area of spheroids and hypoxic regions that can handle over 10 000 images per hour with an error rate of 2%–3%. The ML models were trained using cell growth of six cell lines ( i.e. HepG2, A549, Hep3B, BEAS-2B, HT-29, and HCT116) and hypoxic region variations of two cell lines ( i.e. HepG2 and BEAS-2B); our model can predict the area of spheroids and hypoxic region of certain growth date with high precision. To demonstrate the applicability, HepG2 spheroids were treated with sorafenib, and the efficacy of the drug was evaluated through a comparison of differences in areas of cell size and hypoxic regions with the predicted results. Furthermore, our ML approach has been shown to be applicable to provide the model-driven evaluative criterion for toxicity and drug efficacy using spheroids.},
  archive      = {J_MLST},
  author       = {Jaekak Yoo and Jae Won Choi and Eunha Kim and Eun-Jung Park and Ahruem Baek and Jaeseok Kim and Mun Seok Jeong and Youngwoo Cho and Tae Geol Lee and Min Beom Heo},
  doi          = {10.1088/2632-2153/ad718e},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035063},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Evaluating cell growth and hypoxic regions of 3D spheroids via a machine learning approach},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural force functional for non-equilibrium many-body
colloidal systems. <em>MLST</em>, <em>5</em>(3), 035062. (<a
href="https://doi.org/10.1088/2632-2153/ad7191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We combine power functional theory and machine learning to study non-equilibrium overdamped many-body systems of colloidal particles at the level of one-body fields. We first sample in steady state the one-body fields relevant for the dynamics from computer simulations of Brownian particles under the influence of randomly generated external fields. A neural network is then trained with this data to represent locally in space the formally exact functional mapping from the one-body density and velocity profiles to the one-body internal force field. The trained network is used to analyse the non-equilibrium superadiabatic force field and the transport coefficients such as shear and bulk viscosities. Due to the local learning approach, the network can be applied to systems much larger than the original simulation box in which the one-body fields are sampled. Complemented with the exact non-equilibrium one-body force balance equation and a continuity equation, the network yields viable predictions of the dynamics in time-dependent situations. Even though training is based on steady states only, the predicted dynamics is in good agreement with simulation results. A neural dynamical density functional theory can be straightforwardly implemented as a limiting case in which the internal force field is that of an equilibrium system. The framework is general and directly applicable to other many-body systems of interacting particles following Brownian dynamics.},
  archive      = {J_MLST},
  author       = {Toni Zimmermann and Florian Sammüller and Sophie Hermann and Matthias Schmidt and Daniel de las Heras},
  doi          = {10.1088/2632-2153/ad7191},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035062},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Neural force functional for non-equilibrium many-body colloidal systems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging normalizing flows for orbital-free density
functional theory. <em>MLST</em>, <em>5</em>(3), 035061. (<a
href="https://doi.org/10.1088/2632-2153/ad7226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orbital-free density functional theory (OF-DFT) for real-space systems has historically depended on Lagrange optimization techniques, primarily due to the inability of previously proposed electron density approaches to ensure the normalization constraint. This study illustrates how leveraging contemporary generative models, notably normalizing flows (NFs), can surmount this challenge. We develop a Lagrangian-free optimization framework by employing these machine learning models for the electron density. This diverse approach also integrates cutting-edge variational inference techniques and equivariant deep learning models, offering an innovative reformulation to the OF-DFT problem. We demonstrate the versatility of our framework by simulating a one-dimensional diatomic system, LiH, and comprehensive simulations of hydrogen, lithium hydride, water, and four hydrocarbon molecules. The inherent flexibility of NFs facilitates initialization with promolecular densities, markedly enhancing the efficiency of the optimization process.},
  archive      = {J_MLST},
  author       = {Alexandre de Camargo and Ricky T Q Chen and Rodrigo A Vargas-Hernández},
  doi          = {10.1088/2632-2153/ad7226},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035061},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Leveraging normalizing flows for orbital-free density functional theory},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SemiH: DFT hamiltonian neural network training with
semi-supervised learning. <em>MLST</em>, <em>5</em>(3), 035060. (<a
href="https://doi.org/10.1088/2632-2153/ad7227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decades, density functional theory (DFT) calculations have been utilized in various fields such as materials science and semiconductor devices. However, due to the inherent nature of DFT calculations, which rigorously consider interactions between atoms, they require significant computational cost. To address this, extensive research has recently focused on training neural networks to replace DFT calculations. However, previous methods for training neural networks necessitated an extensive number of DFT simulations to acquire the ground truth (Hamiltonians). Conversely, when dealing with a limited amount of training data, deep learning models often display increased errors in predicting Hamiltonians and band structures for testing data. This phenomenon poses the potential risk of generating inaccurate physical interpretations, including the emergence of unphysical branches within band structures. To tackle this challenge, we propose a novel deep learning-based method for calculating DFT Hamiltonians, specifically tailored to produce accurate results with limited training data. Our framework not only employs supervised learning with the calculated Hamiltonian but also generates pseudo Hamiltonians (targets for unlabeled data) and trains the neural networks on unlabeled data. Particularly, our approach, which leverages unlabeled data, is noteworthy as it marks the first attempt in the field of neural network Hamiltonians. Our framework showcases the superior performance of our framework compared to the state-of-the-art approach across various datasets, such as MoS 2 , Bi 2 Te 3 , HfO 2 , and InGaAs. Moreover, our framework demonstrates enhanced generalization performance by effectively utilizing unlabeled data, achieving noteworthy results when evaluated on data more complex than the training set, such as configurations with more atoms and temperature ranges outside the training data.},
  archive      = {J_MLST},
  author       = {Yucheol Cho and Guenseok Choi and Gyeongdo Ham and Mincheol Shin and Daeshik Kim},
  doi          = {10.1088/2632-2153/ad7227},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035060},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {SemiH: DFT hamiltonian neural network training with semi-supervised learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simultaneous energy and mass calibration of large-radius
jets with the ATLAS detector using a deep neural network. <em>MLST</em>,
<em>5</em>(3), 035051. (<a
href="https://doi.org/10.1088/2632-2153/ad611e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The energy and mass measurements of jets are crucial tasks for the Large Hadron Collider experiments. This paper presents a new calibration method to simultaneously calibrate these quantities for large-radius jets measured with the ATLAS detector using a deep neural network (DNN). To address the specificities of the calibration problem, special loss functions and training procedures are employed, and a complex network architecture, which includes feature annotation and residual connection layers, is used. The DNN-based calibration is compared to the standard numerical approach in an extensive series of tests. The DNN approach is found to perform significantly better in almost all of the tests and over most of the relevant kinematic phase space. In particular, it consistently improves the energy and mass resolutions, with a 30% better energy resolution obtained for transverse momenta p_{\text{T}}\gt 500 GeV.},
  archive      = {J_MLST},
  author       = {G Aad and E Aakvaag and B Abbott and K Abeling and N J Abicht and S H Abidi and A Aboulhorma and H Abramowicz and H Abreu and Y Abulaiti and B S Acharya and C Adam Bourdarios and L Adamczyk and S V Addepalli and M J Addison and J Adelman and A Adiguzel and T Adye and A A Affolder and Y Afik and M N Agaras and J Agarwala and A Aggarwal and C Agheorghiesei and A Ahmad and F Ahmadov and W S Ahmed and S Ahuja and X Ai and G Aielli and A Aikot and M Ait Tamlihat and B Aitbenchikh and I Aizenberg and M Akbiyik and T P A Åkesson and A V Akimov and D Akiyama and N N Akolkar and S Aktas and K Al Khoury and G L Alberghi and J Albert and P Albicocco and G L Albouy and S Alderweireldt and Z L Alegria and M Aleksa and I N Aleksandrov and C Alexa and T Alexopoulos and F Alfonsi and M Algren and M Alhroob and B Ali and H M J Ali and S Ali and S W Alibocus and M Aliev and G Alimonti and W Alkakhi and C Allaire and B M M Allbrooke and J F Allen and C A Allendes Flores and P P Allport and A Aloisio and F Alonso and C Alpigiani and M Alvarez Estevez and A Alvarez Fernandez and M Alves Cardoso and M G Alviggi and M Aly and Y Amaral Coutinho and A Ambler and C Amelung and M Amerl and C G Ames and D Amidei and S P Amor Dos Santos and K R Amos and V Ananiev and C Anastopoulos and T Andeen and J K Anders and S Y Andrean and A Andreazza and S Angelidakis and A Angerami and A V Anisenkov and A Annovi and C Antel and M T Anthony and E Antipov and M Antonelli and F Anulli and M Aoki and T Aoki and J A Aparisi Pozo and M A Aparo and L Aperio Bella and C Appelt and A Apyan and S J Arbiol Val and C Arcangeletti and A T H Arce and E Arena and J-F Arguin and S Argyropoulos and J-H Arling and O Arnaez and H Arnold and G Artoni and H Asada and K Asai and S Asai and N A Asbah and K Assamagan and R Astalos and S Atashi and R J Atkin and M Atkinson and H Atmani and P A Atmasiddha and K Augsten and S Auricchio and A D Auriol and V A Austrup and G Avolio and K Axiotis and G Azuelos and D Babal and H Bachacou and K Bachas and A Bachiu and F Backman and A Badea and T M Baer and P Bagnaia and M Bahmani and D Bahner and K Bai and A J Bailey and V R Bailey and J T Baines and L Baines and O K Baker and E Bakos and D Bakshi Gupta and V Balakrishnan and R Balasubramanian and E M Baldin and P Balek and E Ballabene and F Balli and L M Baltes and W K Balunas and J Balz and E Banas and M Bandieramonte and A Bandyopadhyay and S Bansal and L Barak and M Barakat and E L Barberio and D Barberis and M Barbero and M Z Barel and K N Barends and T Barillari and M-S Barisits and T Barklow and P Baron and D A Baron Moreno and A Baroncelli and G Barone and A J Barr and J D Barr and F Barreiro and J Barreiro Guimarães da Costa and U Barron and M G Barros Teixeira and S Barsov and F Bartels and R Bartoldus and A E Barton and P Bartos and A Basan and M Baselga and A Bassalat and M J Basso and C R Basson and R L Bates and S Batlamous and B Batool and M Battaglia and D Battulga and M Bauce and M Bauer and P Bauer and L T Bazzano Hurrell and J B Beacham and T Beau and J Y Beaucamp and P H Beauchemin and P Bechtle and H P Beck and K Becker and A J Beddall and V A Bednyakov and C P Bee and L J Beemster and T A Beermann and M Begalli and M Begel and A Behera and J K Behr and J F Beirer and F Beisiegel and M Belfkir and G Bella and L Bellagamba and A Bellerive and P Bellos and K Beloborodov and D Benchekroun and F Bendebba and Y Benhammou and K C Benkendorfer and L Beresford and M Beretta and E Bergeaas Kuutmann and N Berger and B Bergmann and J Beringer and G Bernardi and C Bernius and F U Bernlochner and F Bernon and A Berrocal Guardia and T Berry and P Berta and A Berthold and S Bethke and A Betti and A J Bevan and N K Bhalla and M Bhamjee and S Bhatta and D S Bhattacharya and P Bhattarai and K D Bhide and V S Bhopatkar and R M Bianchi and G Bianco and O Biebel and R Bielski and M Biglietti and C S Billingsley and M Bindi and A Bingul and C Bini and A Biondini and C J Birch-sykes and G A Bird and M Birman and M Biros and S Biryukov and T Bisanz and E Bisceglie and J P Biswal and D Biswas and K Bjørke and I Bloch and A Blue and U Blumenschein and J Blumenthal and V S Bobrovnikov and M Boehler and B Boehm and D Bogavac and A G Bogdanchikov and C Bohm and V Boisvert and P Bokan and T Bold and M Bomben and M Bona and M Boonekamp and C D Booth and A G Borbély and I S Bordulev and H M Borecka-Bielska and G Borissov and D Bortoletto and D Boscherini and M Bosman and J D Bossio Sola and K Bouaouda and N Bouchhar and J Boudreau and E V Bouhova-Thacker and D Boumediene and R Bouquet and A Boveia and J Boyd and D Boye and I R Boyko and J Bracinik and N Brahimi and G Brandt and O Brandt and F Braren and B Brau and J E Brau and R Brener and L Brenner and R Brenner and S Bressler and D Britton and D Britzger and I Brock and G Brooijmans and E Brost and L M Brown and L E Bruce and T L Bruckler and P A Bruckman de Renstrom and B Brüers and A Bruni and G Bruni and M Bruschi and N Bruscino and T Buanes and Q Buat and D Buchin and A G Buckley and O Bulekov and B A Bullard and S Burdin and C D Burgard and A M Burger and B Burghgrave and O Burlayenko and J T P Burr and C D Burton and J C Burzynski and E L Busch and V Büscher and P J Bussey and J M Butler and C M Buttar and J M Butterworth and W Buttinger and C J Buxo Vazquez and A R Buzykaev and S Cabrera Urbán and L Cadamuro and D Caforio and H Cai and Y Cai and V M M Cairo and O Cakir and N Calace and P Calafiura and G Calderini and P Calfayan and G Callea and L P Caloba and D Calvet and S Calvet and M Calvetti and R Camacho Toro and S Camarda and D Camarero Munoz and P Camarri and M T Camerlingo and D Cameron and C Camincher and M Campanelli and A Camplani and V Canale and A C Canbay and J Cantero and Y Cao and F Capocasa and M Capua and A Carbone and R Cardarelli and J C J Cardenas and F Cardillo and G Carducci and T Carli and G Carlino and J I Carlotto and B T Carlson and E M Carlson and L Carminati and A Carnelli and M Carnesale and S Caron and E Carquin and S Carrá and G Carratta and A M Carroll and T M Carter and M P Casado and M Caspar and F L Castillo and L Castillo Garcia and V Castillo Gimenez and N F Castro and A Catinaccio and J R Catmore and T Cavaliere and V Cavaliere and N Cavalli and Y C Cekmecelioglu and E Celebi and F Celli and M S Centonze and V Cepaitis and K Cerny and A S Cerqueira and A Cerri and L Cerrito and F Cerutti and B Cervato and A Cervelli and G Cesarini and S A Cetin and D Chakraborty and J Chan and W Y Chan and J D Chapman and E Chapon and B Chargeishvili and D G Charlton and M Chatterjee and C Chauhan and Y Che and S Chekanov and S V Chekulaev and G A Chelkov and A Chen and B Chen and H Chen and J Chen and M Chen and S Chen and S J Chen and X Chen and Y Chen and C L Cheng and H C Cheng and S Cheong and A Cheplakov and E Cheremushkina and E Cherepanova and R Cherkaoui El Moursli and E Cheu and K Cheung and L Chevalier and V Chiarella and G Chiarelli and N Chiedde and G Chiodini and A S Chisholm and A Chitan and M Chitishvili and M V Chizhov and K Choi and Y Chou and E Y S Chow and K L Chu and M C Chu and X Chu and J Chudoba and J J Chwastowski and D Cieri and K M Ciesla and V Cindro and A Ciocio and F Cirotto and Z H Citron and M Citterio and D A Ciubotaru and A Clark and P J Clark and C Clarry and J M Clavijo Columbie and S E Clawson and C Clement and J Clercx and Y Coadou and M Cobal and A Coccaro and R F Coelho Barrue and R Coelho Lopes De Sa and S Coelli and B Cole and J Collot and P Conde Muiño and M P Connell and S H Connell and E I Conroy and F Conventi and H G Cooke and A M Cooper-Sarkar and A Cordeiro Oudot Choi and L D Corpe and M Corradi and F Corriveau and A Cortes-Gonzalez and M J Costa and F Costanza and D Costanzo and B M Cote and G Cowan and K Cranmer and D Cremonini and S Crépé-Renaudin and F Crescioli and M Cristinziani and M Cristoforetti and V Croft and J E Crosby and G Crosetti and A Cueto and T Cuhadar Donszelmann and H Cui and Z Cui and W R Cunningham and F Curcio and P Czodrowski and M M Czurylo and M J Da Cunha Sargedas De Sousa and J V Da Fonseca Pinto and C Da Via and W Dabrowski and T Dado and S Dahbi and T Dai and D Dal Santo and C Dallapiccola and M Dam and G D’amen and V D’Amico and J Damp and J R Dandoy and M Danninger and V Dao and G Darbo and S Darmora and S J Das and S D’Auria and A D’avanzo and C David and T Davidek and B Davis-Purcell and I Dawson and H A Day-hall and K De and R De Asmundis and N De Biase and S De Castro and N De Groot and P de Jong and H De la Torre and A De Maria and A De Salvo and U De Sanctis and F De Santis and A De Santo and J B De Vivie De Regie and D V Dedovich and J Degens and A M Deiana and F Del Corso and J Del Peso and F Del Rio and L Delagrange and F Deliot and C M Delitzsch and M Della Pietra and D Della Volpe and A Dell’Acqua and L Dell’Asta and M Delmastro and P A Delsart and S Demers and M Demichev and S P Denisov and L D’Eramo and D Derendarz and F Derue and P Dervan and K Desch and C Deutsch and F A Di Bello and A Di Ciaccio and L Di Ciaccio and A Di Domenico and C Di Donato and A Di Girolamo and G Di Gregorio and A Di Luca and B Di Micco and R Di Nardo and M Diamantopoulou and F A Dias and T Dias Do Vale and M A Diaz and F G Diaz Capriles and M Didenko and E B Diehl and S Díez Cornell and C Diez Pardos and C Dimitriadi and A Dimitrievska and J Dingfelder and I-M Dinu and S J Dittmeier and F Dittus and F Djama and T Djobava and C Doglioni and A Dohnalova and J Dolejsi and Z Dolezal and K M Dona and M Donadelli and B Dong and J Donini and A D’Onofrio and M D’Onofrio and J Dopke and A Doria and N Dos Santos Fernandes and P Dougan and M T Dova and A T Doyle and M A Draguet and E Dreyer and I Drivas-koulouris and M Drnevich and M Drozdova and D Du and T A du Pree and F Dubinin and M Dubovsky and E Duchovni and G Duckeck and O A Ducu and D Duda and A Dudarev and E R Duden and M D’uffizi and L Duflot and M Dührssen and A E Dumitriu and M Dunford and S Dungs and K Dunne and A Duperrin and H Duran Yildiz and M Düren and A Durglishvili and B L Dwyer and G I Dyckes and M Dyndal and B S Dziedzic and Z O Earnshaw and G H Eberwein and B Eckerova and S Eggebrecht and E Egidio Purcino De Souza and L F Ehrke and G Eigen and K Einsweiler and T Ekelof and P A Ekman and S El Farkh and Y El Ghazali and H El Jarrari and A El Moussaouy and V Ellajosyula and M Ellert and F Ellinghaus and N Ellis and J Elmsheuser and M Elsing and D Emeliyanov and Y Enari and I Ene and S Epari and P A Erland and M Errenst and M Escalier and C Escobar and E Etzion and G Evans and H Evans and L S Evans and A Ezhilov and S Ezzarqtouni and F Fabbri and L Fabbri and G Facini and V Fadeyev and R M Fakhrutdinov and D Fakoudis and S Falciano and L F Falda Ulhoa Coelho and P J Falke and J Faltova and C Fan and Y Fan and Y Fang and M Fanti and M Faraj and Z Farazpay and A Farbin and A Farilla and T Farooque and S M Farrington and F Fassi and D Fassouliotis and M Faucci Giannelli and W J Fawcett and L Fayard and P Federic and P Federicova and O L Fedin and M Feickert and L Feligioni and D E Fellers and C Feng and M Feng and Z Feng and M J Fenton and L Ferencz and R A M Ferguson and S I Fernandez Luengo and P Fernandez Martinez and M J V Fernoux and J Ferrando and A Ferrari and P Ferrari and R Ferrari and D Ferrere and C Ferretti and F Fiedler and P Fiedler and A Filipčič and E K Filmer and F Filthaut and M C N Fiolhais and L Fiorini and W C Fisher and T Fitschen and P M Fitzhugh and I Fleck and P Fleischmann and T Flick and M Flores and L R Flores Castillo and L Flores Sanz De Acedo and F M Follega and N Fomin and J H Foo and A Formica and A C Forti and E Fortin and A W Fortman and M G Foti and L Fountas and D Fournier and H Fox and P Francavilla and S Francescato and S Franchellucci and M Franchini and S Franchino and D Francis and L Franco and V Franco Lima and L Franconi and M Franklin and G Frattari and W S Freund and Y Y Frid and J Friend and N Fritzsche and A Froch and D Froidevaux and J A Frost and Y Fu and S Fuenzalida Garrido and M Fujimoto and K Y Fung and E Furtado De Simas Filho and M Furukawa and J Fuster and A Gabrielli and P Gadow and G Gagliardi and L G Gagnon and S Galantzan and E J Gallas and B J Gallop and K K Gan and S Ganguly and Y Gao and F M Garay Walls and B Garcia and C García and A Garcia Alonso and A G Garcia Caffaro and J E García Navarro and M Garcia-Sciveres and G L Gardner and R W Gardner and N Garelli and D Garg and R B Garg and J M Gargan and C A Garner and C M Garvey and P Gaspar and V K Gassmann and G Gaudio and V Gautam and P Gauzzi and I L Gavrilenko and A Gavrilyuk and C Gay and G Gaycken and E N Gazis and A A Geanta and C M Gee and A Gekow and C Gemme and M H Genest and A D Gentry and S George and W F George and T Geralis and P Gessinger-Befurt and M E Geyik and M Ghani and M Ghneimat and K Ghorbanian and A Ghosal and A Ghosh and B Giacobbe and S Giagu and T Giani and P Giannetti and A Giannini and S M Gibson and M Gignac and D T Gil and A K Gilbert and B J Gilbert and D Gillberg and G Gilles and L Ginabat and D M Gingrich and M P Giordani and P F Giraud and G Giugliarelli and D Giugni and F Giuli and I Gkialas and L K Gladilin and C Glasman and G R Gledhill and G Glemža and M Glisic and I Gnesi and Y Go and M Goblirsch-Kolb and B Gocke and D Godin and B Gokturk and S Goldfarb and T Golling and M G D Gololo and D Golubkov and J P Gombas and A Gomes and G Gomes Da Silva and A J Gomez Delegido and R Gonçalo and L Gonella and A Gongadze and F Gonnella and J L Gonski and R Y González Andana and S González de la Hoz and R Gonzalez Lopez and C Gonzalez Renteria and M V Gonzalez Rodrigues and R Gonzalez Suarez and S Gonzalez-Sevilla and G R Gonzalvo Rodriguez and L Goossens and B Gorini and E Gorini and A Gorišek and T C Gosart and A T Goshaw and M I Gostkin and S Goswami and C A Gottardo and S A Gotz and M Gouighri and V Goumarre and A G Goussiou and N Govender and I Grabowska-Bold and K Graham and E Gramstad and S Grancagnolo and C M Grant and P M Gravila and F G Gravili and H M Gray and M Greco and C Grefe and I M Gregor and P Grenier and S G Grewe and A A Grillo and K Grimm and S Grinstein and J -F Grivaz and E Gross and J Grosse-Knetter and J C Grundy and L Guan and C Gubbels and J G R Guerrero Rojas and G Guerrieri and F Guescini and R Gugel and J A M Guhit and A Guida and E Guilloton and S Guindon and F Guo and J Guo and L Guo and Y Guo and R Gupta and S Gurbuz and S S Gurdasani and G Gustavino and M Guth and P Gutierrez and L F Gutierrez Zagazeta and M Gutsche and C Gutschow and C Gwenlan and C B Gwilliam and E S Haaland and A Haas and M Habedank and C Haber and H K Hadavand and A Hadef and S Hadzic and A I Hagan and J J Hahn and E H Haines and M Haleem and J Haley and J J Hall and G D Hallewell and L Halser and K Hamano and M Hamer and G N Hamity and E J Hampshire and J Han and K Han and L Han and S Han and Y F Han and K Hanagaki and M Hance and D A Hangal and H Hanif and M D Hank and J B Hansen and P H Hansen and K Hara and D Harada and T Harenberg and S Harkusha and M L Harris and Y T Harris and J Harrison and N M Harrison and P F Harrison and N M Hartman and N M Hartmann and Y Hasegawa and R Hauser and C M Hawkes and R J Hawkings and Y Hayashi and S Hayashida and D Hayden and C Hayes and R L Hayes and C P Hays and J M Hays and H S Hayward and F He and M He and Y He and N B Heatley and V Hedberg and A L Heggelund and N D Hehir and C Heidegger and K K Heidegger and W D Heidorn and J Heilman and S Heim and T Heim and J G Heinlein and J J Heinrich and L Heinrich and J Hejbal and A Held and S Hellesund and C M Helling and S Hellman and R C W Henderson and L Henkelmann and A M Henriques Correia and H Herde and Y Hernández Jiménez and L M Herrmann and T Herrmann and G Herten and R Hertenberger and L Hervas and M E Hesping and N P Hessey and E Hill and S J Hillier and J R Hinds and F Hinterkeuser and M Hirose and S Hirose and D Hirschbuehl and T G Hitchings and B Hiti and J Hobbs and R Hobincu and N Hod and M C Hodgkinson and B H Hodkinson and A Hoecker and D D Hofer and J Hofer and T Holm and M Holzbock and L B A H Hommels and B P Honan and J Hong and T M Hong and B H Hooberman and W H Hopkins and Y Horii and S Hou and A S Howard and J Howarth and J Hoya and M Hrabovsky and A Hrynevich and T Hryn’ova and P J Hsu and S -C Hsu and Q Hu and S Huang and X Huang and Y Huang and Z Huang and Z Hubacek and M Huebner and F Huegging and T B Huffman and C A Hugli and M Huhtinen and S K Huiberts and R Hulsken and N Huseynov and J Huston and J Huth and R Hyneman and G Iacobucci and G Iakovidis and I Ibragimov and L Iconomidou-Fayard and J P Iddon and P Iengo and R Iguchi and T Iizawa and Y Ikegami and N Ilic and H Imam and M Ince Lezki and T Ingebretsen Carlson and G Introzzi and M Iodice and V Ippolito and R K Irwin and M Ishino and W Islam and C Issever and S Istin and H Ito and R Iuppa and A Ivina and J M Izen and V Izzo and P Jacka and P Jackson and B P Jaeger and C S Jagfeld and G Jain and P Jain and K Jakobs and T Jakoubek and J Jamieson and K W Janas and M Javurkova and L Jeanty and J Jejelava and P Jenni and C E Jessiman and C Jia and J Jia and X Jia and Z Jia and S Jiggins and J Jimenez Pena and S Jin and A Jinaru and O Jinnouchi and P Johansson and K A Johns and J W Johnson and D M Jones and E Jones and P Jones and R W L Jones and T J Jones and H L Joos and R Joshi and J Jovicevic and X Ju and J J Junggeburth and T Junkermann and A Juste Rozas and M K Juzek and S Kabana and A Kaczmarska and M Kado and H Kagan and M Kagan and A Kahn and C Kahra and T Kaji and E Kajomovitz and N Kakati and I Kalaitzidou and C W Kalderon and N J Kang and D Kar and K Karava and M J Kareem and E Karentzos and I Karkanias and O Karkout and S N Karpov and Z M Karpova and V Kartvelishvili and A N Karyukhin and E Kasimi and J Katzy and S Kaur and K Kawade and M P Kawale and C Kawamoto and T Kawamoto and E F Kay and F I Kaya and S Kazakos and V F Kazanin and Y Ke and J M Keaveney and R Keeler and G V Kehris and J S Keller and A S Kelly and J J Kempster and P D Kennedy and O Kepka and B P Kerridge and S Kersten and B P Kerševan and S Keshri and L Keszeghova and S Ketabchi Haghighat and R A Khan and A Khanov and A G Kharlamov and T Kharlamova and E E Khoda and M Kholodenko and T J Khoo and G Khoriauli and J Khubua and Y A R Khwaira and B Kibirige and A Kilgallon and D W Kim and Y K Kim and N Kimura and M K Kingston and A Kirchhoff and C Kirfel and F Kirfel and J Kirk and A E Kiryunin and C Kitsaki and O Kivernyk and M Klassen and C Klein and L Klein and M H Klein and S B Klein and U Klein and P Klimek and A Klimentov and T Klioutchnikova and P Kluit and S Kluth and E Kneringer and T M Knight and A Knue and R Kobayashi and D Kobylianskii and S F Koch and M Kocian and P Kodyš and D M Koeck and P T Koenig and T Koffas and O Kolay and I Koletsou and T Komarek and K Köneke and A X Y Kong and T Kono and N Konstantinidis and P Kontaxakis and B Konya and R Kopeliansky and S Koperny and K Korcyl and K Kordas and A Korn and S Korn and I Korolkov and N Korotkova and B Kortman and O Kortner and S Kortner and W H Kostecka and V V Kostyukhin and A Kotsokechagia and A Kotwal and A Koulouris and A Kourkoumeli-Charalampidi and C Kourkoumelis and E Kourlitis and O Kovanda and R Kowalewski and W Kozanecki and A S Kozhin and V A Kramarenko and G Kramberger and P Kramer and M W Krasny and A Krasznahorkay and J W Kraus and J A Kremer and T Kresse and J Kretzschmar and K Kreul and P Krieger and S Krishnamurthy and M Krivos and K Krizka and K Kroeninger and H Kroha and J Kroll and K S Krowpman and U Kruchonak and H Krüger and N Krumnack and M C Kruse and O Kuchinskaia and S Kuday and S Kuehn and R Kuesters and T Kuhl and V Kukhtin and Y Kulchitsky and S Kuleshov and M Kumar and N Kumari and P Kumari and A Kupco and T Kupfer and A Kupich and O Kuprash and H Kurashige and L L Kurchaninov and O Kurdysh and Y A Kurochkin and A Kurova and M Kuze and A K Kvam and J Kvita and T Kwan and N G Kyriacou and L A O Laatu and C Lacasta and F Lacava and H Lacker and D Lacour and N N Lad and E Ladygin and B Laforge and T Lagouri and F Z Lahbabi and S Lai and I K Lakomiec and N Lalloue and J E Lambert and S Lammers and W Lampl and C Lampoudis and G Lamprinoudis and A N Lancaster and E Lançon and U Landgraf and M P J Landon and V S Lang and O K B Langrekken and A J Lankford and F Lanni and K Lantzsch and A Lanza and A Lapertosa and J F Laporte and T Lari and F Lasagni Manghi and M Lassnig and V Latonova and A Laudrain and A Laurier and S D Lawlor and Z Lawrence and R Lazaridou and M Lazzaroni and B Le and E M Le Boulicaut and B Leban and A Lebedev and M LeBlanc and F Ledroit-Guillon and A C A Lee and S C Lee and S Lee and T F Lee and L L Leeuw and H P Lefebvre and M Lefebvre and C Leggett and G Lehmann Miotto and M Leigh and W A Leight and W Leinonen and A Leisos and M A L Leite and C E Leitgeb and R Leitner and K J C Leney and T Lenz and S Leone and C Leonidopoulos and A Leopold and C Leroy and R Les and C G Lester and M Levchenko and J Levêque and L J Levinson and G Levrini and M P Lewicki and D J Lewis and A Li and B Li and C Li and C-Q Li and H Li and J Li and K Li and L Li and M Li and Q Y Li and S Li and T Li and X Li and Z Li and S Liang and Z Liang and M Liberatore and B Liberti and K Lie and J Lieber Marin and H Lien and K Lin and R E Lindley and J H Lindon and E Lipeles and A Lipniacka and A Lister and J D Little and B Liu and B X Liu and D Liu and J B Liu and J K K Liu and K Liu and M Liu and M Y Liu and P Liu and Q Liu and X Liu and Y Liu and Y L Liu and Y W Liu and J Llorente Merino and S L Lloyd and E M Lobodzinska and P Loch and T Lohse and K Lohwasser and E Loiacono and M Lokajicek and J D Lomas and J D Long and I Longarini and L Longo and R Longo and I Lopez Paz and A Lopez Solis and N Lorenzo Martinez and A M Lory and G Löschcke Centeno and O Loseva and X Lou and A Lounis and P A Love and G Lu and M Lu and S Lu and Y J Lu and H J Lubatti and C Luci and F L Lucio Alves and F Luehring and I Luise and O Lukianchuk and O Lundberg and B Lund-Jensen and N A Luongo and M S Lutz and A B Lux and D Lynn and R Lysak and E Lytken and V Lyubushkin and T Lyubushkina and M M Lyukova and H Ma and K Ma and L L Ma and W Ma and Y Ma and D M Mac Donell and G Maccarrone and J C MacDonald and P C Machado De Abreu Farias and R Madar and W F Mader and T Madula and J Maeda and T Maeno and H Maguire and V Maiboroda and A Maio and K Maj and O Majersky and S Majewski and N Makovec and V Maksimovic and B Malaescu and Pa Malecki and V P Maleev and F Malek and M Mali and D Malito and U Mallik and S Maltezos and S Malyukov and J Mamuzic and G Mancini and M N Mancini and G Manco and J P Mandalia and I Mandić and L Manhaes de Andrade Filho and I M Maniatis and J Manjarres Ramos and D C Mankad and A Mann and S Manzoni and L Mao and X Mapekula and A Marantis and G Marchiori and M Marcisovsky and C Marcon and M Marinescu and S Marium and M Marjanovic and E J Marshall and Z Marshall and S Marti-Garcia and T A Martin and V J Martin and B Martin dit Latour and L Martinelli and M Martinez and P Martinez Agullo and V I Martinez Outschoorn and P Martinez Suarez and S Martin-Haugh and V S Martoiu and A C Martyniuk and A Marzin and D Mascione and L Masetti and T Mashimo and J Masik and A L Maslennikov and P Massarotti and P Mastrandrea and A Mastroberardino and T Masubuchi and T Mathisen and J Matousek and N Matsuzawa and J Maurer and B Maček and D A Maximov and R Mazini and I Maznas and M Mazza and S M Mazza and E Mazzeo and C Mc Ginn and J P Mc Gowan and S P Mc Kee and C C McCracken and E F McDonald and A E McDougall and J A Mcfayden and R P McGovern and G Mchedlidze and R P Mckenzie and T C Mclachlan and D J Mclaughlin and S J McMahon and C M Mcpartland and R A McPherson and S Mehlhase and A Mehta and D Melini and B R Mellado Garcia and A H Melo and F Meloni and A M Mendes Jacques Da Costa and H Y Meng and L Meng and S Menke and M Mentink and E Meoni and G Mercado and C Merlassino and L Merola and C Meroni and J Metcalfe and A S Mete and C Meyer and J-P Meyer and R P Middleton and L Mijović and G Mikenberg and M Mikestikova and M Mikuž and H Mildner and A Milic and D W Miller and E H Miller and L S Miller and A Milov and D A Milstead and T Min and A A Minaenko and I A Minashvili and L Mince and A I Mincer and B Mindur and M Mineev and Y Mino and L M Mir and M Miralles Lopez and M Mironova and A Mishima and M C Missio and A Mitra and V A Mitsou and Y Mitsumori and O Miu and P S Miyagawa and T Mkrtchyan and M Mlinarevic and T Mlinarevic and M Mlynarikova and S Mobius and P Mogg and M H Mohamed Farook and A F Mohammed and S Mohapatra and G Mokgatitswane and L Moleri and B Mondal and S Mondal and K Mönig and E Monnier and L Monsonis Romero and J Montejo Berlingen and M Montella and F Montereali and F Monticelli and S Monzani and N Morange and A L Moreira De Carvalho and M Moreno Llácer and C Moreno Martinez and P Morettini and S Morgenstern and M Morii and M Morinaga and F Morodei and L Morvaj and P Moschovakos and B Moser and M Mosidze and T Moskalets and P Moskvitina and J Moss and A Moussa and E J W Moyse and O Mtintsilana and S Muanza and J Mueller and D Muenstermann and R Müller and G A Mullier and A J Mullin and J J Mullin and D P Mungo and D Munoz Perez and F J Munoz Sanchez and M Murin and W J Murray and M Muškinja and C Mwewa and A G Myagkov and A J Myers and G Myers and M Myska and B P Nachman and O Nackenhorst and K Nagai and K Nagano and J L Nagle and E Nagy and A M Nairz and Y Nakahama and K Nakamura and K Nakkalil and H Nanjo and R Narayan and E A Narayanan and I Naryshkin and M Naseri and S Nasri and C Nass and G Navarro and J Navarro-Gonzalez and R Nayak and A Nayaz and P Y Nechaeva and F Nechansky and L Nedic and T J Neep and A Negri and M Negrini and C Nellist and C Nelson and K Nelson and S Nemecek and M Nessi and M S Neubauer and F Neuhaus and J Neundorf and R Newhouse and P R Newman and C W Ng and Y W Y Ng and B Ngair and H D N Nguyen and R B Nickerson and R Nicolaidou and J Nielsen and M Niemeyer and J Niermann and N Nikiforou and V Nikolaenko and I Nikolic-Audit and K Nikolopoulos and P Nilsson and I Ninca and H R Nindhito and G Ninio and A Nisati and N Nishu and R Nisius and J-E Nitschke and E K Nkadimeng and T Nobe and D L Noel and T Nommensen and M B Norfolk and R R B Norisam and B J Norman and M Noury and J Novak and T Novak and L Novotny and R Novotny and L Nozka and K Ntekas and N M J Nunes De Moura Junior and J Ocariz and A Ochi and I Ochoa and S Oerdek and J T Offermann and A Ogrodnik and A Oh and C C Ohm and H Oide and R Oishi and M L Ojeda and Y Okumura and L F Oleiro Seabra and S A Olivares Pino and D Oliveira Damazio and D Oliveira Goncalves and J L Oliver and Ö O Öncel and A P O’Neill and A Onofre and P U E Onyisi and M J Oreglia and G E Orellana and D Orestano and N Orlando and R S Orr and V O’Shea and L M Osojnak and R Ospanov and G Otero y Garzon and H Otono and P S Ott and G J Ottino and M Ouchrif and F Ould-Saada and M Owen and R E Owen and K Y Oyulmaz and V E Ozcan and F Ozturk and N Ozturk and S Ozturk and H A Pacey and A Pacheco Pages and C Padilla Aranda and G Padovano and S Pagan Griso and G Palacino and A Palazzo and J Pampel and J Pan and T Pan and D K Panchal and C E Pandini and J G Panduro Vazquez and H D Pandya and H Pang and P Pani and G Panizzo and L Panwar and L Paolozzi and S Parajuli and A Paramonov and C Paraskevopoulos and D Paredes Hernandez and A Pareti and K R Park and T H Park and M A Parker and F Parodi and E W Parrish and V A Parrish and J A Parsons and U Parzefall and B Pascual Dias and L Pascual Dominguez and E Pasqualucci and S Passaggio and F Pastore and P Patel and U M Patel and J R Pater and T Pauly and C I Pazos and J Pearkes and M Pedersen and R Pedro and S V Peleganchuk and O Penc and E A Pender and G D Penn and K E Penski and M Penzin and B S Peralva and A P Pereira Peixoto and L Pereira Sanchez and D V Perepelitsa and E Perez Codina and M Perganti and H Pernegger and O Perrin and K Peters and R F Y Peters and B A Petersen and T C Petersen and E Petit and V Petousis and C Petridou and T Petru and A Petrukhin and M Pettee and N E Pettersson and A Petukhov and K Petukhova and R Pezoa and L Pezzotti and G Pezzullo and T M Pham and T Pham and P W Phillips and G Piacquadio and E Pianori and F Piazza and R Piegaia and D Pietreanu and A D Pilkington and M Pinamonti and J L Pinfold and B C Pinheiro Pereira and A E Pinto Pinoargote and L Pintucci and K M Piper and A Pirttikoski and D A Pizzi and L Pizzimento and A Pizzini and M -A Pleier and V Plesanovs and V Pleskot and E Plotnikova and G Poddar and R Poettgen and L Poggioli and I Pokharel and S Polacek and G Polesello and A Poley and A Polini and C S Pollard and Z B Pollock and E Pompa Pacchi and D Ponomarenko and L Pontecorvo and S Popa and G A Popeneciu and A Poreba and D M Portillo Quintero and S Pospisil and M A Postill and P Postolache and K Potamianos and P A Potepa and I N Potrap and C J Potter and H Potti and T Poulsen and J Poveda and M E Pozo Astigarraga and A Prades Ibanez and J Pretel and D Price and M Primavera and M A Principe Martin and R Privara and T Procter and M L Proffitt and N Proklova and K Prokofiev and G Proto and J Proudfoot and M Przybycien and W W Przygoda and A Psallidas and J E Puddefoot and D Pudzha and D Pyatiizbyantseva and J Qian and D Qichen and Y Qin and T Qiu and A Quadt and M Queitsch-Maitland and G Quetant and R P Quinn and G Rabanal Bolanos and D Rafanoharana and F Ragusa and J L Rainbolt and J A Raine and S Rajagopalan and E Ramakoti and I A Ramirez-Berend and K Ran and N P Rapheeha and H Rasheed and V Raskina and D F Rassloff and A Rastogi and S Rave and B Ravina and I Ravinovich and M Raymond and A L Read and N P Readioff and D M Rebuzzi and G Redlinger and A S Reed and K Reeves and J A Reidelsturz and D Reikher and A Rej and C Rembser and M Renda and M B Rendel and F Renner and A G Rennie and A L Rescia and S Resconi and M Ressegotti and S Rettie and J G Reyes Rivera and E Reynolds and O L Rezanova and P Reznicek and H Riani and N Ribaric and E Ricci and R Richter and S Richter and E Richter-Was and M Ridel and S Ridouani and P Rieck and P Riedler and E M Riefel and J O Rieger and M Rijssenbeek and M Rimoldi and L Rinaldi and T T Rinn and M P Rinnagel and G Ripellino and I Riu and J C Rivera Vergara and F Rizatdinova and E Rizvi and B R Roberts and S H Robertson and D Robinson and C M Robles Gajardo and M Robles Manzano and A Robson and A Rocchi and C Roda and S Rodriguez Bosca and Y Rodriguez Garcia and A Rodriguez Rodriguez and A M Rodríguez Vera and S Roe and J T Roemer and A R Roepe-Gier and J Roggel and O Røhne and R A Rojas and C P A Roland and J Roloff and A Romaniouk and E Romano and M Romano and A C Romero Hernandez and N Rompotis and L Roos and S Rosati and B J Rosser and E Rossi and L P Rossi and L Rossini and R Rosten and M Rotaru and B Rottler and C Rougier and D Rousseau and D Rousso and A Roy and S Roy-Garand and A Rozanov and Z M A Rozario and Y Rozen and A Rubio Jimenez and A J Ruby and V H Ruelas Rivera and T A Ruggeri and A Ruggiero and A Ruiz-Martinez and A Rummler and Z Rurikova and N A Rusakovich and H L Russell and G Russo and J P Rutherfoord and S Rutherford Colmenares and K Rybacki and M Rybar and E B Rye and A Ryzhov and J A Sabater Iglesias and P Sabatini and H F-W Sadrozinski and F Safai Tehrani and B Safarzadeh Samani and M Safdari and S Saha and M Sahinsoy and A Saibel and M Saimpert and M Saito and T Saito and D Salamani and A Salnikov and J Salt and A Salvador Salas and D Salvatore and F Salvatore and A Salzburger and D Sammel and D Sampsonidis and D Sampsonidou and J Sánchez and V Sanchez Sebastian and H Sandaker and C O Sander and J A Sandesara and M Sandhoff and C Sandoval and D P C Sankey and T Sano and A Sansoni and L Santi and C Santoni and H Santos and A Santra and K A Saoucha and J G Saraiva and J Sardain and O Sasaki and K Sato and C Sauer and F Sauerburger and E Sauvan and P Savard and R Sawada and C Sawyer and L Sawyer and I Sayago Galvan and C Sbarra and A Sbrizzi and T Scanlon and J Schaarschmidt and U Schäfer and A C Schaffer and D Schaile and R D Schamberger and C Scharf and M M Schefer and V A Schegelsky and D Scheirich and F Schenck and M Schernau and C Scheulen and C Schiavi and M Schioppa and B Schlag and K E Schleicher and S Schlenker and J Schmeing and M A Schmidt and K Schmieden and C Schmitt and N Schmitt and S Schmitt and L Schoeffel and A Schoening and P G Scholer and E Schopf and M Schott and J Schovancova and S Schramm and T Schroer and H-C Schultz-Coulon and M Schumacher and B A Schumm and Ph Schune and A J Schuy and H R Schwartz and A Schwartzman and T A Schwarz and Ph Schwemling and R Schwienhorst and A Sciandra and G Sciolla and F Scuri and C D Sebastiani and K Sedlaczek and P Seema and S C Seidel and A Seiden and B D Seidlitz and C Seitz and J M Seixas and G Sekhniaidze and L Selem and N Semprini-Cesari and D Sengupta and V Senthilkumar and L Serin and L Serkin and M Sessa and H Severini and F Sforza and A Sfyrla and Q Sha and E Shabalina and R Shaheen and J D Shahinian and D Shaked Renous and L Y Shan and M Shapiro and A Sharma and A S Sharma and P Sharma and P B Shatalov and K Shaw and S M Shaw and A Shcherbakova and Q Shen and D J Sheppard and P Sherwood and L Shi and X Shi and C O Shimmin and J D Shinner and I P J Shipsey and S Shirabe and M Shiyakova and J Shlomi and M J Shochet and J Shojaii and D R Shope and B Shrestha and S Shrestha and E M Shrif and M J Shroff and P Sicho and A M Sickles and E Sideras Haddad and A Sidoti and F Siegert and Dj Sijacki and F Sili and J M Silva and M V Silva Oliveira and S B Silverstein and S Simion and R Simoniello and E L Simpson and H Simpson and L R Simpson and N D Simpson and S Simsek and S Sindhu and P Sinervo and S Singh and S Sinha and M Sioli and I Siral and E Sitnikova and J Sjölin and A Skaf and E Skorda and P Skubic and M Slawinska and V Smakhtin and B H Smart and S Yu Smirnov and Y Smirnov and L N Smirnova and O Smirnova and A C Smith and E A Smith and H A Smith and J L Smith and R Smith and M Smizanska and K Smolek and A A Snesarev and S R Snider and H L Snoek and S Snyder and R Sobie and A Soffer and C A Solans Sanchez and E Yu Soldatov and U Soldevila and A A Solodkov and S Solomon and A Soloshenko and K Solovieva and O V Solovyanov and V Solovyev and P Sommer and A Sonay and W Y Song and A Sopczak and A L Sopio and F Sopkova and J D Sorenson and I R Sotarriva Alvarez and V Sothilingam and O J Soto Sandoval and S Sottocornola and R Soualah and Z Soumaimi and D South and N Soybelman and S Spagnolo and M Spalla and D Sperlich and G Spigo and S Spinali and D P Spiteri and M Spousta and E J Staats and R Stamen and A Stampekis and M Standke and E Stanecka and M V Stange and B Stanislaus and M M Stanitzki and B Stapf and E A Starchenko and G H Stark and J Stark and P Staroba and P Starovoitov and S Stärz and R Staszewski and G Stavropoulos and J Steentoft and P Steinberg and B Stelzer and H J Stelzer and O Stelzer-Chilton and H Stenzel and T J Stevenson and G A Stewart and J R Stewart and M C Stockton and G Stoicea and M Stolarski and S Stonjek and A Straessner and J Strandberg and S Strandberg and M Stratmann and M Strauss and T Strebler and P Strizenec and R Ströhmer and D M Strom and R Stroynowski and A Strubig and S A Stucci and B Stugu and J Stupak and N A Styles and D Su and S Su and W Su and X Su and D Suchy and K Sugizaki and V V Sulin and M J Sullivan and D M S Sultan and L Sultanaliyeva and S Sultansoy and T Sumida and S Sun and O Sunneborn Gudnadottir and N Sur and M R Sutton and H Suzuki and M Svatos and M Swiatlowski and T Swirski and I Sykora and M Sykora and T Sykora and D Ta and K Tackmann and A Taffard and R Tafirout and J S Tafoya Vargas and Y Takubo and M Talby and A A Talyshev and K C Tam and N M Tamir and A Tanaka and J Tanaka and R Tanaka and M Tanasini and Z Tao and S Tapia Araya and S Tapprogge and A Tarek Abouelfadl Mohamed and S Tarem and K Tariq and G Tarna and G F Tartarelli and P Tas and M Tasevsky and E Tassi and A C Tate and G Tateno and Y Tayalati and G N Taylor and W Taylor and A S Tee and R Teixeira De Lima and P Teixeira-Dias and J J Teoh and K Terashi and J Terron and S Terzo and M Testa and R J Teuscher and A Thaler and O Theiner and N Themistokleous and T Theveneaux-Pelzer and O Thielmann and D W Thomas and J P Thomas and E A Thompson and P D Thompson and E Thomson and Y Tian and V Tikhomirov and Yu A Tikhonov and S Timoshenko and D Timoshyn and E X L Ting and P Tipton and S H Tlou and A Tnourji and K Todome and S Todorova-Nova and S Todt and M Togawa and J Tojo and S Tokár and K Tokushuku and O Toldaiev and R Tombs and M Tomoto and L Tompkins and K W Topolnicki and E Torrence and H Torres and E Torró Pastor and M Toscani and C Tosciri and M Tost and D R Tovey and A Traeet and I S Trandafir and T Trefzger and A Tricoli and I M Trigger and S Trincaz-Duvoid and D A Trischuk and B Trocmé and L Truong and M Trzebinski and A Trzupek and F Tsai and M Tsai and A Tsiamis and P V Tsiareshka and S Tsigaridas and A Tsirigotis and V Tsiskaridze and E G Tskhadadze and M Tsopoulou and Y Tsujikawa and I I Tsukerman and V Tsulaia and S Tsuno and K Tsuri and D Tsybychev and Y Tu and A Tudorache and V Tudorache and A N Tuna and S Turchikhin and I Turk Cakir and R Turra and T Turtuvshin and P M Tuts and S Tzamarias and P Tzanis and E Tzovara and F Ukegawa and P A Ulloa Poblete and E N Umaka and G Unal and M Unal and A Undrus and G Unel and J Urban and P Urquijo and P Urrejola and G Usai and R Ushioda and M Usman and Z Uysal and V Vacek and B Vachon and K O H Vadla and T Vafeiadis and A Vaitkus and C Valderanis and E Valdes Santurio and M Valente and S Valentinetti and A Valero and E Valiente Moreno and A Vallier and J A Valls Ferrer and D R Van Arneman and T R Van Daalen and A Van Der Graaf and P Van Gemmeren and M Van Rijnbach and S Van Stroud and I Van Vulpen and M Vanadia and W Vandelli and E R Vandewall and D Vannicola and L Vannoli and R Vari and E W Varnes and C Varni and T Varol and D Varouchas and L Varriale and K E Varvell and M E Vasile and L Vaslin and G A Vasquez and A Vasyukov and R Vavricka and F Vazeille and T Vazquez Schroeder and J Veatch and V Vecchio and M J Veen and I Veliscek and L M Veloce and F Veloso and S Veneziano and A Ventura and S Ventura Gonzalez and A Verbytskyi and M Verducci and C Vergis and M Verissimo De Araujo and W Verkerke and J C Vermeulen and C Vernieri and M Vessella and M C Vetterli and A Vgenopoulos and N Viaux Maira and T Vickey and O E Vickey Boeriu and G H A Viehhauser and L Vigani and M Villa and M Villaplana Perez and E M Villhauer and E Vilucchi and M G Vincter and G S Virdee and A Vishwakarma and A Visibile and C Vittori and I Vivarelli and E Voevodina and F Vogel and J C Voigt and P Vokac and Yu Volkotrub and J Von Ahnen and E Von Toerne and B Vormwald and V Vorobel and K Vorobev and M Vos and K Voss and M Vozak and L Vozdecky and N Vranjes and M Vranjes Milosavljevic and M Vreeswijk and N K Vu and R Vuillermet and O Vujinovic and I Vukotic and S Wada and C Wagner and J M Wagner and W Wagner and S Wahdan and H Wahlberg and M Wakida and J Walder and R Walker and W Walkowiak and A Wall and E J Wallin and T Wamorkar and A Z Wang and C Wang and H Wang and J Wang and R -J Wang and R Wang and S M Wang and S Wang and T Wang and W T Wang and W Wang and X Wang and Y Wang and Z Wang and A Warburton and R J Ward and N Warrack and S Waterhouse and A T Watson and H Watson and M F Watson and E Watton and G Watts and B M Waugh and C Weber and H A Weber and M S Weber and S M Weber and C Wei and Y Wei and A R Weidberg and E J Weik and J Weingarten and M Weirich and C Weiser and C J Wells and T Wenaus and B Wendland and T Wengler and N S Wenke and N Wermes and M Wessels and A M Wharton and A S White and A White and M J White and D Whiteson and L Wickremasinghe and W Wiedenmann and M Wielers and C Wiglesworth and D J Wilbern and H G Wilkens and D M Williams and H H Williams and S Williams and S Willocq and B J Wilson and P J Windischhofer and F I Winkel and F Winklmeier and B T Winter and J K Winter and M Wittgen and M Wobisch and Z Wolffs and J Wollrath and M W Wolter and H Wolters and E L Woodward and S D Worm and B K Wosiek and K W Woźniak and S Wozniewski and K Wraight and C Wu and M Wu and S L Wu and X Wu and Y Wu and Z Wu and J Wuerzinger and T R Wyatt and B M Wynne and S Xella and L Xia and M Xia and J Xiang and M Xie and X Xie and S Xin and A Xiong and J Xiong and D Xu and H Xu and L Xu and R Xu and T Xu and Y Xu and Z Xu and B Yabsley and S Yacoob and Y Yamaguchi and E Yamashita and H Yamauchi and T Yamazaki and Y Yamazaki and J Yan and S Yan and Z Yan and H J Yang and H T Yang and S Yang and T Yang and X Yang and Y Yang and Z Yang and W-M Yao and H Ye and J Ye and S Ye and X Ye and Y Yeh and I Yeletskikh and B K Yeo and M R Yexley and P Yin and K Yorita and S Younas and C J S Young and C Young and C Yu and Y Yu and M Yuan and R Yuan and L Yue and M Zaazoua and B Zabinski and E Zaid and Z K Zak and T Zakareishvili and N Zakharchuk and S Zambito and J A Zamora Saa and J Zang and D Zanzi and O Zaplatilek and C Zeitnitz and H Zeng and J C Zeng and D T Zenger Jr and O Zenin and T Ženiš and S Zenz and S Zerradi and D Zerwas and M Zhai and D F Zhang and J Zhang and K Zhang and L Zhang and P Zhang and R Zhang and S Zhang and T Zhang and X Zhang and Y Zhang and Z Zhang and H Zhao and T Zhao and Y Zhao and Z Zhao and A Zhemchugov and J Zheng and K Zheng and X Zheng and Z Zheng and D Zhong and B Zhou and H Zhou and N Zhou and Y Zhou and C G Zhu and J Zhu and Y Zhu and X Zhuang and K Zhukov and N I Zimine and J Zinsser and M Ziolkowski and L Živković and A Zoccoli and K Zoch and T G Zorbas and O Zormpa and W Zou and L Zwalinski and The ATLAS Collaboration},
  doi          = {10.1088/2632-2153/ad611e},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {035051},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Simultaneous energy and mass calibration of large-radius jets with the ATLAS detector using a deep neural network},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards a comprehensive visualisation of structure in large
scale data sets. <em>MLST</em>, <em>5</em>(3), 030503. (<a
href="https://doi.org/10.1088/2632-2153/ad6fea">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction methods are fundamental to the exploration and visualisation of large data sets. Basic requirements for unsupervised data exploration are flexibility and scalability. However, current methods have computational limitations that restrict our ability to explore data structures to the lower range of scales. We focus on t-SNE and propose a chunk-and-mix protocol that enables the parallel implementation of this algorithm, as well as a self-adaptive parametric scheme that facilitates its parametric configuration. As a proof of concept, we present the pt-SNE algorithm, a parallel version of Barnes-Hat-SNE (an O\left(n\,\mathrm{log}\,n\right) implementation of t-SNE). In pt-SNE, a single free parameter for the size of the neighbourhood, namely the perplexity, modulates the visualisation of the data structure at different scales, from local to global. Thanks to parallelisation, the runtime of the algorithm remains almost independent of the perplexity, which extends the range of scales to be analysed. The pt-SNE converges to a good global embedding comparable to current solutions, although it adds little noise at the local scale. This noise illustrates an unavoidable trade-off between computational speed and accuracy. We expect the same approach to be applicable to faster embedding algorithms than Barnes-Hat-SNE, such as Fast-Fourier Interpolation-based t-SNE or Uniform Manifold Approximation and Projection, thus extending the state of the art and allowing a more comprehensive visualisation and analysis of data structures.},
  archive      = {J_MLST},
  author       = {Joan Garriga and Frederic Bartumeus},
  doi          = {10.1088/2632-2153/ad6fea},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {3},
  pages        = {030503},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Towards a comprehensive visualisation of structure in large scale data sets},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identifying chaotic dynamics in noisy time series through
multimodal deep neural networks. <em>MLST</em>, <em>5</em>(3), 035059.
(<a href="https://doi.org/10.1088/2632-2153/ad7190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chaos detection is the problem of identifying whether a series of measurements is being sampled from an underlying set of chaotic dynamics. The unavoidable presence of measurement noise significantly affects the performance of chaos detectors, as discerning chaotic dynamics from stochastic signals becomes more challenging. This paper presents a computationally efficient multimodal deep neural network tailored for chaos detection by combining information coming from the analysis of time series, recurrence plots and spectrograms. The proposed approach is the first one suitable for multi-class classification of chaotic systems while being robust with respect to measurement noise, and is validated on a dataset of 15 different chaotic and non-chaotic dynamics subject to white, pink or brown colored noise.},
  archive      = {J_MLST},
  author       = {Alessandro Giuseppi and Danilo Menegatti and Antonio Pietrabissa},
  doi          = {10.1088/2632-2153/ad7190},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035059},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Identifying chaotic dynamics in noisy time series through multimodal deep neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Chaotic attractor reconstruction using small reservoirs—the
influence of topology. <em>MLST</em>, <em>5</em>(3), 035058. (<a
href="https://doi.org/10.1088/2632-2153/ad6ee8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting timeseries based upon measured data is needed in a wide range of applications and has been the subject of extensive research. A particularly challenging task is the forecasting of timeseries generated by chaotic dynamics. In recent years reservoir computing has been shown to be an effective method of forecasting chaotic dynamics and reconstructing chaotic attractors from data. In this work strides are made toward smaller and lower complexity reservoirs with the goal of improved hardware implementability and more reliable production of adequate surrogate models. We show that a reservoir of uncoupled nodes more reliably produces long term timeseries predictions than more complex reservoir topologies. We then link the improved attractor reconstruction of the uncoupled reservoir with smaller spectral radii of the resulting surrogate systems. These results indicate that, the node degree plays an important role in determining whether the desired dynamics will be stable in the autonomous surrogate system which is attained via closed-loop operation of the trained reservoir. In terms of hardware implementability, uncoupled nodes would allow for greater freedom in the hardware architecture because no complex coupling setups are needed and because, for uncoupled nodes, the system response is equivalent for space and time multiplexing.},
  archive      = {J_MLST},
  author       = {Lina Jaurigue},
  doi          = {10.1088/2632-2153/ad6ee8},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035058},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Chaotic attractor reconstruction using small reservoirs—the influence of topology},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-driven sparse modeling of oscillations in plasma space
propulsion. <em>MLST</em>, <em>5</em>(3), 035057. (<a
href="https://doi.org/10.1088/2632-2153/ad6d29">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An algorithm to obtain data-driven models of oscillatory phenomena in plasma space propulsion systems is presented, based on sparse regression (SINDy) and Pareto front analysis. The algorithm can incorporate physical constraints, use data bootstrapping for additional robustness, and fine-tuning to different metrics. Standard, weak and integral SINDy formulations are discussed and compared. The scheme is benchmarked for the case of breathing-mode oscillations in Hall effect thrusters, using particle-in-cell/fluid simulation data. Models of varying complexity are obtained for the average plasma properties, and shown to have a clear physical interpretability and agreement with existing 0D models in the literature. Lastly, the algorithm applied is also shown to enable the identification of physical subdomains with qualitatively different plasma dynamics, providing valuable information for more advanced modeling approaches.},
  archive      = {J_MLST},
  author       = {Borja Bayón-Buján and Mario Merino},
  doi          = {10.1088/2632-2153/ad6d29},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035057},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Data-driven sparse modeling of oscillations in plasma space propulsion},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active causal learning for decoding chemical complexities
with targeted interventions. <em>MLST</em>, <em>5</em>(3), 035056. (<a
href="https://doi.org/10.1088/2632-2153/ad6feb">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting and enhancing inherent properties based on molecular structures is paramount to design tasks in medicine, materials science, and environmental management. Most of the current machine learning and deep learning approaches have become standard for predictions, but they face challenges when applied across different datasets due to reliance on correlations between molecular representation and target properties. These approaches typically depend on large datasets to capture the diversity within the chemical space, facilitating a more accurate approximation, interpolation, or extrapolation of the chemical behavior of molecules. In our research, we introduce an active learning approach that discerns underlying cause-effect relationships through strategic sampling with the use of a graph loss function. This method identifies the smallest subset of the dataset capable of encoding the most information representative of a much larger chemical space. The identified causal relations are then leveraged to conduct systematic interventions, optimizing the design task within a chemical space that the models have not encountered previously. While our implementation focused on the QM9 quantum-chemical dataset for a specific design task—finding molecules with a large dipole moment—our active causal learning approach, driven by intelligent sampling and interventions, holds potential for broader applications in molecular, materials design and discovery.},
  archive      = {J_MLST},
  author       = {Zachary R Fox and Ayana Ghosh},
  doi          = {10.1088/2632-2153/ad6feb},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035056},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Active causal learning for decoding chemical complexities with targeted interventions},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). JefiAtten: An attention-based neural network model for
solving maxwell’s equations with charge and current sources.
<em>MLST</em>, <em>5</em>(3), 035055. (<a
href="https://doi.org/10.1088/2632-2153/ad6ee9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present JefiAtten, a novel neural network model employing the attention mechanism to solve Maxwell&#39;s equations efficiently. JefiAtten uses self-attention and cross-attention modules to understand the interplay between charge density, current density, and electromagnetic fields. Our results indicate that JefiAtten can generalize well to a range of scenarios, maintaining accuracy across various spatial distribution and handling amplitude variations. The model showcases an improvement in computation speed after training, compared to traditional integral methods. The adaptability of the model suggests potential for broader applications in computational physics, with further refinements to enhance its predictive capabilities and computational efficiency. Our work is a testament to the efficacy of integrating attention mechanisms with numerical simulations, marking a step forward in the quest for data-driven solutions to physical phenomena.},
  archive      = {J_MLST},
  author       = {Ming-Yan Sun and Peng Xu and Jun-Jie Zhang and Tai-Jiao Du and Jian-Guo Wang},
  doi          = {10.1088/2632-2153/ad6ee9},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035055},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {JefiAtten: An attention-based neural network model for solving maxwell’s equations with charge and current sources},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Emergence of chemotactic strategies with multi-agent
reinforcement learning. <em>MLST</em>, <em>5</em>(3), 035054. (<a
href="https://doi.org/10.1088/2632-2153/ad5f73">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) is a flexible and efficient method for programming micro-robots in complex environments. Here we investigate whether RL can provide insights into biological systems when trained to perform chemotaxis. Namely, whether we can learn about how intelligent agents process given information in order to swim towards a target. We run simulations covering a range of agent shapes, sizes, and swim speeds to determine if the physical constraints on biological swimmers, namely Brownian motion, lead to regions where reinforcement learners&#39; training fails. We find that the RL agents can perform chemotaxis as soon as it is physically possible and, in some cases, even before the active swimming overpowers the stochastic environment. We study the efficiency of the emergent policy and identify convergence in agent size and swim speeds. Finally, we study the strategy adopted by the RL algorithm to explain how the agents perform their tasks. To this end, we identify three emerging dominant strategies and several rare approaches taken. These strategies, whilst producing almost identical trajectories in simulation, are distinct and give insight into the possible mechanisms behind which biological agents explore their environment and respond to changing conditions.},
  archive      = {J_MLST},
  author       = {Samuel Tovey and Christoph Lohrmann and Christian Holm},
  doi          = {10.1088/2632-2153/ad5f73},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035054},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Emergence of chemotactic strategies with multi-agent reinforcement learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Normalizing flows as an enhanced sampling method for
atomistic supercooled liquids. <em>MLST</em>, <em>5</em>(3), 035053. (<a
href="https://doi.org/10.1088/2632-2153/ad6ca0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Normalizing flows can transform a simple prior probability distribution into a more complex target distribution. Here, we evaluate the ability and efficiency of generative machine learning methods to sample the Boltzmann distribution of an atomistic model for glass-forming liquids. This is a notoriously difficult task, as it amounts to ergodically exploring the complex free energy landscape of a disordered and frustrated many-body system. We optimize a normalizing flow model to successfully transform high-temperature configurations of a dense liquid into low-temperature ones, near the glass transition. We perform a detailed comparative analysis with established enhanced sampling techniques developed in the physics literature to assess and rank the performance of normalizing flows against state-of-the-art algorithms. We demonstrate that machine learning methods are very promising, showing a large speedup over conventional molecular dynamics. Normalizing flows show performances comparable to parallel tempering and population annealing, while still falling far behind the swap Monte Carlo algorithm. Our study highlights the potential of generative machine learning models in scientific computing for complex systems, but also points to some of its current limitations and the need for further improvement.},
  archive      = {J_MLST},
  author       = {Gerhard Jung and Giulio Biroli and Ludovic Berthier},
  doi          = {10.1088/2632-2153/ad6ca0},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035053},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Normalizing flows as an enhanced sampling method for atomistic supercooled liquids},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantum support vector data description for anomaly
detection. <em>MLST</em>, <em>5</em>(3), 035052. (<a
href="https://doi.org/10.1088/2632-2153/ad6be8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is a critical problem in data analysis and pattern recognition, finding applications in various domains. We introduce quantum support vector data description (QSVDD), an unsupervised learning algorithm designed for anomaly detection. QSVDD utilizes a shallow-depth quantum circuit to learn a minimum-volume hypersphere that tightly encloses normal data, tailored for the constraints of noisy intermediate-scale quantum (NISQ) computing. Simulation results on the MNIST and Fashion MNIST image datasets, as well as credit card fraud detection, demonstrate that QSVDD outperforms both quantum autoencoder and deep learning-based approaches under similar training conditions. Notably, QSVDD requires an extremely small number of model parameters, which increases logarithmically with the number of input qubits. This enables efficient learning with a simple training landscape, presenting a compact quantum machine learning model with strong performance for anomaly detection.},
  archive      = {J_MLST},
  author       = {Hyeondo Oh and Daniel K Park},
  doi          = {10.1088/2632-2153/ad6be8},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035052},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Quantum support vector data description for anomaly detection},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coincidence anomaly detection for unsupervised locating of
edge localized modes in the DIII-d tokamak dataset. <em>MLST</em>,
<em>5</em>(3), 035050. (<a
href="https://doi.org/10.1088/2632-2153/ad6be7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using supervised learning to train a machine learning model to predict an on-coming edge localized mode (ELM) requires a large number of labeled samples. Creating an appropriate data set from the very large database of discharges at a long-running tokamak, such as DIII-D, would be a very time-consuming process for a human. Considering this need and difficulty, we use coincidence anomaly detection, an unsupervised learning technique, to train an ELM-identifier to identify and label ELMs in the DIII-D discharge database. This ELM-identifier shows, simultaneously, a precision of 0.68 and a recall of 0.63 (AUC is 0.73) on identifying ELMs in example time series pulled from thousands of discharges spanning five years. In a test set of 50 discharges, the algorithm finds over 26 thousand ELM candidates, more than 5 times the existing catalog of ELMs labeled by humans.},
  archive      = {J_MLST},
  author       = {Finn H O’Shea and Semin Joung and David R Smith and Daniel Ratner and Ryan Coffee},
  doi          = {10.1088/2632-2153/ad6be7},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035050},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Coincidence anomaly detection for unsupervised locating of edge localized modes in the DIII-D tokamak dataset},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discovering symbolic laws directly from trajectories with
hamiltonian graph neural networks. <em>MLST</em>, <em>5</em>(3), 035049.
(<a href="https://doi.org/10.1088/2632-2153/ad6be6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The time evolution of physical systems is described by differential equations, which depend on abstract quantities like energy and force. Traditionally, these quantities are derived as functionals based on observables such as positions and velocities. Discovering these governing symbolic laws is the key to comprehending the interactions in nature. Here, we present a Hamiltonian graph neural network ( Hgnn ), a physics-enforced Gnn that learns the dynamics of systems directly from their trajectory. We demonstrate the performance of Hgnn on n- springs, n- pendulums, gravitational systems, and binary Lennard Jones systems; Hgnn learns the dynamics in excellent agreement with the ground truth from small amounts of data. We also evaluate the ability of Hgnn to generalize to larger system sizes, and to a hybrid spring-pendulum system that is a combination of two original systems (spring and pendulum) on which the models are trained independently. Finally, employing symbolic regression on the learned Hgnn , we infer the underlying equations relating to the energy functionals, even for complex systems such as the binary Lennard-Jones liquid. Our framework facilitates the interpretable discovery of interaction laws directly from physical system trajectories. Furthermore, this approach can be extended to other systems with topology-dependent dynamics, such as cells, polydisperse gels, or deformable bodies.},
  archive      = {J_MLST},
  author       = {Suresh Bishnoi and Ravinder Bhattoo and Jayadeva and Sayan Ranu and N M Anoop Krishnan},
  doi          = {10.1088/2632-2153/ad6be6},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035049},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Discovering symbolic laws directly from trajectories with hamiltonian graph neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spectral-bias and kernel-task alignment in physically
informed neural networks. <em>MLST</em>, <em>5</em>(3), 035048. (<a
href="https://doi.org/10.1088/2632-2153/ad652d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physically informed neural networks (PINNs) are a promising emerging method for solving differential equations. As in many other deep learning approaches, the choice of PINN design and training protocol requires careful craftsmanship. Here, we suggest a comprehensive theoretical framework that sheds light on this important problem. Leveraging an equivalence between infinitely over-parameterized neural networks and Gaussian process regression, we derive an integro-differential equation that governs PINN prediction in the large data-set limit—the neurally-informed equation. This equation augments the original one by a kernel term reflecting architecture choices. It allows quantifying implicit bias induced by the network via a spectral decomposition of the source term in the original differential equation.},
  archive      = {J_MLST},
  author       = {Inbar Seroussi and Asaf Miron and Zohar Ringel},
  doi          = {10.1088/2632-2153/ad652d},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035048},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Spectral-bias and kernel-task alignment in physically informed neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Smart pixel sensors: Towards on-sensor filtering of pixel
clusters with deep learning. <em>MLST</em>, <em>5</em>(3), 035047. (<a
href="https://doi.org/10.1088/2632-2153/ad6a00">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Highly granular pixel detectors allow for increasingly precise measurements of charged particle tracks. Next-generation detectors require that pixel sizes will be further reduced, leading to unprecedented data rates exceeding those foreseen at the High- Luminosity Large Hadron Collider. Signal processing that handles data incoming at a rate of \mathcal{O} (40 MHz) and intelligently reduces the data within the pixelated region of the detector at rate will enhance physics performance at high luminosity and enable physics analyses that are not currently possible. Using the shape of charge clusters deposited in an array of small pixels, the physical properties of the traversing particle can be extracted with locally customized neural networks. In this first demonstration, we present a neural network that can be embedded into the on-sensor readout and filter out hits from low momentum tracks, reducing the detector&#39;s data volume by 57.1%–75.7%. The network is designed and simulated as a custom readout integrated circuit with 28 nm CMOS technology and is expected to operate at less than 300 \mu W with an area of less than 0.2 mm 2 . The temporal development of charge clusters is investigated to demonstrate possible future performance gains, and there is also a discussion of future algorithmic and technological improvements that could enhance efficiency, data reduction, and power per area.},
  archive      = {J_MLST},
  author       = {Jieun Yoo and Jennet Dickinson and Morris Swartz and Giuseppe Di Guglielmo and Alice Bean and Douglas Berry and Manuel Blanco Valentin and Karri DiPetrillo and Farah Fahim and Lindsey Gray and James Hirschauer and Shruti R Kulkarni and Ron Lipton and Petar Maksimovic and Corrinne Mills and Mark S Neubauer and Benjamin Parpillon and Gauri Pradhan and Chinar Syal and Nhan Tran and Dahai Wen and Aaron Young},
  doi          = {10.1088/2632-2153/ad6a00},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035047},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Smart pixel sensors: Towards on-sensor filtering of pixel clusters with deep learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automating the discovery of partial differential equations
in dynamical systems. <em>MLST</em>, <em>5</em>(3), 035046. (<a
href="https://doi.org/10.1088/2632-2153/ad682f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying partial differential equations (PDEs) from data is crucial for understanding the governing mechanisms of natural phenomena, yet it remains a challenging task. We present an extension to the ARGOS framework, ARGOS-RAL, which leverages sparse regression with the recurrent adaptive lasso to identify PDEs from limited prior knowledge automatically. Our method automates calculating partial derivatives, constructing a candidate library, and estimating a sparse model. We rigorously evaluate the performance of ARGOS-RAL in identifying canonical PDEs under various noise levels and sample sizes, demonstrating its robustness in handling noisy and non-uniformly distributed data. We also test the algorithm&#39;s performance on datasets consisting solely of random noise to simulate scenarios with severely compromised data quality. Our results show that ARGOS-RAL effectively and reliably identifies the underlying PDEs from data, outperforming the sequential threshold ridge regression method in most cases. We highlight the potential of combining statistical methods, machine learning, and dynamical systems theory to automatically discover governing equations from collected data, streamlining the scientific modeling process.},
  archive      = {J_MLST},
  author       = {Weizhen Li and Rui Carvalho},
  doi          = {10.1088/2632-2153/ad682f},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035046},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Automating the discovery of partial differential equations in dynamical systems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Virtual reality for understanding
artificial-intelligence-driven scientific discovery with an application
in quantum optics. <em>MLST</em>, <em>5</em>(3), 035045. (<a
href="https://doi.org/10.1088/2632-2153/ad5fdb">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Artificial Intelligence (AI) models can propose solutions to scientific problems beyond human capability. To truly make conceptual contributions, researchers need to be capable of understanding the AI-generated structures and extracting the underlying concepts and ideas. When algorithms provide little explanatory reasoning alongside the output, scientists have to reverse-engineer the fundamental insights behind proposals based solely on examples. This task can be challenging as the output is often highly complex and thus not immediately accessible to humans. In this work we show how transferring part of the analysis process into an immersive virtual reality (VR) environment can assist researchers in developing an understanding of AI-generated solutions. We demonstrate the usefulness of VR in finding interpretable configurations of abstract graphs, representing Quantum Optics experiments. Thereby, we can manually discover new generalizations of AI-discoveries as well as new understanding in experimental quantum optics. Furthermore, it allows us to customize the search space in an informed way—as a human-in-the-loop—to achieve significantly faster subsequent discovery iterations. As concrete examples, with this technology, we discover a new resource-efficient 3-dimensional entanglement swapping scheme, as well as a 3-dimensional 4-particle Greenberger–Horne–Zeilinger-state analyzer. Our results show the potential of VR to enhance a researcher&#39;s ability to derive knowledge from graph-based generative AI. This type of AI is a widely used abstract data representation in various scientific fields.},
  archive      = {J_MLST},
  author       = {Philipp Schmidt and Sören Arlt and Carlos Ruiz-Gonzalez and Xuemei Gu and Carla Rodríguez and Mario Krenn},
  doi          = {10.1088/2632-2153/ad5fdb},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035045},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Virtual reality for understanding artificial-intelligence-driven scientific discovery with an application in quantum optics},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty estimation of machine learning spatial
precipitation predictions from satellite data. <em>MLST</em>,
<em>5</em>(3), 035044. (<a
href="https://doi.org/10.1088/2632-2153/ad63f3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Merging satellite and gauge data with machine learning produces high-resolution precipitation datasets, but uncertainty estimates are often missing. We addressed the gap of how to optimally provide such estimates by benchmarking six algorithms, mostly novel even for the more general task of quantifying predictive uncertainty in spatial prediction settings. On 15 years of monthly data from over the contiguous United States, we compared quantile regression (QR), quantile regression forests (QRF), generalized random forests (GRF), gradient boosting machines (GBM), light gradient boosting machine (LightGBM), and quantile regression neural networks (QRNN). Their ability to issue predictive precipitation quantiles at nine quantile levels (0.025, 0.050, 0.100, 0.250, 0.500, 0.750, 0.900, 0.950, 0.975), approximating the full probability distribution, was evaluated using quantile scoring functions and the quantile scoring rule. Predictors at a site were nearby values from two satellite precipitation retrievals, namely Precipitation Estimation from Remotely Sensed Information using Artificial Neural Networks (PERSIANN) and Integrated Multi-satellitE Retrievals (IMERG), and the site&#39;s elevation. The dependent variable was the monthly mean gauge precipitation. With respect to QR, LightGBM showed improved performance in terms of the quantile scoring rule by 11.10%, also surpassing QRF (7.96%), GRF (7.44%), GBM (4.64%) and QRNN (1.73%). Notably, LightGBM outperformed all random forest variants, the current standard in spatial prediction with machine learning. To conclude, we propose a suite of machine learning algorithms for estimating uncertainty in spatial data prediction, supported with a formal evaluation framework based on scoring functions and scoring rules.},
  archive      = {J_MLST},
  author       = {Georgia Papacharalampous and Hristos Tyralis and Nikolaos Doulamis and Anastasios Doulamis},
  doi          = {10.1088/2632-2153/ad63f3},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035044},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Uncertainty estimation of machine learning spatial precipitation predictions from satellite data},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ArtiSAN: Navigating the complexity of material structures
with deep reinforcement learning. <em>MLST</em>, <em>5</em>(3), 035043.
(<a href="https://doi.org/10.1088/2632-2153/ad69ff">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding low-energy atomic ordering in compositionally complex materials is one of the hardest problems in materials discovery, the solution of which can lead to breakthroughs in functional materials—from alloys to ceramics. In this work, we present the Arti ficial S tructure A rranging N et ( ArtiSAN )—a reinforcement learning agent utilizing graph representation that is trained to find low-energy atomic configurations of multicomponent systems through a series of atomic switch operations. ArtiSAN is trained on small alloy supercells ranging from binary to septenary. Strikingly, ArtiSAN generalizes to much larger systems of more than a thousand atoms, which are inaccessible with state-of-the-art methods due to the combinatorially larger search space. The performance of the current ArtiSAN agent is tested and deployed on several compositions that can be correlated with known experimental and high-fidelity computational structures. ArtiSAN demonstrates transfer across size and composition and finds physically meaningful structures using no energy evaluation calls once fully trained. While ArtiSAN will require further modifications to capture all variability in structure search, it is a remarkable step towards solving the structural part of the problem of disordered materials discovery.},
  archive      = {J_MLST},
  author       = {Jonas Elsborg and Arghya Bhowmik},
  doi          = {10.1088/2632-2153/ad69ff},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035043},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {ArtiSAN: Navigating the complexity of material structures with deep reinforcement learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Concept graph embedding models for enhanced accuracy and
interpretability. <em>MLST</em>, <em>5</em>(3), 035042. (<a
href="https://doi.org/10.1088/2632-2153/ad6ad2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In fields requiring high accountability, it is necessary to understand how deep-learning models make decisions when analyzing the causes of image classification. Concept-based interpretation methods have recently been introduced to reveal the internal mechanisms of deep learning models using high-level concepts. However, such methods are constrained by a trade-off between accuracy and interpretability. For instance, in real-world environments, unlike in well-curated training data, the accurate prediction of expected concepts becomes a challenge owing to the various distortions and complexities introduced by different objects. To overcome this tradeoff, we propose concept graph embedding models (CGEM), reflecting the complex dependencies and structures among concepts through the learning of mutual directionalities. The concept graph convolutional neural network (Concept GCN), a downstream task of CGEM, differs from previous methods that solely determine the presence of concepts because it performs a final classification based on the relationships between con- cepts learned through graph embedding. This process endows the model with high resilience even in the presence of incorrect concepts. In addition, we utilize a deformable bipartite GCN for object- centric concept encoding in the earlier stages, which enhances the homogeneity of the concepts. The experimental results show that, based on deformable concept encoding, the CGEM mitigates the trade-off between task accuracy and interpretability. Moreover, it was confirmed that this approach allows the model to increase the resilience and interpretability while maintaining robustness against various real-world concept distortions and incorrect concept interventions. Our code is available at https://github.com/jumpsnack/cgem .},
  archive      = {J_MLST},
  author       = {Sangwon Kim and Byoung Chul Ko},
  doi          = {10.1088/2632-2153/ad6ad2},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035042},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Concept graph embedding models for enhanced accuracy and interpretability},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transfer learning with generative models for object
detection on limited datasets. <em>MLST</em>, <em>5</em>(3), 035041. (<a
href="https://doi.org/10.1088/2632-2153/ad65b5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The availability of data is limited in some fields, especially for object detection tasks, where it is necessary to have correctly labeled bounding boxes around each object. A notable example of such data scarcity is found in the domain of marine biology, where it is useful to develop methods to automatically detect submarine species for environmental monitoring. To address this data limitation, the state-of-the-art machine learning strategies employ two main approaches. The first involves pretraining models on existing datasets before generalizing to the specific domain of interest. The second strategy is to create synthetic datasets specifically tailored to the target domain using methods like copy-paste techniques or ad-hoc simulators. The first strategy often faces a significant domain shift, while the second demands custom solutions crafted for the specific task. In response to these challenges, here we propose a transfer learning framework that is valid for a generic scenario. In this framework, generated images help to improve the performances of an object detector in a few-real data regime. This is achieved through a diffusion-based generative model that was pretrained on large generic datasets. With respect to the state-of-the-art, we find that it is not necessary to fine tune the generative model on the specific domain of interest. We believe that this is an important advance because it mitigates the labor-intensive task of manual labeling the images in object detection tasks. We validate our approach focusing on fishes in an underwater environment, and on the more common domain of cars in an urban setting. Our method achieves detection performance comparable to models trained on thousands of images, using only a few hundreds of input data. Our results pave the way for new generative AI-based protocols for machine learning applications in various domains, for instance ranging from geophysics to biology and medicine.},
  archive      = {J_MLST},
  author       = {M Paiano and S Martina and C Giannelli and F Caruso},
  doi          = {10.1088/2632-2153/ad65b5},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035041},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Transfer learning with generative models for object detection on limited datasets},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unveiling the robustness of machine learning families.
<em>MLST</em>, <em>5</em>(3), 035040. (<a
href="https://doi.org/10.1088/2632-2153/ad62ab">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evaluation of machine learning systems has typically been limited to performance measures on clean and curated datasets, which may not accurately reflect their robustness in real-world situations where data distribution can vary from learning to deployment, and where truthfully predict some instances could be more difficult than others. Therefore, a key aspect in understanding robustness is instance difficulty , which refers to the level of unexpectedness of system failure on a specific instance. We present a framework that evaluates the robustness of different ML models using item response theory-based estimates of instance difficulty for supervised tasks. This framework evaluates performance deviations by applying perturbation methods that simulate noise and variability in deployment conditions. Our findings result in the development of a comprehensive taxonomy of ML techniques, based on both the robustness of the models and the difficulty of the instances, providing a deeper understanding of the strengths and limitations of specific families of ML models. This study is a significant step towards exposing vulnerabilities of particular families of ML models.},
  archive      = {J_MLST},
  author       = {R Fabra-Boluda and C Ferri and M J Ramírez-Quintana and F Martínez-Plumed},
  doi          = {10.1088/2632-2153/ad62ab},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035040},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Unveiling the robustness of machine learning families},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Formation energy prediction of neutral single-atom
impurities in 2D materials using tree-based machine learning.
<em>MLST</em>, <em>5</em>(3), 035039. (<a
href="https://doi.org/10.1088/2632-2153/ad66ae">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We applied tree-based machine learning algorithms to predict the formation energy of impurities in 2D materials, where adsorbates and interstitial defects are investigated. Regression models based on random forest, gradient boosting regression, histogram-based gradient-boosting regression, and light gradient-boosting machine algorithms are employed for training, testing, cross validation, and blind testing. We utilized chemical features from fundamental properties of atoms and supplemented them with structural features from the interaction of the added chemical element with its neighboring host atoms via the Jacobi–Legendre (JL) polynomials. Overall, the prediction accuracy yields optimal \text{MAE} \approx 0.518 , \text{RMSE} \approx 1.14 , and R^2 \approx 0.855 . When trained separately, we obtained lower residual errors RMSE and MAE, and higher R 2 value for predicting the formation energy in the adsorbates than in the interstitial defects. In both cases, the inclusion of the structural features via the JL polynomials improves the prediction accuracy of the formation energy in terms of decreasing RMSE and MAE, and increasing R 2 . This work demonstrates the potential and application of physically meaningful features to obtain physical properties of impurities in 2D materials that otherwise would require higher computational cost.},
  archive      = {J_MLST},
  author       = {Aniwat Kesorn and Rutchapon Hunkao and Cheewawut Na Talang and Chanaprom Cholsuk and Asawin Sinsarp and Tobias Vogl and Sujin Suwanna and Suraphong Yuma},
  doi          = {10.1088/2632-2153/ad66ae},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035039},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Formation energy prediction of neutral single-atom impurities in 2D materials using tree-based machine learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Molecular relaxation by reverse diffusion with time step
prediction. <em>MLST</em>, <em>5</em>(3), 035038. (<a
href="https://doi.org/10.1088/2632-2153/ad652c">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular relaxation, finding the equilibrium state of a non-equilibrium structure, is an essential component of computational chemistry to understand reactivity. Classical force field (FF) methods often rely on insufficient local energy minimization, while neural network FF models require large labeled datasets encompassing both equilibrium and non-equilibrium structures. As a remedy, we propose MoreRed, molecular relaxation by reverse diffusion, a conceptually novel and purely statistical approach where non-equilibrium structures are treated as noisy instances of their corresponding equilibrium states. To enable the denoising of arbitrarily noisy inputs via a generative diffusion model, we further introduce a novel diffusion time step predictor. Notably, MoreRed learns a simpler pseudo potential energy surface (PES) instead of the complex physical PES. It is trained on a significantly smaller, and thus computationally cheaper, dataset consisting of solely unlabeled equilibrium structures, avoiding the computation of non-equilibrium structures altogether. We compare MoreRed to classical FFs, equivariant neural network FFs trained on a large dataset of equilibrium and non-equilibrium data, as well as a semi-empirical tight-binding model. To assess this quantitatively, we evaluate the root-mean-square deviation between the found equilibrium structures and the reference equilibrium structures as well as their energies.},
  archive      = {J_MLST},
  author       = {Khaled Kahouli and Stefaan Simon Pierre Hessmann and Klaus-Robert Müller and Shinichi Nakajima and Stefan Gugler and Niklas Wolf Andreas Gebauer},
  doi          = {10.1088/2632-2153/ad652c},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035038},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Molecular relaxation by reverse diffusion with time step prediction},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trainability issues in quantum policy gradients.
<em>MLST</em>, <em>5</em>(3), 035037. (<a
href="https://doi.org/10.1088/2632-2153/ad6830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research explores the trainability of Parameterized Quantum Circuit-based policies in Reinforcement Learning, an area that has recently seen a surge in empirical exploration. While some studies suggest improved sample complexity using quantum gradient estimation, the efficient trainability of these policies remains an open question. Our findings reveal significant challenges, including standard Barren Plateaus with exponentially small gradients and gradient explosion. These phenomena depend on the type of basis-state partitioning and the mapping of these partitions onto actions. For a polynomial number of actions, a trainable window can be ensured with a polynomial number of measurements if a contiguous-like partitioning of basis-states is employed. These results are empirically validated in a multi-armed bandit environment.},
  archive      = {J_MLST},
  author       = {André Sequeira and Luis Paulo Santos and Luis Soares Barbosa},
  doi          = {10.1088/2632-2153/ad6830},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035037},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Trainability issues in quantum policy gradients},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coincident learning for unsupervised anomaly detection of
scientific instruments. <em>MLST</em>, <em>5</em>(3), 035036. (<a
href="https://doi.org/10.1088/2632-2153/ad64a6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is an important task for complex scientific experiments and other complex systems (e.g. industrial facilities, manufacturing), where failures in a sub-system can lead to lost data, poor performance, or even damage to components. While scientific facilities generate a wealth of data, labeled anomalies may be rare (or even nonexistent), and expensive to acquire. Unsupervised approaches are therefore common and typically search for anomalies either by distance or density of examples in the input feature space (or some associated low-dimensional representation). This paper presents a novel approach called coincident learning for anomaly detection (CoAD), which is specifically designed for multi-modal tasks and identifies anomalies based on coincident behavior across two different slices of the feature space. We define an unsupervised metric, \hat{F}_\beta , out of analogy to the supervised classification F β statistic. CoAD uses \hat{F}_\beta to train an anomaly detection algorithm on unlabeled data , based on the expectation that anomalous behavior in one feature slice is coincident with anomalous behavior in the other. The method is illustrated using a synthetic outlier data set and a MNIST-based image data set, and is compared to prior state-of-the-art on two real-world tasks: a metal milling data set and our motivating task of identifying RF station anomalies in a particle accelerator.},
  archive      = {J_MLST},
  author       = {Ryan Humble and Zhe Zhang and Finn O’Shea and Eric Darve and Daniel Ratner},
  doi          = {10.1088/2632-2153/ad64a6},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035036},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Coincident learning for unsupervised anomaly detection of scientific instruments},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards robust data-driven automated recovery of symbolic
conservation laws from limited data. <em>MLST</em>, <em>5</em>(3),
035035. (<a href="https://doi.org/10.1088/2632-2153/ad6390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conservation laws are an inherent feature in many systems modeling real world phenomena, in particular, those modeling biological and chemical systems. If the form of the underlying dynamical system is known, linear algebra and algebraic geometry methods can be used to identify the conservation laws. Our work focuses on using data-driven methods to identify the conservation law(s) in the absence of the knowledge of system dynamics. We develop a robust data-driven computational framework that automates the process of identifying the number and type of the conservation law(s) while keeping the amount of required data to a minimum. We demonstrate that due to relative stability of singular vectors to noise we are able to reconstruct correct conservation laws without the need for excessive parameter tuning. While we focus primarily on biological examples, the framework proposed herein is suitable for a variety of data science applications and can be coupled with other machine learning approaches.},
  archive      = {J_MLST},
  author       = {Tracey Oellerich and Maria Emelianenko},
  doi          = {10.1088/2632-2153/ad6390},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035035},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Towards robust data-driven automated recovery of symbolic conservation laws from limited data},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the benefit of attention in inverse design of thin films
filters. <em>MLST</em>, <em>5</em>(3), 035034. (<a
href="https://doi.org/10.1088/2632-2153/ad6832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention layers are a crucial component in many modern deep learning models, particularly those used in natural language processing and computer vision. Attention layers have been shown to improve the accuracy and effectiveness of various tasks, such as machine translation, image captioning, etc. Here, the benefit of attention layers in designing optical filters based on a stack of thin film materials is investigated. The superiority of Attention layers over fully-connected Deep Neural Networks is demonstrated for this task.},
  archive      = {J_MLST},
  author       = {Barak Hadad and Omry Oren and Alon Bahabad},
  doi          = {10.1088/2632-2153/ad6832},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035034},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {On the benefit of attention in inverse design of thin films filters},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-perspective feedback-attention coupling model for
continuous-time dynamic graphs. <em>MLST</em>, <em>5</em>(3), 035033.
(<a href="https://doi.org/10.1088/2632-2153/ad66af">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning over graph networks has recently gained popularity, with many models showing promising results. However, several challenges remain: (1) most methods are designed for static or discrete-time dynamic graphs; (2) existing continuous-time dynamic graph algorithms focus on a single evolving perspective; and (3) many continuous-time dynamic graph approaches necessitate numerous temporal neighbors to capture long-term dependencies. In response, this paper introduces a Multi-Perspective Feedback-Attention Coupling (MPFA) model. MPFA incorporates information from both evolving and original perspectives to effectively learn the complex dynamics of dynamic graph evolution processes. The evolving perspective considers the current state of historical interaction events of nodes and uses a temporal attention module to aggregate current state information. This perspective also makes it possible to capture long-term dependencies of nodes using a small number of temporal neighbors. Meanwhile, the original perspective utilizes a feedback attention module with growth characteristic coefficients to aggregate the original state information of node interactions. Experimental results on one dataset organized by ourselves and seven public datasets validate the effectiveness and competitiveness of our proposed model.},
  archive      = {J_MLST},
  author       = {Xiaobo Zhu and Yan Wu and Jin Che and Chao Wang and Liying Wang and Zhanheng Chen},
  doi          = {10.1088/2632-2153/ad66af},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035033},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Multi-perspective feedback-attention coupling model for continuous-time dynamic graphs},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Background suppression for volcano muography with machine
learning. <em>MLST</em>, <em>5</em>(3), 035032. (<a
href="https://doi.org/10.1088/2632-2153/ad64a7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A machine learning (ML) algorithm (deep neural network) is presented to suppress background in muography applications mainly targeting volcanoes. Additionally it could be applied for large scale geological structures, such as ophiolites. The detector system investigated in this article is designed to suppress the low energy background by applying up to 5 lead absorber layers arranged among 8 detectors. This complicated system was simulated with a Monte-Carlo based particle simulation to provide training sample for the ML algorithm. It is shown that the developed deep neural network is capable of suppressing the low energy background considerably better than the classical tracking algorithm, therefore this additional suppression with ML yields in a significant improvement. The target areas of volcanoes lie beneath approximately a kilometer of rock that only a fraction of a percent of muons have enough energy to penetrate. The ML algorithm takes advantage of the directional changes in the absorbers, as well as the correlation between the muons energy and the deposited energy in the detectors. Identifying very high energy muons is also a challenge: the classical algorithm discards considerable fraction of 1 TeV muons which create multiple hits due to bremsstrahlung, while the ML algorithm easily adapts to accept such patterns.},
  archive      = {J_MLST},
  author       = {Gábor Galgóczi and Gábor Albrecht and Gergő Hamar and Dezső Varga},
  doi          = {10.1088/2632-2153/ad64a7},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035032},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Background suppression for volcano muography with machine learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OmniJet-α: The first cross-task foundation model for
particle physics. <em>MLST</em>, <em>5</em>(3), 035031. (<a
href="https://doi.org/10.1088/2632-2153/ad66ad">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foundation models are multi-dataset and multi-task machine learning methods that once pre-trained can be fine-tuned for a large variety of downstream applications. The successful development of such general-purpose models for physics data would be a major breakthrough as they could improve the achievable physics performance while at the same time drastically reduce the required amount of training time and data. We report significant progress on this challenge on several fronts. First, a comprehensive set of evaluation methods is introduced to judge the quality of an encoding from physics data into a representation suitable for the autoregressive generation of particle jets with transformer architectures (the common backbone of foundation models). These measures motivate the choice of a higher-fidelity tokenization compared to previous works. Finally, we demonstrate transfer learning between an unsupervised problem (jet generation) and a classic supervised task (jet tagging) with our new OmniJet - α model. This is the first successful transfer between two different and actively studied classes of tasks and constitutes a major step in the building of foundation models for particle physics.},
  archive      = {J_MLST},
  author       = {Joschka Birk and Anna Hallin and Gregor Kasieczka},
  doi          = {10.1088/2632-2153/ad66ad},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035031},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {OmniJet-α: The first cross-task foundation model for particle physics},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Benchmarking machine learning interatomic potentials via
phonon anharmonicity. <em>MLST</em>, <em>5</em>(3), 030502. (<a
href="https://doi.org/10.1088/2632-2153/ad674a">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning approaches have recently emerged as powerful tools to probe structure-property relationships in crystals and molecules. Specifically, machine learning interatomic potentials (MLIPs) can accurately reproduce first-principles data at a cost similar to that of conventional interatomic potential approaches. While MLIPs have been extensively tested across various classes of materials and molecules, a clear characterization of the anharmonic terms encoded in the MLIPs is lacking. Here, we benchmark popular MLIPs using the anharmonic vibrational Hamiltonian of ThO 2 in the fluorite crystal structure, which was constructed from density functional theory (DFT) using our highly accurate and efficient irreducible derivative methods. The anharmonic Hamiltonian was used to generate molecular dynamics (MD) trajectories, which were used to train three classes of MLIPs: Gaussian approximation potentials, artificial neural networks (ANN), and graph neural networks (GNN). The results were assessed by directly comparing phonons and their interactions, as well as phonon linewidths, phonon lineshifts, and thermal conductivity. The models were also trained on a DFT MD dataset, demonstrating good agreement up to fifth-order for the ANN and GNN. Our analysis demonstrates that MLIPs have great potential for accurately characterizing anharmonicity in materials systems at a fraction of the cost of conventional first principles-based approaches.},
  archive      = {J_MLST},
  author       = {Sasaank Bandi and Chao Jiang and Chris A Marianetti},
  doi          = {10.1088/2632-2153/ad674a},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {030502},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Benchmarking machine learning interatomic potentials via phonon anharmonicity},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physics-informed neural network for turbulent flow
reconstruction in composite porous-fluid systems. <em>MLST</em>,
<em>5</em>(3), 035030. (<a
href="https://doi.org/10.1088/2632-2153/ad63f4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the implementation of physics-informed neural networks (PINNs) to analyze turbulent flow in composite porous-fluid systems. These systems are composed of a fluid-saturated porous medium and an adjacent fluid, where the flow properties are exchanged across the porous-fluid interface. The segregated PINN model employs a novel approach combining supervised learning and enforces fidelity to flow physics through penalization by the Reynolds-averaged Navier-Stokes (RANS) equations. Two cases were simulated for this purpose: solid block, i.e. porous media with zero porosity, and porous block with a defined porosity. The effect of providing internal training data on the accuracy of the PINN predictions for prominent flow features, including flow leakage, channeling effect and wake recirculation was investigated. Additionally, L 2 norm error, which evaluates the prediction accuracy for flow variables was studied. Furthermore, PINN training time in both cases with internal training data was considered in this study. Results showed that the PINN model predictions with second-order internal training data achieved high accuracy for the prominent flow features compared to the RANS data, within a 20% L 2 norm error of second-order statistics in the solid block case. In addition, for the porous block case, providing training data at the porous-fluid interface showed errors of 18.04% and 19.94% for second-order statistics, representing an increase in prediction accuracy by 7% compared to without interface training data. The study elucidates the impact of the internal training data distribution on the PINN training in complex turbulent flow dynamics, underscoring the necessity of turbulent second-order statistics variables in PINN training and an additional velocity gradient treatment to enhance PINN prediction.},
  archive      = {J_MLST},
  author       = {Seohee Jang and Mohammad Jadidi and Saleh Rezaeiravesh and Alistair Revell and Yasser Mahmoudi},
  doi          = {10.1088/2632-2153/ad63f4},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035030},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Physics-informed neural network for turbulent flow reconstruction in composite porous-fluid systems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Negative order sobolev cubatures: Preconditioners of partial
differential equation learning tasks circumventing numerical stiffness.
<em>MLST</em>, <em>5</em>(3), 035029. (<a
href="https://doi.org/10.1088/2632-2153/ad62ac">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a variational approach aimed at enhancing the training of physics-informed neural networks (PINNs) and more general surrogate models for learning partial differential equations (PDE). In particular, we extend our formerly introduced notion of Sobolev cubatures to negative orders, enabling the approximation of negative order Sobolev norms. We mathematically prove the effect of negative order Sobolev cubatures in improving the condition number of discrete PDE learning problems, providing balancing scalars that mitigate numerical stiffness issues caused by loss imbalances. Additionally, we consider polynomial surrogate models ( PSMs ), which maintain the flexibility of PINN formulations while preserving the convexity structure of the PDE operators. The combination of negative order Sobolev cubatures and PSMs delivers well-conditioned discrete optimization problems, solvable via an exponentially fast convergent gradient descent for λ -convex losses. Our theoretical contributions are supported by numerical experiments, addressing linear and non-linear, forward and inverse PDE problems. These experiments show that the Sobolev cubature-based PSMs emerge as the superior state-of-the-art PINN technique.},
  archive      = {J_MLST},
  author       = {Juan-Esteban Suarez Cardona and Phil-Alexander Hofmann and Michael Hecht},
  doi          = {10.1088/2632-2153/ad62ac},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035029},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Negative order sobolev cubatures: Preconditioners of partial differential equation learning tasks circumventing numerical stiffness},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing the rationale of convolutional neural networks for
glitch classification in gravitational wave detectors: A visual
explanation. <em>MLST</em>, <em>5</em>(3), 035028. (<a
href="https://doi.org/10.1088/2632-2153/ad6391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the pursuit of detecting gravitational waves, ground-based interferometers (e.g. LIGO, Virgo, and KAGRA) face a significant challenge: achieving the extremely high sensitivity required to detect fluctuations at distances significantly smaller than the diameter of an atomic nucleus. Cutting-edge materials and innovative engineering techniques have been employed to enhance the stability and precision of the interferometer apparatus over the years. These efforts are crucial for reducing the noise that masks the subtle gravitational wave signals. Various sources of interference, such as seismic activity, thermal fluctuations, and other environmental factors, contribute to the total noise spectra characteristic of the detector. Therefore, addressing these sources is essential to enhance the interferometer apparatus&#39;s stability and precision. Recent research has emphasised the importance of classifying non-stationary and non-Gaussian glitches, employing sophisticated algorithms and machine learning methods to distinguish genuine gravitational wave signals from instrumental artefacts. The time-frequency-amplitude representation of these transient disturbances exhibits a wide range of new shapes, variability, and features, reflecting the evolution of interferometer technology. In this study, we developed a convolutional neural network model to classify glitches using spectrogram images from the Gravity Spy O1 dataset. We employed score-class activation mapping and the uniform manifold approximation and projection algorithm to visualise and understand the classification decisions made by our model. We assessed the model&#39;s validity and investigated the causes of misclassification from these results.},
  archive      = {J_MLST},
  author       = {Naoki Koyama and Yusuke Sakai and Seiya Sasaoka and Diego Dominguez and Kentaro Somiya and Yuto Omae and Yoshikazu Terada and Marco Meyer-Conde and Hirotaka Takahashi},
  doi          = {10.1088/2632-2153/ad6391},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035028},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Enhancing the rationale of convolutional neural networks for glitch classification in gravitational wave detectors: A visual explanation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Controlling optical-cavity locking using reinforcement
learning. <em>MLST</em>, <em>5</em>(3), 035027. (<a
href="https://doi.org/10.1088/2632-2153/ad638f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study applies an effective methodology based on Reinforcement Learning to a control system. Using the Pound–Drever–Hall locking scheme, we match the wavelength of a controlled laser to the length of a Fabry-Pérot cavity such that the cavity length is an exact integer multiple of the laser wavelength. Typically, long-term drift of the cavity length and laser wavelength exceeds the dynamic range of this control if only the laser&#39;s piezoelectric transducer is actuated, so the same error signal also controls the temperature of the laser crystal. In this work, we instead implement this feedback control grounded on Q-Learning. Our system learns in real-time, eschewing reliance on historical data, and exhibits adaptability to system variations post-training. This adaptive quality ensures continuous updates to the learning agent. This innovative approach maintains lock for eight days on average.},
  archive      = {J_MLST},
  author       = {Edoardo Fazzari and Hudson A Loughlin and Chris Stoughton},
  doi          = {10.1088/2632-2153/ad638f},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035027},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Controlling optical-cavity locking using reinforcement learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient bayesian inference using physics-informed
invertible neural networks for inverse problems. <em>MLST</em>,
<em>5</em>(3), 035026. (<a
href="https://doi.org/10.1088/2632-2153/ad5f74">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an innovative approach to tackle Bayesian inverse problems using physics-informed invertible neural networks (PI-INN). Serving as a neural operator model, PI-INN employs an invertible neural network (INN) to elucidate the relationship between the parameter field and the solution function in latent variable spaces. Specifically, the INN decomposes the latent variable of the parameter field into two distinct components: the expansion coefficients that represent the solution to the forward problem, and the noise that captures the inherent uncertainty associated with the inverse problem. Through precise estimation of the forward mapping and preservation of statistical independence between expansion coefficients and latent noise, PI-INN offers an accurate and efficient generative model for resolving Bayesian inverse problems, even in the absence of labeled data. For a given solution function, PI-INN can provide tractable and accurate estimates of the posterior distribution of the underlying parameter field. Moreover, capitalizing on the INN&#39;s characteristics, we propose a novel independent loss function to effectively ensure the independence of the INN&#39;s decomposition results. The efficacy and precision of the proposed PI-INN are demonstrated through a series of numerical experiments.},
  archive      = {J_MLST},
  author       = {Xiaofei Guan and Xintong Wang and Hao Wu and Zihao Yang and Peng Yu},
  doi          = {10.1088/2632-2153/ad5f74},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035026},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Efficient bayesian inference using physics-informed invertible neural networks for inverse problems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable gaussian processes: A loss landscape
perspective. <em>MLST</em>, <em>5</em>(3), 035025. (<a
href="https://doi.org/10.1088/2632-2153/ad62ad">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior beliefs about the latent function to shape inductive biases can be incorporated into a Gaussian process (GP) via the kernel. However, beyond kernel choices, the decision-making process of GP models remains poorly understood. In this work, we contribute an analysis of the loss landscape for GP models using methods from chemical physics. We demonstrate ν -continuity for Matérn kernels and outline aspects of catastrophe theory at critical points in the loss landscape. By directly including ν in the hyperparameter optimisation for Matérn kernels, we find that typical values of ν can be far from optimal in terms of performance. We also provide an a priori method for evaluating the effect of GP ensembles and discuss various voting approaches based on physical properties of the loss landscape. The utility of these approaches is demonstrated for various synthetic and real datasets. Our findings provide insight into hyperparameter optimisation for GPs and offer practical guidance for improving their performance and interpretability in a range of applications.},
  archive      = {J_MLST},
  author       = {Maximilian P Niroomand and Luke Dicks and Edward O Pyzer-Knapp and David J Wales},
  doi          = {10.1088/2632-2153/ad62ad},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035025},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Explainable gaussian processes: A loss landscape perspective},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Datacube segmentation via deep spectral clustering.
<em>MLST</em>, <em>5</em>(3), 035024. (<a
href="https://doi.org/10.1088/2632-2153/ad622f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extended vision techniques are ubiquitous in physics. However, the data cubes steaming from such analysis often pose a challenge in their interpretation, due to the intrinsic difficulty in discerning the relevant information from the spectra composing the data cube. Furthermore, the huge dimensionality of data cube spectra poses a complex task in its statistical interpretation; nevertheless, this complexity contains a massive amount of statistical information that can be exploited in an unsupervised manner to outline some essential properties of the case study at hand, e.g. it is possible to obtain an image segmentation via (deep) clustering of data-cube&#39;s spectra, performed in a suitably defined low-dimensional embedding space. To tackle this topic, we explore the possibility of applying unsupervised clustering methods in encoded space, i.e. perform deep clustering on the spectral properties of datacube pixels. A statistical dimensional reduction is performed by an ad hoc trained (variational) AutoEncoder, in charge of mapping spectra into lower dimensional metric spaces, while the clustering process is performed by a (learnable) iterative K-means clustering algorithm. We apply this technique to two different use cases, of different physical origins: a set of macro mapping x-ray fluorescence (MA-XRF) synthetic data on pictorial artworks, and a dataset of simulated astrophysical observations.},
  archive      = {J_MLST},
  author       = {Alessandro Bombini and Fernando García-Avello Bofías and Caterina Bracci and Michele Ginolfi and Chiara Ruberto},
  doi          = {10.1088/2632-2153/ad622f},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035024},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Datacube segmentation via deep spectral clustering},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Proton dose deposition matrix prediction using multi-source
feature driven deep learning approach. <em>MLST</em>, <em>5</em>(3),
035023. (<a href="https://doi.org/10.1088/2632-2153/ad6231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proton dose deposition results are influenced by various factors, such as irradiation angle, beamlet energy and other parameters. The calculation of the proton dose deposition matrix (DDM) can be highly complex but is crucial in intensity-modulated proton therapy (IMPT). In this work, we present a novel deep learning (DL) approach using multi-source features for proton DDM prediction. The DL5 proton DDM prediction method involves five input features containing beamlet geometry, dosimetry and treatment machine information like patient CT data, beamlet energy, distance from voxel to beamlet axis, distance from voxel to body surface, and pencil beam (PB) dose. The dose calculated by Monte Carlo (MC) method was used as the ground truth dose label. A total of 40 000 features, corresponding to 8000 beamlets, were obtained from head patient datasets and used for the training data. Additionally, seventeen head patients not included in the training process were utilized as testing cases. The DL5 method demonstrates high proton beamlet dose prediction accuracy, with an average determination coefficient R 2 of 0.93 when compared to the MC dose. Accurate beamlet dose estimation can be achieved in as little as 1.5 milliseconds for an individual proton beamlet. F or IMPT plan dose comparisons to the dose calculated by the MC method, the DL5 method exhibited gamma pass rates of γ (2 mm, 2%) and γ (3 mm, 3%) ranging from 98.15% to 99.89% and 98.80% to 99.98%, respectively, across all 17 testing cases. On average, the DL5 method increased the gamma pass rates to γ (2 mm, 2%) from 82.97% to 99.23% and to γ (3 mm, 3%) from 85.27% to 99.75% when compared with the PB method. The proposed DL5 model enables rapid and precise dose calculation in IMPT plan, which has the potential to significantly enhance the efficiency and quality of proton radiation therapy.},
  archive      = {J_MLST},
  author       = {Peng Zhou and Shengxiu Jiao and Xiaoqian Zhao and Shuzhan Yao and Honghao Xu and Chuan Chen},
  doi          = {10.1088/2632-2153/ad6231},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035023},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Proton dose deposition matrix prediction using multi-source feature driven deep learning approach},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Retrieving past quantum features with deep hybrid
classical-quantum reservoir computing. <em>MLST</em>, <em>5</em>(3),
035022. (<a href="https://doi.org/10.1088/2632-2153/ad5f12">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning techniques have achieved impressive results in recent years and the possibility of harnessing the power of quantum physics opens new promising avenues to speed up classical learning methods. Rather than viewing classical and quantum approaches as exclusive alternatives, their integration into hybrid designs has gathered increasing interest, as seen in variational quantum algorithms, quantum circuit learning, and kernel methods. Here we introduce deep hybrid classical-quantum reservoir computing for temporal processing of quantum states where information about, for instance, the entanglement or the purity of past input states can be extracted via a single-step measurement. We find that the hybrid setup cascading two reservoirs not only inherits the strengths of both of its constituents but is even more than just the sum of its parts, outperforming comparable non-hybrid alternatives. The quantum layer is within reach of state-of-the-art multimode quantum optical platforms while the classical layer can be implemented in silico.},
  archive      = {J_MLST},
  author       = {Johannes Nokkala and Gian Luca Giorgi and Roberta Zambrini},
  doi          = {10.1088/2632-2153/ad5f12},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035022},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Retrieving past quantum features with deep hybrid classical-quantum reservoir computing},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal hybrid modeling with double machine
learning—applications in carbon flux modeling. <em>MLST</em>,
<em>5</em>(3), 035021. (<a
href="https://doi.org/10.1088/2632-2153/ad5a60">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid modeling integrates machine learning with scientific knowledge to enhance interpretability, generalization, and adherence to natural laws. Nevertheless, equifinality and regularization biases pose challenges in hybrid modeling to achieve these purposes. This paper introduces a novel approach to estimating hybrid models via a causal inference framework, specifically employing double machine learning (DML) to estimate causal effects. We showcase its use for the Earth sciences on two problems related to carbon dioxide fluxes. In the Q 10 model, we demonstrate that DML-based hybrid modeling is superior in estimating causal parameters over end-to-end deep neural network approaches, proving efficiency, robustness to bias from regularization methods, and circumventing equifinality. Our approach, applied to carbon flux partitioning, exhibits flexibility in accommodating heterogeneous causal effects. The study emphasizes the necessity of explicitly defining causal graphs and relationships, advocating for this as a general best practice. We encourage the continued exploration of causality in hybrid models for more interpretable and trustworthy results in knowledge-guided machine learning.},
  archive      = {J_MLST},
  author       = {Kai-Hendrik Cohrs and Gherardo Varando and Nuno Carvalhais and Markus Reichstein and Gustau Camps-Valls},
  doi          = {10.1088/2632-2153/ad5a60},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035021},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Causal hybrid modeling with double machine learning—applications in carbon flux modeling},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An extended de bruijn graph for feature engineering over
biological sequential data. <em>MLST</em>, <em>5</em>(3), 035020. (<a
href="https://doi.org/10.1088/2632-2153/ad5fde">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we introduce a novel de Bruijn graph (dBG) based framework for feature engineering in biological sequential data such as proteins. This framework simplifies feature extraction by dynamically generating high-quality, interpretable features for traditional AI (TAI) algorithms. Our framework accounts for amino acid substitutions by efficiently adjusting the edge weights in the dBG using a secondary trie structure. We extract motifs from the dBG by traversing the heavy edges, and then incorporate alignment algorithms like BLAST and Smith–Waterman to generate features for TAI algorithms. Empirical validation on TIMP (tissue inhibitors of matrix metalloproteinase) data demonstrates significant accuracy improvements over a robust baseline, state-of-the-art PLM models, and those from the popular GLAM2 tool. Furthermore, our framework successfully identified Glycine and Arginine-rich motifs with high coverage, highlighting it is potential in general pattern discovery.},
  archive      = {J_MLST},
  author       = {Mert Onur Cakiroglu and Hasan Kurban and Parichit Sharma and M Oguzhan Kulekci and Elham Khorasani Buxton and Maryam Raeeszadeh-Sarmazdeh and Mehmet M Dalkilic},
  doi          = {10.1088/2632-2153/ad5fde},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035020},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {An extended de bruijn graph for feature engineering over biological sequential data},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simulation-based inference on virtual brain models of
disorders. <em>MLST</em>, <em>5</em>(3), 035019. (<a
href="https://doi.org/10.1088/2632-2153/ad6230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Connectome-based models, also known as virtual brain models (VBMs), have been well established in network neuroscience to investigate pathophysiological causes underlying a large range of brain diseases. The integration of an individual&#39;s brain imaging data in VBMs has improved patient-specific predictivity, although Bayesian estimation of spatially distributed parameters remains challenging even with state-of-the-art Monte Carlo sampling. VBMs imply latent nonlinear state space models driven by noise and network input, necessitating advanced probabilistic machine learning techniques for widely applicable Bayesian estimation. Here we present simulation-based inference on VBMs (SBI-VBMs), and demonstrate that training deep neural networks on both spatio-temporal and functional features allows for accurate estimation of generative parameters in brain disorders. The systematic use of brain stimulation provides an effective remedy for the non-identifiability issue in estimating the degradation limited to smaller subset of connections. By prioritizing model structure over data, we show that the hierarchical structure in SBI-VBMs renders the inference more effective, precise and biologically plausible. This approach could broadly advance precision medicine by enabling fast and reliable prediction of patient-specific brain disorders.},
  archive      = {J_MLST},
  author       = {Meysam Hashemi and Abolfazl Ziaeemehr and Marmaduke M Woodman and Jan Fousek and Spase Petkoski and Viktor K Jirsa},
  doi          = {10.1088/2632-2153/ad6230},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035019},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Simulation-based inference on virtual brain models of disorders},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised representations and node embedding graph
neural networks for accurate and multi-scale analysis of materials.
<em>MLST</em>, <em>5</em>(3), 035018. (<a
href="https://doi.org/10.1088/2632-2153/ad612b">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised machine learning algorithms, such as graph neural networks (GNN), have successfully predicted material properties. However, the superior performance of GNN usually relies on end-to-end learning on large material datasets, which may lose the physical insight of multi-scale information about materials. And the process of labeling data consumes many resources and inevitably introduces errors, which constrains the accuracy of prediction. We propose to train the GNN model by self-supervised learning on the node and edge information of the crystal graph. Compared with the popular manually constructed material descriptors, the self-supervised atomic representation can reach better prediction performance on material properties. Furthermore, it may provide physical insights by tuning the range information. Applying the self-supervised atomic representation on the magnetic moment datasets, we show how they can extract rules and information from the magnetic materials. To incorporate rich physical information into the GNN model, we develop the node embedding graph neural networks (NEGNN) framework and show significant improvements in the prediction performance. The self-supervised material representation and the NEGNN framework may investigate in-depth information from materials and can be applied to small datasets with increased prediction accuracy.},
  archive      = {J_MLST},
  author       = {Jian-Gang Kong and Ke-Lin Zhao and Jian Li and Qing-Xu Li and Yu Liu and Rui Zhang and Jia-Ji Zhu and Kai Chang},
  doi          = {10.1088/2632-2153/ad612b},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035018},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Self-supervised representations and node embedding graph neural networks for accurate and multi-scale analysis of materials},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ultrafast jet classification at the HL-LHC. <em>MLST</em>,
<em>5</em>(3), 035017. (<a
href="https://doi.org/10.1088/2632-2153/ad5f10">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three machine learning models are used to perform jet origin classification. These models are optimized for deployment on a field-programmable gate array device. In this context, we demonstrate how latency and resource consumption scale with the input size and choice of algorithm. Moreover, the models proposed here are designed to work on the type of data and under the foreseen conditions at the CERN large hadron collider during its high-luminosity phase. Through quantization-aware training and efficient synthetization for a specific field programmable gate array, we show that \mathcal{O}(100) ns inference of complex architectures such as Deep Sets and Interaction Networks is feasible at a relatively low computational resource cost.},
  archive      = {J_MLST},
  author       = {Patrick Odagiu and Zhiqiang Que and Javier Duarte and Johannes Haller and Gregor Kasieczka and Artur Lobanov and Vladimir Loncar and Wayne Luk and Jennifer Ngadiuba and Maurizio Pierini and Philipp Rincke and Arpita Seksaria and Sioni Summers and Andre Sznajder and Alexander Tapper and Thea K Årrestad},
  doi          = {10.1088/2632-2153/ad5f10},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035017},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Ultrafast jet classification at the HL-LHC},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An active learning enhanced data programming (ActDP)
framework for ECG time series. <em>MLST</em>, <em>5</em>(3), 035016. (<a
href="https://doi.org/10.1088/2632-2153/ad5fda">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised machine learning learns a mapping from input data to output labels, based on the patterns and relationships present in a huge labelled training data.Getting labelled data generally requires a substantial allocation of resources in terms of cost and time. In such scenarios, weak supervised learning techniques like data programming (DP) and active learning (AL) can be advantageous for time-series classification tasks. These paradigms can be used to assign data labels in an automated manner, and time-series classification can subsequently be carried out on the labeled data. This work proposes a novel framework titled AL enhanced data programming (ActDP). It uses a combination of DP and AL for electrocardiogram (ECG) beat classification using single-lead data. ECG beat classification is pivotal in cardiology and healthcare applications for diagnosing a broad spectrum of heart conditions and arrhythmias. To establish the usefulness of this proposed ActDP framework, the experiments have been conducted using the MIT-BIH dataset with 94,224 ECG beats. DP assigns a probabilistic label to each ECG beat using nine novel polar labelling functions and a generative model in this work. Further, AL improves the result of DP by replacing the labels for sampled ECG beats of a generative model with ground truth. Subsequently, a discriminative model is trained on these labels for each iteration. The experimental results show that by incorporating AL into DP in the ActDP framework, the accuracy of ECG classification strictly increases from 85.7% to 97.34% in 58 iterations. Comparatively, the proposed framework (ActDP) has demonstrated a higher classification accuracy of 97.34%. In contrast, DP with data augmentation (DA) achieves an accuracy of 92.2%, while DP without DA results in an accuracy of 85.7%, few-shot learning techniques yield 87.5%–89.2%, and multi-instance learning methods achieve accuracies in the range of 88.9%–94.1%},
  archive      = {J_MLST},
  author       = {Priyanka Gupta and Manik Gupta and Vijay Kumar},
  doi          = {10.1088/2632-2153/ad5fda},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035016},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {An active learning enhanced data programming (ActDP) framework for ECG time series},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solving deep-learning density functional theory via
variational autoencoders. <em>MLST</em>, <em>5</em>(3), 035015. (<a
href="https://doi.org/10.1088/2632-2153/ad611f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, machine learning models, chiefly deep neural networks, have revealed suited to learn accurate energy-density functionals from data. However, problematic instabilities have been shown to occur in the search of ground-state density profiles via energy minimization. Indeed, any small noise can lead astray from realistic profiles, causing the failure of the learned functional and, hence, strong violations of the variational property. In this article, we employ variational autoencoders (VAEs) to build a compressed, flexible, and regular representation of the ground-state density profiles of various quantum models. Performing energy minimization in this compressed space allows us to avoid both numerical instabilities and variational biases due to excessive constraints. Our tests are performed on one-dimensional single-particle models from the literature in the field and, notably, on a three-dimensional disordered potential. In all cases, the ground-state energies are estimated with errors below the chemical accuracy and the density profiles are accurately reproduced without numerical artifacts. Furthermore, we show that it is possible to perform transfer learning, applying pre-trained VAEs to different potentials.},
  archive      = {J_MLST},
  author       = {Emanuele Costa and Giuseppe Scriva and Sebastiano Pilati},
  doi          = {10.1088/2632-2153/ad611f},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035015},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Solving deep-learning density functional theory via variational autoencoders},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantum extreme learning of molecular potential energy
surfaces and force fields. <em>MLST</em>, <em>5</em>(3), 035014. (<a
href="https://doi.org/10.1088/2632-2153/ad6120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum machine learning algorithms are expected to play a pivotal role in quantum chemistry simulations in the immediate future. One such key application is the training of a quantum neural network to learn the potential energy surface and force field of molecular systems. We address this task by using the quantum extreme learning machine paradigm. This particular supervised learning routine allows for resource-efficient training, consisting of a simple linear regression performed on a classical computer. We have tested a setup that can be used to study molecules of any dimension and is optimized for immediate use on NISQ devices with a limited number of native gates. We have applied this setup to three case studies: lithium hydride, water, and formamide, carrying out both noiseless simulations and actual implementation on IBM quantum hardware. Compared to other supervised learning routines, the proposed setup requires minimal quantum resources, making it feasible for direct implementation on quantum platforms, while still achieving a high level of predictive accuracy compared to simulations. Our encouraging results pave the way towards the future application to more complex molecules, being the proposed setup scalable.},
  archive      = {J_MLST},
  author       = {Gabriele Lo Monaco and Marco Bertini and Salvatore Lorenzo and G Massimo Palma},
  doi          = {10.1088/2632-2153/ad6120},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035014},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Quantum extreme learning of molecular potential energy surfaces and force fields},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing data acquisition: A bayesian approach for
efficient machine learning model training. <em>MLST</em>, <em>5</em>(3),
035013. (<a href="https://doi.org/10.1088/2632-2153/ad605f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acquiring a substantial number of data points for training accurate machine learning (ML) models is a big challenge in scientific fields where data collection is resource-intensive. Here, we propose a novel approach for constructing a minimal yet highly informative database for training ML models in complex multi-dimensional parameter spaces. To achieve this, we mimic the underlying relation between the output and input parameters using Gaussian process regression (GPR). Using a set of known data, GPR provides predictive means and standard deviation for the unknown data. Given the predicted standard deviation by GPR, we select data points using Bayesian optimization to obtain an efficient database for training ML models. We compare the performance of ML models trained on databases obtained through this method, with databases obtained using traditional approaches. Our results demonstrate that the ML models trained on the database obtained using Bayesian optimization approach consistently outperform the other two databases, achieving high accuracy with a significantly smaller number of data points. Our work contributes to the resource-efficient collection of data in high-dimensional complex parameter spaces, to achieve high precision ML predictions.},
  archive      = {J_MLST},
  author       = {M R Mahani and Igor A Nechepurenko and Yasmin Rahimof and Andreas Wicht},
  doi          = {10.1088/2632-2153/ad605f},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035013},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Optimizing data acquisition: A bayesian approach for efficient machine learning model training},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time confinement regime detection in fusion plasmas
with convolutional neural networks and high-bandwidth edge fluctuation
measurements. <em>MLST</em>, <em>5</em>(3), 035012. (<a
href="https://doi.org/10.1088/2632-2153/ad605e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A real-time detection of the plasma confinement regime can enable new advanced plasma control capabilities for both the access to and sustainment of enhanced confinement regimes in fusion devices. For example, a real-time indication of the confinement regime can facilitate transition to the high-performing wide-pedestal (WP) quiescent H-mode, or avoid unwanted transitions to lower confinement regimes that may induce plasma termination. To demonstrate real-time confinement regime detection, we use the 2D beam emission spectroscopy (BES) diagnostic system to capture localized density fluctuations of long wavelength turbulent modes in the edge region at a 1 MHz sampling rate. BES data from 330 discharges in either L-mode, H-mode, quiescent H (QH)-mode, or WP QH-mode were collected from the DIII-D tokamak and curated to develop a high-quality database to train a deep-learning classification model for real-time confinement detection. We utilize the 6 × 8 spatial configuration with a time window of 1024 µ s and recast the input to obtain spectral-like features via fast Fourier transform preprocessing. We employ a shallow 3D convolutional neural network for the multivariate time-series classification task and utilize a softmax in the final dense layer to retrieve a probability distribution over the different confinement regimes. Our model classifies the global confinement state on 44 unseen test discharges with an average F 1 score of 0.94, using only ∼1 ms snippets of BES data at a time. This activity demonstrates the feasibility for real-time data analysis of fluctuation diagnostics in future devices such as ITER, where the need for reliable and advanced plasma control is urgent.},
  archive      = {J_MLST},
  author       = {K Gill and D Smith and S Joung and B Geiger and G McKee and J Zimmerman and R Coffee and A Jalalvand and E Kolemen},
  doi          = {10.1088/2632-2153/ad605e},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035012},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Real-time confinement regime detection in fusion plasmas with convolutional neural networks and high-bandwidth edge fluctuation measurements},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Alzheimer’s disease detection and stage identification from
magnetic resonance brain images using vision transformer. <em>MLST</em>,
<em>5</em>(3), 035011. (<a
href="https://doi.org/10.1088/2632-2153/ad5fdc">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning techniques applied in neuroimaging have prompted researchers to build models for early diagnosis of brain illnesses such as Alzheimer&#39;s disease (AD). Although this task is difficult, advanced deep-learning (DL) approaches can be used. These DL models are effective, but difficult to interpret, time-consuming, and resource-intensive. Therefore, neuroscientists are interested in employing novel, less complex structures such as transformers that have superior pattern-extraction capabilities. In this study, an automated framework for accurate AD diagnosis and precise stage identification was developed by employing vision transformers (ViTs) with fewer computational resources. ViT, which captures the global context as opposed to convolutional neural networks (CNNs) with local receptive fields, is more efficient for brain image processing than CNN because the brain is a highly complex network with connected parts. The self-attention mechanism in the ViT helps to achieve this goal. Magnetic resonance brain images belonging to four stages were utilized to develop the proposed model, which achieved 99.83% detection accuracy, 99.69% sensitivity, 99.88% specificity, and 0.17% misclassification rate. Moreover, to prove the ability of the model to generalize, the mean distances of the transformer blocks and attention heat maps were visualized to understand what the model learned from the MRI input image.},
  archive      = {J_MLST},
  author       = {Mohammad H Alshayeji},
  doi          = {10.1088/2632-2153/ad5fdc},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035011},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Alzheimer’s disease detection and stage identification from magnetic resonance brain images using vision transformer},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guided quantum compression for high dimensional data
classification. <em>MLST</em>, <em>5</em>(3), 035010. (<a
href="https://doi.org/10.1088/2632-2153/ad5fdd">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum machine learning provides a fundamentally different approach to analyzing data. However, many interesting datasets are too complex for currently available quantum computers. Present quantum machine learning applications usually diminish this complexity by reducing the dimensionality of the data, e.g. via auto-encoders, before passing it through the quantum models. Here, we design a classical-quantum paradigm that unifies the dimensionality reduction task with a quantum classification model into a single architecture: the guided quantum compression model. We exemplify how this architecture outperforms conventional quantum machine learning approaches on a challenging binary classification problem: identifying the Higgs boson in proton-proton collisions at the LHC. Furthermore, the guided quantum compression model shows better performance compared to the deep learning benchmark when using solely the kinematic variables in our dataset.},
  archive      = {J_MLST},
  author       = {Vasilis Belis and Patrick Odagiu and Michele Grossi and Florentin Reiter and Günther Dissertori and Sofia Vallecorsa},
  doi          = {10.1088/2632-2153/ad5fdd},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035010},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Guided quantum compression for high dimensional data classification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep probabilistic direction prediction in 3D with
applications to directional dark matter detectors. <em>MLST</em>,
<em>5</em>(3), 035009. (<a
href="https://doi.org/10.1088/2632-2153/ad5f13">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the first method to probabilistically predict 3D direction in a deep neural network model. The probabilistic predictions are modeled as a heteroscedastic von Mises-Fisher distribution on the sphere \mathbb{S}^2 , giving a simple way to quantify aleatoric uncertainty. This approach generalizes the cosine distance loss which is a special case of our loss function when the uncertainty is assumed to be uniform across samples. We develop approximations required to make the likelihood function and gradient calculations stable. The method is applied to the task of predicting the 3D directions of electrons, the most complex signal in a class of experimental particle physics detectors designed to demonstrate the particle nature of dark matter and study solar neutrinos. Using simulated Monte Carlo data, the initial direction of recoiling electrons is inferred from their tortuous trajectories, as captured by the 3D detectors. For 40\, keV electrons in a 70% He 30% CO 2 gas mixture at STP, the new approach achieves a mean cosine distance of 0.104 (26 ∘ ) compared to 0.556 (64 ∘ ) achieved by a non-machine learning algorithm. We show that the model is well-calibrated and accuracy can be increased further by removing samples with high predicted uncertainty. This advancement in probabilistic 3D directional learning could increase the sensitivity of directional dark matter detectors.},
  archive      = {J_MLST},
  author       = {Majd Ghrear and Peter Sadowski and Sven E Vahsen},
  doi          = {10.1088/2632-2153/ad5f13},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035009},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deep probabilistic direction prediction in 3D with applications to directional dark matter detectors},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CResU-net: A method for landslide mapping using deep
learning. <em>MLST</em>, <em>5</em>(3), 035008. (<a
href="https://doi.org/10.1088/2632-2153/ad5f17">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Landslides, which can occur due to earthquakes and heavy rainfall, pose significant challenges across large areas. To effectively manage these disasters, it is crucial to have fast and reliable automatic detection methods for mapping landslides. In recent years, deep learning methods, particularly convolutional neural and fully convolutional networks, have been successfully applied to various fields, including landslide detection, with remarkable accuracy and high reliability. However, most of these models achieved high detection performance based on high-resolution satellite images. In this research, we introduce a modified Residual U-Net combined with the Convolutional Block Attention Module, a deep learning method, for automatic landslide mapping. The proposed method is trained and assessed using freely available data sets acquired from Sentinel-2 sensors, digital elevation models, and slope data from ALOS PALSAR with a spatial resolution of 10 m. Compared to the original ResU-Net model, the proposed architecture achieved higher accuracy, with the F1-score improving by 9.1% for the landslide class. Additionally, it offers a lower computational cost, with 1.38 giga multiply-accumulate operations per second (GMACS) needed to execute the model compared to 2.68 GMACS in the original model. The source code is available at https://github.com/manhhv87/LandSlideMapping.git .},
  archive      = {J_MLST},
  author       = {Thang M Pham and Nam Do and Ha T T Pham and Hanh T Bui and Thang T Do and Manh V Hoang},
  doi          = {10.1088/2632-2153/ad5f17},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035008},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {CResU-net: A method for landslide mapping using deep learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). End-to-end simulation of particle physics events with flow
matching and generator oversampling. <em>MLST</em>, <em>5</em>(3),
035007. (<a href="https://doi.org/10.1088/2632-2153/ad563c">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The simulation of high-energy physics collision events is a key element for data analysis at present and future particle accelerators. The comparison of simulation predictions to data allows looking for rare deviations that can be due to new phenomena not previously observed. We show that novel machine learning algorithms, specifically Normalizing Flows and Flow Matching, can be used to replicate accurate simulations from traditional approaches with several orders of magnitude of speed-up. The classical simulation chain starts from a physics process of interest, computes energy deposits of particles and electronics response, and finally employs the same reconstruction algorithms used for data. Eventually, the data are reduced to some high-level analysis format. Instead, we propose an end-to-end approach, simulating the final data format directly from physical generator inputs, skipping any intermediate steps. We use particle jets simulation as a benchmark for comparing both discrete and continuous Normalizing Flows models. The models are validated across a variety of metrics to identify the most accurate. We discuss the scaling of performance with the increase in training data, as well as the generalization power of these models on physical processes different from the training one. We investigate sampling multiple times from the same physical generator inputs, a procedure we name oversampling , and we show that it can effectively reduce the statistical uncertainties of a dataset. This class of ML algorithms is found to be capable of learning the expected detector response independently of the physical input process. The speed and accuracy of the models, coupled with the stability of the training procedure, make them a compelling tool for the needs of current and future experiments.},
  archive      = {J_MLST},
  author       = {F Vaselli and F Cattafesta and P Asenov and A Rizzi},
  doi          = {10.1088/2632-2153/ad563c},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035007},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {End-to-end simulation of particle physics events with flow matching and generator oversampling},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty quantification by direct propagation of shallow
ensembles. <em>MLST</em>, <em>5</em>(3), 035006. (<a
href="https://doi.org/10.1088/2632-2153/ad594a">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical learning algorithms provide a generally-applicable framework to sidestep time-consuming experiments, or accurate physics-based modeling, but they introduce a further source of error on top of the intrinsic limitations of the experimental or theoretical setup. Uncertainty estimation is essential to quantify this error, and to make application of data-centric approaches more trustworthy. To ensure that uncertainty quantification is used widely, one should aim for algorithms that are accurate, but also easy to implement and apply. In particular, including uncertainty quantification on top of an existing architecture should be straightforward, and add minimal computational overhead. Furthermore, it should be easy to manipulate or combine multiple machine-learning predictions, propagating uncertainty over further modeling steps. We compare several well-established uncertainty quantification frameworks against these requirements, and propose a practical approach, which we dub direct propagation of shallow ensembles, that provides a good compromise between ease of use and accuracy. We present benchmarks for generic datasets, and an in-depth study of applications to the field of atomistic machine learning for chemistry and materials. These examples underscore the importance of using a formulation that allows propagating errors without making strong assumptions on the correlations between different predictions of the model.},
  archive      = {J_MLST},
  author       = {Matthias Kellner and Michele Ceriotti},
  doi          = {10.1088/2632-2153/ad594a},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035006},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Uncertainty quantification by direct propagation of shallow ensembles},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the prediction of the turbulent flow behind cylinder
arrays via echo state networks. <em>MLST</em>, <em>5</em>(3), 035005.
(<a href="https://doi.org/10.1088/2632-2153/ad5414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims at the prediction of the turbulent flow behind cylinder arrays by the application of Echo State Networks (ESN). Three different arrangements of arrays of seven cylinders are chosen for the current study. These represent different flow regimes: single bluff body flow, transient flow, and co-shedding flow. This allows the investigation of turbulent flows that fundamentally originate from wake flows yet exhibit highly diverse dynamics. The data is reduced by Proper Orthogonal Decomposition (POD) which is optimal in terms of kinetic energy. The Time Coefficients of the POD Modes (TCPM) are predicted by the ESN. The network architecture is optimized with respect to its three main hyperparameters, Input Scaling (INS), Spectral Radius (SR), and Leaking Rate (LR), in order to produce the best predictions in terms of Weighted Prediction Score (WPS), a metric leveling statistic and deterministic prediction. In general, the ESN is capable of imitating the complex dynamics of turbulent flows even for longer periods of several vortex shedding cycles. Furthermore, the mutual interdependencies of the TCPM are well preserved. However, optimal hyperparameters depend strongly on the flow characteristics. Generally, as flow dynamics become faster and more intermittent, larger LR and INS values result in better predictions, whereas less clear trends for SR are observable.},
  archive      = {J_MLST},
  author       = {M Sharifi Ghazijahani and C Cierpka},
  doi          = {10.1088/2632-2153/ad5414},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035005},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {On the prediction of the turbulent flow behind cylinder arrays via echo state networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AMCG: A graph dual atomic-molecular conditional molecular
generator. <em>MLST</em>, <em>5</em>(3), 035004. (<a
href="https://doi.org/10.1088/2632-2153/ad5bbf">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drug design is both a time consuming and expensive endeavour. Computational strategies offer viable options to address this task; deep learning approaches in particular are indeed gaining traction for their capability of dealing with chemical structures. A straightforward way to represent such structures is via their molecular graph, which in turn can be naturally processed by graph neural networks. This paper introduces AMCG, a dual atomic-molecular, conditional, latent-space, generative model built around graph processing layers able to support both unconditional and conditional molecular graph generation. Among other features, AMCG is a one-shot model allowing for fast sampling, explicit atomic type histogram assignation and property optimization via gradient ascent. The model was trained on the Quantum Machines 9 (QM9) and ZINC datasets, achieving state-of-the-art performances. Together with classic benchmarks, AMCG was also tested by generating large-scale sampled sets, showing robustness in terms of sustainable throughput of valid, novel and unique molecules.},
  archive      = {J_MLST},
  author       = {Carlo Abate and Sergio Decherchi and Andrea Cavalli},
  doi          = {10.1088/2632-2153/ad5bbf},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035004},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {AMCG: A graph dual atomic-molecular conditional molecular generator},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The r-mAtrIx net. <em>MLST</em>, <em>5</em>(3), 035003. (<a
href="https://doi.org/10.1088/2632-2153/ad56f9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide a novel neural network architecture that can: i) output R-matrix for a given quantum integrable spin chain, ii) search for an integrable Hamiltonian and the corresponding R-matrix under assumptions of certain symmetries or other restrictions, iii) explore the space of Hamiltonians around already learned models and reconstruct the family of integrable spin chains which they belong to. The neural network training is done by minimizing loss functions encoding Yang–Baxter equation, regularity and other model-specific restrictions such as hermiticity. Holomorphy is implemented via the choice of activation functions. We demonstrate the work of our neural network on the spin chains of difference form with two-dimensional local space. In particular, we reconstruct the R-matrices for all 14 classes. We also demonstrate its utility as an Explorer , scanning a certain subspace of Hamiltonians and identifying integrable classes after clusterisation. The last strategy can be used in future to carve out the map of integrable spin chains with higher dimensional local space and in more general settings where no analytical methods are available.},
  archive      = {J_MLST},
  author       = {Shailesh Lal and Suvajit Majumder and Evgeny Sobko},
  doi          = {10.1088/2632-2153/ad56f9},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035003},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {The R-mAtrIx net},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TomOpt: Differential optimisation for task- and
constraint-aware design of particle detectors in the context of muon
tomography. <em>MLST</em>, <em>5</em>(3), 035002. (<a
href="https://doi.org/10.1088/2632-2153/ad52e7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a software package, TomOpt, developed to optimise the geometrical layout and specifications of detectors designed for tomography by scattering of cosmic-ray muons. The software exploits differentiable programming for the modeling of muon interactions with detectors and scanned volumes, the inference of volume properties, and the optimisation cycle performing the loss minimisation. In doing so, we provide the first demonstration of end-to-end-differentiable and inference-aware optimisation of particle physics instruments. We study the performance of the software on a relevant benchmark scenario and discuss its potential applications. Our code is available on Github (Strong et al 2024 available at: https://github.com/GilesStrong/tomopt ).},
  archive      = {J_MLST},
  author       = {Giles C Strong and Maxime Lagrange and Aitor Orio and Anna Bordignon and Florian Bury and Tommaso Dorigo and Andrea Giammanco and Mariam Heikal and Jan Kieseler and Max Lamparth and Pablo Martínez Ruíz del Árbol and Federico Nardi and Pietro Vischia and Haitham Zaraket},
  doi          = {10.1088/2632-2153/ad52e7},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035002},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {TomOpt: Differential optimisation for task- and constraint-aware design of particle detectors in the context of muon tomography},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mixed noise and posterior estimation with conditional
deepGEM. <em>MLST</em>, <em>5</em>(3), 035001. (<a
href="https://doi.org/10.1088/2632-2153/ad5926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop an algorithm for jointly estimating the posterior and the noise parameters in Bayesian inverse problems, which is motivated by indirect measurements and applications from nanometrology with a mixed noise model. We propose to solve the problem by an expectation maximization (EM) algorithm. Based on the current noise parameters, we learn in the E-step a conditional normalizing flow that approximates the posterior. In the M-step, we propose to find the noise parameter updates again by an EM algorithm, which has analytical formulas. We compare the training of the conditional normalizing flow with the forward and reverse Kullback–Leibler divergence, and show that our model is able to incorporate information from many measurements, unlike previous approaches.},
  archive      = {J_MLST},
  author       = {Paul Hagemann and Johannes Hertrich and Maren Casfor and Sebastian Heidenreich and Gabriele Steidl},
  doi          = {10.1088/2632-2153/ad5926},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Mixed noise and posterior estimation with conditional deepGEM},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Benchmarking of machine learning interatomic potentials for
reactive hydrogen dynamics at metal surfaces. <em>MLST</em>,
<em>5</em>(3), 030501. (<a
href="https://doi.org/10.1088/2632-2153/ad5f11">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulations of chemical reaction probabilities in gas surface dynamics require the calculation of ensemble averages over many tens of thousands of reaction events to predict dynamical observables that can be compared to experiments. At the same time, the energy landscapes need to be accurately mapped, as small errors in barriers can lead to large deviations in reaction probabilities. This brings a particularly interesting challenge for machine learning interatomic potentials, which are becoming well-established tools to accelerate molecular dynamics simulations. We compare state-of-the-art machine learning interatomic potentials with a particular focus on their inference performance on CPUs and suitability for high throughput simulation of reactive chemistry at surfaces. The considered models include polarizable atom interaction neural networks (PaiNN), recursively embedded atom neural networks (REANN), the MACE equivariant graph neural network, and atomic cluster expansion potentials (ACE). The models are applied to a dataset on reactive molecular hydrogen scattering on low-index surface facets of copper. All models are assessed for their accuracy, time-to-solution, and ability to simulate reactive sticking probabilities as a function of the rovibrational initial state and kinetic incidence energy of the molecule. REANN and MACE models provide the best balance between accuracy and time-to-solution and can be considered the current state-of-the-art in gas-surface dynamics. PaiNN models require many features for the best accuracy, which causes significant losses in computational efficiency. ACE models provide the fastest time-to-solution, however, models trained on the existing dataset were not able to achieve sufficiently accurate predictions in all cases.},
  archive      = {J_MLST},
  author       = {Wojciech G Stark and Cas van der Oord and Ilyes Batatia and Yaolong Zhang and Bin Jiang and Gábor Csányi and Reinhard J Maurer},
  doi          = {10.1088/2632-2153/ad5f11},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {030501},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Benchmarking of machine learning interatomic potentials for reactive hydrogen dynamics at metal surfaces},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable machine learning assisted molecular-level
insights for enhanced specific stiffness exploiting the large
compositional space of AlCoCrFeNi high entropy alloys. <em>MLST</em>,
<em>5</em>(2), 025082. (<a
href="https://doi.org/10.1088/2632-2153/ad55a4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Design of high entropy alloys (HEA) presents a significant challenge due to the large compositional space and composition-specific variation in their functional behavior. The traditional alloy design would include trial-and-error prototyping and high-throughput experimentation, which again is challenging due to large-scale fabrication and experimentation. To address these challenges, this article presents a computational strategy for HEA design based on the seamless integration of quasi-random sampling, molecular dynamics (MD) simulations and machine learning (ML). A limited number of algorithmically chosen molecular-level simulations are performed to create a Gaussian process-based computational mapping between the varying concentrations of constituent elements of the HEA and effective properties like Young&#39;s modulus and density. The computationally efficient ML models are subsequently exploited for large-scale predictions and multi-objective functionality attainment with non-aligned goals. The study reveals that there exists a strong negative correlation between Al concentration and the desired effective properties of AlCoCrFeNi HEA, whereas the Ni concentration exhibits a strong positive correlation. The deformation mechanism further shows that excessive increase of Al concentration leads to a higher percentage of face-centered cubic to body-centered cubic phase transformation which is found to be relatively lower in the HEA with reduced Al concentration. Such physical insights during the deformation process would be crucial in the alloy design process along with the data-driven predictions. As an integral part of this investigation, the developed ML models are interpreted based on Shapley Additive exPlanations, which are essential to explain and understand the model&#39;s mechanism along with meaningful deployment. The data-driven strategy presented here will lead to devising an efficient explainable ML-based bottom-up approach to alloy design for multi-objective non-aligned functionality attainment.},
  archive      = {J_MLST},
  author       = {K K Gupta and S Barman and S Dey and T Mukhopadhyay},
  doi          = {10.1088/2632-2153/ad55a4},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {2},
  pages        = {025082},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Explainable machine learning assisted molecular-level insights for enhanced specific stiffness exploiting the large compositional space of AlCoCrFeNi high entropy alloys},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GPU optimization techniques to accelerate optiGAN—a particle
simulation GAN. <em>MLST</em>, <em>5</em>(2), 027001. (<a
href="https://doi.org/10.1088/2632-2153/ad51c9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for specialized hardware to train AI models has increased in tandem with the increase in the model complexity over the recent years. Graphics processing unit (GPU) is one such hardware that is capable of parallelizing operations performed on a large chunk of data. Companies like Nvidia, AMD, and Google have been constantly scaling-up the hardware performance as fast as they can. Nevertheless, there is still a gap between the required processing power and processing capacity of the hardware. To increase the hardware utilization, the software has to be optimized too. In this paper, we present some general GPU optimization techniques we used to efficiently train the optiGAN model, a Generative Adversarial Network that is capable of generating multidimensional probability distributions of optical photons at the photodetector face in radiation detectors, on an 8GB Nvidia Quadro RTX 4000 GPU. We analyze and compare the performances of all the optimizations based on the execution time and the memory consumed using the Nvidia Nsight Systems profiler tool. The optimizations gave approximately a 4.5x increase in the runtime performance when compared to a naive training on the GPU, without compromising the model performance. Finally we discuss optiGANs future work and how we are planning to scale the model on GPUs.},
  archive      = {J_MLST},
  author       = {Anirudh Srikanth and Carlotta Trigila and Emilie Roncali},
  doi          = {10.1088/2632-2153/ad51c9},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {027001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {GPU optimization techniques to accelerate optiGAN—a particle simulation GAN},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deciphering peptide-protein interactions via
composition-based prediction: A case study with survivin/BIRC5.
<em>MLST</em>, <em>5</em>(2), 025081. (<a
href="https://doi.org/10.1088/2632-2153/ad5784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of atomic physics and chemistry, composition emerges as the most powerful means of describing matter. Mendeleev&#39;s periodic table and chemical formulas, while not entirely free from ambiguities, provide robust approximations for comprehending the properties of atoms, chemicals, and their collective behaviours, which stem from the dynamic interplay of their constituents. Our study illustrates that protein-protein interactions follow a similar paradigm, wherein the composition of peptides plays a pivotal role in predicting their interactions with the protein survivin, using an elegantly simple model. An analysis of these predictions within the context of the human proteome not only confirms the known cellular locations of survivin and its interaction partners, but also introduces novel insights into biological functionality. It becomes evident that electrostatic- and primary structure-based descriptions fall short in predictive power, leading us to speculate that protein interactions are orchestrated by the collective dynamics of functional groups.},
  archive      = {J_MLST},
  author       = {Atsarina Larasati Anindya and Torbjörn Nur Olsson and Maja Jensen and Maria-Jose Garcia-Bonete and Sally P Wheatley and Maria I Bokarewa and Stefano A Mezzasalma and Gergely Katona},
  doi          = {10.1088/2632-2153/ad5784},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025081},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deciphering peptide-protein interactions via composition-based prediction: A case study with survivin/BIRC5},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive particle swarm optimization with information
interaction mechanism. <em>MLST</em>, <em>5</em>(2), 025080. (<a
href="https://doi.org/10.1088/2632-2153/ad55a5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an adaptive particle swarm optimization with information interaction mechanism (APSOIIM) to enhance the optimization ability of the PSO algorithm. Firstly, a chaotic sequence strategy is employed to generate uniformly distributed particles and to improve their convergence speed at the initialization stage of the algorithm. Then, an interaction information mechanism is introduced to boost the diversity of the population as the search process unfolds, which can effectively interact with the optimal information of neighboring particles to enhance the exploration and exploitation abilities. Therefore, the proposed algorithm may avoid premature and perform a more accurate local search. Besides, the convergence was proven to verify the robustness and efficiency of the proposed APSOIIM algorithm. Finally, the proposed APSOIIM was applied to solve the CEC2014 and CEC2017 benchmark functions as well as famous engineering optimization problems. The experimental results demonstrate that the proposed APSOIIM has significant advantages over the compared algorithms.},
  archive      = {J_MLST},
  author       = {Rui Liu and Lisheng Wei and Pinggai Zhang},
  doi          = {10.1088/2632-2153/ad55a5},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025080},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {An adaptive particle swarm optimization with information interaction mechanism},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unification of symmetries inside neural networks:
Transformer, feedforward and neural ODE. <em>MLST</em>, <em>5</em>(2),
025079. (<a href="https://doi.org/10.1088/2632-2153/ad5927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the inner workings of neural networks, including transformers, remains one of the most challenging puzzles in machine learning. This study introduces a novel approach by applying the principles of gauge symmetries, a key concept in physics, to neural network architectures. By regarding model functions as physical observables, we find that parametric redundancies of various machine learning models can be interpreted as gauge symmetries. We mathematically formulate the parametric redundancies in neural ODEs, and find that their gauge symmetries are given by spacetime diffeomorphisms, which play a fundamental role in Einstein&#39;s theory of gravity. Viewing neural ODEs as a continuum version of feedforward neural networks, we show that the parametric redundancies in feedforward neural networks are indeed lifted to diffeomorphisms in neural ODEs. We further extend our analysis to transformer models, finding natural correspondences with neural ODEs and their gauge symmetries. The concept of gauge symmetries sheds light on the complex behavior of deep learning models through physics and provides us with a unifying perspective for analyzing various machine learning architectures.},
  archive      = {J_MLST},
  author       = {Koji Hashimoto and Yuji Hirono and Akiyoshi Sannai},
  doi          = {10.1088/2632-2153/ad5927},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025079},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Unification of symmetries inside neural networks: Transformer, feedforward and neural ODE},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unlearning regularization for boltzmann machines.
<em>MLST</em>, <em>5</em>(2), 025078. (<a
href="https://doi.org/10.1088/2632-2153/ad5a5f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boltzmann machines (BMs) are graphical models with interconnected binary units, employed for the unsupervised modeling of data distributions. When trained on real data, BMs show the tendency to behave like critical systems, displaying a high susceptibility of the model under a small rescaling of the inferred parameters. This behavior is not convenient for the purpose of generating data, because it slows down the sampling process, and induces the model to overfit the training-data. In this study, we introduce a regularization method for BMs to improve the robustness of the model under rescaling of the parameters. The new technique shares formal similarities with the unlearning algorithm, an iterative procedure used to improve memory associativity in Hopfield-like neural networks. We test our unlearning regularization on synthetic data generated by two simple models, the Curie–Weiss ferromagnetic model and the Sherrington–Kirkpatrick spin glass model. We show that it outperforms L p -norm schemes and discuss the role of parameter initialization. Eventually, the method is applied to learn the activity of real neuronal cells, confirming its efficacy at shifting the inferred model away from criticality and coming out as a powerful candidate for actual scientific implementations.},
  archive      = {J_MLST},
  author       = {Enrico Ventura and Simona Cocco and Rémi Monasson and Francesco Zamponi},
  doi          = {10.1088/2632-2153/ad5a5f},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025078},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Unlearning regularization for boltzmann machines},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Performance deterioration of deep learning models after
clinical deployment: A case study with auto-segmentation for definitive
prostate cancer radiotherapy. <em>MLST</em>, <em>5</em>(2), 025077. (<a
href="https://doi.org/10.1088/2632-2153/ad580f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our study aims to explore the long-term performance patterns for deep learning (DL) models deployed in clinic and to investigate their efficacy in relation to evolving clinical practices. We conducted a retrospective study simulating the clinical implementation of our DL model involving 1328 prostate cancer patients treated between January 2006 and August 2022. We trained and validated a U-Net-based auto-segmentation model on data obtained from 2006 to 2011 and tested on data from 2012 to 2022, simulating the model&#39;s clinical deployment starting in 2012. We visualized the trends of the model performance using exponentially weighted moving average (EMA) curves. Additionally, we performed Wilcoxon Rank Sum Test and multiple linear regression to investigate Dice similarity coefficient (DSC) variations across distinct periods and the impact of clinical factors, respectively. Initially, from 2012 to 2014, the model showed high performance in segmenting the prostate, rectum, and bladder. Post-2015, a notable decline in EMA DSC was observed for the prostate and rectum, while bladder contours remained stable. Key factors impacting the prostate contour quality included physician contouring styles, using various hydrogel spacers, CT scan slice thickness, MRI-guided contouring, and intravenous (IV) contrast ( p &lt; 0.0001, p &lt; 0.0001, p = 0.0085, p = 0.0012, p &lt; 0.0001, respectively). Rectum contour quality was notably influenced by factors such as slice thickness, physician contouring styles, and the use of various hydrogel spacers. The quality of the bladder contour was primarily affected by IV contrast. The deployed DL model exhibited a substantial decline in performance over time, aligning with the evolving clinical settings.},
  archive      = {J_MLST},
  author       = {Biling Wang and Michael Dohopolski and Ti Bai and Junjie Wu and Raquibul Hannan and Neil Desai and Aurelie Garant and Daniel Yang and Dan Nguyen and Mu-Han Lin and Robert Timmerman and Xinlei Wang and Steve B Jiang},
  doi          = {10.1088/2632-2153/ad580f},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025077},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Performance deterioration of deep learning models after clinical deployment: A case study with auto-segmentation for definitive prostate cancer radiotherapy},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Merging automatic differentiation and the adjoint method for
photonic inverse design. <em>MLST</em>, <em>5</em>(2), 025076. (<a
href="https://doi.org/10.1088/2632-2153/ad5411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing the shapes and topology of physical devices is crucial for both scientific and technological advancements, given their wide-ranging implications across numerous industries and research areas. Innovations in shape and topology optimization have been observed across a wide range of fields, notably structural mechanics, fluid mechanics, and more recently, photonics. Gradient-based inverse design techniques have been particularly successful for photonic and optical problems, resulting in integrated, miniaturized hardware that has set new standards in device performance. To calculate the gradients, there are typically two approaches: namely, either by implementing specialized solvers using automatic differentiation (AD) or by deriving analytical solutions for gradient calculation and adjoint sources by hand. In this work, we propose a middle ground and present a hybrid approach that leverages and enables the benefits of AD for handling gradient derivation while using existing, proven but black-box photonic solvers for numerical solutions. Utilizing the adjoint method, we make existing numerical solvers differentiable and seamlessly integrate them into an AD framework. Further, this enables users to integrate the optimization environment seamlessly with other autodifferentiable components such as machine learning, geometry generation, or intricate post-processing which could lead to better photonic design workflows. We illustrate the approach through two distinct photonic optimization problems: optimizing the Purcell factor of a magnetic dipole in the vicinity of an optical nanocavity and enhancing the light extraction efficiency of a µ LED.},
  archive      = {J_MLST},
  author       = {Alexander Luce and Rasoul Alaee and Fabian Knorr and Florian Marquardt},
  doi          = {10.1088/2632-2153/ad5411},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025076},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Merging automatic differentiation and the adjoint method for photonic inverse design},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Finetuning foundation models for joint analysis optimization
in high energy physics. <em>MLST</em>, <em>5</em>(2), 025075. (<a
href="https://doi.org/10.1088/2632-2153/ad55a3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we demonstrate that significant gains in performance and data efficiency can be achieved in High Energy Physics (HEP) by moving beyond the standard paradigm of sequential optimization or reconstruction and analysis components. We conceptually connect HEP reconstruction and analysis to modern machine learning workflows such as pretraining, finetuning, domain adaptation and high-dimensional embedding spaces and quantify the gains in the example usecase of searches of heavy resonances decaying via an intermediate di-Higgs system to four b -jets. To our knowledge this is the first example of a low-level feature extraction network finetuned for a downstream HEP analysis objective.},
  archive      = {J_MLST},
  author       = {Matthias Vigl and Nicole Hartman and Lukas Heinrich},
  doi          = {10.1088/2632-2153/ad55a3},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025075},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Finetuning foundation models for joint analysis optimization in high energy physics},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse autoregressive neural networks for classical spin
systems. <em>MLST</em>, <em>5</em>(2), 025074. (<a
href="https://doi.org/10.1088/2632-2153/ad5783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient sampling and approximation of Boltzmann distributions involving large sets of binary variables, or spins, are pivotal in diverse scientific fields even beyond physics. Recent advances in generative neural networks have significantly impacted this domain. However, these neural networks are often treated as black boxes, with architectures primarily influenced by data-driven problems in computational science. Addressing this gap, we introduce a novel autoregressive neural network architecture named TwoBo, specifically designed for sparse two-body interacting spin systems. We directly incorporate the Boltzmann distribution into its architecture and parameters, resulting in enhanced convergence speed, superior free energy accuracy, and reduced trainable parameters. We perform numerical experiments on disordered, frustrated systems with more than 1000 spins on grids and random graphs, and demonstrate its advantages compared to previous autoregressive and recurrent architectures. Our findings validate a physically informed approach and suggest potential extensions to multivalued variables and many-body interaction systems, paving the way for broader applications in scientific research.},
  archive      = {J_MLST},
  author       = {Indaco Biazzo and Dian Wu and Giuseppe Carleo},
  doi          = {10.1088/2632-2153/ad5783},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025074},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Sparse autoregressive neural networks for classical spin systems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning methods for hamiltonian parameter estimation
and magnetic domain image generation in twisted van der waals magnets.
<em>MLST</em>, <em>5</em>(2), 025073. (<a
href="https://doi.org/10.1088/2632-2153/ad56fa">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of twist engineering in van der Waals magnets has opened new frontiers in the field of two-dimensional magnetism, yielding distinctive magnetic domain structures. Despite the introduction of numerous theoretical methods, limitations persist in terms of accuracy or efficiency due to the complex nature of the magnetic Hamiltonians pertinent to these systems. In this study, we introduce a deep-learning approach to tackle these challenges. Utilizing customized, fully connected networks, we develop two deep-neural-network kernels that facilitate efficient and reliable analysis of twisted van der Waals magnets. Our regression model is adept at estimating the magnetic Hamiltonian parameters of twisted bilayer CrI 3 from its magnetic domain images generated through atomistic spin simulations. The &#39;generative model&#39; excels in producing precise magnetic domain images from the provided magnetic parameters. The trained networks for these models undergo thorough validation, including statistical error analysis and assessment of robustness against noisy injections. These advancements not only extend the applicability of deep-learning methods to twisted van der Waals magnets but also streamline future investigations into these captivating yet poorly understood systems.},
  archive      = {J_MLST},
  author       = {Woo Seok Lee and Taegeun Song and Kyoung-Min Kim},
  doi          = {10.1088/2632-2153/ad56fa},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025073},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deep learning methods for hamiltonian parameter estimation and magnetic domain image generation in twisted van der waals magnets},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessing the quality of random number generators through
neural networks. <em>MLST</em>, <em>5</em>(2), 025072. (<a
href="https://doi.org/10.1088/2632-2153/ad56fb">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we address the use of Neural Networks (NNs) for the assessment of the quality and hence safety of several Random Number Generators (RNGs), focusing both on the vulnerability of classical Pseudo Random Number Generators (PRNGs), such as Linear Congruential Generators (LCGs) and the RC4 algorithm, and extending our analysis to non-conventional data sources, such as Quantum Random Number Generators (QRNGs) based on Vertical-Cavity Surface-Emitting Laser (VCSEL). Among the results found, we have classified the generators based on the capability of the NN to distinguish between the RNG and a Golden Standard RNG (GSRNG). We show that sequences from simple PRNGs like LCGs and RC4 can be distinguished from the GSRNG. We also show that sequences from LCG on elliptic curves and VCSEL-based QRNG can not be distinguished from the GSRNG even with the biggest long-short term memory or convolutional neural networks (CNNs) that we have considered. We underline the fundamental role of design decisions in enhancing the safety of RNGs. The influence of network architecture design and associated hyper-parameters variations was also explored. We show that longer sequence lengths and CNNs are more effective for discriminating RNGs against the GSRNG. Moreover, in the prediction domain, the proposed model is able to deftly distinguish between the raw data of our QRNG and data from the GSRNG exhibiting a cross-entropy error of 0.52 on the test data-set used. All these findings reveal the potential of NNs to enhance the security of RNGs, while highlighting the robustness of certain QRNGs, in particular the VCSEL-based variants, for high-quality random number generation applications.},
  archive      = {J_MLST},
  author       = {José Luis Crespo and Javier González-Villa and Jaime Gutiérrez and Angel Valle},
  doi          = {10.1088/2632-2153/ad56fb},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025072},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Assessing the quality of random number generators through neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Completion of partial chemical equations. <em>MLST</em>,
<em>5</em>(2), 025071. (<a
href="https://doi.org/10.1088/2632-2153/ad5413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring missing molecules in chemical equations is an important task in chemistry and drug discovery. In fact, the completion of chemical equations with necessary reagents is important for improving existing datasets by detecting missing compounds, making them compatible with deep learning models that require complete information about reactants, products, and reagents in a chemical equation for increased performance. Here, we present a deep learning model to predict missing molecules using a multi-task approach, which can ultimately be viewed as a generalization of the forward reaction prediction and retrosynthesis models, since both can be expressed in terms of incomplete chemical equations. We illustrate that a single trained model, based on the transformer architecture and acting on reaction SMILES strings, can address the prediction of products (forward), precursors (retro) or any other molecule in arbitrary positions such as solvents, catalysts or reagents (completion). Our aim is to assess whether a unified model trained simultaneously on different tasks can effectively leverage diverse knowledge from various prediction tasks within the chemical domain, compared to models trained individually on each application. The multi-task models demonstrate top-1 performance of 72.4%, 16.1%, and 30.5% for the forward, retro, and completion tasks, respectively. For the same model we computed round-trip accuracy of 83.4%. The completion task exhibiting improvements due to the multi-task approach.},
  archive      = {J_MLST},
  author       = {Federico Zipoli and Zeineb Ayadi and Philippe Schwaller and Teodoro Laino and Alain C Vaucher},
  doi          = {10.1088/2632-2153/ad5413},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025071},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Completion of partial chemical equations},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Journey over destination: Dynamic sensor placement enhances
generalization. <em>MLST</em>, <em>5</em>(2), 025070. (<a
href="https://doi.org/10.1088/2632-2153/ad4e06">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing complex, high-dimensional global fields from limited data points is a challenge across various scientific and industrial domains. This is particularly important for recovering spatio-temporal fields using sensor data from, for example, laboratory-based scientific experiments, weather forecasting, or drone surveys. Given the prohibitive costs of specialized sensors and the inaccessibility of certain regions of the domain, achieving full field coverage is typically not feasible. Therefore, the development of machine learning algorithms trained to reconstruct fields given a limited dataset is of critical importance. In this study, we introduce a general approach that employs moving sensors to enhance data exploitation during the training of an attention based neural network, thereby improving field reconstruction. The training of sensor locations is accomplished using an end-to-end workflow, ensuring differentiability in the interpolation of field values associated to the sensors, and is simple to implement using differentiable programming. Additionally, we have incorporated a correction mechanism to prevent sensors from entering invalid regions within the domain. We evaluated our method using two distinct datasets; the results show that our approach enhances learning, as evidenced by improved test scores.},
  archive      = {J_MLST},
  author       = {Agnese Marcato and Eric Guiltinan and Hari Viswanathan and Daniel O’Malley and Nicholas Lubbers and Javier E Santos},
  doi          = {10.1088/2632-2153/ad4e06},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025070},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Journey over destination: Dynamic sensor placement enhances generalization},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning meets kepler: Inverting kepler’s equation
for all vs all conjunction analysis. <em>MLST</em>, <em>5</em>(2),
025069. (<a href="https://doi.org/10.1088/2632-2153/ad51cc">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number of satellites in orbit around Earth is increasing rapidly, with the risk of collision rising accordingly. Trends of the global population of satellites need to be analyzed to test the viability and impact of proposed rules and laws affecting the satellite population and collision avoidance strategies. This requires large scale simulations of satellites that are propagated on long timescales to compute the large amounts of actionable close encounters (called conjunctions), which could lead to collisions. Rigorously checking for conjunctions by computing future states of orbits is computationally expensive due to the large amount of objects involved and conjunction filters are thus used to remove non-conjuncting orbit pairs from the list of possible conjunctions. In this work, we explore the possibility of machine learning (ML) based conjunction filters using several algorithms such as eXtreme Gradient Boosting, TabNet and (physics-informed) neural networks and deep operator networks. To show the viability and the potential of ML based filters, these algorithms are trained to predict the future state of orbits. For the physics-informed approaches, multiple partial differential equations are set up using the Kepler equation as a basis. The empirical results demonstrate that physics-informed deep operator networks are capable of predicting the future state of orbits using these equations (RMSE: 0.136) and outperform eXtreme Gradient Boosting (RMSE: 0.568) and TabNet (RMSE: 0.459). We also propose a filter based on the trained deep operator network which is shown to outperforms the filter capability of the commonly used perigee-apogee test and the orbit path filter on a synthetic dataset, while being on average 3.2 times faster to compute than a rigorous conjunction check.},
  archive      = {J_MLST},
  author       = {Kevin Otto and Simon Burgis and Kristian Kersting and Reinhold Bertrand and Devendra Singh Dhami},
  doi          = {10.1088/2632-2153/ad51cc},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025069},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning meets kepler: Inverting kepler’s equation for all vs all conjunction analysis},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STG-MTL: Scalable task grouping for multi-task learning
using data maps. <em>MLST</em>, <em>5</em>(2), 025068. (<a
href="https://doi.org/10.1088/2632-2153/ad4e04">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Task Learning (MTL) is a powerful technique that has gained popularity due to its performance improvement over traditional Single-Task Learning (STL). However, MTL is often challenging because there is an exponential number of possible task groupings, which can make it difficult to choose the best one because some groupings might produce performance degradation due to negative interference between tasks. That is why existing solutions are severely suffering from scalability issues, limiting any practical application. In our paper, we propose a new data-driven method that addresses these challenges and provides a scalable and modular solution for classification task grouping based on a re-proposed data-driven features, Data Maps, which capture the training dynamics for each classification task during the MTL training. Through a theoretical comparison with other techniques, we manage to show that our approach has the superior scalability. Our experiments show a better performance and verify the method&#39;s effectiveness, even on an unprecedented number of tasks (up to 100 tasks on CIFAR100). Being the first to work on such number of tasks, our comparisons on the resulting grouping shows similar grouping to the mentioned in the dataset, CIFAR100. Finally, we provide a modular implementation 3 for easier integration and testing, with examples from multiple datasets and tasks.},
  archive      = {J_MLST},
  author       = {Ammar Sherif and Abubakar Abid and Mustafa Elattar and Mohamed ElHelw},
  doi          = {10.1088/2632-2153/ad4e04},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025068},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {STG-MTL: Scalable task grouping for multi-task learning using data maps},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interpreting variational quantum models with active paths in
parameterized quantum circuits. <em>MLST</em>, <em>5</em>(2), 025067.
(<a href="https://doi.org/10.1088/2632-2153/ad5412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational quantum machine learning (VQML) models based on parameterized quantum circuits (PQC) have been expected to offer a potential quantum advantage for machine learning (ML) applications. However, comparison between VQML models and their classical counterparts is hard due to the lack of interpretability of VQML models. In this study, we introduce a graphical approach to analyze the PQC and the corresponding operation of VQML models to deal with this problem. In particular, we utilize the Stokes representation of quantum states to treat VQML models as network models based on the corresponding representations of basic gates. From this approach, we suggest the notion of active paths in the networks and relate the expressivity of VQML models with it. We investigate the growth of active paths in VQML models and observe that the expressivity of VQML models can be significantly limited for certain cases. Then we construct classical models inspired by our graphical interpretation of VQML models and show that they can emulate or outperform the outputs of VQML models for these cases. Our result provides a new way to interpret the operation of VQML models and facilitates the interconnection between quantum and classical ML areas.},
  archive      = {J_MLST},
  author       = {Kyungmin Lee and Hyungjun Jeon and Dongkyu Lee and Bongsang Kim and Jeongho Bang and Taehyun Kim},
  doi          = {10.1088/2632-2153/ad5412},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025067},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Interpreting variational quantum models with active paths in parameterized quantum circuits},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning pulses for transmon qubit entangling
gates. <em>MLST</em>, <em>5</em>(2), 025066. (<a
href="https://doi.org/10.1088/2632-2153/ad4f4d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The utility of a quantum computer is highly dependent on the ability to reliably perform accurate quantum logic operations. For finding optimal control solutions, it is of particular interest to explore model-free approaches, since their quality is not constrained by the limited accuracy of theoretical models for the quantum processor—in contrast to many established gate implementation strategies. In this work, we utilize a continuous control reinforcement learning algorithm to design entangling two-qubit gates for superconducting qubits; specifically, our agent constructs cross-resonance and CNOT gates without any prior information about the physical system. Using a simulated environment of fixed-frequency fixed-coupling transmon qubits, we demonstrate the capability to generate novel pulse sequences that outperform the standard cross-resonance gates in both fidelity and gate duration, while maintaining a comparable susceptibility to stochastic unitary noise. We further showcase an augmentation in training and input information that allows our agent to adapt its pulse design abilities to drifting hardware characteristics, importantly, with little to no additional optimization. Our results exhibit clearly the advantages of unbiased adaptive-feedback learning-based optimization methods for transmon gate design.},
  archive      = {J_MLST},
  author       = {Ho Nam Nguyen and Felix Motzoi and Mekena Metcalf and K Birgitta Whaley and Marin Bukov and Markus Schmitt},
  doi          = {10.1088/2632-2153/ad4f4d},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025066},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Reinforcement learning pulses for transmon qubit entangling gates},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Symbolic regression as a feature engineering method for
machine and deep learning regression tasks. <em>MLST</em>,
<em>5</em>(2), 025065. (<a
href="https://doi.org/10.1088/2632-2153/ad513a">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of machine and deep learning (DL) regression tasks, the role of effective feature engineering (FE) is pivotal in enhancing model performance. Traditional approaches of FE often rely on domain expertise to manually design features for machine learning (ML) models. In the context of DL models, the FE is embedded in the neural network&#39;s architecture, making it hard for interpretation. In this study, we propose to integrate symbolic regression (SR) as an FE process before a ML model to improve its performance. We show, through extensive experimentation on synthetic and 21 real-world datasets, that the incorporation of SR-derived features significantly enhances the predictive capabilities of both machine and DL regression models with 34%–86% root mean square error (RMSE) improvement in synthetic datasets and 4%–11.5% improvement in real-world datasets. In an additional realistic use case, we show the proposed method improves the ML performance in predicting superconducting critical temperatures based on Eliashberg theory by more than 20% in terms of RMSE. These results outline the potential of SR as an FE component in data-driven models, improving them in terms of performance and interpretability.},
  archive      = {J_MLST},
  author       = {Assaf Shmuel and Oren Glickman and Teddy Lazebnik},
  doi          = {10.1088/2632-2153/ad513a},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025065},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Symbolic regression as a feature engineering method for machine and deep learning regression tasks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A quantum inspired approach to learning dynamical laws from
data—block-sparsity and gauge-mediated weight sharing. <em>MLST</em>,
<em>5</em>(2), 025064. (<a
href="https://doi.org/10.1088/2632-2153/ad4f4e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed an increased interest in recovering dynamical laws of complex systems in a largely data-driven fashion under meaningful hypotheses. In this work, we propose a scalable and numerically robust method for this task, utilizing efficient block-sparse tensor train representations of dynamical laws, inspired by similar approaches in quantum many-body systems. Low-rank tensor train representations have been previously derived for dynamical laws of one-dimensional systems. We extend this result to efficient representations of systems with K -mode interactions and controlled approximations of systems with decaying interactions. We further argue that natural structure assumptions on dynamical laws, such as bounded polynomial degrees, can be exploited in the form of block-sparse support patterns of tensor-train cores. Additional structural similarities between interactions of certain modes can be accounted for by weight sharing within the ansatz. To make use of these structure assumptions, we propose a novel optimization algorithm, block-sparsity restricted alternating least squares with gauge-mediated weight sharing. The algorithm is inspired by similar notions in machine learning and achieves a significant improvement in performance over previous approaches. We demonstrate the performance of the method numerically on three one-dimensional systems—the Fermi–Pasta–Ulam–Tsingou system, rotating magnetic dipoles and point particles interacting via modified Lennard–Jones potentials, observing a highly accurate and noise-robust recovery.},
  archive      = {J_MLST},
  author       = {J Fuksa and M Götte and I Roth and J Eisert},
  doi          = {10.1088/2632-2153/ad4f4e},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025064},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A quantum inspired approach to learning dynamical laws from data—block-sparsity and gauge-mediated weight sharing},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Investigating the ability of PINNs to solve burgers’ PDE
near finite-time blowup. <em>MLST</em>, <em>5</em>(2), 025063. (<a
href="https://doi.org/10.1088/2632-2153/ad51cd">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physics Informed Neural Networks (PINNs) have been achieving ever newer feats of solving complicated Partial Differential Equations (PDEs) numerically while offering an attractive trade-off between accuracy and speed of inference. A particularly challenging aspect of PDEs is that there exist simple PDEs which can evolve into singular solutions in finite time starting from smooth initial conditions. In recent times some striking experiments have suggested that PINNs might be good at even detecting such finite-time blow-ups. In this work, we embark on a program to investigate this stability of PINNs from a rigorous theoretical viewpoint. Firstly, we derive error bounds for PINNs for Burgers&#39; PDE, in arbitrary dimensions, under conditions that allow for a finite-time blow-up. Our bounds give a theoretical justification for the functional regularization terms that have been reported to be useful for training PINNs near finite-time blow-up. Then we demonstrate via experiments that our bounds are significantly correlated to the \ell_2 -distance of the neurally found surrogate from the true blow-up solution, when computed on sequences of PDEs that are getting increasingly close to a blow-up.},
  archive      = {J_MLST},
  author       = {Dibyakanti Kumar and Anirbit Mukherjee},
  doi          = {10.1088/2632-2153/ad51cd},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025063},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Investigating the ability of PINNs to solve burgers’ PDE near finite-time blowup},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep artificial neural network-powered phase field model for
predicting damage characteristic in brittle composite under varying
configurations. <em>MLST</em>, <em>5</em>(2), 025062. (<a
href="https://doi.org/10.1088/2632-2153/ad52e8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces a novel artificial neural network (ANN)-powered phase field model, offering rapid and precise predictions of fracture propagation in brittle materials. To improve the capabilities of the ANN model, we incorporate a loop of conditions into its core to regulate the absolute percentage error for each observation point, that filters and consistently selects the most accurate outcome. This algorithm enables our model to better adapt to the highly sensitive validation data arising from varying configurations. The effectiveness of the approach is illustrated through three examples involving changes in the microgeometry and material properties of steel fiber-reinforced high-strength concrete structures. Indeed, the predicted outcomes from the improved ANN phase field model in terms of stress–strain relationship, and crack propagation path demonstrates an outperformance compared with that based on the extreme gradient boosting method, a leading regression machine learning technique for tabular data. Additionally, the introduced model exhibits a remarkable speed advantage, being 180 times faster than traditional phase field simulations, and provides results at nearly any fiber location, demonstrating superiority over the phase field model. This study marks a significant advancement in the application of artificial intelligence for accurately predicting crack propagation paths in composite materials, particularly in cases involving the relative positioning of the fiber and initial crack location.},
  archive      = {J_MLST},
  author       = {Hoang-Quan Nguyen and Ba-Anh Le and Bao-Viet Tran and Thai-Son Vu and Thi-Loan Bui},
  doi          = {10.1088/2632-2153/ad52e8},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025062},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deep artificial neural network-powered phase field model for predicting damage characteristic in brittle composite under varying configurations},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The twin peaks of learning neural networks. <em>MLST</em>,
<em>5</em>(2), 025061. (<a
href="https://doi.org/10.1088/2632-2153/ad524d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent works demonstrated the existence of a double-descent phenomenon for the generalization error of neural networks, where highly overparameterized models escape overfitting and achieve good test performance, at odds with the standard bias-variance trade-off described by statistical learning theory. In the present work, we explore a link between this phenomenon and the increase of complexity and sensitivity of the function represented by neural networks. In particular, we study the Boolean mean dimension (BMD), a metric developed in the context of Boolean function analysis. Focusing on a simple teacher-student setting for the random feature model, we derive a theoretical analysis based on the replica method that yields an interpretable expression for the BMD, in the high dimensional regime where the number of data points, the number of features, and the input size grow to infinity. We find that, as the degree of overparameterization of the network is increased, the BMD reaches an evident peak at the interpolation threshold, in correspondence with the generalization error peak, and then slowly approaches a low asymptotic value. The same phenomenology is then traced in numerical experiments with different model classes and training setups. Moreover, we find empirically that adversarially initialized models tend to show higher BMD values, and that models that are more robust to adversarial attacks exhibit a lower BMD.},
  archive      = {J_MLST},
  author       = {Elizaveta Demyanenko and Christoph Feinauer and Enrico M Malatesta and Luca Saglietti},
  doi          = {10.1088/2632-2153/ad524d},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025061},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {The twin peaks of learning neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning inspired models for hall effects in
non-collinear magnets. <em>MLST</em>, <em>5</em>(2), 025060. (<a
href="https://doi.org/10.1088/2632-2153/ad51ca">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The anomalous Hall effect has been front and center in solid state research and material science for over a century now, and the complex transport phenomena in nontrivial magnetic textures have gained an increasing amount of attention, both in theoretical and experimental studies. However, a clear path forward to capturing the influence of magnetization dynamics on anomalous Hall effect even in smallest frustrated magnets or spatially extended magnetic textures is still intensively sought after. In this work, we present an expansion of the anomalous Hall tensor into symmetrically invariant objects, encoding the magnetic configuration up to arbitrary power of spin. We show that these symmetric invariants can be utilized in conjunction with advanced regularization techniques in order to build models for the electric transport in magnetic textures which are, on one hand, complete with respect to the point group symmetry of the underlying lattice, and on the other hand, depend on a minimal number of order parameters only. Here, using a four-band tight-binding model on a honeycomb lattice, we demonstrate that the developed method can be used to address the importance and properties of higher-order contributions to transverse transport. The efficiency and breadth enabled by this method provides an ideal systematic approach to tackle the inherent complexity of response properties of noncollinear magnets, paving the way to the exploration of electric transport in intrinsically frustrated magnets as well as large-scale magnetic textures.},
  archive      = {J_MLST},
  author       = {Jonathan Kipp and Fabian R Lux and Thorben Pürling and Abigail Morrison and Stefan Blügel and Daniele Pinna and Yuriy Mokrousov},
  doi          = {10.1088/2632-2153/ad51ca},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025060},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning inspired models for hall effects in non-collinear magnets},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decoding characteristics of key physical properties in
silver nanoparticles by attaining centroids for cytotoxicity prediction
through data cleansing. <em>MLST</em>, <em>5</em>(2), 025059. (<a
href="https://doi.org/10.1088/2632-2153/ad51cb">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research underscores the profound impact of data cleansing, ensuring dataset integrity and providing a structured foundation for unraveling convoluted connections between diverse physical properties and cytotoxicity. As the scientific community delves deeper into this interplay, it becomes clear that precise data purification is a fundamental aspect of investigating parameters within datasets. The study presents the need for data filtration in the background of machine learning (ML) that has widened its horizon into the field of biological application through the amalgamation of predictive systems and algorithms that delve into the intricate characteristics of cytotoxicity of nanoparticles. The reliability and accuracy of models in the ML landscape hinge on the quality of input data, making data cleansing a critical component of the pre-processing pipeline. The main encounter faced here is the lengthy, broad and complex datasets that have to be toned down for further studies. Through a thorough data cleansing process, this study addresses the complexities arising from diverse sources, resulting in a refined dataset. The filtration process employs K-means clustering to derive centroids, revealing the correlation between the physical properties of nanoparticles, viz, concentration, zeta potential, hydrodynamic diameter, morphology, and absorbance wavelength, and cytotoxicity outcomes measured in terms of cell viability. The cell lines considered for determining the centroid values that predicts the cytotoxicity of silver nanoparticles are human and animal cell lines which were categorized as normal and carcinoma type. The objective of the study is to simplify the high-dimensional data for accurate analysis of the parameters that affect the cytotoxicity of silver NPs through centroids.},
  archive      = {J_MLST},
  author       = {Anjana S Desai and Anindita Bandopadhyaya and Aparna Ashok and Maneesha and Neeru Bhagat},
  doi          = {10.1088/2632-2153/ad51cb},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025059},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Decoding characteristics of key physical properties in silver nanoparticles by attaining centroids for cytotoxicity prediction through data cleansing},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reducing training data needs with minimal multilevel machine
learning (M3L). <em>MLST</em>, <em>5</em>(2), 025058. (<a
href="https://doi.org/10.1088/2632-2153/ad4ae5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many machine learning applications in science, data acquisition, not training, is the bottleneck even when avoiding experiments and relying on computation and simulation. Correspondingly, and in order to reduce cost and carbon footprint, training data efficiency is key. We introduce minimal multilevel machine learning (M3L) which optimizes training data set sizes using a loss function at multiple levels of reference data in order to minimize a combination of prediction error with overall training data acquisition costs (as measured by computational wall-times). Numerical evidence has been obtained for calculated atomization energies and electron affinities of thousands of organic molecules at various levels of theory including HF, MP2, DLPNO-CCSD(T), DFHFCABS, PNOMP2F12, and PNOCCSD(T)F12, and treating them with basis sets TZ, cc-pVTZ, and AVTZ-F12. Our M3L benchmarks for reaching chemical accuracy in distinct chemical compound sub-spaces indicate substantial computational cost reductions by factors of ∼1.01, 1.1, 3.8, 13.8, and 25.8 when compared to heuristic sub-optimal multilevel machine learning (M2L) for the data sets QM7b, QM9 ^\mathrm{LCCSD(T)} , Electrolyte Genome Project, QM9 ^\mathrm{CCSD(T)}_\mathrm{AE} , and QM9 ^\mathrm{CCSD(T)}_\mathrm{EA} , respectively. Furthermore, we use M2L to investigate the performance for 76 density functionals when used within multilevel learning and building on the following levels drawn from the hierarchy of Jacobs Ladder: LDA, GGA, mGGA, and hybrid functionals. Within M2L and the molecules considered, mGGAs do not provide any noticeable advantage over GGAs. Among the functionals considered and in combination with LDA, the three on average top performing GGA and Hybrid levels for atomization energies on QM9 using M3L correspond respectively to PW91, KT2, B97D, and τ -HCTH, B3LYP \ast (VWN5), and TPSSH.},
  archive      = {J_MLST},
  author       = {Stefan Heinen and Danish Khan and Guido Falk von Rudorff and Konstantin Karandashev and Daniel Jose Arismendi Arrieta and Alastair J A Price and Surajit Nandi and Arghya Bhowmik and Kersti Hermansson and O Anatole von Lilienfeld},
  doi          = {10.1088/2632-2153/ad4ae5},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025058},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Reducing training data needs with minimal multilevel machine learning (M3L)},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incorporating background knowledge in symbolic regression
using a computer algebra system. <em>MLST</em>, <em>5</em>(2), 025057.
(<a href="https://doi.org/10.1088/2632-2153/ad4a1e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symbolic regression (SR) can generate interpretable, concise expressions that fit a given dataset, allowing for more human understanding of the structure than black-box approaches. The addition of background knowledge (in the form of symbolic mathematical constraints) allows for the generation of expressions that are meaningful with respect to theory while also being consistent with data. We specifically examine the addition of constraints to traditional genetic algorithm (GA) based SR (PySR) as well as a Markov-chain Monte Carlo (MCMC) based Bayesian SR architecture (Bayesian Machine Scientist), and apply these to rediscovering adsorption equations from experimental, historical datasets. We find that, while hard constraints prevent GA and MCMC SR from searching, soft constraints can lead to improved performance both in terms of search effectiveness and model meaningfulness, with computational costs increasing by about an order of magnitude. If the constraints do not correlate well with the dataset or expected models, they can hinder the search of expressions. We find incorporating these constraints in Bayesian SR (as the Bayesian prior) is better than by modifying the fitness function in the GA.},
  archive      = {J_MLST},
  author       = {Charles Fox and Neil D Tran and F Nikki Nacion and Samiha Sharlin and Tyler R Josephson},
  doi          = {10.1088/2632-2153/ad4a1e},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025057},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Incorporating background knowledge in symbolic regression using a computer algebra system},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physics-inspired spatiotemporal-graph AI ensemble for the
detection of higher order wave mode signals of spinning binary black
hole mergers. <em>MLST</em>, <em>5</em>(2), 025056. (<a
href="https://doi.org/10.1088/2632-2153/ad4c37">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new class of AI models for the detection of quasi-circular, spinning, non-precessing binary black hole mergers whose waveforms include the higher order gravitational wave modes (\ell, |m|) = \{(2, 2), (2, 1), (3, 3), (3, 2), (4, 4)\} , and mode mixing effects in the \ell = 3, |m| = 2 harmonics. These AI models combine hybrid dilated convolution neural networks to accurately model both short- and long-range temporal sequential information of gravitational waves; and graph neural networks to capture spatial correlations among gravitational wave observatories to consistently describe and identify the presence of a signal in a three detector network encompassing the Advanced LIGO and Virgo detectors. We first trained these spatiotemporal-graph AI models using synthetic noise, using 1.2 million modeled waveforms to densely sample this signal manifold, within 1.7 h using 256 NVIDIA A100 GPUs in the Polaris supercomputer at the Argonne Leadership Computing Facility. This distributed training approach exhibited optimal classification performance, and strong scaling up to 512 NVIDIA A100 GPUs. With these AI ensembles we processed data from a three detector network, and found that an ensemble of 4 AI models achieves state-of-the-art performance for signal detection, and reports two misclassifications for every decade of searched data. We distributed AI inference over 128 GPUs in the Polaris supercomputer and 128 nodes in the Theta supercomputer, and completed the processing of a decade of gravitational wave data from a three detector network within 3.5 h. Finally, we fine-tuned these AI ensembles to process the entire month of February 2020, which is part of the O3b LIGO/Virgo observation run, and found 6 gravitational waves, concurrently identified in Advanced LIGO and Advanced Virgo data, and zero false positives. This analysis was completed in one hour using one NVIDIA A100 GPU.},
  archive      = {J_MLST},
  author       = {Minyang Tian and E A Huerta and Huihuo Zheng and Prayush Kumar},
  doi          = {10.1088/2632-2153/ad4c37},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025056},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Physics-inspired spatiotemporal-graph AI ensemble for the detection of higher order wave mode signals of spinning binary black hole mergers},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine-learning strategies for the accurate and efficient
analysis of x-ray spectroscopy. <em>MLST</em>, <em>5</em>(2), 021001.
(<a href="https://doi.org/10.1088/2632-2153/ad5074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational spectroscopy has emerged as a critical tool for researchers looking to achieve both qualitative and quantitative interpretations of experimental spectra. Over the past decade, increased interactions between experiment and theory have created a positive feedback loop that has stimulated developments in both domains. In particular, the increased accuracy of calculations has led to them becoming an indispensable tool for the analysis of spectroscopies across the electromagnetic spectrum. This progress is especially well demonstrated for short-wavelength techniques, e.g. core-hole (x-ray) spectroscopies, whose prevalence has increased following the advent of modern x-ray facilities including third-generation synchrotrons and x-ray free-electron lasers. While calculations based on well-established wavefunction or density-functional methods continue to dominate the greater part of spectral analyses in the literature, emerging developments in machine-learning algorithms are beginning to open up new opportunities to complement these traditional techniques with fast, accurate, and affordable &#39; black-box &#39; approaches. This Topical Review recounts recent progress in data-driven/machine-learning approaches for computational x-ray spectroscopy. We discuss the achievements and limitations of the presently-available approaches and review the potential that these techniques have to expand the scope and reach of computational and experimental x-ray spectroscopic studies.},
  archive      = {J_MLST},
  author       = {Thomas Penfold and Luke Watson and Clelia Middleton and Tudur David and Sneha Verma and Thomas Pope and Julia Kaczmarek and Conor Rankine},
  doi          = {10.1088/2632-2153/ad5074},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {021001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine-learning strategies for the accurate and efficient analysis of x-ray spectroscopy},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synergizing human expertise and AI efficiency with language
model for microscopy operation and automated experiment design *.
<em>MLST</em>, <em>5</em>(2), 02LT01. (<a
href="https://doi.org/10.1088/2632-2153/ad52e9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of large language models (LLMs), in both the open source and proprietary domains, attention is turning to how to exploit such artificial intelligence (AI) systems in assisting complex scientific tasks, such as material synthesis, characterization, analysis and discovery. Here, we explore the utility of LLMs, particularly ChatGPT4, in combination with application program interfaces (APIs) in tasks of experimental design, programming workflows, and data analysis in scanning probe microscopy, using both in-house developed APIs and APIs given by a commercial vendor for instrument control. We find that the LLM can be especially useful in converting ideations of experimental workflows to executable code on microscope APIs. Beyond code generation, we find that the GPT4 is capable of analyzing microscopy images in a generic sense. At the same time, we find that GPT4 suffers from an inability to extend beyond basic analyses for more in-depth technical experimental design. We argue that an LLM specifically fine-tuned for individual scientific domains can potentially be a better language interface for converting scientific ideations from human experts to executable workflows. Such a synergy between human expertise and LLM efficiency in experimentation can open new doors for accelerating scientific research, enabling effective experimental protocols sharing in the scientific community.},
  archive      = {J_MLST},
  author       = {Yongtao Liu and Marti Checa and Rama K Vasudevan},
  doi          = {10.1088/2632-2153/ad52e9},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {02LT01},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Synergizing human expertise and AI efficiency with language model for microscopy operation and automated experiment design *},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beyond dynamics: Learning to discover conservation
principles. <em>MLST</em>, <em>5</em>(2), 025055. (<a
href="https://doi.org/10.1088/2632-2153/ad4a20">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discovery of conservation principles is crucial for understanding the fundamental behavior of both classical and quantum physical systems across numerous domains. This paper introduces an innovative method that merges representation learning and topological analysis to explore the topology of conservation law spaces. Notably, the robustness of our approach to noise makes it suitable for complex experimental setups and its aptitude extends to the analysis of quantum systems, as successfully demonstrated in our paper. We exemplify our method&#39;s potential to unearth previously unknown conservation principles and endorse interdisciplinary research through a variety of physical simulations. In conclusion, this work emphasizes the significance of data-driven techniques in deepening our comprehension of the principles governing classical and quantum physical systems.},
  archive      = {J_MLST},
  author       = {Antonii Belyshev and Alexander Kovrigin and Andrey Ustyuzhanin},
  doi          = {10.1088/2632-2153/ad4a20},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025055},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Beyond dynamics: Learning to discover conservation principles},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transformer-powered surrogates close the ICF
simulation-experiment gap with extremely limited data. <em>MLST</em>,
<em>5</em>(2), 025054. (<a
href="https://doi.org/10.1088/2632-2153/ad4e03">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in machine learning, specifically transformer architecture, have led to significant advancements in commercial domains. These powerful models have demonstrated superior capability to learn complex relationships and often generalize better to new data and problems. This paper presents a novel transformer-powered approach for enhancing prediction accuracy in multi-modal output scenarios, where sparse experimental data is supplemented with simulation data. The proposed approach integrates transformer-based architecture with a novel graph-based hyper-parameter optimization technique. The resulting system not only effectively reduces simulation bias, but also achieves superior prediction accuracy compared to the prior method. We demonstrate the efficacy of our approach on inertial confinement fusion experiments, where only 10 shots of real-world data are available, as well as synthetic versions of these experiments.},
  archive      = {J_MLST},
  author       = {Matthew L Olson and Shusen Liu and Jayaraman J Thiagarajan and Bogdan Kustowski and Weng-Keen Wong and Rushil Anirudh},
  doi          = {10.1088/2632-2153/ad4e03},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025054},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Transformer-powered surrogates close the ICF simulation-experiment gap with extremely limited data},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autoencoders for discovering manifold dimension and
coordinates in data from complex dynamical systems. <em>MLST</em>,
<em>5</em>(2), 025053. (<a
href="https://doi.org/10.1088/2632-2153/ad4ba5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While many phenomena in physics and engineering are formally high-dimensional, their long-time dynamics often live on a lower-dimensional manifold. The present work introduces an autoencoder framework that combines implicit regularization with internal linear layers and L 2 regularization (weight decay) to automatically estimate the underlying dimensionality of a data set, produce an orthogonal manifold coordinate system, and provide the mapping functions between the ambient space and manifold space, allowing for out-of-sample projections. We validate our framework&#39;s ability to estimate the manifold dimension for a series of datasets from dynamical systems of varying complexities and compare to other state-of-the-art estimators. We analyze the training dynamics of the network to glean insight into the mechanism of low-rank learning and find that collectively each of the implicit regularizing layers compound the low-rank representation and even self-correct during training. Analysis of gradient descent dynamics for this architecture in the linear case reveals the role of the internal linear layers in leading to faster decay of a &#39;collective weight variable&#39; incorporating all layers, and the role of weight decay in breaking degeneracies and thus driving convergence along directions in which no decay would occur in its absence. We show that this framework can be naturally extended for applications of state-space modeling and forecasting by generating a data-driven dynamic model of a spatiotemporally chaotic partial differential equation using only the manifold coordinates. Finally, we demonstrate that our framework is robust to hyperparameter choices.},
  archive      = {J_MLST},
  author       = {Kevin Zeng and Carlos E Pérez De Jesús and Andrew J Fox and Michael D Graham},
  doi          = {10.1088/2632-2153/ad4ba5},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025053},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Autoencoders for discovering manifold dimension and coordinates in data from complex dynamical systems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards XAI agnostic explainability to assess differential
diagnosis for meningitis diseases. <em>MLST</em>, <em>5</em>(2), 025052.
(<a href="https://doi.org/10.1088/2632-2153/ad4a1f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meningitis, characterized by meninges and cerebrospinal fluid inflammation, poses diagnostic challenges due to diverse clinical manifestations. This work introduces an explainable AI automatic medical decision methodology that determines critical features and their relevant values for the differential diagnosis of various meningitis cases. We proceed with knowledge acquisition to define the rules for this research. Currently, we have established the etiological diagnosis of Meningococcaemia, Meningococcal Meningitis, Tuberculous Meningitis, Aseptic Meningitis, Haemophilus influenzae Meningitis, and Pneumococcal Meningitis. The data preprocessing was conducted after collecting data from samples with meningitis diseases at Setif Hospital in Algeria. Tree-based ensemble methods were then applied to assess the model&#39;s performance. Finally, we implement an XAI agnostic explainability approach based on the SHapley Additive exPlanations technique to attribute each feature&#39;s contribution to the model&#39;s output. Experiments were conducted on the collected dataset and the SINAN database, obtained from the Brazilian Government&#39;s Health Information System on Notifiable Diseases, which comprises 6729 patients aged over 18 years. The Extreme Gradient Boosting model was chosen for its superior performance metrics (Accuracy: 0.90, AUROC: 0.94, and F1-score: 0.98). Setif&#39;s hospital data revealed notable performance metrics (Accuracy: 0.7143, F1-Score: 0.7857). This study&#39;s findings showcase each feature&#39;s contribution to the model&#39;s predictions and diagnosis. It also reveals critical biomarker ranges associated with distinct types of Meningitis. Significant diagnostic effect was found for Meningococcal Meningitis with elevated neutrophil levels ( \gt 40%) and balanced lymphocyte levels (40%–60%). Tuberculous Meningitis demonstrated low neutrophil levels ( \lt 60%) and elevated lymphocyte levels ( \gt 60%). H. influenzae meningitis exhibited a predominance of neutrophils ( \gt 80%), while Aseptic meningitis showed lower neutrophil levels ( \lt 40%) and lymphocyte levels within the range of 50%–60%. The majority of the AI automatic medical decision results are twinned with validation by our team of infectious disease experts, confirming the alignment of algorithmic diagnoses with clinical practices.},
  archive      = {J_MLST},
  author       = {Aya Messai and Ahlem Drif and Amel Ouyahia and Meriem Guechi and Mounira Rais and Lars Kaderali and Hocine Cherifi},
  doi          = {10.1088/2632-2153/ad4a1f},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025052},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Towards XAI agnostic explainability to assess differential diagnosis for meningitis diseases},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global system errors to simultaneously improve the
identification of subsystems with mixed data gaussian process
regression. <em>MLST</em>, <em>5</em>(2), 025051. (<a
href="https://doi.org/10.1088/2632-2153/ad4e05">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the use of Gaussian process regression for system identification in control engineering. It introduces two novel approaches that utilize the data from a measured global system error. The paper demonstrates these approaches by identifying a simulated system with three subsystems, a one degree of freedom mass with two antagonist muscles. The first approach uses this whole-system error data alone, achieving accuracy on the same order of magnitude as subsystem-specific data ( 9.28\pm0.87 \text{N } \text{vs. } 6.96\pm0.32 \text{N} of total model errors). This is significant, as it shows that the same data set can be used to identify unique subsystems, as opposed to requiring a set of data descriptive of only a single subsystem. The second approach demonstrated in this paper mixes traditional subsystem-specific data with the whole system error data, achieving up to 98.71% model improvement.},
  archive      = {J_MLST},
  author       = {Cameron J LaMack and Eric M Schearer},
  doi          = {10.1088/2632-2153/ad4e05},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025051},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Global system errors to simultaneously improve the identification of subsystems with mixed data gaussian process regression},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing particle string detection in electrorheological
plasmas using asymmetrical kernel convolutional networks. <em>MLST</em>,
<em>5</em>(2), 025050. (<a
href="https://doi.org/10.1088/2632-2153/ad4d3e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under different plasma conditions and electric fields in a complex plasma the plasma particles organize themselves in a string-like or chain-like manner. A phase transition from string-like to an isotropic particle distribution is observed at different electrical conditions. The streaming of charged ions around plasma particles with the surrounding electric field gives the plasma its electrorheological properties. The visibility of individual particles in a complex plasma opens up the opportunity to examine properties and phase transitions of such electrorheological fluids in detail. Because of the limited one-dimensional symmetry, determining the configuration of a particle and recognizing strings in particle distributions is not always straightforward. Several approaches have already been used to analyse particle clouds while either considering each particle locally or considering the particle cloud as a whole without providing information about single particle configurations. This paper presents a new machine learning approach that takes advantage of particle distributions over the entire particle cloud and detects all string-like particles at once, using a convolutional neural network in form of an encoder-decoder network with asymmetric kernel convolutions. This not only enhances the result quality but also accelerates the evaluation process, possibly enabling real-time analyses on electrorheological phase transitions, while achieving an accuracy of over 95% on manually labelled data.},
  archive      = {J_MLST},
  author       = {M Klein and N Dormagen and C Dietz and M H Thoma and M Schwarz},
  doi          = {10.1088/2632-2153/ad4d3e},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025050},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Enhancing particle string detection in electrorheological plasmas using asymmetrical kernel convolutional networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised learning of quantum many-body scars using
intrinsic dimension. <em>MLST</em>, <em>5</em>(2), 025049. (<a
href="https://doi.org/10.1088/2632-2153/ad4d3f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum many-body scarred systems contain both thermal and non-thermal scar eigenstates in their spectra. When these systems are quenched from special initial states which share high overlap with scar eigenstates, the system undergoes dynamics with atypically slow relaxation and periodic revival. This scarring phenomenon poses a potential avenue for circumventing decoherence in various quantum engineering applications. Given access to an unknown scar system, current approaches for identification of special states leading to non-thermal dynamics rely on costly measures such as entanglement entropy. In this work, we show how two dimensionality reduction techniques, multidimensional scaling and intrinsic dimension estimation, can be used to learn structural properties of dynamics in the PXP model and distinguish between thermal and scar initial states. The latter method is shown to be robust against limited sample sizes and experimental measurement errors.},
  archive      = {J_MLST},
  author       = {Harvey Cao and Dimitris G Angelakis and Daniel Leykam},
  doi          = {10.1088/2632-2153/ad4d3f},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025049},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Unsupervised learning of quantum many-body scars using intrinsic dimension},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning the dynamics of a one-dimensional plasma model with
graph neural networks. <em>MLST</em>, <em>5</em>(2), 025048. (<a
href="https://doi.org/10.1088/2632-2153/ad4ba6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore the possibility of fully replacing a plasma physics kinetic simulator with a graph neural network-based simulator. We focus on this class of surrogate models given the similarity between their message-passing update mechanism and the traditional physics solver update, and the possibility of enforcing known physical priors into the graph construction and update. We show that our model learns the kinetic plasma dynamics of the one-dimensional plasma model, a predecessor of contemporary kinetic plasma simulation codes, and recovers a wide range of well-known kinetic plasma processes, including plasma thermalization, electrostatic fluctuations about thermal equilibrium, and the drag on a fast sheet and Landau damping. We compare the performance against the original plasma model in terms of run-time, conservation laws, and temporal evolution of key physical quantities. The limitations of the model are presented and possible directions for higher-dimensional surrogate models for kinetic plasmas are discussed.},
  archive      = {J_MLST},
  author       = {Diogo D Carvalho and Diogo R Ferreira and Luís O Silva},
  doi          = {10.1088/2632-2153/ad4ba6},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025048},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Learning the dynamics of a one-dimensional plasma model with graph neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised segmentation of abdominal organs and liver
tumor: Uncertainty rectified curriculum labeling meets x-fuse.
<em>MLST</em>, <em>5</em>(2), 025047. (<a
href="https://doi.org/10.1088/2632-2153/ad4c38">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precise liver tumors and associated organ segmentation hold immense value for surgical and radiological intervention, enabling anatomical localization for pre-operative planning and intra-operative guidance. Modern deep learning models for medical image segmentation have evolved from convolution neural networks to transformer architectures, significantly boosting global context understanding. However, accurate delineation especially of hepatic lesions remains an enduring challenge due to models&#39; predominant focus solely on spatial feature extraction failing to adequately characterize complex medical anatomies. Moreover, the relative paucity of expertly annotated medical imaging data restricts model exposure to diverse pathological presentations. In this paper, we present a three-phrased cascaded segmentation framework featuring an X-Fuse model that synergistically integrates spatial and frequency domain&#39;s complementary information in dual encoders to enrich latent feature representation. To enhance model generalizability, building upon X-Fuse topology and taking advantage of additional unlabeled pathological data, our proposed integration of curriculum pseudo-labeling with Jensen–Shannon variance-based uncertainty rectification promotes optimized pseudo supervision in the context of semi-supervised learning. We further introduce a tumor-focus augmentation technique including training-free copy-paste and knowledge-based synthesis that show efficacy in simplicity, contributing to the substantial elevation of model adaptability on diverse lesional morphologies. Extensive experiments and modular evaluations on a holdout test set demonstrate that our methods significantly outperform existing state-of-the-art segmentation models in both supervised and semi-supervised settings, as measured by the Dice similarity coefficient, achieving superior delineation of bones (95.42%), liver (96.26%), and liver tumors (89.53%) with 16.41% increase comparing to V-Net on supervised-only and augmented-absent scenario. Our method marks a significant step toward the realization of more reliable and robust AI-assisted diagnostic tools for liver tumor intervention. We have made the codes publicly available [ https://github.com/lyupengju/X-Fuse ].},
  archive      = {J_MLST},
  author       = {Pengju Lyu and Wenjian Liu and Tingyi Lin and Jie Zhang and Yao Liu and Cheng Wang and Jianjun Zhu},
  doi          = {10.1088/2632-2153/ad4c38},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025047},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Semi-supervised segmentation of abdominal organs and liver tumor: Uncertainty rectified curriculum labeling meets X-fuse},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interpolation of environmental data using deep learning and
model inference. <em>MLST</em>, <em>5</em>(2), 025046. (<a
href="https://doi.org/10.1088/2632-2153/ad4b94">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The temporal resolution of environmental data sets plays a major role in the granularity of the information that can be derived from the data. In most cases, it is required that different data sets have a common temporal resolution to enable their consistent evaluations and applications in making informed decisions. This study leverages deep learning with long short-term memory (LSTM) neural networks and model inference to enhance the temporal resolution of climate datasets, specifically temperature, and precipitation, from daily to sub-daily scales. We trained our model to learn the relationship between daily and sub-daily data, subsequently applying this knowledge to increase the resolution of a separate dataset with a coarser (daily) temporal resolution. Our findings reveal a high degree of accuracy for temperature predictions, evidenced by a correlation of 0.99 and a mean absolute error of 0.21 °C, between the actual and predicted sub-daily values. In contrast, the approach was less effective for precipitation, achieving an explained variance of only 37%, compared to 98% for temperature. Further, besides the sub-daily interpolation of the climate data sets, we adapted our approach to increase the resolution of the Normalized difference vegetation index of Landsat (from 16 d to 5 d interval) using the LSTM model pre-trained from the Sentinel 2 Normalized difference vegetation index—that exists at a relatively higher temporal resolution. The explained variance between the predicted Landsat and Sentinel 2 data is 70% with a mean absolute error of 0.03. These results suggest that our method is particularly suitable for environmental datasets with less pronounced short-term variability, offering a promising tool for improving the resolution and utility of the data.},
  archive      = {J_MLST},
  author       = {Chibuike Chiedozie Ibebuchi and Itohan-Osa Abu},
  doi          = {10.1088/2632-2153/ad4b94},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025046},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Interpolation of environmental data using deep learning and model inference},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid quantum physics-informed neural networks for
simulating computational fluid dynamics in complex shapes.
<em>MLST</em>, <em>5</em>(2), 025045. (<a
href="https://doi.org/10.1088/2632-2153/ad43b2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding the distribution of the velocities and pressures of a fluid by solving the Navier–Stokes equations is a principal task in the chemical, energy, and pharmaceutical industries, as well as in mechanical engineering and in design of pipeline systems. With existing solvers, such as OpenFOAM and Ansys, simulations of fluid dynamics in intricate geometries are computationally expensive and require re-simulation whenever the geometric parameters or the initial and boundary conditions are altered. Physics-informed neural networks (PINNs) are a promising tool for simulating fluid flows in complex geometries, as they can adapt to changes in the geometry and mesh definitions, allowing for generalization across fluid parameters and transfer learning across different shapes. We present a hybrid quantum PINN (HQPINN) that simulates laminar fluid flow in 3D Y -shaped mixers. Our approach combines the expressive power of a quantum model with the flexibility of a PINN, resulting in a 21% higher accuracy compared to a purely classical neural network. Our findings highlight the potential of machine learning approaches, and in particular HQPINN, for complex shape optimization tasks in computational fluid dynamics. By improving the accuracy of fluid simulations in complex geometries, our research using hybrid quantum models contributes to the development of more efficient and reliable fluid dynamics solvers.},
  archive      = {J_MLST},
  author       = {Alexandr Sedykh and Maninadh Podapaka and Asel Sagingalieva and Karan Pinto and Markus Pflitsch and Alexey Melnikov},
  doi          = {10.1088/2632-2153/ad43b2},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025045},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Hybrid quantum physics-informed neural networks for simulating computational fluid dynamics in complex shapes},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unifying o(3) equivariant neural networks design with
tensor-network formalism. <em>MLST</em>, <em>5</em>(2), 025044. (<a
href="https://doi.org/10.1088/2632-2153/ad4a04">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many learning tasks, including learning potential energy surfaces from ab initio calculations, involve global spatial symmetries and permutational symmetry between atoms or general particles. Equivariant graph neural networks are a standard approach to such problems, with one of the most successful methods employing tensor products between various tensors that transform under the spatial group. However, as the number of different tensors and the complexity of relationships between them increase, maintaining parsimony and equivariance becomes increasingly challenging. In this paper, we propose using fusion diagrams, a technique widely employed in simulating SU(2)-symmetric quantum many-body problems, to design new spatial equivariant components for neural networks. This results in a diagrammatic approach to constructing novel neural network architectures. When applied to particles within a given local neighborhood, the resulting components, which we term &#39;fusion blocks,&#39; serve as universal approximators of any continuous equivariant function defined on the neighborhood. We incorporate a fusion block into pre-existing equivariant architectures (Cormorant and MACE), leading to improved performance with fewer parameters on a range of challenging chemical problems. Furthermore, we apply group-equivariant neural networks to study non-adiabatic molecular dynamics of stilbene cis-trans isomerization. Our approach, which combines tensor networks with equivariant neural networks, suggests a potentially fruitful direction for designing more expressive equivariant neural networks.},
  archive      = {J_MLST},
  author       = {Zimu Li and Zihan Pengmei and Han Zheng and Erik Thiede and Junyu Liu and Risi Kondor},
  doi          = {10.1088/2632-2153/ad4a04},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025044},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Unifying o(3) equivariant neural networks design with tensor-network formalism},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature selection for high-dimensional neural network
potentials with the adaptive group lasso. <em>MLST</em>, <em>5</em>(2),
025043. (<a href="https://doi.org/10.1088/2632-2153/ad450e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural network potentials are a powerful tool for atomistic simulations, allowing to accurately reproduce ab initio potential energy surfaces with computational performance approaching classical force fields. A central component of such potentials is the transformation of atomic positions into a set of atomic features in a most efficient and informative way. In this work, a feature selection method is introduced for high dimensional neural network potentials, based on the adaptive group lasso (AGL) approach. It is shown that the use of an embedded method, taking into account the interplay between features and their action in the estimator, is necessary to optimize the number of features. The method&#39;s efficiency is tested on three different monoatomic systems, including Lennard–Jones as a simple test case, Aluminium as a system characterized by predominantly radial interactions, and Boron as representative of a system with strongly directional components in the interactions. The AGL is compared with unsupervised filter methods and found to perform consistently better in reducing the number of features needed to reproduce the reference simulation data at a similar level of accuracy as the starting feature set. In particular, our results show the importance of taking into account model predictions in feature selection for interatomic potentials.},
  archive      = {J_MLST},
  author       = {Johannes Sandberg and Thomas Voigtmann and Emilie Devijver and Noel Jakse},
  doi          = {10.1088/2632-2153/ad450e},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025043},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Feature selection for high-dimensional neural network potentials with the adaptive group lasso},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multifidelity approach to continual learning for physical
systems. <em>MLST</em>, <em>5</em>(2), 025042. (<a
href="https://doi.org/10.1088/2632-2153/ad45b2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel continual learning method based on multifidelity deep neural networks. This method learns the correlation between the output of previously trained models and the desired output of the model on the current training dataset, limiting catastrophic forgetting. On its own the multifidelity continual learning method shows robust results that limit forgetting across several datasets. Additionally, we show that the multifidelity method can be combined with existing continual learning methods, including replay and memory aware synapses, to further limit catastrophic forgetting. The proposed continual learning method is especially suited for physical problems where the data satisfy the same physical laws on each domain, or for physics-informed neural networks, because in these cases we expect there to be a strong correlation between the output of the previous model and the model on the current training domain.},
  archive      = {J_MLST},
  author       = {Amanda Howard and Yucheng Fu and Panos Stinis},
  doi          = {10.1088/2632-2153/ad45b2},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025042},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A multifidelity approach to continual learning for physical systems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploiting data diversity in multi-domain federated
learning. <em>MLST</em>, <em>5</em>(2), 025041. (<a
href="https://doi.org/10.1088/2632-2153/ad4768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is an evolving machine learning technique that allows collaborative model training without sharing the original data among participants. In real-world scenarios, data residing at multiple clients are often heterogeneous in terms of different resolutions, magnifications, scanners, or imaging protocols, and thus challenging for global FL model convergence in collaborative training. Most of the existing FL methods consider data heterogeneity within one domain by assuming same data variation in each client site. In this paper, we consider data heterogeneity in FL with different domains of heterogeneous data by raising the problems of domain-shift, class-imbalance, and missing data. We propose a method, multi-domain FL as a solution to heterogeneous training data from multiple domains by training robust vision transformer model. We use two loss functions, one for correctly predicting class labels and other for encouraging similarity and dissimilarity over latent features, to optimize the global FL model. We perform various experiments using different convolution-based networks and non-convolutional Transformer architectures on multi-domain datasets. We evaluate the proposed approach on benchmark datasets and compare with the existing FL methods. Our results show the superiority of the proposed approach which performs better in term of robust FL global model than the exiting methods.},
  archive      = {J_MLST},
  author       = {Hussain Ahmad Madni and Rao Muhammad Umer and Gian Luca Foresti},
  doi          = {10.1088/2632-2153/ad4768},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025041},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Exploiting data diversity in multi-domain federated learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive machine learning-based investigation for the
index-value prediction of 2G HTS coated conductor tapes. <em>MLST</em>,
<em>5</em>(2), 025040. (<a
href="https://doi.org/10.1088/2632-2153/ad45b1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Index-value, or so-called n- value prediction is of paramount importance for understanding the superconductors&#39; behaviour specially when modeling of superconductors is needed. This parameter is dependent on several physical quantities including temperature, the magnetic field&#39;s density and orientation, and affects the behaviour of high-temperature superconducting devices made out of coated conductors in terms of losses and quench propagation. In this paper, a comprehensive analysis of many machine learning (ML) methods for estimating the n- value has been carried out. The results demonstrated that cascade forward neural network (CFNN) excels in this scope. Despite needing considerably higher training time when compared to the other attempted models, it performs at the highest accuracy, with 0.48 root mean squared error (RMSE) and 99.72% Pearson coefficient for goodness of fit ( R -squared). In contrast, the rigid regression method had the worst predictions with 4.92 RMSE and 37.29% R -squared. Also, random forest, boosting methods, and simple feed forward neural network can be considered as a middle accuracy model with faster training time than CFNN. The findings of this study not only advance modeling of superconductors but also pave the way for applications and further research on ML plug-and-play codes for superconducting studies including modeling of superconducting devices.},
  archive      = {J_MLST},
  author       = {Shahin Alipour Bonab and Giacomo Russo and Antonio Morandi and Mohammad Yazdani-Asrami},
  doi          = {10.1088/2632-2153/ad45b1},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025040},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A comprehensive machine learning-based investigation for the index-value prediction of 2G HTS coated conductor tapes},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning a general model of single phase flow in complex 3D
porous media. <em>MLST</em>, <em>5</em>(2), 025039. (<a
href="https://doi.org/10.1088/2632-2153/ad45af">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling effective transport properties of 3D porous media, such as permeability, at multiple scales is challenging as a result of the combined complexity of the pore structures and fluid physics—in particular, confinement effects which vary across the nanoscale to the microscale. While numerical simulation is possible, the computational cost is prohibitive for realistic domains, which are large and complex. Although machine learning (ML) models have been proposed to circumvent simulation, none so far has simultaneously accounted for heterogeneous 3D structures, fluid confinement effects, and multiple simulation resolutions. By utilizing numerous computer science techniques to improve the scalability of training, we have for the first time developed a general flow model that accounts for the pore-structure and corresponding physical phenomena at scales from Angstrom to the micrometer. Using synthetic computational domains for training, our ML model exhibits strong performance ( R 2 = 0.9) when tested on extremely diverse real domains at multiple scales.},
  archive      = {J_MLST},
  author       = {Javier E Santos and Agnese Marcato and Qinjun Kang and Mohamed Mehana and Daniel O’Malley and Hari Viswanathan and Nicholas Lubbers},
  doi          = {10.1088/2632-2153/ad45af},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025039},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Learning a general model of single phase flow in complex 3D porous media},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Closed-loop koopman operator approximation. <em>MLST</em>,
<em>5</em>(2), 025038. (<a
href="https://doi.org/10.1088/2632-2153/ad45b0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a method to identify a Koopman model of a feedback-controlled system given a known controller. The Koopman operator allows a nonlinear system to be rewritten as an infinite-dimensional linear system by viewing it in terms of an infinite set of lifting functions. A finite-dimensional approximation of the Koopman operator can be identified from data by choosing a finite subset of lifting functions and solving a regression problem in the lifted space. Existing methods are designed to identify open-loop systems. However, it is impractical or impossible to run experiments on some systems, such as unstable systems, in an open-loop fashion. The proposed method leverages the linearity of the Koopman operator, along with knowledge of the controller and the structure of the closed-loop (CL) system, to simultaneously identify the CL and plant systems. The advantages of the proposed CL Koopman operator approximation method are demonstrated in simulation using a Duffing oscillator and experimentally using a rotary inverted pendulum system. An open-source software implementation of the proposed method is publicly available, along with the experimental dataset generated for this paper.},
  archive      = {J_MLST},
  author       = {Steven Dahdah and James Richard Forbes},
  doi          = {10.1088/2632-2153/ad45b0},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025038},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Closed-loop koopman operator approximation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning for efficient grazing-exit x-ray absorption
near edge structure spectroscopy analysis: Bayesian optimization
approach. <em>MLST</em>, <em>5</em>(2), 025037. (<a
href="https://doi.org/10.1088/2632-2153/ad4253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In materials science, traditional techniques for analyzing layered structures are essential for obtaining information about local structure, electronic properties and chemical states. While valuable, these methods often require high vacuum environments and have limited depth profiling capabilities. The grazing exit x-ray absorption near-edge structure (GE-XANES) technique addresses these limitations by providing depth-resolved insight at ambient conditions, facilitating in situ material analysis without special sample preparation. However, GE-XANES is limited by long data acquisition times, which hinders its practicality for various applications. To overcome this, we have incorporated Bayesian optimization (BO) into the GE-XANES data acquisition process. This innovative approach potentially reduces measurement time by a factor of 50. We have used a standard GE-XANES experiment, which serve as reference, to validate the effectiveness and accuracy of the BO-informed experimental setup. Our results show that this optimized approach maintains data quality while significantly improving efficiency, making GE-XANES more accessible to a wider range of materials science applications.},
  archive      = {J_MLST},
  author       = {Cafer Tufan Cakir and Can Bogoclu and Franziska Emmerling and Christina Streli and Ana Guilherme Buzanich and Martin Radtke},
  doi          = {10.1088/2632-2153/ad4253},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025037},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning for efficient grazing-exit x-ray absorption near edge structure spectroscopy analysis: Bayesian optimization approach},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accuracy vs memory advantage in the quantum simulation of
stochastic processes. <em>MLST</em>, <em>5</em>(2), 025036. (<a
href="https://doi.org/10.1088/2632-2153/ad444a">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many inference scenarios rely on extracting relevant information from known data in order to make future predictions. When the underlying stochastic process satisfies certain assumptions, there is a direct mapping between its exact classical and quantum simulators, with the latter asymptotically using less memory. Here we focus on studying whether such quantum advantage persists when those assumptions are not satisfied, and the model is doomed to have imperfect accuracy. By studying the trade-off between accuracy and memory requirements, we show that quantum models can reach the same accuracy with less memory, or alternatively, better accuracy with the same memory. Finally, we discuss the implications of this result for learning tasks.},
  archive      = {J_MLST},
  author       = {Leonardo Banchi},
  doi          = {10.1088/2632-2153/ad444a},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025036},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Accuracy vs memory advantage in the quantum simulation of stochastic processes},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physics-informed neural networks for an optimal
counterdiabatic quantum computation. <em>MLST</em>, <em>5</em>(2),
025035. (<a href="https://doi.org/10.1088/2632-2153/ad450f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel methodology that leverages physics-informed neural networks to optimize quantum circuits in systems with \mathrm{N}_{\mathrm{Q}} qubits by addressing the counterdiabatic (CD) protocol is introduced. The primary purpose is to employ physics-inspired deep learning techniques for accurately modeling the time evolution of various physical observables within quantum systems. To achieve this, we integrate essential physical information into an underlying neural network to effectively tackle the problem. Specifically, the imposition of the solution to meet the principle of least action, along with the hermiticity condition on all physical observables, among others, ensuring the acquisition of appropriate CD terms based on underlying physics. This approach provides a reliable alternative to previous methodologies relying on classical numerical approximations, eliminating their inherent constraints. The proposed method offers a versatile framework for optimizing physical observables relevant to the problem, such as the scheduling function, gauge potential, temporal evolution of energy levels, among others. This methodology has been successfully applied to 2-qubit representing \mathrm{H}_{2} molecule using the STO-3G basis, demonstrating the derivation of a desirable decomposition for non-adiabatic terms through a linear combination of Pauli operators. This attribute confers significant advantages for practical implementation within quantum computing algorithms.},
  archive      = {J_MLST},
  author       = {Antonio Ferrer-Sánchez and Carlos Flores-Garrigos and Carlos Hernani-Morales and José J Orquín-Marqués and Narendra N Hegade and Alejandro Gomez Cadavid and Iraitz Montalban and Enrique Solano and Yolanda Vives-Gilabert and José D Martín-Guerrero},
  doi          = {10.1088/2632-2153/ad450f},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025035},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Physics-informed neural networks for an optimal counterdiabatic quantum computation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learned environment-dependent corrections for a
<span
class="math inline"><em>s</em><em>p</em><em>d</em><em>s</em><sup>*</sup></span>
empirical tight-binding basis. <em>MLST</em>, <em>5</em>(2), 025034. (<a
href="https://doi.org/10.1088/2632-2153/ad4510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Empirical tight-binding (ETB) methods have become a common choice to simulate electronic and transport properties for systems composed of thousands of atoms. However, their performance is profoundly dependent on the way the empirical parameters were fitted, and the found parametrizations often exhibit poor transferability. In order to mitigate some of the the criticalities of this method, we introduce a novel Δ-learning scheme, called MLΔTB. After being trained on a custom data set composed of ab-initio band structures, the framework is able to correlate the local atomistic environment to a correction on the on-site ETB parameters, for each atom in the system. The converged algorithm is applied to simulate the electronic properties of random GaAsSb alloys, and displays remarkable agreement both with experimental and ab-initio test data. Some noteworthy characteristics of MLΔTB include the ability to be trained on few instances, to be applied on 3D supercells of arbitrary size, to be rotationally invariant, and to predict physical properties that are not exhibited by the training set.},
  archive      = {J_MLST},
  author       = {Daniele Soccodato and Gabriele Penazzi and Alessandro Pecchia and Anh-Luan Phan and Matthias Auf der Maur},
  doi          = {10.1088/2632-2153/ad4510},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025034},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learned environment-dependent corrections for a $spds^*$ empirical tight-binding basis},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distilling particle knowledge for fast reconstruction at
high-energy physics experiments. <em>MLST</em>, <em>5</em>(2), 025033.
(<a href="https://doi.org/10.1088/2632-2153/ad43b1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation is a form of model compression that allows artificial neural networks of different sizes to learn from one another. Its main application is the compactification of large deep neural networks to free up computational resources, in particular on edge devices. In this article, we consider proton-proton collisions at the High-Luminosity Large Hadron Collider (HL-LHC) and demonstrate a successful knowledge transfer from an event-level graph neural network (GNN) to a particle-level small deep neural network (DNN). Our algorithm, DistillNet , is a DNN that is trained to learn about the provenance of particles, as provided by the soft labels that are the GNN outputs, to predict whether or not a particle originates from the primary interaction vertex. The results indicate that for this problem, which is one of the main challenges at the HL-LHC, there is minimal loss during the transfer of knowledge to the small student network, while improving significantly the computational resource needs compared to the teacher. This is demonstrated for the distilled student network on a CPU, as well as for a quantized and pruned student network deployed on an field programmable gate array. Our study proves that knowledge transfer between networks of different complexity can be used for fast artificial intelligence (AI) in high-energy physics that improves the expressiveness of observables over non-AI-based reconstruction algorithms. Such an approach can become essential at the HL-LHC experiments, e.g. to comply with the resource budget of their trigger stages.},
  archive      = {J_MLST},
  author       = {A Bal and T Brandes and F Iemmi and M Klute and B Maier and V Mikuni and T K Årrestad},
  doi          = {10.1088/2632-2153/ad43b1},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025033},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Distilling particle knowledge for fast reconstruction at high-energy physics experiments},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multiscale and multicriteria generative adversarial
network to synthesize 1-dimensional turbulent fields. <em>MLST</em>,
<em>5</em>(2), 025032. (<a
href="https://doi.org/10.1088/2632-2153/ad43b3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a new neural network stochastic model to generate a 1-dimensional stochastic field with turbulent velocity statistics. Both the model architecture and training procedure ground on the Kolmogorov and Obukhov statistical theories of fully developed turbulence, so guaranteeing descriptions of (1) energy distribution, (2) energy cascade and (3) intermittency across scales in agreement with experimental observations. The model is a generative adversarial network (GAN) with multiple multiscale optimization criteria. First, we use three physics-based criteria: the variance, skewness and flatness of the increments of the generated field, that retrieve respectively the turbulent energy distribution, energy cascade and intermittency across scales. Second, the GAN criterion, based on reproducing statistical distributions, is used on segments of different length of the generated field. Furthermore, to mimic multiscale decompositions frequently used in turbulence&#39;s studies, the model architecture is fully convolutional with kernel sizes varying along the multiple layers of the model. To train our model, we use turbulent velocity signals from grid turbulence at Modane wind tunnel.},
  archive      = {J_MLST},
  author       = {Carlos Granero Belinchon and Manuel Cabeza Gallucci},
  doi          = {10.1088/2632-2153/ad43b3},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025032},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A multiscale and multicriteria generative adversarial network to synthesize 1-dimensional turbulent fields},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards a machine-learned poisson solver for low-temperature
plasma simulations in complex geometries. <em>MLST</em>, <em>5</em>(2),
025031. (<a href="https://doi.org/10.1088/2632-2153/ad4230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Poisson&#39;s equation plays an important role in modeling many physical systems. In electrostatic self-consistent low-temperature plasma (LTP) simulations, Poisson&#39;s equation is solved at each simulation time step, which can amount to a significant computational cost for the entire simulation. In this paper, we describe the development of a generic machine-learned Poisson solver specifically designed for the requirements of LTP simulations in complex 2D reactor geometries on structured Cartesian grids. Here, the reactor geometries can consist of inner electrodes and dielectric materials as often found in LTP simulations. The approach leverages a hybrid CNN-transformer network architecture in combination with a weighted multiterm loss function. We train the network using highly randomized synthetic data to ensure the generalizability of the learned solver to unseen reactor geometries. The results demonstrate that the learned solver is able to produce quantitatively and qualitatively accurate solutions. Furthermore, it generalizes well on new reactor geometries such as reference geometries found in the literature. To increase the numerical accuracy of the solutions required in LTP simulations, we employ a conventional iterative solver to refine the raw predictions, especially to recover the high-frequency features not resolved by the initial prediction. With this, the proposed learned Poisson solver provides the required accuracy and is potentially faster than a pure GPU-based conventional iterative solver. This opens up new possibilities for developing a generic and high-performing learned Poisson solver for LTP systems in complex geometries.},
  archive      = {J_MLST},
  author       = {Ihda Chaerony Siffa and Markus M Becker and Klaus-Dieter Weltmann and Jan Trieschmann},
  doi          = {10.1088/2632-2153/ad4230},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025031},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Towards a machine-learned poisson solver for low-temperature plasma simulations in complex geometries},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analysis of machine learning prediction reliability based on
sampling distance evaluation with feature decorrelation. <em>MLST</em>,
<em>5</em>(2), 025030. (<a
href="https://doi.org/10.1088/2632-2153/ad4231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite successful use in a wide variety of disciplines for data analysis and prediction, machine learning (ML) methods suffer from a lack of understanding of the reliability of predictions due to the lack of transparency and black-box nature of ML models. In materials science and other fields, typical ML model results include a significant number of low-quality predictions. This problem is known to be particularly acute for target systems which differ significantly from the data used for ML model training. However, to date, a general method for uncertainty quantification (UQ) of ML predictions has not been available. Focusing on the intuitive and computationally efficient similarity-based UQ, we show that a simple metric based on Euclidean feature space distance and sampling density together with the decorrelation of the features using Gram–Schmidt orthogonalization allows effective separation of the accurately predicted data points from data points with poor prediction accuracy. To demonstrate the generality of the method, we apply it to support vector regression models for various small data sets in materials science and other fields. We also show that this metric is a more effective UQ tool than the standard approach of using the average distance of k nearest neighbors ( k = 1–10) in features space for similarity evaluation. Our method is computationally simple, can be used with any ML learning method and enables analysis of the sources of the ML prediction errors. Therefore, it is suitable for use as a standard technique for the estimation of ML prediction reliability for small data sets and as a tool for data set design.},
  archive      = {J_MLST},
  author       = {Evan Askanazi and Ilya Grinberg},
  doi          = {10.1088/2632-2153/ad4231},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025030},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Analysis of machine learning prediction reliability based on sampling distance evaluation with feature decorrelation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discovering quantum circuit components with program
synthesis. <em>MLST</em>, <em>5</em>(2), 025029. (<a
href="https://doi.org/10.1088/2632-2153/ad4252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite rapid progress in the field, it is still challenging to discover new ways to leverage quantum computation: all quantum algorithms must be designed by hand, and quantum mechanics is notoriously counterintuitive. In this paper, we study how artificial intelligence, in the form of program synthesis, may help overcome some of these difficulties, by showing how a computer can incrementally learn concepts relevant to quantum circuit synthesis with experience, and reuse them in unseen tasks. In particular, we focus on the decomposition of unitary matrices into quantum circuits, and show how, starting from a set of elementary gates, we can automatically discover a library of useful new composite gates and use them to decompose increasingly complicated unitaries.},
  archive      = {J_MLST},
  author       = {Leopoldo Sarra and Kevin Ellis and Florian Marquardt},
  doi          = {10.1088/2632-2153/ad4252},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025029},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Discovering quantum circuit components with program synthesis},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prediction of mild cognitive impairment using EEG signal and
BiLSTM network. <em>MLST</em>, <em>5</em>(2), 025028. (<a
href="https://doi.org/10.1088/2632-2153/ad38fe">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mild cognitive impairment (MCI) is a cognitive disease that primarily affects elderly persons. Patients with MCI have impairments in one or more cognitive areas, such as memory, attention, language, and problem-solving. The risk of Alzheimer&#39;s disease development is 10 times higher among individuals who meet the MCI diagnosis than in those who do not have such a diagnosis. Identifying the primary neurophysiological variations between those who are suffering from cognitive impairment and those who are ageing normally may provide helpful techniques to assess the effectiveness of therapies. Event-related Potentials (ERPs) are utilized to investigate the processing of sensory, cognitive, and motor information in the brain. ERPs enable excellent temporal resolution of underlying brain activity. ERP data is complex due to the temporal variation that occurs in the time domain. It is actually a type of electroencephalography (EEG) signal that is time-locked to a specific event or behavior. To remove artifacts from the data, this work utilizes Independent component analysis, finite impulse response filter, and fast Fourier transformation as preprocessing techniques. The bidirectional long short-term memory network is utilized to retain the spatial relationships between the ERP data while learning changes in temporal information for a long time. This network performed well both in modeling and information extraction from the signals. To validate the model performance, the proposed framework is tested on two benchmark datasets. The proposed framework achieved a state-of-the-art accuracy of 96.03% on the SJTU Emotion EEG Dataset dataset and 97.31% on the Chung–Ang University Hospital EEG dataset for the classification tasks.},
  archive      = {J_MLST},
  author       = {Tahani Jaser Alahmadi and Atta Ur Rahman and Zaid Ali Alhababi and Sania Ali and Hend Khalid Alkahtani},
  doi          = {10.1088/2632-2153/ad38fe},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025028},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Prediction of mild cognitive impairment using EEG signal and BiLSTM network},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accelerate microstructure evolution simulation using graph
neural networks with adaptive spatiotemporal resolution. <em>MLST</em>,
<em>5</em>(2), 025027. (<a
href="https://doi.org/10.1088/2632-2153/ad3e4b">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surrogate models driven by sizeable datasets and scientific machine-learning methods have emerged as an attractive microstructure simulation tool with the potential to deliver predictive microstructure evolution dynamics with huge savings in computational costs. Taking 2D and 3D grain growth simulations as an example, we present a completely overhauled computational framework based on graph neural networks with not only excellent agreement to both the ground truth phase-field methods and theoretical predictions, but enhanced accuracy and efficiency compared to previous works based on convolutional neural networks. These improvements can be attributed to the graph representation, both improved predictive power and a more flexible data structure amenable to adaptive mesh refinement. As the simulated microstructures coarsen, our method can adaptively adopt remeshed grids and larger timesteps to achieve further speedup. The data-to-model pipeline with training procedures together with the source codes are provided.},
  archive      = {J_MLST},
  author       = {Shaoxun Fan and Andrew L Hitt and Ming Tang and Babak Sadigh and Fei Zhou},
  doi          = {10.1088/2632-2153/ad3e4b},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025027},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Accelerate microstructure evolution simulation using graph neural networks with adaptive spatiotemporal resolution},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning based event reconstruction for cyclotron
radiation emission spectroscopy. <em>MLST</em>, <em>5</em>(2), 025026.
(<a href="https://doi.org/10.1088/2632-2153/ad3ee3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of the cyclotron radiation emission spectroscopy (CRES) technology is to build precise particle energy spectra. This is achieved by identifying the start frequencies of charged particle trajectories which, when exposed to an external magnetic field, leave semi-linear profiles (called tracks) in the time–frequency plane. Due to the need for excellent instrumental energy resolution in application, highly efficient and accurate track reconstruction methods are desired. Deep learning convolutional neural networks (CNNs) - particularly suited to deal with information-sparse data and which offer precise foreground localization—may be utilized to extract track properties from measured CRES signals (called events) with relative computational ease. In this work, we develop a novel machine learning based model which operates a CNN and a support vector machine in tandem to perform this reconstruction. A primary application of our method is shown on simulated CRES signals which mimic those of the Project 8 experiment—a novel effort to extract the unknown absolute neutrino mass value from a precise measurement of tritium β − -decay energy spectrum. When compared to a point-clustering based technique used as a baseline, we show a relative gain of 24.1% in event reconstruction efficiency and comparable performance in accuracy of track parameter reconstruction.},
  archive      = {J_MLST},
  author       = {A Ashtari Esfahani and S Böser and N Buzinsky and M C Carmona-Benitez and R Cervantes and C Claessens and L de Viveiros and M Fertl and J A Formaggio and J K Gaison and L Gladstone and M Grando and M Guigue and J Hartse and K M Heeger and X Huyan and A M Jones and K Kazkaz and M Li and A Lindman and A Marsteller and C Matthé and R Mohiuddin and B Monreal and E C Morrison and R Mueller and J A Nikkel and E Novitski and N S Oblath and J I Peña and W Pettus and R Reimann and R G H Robertson and L Saldaña and M Schram and P L Slocum and J Stachurska and Y-H Sun and P T Surukuchi and A B Telles and F Thomas and M Thomas and L A Thorne and T Thümmler and L Tvrznikova and W Van De Pontseele and B A VanDevender and T E Weiss and T Wendler and E Zayas and A Ziegler},
  doi          = {10.1088/2632-2153/ad3ee3},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025026},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deep learning based event reconstruction for cyclotron radiation emission spectroscopy},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Atomic force microscopy simulations for CO-functionalized
tips with deep learning. <em>MLST</em>, <em>5</em>(2), 025025. (<a
href="https://doi.org/10.1088/2632-2153/ad3ee6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Atomic force microscopy (AFM) operating in the frequency modulation mode with a metal tip functionalized with a CO molecule is able to image the internal structure of molecules with an unprecedented resolution. The interpretation of these images is often difficult, making the support of theoretical simulations important. Current simulation methods, particularly the most accurate ones, require expertise and resources to perform ab initio calculations for the necessary inputs (i.e charge density and electrostatic potential of the molecule). Here, we propose a computationally inexpensive and fast alternative to the physical simulation of these AFM images based on a conditional generative adversarial network (CGAN), that avoids all force calculations, and uses as the only input a 2D ball–and–stick depiction of the molecule. We discuss the performance of the model when trained with different subsets extracted from the previously published QUAM-AFM database. Our CGAN reproduces accurately the intramolecular contrast observed in the simulated images for quasi–planar molecules, but has limitations for molecules with a substantial internal corrugation, due to the strictly 2D character of the input.},
  archive      = {J_MLST},
  author       = {Jaime Carracedo-Cosme and Prokop Hapala and Rubén Pérez},
  doi          = {10.1088/2632-2153/ad3ee6},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025025},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Atomic force microscopy simulations for CO-functionalized tips with deep learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quadratic hyper-surface kernel-free large margin
distribution machine-based regression and its least-square form.
<em>MLST</em>, <em>5</em>(2), 025024. (<a
href="https://doi.org/10.1088/2632-2153/ad40fc">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ε -Support vector regression ( ε -SVR) is a powerful machine learning approach that focuses on minimizing the margin, which represents the tolerance range between predicted and actual values. However, recent theoretical studies have highlighted that simply minimizing structural risk does not necessarily result in well margin distribution. Instead, it has been shown that the distribution of margins plays a more crucial role in achieving better generalization performance. Furthermore, the kernel-free technique offers a significant advantage as it effectively reduces the overall running time and simplifies the parameter selection process compared to the kernel trick. Based on existing kernel-free regression methods, we present two efficient and robust approaches named quadratic hyper-surface kernel-free large margin distribution machine-based regression (QLDMR) and quadratic hyper-surface kernel-free least squares large margin distribution machine-based regression (QLSLDMR). The QLDMR optimizes the margin distribution by considering both ε -insensitive loss and quadratic loss function similar to the large-margin distribution machine-based regression (LDMR). QLSLDMR aims to reduce the cost of the computing process of QLDMR, which transforms inequality constraints into an equality constraint inspired by least squares support vector machines (LSSVR). Both models combined the spirit of optimal margin distribution with kernel-free technique and after simplification are convex so that they can be solved by some classical methods. Experimental results demonstrate the superiority of the optimal margin distribution combined with the kernel-free technique in robustness, generalization, and efficiency.},
  archive      = {J_MLST},
  author       = {Hao He and Kuaini Wang and Yuzhu Jiang and Huimin pei},
  doi          = {10.1088/2632-2153/ad40fc},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025024},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Quadratic hyper-surface kernel-free large margin distribution machine-based regression and its least-square form},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Random feedback alignment algorithms to train neural
networks: Why do they align? <em>MLST</em>, <em>5</em>(2), 025023. (<a
href="https://doi.org/10.1088/2632-2153/ad3ee5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feedback alignment algorithms are an alternative to backpropagation to train neural networks, whereby some of the partial derivatives that are required to compute the gradient are replaced by random terms. This essentially transforms the update rule into a random walk in weight space. Surprisingly, learning still works with those algorithms, including training of deep neural networks. The performance of FA is generally attributed to an alignment of the update of the random walker with the true gradient—the eponymous gradient alignment—which drives an approximate gradient descent. The mechanism that leads to this alignment remains unclear, however. In this paper, we use mathematical reasoning and simulations to investigate gradient alignment. We observe that the feedback alignment update rule has fixed points, which correspond to extrema of the loss function. We show that gradient alignment is a stability criterion for those fixed points. It is only a necessary criterion for algorithm performance. Experimentally, we demonstrate that high levels of gradient alignment can lead to poor algorithm performance and that the alignment is not always driving the gradient descent.},
  archive      = {J_MLST},
  author       = {Dominique Chu and Florian Bacho},
  doi          = {10.1088/2632-2153/ad3ee5},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025023},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Random feedback alignment algorithms to train neural networks: Why do they align?},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Review and experimental benchmarking of machine learning
algorithms for efficient optimization of cold atom experiments.
<em>MLST</em>, <em>5</em>(2), 025022. (<a
href="https://doi.org/10.1088/2632-2153/ad3cb6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generation of cold atom clouds is a complex process which involves the optimization of noisy data in high dimensional parameter spaces. Optimization can be challenging both in and especially outside of the lab due to lack of time, expertise, or access for lengthy manual optimization. In recent years, it was demonstrated that machine learning offers a solution since it can optimize high dimensional problems quickly, without knowledge of the experiment itself. In this paper we present results showing the benchmarking of nine different optimization techniques and implementations, alongside their ability to optimize a rubidium (Rb) cold atom experiment. The investigations are performed on a 3D 87 Rb molasses with 10 and 18 adjustable parameters, respectively, where the atom number obtained by absorption imaging was chosen as the test problem. We further compare the best performing optimizers under different effective noise conditions by reducing the signal-to-noise ratio of the images via adapting the atomic vapor pressure in the 2D+ magneto-optical trap and the detection laser frequency stability.},
  archive      = {J_MLST},
  author       = {Oliver Anton and Victoria A Henderson and Elisa Da Ros and Ivan Sekulic and Sven Burger and Philipp-Immanuel Schneider and Markus Krutzik},
  doi          = {10.1088/2632-2153/ad3cb6},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025022},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Review and experimental benchmarking of machine learning algorithms for efficient optimization of cold atom experiments},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal protein representation learning and target-aware
variational auto-encoders for protein-binding ligand generation.
<em>MLST</em>, <em>5</em>(2), 025021. (<a
href="https://doi.org/10.1088/2632-2153/ad3ee4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Without knowledge of specific pockets, generating ligands based on the global structure of a protein target plays a crucial role in drug discovery as it helps reduce the search space for potential drug-like candidates in the pipeline. However, contemporary methods require optimizing tailored networks for each protein, which is arduous and costly. To address this issue, we introduce TargetVAE , a target-aware variational auto-encoder that generates ligands with desirable properties including high binding affinity and high synthesizability to arbitrary target proteins, guided by a multimodal deep neural network built based on geometric and sequence models, named Protein Multimodal Network (PMN), as the prior for the generative model. PMN unifies different representations of proteins (e.g. primary structure—sequence of amino acids, 3D tertiary structure, and residue-level graph) into a single representation. Our multimodal architecture learns from the entire protein structure and is able to capture their sequential, topological, and geometrical information by utilizing language modeling, graph neural networks, and geometric deep learning. We showcase the superiority of our approach by conducting extensive experiments and evaluations, including predicting protein-ligand binding affinity in the PBDBind v2020 dataset as well as the assessment of generative model quality, ligand generation for unseen targets, and docking score computation. Empirical results demonstrate the promising and competitive performance of our proposed approach. Our software package is publicly available at https://github.com/HySonLab/Ligand_Generation .},
  archive      = {J_MLST},
  author       = {Nhat Khang Ngo and Truong Son Hy},
  doi          = {10.1088/2632-2153/ad3ee4},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025021},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Multimodal protein representation learning and target-aware variational auto-encoders for protein-binding ligand generation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GWAK: Gravitational-wave anomalous knowledge with recurrent
autoencoders. <em>MLST</em>, <em>5</em>(2), 025020. (<a
href="https://doi.org/10.1088/2632-2153/ad3a31">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matched-filtering detection techniques for gravitational-wave (GW) signals in ground-based interferometers rely on having well-modeled templates of the GW emission. Such techniques have been traditionally used in searches for compact binary coalescences (CBCs), and have been employed in all known GW detections so far. However, interesting science cases aside from compact mergers do not yet have accurate enough modeling to make matched filtering possible, including core-collapse supernovae and sources where stochasticity may be involved. Therefore the development of techniques to identify sources of these types is of significant interest. In this paper, we present a method of anomaly detection based on deep recurrent autoencoders to enhance the search region to unmodeled transients. We use a semi-supervised strategy that we name &#39;Gravitational Wave Anomalous Knowledge&#39; (GWAK). While the semi-supervised approach to this problem entails a potential reduction in accuracy compared to fully supervised methods, it offers a generalizability advantage by enhancing the reach of experimental sensitivity beyond the constraints of pre-defined signal templates. We construct a low-dimensional embedded space using the GWAK method, capturing the physical signatures of distinct signals on each axis of the space. By introducing signal priors that capture some of the salient features of GW signals, we allow for the recovery of sensitivity even when an unmodeled anomaly is encountered. We show that regions of the GWAK space can identify CBCs, detector glitches and also a variety of unmodeled astrophysical sources.},
  archive      = {J_MLST},
  author       = {Ryan Raikman and Eric A Moreno and Ekaterina Govorkova and Ethan J Marx and Alec Gunny and William Benoit and Deep Chatterjee and Rafia Omer and Muhammed Saleem and Dylan S Rankin and Michael W Coughlin and Philip C Harris and Erik Katsavounidis},
  doi          = {10.1088/2632-2153/ad3a31},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025020},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {GWAK: Gravitational-wave anomalous knowledge with recurrent autoencoders},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning-based spatiotemporal multi-event
reconstruction for delay line detectors. <em>MLST</em>, <em>5</em>(2),
025019. (<a href="https://doi.org/10.1088/2632-2153/ad3d2d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate observation of two or more particles within a very narrow time window has always been a challenge in modern physics. It creates the possibility of correlation experiments, such as the ground-breaking Hanbury Brown–Twiss experiment, leading to new physical insights. For low-energy electrons, one possibility is to use a Microchannel plate with subsequent delay lines for the readout of the incident particle hits, a setup called a Delay Line Detector. The spatial and temporal coordinates of more than one particle can be fully reconstructed outside a region called the dead radius. For interesting events, where two electrons are close in space and time, the determination of the individual positions of the electrons requires elaborate peak finding algorithms. While classical methods work well with single particle hits, they fail to identify and reconstruct events caused by multiple nearby particles. To address this challenge, we present a new spatiotemporal machine learning model to identify and reconstruct the position and time of such multi-hit particle signals. This model achieves a much better resolution for nearby particle hits compared to the classical approach, removing some of the artifacts and reducing the dead radius a factor of eight. We show that machine learning models can be effective in improving the spatiotemporal performance of delay line detectors.},
  archive      = {J_MLST},
  author       = {Marco Knipfer and Stefan Meier and Tobias Volk and Jonas Heimerl and Peter Hommelhoff and Sergei Gleyzer},
  doi          = {10.1088/2632-2153/ad3d2d},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025019},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deep learning-based spatiotemporal multi-event reconstruction for delay line detectors},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalization of graph-based active learning relaxation
strategies across materials. <em>MLST</em>, <em>5</em>(2), 025018. (<a
href="https://doi.org/10.1088/2632-2153/ad37f0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although density functional theory (DFT) has aided in accelerating the discovery of new materials, such calculations are computationally expensive, especially for high-throughput efforts. This has prompted an explosion in exploration of machine learning (ML) assisted techniques to improve the computational efficiency of DFT. In this study, we present a comprehensive investigation of the broader application of Finetuna, an active learning framework to accelerate structural relaxation in DFT with prior information from Open Catalyst Project pretrained graph neural networks. We explore the challenges associated with out-of-domain systems: alcohol ( C_{\gt2} ) on metal surfaces as larger adsorbates, metal oxides with spin polarization, and three-dimensional (3D) structures like zeolites and metal organic frameworks. By pre-training ML models on large datasets and fine-tuning the model along the simulation, we demonstrate the framework&#39;s ability to conduct relaxations with fewer DFT calculations. Depending on the similarity of the test systems to the training systems, a more conservative querying strategy is applied. Our best-performing Finetuna strategy reduces the number of DFT single-point calculations by 80% for alcohols and 3D structures, and 42% for oxide systems.},
  archive      = {J_MLST},
  author       = {Xiaoxiao Wang and Joseph Musielewicz and Richard Tran and Sudheesh Kumar Ethirajan and Xiaoyan Fu and Hilda Mera and John R Kitchin and Rachel C Kurchin and Zachary W Ulissi},
  doi          = {10.1088/2632-2153/ad37f0},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025018},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Generalization of graph-based active learning relaxation strategies across materials},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Training quantum boltzmann machines with the β-variational
quantum eigensolver. <em>MLST</em>, <em>5</em>(2), 025017. (<a
href="https://doi.org/10.1088/2632-2153/ad370f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quantum Boltzmann machine (QBM) is a generative machine learning model for both classical data and quantum states. Training the QBM consists of minimizing the relative entropy from the model to the target state. This requires QBM expectation values which are computationally intractable for large models in general. It is therefore important to develop heuristic training methods that work well in practice. In this work, we study a heuristic method characterized by a nested loop: the inner loop trains the β -variational quantum eigensolver ( β -VQE) by Liu et al (2021 Mach. Learn.: Sci. Technol. 2 025011) to approximate the QBM expectation values; the outer loop trains the QBM to minimize the relative entropy to the target. We show that low-rank representations obtained by β -VQE provide an efficient way to learn low-rank target states, such as classical data and low-temperature quantum tomography. We test the method on both classical and quantum target data with numerical simulations of up to 10 qubits. For the cases considered here, the obtained QBMs can model the target to high fidelity. We implement a trained model on a physical quantum device. The approach offers a valuable route towards variationally training QBMs on near-term quantum devices.},
  archive      = {J_MLST},
  author       = {Onno Huijgen and Luuk Coopmans and Peyman Najafi and Marcello Benedetti and Hilbert J Kappen},
  doi          = {10.1088/2632-2153/ad370f},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025017},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Training quantum boltzmann machines with the β-variational quantum eigensolver},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural networks for separation of cosmic gamma rays and
hadronic cosmic rays in air shower observation with a large area surface
detector array. <em>MLST</em>, <em>5</em>(2), 025016. (<a
href="https://doi.org/10.1088/2632-2153/ad3a33">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Tibet AS γ experiment has been observing cosmic gamma rays and cosmic rays in the energy range from teraelectron volts to several tens of petaelectron volts with a surface detector array since 1990. The derivation of cosmic gamma-ray flux is made by finding the excess distribution of the arrival direction of air showers above background cosmic rays. In 2014, the underground water Cherenkov muon detector (MD) was added to separate cosmic gamma rays from the background on the basis of the muon-less feature of the air showers of gamma-ray origin; hybrid observations using these two detectors were started at this time. In the present study, we developed methods to separate gamma-ray-induced air showers and hadronic cosmic-ray-induced ones using the measured particle number density distribution to improve the sensitivity of cosmic gamma-ray measurements using the Tibet air shower array data alone before the installation of the MD. We tested two approaches based on neural networks. The first method used feature values representing the lateral spread of the secondary particles, and the second method used the shower image data. To compare the separation performance of each method, we analyzed Monte Carlo air shower events in the vertically incident direction with mono-initial-energy gamma rays and protons. When discriminated by a single feature, the feature with the highest separation performance has an area under the curve (AUC) value of 0.701 for a gamma-ray energy of 10 TeV and 0.808 for 100 TeV. A separation method with a multilayer perceptron (MLP) based on multiple features has AUC values of 0.761 for a gamma-ray energy of 10 TeV and 0.854 for 100 TeV, which represents an improvement of approximately 5% in the AUC value compared with the single-feature case. We also found that the feature values that effectively contribute to the separation vary depending on the energy. A separation method with a convolutional neural network (CNN) using the shower image data has AUC values of 0.781 for a gamma-ray energy of 10 TeV and 0.901 for 100 TeV, which are approximately 5% higher than those of the MLP method. We applied the CNN separation method to Monte Carlo gamma-ray and cosmic-ray events from the Crab Nebula in the energy range 10–100 TeV. The AUC values range from 0.753 to 0.879, and the significance of the observed gamma-ray excess is improved by 1.3 to 1.8 times compared with the case without the separation procedure.},
  archive      = {J_MLST},
  author       = {Sousuke Okukawa and Kazuyuki Hara and Kinya Hibino and Yusaku Katayose and Kazumasa Kawata and Munehiro Ohnishi and Takashi Sako and Takashi K Sako and Makio Shibata and Atsushi Shiomi and Masato Takita},
  doi          = {10.1088/2632-2153/ad3a33},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025016},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Neural networks for separation of cosmic gamma rays and hadronic cosmic rays in air shower observation with a large area surface detector array},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transferring predictions of formation energy across lattices
of increasing size*. <em>MLST</em>, <em>5</em>(2), 025015. (<a
href="https://doi.org/10.1088/2632-2153/ad3d2c">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we show the transferability of graph convolutional neural network (GCNN) predictions of the formation energy of the nickel-platinum solid solution alloy across atomic structures of increasing sizes. The original dataset was generated with the large-scale atomic/molecular massively parallel simulator using the second nearest-neighbor modified embedded-atom method empirical interatomic potential. Geometry optimization was performed on the initially randomly generated face centered cubic crystal structures and the formation energy has been calculated at each step of the geometry optimization, with configurations spanning the whole compositional range. Using data from various steps of the geometry optimization, we first trained our open-source, scalable implementation of GCNN called HydraGNN on a lattice of 256 atoms, which accounts well for the short-range interactions. Using this data, we predicted the formation energy for lattices of 864 atoms and 2048 atoms, which resulted in lower-than-expected accuracy due to the long-range interactions present in these larger lattices. We accounted for the long-range interactions by including a small amount of training data representative for those two larger sizes, whereupon the predictions of HydraGNN scaled linearly with the size of the lattice. Therefore, our strategy ensured scalability while reducing significantly the computational cost of training on larger lattice sizes.},
  archive      = {J_MLST},
  author       = {Massimiliano Lupo Pasini and Mariia Karabin and Markus Eisenbach},
  doi          = {10.1088/2632-2153/ad3d2c},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025015},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Transferring predictions of formation energy across lattices of increasing size*},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GRINN: A physics-informed neural network for solving
hydrodynamic systems in the presence of self-gravity. <em>MLST</em>,
<em>5</em>(2), 025014. (<a
href="https://doi.org/10.1088/2632-2153/ad3a32">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling self-gravitating gas flows is essential to answering many fundamental questions in astrophysics. This spans many topics including planet-forming disks, star-forming clouds, galaxy formation, and the development of large-scale structures in the Universe. However, the nonlinear interaction between gravity and fluid dynamics offers a formidable challenge to solving the resulting time-dependent partial differential equations (PDEs) in three dimensions (3D). By leveraging the universal approximation capabilities of a neural network within a mesh-free framework, physics informed neural networks (PINNs) offer a new way of addressing this challenge. We introduce the gravity-informed neural network (GRINN), a PINN-based code, to simulate 3D self-gravitating hydrodynamic systems. Here, we specifically study gravitational instability and wave propagation in an isothermal gas. Our results match a linear analytic solution to within 1% in the linear regime and a conventional grid code solution to within 5% as the disturbance grows into the nonlinear regime. We find that the computation time of the GRINN does not scale with the number of dimensions. This is in contrast to the scaling of the grid-based code for the hydrodynamic and self-gravity calculations as the number of dimensions is increased. Our results show that the GRINN computation time is longer than the grid code in one- and two- dimensional calculations but is an order of magnitude lesser than the grid code in 3D with similar accuracy. Physics-informed neural networks like GRINN thus show promise for advancing our ability to model 3D astrophysical flows.},
  archive      = {J_MLST},
  author       = {Sayantan Auddy and Ramit Dey and Neal J Turner and Shantanu Basu},
  doi          = {10.1088/2632-2153/ad3a32},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025014},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {GRINN: A physics-informed neural network for solving hydrodynamic systems in the presence of self-gravity},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Classifying rock types by geostatistics and random forests
in tandem. <em>MLST</em>, <em>5</em>(2), 025013. (<a
href="https://doi.org/10.1088/2632-2153/ad3c0f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rock type classification is crucial for evaluating mineral resources in ore deposits and for rock mechanics. Mineral deposits are formed in a variety of rock bodies and rock types. However, the rock type identification in drill core samples is often complicated by overprinting and weathering processes. An approach to classifying rock types from drill core data relies on whole-rock geochemical assays as features. There are few studies on rock type classification from a limited number of metal grades and dry bulk density as features. The novelty in our approach is the introduction of two sets of feature variables (proxies) at sampled data points, generated by geostatistical leave-one-out cross-validation and by kriging for removing short-scale spatial variation of the measured features. We applied our proposal to a dataset from a porphyry Cu–Au deposit in Mongolia. The model performances on a testing data subset indicate that, when the training dataset is not large, the performance of the classifier (a random forest) substantially improves by incorporating the proxy features as a complement to the original measured features. At each training data point, these proxy features throw light based on the underlying spatial data correlation structure, scales of variations, sampling design, and values of features observed at neighboring points, and show the benefits of combining geostatistics with machine learning.},
  archive      = {J_MLST},
  author       = {Parag Jyoti Dutta and Xavier Emery},
  doi          = {10.1088/2632-2153/ad3c0f},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025013},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Classifying rock types by geostatistics and random forests in tandem},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differentiable simulation of a liquid argon time projection
chamber. <em>MLST</em>, <em>5</em>(2), 025012. (<a
href="https://doi.org/10.1088/2632-2153/ad2cf0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Liquid argon time projection chambers (LArTPCs) are widely used in particle detection for their tracking and calorimetric capabilities. The particle physics community actively builds and improves high-quality simulators for such detectors in order to develop physics analyses in a realistic setting. The ability of these simulators to mimic real, measured data is limited by the modeling of the physical detectors used for data collection. This modeling can be improved by performing dedicated calibration measurements. Conventional approaches calibrate individual detector parameters or processes one at a time. However, the impact of detector processes is entangled, making this a poor description of the underlying physics. We introduce a differentiable simulator that enables a gradient-based optimization, allowing for the first time a simultaneous calibration of all detector parameters. We describe the procedure of making a differentiable simulator, highlighting the challenges of retaining the physics quality of the standard, non-differentiable version while providing meaningful gradient information. We further discuss the advantages and drawbacks of using our differentiable simulator for calibration. Finally, we provide a starting point for extensions to our approach, including applications of the differentiable simulator to physics analysis pipelines.},
  archive      = {J_MLST},
  author       = {Sean Gasiorowski and Yifan Chen and Youssef Nashed and Pierre Granger and Camelia Mironov and Ka Vang Tsang and Daniel Ratner and Kazuhiro Terao},
  doi          = {10.1088/2632-2153/ad2cf0},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025012},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Differentiable simulation of a liquid argon time projection chamber},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A method for quantifying the generalization capabilities of
generative models for solving ising models. <em>MLST</em>,
<em>5</em>(2), 025011. (<a
href="https://doi.org/10.1088/2632-2153/ad3710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For Ising models with complex energy landscapes, whether the ground state can be found by neural networks depends heavily on the Hamming distance between the training datasets and the ground state. Despite the fact that various recently proposed generative models have shown good performance in solving Ising models, there is no adequate discussion on how to quantify their generalization capabilities. Here we design a Hamming distance regularizer in the framework of a class of generative models, variational autoregressive networks (VANs), to quantify the generalization capabilities of various network architectures combined with VAN. The regularizer can control the size of the overlaps between the ground state and the training datasets generated by networks, which, together with the success rates of finding the ground state, form a quantitative metric to quantify their generalization capabilities. We conduct numerical experiments on several prototypical network architectures combined with VAN, including feed-forward neural networks, recurrent neural networks, and graph neural networks, to quantify their generalization capabilities when solving Ising models. Moreover, considering the fact that the quantification of the generalization capabilities of networks on small-scale problems can be used to predict their relative performance on large-scale problems, our method is of great significance for assisting in the Neural Architecture Search field of searching for the optimal network architectures when solving large-scale Ising models.},
  archive      = {J_MLST},
  author       = {Qunlong Ma and Zhi Ma and Ming Gao},
  doi          = {10.1088/2632-2153/ad3710},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025011},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A method for quantifying the generalization capabilities of generative models for solving ising models},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generation of conformational ensembles of small molecules
via surrogate model-assisted molecular dynamics. <em>MLST</em>,
<em>5</em>(2), 025010. (<a
href="https://doi.org/10.1088/2632-2153/ad3b64">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate prediction of thermodynamic properties is crucial in various fields such as drug discovery and materials design. This task relies on sampling from the underlying Boltzmann distribution, which is challenging using conventional approaches such as simulations. In this work, we introduce surrogate model-assisted molecular dynamics (SMA-MD), a new procedure to sample the equilibrium ensemble of molecules. First, SMA-MD leverages deep generative models to enhance the sampling of slow degrees of freedom. Subsequently, the generated ensemble undergoes statistical reweighting, followed by short simulations. Our empirical results show that SMA-MD generates more diverse and lower energy ensembles than conventional MD simulations. Furthermore, we showcase the application of SMA-MD for the computation of thermodynamical properties by estimating implicit solvation free energies.},
  archive      = {J_MLST},
  author       = {Juan Viguera Diez and Sara Romeo Atance and Ola Engkvist and Simon Olsson},
  doi          = {10.1088/2632-2153/ad3b64},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025010},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Generation of conformational ensembles of small molecules via surrogate model-assisted molecular dynamics},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural networks as effective surrogate models of
radio-frequency quadrupole particle accelerator simulations.
<em>MLST</em>, <em>5</em>(2), 025009. (<a
href="https://doi.org/10.1088/2632-2153/ad3a30">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio-frequency quadrupoles (RFQs) are multi-purpose linear particle accelerators that simultaneously bunch and accelerate charged particle beams. They are ubiquitous in accelerator physics, especially as injectors to higher-energy machines, owing to their impressive efficiency. The design and optimization of these devices can be lengthy due to the need to repeatedly perform high-fidelity simulations. Several recent papers have demonstrated that machine learning can be used to build surrogate models (fast-executing replacements of computationally costly beam simulations) for order-of-magnitude computing time speedups. However, while these pilot studies are encouraging, there is room to improve their predictive accuracy. Particularly, beam summary statistics such as emittances (an important figure of merit in particle accelerator physics) have historically been challenging to predict. For the first time, we present a surrogate model trained on 200 000 samples that yields \lt 6% mean average percent error for the predictions of all relevant beam output parameters from defining RFQ design parameters, solving the problem of poor emittance predictions by identifying and including hidden variables which were not accounted for previously. These surrogate models were made possible by using the Julia language and GPU computing; we briefly discuss both. We demonstrate the utility of surrogate modeling by performing a multi-objective optimization using our best model as a callback in the objective function to select an optimal RFQ design. We consider trade-offs in RFQ performance for various choices of Pareto-optimal design variables—common issues for any multi-objective optimization scheme. Lastly, we make recommendations for input data preparation, selection, and neural network architectures that pave the way for future development of production-capable surrogate models for RFQs and other particle accelerators.},
  archive      = {J_MLST},
  author       = {Joshua Villarreal and Daniel Winklehner and Daniel Koser and Janet M Conrad},
  doi          = {10.1088/2632-2153/ad3a30},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025009},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Neural networks as effective surrogate models of radio-frequency quadrupole particle accelerator simulations},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A machine learning constitutive model for plasticity and
strain hardening of polycrystalline metals based on data from
micromechanical simulations. <em>MLST</em>, <em>5</em>(2), 025008. (<a
href="https://doi.org/10.1088/2632-2153/ad379e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) methods have emerged as promising tools for generating constitutive models directly from mechanical data. Constitutive models are fundamental in describing and predicting the mechanical behavior of materials under arbitrary loading conditions. In recent approaches, the yield function, central to constitutive models, has been formulated in a data-oriented manner using ML. Many ML approaches have primarily focused on initial yielding, and the effect of strain hardening has not been widely considered. However, taking strain hardening into account is crucial for accurately describing the deformation behavior of polycrystalline metals. To address this problem, the present study introduces an ML-based yield function formulated as a support vector classification model, which encompasses strain hardening. This function was trained using a 12-dimensional feature vector that includes stress and plastic strain components resulting from crystal plasticity finite element method (CPFEM) simulations on a 3-dimensional RVE with 343 grains with a random crystallographic texture. These simulations were carried out to mimic multi-axial mechanical testing of the polycrystal under proportional loading in 300 different directions, which were selected to ensure proper coverage of the full stress space. The training data were directly taken from the stress–strain results obtained for the 300 multi-axial load cases. It is shown that the ML yield function trained on these data describes not only the initial yield behavior but also the flow stresses in the plastic regime with a very high accuracy and robustness. The workflow introduced in this work to generate synthetic mechanical data based on realistic CPFEM simulations and to train an ML yield function, including strain hardening, will open new possibilities in microstructure-sensitive materials modeling and thus pave the way for obtaining digital material twins.},
  archive      = {J_MLST},
  author       = {Ronak Shoghi and Alexander Hartmaier},
  doi          = {10.1088/2632-2153/ad379e},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025008},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A machine learning constitutive model for plasticity and strain hardening of polycrystalline metals based on data from micromechanical simulations},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using convolutional neural networks for stereological
characterization of 3D hetero-aggregates based on synthetic STEM data.
<em>MLST</em>, <em>5</em>(2), 025007. (<a
href="https://doi.org/10.1088/2632-2153/ad38fd">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 3D nano/microstructure of materials can significantly influence their macroscopic properties. In order to enable a better understanding of such structure-property relationships, 3D microscopy techniques can be deployed, which are however often expensive in both time and costs. Often 2D imaging techniques are more accessible, yet they have the disadvantage that the 3D nano/microstructure of materials cannot be directly retrieved from such measurements. The motivation of this work is to overcome the issues of characterizing 3D structures from 2D measurements for hetero-aggregate materials. For this purpose, a method is presented that relies on machine learning combined with methods of spatial stochastic modeling for characterizing the 3D nano/microstructure of materials from 2D data. More precisely, a stochastic model is utilized for the generation of synthetic training data. This kind of training data has the advantage that time-consuming experiments for the synthesis of differently structured materials followed by their 3D imaging can be avoided. More precisely, a parametric stochastic 3D model is presented, from which a wide spectrum of virtual hetero-aggregates can be generated. Additionally, the virtual structures are passed to a physics-based simulation tool in order to generate virtual scanning transmission electron microscopy (STEM) images. The preset parameters of the 3D model together with the simulated STEM images serve as a database for the training of convolutional neural networks, which can be used to determine the parameters of the underlying 3D model and, consequently, to predict 3D structures of hetero-aggregates from 2D STEM images. Furthermore, an error analysis is performed with respect to structural descriptors, e.g. the hetero-coordination number. The proposed method is applied to image data of TiO 2 -WO 3 hetero-aggregates, which are highly relevant in photocatalysis processes. However, the proposed method can be transferred to other types of aggregates and to different 2D microscopy techniques. Consequently, the method is relevant for industrial or laboratory setups in which product quality is to be quantified by means of inexpensive 2D image acquisition.},
  archive      = {J_MLST},
  author       = {Lukas Fuchs and Tom Kirstein and Christoph Mahr and Orkun Furat and Valentin Baric and Andreas Rosenauer and Lutz Mädler and Volker Schmidt},
  doi          = {10.1088/2632-2153/ad38fd},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025007},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Using convolutional neural networks for stereological characterization of 3D hetero-aggregates based on synthetic STEM data},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph convolutional multi-mesh autoencoder for steady
transonic aircraft aerodynamics. <em>MLST</em>, <em>5</em>(2), 025006.
(<a href="https://doi.org/10.1088/2632-2153/ad36ad">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Calculating aerodynamic loads around an aircraft using computational fluid dynamics is a user&#39;s and computer-intensive task. An attractive alternative is to leverage neural networks (NNs) bypassing the need of solving the governing fluid equations at all flight conditions of interest. NNs have the ability to infer highly nonlinear predictions if a reference dataset is available. This work presents a geometric deep learning based multi-mesh autoencoder framework for steady-state transonic aerodynamics. The framework builds on graph NNs which are designed for irregular and unstructured spatial discretisations, embedded in a multi-resolution algorithm for dimensionality reduction. The test case is for the NASA common research model wing/body aircraft configuration. Thorough studies are presented discussing the model predictions in terms of vector fields, pressure and shear-stress coefficients, and scalar fields, total force and moment coefficients, for a range of nonlinear conditions involving shock waves and flow separation. We note that the cost of the model prediction is minimal having used an existing database.},
  archive      = {J_MLST},
  author       = {David Massegur and Andrea Da Ronch},
  doi          = {10.1088/2632-2153/ad36ad},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025006},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Graph convolutional multi-mesh autoencoder for steady transonic aircraft aerodynamics},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generation model meets swin transformer for unsupervised
low-dose CT reconstruction. <em>MLST</em>, <em>5</em>(2), 025005. (<a
href="https://doi.org/10.1088/2632-2153/ad370e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computed tomography (CT) has evolved into an indispensable tool for clinical diagnosis. Reducing radiation dose crucially minimizes adverse effects but may introduce noise and artifacts in reconstructed images, affecting diagnostic processes for physicians. Scholars have tackled deep learning training instability by exploring diffusion models. Given the scarcity of clinical data, we propose the unsupervised image domain score generation model (UISG) for low-dose CT reconstruction. During training, normal-dose CT images are utilized as network inputs to train a score-based generative model that captures the prior distribution of CT images. In the iterative reconstruction, the initial CT image is obtained using a filtered back-projection algorithm. Subsequently, diffusion-based prior, high-frequency convolutional sparse coding prior, and data-consistency steps are employed to obtain the high-quality reconstructed image. Given the global characteristics of noise, the score network of the diffusion model utilizes a swin transformer structure to enhance the model&#39;s ability to capture long-range dependencies. Furthermore, convolutional sparse coding is applied exclusively to the high-frequency components of the image, to prevent over-smoothing or the loss of crucial anatomical details during the denoising process. Quantitative and qualitative results indicate that UISG outperforms competing methods in terms of denoising and generalization performance.},
  archive      = {J_MLST},
  author       = {Yu Li and Xueqin Sun and Sukai Wang and Yingwei Qin and Jinxiao Pan and Ping Chen},
  doi          = {10.1088/2632-2153/ad370e},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025005},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Generation model meets swin transformer for unsupervised low-dose CT reconstruction},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active robotic search for victims using ensemble deep
learning techniques. <em>MLST</em>, <em>5</em>(2), 025004. (<a
href="https://doi.org/10.1088/2632-2153/ad33df">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, legged quadruped robots have proved to be a valuable support to humans in dealing with search and rescue operations. These robots can move with great ability in complex terrains, unstructured environments or regions with many obstacles. This work employs the quadruped robot A1 Rescue Tasks UPM Robot (ARTU-R) by Unitree, equipped with an RGB-D camera and a lidar, to perform victim searches in post-disaster scenarios. Exploration is done not by following a pre-planned path (as common methods) but by prioritising the areas most likely to harbour victims. To accomplish that task, both indirect search and next best view techniques have been used. When ARTU-R gets inside an unstructured and unknown environment, it selects the next exploration point from a series of candidates. This operation is performed by comparing, for each candidate, the distance to reach it, the unexplored space around it and the probability of a victim being in its vicinity. This probability value is obtained using a Random Forest, which processes the information provided by a convolutional neural network. Unlike other AI techniques, random forests are not black box models; humans can understand their decision-making processes. The system, once integrated, achieves speeds comparable to other state-of-the-art algorithms in terms of exploration, but concerning victim detection, the tests show that the resulting smart exploration generates logical paths—from a human point of view—and that ARTU-R tends to move first to the regions where victims are present.},
  archive      = {J_MLST},
  author       = {Jorge F García-Samartín and Christyan Cruz Ulloa and Jaime del Cerro and Antonio Barrientos},
  doi          = {10.1088/2632-2153/ad33df},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025004},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Active robotic search for victims using ensemble deep learning techniques},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the expressivity of embedding quantum kernels.
<em>MLST</em>, <em>5</em>(2), 025003. (<a
href="https://doi.org/10.1088/2632-2153/ad2f51">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most natural connections between quantum and classical machine learning has been established in the context of kernel methods. Kernel methods rely on kernels, which are inner products of feature vectors living in large feature spaces. Quantum kernels are typically evaluated by explicitly constructing quantum feature states and then taking their inner product, here called embedding quantum kernels. Since classical kernels are usually evaluated without using the feature vectors explicitly, we wonder how expressive embedding quantum kernels are. In this work, we raise the fundamental question: can all quantum kernels be expressed as the inner product of quantum feature states? Our first result is positive: Invoking computational universality, we find that for any kernel function there always exists a corresponding quantum feature map and an embedding quantum kernel. The more operational reading of the question is concerned with efficient constructions, however. In a second part, we formalize the question of universality of efficient embedding quantum kernels. For shift-invariant kernels, we use the technique of random Fourier features to show that they are universal within the broad class of all kernels which allow a variant of efficient Fourier sampling. We then extend this result to a new class of so-called composition kernels, which we show also contains projected quantum kernels introduced in recent works. After proving the universality of embedding quantum kernels for both shift-invariant and composition kernels, we identify the directions towards new, more exotic, and unexplored quantum kernel families, for which it still remains open whether they correspond to efficient embedding quantum kernels.},
  archive      = {J_MLST},
  author       = {Elies Gil-Fuster and Jens Eisert and Vedran Dunjko},
  doi          = {10.1088/2632-2153/ad2f51},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025003},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {On the expressivity of embedding quantum kernels},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using positional tracking to improve abdominal ultrasound
machine learning classification. <em>MLST</em>, <em>5</em>(2), 025002.
(<a href="https://doi.org/10.1088/2632-2153/ad379d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnostic abdominal ultrasound screening and monitoring protocols are based around gathering a set of standard cross sectional images that ensure the coverage of relevant anatomical structures during the collection procedure. This allows clinicians to make diagnostic decisions with the best picture available from that modality. Currently, there is very little assistance provided to sonographers to ensure adherence to collection protocols, with previous studies suggesting that traditional image only machine learning classification can provide only limited assistance in supporting this task, for example it can be difficult to differentiate between multiple liver cross sections or those of the left and right kidney from image post collection. In this proof of concept, positional tracking information was added to the image input of a neural network to provide the additional context required to recognize six otherwise difficult to identify edge cases. In this paper optical and sensor based infrared tracking (IR) was used to track the position of an ultrasound probe during the collection of clinical cross sections on an abdominal phantom. Convolutional neural networks were then trained using both image-only and image with positional data, the classification accuracy results were then compared. The addition of positional information significantly improved average classification results from ∼90% for image-only to 95% for optical IR position tracking and 93% for Sensor-based IR in common abdominal cross sections. While there is further work to be done, the addition of low-cost positional tracking to machine learning ultrasound classification will allow for significantly increased accuracy for identifying important diagnostic cross sections, with the potential to not only provide validation of adherence to protocol but also could provide navigation prompts to assist in user training and in ensuring adherence in capturing cross sections in future.},
  archive      = {J_MLST},
  author       = {Alistair Lawley and Rory Hampson and Kevin Worrall and Gordon Dobie},
  doi          = {10.1088/2632-2153/ad379d},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025002},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Using positional tracking to improve abdominal ultrasound machine learning classification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ensemble classifiers fed by functional connectivity during
cognitive processing differentiate parkinson’s disease even being under
medication. <em>MLST</em>, <em>5</em>(2), 025001. (<a
href="https://doi.org/10.1088/2632-2153/ad370d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain–computer interface technologies, as a type of human-computer interaction, provide a control ability on machines and intelligent systems via human brain functions without needing physical contact. Moreover, it has a considerable contribution to the detection of cognitive state changes, which gives a clue for neurodegenerative diseases, including Parkinson&#39;s disease (PD), in recent years. Although various studies implemented different machine learning models with several EEG features to detect PD and receive remarkable performances, there is a lack of knowledge on how brain connectivity during a cognitive task contributes to the differentiation of PD, even being under medication. To fill this gap, this study used three ensemble classifiers, which were fed by functional connectivity through cognitive response coherence (CRC) with varying selected features in different frequency bands upon application of the 3-Stimulation auditory oddball paradigm to differentiate PD medication ON and OFF and healthy controls (HC). The results revealed that the most remarkable performances were exhibited in slow frequency bands (delta and theta) in comparison to high frequency and wide range bands, especially in terms of target sounds. Moreover, in the delta band, target CRC distinguishes all groups from each other with accuracy rates of 80% for HC vs PD-OFF, 80% for HC vs PD-ON, and 81% for PD-ON vs PD-OFF. In the theta band, again target sounds were the most distinctive stimuli to classify HCxPD-OFF (80% accuracy), HCxPD-ON (80.5% accuracy) with quite good performances, and PD-ONxPD-OFF (76% accuracy) with acceptable performance. Besides, this study achieved a state-of-the-art performance with an accuracy of 87.5% in classifying PD-ONxPD-OFF via CRC of standard sounds in the delta band. Overall, the findings revealed that brain connectivity contributes to identifying PD and HC as well as the medication state of PD, especially in the slow frequency bands.},
  archive      = {J_MLST},
  author       = {Emine Elif Tülay},
  doi          = {10.1088/2632-2153/ad370d},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Ensemble classifiers fed by functional connectivity during cognitive processing differentiate parkinson’s disease even being under medication},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analysis and benchmarking of feature reduction for
classification under computational constraints. <em>MLST</em>,
<em>5</em>(2), 020501. (<a
href="https://doi.org/10.1088/2632-2153/ad3726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning is most often expensive in terms of computational and memory costs due to training with large volumes of data. Current computational limitations of many computing systems motivate us to investigate practical approaches, such as feature selection and reduction, to reduce the time and memory costs while not sacrificing the accuracy of classification algorithms. In this work, we carefully review, analyze, and identify the feature reduction methods that have low costs/overheads in terms of time and memory. Then, we evaluate the identified reduction methods in terms of their impact on the accuracy, precision, time, and memory costs of traditional classification algorithms. Specifically, we focus on the least resource intensive feature reduction methods that are available in Scikit-Learn library. Since our goal is to identify the best performing low-cost reduction methods, we do not consider complex expensive reduction algorithms in this study. In our evaluation, we find that at quadratic-scale feature reduction, the classification algorithms achieve the best trade-off among competitive performance metrics. Results show that the overall training times are reduced 61%, the model sizes are reduced 6 × , and accuracy scores increase 25% compared to the baselines on average with quadratic scale reduction.},
  archive      = {J_MLST},
  author       = {Omer Subasi and Sayan Ghosh and Joseph Manzano and Bruce Palmer and Andrés Marquez},
  doi          = {10.1088/2632-2153/ad3726},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {020501},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Analysis and benchmarking of feature reduction for classification under computational constraints},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generative adversarial networks for data-scarce radiative
heat transfer applications. <em>MLST</em>, <em>5</em>(1), 015060. (<a
href="https://doi.org/10.1088/2632-2153/ad33e1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial networks (GANs) are one of the most robust and versatile techniques in the field of generative artificial intelligence. In this work, we report on an application of GANs in the domain of synthetic spectral data generation for data-scarce radiative heat transfer applications, an area where their use has not been previously reported. We demonstrate the proposed approach by applying it to an illustrative problem within the realm of near-field radiative heat transfer involving a multilayered hyperbolic metamaterial. We find that a successful generation of spectral data requires two modifications to conventional GANs: (i) the introduction of Wasserstein GANs (WGANs) to avoid mode collapse, and, (ii) the conditioning of WGANs to obtain accurate labels for the generated data. We show that a simple feed-forward neural network (FFNN), when augmented with data generated by a CWGAN, enhances significantly its performance under conditions of limited data availability. In addition, we show that CWGANs can act as a surrogate model with improved performance in the low-data regime with respect to simple FFNNs. Overall, this work contributes to highlight the potential of generative machine learning algorithms in scientific applications beyond image generation and optimization.},
  archive      = {J_MLST},
  author       = {J J García-Esteban and J C Cuevas and J Bravo-Abad},
  doi          = {10.1088/2632-2153/ad33e1},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {1},
  pages        = {015060},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Generative adversarial networks for data-scarce radiative heat transfer applications},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient interpolation of molecular properties across
chemical compound space with low-dimensional descriptors. <em>MLST</em>,
<em>5</em>(1), 015059. (<a
href="https://doi.org/10.1088/2632-2153/ad360e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We demonstrate accurate data-starved models of molecular properties for interpolation in chemical compound spaces with low-dimensional descriptors. Our starting point is based on three-dimensional, universal, physical descriptors derived from the properties of the distributions of the eigenvalues of Coulomb matrices. To account for the shape and composition of molecules, we combine these descriptors with six-dimensional features informed by the Gershgorin circle theorem. We use the nine-dimensional descriptors thus obtained for Gaussian process regression based on kernels with variable functional form, leading to extremely efficient, low-dimensional interpolation models. The resulting models trained with 100 molecules are able to predict the product of entropy and temperature ( S × T ) and zero point vibrational energy (ZPVE) with the absolute error under 1 kcal mol −1 for {\gt}78 % and under 1.3 kcal mol −1 for {\gt}92 % of molecules in the test data. The test data comprises 20 000 molecules with complexity varying from three atoms to 29 atoms and the ranges of S × T and ZPVE covering 36 kcal mol −1 and 161 kcal mol −1 , respectively. We also illustrate that the descriptors based on the Gershgorin circle theorem yield more accurate models of molecular entropy than those based on graph neural networks that explicitly account for the atomic connectivity of molecules.},
  archive      = {J_MLST},
  author       = {Yun-Wen Mao and Roman V Krems},
  doi          = {10.1088/2632-2153/ad360e},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {1},
  pages        = {015059},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Efficient interpolation of molecular properties across chemical compound space with low-dimensional descriptors},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Laziness, barren plateau, and noises in machine learning.
<em>MLST</em>, <em>5</em>(1), 015058. (<a
href="https://doi.org/10.1088/2632-2153/ad35a3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We define laziness to describe a large suppression of variational parameter updates for neural networks, classical or quantum. In the quantum case, the suppression is exponential in the number of qubits for randomized variational quantum circuits. We discuss the difference between laziness and barren plateau in quantum machine learning created by quantum physicists in McClean et al (2018 Nat. Commun. 9 1–6) for the flatness of the loss function landscape during gradient descent. We address a novel theoretical understanding of those two phenomena in light of the theory of neural tangent kernels. For noiseless quantum circuits, without the measurement noise, the loss function landscape is complicated in the overparametrized regime with a large number of trainable variational angles. Instead, around a random starting point in optimization, there are large numbers of local minima that are good enough and could minimize the mean square loss function, where we still have quantum laziness, but we do not have barren plateaus. However, the complicated landscape is not visible within a limited number of iterations, and low precision in quantum control and quantum sensing. Moreover, we look at the effect of noises during optimization by assuming intuitive noise models, and show that variational quantum algorithms are noise-resilient in the overparametrization regime. Our work precisely reformulates the quantum barren plateau statement towards a precision statement and justifies the statement in certain noise models, injects new hope toward near-term variational quantum algorithms, and provides theoretical connections toward classical machine learning. Our paper provides conceptual perspectives about quantum barren plateaus, together with discussions about the gradient descent dynamics in Liu et al (2023 Phys. Rev. Lett. 130 150601).},
  archive      = {J_MLST},
  author       = {Junyu Liu and Zexi Lin and Liang Jiang},
  doi          = {10.1088/2632-2153/ad35a3},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {1},
  pages        = {015058},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Laziness, barren plateau, and noises in machine learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Qudit machine learning. <em>MLST</em>, <em>5</em>(1),
015057. (<a href="https://doi.org/10.1088/2632-2153/ad360d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a comprehensive investigation into the learning capabilities of a simple d -level system (qudit). Our study is specialized for classification tasks using real-world databases, specifically the Iris, breast cancer, and MNIST datasets. We explore various learning models in the metric learning framework, along with different encoding strategies. In particular, we employ data re-uploading techniques and maximally orthogonal states to accommodate input data within low-dimensional systems. Our findings reveal optimal strategies, indicating that when the dimension of input feature data and the number of classes are not significantly larger than the qudit&#39;s dimension, our results show favorable comparisons against the best classical models. This trend holds true even for small quantum systems, with dimensions d &lt; 5 and utilizing algorithms with a few layers ( L = 1,2 ). However, for high-dimensional data such as MNIST, we adopt a hybrid approach involving dimensional reduction through a convolutional neural network. In this context, we observe that small quantum systems often act as bottlenecks, resulting in lower accuracy compared to their classical counterparts.},
  archive      = {J_MLST},
  author       = {Sebastián Roca-Jerat and Juan Román-Roche and David Zueco},
  doi          = {10.1088/2632-2153/ad360d},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015057},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Qudit machine learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging trust for joint multi-objective and
multi-fidelity optimization. <em>MLST</em>, <em>5</em>(1), 015056. (<a
href="https://doi.org/10.1088/2632-2153/ad35a4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the pursuit of efficient optimization of expensive-to-evaluate systems, this paper investigates a novel approach to Bayesian multi-objective and multi-fidelity (MOMF) optimization. Traditional optimization methods, while effective, often encounter prohibitively high costs in multi-dimensional optimizations of one or more objectives. Multi-fidelity approaches offer potential remedies by utilizing multiple, less costly information sources, such as low-resolution approximations in numerical simulations. However, integrating these two strategies presents a significant challenge. We propose the innovative use of a trust metric to facilitate the joint optimization of multiple objectives and data sources. Our methodology introduces a modified multi-objective (MO) optimization policy incorporating the trust gain per evaluation cost as one of the objectives of a Pareto optimization problem. This modification enables simultaneous MOMF optimization, which proves effective in establishing the Pareto set and front at a fraction of the cost. Two specific methods of MOMF optimization are presented and compared: a holistic approach selecting both the input parameters and the fidelity parameter jointly, and a sequential approach for benchmarking. Through benchmarks on synthetic test functions, our novel approach is shown to yield significant cost reductions—up to an order of magnitude compared to pure MO optimization. Furthermore, we find that joint optimization of the trust and objective domains outperforms sequentially addressing them. We validate our findings with the specific use case of optimizing particle-in-cell simulations of laser-plasma acceleration, highlighting the practical potential of our method in the Pareto optimization of highly expensive black-box functions. Implementation of the methods in existing Bayesian optimization frameworks is straightforward, with immediate extensions e.g. to batch optimization possible. Given their ability to handle various continuous or discrete fidelity dimensions, these techniques have wide-ranging applicability in tackling simulation challenges across various scientific computing fields such as plasma physics and fluid dynamics.},
  archive      = {J_MLST},
  author       = {Faran Irshad and Stefan Karsch and Andreas Döpp},
  doi          = {10.1088/2632-2153/ad35a4},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015056},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Leveraging trust for joint multi-objective and multi-fidelity optimization},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Actively learning costly reward functions for reinforcement
learning. <em>MLST</em>, <em>5</em>(1), 015055. (<a
href="https://doi.org/10.1088/2632-2153/ad33e0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer of recent advances in deep reinforcement learning to real-world applications is hindered by high data demands and thus low efficiency and scalability. Through independent improvements of components such as replay buffers or more stable learning algorithms, and through massively distributed systems, training time could be reduced from several days to several hours for standard benchmark tasks. However, while rewards in simulated environments are well-defined and easy to compute, reward evaluation becomes the bottleneck in many real-world environments, e.g. in molecular optimization tasks, where computationally demanding simulations or even experiments are required to evaluate states and to quantify rewards. When ground-truth evaluations become orders of magnitude more expensive than in research scenarios, direct transfer of recent advances would require massive amounts of scale, just for evaluating rewards rather than training the models. We propose to alleviate this problem by replacing costly ground-truth rewards with rewards modeled by neural networks, counteracting non-stationarity of state and reward distributions during training with an active learning component. We demonstrate that using our proposed method, it is possible to train agents in complex real-world environments orders of magnitudes faster than would be possible when using ground-truth rewards. By enabling the application of RL methods to new domains, we show that we can find interesting and non-trivial solutions to real-world optimization problems in chemistry, materials science and engineering. We demonstrate speed-up factors of 50–3000 when applying our approach to challenges of molecular design and airfoil optimization.},
  archive      = {J_MLST},
  author       = {André Eberhard and Houssam Metni and Georg Fahland and Alexander Stroh and Pascal Friederich},
  doi          = {10.1088/2632-2153/ad33e0},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015055},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Actively learning costly reward functions for reinforcement learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimized multifidelity machine learning for quantum
chemistry. <em>MLST</em>, <em>5</em>(1), 015054. (<a
href="https://doi.org/10.1088/2632-2153/ad2cef">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) provides access to fast and accurate quantum chemistry (QC) calculations for various properties of interest such as excitation energies. It is often the case that high accuracy in prediction using a ML model, demands a large and costly training set. Various solutions and procedures have been presented to reduce this cost. These include methods such as Δ-ML, hierarchical-ML, and multifidelity machine learning (MFML). MFML combines various Δ-ML like sub-models for various fidelities according to a fixed scheme derived from the sparse grid combination technique. In this work we implement an optimization procedure to combine multifidelity models in a flexible scheme resulting in optimized MFML (o-MFML) that provides superior prediction capabilities. This hyperparameter optimization is carried out on a holdout validation set of the property of interest. This work benchmarks the o-MFML method in predicting the atomization energies on the QM7b dataset, and again in the prediction of excitation energies for three molecules of growing size. The results indicate that o-MFML is a strong methodological improvement over MFML and provides lower error of prediction. Even in cases of poor data distributions and lack of clear hierarchies among the fidelities, which were previously identified as issues for multifidelity methods, the o-MFML is advantageous for the prediction of quantum chemical properties.},
  archive      = {J_MLST},
  author       = {Vivin Vinod and Ulrich Kleinekathöfer and Peter Zaspel},
  doi          = {10.1088/2632-2153/ad2cef},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015054},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Optimized multifidelity machine learning for quantum chemistry},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The impact of memory on learning sequence-to-sequence tasks.
<em>MLST</em>, <em>5</em>(1), 015053. (<a
href="https://doi.org/10.1088/2632-2153/ad2feb">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent success of neural networks in natural language processing has drawn renewed attention to learning sequence-to-sequence (seq2seq) tasks. While there exists a rich literature that studies classification and regression tasks using solvable models of neural networks, seq2seq tasks have not yet been studied from this perspective. Here, we propose a simple model for a seq2seq task that has the advantage of providing explicit control over the degree of memory, or non-Markovianity, in the sequences—the stochastic switching-Ornstein–Uhlenbeck (SSOU) model. We introduce a measure of non-Markovianity to quantify the amount of memory in the sequences. For a minimal auto-regressive (AR) learning model trained on this task, we identify two learning regimes corresponding to distinct phases in the stationary state of the SSOU process. These phases emerge from the interplay between two different time scales that govern the sequence statistics. Moreover, we observe that while increasing the integration window of the AR model always improves performance, albeit with diminishing returns, increasing the non-Markovianity of the input sequences can improve or degrade its performance. Finally, we perform experiments with recurrent and convolutional neural networks that show that our observations carry over to more complicated neural network architectures.},
  archive      = {J_MLST},
  author       = {Alireza Seif and Sarah A M Loos and Gennaro Tucci and Édgar Roldán and Sebastian Goldt},
  doi          = {10.1088/2632-2153/ad2feb},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015053},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {The impact of memory on learning sequence-to-sequence tasks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autonomous data extraction from peer reviewed literature for
training machine learning models of oxidation potentials. <em>MLST</em>,
<em>5</em>(1), 015052. (<a
href="https://doi.org/10.1088/2632-2153/ad2f52">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an automated data-collection pipeline involving a convolutional neural network and a large language model to extract user-specified tabular data from peer-reviewed literature. The pipeline is applied to 74 reports published between 1957 and 2014 with experimentally-measured oxidation potentials for 592 organic molecules (−0.75 to 3.58 V). After data curation (solvents, reference electrodes, and missed data points), we trained multiple supervised machine learning (ML) models reaching prediction errors similar to experimental uncertainty (∼0.2 V). For experimental measurements of identical molecules reported in multiple studies, we identified the most likely value based on out-of-sample ML predictions. Using the trained ML models, we then estimated oxidation potentials of ∼132k small organic molecules from the QM9 (quantum mechanics data for organic molecules with up to 9 atoms not counting hydrogens) data set, with predicted values spanning 0.21–3.46 V. Analysis of the QM9 predictions in terms of plausible descriptor-property trends suggests that aliphaticity increases the oxidation potential of an organic molecule on average from ∼1.5 V to ∼2 V, while an increase in number of heavy atoms lowers it systematically. The pipeline introduced offers significant reductions in human labor otherwise required for conventional manual data collection of experimental results, and exemplifies how to accelerate scientific research through automation.},
  archive      = {J_MLST},
  author       = {Siwoo Lee and Stefan Heinen and Danish Khan and O Anatole von Lilienfeld},
  doi          = {10.1088/2632-2153/ad2f52},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015052},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Autonomous data extraction from peer reviewed literature for training machine learning models of oxidation potentials},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Observing schrödinger’s cat with artificial intelligence:
Emergent classicality from information bottleneck. <em>MLST</em>,
<em>5</em>(1), 015051. (<a
href="https://doi.org/10.1088/2632-2153/ad3330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We train a generative language model on the randomized local measurement data collected from Schrödinger&#39;s cat quantum state. We demonstrate that the classical reality emerges in the language model due to the information bottleneck: although our training data contains the full quantum information about Schrödinger&#39;s cat, a weak language model can only learn to capture the classical reality of the cat from the data. We identify the quantum–classical boundary in terms of both the size of the quantum system and the information processing power of the classical intelligent agent, which indicates that a stronger agent can realize more quantum nature in the environmental noise surrounding the quantum system. Our approach opens up a new avenue for using the big data generated on noisy intermediate-scale quantum devices to train generative models for representation learning of quantum operators, which might be a step toward our ultimate goal of creating an artificial intelligence quantum physicist.},
  archive      = {J_MLST},
  author       = {Zhelun Zhang and Yi-Zhuang You},
  doi          = {10.1088/2632-2153/ad3330},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015051},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Observing schrödinger’s cat with artificial intelligence: Emergent classicality from information bottleneck},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inverting the kohn–sham equations with physics-informed
machine learning. <em>MLST</em>, <em>5</em>(1), 015050. (<a
href="https://doi.org/10.1088/2632-2153/ad3159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic structure theory calculations offer an understanding of matter at the quantum level, complementing experimental studies in materials science and chemistry. One of the most widely used methods, density functional theory, maps a set of real interacting electrons to a set of fictitious non-interacting electrons that share the same probability density. Ensuring that the density remains the same depends on the exchange-correlation (XC) energy and, by a derivative, the XC potential. Inversions provide a method to obtain exact XC potentials from target electronic densities, in hopes of gaining insights into accuracy-boosting approximations. Neural networks provide a new avenue to perform inversions by learning the mapping from density to potential. In this work, we learn this mapping using physics-informed machine learning methods, namely physics informed neural networks and Fourier neural operators. We demonstrate the capabilities of these two methods on a dataset of one-dimensional atomic and molecular models. The capabilities of each approach are discussed in conjunction with this proof-of-concept presentation. The primary finding of our investigation is that the combination of both approaches has the greatest potential for inverting the Kohn–Sham equations at scale.},
  archive      = {J_MLST},
  author       = {Vincent Martinetto and Karan Shah and Attila Cangi and Aurora Pribram-Jones},
  doi          = {10.1088/2632-2153/ad3159},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015050},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Inverting the Kohn–Sham equations with physics-informed machine learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Determination of droplet size from wide-angle light
scattering image data using convolutional neural networks.
<em>MLST</em>, <em>5</em>(1), 015049. (<a
href="https://doi.org/10.1088/2632-2153/ad2f53">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wide-angle light scattering (WALS) offers the possibility of a highly temporally and spatially resolved measurement of droplets in spray-based methods for nanoparticle synthesis. The size of these droplets is a critical variable affecting the final properties of synthesized materials such as hetero-aggregates. However, conventional methods for determining droplet sizes from WALS image data are labor-intensive and may introduce biases, particularly when applied to complex systems like spray flame synthesis. To address these challenges, we introduce a fully automatic machine learning-based approach that employs convolutional neural networks (CNNs) in order to streamline the droplet sizing process. This CNN-based methodology offers further advantages: it requires few manual labels and can utilize transfer learning, making it a promising alternative to conventional methods, specifically with respect to efficiency. To evaluate the performance of our machine learning models, we consider WALS data from an ethanol spray flame process at various heights above burner surface, where the models are trained and cross-validated on a large dataset comprising nearly 35000 WALS images.},
  archive      = {J_MLST},
  author       = {Tom Kirstein and Simon Aßmann and Orkun Furat and Stefan Will and Volker Schmidt},
  doi          = {10.1088/2632-2153/ad2f53},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015049},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Determination of droplet size from wide-angle light scattering image data using convolutional neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transforming two-dimensional tensor networks into quantum
circuits for supervised learning. <em>MLST</em>, <em>5</em>(1), 015048.
(<a href="https://doi.org/10.1088/2632-2153/ad2fec">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There have been numerous quantum neural networks reported, but they struggle to match traditional neural networks in accuracy. Given the huge improvement of the neural network models&#39; accuracy by two-dimensional tensor network (TN) states in classical tensor network machine learning (TNML), it is promising to explore whether its application in quantum machine learning can extend the performance boundary of the models. Here, we transform two-dimensional TNs into quantum circuits for supervised learning. Specifically, we encode two-dimensional TNs into quantum circuits through rigorous mathematical proofs for constructing model ansätze, including string-bond states, entangled-plaquette states and isometric TN states. In addition, we propose adaptive data encoding methods and combine with TNs. We construct a tensor-network-inspired quantum circuit (TNQC) supervised learning framework for transferring TNML from classical to quantum, and build several novel two-dimensional TN-inspired quantum classifiers based on this framework. Finally, we propose a parallel quantum machine learning method for multi-class classification to construct 2D TNQC-based multi-class classifiers. Classical simulation results on the MNIST benchmark dataset show that our proposed models achieve the state-of-the-art accuracy performance, significantly outperforming other quantum classifiers on both binary and multi-class classification tasks, and beat simple convolutional classifiers on a fair track with identical inputs. The noise resilience of the models makes them successfully run and work in a real quantum computer.},
  archive      = {J_MLST},
  author       = {Zhihui Song and Jinchen Xu and Xin Zhou and Xiaodong Ding and Zheng Shan},
  doi          = {10.1088/2632-2153/ad2fec},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015048},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Transforming two-dimensional tensor networks into quantum circuits for supervised learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An iterative deep learning procedure for determining
electron scattering cross-sections from transport coefficients.
<em>MLST</em>, <em>5</em>(1), 015047. (<a
href="https://doi.org/10.1088/2632-2153/ad2fed">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose improvements to the artificial neural network (ANN) method of determining electron scattering cross-sections from swarm data proposed by coauthors. A limitation inherent to this problem, known as the inverse swarm problem, is the non-unique nature of its solutions, particularly when there exists multiple cross-sections that each describe similar scattering processes. Considering this, prior methods leveraged existing knowledge of a particular cross-section set to reduce the solution space of the problem. To reduce the need for prior knowledge, we propose the following modifications to the ANN method. First, we propose a multi-branch ANN (MBANN) that assigns an independent branch of hidden layers to each cross-section output. We show that in comparison with an equivalent conventional ANN, the MBANN architecture enables an efficient and physics informed feature map of each cross-section. Additionally, we show that the MBANN solution can be improved upon by successive networks that are each trained using perturbations of the previous regression. Crucially, the method requires much less input data and fewer restrictive assumptions, and only assumes knowledge of energy loss thresholds and the number of cross-sections present.},
  archive      = {J_MLST},
  author       = {Dale L Muccignat and Gregory G Boyle and Nathan A Garland and Peter W Stokes and Ronald D White},
  doi          = {10.1088/2632-2153/ad2fed},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015047},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {An iterative deep learning procedure for determining electron scattering cross-sections from transport coefficients},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WaveFormer: Transformer-based denoising method for
gravitational-wave data. <em>MLST</em>, <em>5</em>(1), 015046. (<a
href="https://doi.org/10.1088/2632-2153/ad2f54">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of gravitational-wave astronomy and the discovery of more compact binary coalescences, data quality improvement techniques are desired to handle the complex and overwhelming noise in gravitational wave (GW) observational data. Though recent machine learning-based studies have shown promising results for data denoising, they are unable to precisely recover both the GW signal amplitude and phase. To address such an issue, we develop a deep neural network centered workflow, WaveFormer, for significant noise suppression and signal recovery on observational data from the Laser Interferometer Gravitational-Wave Observatory (LIGO). The WaveFormer has a science-driven architecture design with hierarchical feature extraction across a broad frequency spectrum. As a result, the overall noise and glitch are decreased by more than one order of magnitude and the signal recovery error is roughly 1% and 7% for the phase and amplitude, respectively. Moreover, on 75 reported binary black hole events of LIGO we obtain a significant improvement of inverse false alarm rate. Our work highlights the potential of large neural networks in GW data analysis and, while primarily demonstrated on LIGO data, its adaptable design indicates promise for broader application within the International Gravitational-Wave Observatories Network in future observational runs.},
  archive      = {J_MLST},
  author       = {He Wang and Yue Zhou and Zhoujian Cao and Zongkuan Guo and Zhixiang Ren},
  doi          = {10.1088/2632-2153/ad2f54},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015046},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {WaveFormer: Transformer-based denoising method for gravitational-wave data},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gaussian-process-regression-based method for the
localization of exceptional points in complex resonance spectra.
<em>MLST</em>, <em>5</em>(1), 015045. (<a
href="https://doi.org/10.1088/2632-2153/ad2e16">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resonances in open quantum systems depending on at least two controllable parameters can show the phenomenon of exceptional points (EPs), where not only the eigenvalues but also the eigenvectors of two or more resonances coalesce. Their exact localization in the parameter space is challenging, in particular in systems, where the computation of the quantum spectra and resonances is numerically very expensive. We introduce an efficient machine learning algorithm to find EPs based on Gaussian process regression (GPR). The GPR-model is trained with an initial set of eigenvalue pairs belonging to an EP and used for a first estimation of the EP position via a numerically cheap root search. The estimate is then improved iteratively by adding selected exact eigenvalue pairs as training points to the GPR-model. The GPR-based method is developed and tested on a simple low-dimensional matrix model and then applied to a challenging real physical system, viz., the localization of EPs in the resonance spectra of excitons in cuprous oxide in external electric and magnetic fields. The precise computation of EPs, by taking into account the complete valence band structure and central-cell corrections of the crystal, can be the basis for the experimental observation of EPs in this system.},
  archive      = {J_MLST},
  author       = {Patrick Egenlauf and Patric Rommel and Jörg Main},
  doi          = {10.1088/2632-2153/ad2e16},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015045},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Gaussian-process-regression-based method for the localization of exceptional points in complex resonance spectra},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust errant beam prognostics with conditional modeling for
particle accelerators. <em>MLST</em>, <em>5</em>(1), 015044. (<a
href="https://doi.org/10.1088/2632-2153/ad2e18">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Particle accelerators are complex and comprise thousands of components, with many pieces of equipment running at their peak power. Consequently, they can fault and abort operations for numerous reasons, lowering efficiency and science output. To avoid these faults, we apply anomaly detection techniques to predict unusual behavior and perform preemptive actions to improve the total availability. Supervised machine learning (ML) techniques such as siamese neural network models can outperform the often-used unsupervised or semi-supervised approaches for anomaly detection by leveraging the label information. One of the challenges specific to anomaly detection for particle accelerators is the data&#39;s variability due to accelerator configuration changes within a production run of several months. ML models fail at providing accurate predictions when data changes due to changes in the configuration. To address this challenge, we include the configuration settings into our models and training to improve the results. Beam configurations are used as a conditional input for the model to learn any cross-correlation between the data from different conditions and retain its performance. We employ conditional siamese neural network (CSNN) models and conditional variational auto encoder (CVAE) models to predict errant beam pulses at the spallation neutron source under different system configurations and compare their performance. We demonstrate that CSNNs outperform CVAEs in our application.},
  archive      = {J_MLST},
  author       = {Kishansingh Rajput and Malachi Schram and Willem Blokland and Yasir Alanazi and Pradeep Ramuhalli and Alexander Zhukov and Charles Peters and Ricardo Vilalta},
  doi          = {10.1088/2632-2153/ad2e18},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015044},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Robust errant beam prognostics with conditional modeling for particle accelerators},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spectral density classification for environment
spectroscopy. <em>MLST</em>, <em>5</em>(1), 015043. (<a
href="https://doi.org/10.1088/2632-2153/ad2cf1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral densities encode the relevant information characterizing the system–environment interaction in an open-quantum system problem. Such information is key to determining the system&#39;s dynamics. In this work, we leverage the potential of machine learning techniques to reconstruct the features of the environment. Specifically, we show that the time evolution of a system observable can be used by an artificial neural network to infer the main features of the spectral density. In particular, for relevant examples of spin-boson models, we can classify with high accuracy the Ohmicity parameter of the environment as either Ohmic, sub-Ohmic or super-Ohmic, thereby distinguishing between different forms of dissipation.},
  archive      = {J_MLST},
  author       = {J Barr and G Zicari and A Ferraro and M Paternostro},
  doi          = {10.1088/2632-2153/ad2cf1},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015043},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Spectral density classification for environment spectroscopy},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WATUNet: A deep neural network for segmentation of
volumetric sweep imaging ultrasound. <em>MLST</em>, <em>5</em>(1),
015042. (<a href="https://doi.org/10.1088/2632-2153/ad2e15">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Limited access to breast cancer diagnosis globally leads to delayed treatment. Ultrasound, an effective yet underutilized method, requires specialized training for sonographers, which hinders its widespread use. Volume sweep imaging (VSI) is an innovative approach that enables untrained operators to capture high-quality ultrasound images. Combined with deep learning, like convolutional neural networks, it can potentially transform breast cancer diagnosis, enhancing accuracy, saving time and costs, and improving patient outcomes. The widely used UNet architecture, known for medical image segmentation, has limitations, such as vanishing gradients and a lack of multi-scale feature extraction and selective region attention. In this study, we present a novel segmentation model known as Wavelet_Attention_UNet (WATUNet). In this model, we incorporate wavelet gates and attention gates between the encoder and decoder instead of a simple connection to overcome the limitations mentioned, thereby improving model performance. Two datasets are utilized for the analysis: the public &#39;Breast Ultrasound Images&#39; dataset of 780 images and a private VSI dataset of 3818 images, captured at the University of Rochester by the authors. Both datasets contained segmented lesions categorized into three types: no mass, benign mass, and malignant mass. Our segmentation results show superior performance compared to other deep networks. The proposed algorithm attained a Dice coefficient of 0.94 and an F1 score of 0.94 on the VSI dataset and scored 0.93 and 0.94 on the public dataset, respectively. Moreover, our model significantly outperformed other models in McNemar&#39;s test with false discovery rate correction on a 381-image VSI set. The experimental findings demonstrate that the proposed WATUNet model achieves precise segmentation of breast lesions in both standard-of-care and VSI images, surpassing state-of-the-art models. Hence, the model holds considerable promise for assisting in lesion identification, an essential step in the clinical diagnosis of breast lesions.},
  archive      = {J_MLST},
  author       = {Donya Khaledyan and Thomas J Marini and Avice O’Connell and Steven Meng and Jonah Kan and Galen Brennan and Yu Zhao and Timothy M Baran and Kevin J Parker},
  doi          = {10.1088/2632-2153/ad2e15},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015042},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {WATUNet: A deep neural network for segmentation of volumetric sweep imaging ultrasound},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online meta-learned gradient norms for active learning in
science and technology. <em>MLST</em>, <em>5</em>(1), 015041. (<a
href="https://doi.org/10.1088/2632-2153/ad2e17">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acquisition of scientific data can be expensive and time-consuming. Active learning is a solution to reduce costs and time by guiding the selection of scientific experiments. Autonomous and automatic identification of the most essential samples to annotate by active learning can also help to mitigate human bias. Previous research has demonstrated that unlabelled samples causing the largest gradient norms of neural network models can promote active learning in classification. However, gradient norm estimation in regression is non-trivial because the continuous one-dimensional output of regression significantly differs from classification. In this study, we propose a new active learning method that uses meta-learning to estimate the gradient norm of the unlabelled sample in regression. Specifically, we use a separate model to be a selector that learns knowledge from the previous active learning results and is used to predict the gradient norms of unlabelled samples. In each active learning iteration, we estimate and select unlabelled samples with the largest gradient norms to annotate. Our method is evaluated on six regression data sets in various domains, which include costly scientific data.},
  archive      = {J_MLST},
  author       = {Haiqi Dong and Amanda S Barnard and Amanda J Parker},
  doi          = {10.1088/2632-2153/ad2e17},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015041},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Online meta-learned gradient norms for active learning in science and technology},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantum machine learning for image classification.
<em>MLST</em>, <em>5</em>(1), 015040. (<a
href="https://doi.org/10.1088/2632-2153/ad2aef">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image classification, a pivotal task in multiple industries, faces computational challenges due to the burgeoning volume of visual data. This research addresses these challenges by introducing two quantum machine learning models that leverage the principles of quantum mechanics for effective computations. Our first model, a hybrid quantum neural network with parallel quantum circuits, enables the execution of computations even in the noisy intermediate-scale quantum era, where circuits with a large number of qubits are currently infeasible. This model demonstrated a record-breaking classification accuracy of 99.21% on the full MNIST dataset, surpassing the performance of known quantum–classical models, while having eight times fewer parameters than its classical counterpart. Also, the results of testing this hybrid model on a Medical MNIST (classification accuracy over 99%), and on CIFAR-10 (classification accuracy over 82%), can serve as evidence of the generalizability of the model and highlights the efficiency of quantum layers in distinguishing common features of input data. Our second model introduces a hybrid quantum neural network with a Quanvolutional layer, reducing image resolution via a convolution process. The model matches the performance of its classical counterpart, having four times fewer trainable parameters, and outperforms a classical model with equal weight parameters. These models represent advancements in quantum machine learning research and illuminate the path towards more accurate image classification systems.},
  archive      = {J_MLST},
  author       = {Arsenii Senokosov and Alexandr Sedykh and Asel Sagingalieva and Basil Kyriacou and Alexey Melnikov},
  doi          = {10.1088/2632-2153/ad2aef},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015040},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Quantum machine learning for image classification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stochastic gradient descent with random label noises: Doubly
stochastic models and inference stabilizer. <em>MLST</em>,
<em>5</em>(1), 015039. (<a
href="https://doi.org/10.1088/2632-2153/ad13ba">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random label noise (or observational noise) widely exists in practical machine learning settings. While previous studies primarily focused on the effects of label noise to the performance of learning, our work intends to investigate the implicit regularization effects of label noise, under mini-batch sampling settings of stochastic gradient descent (SGD), with the assumption that label noise is unbiased. Specifically, we analyze the learning dynamics of SGD over the quadratic loss with unbiased label noise (ULN), where we model the dynamics of SGD as a stochastic differentiable equation with two diffusion terms (namely a doubly stochastic model). While the first diffusion term is caused by mini-batch sampling over the (label-noiseless) loss gradients, as in many other works on SGD (Zhu et al 2019 ICML 7654–63; Wu et al 2020 Int. Conf. on Machine Learning (PMLR) pp 10367–76), our model investigates the second noise term of SGD dynamics, which is caused by mini-batch sampling over the label noise, as an implicit regularizer. Our theoretical analysis finds such an implicit regularizer would favor some convergence points that could stabilize model outputs against perturbations of parameters (namely inference stability ). Though similar phenomenon have been investigated by Blanc et al (2020 Conf. on Learning Theory (PMLR) pp 483–513), our work does not assume SGD as an Ornstein–Uhlenbeck-like process and achieves a more generalizable result with convergence of the approximation proved. To validate our analysis, we design two sets of empirical studies to analyze the implicit regularizer of SGD with unbiased random label noise for deep neural network training and linear regression. Our first experiment studies the noisy self-distillation tricks for deep learning, where student networks are trained using the outputs from well-trained teachers with additive unbiased random label noise. Our experiment shows that the implicit regularizer caused by the label noise tends to select models with improved inference stability. We also carry out experiments on SGD-based linear regression with ULN, where we plot the trajectories of parameters learned in every step and visualize the effects of implicit regularization. The results back up our theoretical findings.},
  archive      = {J_MLST},
  author       = {Haoyi Xiong and Xuhong Li and Boyang Yu and Dongrui Wu and Zhanxing Zhu and Dejing Dou},
  doi          = {10.1088/2632-2153/ad13ba},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015039},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Stochastic gradient descent with random label noises: Doubly stochastic models and inference stabilizer},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prediction of 4D stress field evolution around additive
manufacturing-induced porosity through progressive deep-learning
frameworks. <em>MLST</em>, <em>5</em>(1), 015038. (<a
href="https://doi.org/10.1088/2632-2153/ad290c">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the application of machine learning models to predict time-evolving stress fields in complex three-dimensional structures trained with full-scale finite element simulation data. Two novel architectures, the multi-decoder CNN (MUDE-CNN) and the multiple encoder–decoder model with transfer learning (MTED-TL), were introduced to address the challenge of predicting the progressive and spatial evolutional of stress distributions around defects. The MUDE-CNN leveraged a shared encoder for simultaneous feature extraction and employed multiple decoders for distinct time frame predictions, while MTED-TL progressively transferred knowledge from one encoder–decoder block to another, thereby enhancing prediction accuracy through transfer learning. These models were evaluated to assess their accuracy, with a particular focus on predicting temporal stress fields around an additive manufacturing (AM)-induced isolated pore, as understanding such defects is crucial for assessing mechanical properties and structural integrity in materials and components fabricated via AM. The temporal model evaluation demonstrated MTED-TL&#39;s consistent superiority over MUDE-CNN, owing to transfer learning&#39;s advantageous initialization of weights and smooth loss curves. Furthermore, an autoregressive training framework was introduced to improve temporal predictions, consistently outperforming both MUDE-CNN and MTED-TL. By accurately predicting temporal stress fields around AM-induced defects, these models can enable real-time monitoring and proactive defect mitigation during the fabrication process. This capability ensures enhanced component quality and enhances the overall reliability of additively manufactured parts.},
  archive      = {J_MLST},
  author       = {Mohammad Rezasefat and James D Hogan},
  doi          = {10.1088/2632-2153/ad290c},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015038},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Prediction of 4D stress field evolution around additive manufacturing-induced porosity through progressive deep-learning frameworks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-driven lie point symmetry detection for continuous
dynamical systems. <em>MLST</em>, <em>5</em>(1), 015037. (<a
href="https://doi.org/10.1088/2632-2153/ad2629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symmetry detection, the task of discovering the underlying symmetries of a given dataset, has been gaining popularity in the machine learning community, particularly in science and engineering applications. Most previous works focus on detecting &#39;canonical&#39; symmetries such as translation, scaling, and rotation, and cast the task as a modeling problem involving complex inductive biases and architecture design of neural networks. We challenge these assumptions and propose that instead of constructing biases, we can learn to detect symmetries from raw data without prior knowledge. The approach presented in this paper provides a flexible way to scale up the detection procedure to non-canonical symmetries, and has the potential to detect both known and unknown symmetries alike. Concretely, we focus on predicting the generators of Lie point symmetries of partial differential equations, more specifically, evolutionary equations for ease of data generation. Our results demonstrate that well-established neural network architectures are capable of recognizing symmetry generators, even in unseen dynamical systems. These findings have the potential to make non-canonical symmetries more accessible to applications, including model selection, sparse identification, and data interpretability.},
  archive      = {J_MLST},
  author       = {Alex Gabel and Rick Quax and Efstratios Gavves},
  doi          = {10.1088/2632-2153/ad2629},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015037},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Data-driven lie point symmetry detection for continuous dynamical systems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparison of neural network architectures for feature
extraction from binary black hole merger waveforms. <em>MLST</em>,
<em>5</em>(1), 015036. (<a
href="https://doi.org/10.1088/2632-2153/ad2972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We evaluate several neural-network architectures, both convolutional and recurrent, for gravitational-wave time-series feature extraction by performing point parameter estimation on noisy waveforms from binary-black-hole mergers. We build datasets of 100 000 elements for each of four different waveform models (or approximants) in order to test how approximant choice affects feature extraction. Our choices include SEOBNRv4P and IMRPhenomPv3 , which contain only the dominant quadrupole emission mode, alongside IMRPhenomPv3HM and NRHybSur3dq8 , which also account for high-order modes. Each dataset element is injected into detector noise corresponding to the third observing run of the LIGO-Virgo-KAGRA (LVK) collaboration. We identify the temporal convolutional network architecture as the overall best performer in terms of training and validation losses and absence of overfitting to data. Comparison of results between datasets shows that the choice of waveform approximant for the creation of a dataset conditions the feature extraction ability of a trained network. Hence, care should be taken when building a dataset for the training of neural networks, as certain approximants may result in better network convergence of evaluation metrics. However, this performance does not necessarily translate to data which is more faithful to numerical relativity simulations. We also apply this network on actual signals from LVK runs, finding that its feature-extracting performance can be effective on real data.},
  archive      = {J_MLST},
  author       = {Osvaldo Gramaxo Freitas and Juan Calderón Bustillo and José A Font and Solange Nunes and Antonio Onofre and Alejandro Torres-Forné},
  doi          = {10.1088/2632-2153/ad2972},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015036},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Comparison of neural network architectures for feature extraction from binary black hole merger waveforms},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bridging the gap between high-level quantum chemical methods
and deep learning models. <em>MLST</em>, <em>5</em>(1), 015035. (<a
href="https://doi.org/10.1088/2632-2153/ad27e1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised deep learning (DL) models are becoming ubiquitous in computational chemistry because they can efficiently learn complex input-output relationships and predict chemical properties at a cost significantly lower than methods based on quantum mechanics. The central challenge in many DL applications is the need to invest considerable computational resources in generating large ( N \gt 1 \times 10^5 ) training sets such that the resulting DL model can be generalized reliably to unseen systems. The lack of better alternatives has encouraged the use of low-cost and relatively inaccurate density-functional theory (DFT) methods to generate training data, leading to DL models that lack accuracy and reliability. In this article, we describe a robust and easily implemented approach based on property-specific atom-centered potentials (ACPs) that resolves this central challenge in DL model development. ACPs are one-electron potentials that are applied in combination with a computationally inexpensive but inaccurate quantum mechanical method (e.g. double- ζ DFT) and fitted against relatively few high-level data ( N \approx 1\times 10^{3} – 1\times 10^{4} ), possibly obtained from the literature. The resulting ACP-corrected methods retain the low cost of the double- ζ DFT approach, while generating high-level-quality data in unseen systems for the specific property for which they were designed. With this approach, we demonstrate that ACPs can be used as an intermediate method between high-level approaches and DL model development, enabling the calculation of large and accurate DL training sets for the chemical property of interest. We demonstrate the effectiveness of the proposed approach by predicting bond dissociation enthalpies, reaction barrier heights, and reaction energies with chemical accuracy at a computational cost lower than the DFT methods routinely used for DL training data set generation.},
  archive      = {J_MLST},
  author       = {Viki Kumar Prasad and Alberto Otero-de-la-Roza and Gino A DiLabio},
  doi          = {10.1088/2632-2153/ad27e1},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015035},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Bridging the gap between high-level quantum chemical methods and deep learning models},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regression transients modeling of solid rocket motor burning
surfaces with physics-guided neural network. <em>MLST</em>,
<em>5</em>(1), 015034. (<a
href="https://doi.org/10.1088/2632-2153/ad2973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring the burning surface regression in ground static ignition tests is crucial for predicting the internal ballistic performance of solid rocket motors (SRMs). A previously proposed ultra-sparse computed tomography imaging method provides a possibility for real-time monitoring. However, sample shortages of SRMs highlights the need for monitoring accuracy, especially given the high cost associated with the design and development of SRM systems. Therefore, constructing datasets via regression simulations to compensate for SRM sample shortages is critical. To address this issue, we recommend adopting the level-set method to dynamically track the burning surface by solving partial differential equations (PDEs). The computational cost of numerical solution is prohibitive for scientific applications involving large-scale spatiotemporal domains. The physics-informed neural network (PINN) and neural operator have been used to accelerate the solution of PDE, showing satisfactory prediction performance and high computational efficiency. We designed a physics-guided network, named LS-PhyNet, that couples the potential physical mechanisms of burning surface regression into the deep learning framework. The proposed method is capable of encoding well-established traditional numerical discretization methods into the network architecture to leverage prior knowledge of underlying physics, thus providing the model with enhanced expressive power and interpretability. Experimental results prove that LS-PhyNet can better reproduce the burning surfaces obtained by numerical solution with only small data regimes, providing a new paradigm for real-time monitoring of burning surface regression transients during static ignition tests.},
  archive      = {J_MLST},
  author       = {XueQin Sun and Yu Li and YiHong Li and SuKai Wang and Xuan Li and Ming Lu and Ping Chen},
  doi          = {10.1088/2632-2153/ad2973},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015034},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Regression transients modeling of solid rocket motor burning surfaces with physics-guided neural network},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supervised and unsupervised learning of <span
class="math inline">(1 + 1)</span>-dimensional even-offspring branching
annihilating random walks. <em>MLST</em>, <em>5</em>(1), 015033. (<a
href="https://doi.org/10.1088/2632-2153/ad27e2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) of phase transitions (PTs) has gradually become an effective approach that enables us to explore the nature of various PTs more promptly in equilibrium and nonequilibrium systems. Unlike equilibrium systems, non-equilibrium systems display more complicated and diverse features because of the extra dimension of time, which is not readily tractable, both theoretically and numerically. The combination of ML and most renowned nonequilibrium model, directed percolation (DP), led to some significant findings. In this study, ML is applied to (1+1) -d, even offspring branching annihilating random walks (BAW), whose universality class is not DP-like. The supervised learning of (1+1) -d BAW via convolutional neural networks (CNN) results in a more accurate prediction of the critical point than the Monte Carlo (MC) simulation for the same system sizes. The dynamic exponent z and spatial correlation length correlation exponent \nu_{\perp} were also measured and found to be consistent with their respective theoretical values. Furthermore, the unsupervised learning of (1+1) -d BAW via an autoencoder (AE) gives rise to a transition point, which is the same as the critical point. The latent layer of AE, through a single neuron, can be regarded as the order parameter of the system being properly re-scaled. Therefore, we believe that ML has exciting application prospects in reaction-diffusion systems such as BAW and DP.},
  archive      = {J_MLST},
  author       = {Yanyang Wang and Wei Li and Feiyi Liu and Jianmin Shen},
  doi          = {10.1088/2632-2153/ad27e2},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015033},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Supervised and unsupervised learning of $(1+1)$-dimensional even-offspring branching annihilating random walks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physics informed token transformer for solving partial
differential equations. <em>MLST</em>, <em>5</em>(1), 015032. (<a
href="https://doi.org/10.1088/2632-2153/ad27e3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving partial differential equations (PDEs) is the core of many fields of science and engineering. While classical approaches are often prohibitively slow, machine learning models often fail to incorporate complete system information. Over the past few years, transformers have had a significant impact on the field of Artificial Intelligence and have seen increased usage in PDE applications. However, despite their success, transformers currently lack integration with physics and reasoning. This study aims to address this issue by introducing Physics Informed Token Transformer (PITT). The purpose of PITT is to incorporate the knowledge of physics by embedding PDEs into the learning process. PITT uses an equation tokenization method to learn an analytically-driven numerical update operator. By tokenizing PDEs and embedding partial derivatives, the transformer models become aware of the underlying knowledge behind physical processes. To demonstrate this, PITT is tested on challenging 1D and 2D PDE operator learning tasks. The results show that PITT outperforms popular neural operator models and has the ability to extract physically relevant information from governing equations.},
  archive      = {J_MLST},
  author       = {Cooper Lorsung and Zijie Li and Amir Barati Farimani},
  doi          = {10.1088/2632-2153/ad27e3},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015032},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Physics informed token transformer for solving partial differential equations},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep energy-pressure regression for a thermodynamically
consistent EOS model. <em>MLST</em>, <em>5</em>(1), 015031. (<a
href="https://doi.org/10.1088/2632-2153/ad2626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we aim to explore novel machine learning (ML) techniques to facilitate and accelerate the construction of universal equation-Of-State (EOS) models with a high accuracy while ensuring important thermodynamic consistency. When applying ML to fit a universal EOS model, there are two key requirements: (1) a high prediction accuracy to ensure precise estimation of relevant physics properties and (2) physical interpretability to support important physics-related downstream applications. We first identify a set of fundamental challenges from the accuracy perspective, including an extremely wide range of input/output space and highly sparse training data. We demonstrate that while a neural network (NN) model may fit the EOS data well, the black-box nature makes it difficult to provide physically interpretable results, leading to weak accountability of prediction results outside the training range and lack of guarantee to meet important thermodynamic consistency constraints. To this end, we propose a principled deep regression model that can be trained following a meta-learning style to predict the desired quantities with a high accuracy using scarce training data. We further introduce a uniquely designed kernel-based regularizer for accurate uncertainty quantification. An ensemble technique is leveraged to battle model overfitting with improved prediction stability. Auto-differentiation is conducted to verify that necessary thermodynamic consistency conditions are maintained. Our evaluation results show an excellent fit of the EOS table and the predicted values are ready to use for important physics-related tasks.},
  archive      = {J_MLST},
  author       = {Dayou Yu and Deep Shankar Pandey and Joshua Hinz and Deyan Mihaylov and Valentin V Karasiev and S X Hu and Qi Yu},
  doi          = {10.1088/2632-2153/ad2626},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015031},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deep energy-pressure regression for a thermodynamically consistent EOS model},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Functional data learning using convolutional neural
networks. <em>MLST</em>, <em>5</em>(1), 015030. (<a
href="https://doi.org/10.1088/2632-2153/ad2627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we show how convolutional neural networks (CNNs) can be used in regression and classification learning problems for noisy and non-noisy functional data (FD). The main idea is to transform the FD into a 28 by 28 image. We use a specific but typical architecture of a CNN to perform all the regression exercises of parameter estimation and functional form classification. First, we use some functional case studies of FD with and without random noise to showcase the strength of the new method. In particular, we use it to estimate exponential growth and decay rates, the bandwidths of sine and cosine functions, and the magnitudes and widths of curve peaks. We also use it to classify the monotonicity and curvatures of FD, the algebraic versus exponential growth, and the number of peaks of FD. Second, we apply the same CNNs to Lyapunov exponent estimation in noisy and non-noisy chaotic data, in estimating rates of disease transmission from epidemic curves, and in detecting the similarity of drug dissolution profiles. Finally, we apply the method to real-life data to detect Parkinson&#39;s disease patients in a classification problem. We performed ablation analysis and compared the new method with other commonly used neural networks for FD and showed that it outperforms them in all applications. Although simple, the method shows high accuracy and is promising for future use in engineering and medical applications.},
  archive      = {J_MLST},
  author       = {J Galarza and T Oraby},
  doi          = {10.1088/2632-2153/ad2627},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015030},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Functional data learning using convolutional neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep quantum graph dreaming: Deciphering neural network
insights into quantum experiments. <em>MLST</em>, <em>5</em>(1), 015029.
(<a href="https://doi.org/10.1088/2632-2153/ad2628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite their promise to facilitate new scientific discoveries, the opaqueness of neural networks presents a challenge in interpreting the logic behind their findings. Here, we use a eXplainable-AI technique called inception or deep dreaming , which has been invented in machine learning for computer vision. We use this technique to explore what neural networks learn about quantum optics experiments. Our story begins by training deep neural networks on the properties of quantum systems. Once trained, we &#39;invert&#39; the neural network—effectively asking how it imagines a quantum system with a specific property, and how it would continuously modify the quantum system to change a property. We find that the network can shift the initial distribution of properties of the quantum system, and we can conceptualize the learned strategies of the neural network. Interestingly, we find that, in the first layers, the neural network identifies simple properties, while in the deeper ones, it can identify complex quantum structures and even quantum entanglement. This is in reminiscence of long-understood properties known in computer vision, which we now identify in a complex natural science task. Our approach could be useful in a more interpretable way to develop new advanced AI-based scientific discovery techniques in quantum physics.},
  archive      = {J_MLST},
  author       = {Tareq Jaouni and Sören Arlt and Carlos Ruiz-Gonzalez and Ebrahim Karimi and Xuemei Gu and Mario Krenn},
  doi          = {10.1088/2632-2153/ad2628},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015029},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deep quantum graph dreaming: Deciphering neural network insights into quantum experiments},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning cosmic ray transport from density maps of
simulated, turbulent gas. <em>MLST</em>, <em>5</em>(1), 015028. (<a
href="https://doi.org/10.1088/2632-2153/ad262a">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coarse-grained propagation of galactic cosmic rays (CRs) is traditionally constrained by phenomenological models of Milky Way CR propagation fit to a variety of direct and indirect observables; however, constraining the fine-grained transport of CRs along individual magnetic field lines—for instance, diffusive vs streaming transport models—is an unsolved challenge. Leveraging a recent training set of magnetohydrodynamic turbulent box simulations, with CRs spanning a range of transport parameters, we use convolutional neural networks (CNNs) trained solely on gas density maps to classify CR transport regimes. We find that even relatively simple CNNs can quite effectively classify density slices to corresponding CR transport parameters, distinguishing between streaming and diffusive transport, as well as magnitude of diffusivity, with class accuracies between 92% and 99%. As we show, the transport-dependent imprints that CRs leave on the gas are not all tied to the resulting density power spectra: classification accuracies are still high even when image spectra are flattened (85%–98% accuracy), highlighting CR transport-dependent changes to turbulent phase information. We interpret our results with saliency maps and image modifications, and we discuss physical insights and future applications.},
  archive      = {J_MLST},
  author       = {Chad Bustard and John Wu},
  doi          = {10.1088/2632-2153/ad262a},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015028},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deep learning cosmic ray transport from density maps of simulated, turbulent gas},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An open-source robust machine learning platform for
real-time detection and classification of 2D material flakes.
<em>MLST</em>, <em>5</em>(1), 015027. (<a
href="https://doi.org/10.1088/2632-2153/ad2287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most widely used method for obtaining high-quality two-dimensional (2D) materials is through mechanical exfoliation of bulk crystals. Manual identification of suitable flakes from the resulting random distribution of crystal thicknesses and sizes on a substrate is a time-consuming, tedious task. Here, we present a platform for fully automated scanning, detection, and classification of 2D materials, the source code of which we make openly available. Our platform is designed to be accurate, reliable, fast, and versatile in integrating new materials, making it suitable for everyday laboratory work. The implementation allows fully automated scanning and analysis of wafers with an average inference time of 100 ms for images of 2.3 Mpixels. The developed detection algorithm is based on a combination of the flakes&#39; optical contrast toward the substrate and their geometric shape. We demonstrate that it is able to detect the majority of exfoliated flakes of various materials, with an average recall (AR50) between 67% and 89%. We also show that the algorithm can be trained with as few as five flakes of a given material, which we demonstrate for the examples of few-layer graphene, WSe 2 , MoSe 2 , CrI 3 , 1T-TaS 2 and hexagonal BN. Our platform has been tested over a two-year period, during which more than 10 6 images of multiple different materials were acquired by over 30 individual researchers.},
  archive      = {J_MLST},
  author       = {Jan-Lucas Uslu and Taoufiq Ouaj and David Tebbe and Alexey Nekrasov and Jo Henri Bertram and Marc Schütte and Kenji Watanabe and Takashi Taniguchi and Bernd Beschoten and Lutz Waldecker and Christoph Stampfer},
  doi          = {10.1088/2632-2153/ad2287},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015027},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {An open-source robust machine learning platform for real-time detection and classification of 2D material flakes},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Qualitative and quantitative enhancement of parameter
estimation for model-based diagnostics using automatic differentiation
with an application to inertial fusion. <em>MLST</em>, <em>5</em>(1),
015026. (<a href="https://doi.org/10.1088/2632-2153/ad2493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameter estimation using observables is a fundamental concept in the experimental sciences. Mathematical models that represent the physical processes can enable reconstructions of the experimental observables and greatly assist in parameter estimation by turning it into an optimization problem which can be solved by gradient-free or gradient-based methods. In this work, the recent rise in flexible frameworks for developing differentiable scientific computing programs is leveraged in order to dramatically accelerate data analysis of a common experimental diagnostic relevant to laser–plasma and inertial fusion experiments, Thomson scattering. A differentiable Thomson-scattering data analysis tool is developed that uses reverse-mode automatic differentiation (AD) to calculate gradients. By switching from finite differencing to reverse-mode AD, three distinct outcomes are achieved. First, gradient descent is accelerated dramatically to the extent that it enables near real-time usage in laser–plasma experiments. Second, qualitatively novel quantities which require \mathcal{O}(10^3) parameters can now be included in the analysis of data which enables unprecedented measurements of small-scale laser–plasma phenomena. Third, uncertainty estimation approaches that leverage the value of the Hessian become accurate and efficient because reverse-mode AD can be used for calculating the Hessian.},
  archive      = {J_MLST},
  author       = {A L Milder and A S Joglekar and W Rozmus and D H Froula},
  doi          = {10.1088/2632-2153/ad2493},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015026},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Qualitative and quantitative enhancement of parameter estimation for model-based diagnostics using automatic differentiation with an application to inertial fusion},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MS2OD: Outlier detection using minimum spanning tree and
medoid selection. <em>MLST</em>, <em>5</em>(1), 015025. (<a
href="https://doi.org/10.1088/2632-2153/ad2492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an essential task in data mining, outlier detection identifies abnormal patterns in numerous applications, among which clustering-based outlier detection is one of the most popular methods for its effectiveness in detecting cluster-related outliers, especially in medical applications. This article presents an advanced method to extract cluster-based outliers by employing a scaled minimum spanning tree (MST) data structure and a new medoid selection method: 1. we compute a scaled MST and iteratively cut the current longest edge to obtain clusters; 2. we apply a new medoid selection method, considering the noise effect to improve the quality of cluster-based outlier identification. The experimental results on real-world data, including extensive medical corpora and other semantically meaningful datasets, demonstrate the wide applicability and outperforming metrics of the proposed method.},
  archive      = {J_MLST},
  author       = {Jia Li and Jiangwei Li and Chenxu Wang and Fons J Verbeek and Tanja Schultz and Hui Liu},
  doi          = {10.1088/2632-2153/ad2492},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015025},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {MS2OD: Outlier detection using minimum spanning tree and medoid selection},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ATSFCNN: A novel attention-based triple-stream fused CNN
model for hyperspectral image classification. <em>MLST</em>,
<em>5</em>(1), 015024. (<a
href="https://doi.org/10.1088/2632-2153/ad1d05">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the convolutional neural network (CNN) has gained increasing importance in hyperspectral image (HSI) classification thanks to its superior performance. However, most of the previous research has mainly focused on 2D-CNN, and the limited applications of 3D-CNN have been attributed to its complexity, despite its potential to enhance information extraction between adjacent channels of the image. Moreover, 1D-CNN is typically restricted to the field of signal processing as it ignores the spatial information of HSIs. In this paper, we propose a novel CNN model named attention-based triple-stream fused CNN (ATSFCNN) that fuses the features of 1D-CNN, 2D-CNN, and 3D-CNN to consider all the relevant information of the hyperspectral dataset. Our contributions are twofold: First, we propose a strategy to extract and homogenize features from 1D, 2D, and 3D CNN. Secondly, we propose a way to efficiently fuse these features. This attention-based methodology adeptly integrates features from the triple streams, thereby transcending the former limitations of singular stream utilization. Consequently, it becomes capable of attaining elevated outcomes in the context of hyperspectral classification, marked by increased levels of both accuracy and stability. We compared the results of ATSFCNN with those of other deep learning models, including 1D-CNN, 2D-CNN, 2D-CNN+PCA, 3D-CNN, and 3D-CNN+PCA, and demonstrated its superior performance and robustness. Quantitative assessments, predicated on the metrics of overall accuracy (OA), average accuracy (AA), and kappa coefficient ( κ ) emphatically corroborate the preeminence of ATSFCNN. Notably, spanning the three remote sensing datasets, ATSFCNN consistently achieves peak levels of OA, quantified at 98.38%, 97.09%, and 96.93% respectively. This prowess is further accentuated by concomitant AA scores of 98.47%, 95.80%, and 95.80%, as well as kappa coefficient values amounting to 97.41%, 96.14%, and 95.21%.},
  archive      = {J_MLST},
  author       = {Jizhen Cai and Clotilde Boust and Alamin Mansouri},
  doi          = {10.1088/2632-2153/ad1d05},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015024},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {ATSFCNN: A novel attention-based triple-stream fused CNN model for hyperspectral image classification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Circumventing data imbalance in magnetic ground state data
for magnetic moment predictions. <em>MLST</em>, <em>5</em>(1), 015023.
(<a href="https://doi.org/10.1088/2632-2153/ad23fb">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic materials play a crucial role in the transition to more sustainable forms of energy and electric vehicles. There is an anticipated shortage in magnetic materials in the future, and as a result there is an urgent need to discover and design new magnetic materials. Computational magnetic material design using density functional theory is daunting because of the challenge in identifying magnetic ground states from a combinatorially large set of possibilities. Machine learning offers a path forward by enabling efficient surrogate models that can more readily enumerate these states, but there is a dearth of training data available, and what is available tends to be imbalanced with too much non-magnetic data. In this work we show that the discrete and previously tackled data imbalance that exists at the level of the magnetic ordering leads to an imbalanced continuous distribution with many zeros when the data is unraveled at the atomic magnetic moment level, which subsequently leads to models with low accuracy for magnetic properties. We mitigate this by using a two-part model framework. Our scheme is able to classify atoms into magnetic and non-magnetic with an F1 score and Matthew&#39;s correlation coefficient (MCC) of ~91% and then to provide an implicit embedding representation that maps directly onto the magnitude of the magnetic moment with a mean absolute error of 0.1 \mu_{\text{B}} . Beyond screening for new magnetic materials, we demonstrate an additional practical use case of our scheme: the provision of good initial guesses for magnetic moments in first-principles electronic relaxations. Such initialization is shown to lead to faster convergence to configurations that lie closer to the ground state.},
  archive      = {J_MLST},
  author       = {Rohan Yuri Sanspeur and John R Kitchin},
  doi          = {10.1088/2632-2153/ad23fb},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015023},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Circumventing data imbalance in magnetic ground state data for magnetic moment predictions},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse optical flow outliers elimination method based on
borda stochastic neighborhood graph. <em>MLST</em>, <em>5</em>(1),
015022. (<a href="https://doi.org/10.1088/2632-2153/ad1a50">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the tracking of moving targets in dynamic scenes, efficiently handling outliers in the optical flow and maintaining robustness across various motion amplitudes represents a critical challenge. So far, studies have used thresholding and local consistency based approaches to deal with optical outliers. However, there is subjectivity through expert-defined thresholds or delineated regions, and therefore these methods do not perform consistently enough under different target motion amplitudes. Other studies have focused on complex statistical-mathematical modeling which, although theoretically valid, requires significant computational resources. Aiming at the above problems this paper proposes a new method to calculate the optical outliers by using stochastic neighborhood graph combined with the Borda counting method, which reduces the computation amount on the basis of objectively eliminating the outliers. Sparse optical flow (SOF) values are used as the overall population and the outlier and inlier SOF values are used as samples. Analyze the dissimilarity between SOF data points, obtaining the dissimilarity matrix, introducing the Gaussian function to smooth and reduce the dimensionality of the dissimilarity matrix, and then normalizing the smoothing matrix to generate the binding matrix, where the probability sum of each node to other nodes in the matrix is equal to 1. Stochastic neighborhood graphs are then generated based on a binding matrix to obtain the outlier probabilities of data points in different neighborhood graphs, and outlier samples are obtained based on the probability. To avoid the subjectivity of the expert thresholds, the outlier probabilities are weighted and ranked to calculate the data point Borda scores to obtain accurate optical outliers. The experimental results show that the method in this paper is robust to different amplitude motions and real scenarios, and the accuracy, precision and recall of outliers elimination are better than the current mainstream algorithms.},
  archive      = {J_MLST},
  author       = {Yifan Wang and Yang Li and Jiaqi Wang and Haofeng Lv and Jinshi Guo},
  doi          = {10.1088/2632-2153/ad1a50},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015022},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Sparse optical flow outliers elimination method based on borda stochastic neighborhood graph},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving cross-subject classification performance of motor
imagery signals: A data augmentation-focused deep learning framework.
<em>MLST</em>, <em>5</em>(1), 015021. (<a
href="https://doi.org/10.1088/2632-2153/ad200c">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motor imagery brain-computer interfaces (MI-BCIs) have gained a lot of attention in recent years thanks to their potential to enhance rehabilitation and control of prosthetic devices for individuals with motor disabilities. However, accurate classification of motor imagery signals remains a challenging task due to the high inter-subject variability and non-stationarity in the electroencephalogram (EEG) data. In the context of MI-BCIs, with limited data availability, the acquisition of EEG data can be difficult. In this study, several data augmentation techniques have been compared with the proposed data augmentation technique adaptive cross-subject segment replacement (ACSSR). This technique, in conjunction with the proposed deep learning framework, allows for a combination of similar subject pairs to take advantage of one another and boost the classification performance of MI-BCIs. The proposed framework features a multi-domain feature extractor based on common spatial patterns with a sliding window and a parallel two-branch convolutional neural network. The performance of the proposed methodology has been evaluated on the multi-class BCI Competition IV Dataset 2a through repeated 10-fold cross-validation. Experimental results indicated that the implementation of the ACSSR method (80.47%) in the proposed framework has led to a considerable improvement in the classification performance compared to the classification without data augmentation (77.63%), and other fundamental data augmentation techniques used in the literature. The study contributes to the advancements for the development of effective MI-BCIs by showcasing the ability of the ACSSR method to address the challenges in motor imagery signal classification tasks.},
  archive      = {J_MLST},
  author       = {Enes Ozelbas and Emine Elif Tülay and Serhat Ozekes},
  doi          = {10.1088/2632-2153/ad200c},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015021},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Improving cross-subject classification performance of motor imagery signals: A data augmentation-focused deep learning framework},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Amortized simulation-based frequentist inference for
tractable and intractable likelihoods. <em>MLST</em>, <em>5</em>(1),
015020. (<a href="https://doi.org/10.1088/2632-2153/ad218e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-fidelity simulators that connect theoretical models with observations are indispensable tools in many sciences. If the likelihood is known, inference can proceed using standard techniques. However, when the likelihood is intractable or unknown, a simulator makes it possible to infer the parameters of a theoretical model directly from real and simulated observations when coupled with machine learning. We introduce an extension of the recently proposed likelihood-free frequentist inference ( LF2I ) approach that makes it possible to construct confidence sets with the p -value function and to use the same function to check the coverage explicitly at any given parameter point. Like LF2I , this extension yields provably valid confidence sets in parameter inference problems for which a high-fidelity simulator is available. The utility of our algorithm is illustrated by applying it to three pedagogically interesting examples: the first is from cosmology, the second from high-energy physics and astronomy, both with tractable likelihoods, while the third, with an intractable likelihood, is from epidemiology 3 .},
  archive      = {J_MLST},
  author       = {Ali Al Kadhim and Harrison B Prosper and Olivia F Prosper},
  doi          = {10.1088/2632-2153/ad218e},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015020},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Amortized simulation-based frequentist inference for tractable and intractable likelihoods},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ML-based regionalization of climate variables to forecast
seasonal precipitation for water resources management. <em>MLST</em>,
<em>5</em>(1), 015019. (<a
href="https://doi.org/10.1088/2632-2153/ad1d04">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous dams and reservoirs have been constructed in South Korea, considering the distribution of seasonal precipitation which highly deviates from the actual one with high precipitation amount in summer and very low amount in other seasons. These water-related structures should be properly managed in order to meet seasonal demands of water resources wherein the forecasting of seasonal precipitation plays a critical role. However, owing to the impact of diverse complex weather systems, seasonal precipitation forecasting has been a challenging task. The current study proposes a novel procedure for forecasting seasonal precipitation by: (1) regionalizing the influential climate variables to the seasonal precipitation with k -means clustering; (2) extracting the features from the regionalized climate variables with machine learning-based algorithms such as principal component analysis (PCA), independent component analysis (ICA), and Autoencoder; and (3) finally regressing the extracted features with one linear model of generalized linear model (GLM) and another nonlinear model of support vector machine (SVM). Two globally gridded climate variables-mean sea level pressure (MSLP) and sea surface temperature (SST)-were teleconnected with the seasonal precipitation of South Korea, denoted as accumulated seasonal precipitation (ASP). Results indicated that k -means clustering successfully regionalized the highly correlated climate variables with the ASP, and all three extraction algorithms-PCA, ICA, and Autoencoder-combined with the GLM and SVM models presented their superiority in different seasons. In particular, the PCA combined with the linear GLM model performed better, and the Autoencoder combined with the nonlinear SVM model did better. It can be concluded that the proposed forecasting procedure of the seasonal precipitation, combined with several ML-based algorithms, can be a good alternative.},
  archive      = {J_MLST},
  author       = {Taesam Lee and Chang-Hee Won and Vijay P Singh},
  doi          = {10.1088/2632-2153/ad1d04},
  journal      = {Machine Learning: Science and Technology},
  month        = {1},
  number       = {1},
  pages        = {015019},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {ML-based regionalization of climate variables to forecast seasonal precipitation for water resources management},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoRe optimizer: An all-in-one solution for machine learning.
<em>MLST</em>, <em>5</em>(1), 015018. (<a
href="https://doi.org/10.1088/2632-2153/ad1f76">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The optimization algorithm and its hyperparameters can significantly affect the training speed and resulting model accuracy in machine learning (ML) applications. The wish list for an ideal optimizer includes fast and smooth convergence to low error, low computational demand, and general applicability. Our recently introduced continual resilient (CoRe) optimizer has shown superior performance compared to other state-of-the-art first-order gradient-based optimizers for training lifelong ML potentials. In this work we provide an extensive performance comparison of the CoRe optimizer and nine other optimization algorithms including the Adam optimizer and resilient backpropagation (RPROP) for diverse ML tasks. We analyze the influence of different hyperparameters and provide generally applicable values. The CoRe optimizer yields best or competitive performance in every investigated application, while only one hyperparameter needs to be changed depending on mini-batch or batch learning.},
  archive      = {J_MLST},
  author       = {Marco Eckhoff and Markus Reiher},
  doi          = {10.1088/2632-2153/ad1f76},
  journal      = {Machine Learning: Science and Technology},
  month        = {1},
  number       = {1},
  pages        = {015018},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {CoRe optimizer: An all-in-one solution for machine learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ELUQuant: Event-level uncertainty quantification in deep
inelastic scattering. <em>MLST</em>, <em>5</em>(1), 015017. (<a
href="https://doi.org/10.1088/2632-2153/ad2098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a physics-informed Bayesian neural network with flow-approximated posteriors using multiplicative normalizing flows for detailed uncertainty quantification (UQ) at the physics event-level. Our method is capable of identifying both heteroskedastic aleatoric and epistemic uncertainties, providing granular physical insights. Applied to deep inelastic scattering (DIS) events, our model effectively extracts the kinematic variables x , Q 2 , and y , matching the performance of recent deep learning regression techniques but with the critical enhancement of event-level UQ. This detailed description of the underlying uncertainty proves invaluable for decision-making, especially in tasks like event filtering. It also allows for the reduction of true inaccuracies without directly accessing the ground truth. A thorough DIS simulation using the H1 detector at HERA indicates possible applications for the future electron–ion collider. Additionally, this paves the way for related tasks such as data quality monitoring and anomaly detection. Remarkably, our approach effectively processes large samples at high rates.},
  archive      = {J_MLST},
  author       = {C Fanelli and J Giroux},
  doi          = {10.1088/2632-2153/ad2098},
  journal      = {Machine Learning: Science and Technology},
  month        = {1},
  number       = {1},
  pages        = {015017},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {ELUQuant: Event-level uncertainty quantification in deep inelastic scattering},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variance extrapolation method for neural-network variational
monte carlo. <em>MLST</em>, <em>5</em>(1), 015016. (<a
href="https://doi.org/10.1088/2632-2153/ad1f75">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constructing more expressive ansatz has been a primary focus for quantum Monte Carlo, aimed at more accurate ab initio calculations. However, with more powerful ansatz, e.g. various recent developed models based on neural-network architectures, the training becomes more difficult and expensive, which may have a counterproductive effect on the accuracy of calculation. In this work, we propose to make use of the training data to perform empirical variance extrapolation when using neural-network ansatz in variational Monte Carlo. We show that this approach can speed up the convergence and surpass the ansatz limitation to obtain an improved estimation of the energy. Moreover, variance extrapolation greatly enhances the error cancellation capability, resulting in significantly improved relative energy outcomes, which are the keys to chemistry and physics problems.},
  archive      = {J_MLST},
  author       = {Weizhong Fu and Weiluo Ren and Ji Chen},
  doi          = {10.1088/2632-2153/ad1f75},
  journal      = {Machine Learning: Science and Technology},
  month        = {1},
  number       = {1},
  pages        = {015016},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Variance extrapolation method for neural-network variational monte carlo},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Phase transitions in the mini-batch size for sparse and
dense two-layer neural networks. <em>MLST</em>, <em>5</em>(1), 015015.
(<a href="https://doi.org/10.1088/2632-2153/ad1de6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of mini-batches of data in training artificial neural networks is nowadays very common. Despite its broad usage, theories explaining quantitatively how large or small the optimal mini-batch size should be are missing. This work presents a systematic attempt at understanding the role of the mini-batch size in training two-layer neural networks. Working in the teacher-student scenario, with a sparse teacher, and focusing on tasks of different complexity, we quantify the effects of changing the mini-batch size m . We find that often the generalization performances of the student strongly depend on m and may undergo sharp phase transitions at a critical value m_\textrm{c} , such that for m\lt m_\textrm{c} the training process fails, while for m\gt m_\textrm{c} the student learns perfectly or generalizes very well the teacher. Phase transitions are induced by collective phenomena firstly discovered in statistical mechanics and later observed in many fields of science. Observing a phase transition by varying the mini-batch size across different architectures raises several questions about the role of this hyperparameter in the neural network learning process.},
  archive      = {J_MLST},
  author       = {Raffaele Marino and Federico Ricci-Tersenghi},
  doi          = {10.1088/2632-2153/ad1de6},
  journal      = {Machine Learning: Science and Technology},
  month        = {1},
  number       = {1},
  pages        = {015015},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Phase transitions in the mini-batch size for sparse and dense two-layer neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing collective behavior of communicating active
particles with machine learning. <em>MLST</em>, <em>5</em>(1), 015014.
(<a href="https://doi.org/10.1088/2632-2153/ad1c33">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bacteria and other self-propelling microorganisms produce and respond to signaling molecules to communicate with each other (quorum sensing) and to direct their collective behavior. Here, we explore agents (active particles) which communicate with each other to coordinate their collective dynamics for maximizing nutrient consumption. Using reinforcement learning and neural networks, we identify three different strategies: a &#39;clustering strategy&#39;, where the agents accumulate in regions of high nutrient concentration; a &#39;spreading strategy&#39;, where particles stay away from each other to avoid competing for sparse resources; and an &#39;adaptive strategy&#39;, where the agents adaptively decide to either follow or stay away from others. Our work exemplifies the idea that machine learning can be used to determine parameters that are evolutionarily optimized in biological systems but often occur as unknown parameters in mathematical models describing their dynamics.},
  archive      = {J_MLST},
  author       = {Jens Grauer and Fabian Jan Schwarzendahl and Hartmut Löwen and Benno Liebchen},
  doi          = {10.1088/2632-2153/ad1c33},
  journal      = {Machine Learning: Science and Technology},
  month        = {1},
  number       = {1},
  pages        = {015014},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Optimizing collective behavior of communicating active particles with machine learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). <span
class="math inline"><em>δ</em><em>A</em><em>R</em><em>D</em></span> loss
for low-contrast medical image segmentation. <em>MLST</em>,
<em>5</em>(1), 015013. (<a
href="https://doi.org/10.1088/2632-2153/ad1d06">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation is essential to image-based disease analysis and has proven to be significantly helpful for doctors to make decisions. Due to the low-contrast of some medical images, the accurate segmentation of medical images has always been a challenging problem. The experiment found that UNet with current loss functions cannot capture subtle information in target contours or regions in low-contrast medical images, which are crucial for subsequent disease diagnosis. We propose a robust loss by incorporating the difference in average radial derivative (ARD), length and region area to further help the network to achieve more accurate segmentation results. We evaluated the proposed loss function using UNet as the base segmentation network compared to five conventional loss functions on one private and four public medical image datasets. Experimental results illustrate that UNet with the proposed loss function can achieve the best segmentation performance, even better than the outstanding deep learning models with original loss functions. Furthermore, three representative datasets were chosen to validate the effectiveness of the proposed δ ARD loss function with seven different models. The experiments revealed δ ARD loss&#39;s plug-and-play feature and its robustness over multiple models and datasets.},
  archive      = {J_MLST},
  author       = {Yu Zhao and Xiaoyan Shen and Jiadong Chen and Wei Qian and He Ma and Liang Sang},
  doi          = {10.1088/2632-2153/ad1d06},
  journal      = {Machine Learning: Science and Technology},
  month        = {1},
  number       = {1},
  pages        = {015013},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {$\delta ARD$ loss for low-contrast medical image segmentation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep kernel methods learn better: From cards to process
optimization. <em>MLST</em>, <em>5</em>(1), 015012. (<a
href="https://doi.org/10.1088/2632-2153/ad1a4f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability of deep learning methods to perform classification and regression tasks relies heavily on their capacity to uncover manifolds in high-dimensional data spaces and project them into low-dimensional representation spaces. In this study, we investigate the structure and character of the manifolds generated by classical variational autoencoder (VAE) approaches and deep kernel learning (DKL). In the former case, the structure of the latent space is determined by the properties of the input data alone, while in the latter, the latent manifold forms as a result of an active learning process that balances the data distribution and target functionalities. We show that DKL with active learning can produce a more compact and smooth latent space which is more conducive to optimization compared to previously reported methods, such as the VAE. We demonstrate this behavior using a simple cards dataset and extend it to the optimization of domain-generated trajectories in physical systems. Our findings suggest that latent manifolds constructed through active learning have a more beneficial structure for optimization problems, especially in feature-rich target-poor scenarios that are common in domain sciences, such as materials synthesis, energy storage, and molecular discovery. The Jupyter Notebooks that encapsulate the complete analysis accompany the article.},
  archive      = {J_MLST},
  author       = {Mani Valleti and Rama K Vasudevan and Maxim A Ziatdinov and Sergei V Kalinin},
  doi          = {10.1088/2632-2153/ad1a4f},
  journal      = {Machine Learning: Science and Technology},
  month        = {1},
  number       = {1},
  pages        = {015012},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deep kernel methods learn better: From cards to process optimization},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-stage machine learning algorithm for estimating
personal dose equivalent using thermoluminescent dosimeter.
<em>MLST</em>, <em>5</em>(1), 015011. (<a
href="https://doi.org/10.1088/2632-2153/ad1c31">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the present age, marked by data-driven advancements in various fields, the importance of machine learning (ML) holds a prominent position. The ability of ML algorithms to resolve complex patterns and extract insights from large datasets has solidified its transformative potential in various scientific domains. This paper introduces an innovative application of ML techniques in the domain of radiation dosimetry. Specifically, it shows the applicability of ML in estimating the radiation dose received by occupational workers. This estimation is expressed in terms of personal dose equivalent, and it involves the utilization of thermoluminescence signals emitted by CaSO 4 :Dy-based personnel monitoring badges. To estimate personal dose equivalent, three-stage algorithm driven by ML models is proposed. This algorithm systematically identifies the photon energy ranges, calculates the average photon energy, and determines personal dose equivalent. By implementing this approach to the conventional three-element dosimeter, the study overcomes existing limitations and enhances accuracy in dose estimation. The algorithm demonstrates 97.8% classification accuracy in discerning photon energy ranges and achieves a coefficient of determination of 0.988 for estimating average photon energy. Importantly, it also reduces the coefficient of variation of relative deviations by up to 6% for estimated personal dose equivalent, compared to existing algorithms. The study improves accuracy and establishes a new methodology for evaluating radiation exposure to occupational workers using conventional thermoluminescent dosimeter badge.},
  archive      = {J_MLST},
  author       = {Munir S Pathan and S M Pradhan and T Palani Selvam and B K Sapra},
  doi          = {10.1088/2632-2153/ad1c31},
  journal      = {Machine Learning: Science and Technology},
  month        = {1},
  number       = {1},
  pages        = {015011},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A multi-stage machine learning algorithm for estimating personal dose equivalent using thermoluminescent dosimeter},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mud-net: Multi-domain deep unrolling network for
simultaneous sparse-view and metal artifact reduction in computed
tomography. <em>MLST</em>, <em>5</em>(1), 015010. (<a
href="https://doi.org/10.1088/2632-2153/ad1b8e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse-view computed tomography (SVCT) is regarded as a promising technique to accelerate data acquisition and reduce radiation dose. However, in the presence of metallic implants, SVCT inevitably makes the reconstructed CT images suffer from severe metal artifacts and streaking artifacts due to the lack of sufficient projection data. Previous stand-alone SVCT and metal artifact reduction (MAR) methods to solve the problem of simultaneously sparse-view and metal artifact reduction (SVMAR) are plagued by insufficient correction accuracy. To overcome this limitation, we propose a multi-domain deep unrolling network, called Mud-Net, for SVMAR. Specifically, we establish a joint sinogram, image, artifact, and coding domains deep unrolling reconstruction model to recover high-quality CT images from the under-sampled sinograms corrupted by metallic implants. To train this multi-domain network effectively, we embed multi-domain knowledge into the network training process. Comprehensive experiments demonstrate that our method is superior to both existing MAR methods in the full-view MAR task and previous SVCT methods in the SVMAR task.},
  archive      = {J_MLST},
  author       = {Baoshun Shi and Ke Jiang and Shaolei Zhang and Qiusheng Lian and Yanwei Qin and Yunsong Zhao},
  doi          = {10.1088/2632-2153/ad1b8e},
  journal      = {Machine Learning: Science and Technology},
  month        = {1},
  number       = {1},
  pages        = {015010},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Mud-net: Multi-domain deep unrolling network for simultaneous sparse-view and metal artifact reduction in computed tomography},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Application of autoencoders artificial neural network and
principal component analysis for pattern extraction and spatial
regionalization of global temperature data. <em>MLST</em>,
<em>5</em>(1), 015009. (<a
href="https://doi.org/10.1088/2632-2153/ad1c34">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial regionalization is instrumental in simplifying the spatial complexity of the climate system. To identify regions of significant climate variability, pattern extraction is often required prior to spatial regionalization with a clustering algorithm. In this study, the autoencoder (AE) artificial neural network was applied to extract the inherent patterns of global temperature data (from 1901 to 2021). Subsequently, Fuzzy C-means clustering was applied to the extracted patterns to classify the global temperature regions. Our analysis involved comparing AE-based and principal component analysis (PCA)-based clustering results to assess consistency. We determined the number of clusters by examining the average percentage decrease in Fuzzy Partition Coefficient (FPC) and its 95% confidence interval, seeking a balance between obtaining a high FPC and avoiding over-segmentation. This approach suggested that for a more general model, four clusters is reasonable. The Adjusted Rand Index between the AE-based and PCA-based clusters is 0.75, indicating that the AE-based and PCA-based clusters have considerable overlap. The observed difference between the AE-based clusters and PCA-based clusters is suggested to be associated with AE&#39;s capability to learn and extract complex non-linear patterns, and this attribute, for example, enabled the clustering algorithm to accurately detect the Himalayas region as the &#39;third pole&#39; with similar temperature characteristics as the polar regions. Finally, when the analysis period is divided into two (1901–1960 and 1961–2021), the Adjusted Rand Index between the two clusters is 0.96 which suggests that historical climate change has not significantly affected the defined temperature regions over the two periods. In essence, this study indicates both AE&#39;s potential to enhance our understanding of climate variability and reveals the stability of the historical temperature regions.},
  archive      = {J_MLST},
  author       = {Chibuike Chiedozie Ibebuchi and Omon A Obarein and Itohan-Osa Abu},
  doi          = {10.1088/2632-2153/ad1c34},
  journal      = {Machine Learning: Science and Technology},
  month        = {1},
  number       = {1},
  pages        = {015009},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Application of autoencoders artificial neural network and principal component analysis for pattern extraction and spatial regionalization of global temperature data},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evidence networks: Simple losses for fast, amortized, neural
bayesian model comparison. <em>MLST</em>, <em>5</em>(1), 015008. (<a
href="https://doi.org/10.1088/2632-2153/ad1a4d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evidence Networks can enable Bayesian model comparison when state-of-the-art methods (e.g. nested sampling) fail and even when likelihoods or priors are intractable or unknown. Bayesian model comparison, i.e. the computation of Bayes factors or evidence ratios, can be cast as an optimization problem. Though the Bayesian interpretation of optimal classification is well-known, here we change perspective and present classes of loss functions that result in fast, amortized neural estimators that directly estimate convenient functions of the Bayes factor. This mitigates numerical inaccuracies associated with estimating individual model probabilities. We introduce the leaky parity-odd power (l-POP) transform, leading to the novel &#39;l-POP-Exponential&#39; loss function. We explore neural density estimation for data probability in different models, showing it to be less accurate and scalable than Evidence Networks. Multiple real-world and synthetic examples illustrate that Evidence Networks are explicitly independent of dimensionality of the parameter space and scale mildly with the complexity of the posterior probability density function. This simple yet powerful approach has broad implications for model inference tasks. As an application of Evidence Networks to real-world data we compute the Bayes factor for two models with gravitational lensing data of the Dark Energy Survey. We briefly discuss applications of our methods to other, related problems of model comparison and evaluation in implicit inference settings.},
  archive      = {J_MLST},
  author       = {Niall Jeffrey and Benjamin D Wandelt},
  doi          = {10.1088/2632-2153/ad1a4d},
  journal      = {Machine Learning: Science and Technology},
  month        = {1},
  number       = {1},
  pages        = {015008},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Evidence networks: Simple losses for fast, amortized, neural bayesian model comparison},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-resolution imaging in acoustic microscopy using deep
learning. <em>MLST</em>, <em>5</em>(1), 015007. (<a
href="https://doi.org/10.1088/2632-2153/ad1c30">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acoustic microscopy is a cutting-edge label-free imaging technology that allows us to see the surface and interior structure of industrial and biological materials. The acoustic image is created by focusing high-frequency acoustic waves on the object and then detecting reflected signals. On the other hand, the quality of the acoustic image&#39;s resolution is influenced by the signal-to-noise ratio, the scanning step size, and the frequency of the transducer. Deep learning-based high-resolution imaging in acoustic microscopy is proposed in this paper. To illustrate four times resolution improvement in acoustic images, five distinct models are used: SRGAN, ESRGAN, IMDN, DBPN-RES-MR64-3, and SwinIR. The trained model&#39;s performance is assessed by calculating the PSNR (Peak Signal to Noise Ratio) and SSIM (Structural Similarity Index) between the network-predicted and ground truth images. To avoid the model from over-fitting, transfer learning was incorporated during the procedure. SwinIR had average SSIM and PSNR values of 0.95 and 35, respectively. The model was also evaluated using a biological sample from Reindeer Antler, yielding an SSIM score of 0.88 and a PSNR score of 32.93. Our framework is relevant to a wide range of industrial applications, including electronic production, material micro-structure analysis, and other biological applications in general.},
  archive      = {J_MLST},
  author       = {Pragyan Banerjee and Shivam Milind Akarte and Prakhar Kumar and Muhammad Shamsuzzaman and Ankit Butola and Krishna Agarwal and Dilip K Prasad and Frank Melandsø and Anowarul Habib},
  doi          = {10.1088/2632-2153/ad1c30},
  journal      = {Machine Learning: Science and Technology},
  month        = {1},
  number       = {1},
  pages        = {015007},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {High-resolution imaging in acoustic microscopy using deep learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning of crystalline defects from TEM images: A
solution for the problem of “never enough training data.” <em>MLST</em>,
<em>5</em>(1), 015006. (<a
href="https://doi.org/10.1088/2632-2153/ad1a4e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crystalline defects, such as line-like dislocations, play an important role for the performance and reliability of many metallic devices. Their interaction and evolution still poses a multitude of open questions to materials science and materials physics. In-situ transmission electron microscopy (TEM) experiments can provide important insights into how dislocations behave and move. The analysis of individual video frames from such experiments can provide useful insights but is limited by the capabilities of automated identification, digitization, and quantitative extraction of the dislocations as curved objects. The vast amount of data also makes manual annotation very time consuming, thereby limiting the use of deep learning (DL)-based, automated image analysis and segmentation of the dislocation microstructure. In this work, a parametric model for generating synthetic training data for segmentation of dislocations is developed. Even though domain scientists might dismiss synthetic images as artificial, our findings show that they can result in superior performance. Additionally, we propose an enhanced DL method optimized for segmenting overlapping or intersecting dislocation lines. Upon testing this framework on four distinct real datasets, we find that a model trained only on synthetic training data can also yield high-quality results on real images–even more so if the model is further fine-tuned on a few real images. Our approach demonstrates the potential of synthetic data in overcoming the limitations of manual annotation of TEM image data of dislocation microstructure, paving the way for more efficient and accurate analysis of dislocation microstructures. Last but not least, segmenting such thin, curvilinear structures is a task that is ubiquitous in many fields, which makes our method a potential candidate for other applications as well.},
  archive      = {J_MLST},
  author       = {Kishan Govind and Daniela Oliveros and Antonin Dlouhy and Marc Legros and Stefan Sandfeld},
  doi          = {10.1088/2632-2153/ad1a4e},
  journal      = {Machine Learning: Science and Technology},
  month        = {1},
  number       = {1},
  pages        = {015006},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deep learning of crystalline defects from TEM images: A solution for the problem of ‘never enough training data’},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discovering interpretable physical models using symbolic
regression and discrete exterior calculus. <em>MLST</em>, <em>5</em>(1),
015005. (<a href="https://doi.org/10.1088/2632-2153/ad1af2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational modeling is a key resource to gather insight into physical systems in modern scientific research and engineering. While access to large amount of data has fueled the use of machine learning to recover physical models from experiments and increase the accuracy of physical simulations, purely data-driven models have limited generalization and interpretability. To overcome these limitations, we propose a framework that combines symbolic regression (SR) and discrete exterior calculus (DEC) for the automated discovery of physical models starting from experimental data. Since these models consist of mathematical expressions, they are interpretable and amenable to analysis, and the use of a natural, general-purpose discrete mathematical language for physics favors generalization with limited input data. Importantly, DEC provides building blocks for the discrete analog of field theories, which are beyond the state-of-the-art applications of SR to physical problems. Further, we show that DEC allows to implement a strongly-typed SR procedure that guarantees the mathematical consistency of the recovered models and reduces the search space of symbolic expressions. Finally, we prove the effectiveness of our methodology by re-discovering three models of continuum physics from synthetic experimental data: Poisson equation, the Euler&#39;s elastica and the equations of linear elasticity. Thanks to their general-purpose nature, the methods developed in this paper may be applied to diverse contexts of physical modeling.},
  archive      = {J_MLST},
  author       = {Simone Manti and Alessandro Lucantonio},
  doi          = {10.1088/2632-2153/ad1af2},
  journal      = {Machine Learning: Science and Technology},
  month        = {1},
  number       = {1},
  pages        = {015005},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Discovering interpretable physical models using symbolic regression and discrete exterior calculus},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multipoint-BAX: A new approach for efficiently tuning
particle accelerator emittance via virtual objectives. <em>MLST</em>,
<em>5</em>(1), 015004. (<a
href="https://doi.org/10.1088/2632-2153/ad169f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although beam emittance is critical for the performance of high-brightness accelerators, optimization is often time limited as emittance calculations, commonly done via quadrupole scans, are typically slow. Such calculations are a type of multipoint query , i.e. each query requires multiple secondary measurements. Traditional black-box optimizers such as Bayesian optimization are slow and inefficient when dealing with such objectives as they must acquire the full series of measurements, but return only the emittance, with each query. We propose a new information-theoretic algorithm, Multipoint-BAX , for black-box optimization on multipoint queries, which queries and models individual beam-size measurements using techniques from Bayesian Algorithm Execution (BAX). Our method avoids the slow multipoint query on the accelerator by acquiring points through a virtual objective , i.e. calculating the emittance objective from a fast learned model rather than directly from the accelerator. We use Multipoint-BAX to minimize emittance at the Linac Coherent Light Source (LCLS) and the Facility for Advanced Accelerator Experimental Tests II (FACET-II). In simulation, our method is 20 × faster and more robust to noise compared to existing methods. In live tests, it matched the hand-tuned emittance at FACET-II and achieved a 24% lower emittance than hand-tuning at LCLS. Our method represents a conceptual shift for optimizing multipoint queries, and we anticipate that it can be readily adapted to similar problems in particle accelerators and other scientific instruments.},
  archive      = {J_MLST},
  author       = {Sara Ayoub Miskovich and Willie Neiswanger and William Colocho and Claudio Emma and Jacqueline Garrahan and Timothy Maxwell and Christopher Mayes and Stefano Ermon and Auralee Edelen and Daniel Ratner},
  doi          = {10.1088/2632-2153/ad169f},
  journal      = {Machine Learning: Science and Technology},
  month        = {1},
  number       = {1},
  pages        = {015004},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Multipoint-BAX: A new approach for efficiently tuning particle accelerator emittance via virtual objectives},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synthetic pre-training for neural-network interatomic
potentials. <em>MLST</em>, <em>5</em>(1), 015003. (<a
href="https://doi.org/10.1088/2632-2153/ad1626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) based interatomic potentials have transformed the field of atomistic materials modelling. However, ML potentials depend critically on the quality and quantity of quantum-mechanical reference data with which they are trained, and therefore developing datasets and training pipelines is becoming an increasingly central challenge. Leveraging the idea of &#39;synthetic&#39; (artificial) data that is common in other areas of ML research, we here show that synthetic atomistic data, themselves obtained at scale with an existing ML potential, constitute a useful pre-training task for neural-network (NN) interatomic potential models. Once pre-trained with a large synthetic dataset, these models can be fine-tuned on a much smaller, quantum-mechanical one, improving numerical accuracy and stability in computational practice. We demonstrate feasibility for a series of equivariant graph-NN potentials for carbon, and we carry out initial experiments to test the limits of the approach.},
  archive      = {J_MLST},
  author       = {John L A Gardner and Kathryn T Baker and Volker L Deringer},
  doi          = {10.1088/2632-2153/ad1626},
  journal      = {Machine Learning: Science and Technology},
  month        = {1},
  number       = {1},
  pages        = {015003},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Synthetic pre-training for neural-network interatomic potentials},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural network field theories: Non-gaussianity, actions, and
locality. <em>MLST</em>, <em>5</em>(1), 015002. (<a
href="https://doi.org/10.1088/2632-2153/ad17d3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both the path integral measure in field theory (FT) and ensembles of neural networks (NN) describe distributions over functions. When the central limit theorem can be applied in the infinite-width (infinite- N ) limit, the ensemble of networks corresponds to a free FT. Although an expansion in 1/N corresponds to interactions in the FT, others, such as in a small breaking of the statistical independence of network parameters, can also lead to interacting theories. These other expansions can be advantageous over the 1/N -expansion, for example by improved behavior with respect to the universal approximation theorem. Given the connected correlators of a FT, one can systematically reconstruct the action order-by-order in the expansion parameter, using a new Feynman diagram prescription whose vertices are the connected correlators. This method is motivated by the Edgeworth expansion and allows one to derive actions for NN FT. Conversely, the correspondence allows one to engineer architectures realizing a given FT by representing action deformations as deformations of NN parameter densities. As an example, φ 4 theory is realized as an infinite- N NN FT.},
  archive      = {J_MLST},
  author       = {Mehmet Demirtas and James Halverson and Anindita Maiti and Matthew D Schwartz and Keegan Stoner},
  doi          = {10.1088/2632-2153/ad17d3},
  journal      = {Machine Learning: Science and Technology},
  month        = {1},
  number       = {1},
  pages        = {015002},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Neural network field theories: Non-gaussianity, actions, and locality},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable representation learning of small quantum states.
<em>MLST</em>, <em>5</em>(1), 015001. (<a
href="https://doi.org/10.1088/2632-2153/ad16a0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised machine learning models build an internal representation of their training data without the need for explicit human guidance or feature engineering. This learned representation provides insights into which features of the data are relevant for the task at hand. In the context of quantum physics, training models to describe quantum states without human intervention offers a promising approach to gaining insight into how machines represent complex quantum states. The ability to interpret the learned representation may offer a new perspective on non-trivial features of quantum systems and their efficient representation. We train a generative model on two-qubit density matrices generated by a parameterized quantum circuit. In a series of computational experiments, we investigate the learned representation of the model and its internal understanding of the data. We observe that the model learns an interpretable representation which relates the quantum states to their underlying entanglement characteristics. In particular, our results demonstrate that the latent representation of the model is directly correlated with the entanglement measure concurrence. The insights from this study represent proof of concept toward interpretable machine learning of quantum states. Our approach offers insight into how machines learn to represent small-scale quantum systems autonomously.},
  archive      = {J_MLST},
  author       = {Felix Frohnert and Evert van Nieuwenburg},
  doi          = {10.1088/2632-2153/ad16a0},
  journal      = {Machine Learning: Science and Technology},
  month        = {1},
  number       = {1},
  pages        = {015001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Explainable representation learning of small quantum states},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ten years of generative adversarial nets (GANs): A survey of
the state-of-the-art. <em>MLST</em>, <em>5</em>(1), 011001. (<a
href="https://doi.org/10.1088/2632-2153/ad1f77">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial networks (GANs) have rapidly emerged as powerful tools for generating realistic and diverse data across various domains, including computer vision and other applied areas, since their inception in 2014. Consisting of a discriminative network and a generative network engaged in a minimax game, GANs have revolutionized the field of generative modeling. In February 2018, GAN secured the leading spot on the &#39;Top Ten Global Breakthrough Technologies List&#39; issued by the Massachusetts Science and Technology Review. Over the years, numerous advancements have been proposed, leading to a rich array of GAN variants, such as conditional GAN, Wasserstein GAN, cycle-consistent GAN, and StyleGAN, among many others. This survey aims to provide a general overview of GANs, summarizing the latent architecture, validation metrics, and application areas of the most widely recognized variants. We also delve into recent theoretical developments, exploring the profound connection between the adversarial principle underlying GAN and Jensen–Shannon divergence while discussing the optimality characteristics of the GAN framework. The efficiency of GAN variants and their model architectures will be evaluated along with training obstacles as well as training solutions. In addition, a detailed discussion will be provided, examining the integration of GANs with newly developed deep learning frameworks such as transformers, physics-informed neural networks, large language models, and diffusion models. Finally, we reveal several issues as well as future research outlines in this field.},
  archive      = {J_MLST},
  author       = {Tanujit Chakraborty and Ujjwal Reddy K S and Shraddha M Naik and Madhurima Panja and Bayapureddy Manvitha},
  doi          = {10.1088/2632-2153/ad1f77},
  journal      = {Machine Learning: Science and Technology},
  month        = {1},
  number       = {1},
  pages        = {011001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Ten years of generative adversarial nets (GANs): A survey of the state-of-the-art},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Redefining the north atlantic oscillation index generation
using autoencoder neural network. <em>MLST</em>, <em>5</em>(1), 01LT01.
(<a href="https://doi.org/10.1088/2632-2153/ad1c32">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the spatial patterns of the North Atlantic Oscillation (NAO) is vital for climate science. For this reason, empirical orthogonal function (EOF) analysis is commonly applied to sea-level pressure (SLP) anomaly data in the North Atlantic region. This study evaluated the traditional EOF-based definition of the NAO index against the autoencoder (AE) neural network-based definition, using the Hurrell NAO Index (Station-Based) as a reference. Specifically, EOF and AE were applied to monthly SLP anomaly data from ERA5 (1950–2022) to derive spatial modes of variability in the North Atlantic region. Both methods produced spatial patterns consistent with the traditional NAO definition, with dipole centers of action between the Icelandic Low and the Azores High. During boreal winter (December to March), when the NAO is most active, the AE-based method achieved a correlation of 0.96 with the reference NAO index, outperforming the EOF-based method&#39;s correlation of 0.90. The all-season Adjusted R-squared values were 50% for the AE-based index and 34% for the EOF-based index. Notably, the AE-based index revealed several other non-linear patterns of the NAO, with more than one encoded pattern correlating at least 0.90 with the reference NAO index during boreal winter. These results not only demonstrate the AE&#39;s superiority over traditional EOF in representing the station-based index but also uncover previously unexplored complexities in the NAO that are close to the reference temporal pattern. This suggests that AE offers a promising approach for defining climate modes of variability, potentially capturing intricacies that traditional linear methods like EOF might miss.},
  archive      = {J_MLST},
  author       = {Chibuike Chiedozie Ibebuchi},
  doi          = {10.1088/2632-2153/ad1c32},
  journal      = {Machine Learning: Science and Technology},
  month        = {1},
  number       = {1},
  pages        = {01LT01},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Redefining the north atlantic oscillation index generation using autoencoder neural network},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generating artificial displacement data of cracked specimen
using physics-guided adversarial networks. <em>MLST</em>, <em>4</em>(4),
045063. (<a href="https://doi.org/10.1088/2632-2153/ad15b2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital image correlation (DIC) has become a valuable tool to monitor and evaluate mechanical experiments of cracked specimen, but the automatic detection of cracks is often difficult due to inherent noise and artefacts. Machine learning models have been extremely successful in detecting crack paths and crack tips using DIC-measured, interpolated full-field displacements as input to a convolution-based segmentation model. Still, big data is needed to train such models. However, scientific data is often scarce as experiments are expensive and time-consuming. In this work, we present a method to directly generate large amounts of artificial displacement data of cracked specimen resembling real interpolated DIC displacements. The approach is based on generative adversarial networks (GANs). During training, the discriminator receives physical domain knowledge in the form of the derived von Mises equivalent strain. We show that this physics-guided approach leads to improved results in terms of visual quality of samples, sliced Wasserstein distance, and geometry score when compared to a classical unguided GAN approach.},
  archive      = {J_MLST},
  author       = {David Melching and Erik Schultheis and Eric Breitbarth},
  doi          = {10.1088/2632-2153/ad15b2},
  journal      = {Machine Learning: Science and Technology},
  month        = {1},
  number       = {4},
  pages        = {045063},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Generating artificial displacement data of cracked specimen using physics-guided adversarial networks},
  volume       = {4},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
