<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JRSSSC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jrsssc---69">JRSSSC - 69</h2>
<ul>
<li><details>
<summary>
(2024). Contents of volume 73, 2024. <em>JRSSSC</em>,
<em>73</em>(5), 1411–1414. (<a
href="https://doi.org/10.1093/jrsssc/qlae056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSC},
  doi          = {10.1093/jrsssc/qlae056},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {5},
  pages        = {1411-1414},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Contents of volume 73, 2024},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian valuation framework for catastrophe bonds.
<em>JRSSSC</em>, <em>73</em>(5), 1389–1410. (<a
href="https://doi.org/10.1093/jrsssc/qlae041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Catastrophe (CAT) bond markets are incomplete and hence carry uncertainty in instrument pricing. Various pricing approaches have been proposed, but none treats the uncertainty in catastrophes and interest rates in a sufficiently flexible and statistically reliable way within an asset valuation framework. Consequently, little is known empirically about the expected risk premium of CAT bonds. The primary contribution of this article is to present a Bayesian CAT bond valuation framework based on uncertainty quantification of catastrophes and interest rates. We leverage this framework to estimate fair value prices and expected risk premiums for CAT bonds with varying catastrophe risk profiles.},
  archive      = {J_JRSSSC},
  author       = {Domfeh, Dixon and Chatterjee, Arpita and Dixon, Matthew},
  doi          = {10.1093/jrsssc/qlae041},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {5},
  pages        = {1389-1410},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A bayesian valuation framework for catastrophe bonds},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive functional regression framework for locally
heterogeneous signals in spectroscopy. <em>JRSSSC</em>, <em>73</em>(5),
1370–1388. (<a href="https://doi.org/10.1093/jrsssc/qlae040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been growing attention towards food nutritional properties, traceability, and production systems prioritizing environmental sustainability. Consequently, there is a rising demand for tools evaluating food quality and authenticity, with mid-infrared (MIR) spectroscopy techniques playing a pivotal role to collect vast amounts of data. These data pose some challenges that existing methods struggle to address, thus necessitating the development of new statistical techniques. We introduce an adaptive functional regression framework allowing for the definition of a flexible estimator accommodating different degrees of smoothness. We provide an optimization procedure handling both Gaussian and non-Gaussian responses, and allowing for the inclusion of scalar covariates. Our proposal is applied to MIR spectroscopy data, providing excellent performances when predicting milk composition and cows’ dietary regimens. Furthermore, the developed inferential routine enhances the interpretability of the results, providing valuable insights leading to a deeper understanding of the relation between specific wavenumbers and milk characteristics.},
  archive      = {J_JRSSSC},
  author       = {Ferraccioli, Federico and Casa, Alessandro and Stefanucci, Marco},
  doi          = {10.1093/jrsssc/qlae040},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {5},
  pages        = {1370-1388},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {An adaptive functional regression framework for locally heterogeneous signals in spectroscopy},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint modelling of survival and backwards recurrence
outcomes: An analysis of factors associated with fertility treatment in
the u.s. <em>JRSSSC</em>, <em>73</em>(5), 1355–1369. (<a
href="https://doi.org/10.1093/jrsssc/qlae039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The motivation for this paper is to determine factors associated with time-to-fertility treatment (TTFT) among women currently attempting pregnancy in a cross-sectional sample. Challenges arise due to dependence between time-to-pregnancy (TTP) and TTFT. We propose appending a marginal accelerated failure time model to identify risk factors of TTFT with a model for TTP where fertility treatment is included as a time-varying treatment to account for their dependence. The latter requires extending backwards recurrence survival methods to incorporate time-varying covariates with time-varying coefficients. Since backwards recurrence survival methods are a function of mean survival, computational difficulties arise in formulating mean survival when fertility treatment is unobserved, i.e. when TTFT is censored. We address these challenges by developing computationally friendly forms for the double expectation of TTP and TTFT. The performance is validated via comprehensive simulation studies. We apply our approach to the National Survey of Family Growth and explore factors related to prolonged TTFT in the U.S.},
  archive      = {J_JRSSSC},
  author       = {Guo, Siyuan and Zhang, Jiajia and McLain, Alexander C},
  doi          = {10.1093/jrsssc/qlae039},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {5},
  pages        = {1355-1369},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Joint modelling of survival and backwards recurrence outcomes: An analysis of factors associated with fertility treatment in the U.S.},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non-parametric bayesian approach to multiple treatment
comparisons in network meta-analysis with application to comparisons of
anti-depressants. <em>JRSSSC</em>, <em>73</em>(5), 1333–1354. (<a
href="https://doi.org/10.1093/jrsssc/qlae038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network meta-analysis is a powerful tool to synthesize evidence from independent studies and compare multiple treatments simultaneously. A critical task of performing a network meta-analysis is to offer ranks of all available treatment options for a specific disease outcome. Frequently, the estimated treatment rankings are accompanied by a large amount of uncertainty, suffer from multiplicity issues, and rarely permit possible ties of treatments with similar performance. These issues make interpreting rankings problematic as they are often treated as absolute metrics. To address these shortcomings, we formulate a ranking strategy that adapts to scenarios with high-order uncertainty by producing more conservative results. This improves the interpretability while simultaneously accounting for multiple comparisons. To admit ties between treatment effects in cases where differences between treatment effects are negligible, we also develop a Bayesian non-parametric approach for network meta-analysis. The approach capitalizes on the induced clustering mechanism of Bayesian non-parametric methods, producing a positive probability that two treatment effects are equal. We demonstrate the utility of the procedure through numerical experiments and a network meta-analysis designed to study antidepressant treatments.},
  archive      = {J_JRSSSC},
  author       = {Barrientos, Andrés F and Page, Garritt L and Lin, Lifeng},
  doi          = {10.1093/jrsssc/qlae038},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {5},
  pages        = {1333-1354},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Non-parametric bayesian approach to multiple treatment comparisons in network meta-analysis with application to comparisons of anti-depressants},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A semi-parametric maximum-likelihood analysis of measurement
error in population size estimation. <em>JRSSSC</em>, <em>73</em>(5),
1310–1332. (<a href="https://doi.org/10.1093/jrsssc/qlae037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work addresses the challenge of measurement errors in capture–recapture (CR) studies with covariates. These errors can introduce bias and undermine inference quality. To address this issue, we introduce a nonparametric measurement error model tailored to the ‘repeated counts’ setting, employing EM-type algorithms for parameter estimation. We use the Horvitz–Thompson estimator for population size estimates. Rigorous simulations, covering varying degrees of measurement error reliability, confirm our approach’s effectiveness. Applied to benchmark datasets, it consistently provides more accurate point estimates and robust uncertainty quantification, enhancing the reliability of CR analyses.},
  archive      = {J_JRSSSC},
  author       = {Alaimo Di Loro, Pierfrancesco and Maruotti, Antonello},
  doi          = {10.1093/jrsssc/qlae037},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {5},
  pages        = {1310-1332},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A semi-parametric maximum-likelihood analysis of measurement error in population size estimation},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel CFA + EFA model to detect aberrant respondents.
<em>JRSSSC</em>, <em>73</em>(5), 1283–1309. (<a
href="https://doi.org/10.1093/jrsssc/qlae036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aberrant respondents are common but yet extremely detrimental to the quality of social surveys or questionnaires. Recently, factor mixture models (FMMs) have been employed to identify individuals providing deceptive or careless responses. We propose a comprehensive FMM for continuous outcomes that combines confirmatory and exploratory factor models to classify both the nonaberrant and aberrant respondents. The flexibility of the proposed classification model allows for the identification of two of the most common aberrant response styles, namely faking and careless responding. We validated our approach by means of two simulations and two case studies. The results indicate the effectiveness of the proposed model in dealing with aberrant responses in social and behavioural surveys.},
  archive      = {J_JRSSSC},
  author       = {Cao, Niccolò and Finos, Livio and Lombardi, Luigi and Calcagnì, Antonio},
  doi          = {10.1093/jrsssc/qlae036},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {5},
  pages        = {1283-1309},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A novel CFA + EFA model to detect aberrant respondents},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A state-space perspective on modelling and inference for
online skill rating. <em>JRSSSC</em>, <em>73</em>(5), 1262–1282. (<a
href="https://doi.org/10.1093/jrsssc/qlae035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We summarize popular methods used for skill rating in competitive sports, along with their inferential paradigms and introduce new approaches based on sequential Monte Carlo and discrete hidden Markov models. We advocate for a state-space model perspective, wherein players’ skills are represented as time-varying, and match results serve as observed quantities. We explore the steps to construct the model and the three stages of inference: filtering, smoothing, and parameter estimation. We examine the challenges of scaling up to numerous players and matches, highlighting the main approximations and reductions which facilitate statistical and computational efficiency. We additionally compare approaches in a realistic experimental pipeline that can be easily reproduced and extended with our open-source Python package, abile .},
  archive      = {J_JRSSSC},
  author       = {Duffield, Samuel and Power, Samuel and Rimella, Lorenzo},
  doi          = {10.1093/jrsssc/qlae035},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {5},
  pages        = {1262-1282},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A state-space perspective on modelling and inference for online skill rating},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating spatially varying health effects of wildland fire
smoke using mobile health data. <em>JRSSSC</em>, <em>73</em>(5),
1242–1261. (<a href="https://doi.org/10.1093/jrsssc/qlae034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wildland fire smoke exposures are an increasing threat to public health, highlighting the need for studying the effects of protective behaviours on reducing health outcomes. Emerging smartphone applications provide unprecedented opportunities to deliver health risk communication messages to a large number of individuals in real-time and subsequently study the effectiveness, but also pose methodological challenges. Smoke Sense, a citizen science project, provides an interactive smartphone app platform for participants to engage with information about air quality, and ways to record their own health symptoms and actions taken to reduce smoke exposure. We propose a doubly robust estimator of the structural nested mean model that accounts for spatially and time-varying effects via a local estimating equation approach with geographical kernel weighting. Moreover, our analytical framework also handles informative missingness by inverse probability weighting of estimating functions. We evaluate the method using extensive simulation studies and apply it to Smoke Sense data to increase the knowledge base about the relationship between health preventive measures and health-related outcomes. Our results show that the protective behaviours’ effects vary over space and time and find that protective behaviours have more significant effects on reducing health symptoms in the Southwest than the Northwest region of the U.S.},
  archive      = {J_JRSSSC},
  author       = {Wu, Lili and Gao, Chenyin and Yang, Shu and Reich, Brian J and Rappold, Ana G},
  doi          = {10.1093/jrsssc/qlae034},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {5},
  pages        = {1242-1261},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Estimating spatially varying health effects of wildland fire smoke using mobile health data},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Walking fingerprinting. <em>JRSSSC</em>, <em>73</em>(5),
1221–1241. (<a href="https://doi.org/10.1093/jrsssc/qlae033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of predicting an individual’s identity from accelerometry data collected during walking. In a previous paper, we transformed the accelerometry time series into an image by constructing the joint distribution of the acceleration and lagged acceleration for a vector of lags. Predictors derived by partitioning this image into grid cells were used in logistic regression to predict individuals. Here, we (a) implement machine learning methods for prediction using the grid cell-derived predictors; (b) derive inferential methods to screen for the most predictive grid cells while adjusting for correlation and multiple comparisons; and (c) develop a novel multivariate functional regression model that avoids partitioning the predictor space. Prediction methods are compared on two open source acceleometry data sets collected from: (a) 32 individuals walking on a 1.06 km path; and (b) six repetitions of walking on a 20 m path on two occasions at least 1 week apart for 153 study participants. In the 32-individual study, all methods achieve at least 95% rank-1 accuracy, while in the 153-individual study, accuracy varies from 41% to 98%, depending on the method and prediction task. Methods provide insights into why some individuals are easier to predict than others.},
  archive      = {J_JRSSSC},
  author       = {Koffman, Lily and Crainiceanu, Ciprian and Leroux, Andrew},
  doi          = {10.1093/jrsssc/qlae033},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {5},
  pages        = {1221-1241},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Walking fingerprinting},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatio-temporal forecasting for the US drought monitor.
<em>JRSSSC</em>, <em>73</em>(5), 1203–1220. (<a
href="https://doi.org/10.1093/jrsssc/qlae032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The US Drought Monitor is the leading drought monitoring tool in the United States. Updated weekly and freely distributed, it records the drought conditions as geo-referenced polygons showing one of six ordered levels. These levels are determined by a mixture of quantitative environmental measurements and local expert opinion across the entire United States. At present, forecasts of the Drought Monitor only convey the expected direction of drought development (i.e. worsen, persist, subside) and do not communicate any uncertainty. This limits the utility of forecasts. In this paper, we describe a Bayesian spatio-temporal ordinal hierarchical model for use in modelling and projecting drought conditions. The model is flexible, scalable, and interpretable. By viewing drought data as areal rather than point-referenced, we reduce the cost of sampling from the posterior by avoiding dense matrix inversion. Draws from the posterior predictive distribution produce future forecasts of actual drought levels—rather than only the direction of drought development—and all sources of uncertainty are propagated into the posterior. Spatial random effects and an autoregressive model structure capture spatial and temporal dependence, and help ensure smoothness in forecasts over space and time. The result is a framework for modelling and forecasting drought levels and capturing forecast uncertainty.},
  archive      = {J_JRSSSC},
  author       = {Erhardt, Robert and Hepler, Staci and Wolodkin, Daniel and Greene, Andy},
  doi          = {10.1093/jrsssc/qlae032},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {5},
  pages        = {1203-1220},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Spatio-temporal forecasting for the US drought monitor},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fractional hawkes process model for earthquake aftershock
sequences. <em>JRSSSC</em>, <em>73</em>(5), 1185–1202. (<a
href="https://doi.org/10.1093/jrsssc/qlae031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new type of Hawkes process, known as the fractional Hawkes Process (FHP), has been recently introduced. This process uses a Mittag-Leffler density as the kernel function which is asymptotically a power law and so similar to the Omori–Utsu law, suggesting the FHP may be an appropriate earthquake model. However, it is currently an unmarked point process meaning it is independent of an earthquake’s magnitude. We extend the existing FHP, by incorporating Utsu’s aftershock productivity law and a time-scaling parameter from the fractional Zener Model to a marked version so that it may better model earthquake aftershock sequences. We call this model the ‘Seismic Fractional Hawkes Process’ (SFHP). We then estimate parameters via maximum likelihood and provide evidence for these estimates being consistent and asymptotically normal via a simulation study. The SFHP is then compared to the epidemic type aftershock sequence and FHP models on four aftershock sequences from Southern California and New Zealand. While it is inconclusive if the seismic fractional Hawkes process performs better in a retrospective predictive performance experiment, it does perform favourably against both models in terms of information criteria and residual diagnostics especially when the aftershock clustering is stronger.},
  archive      = {J_JRSSSC},
  author       = {Davis, Louis and Baeumer, Boris and Wang, Ting},
  doi          = {10.1093/jrsssc/qlae031},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {5},
  pages        = {1185-1202},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A fractional hawkes process model for earthquake aftershock sequences},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning local cascading failure pattern from massive
network failure data. <em>JRSSSC</em>, <em>73</em>(5), 1155–1184. (<a
href="https://doi.org/10.1093/jrsssc/qlae030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a novel multivariate point process regression model for a large-scale physically distributed network infrastructure with two failure modes, i.e. primary failures caused by the long-term usage and degradation of assets, and cascading failures triggered by primary failures in a short period. We exploit large-scale field pipe failure data from a UK-based water utility to support the rationale of considering the two failure modes. The two modes are not self-revealed in the data. To make the inference of the large-scale problem possible, we introduce a time window for cascading failures, based on which the likelihood of the pipe failure process can be decomposed into two parts, one for the primary failures and the other for the cascading failure processes modulated by the primary failure processes. The window length for cascading failures is treated as a tuning parameter, and determined through maximizing the likelihood based on all failure data. To illustrate the effectiveness of the model, two case studies are presented based on real data from the UK-based water utility. Interesting features of the cascading failures are identified from massive field pipe failure data. The results provide insights on advanced modelling and practical decision-making for both researchers and practitioners.},
  archive      = {J_JRSSSC},
  author       = {Xiao, Xun and Ye, Zhisheng and Revie, Matthew},
  doi          = {10.1093/jrsssc/qlae030},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {5},
  pages        = {1155-1184},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Learning local cascading failure pattern from massive network failure data},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variable importance based interaction modelling with an
application on initial spread of COVID-19 in china. <em>JRSSSC</em>,
<em>73</em>(5), 1134–1154. (<a
href="https://doi.org/10.1093/jrsssc/qlae029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interaction selection for linear regression is useful in many fields of modern science, yet very challenging. Existing methods focus on finding one optimal model but they may perform poorly in terms of stability for high-dimensional data, and they do not typically deal with categorical predictors. In this paper, we introduce a variable importance based interaction modelling (VIBIM) procedure for learning interactions in a linear regression model with both continuous and categorical predictors. We apply the VIBIM procedure to a COVID-19 data and show that the VIBIM approach leads to better models in terms of interpretability, stability, reliability, and prediction.},
  archive      = {J_JRSSSC},
  author       = {Zhang, Jianqiang and Chen, Ze and Yang, Yuhong and Xu, Wangli},
  doi          = {10.1093/jrsssc/qlae029},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {5},
  pages        = {1134-1154},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Variable importance based interaction modelling with an application on initial spread of COVID-19 in china},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-phase biomarker studies for disease progression with
multiple registries. <em>JRSSSC</em>, <em>73</em>(5), 1111–1133. (<a
href="https://doi.org/10.1093/jrsssc/qlae028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the design and analysis of two-phase studies of the association between an expensive biomarker and disease progression when phase I data are obtained by pooling registries having different outcome-dependent recruitment schemes. We utilize two analysis methods, namely maximum-likelihood and inverse probability weighting (IPW), to handle missing covariates arising from a two-phase design. In the likelihood framework, we derive a class of residual-dependent designs for phase II sub-sampling from an observed data likelihood accounting for the phase I sampling plans used by the different registries. In the IPW approach, we derive and evaluate optimal stratified designs that approximate Neyman allocation. Simulation studies and an application to a motivating example demonstrate the finite sample improvements from the proposed designs over simple random sampling and standard stratified sampling schemes.},
  archive      = {J_JRSSSC},
  author       = {Mao, Fangya and Cook, Richard J},
  doi          = {10.1093/jrsssc/qlae028},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {5},
  pages        = {1111-1133},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Two-phase biomarker studies for disease progression with multiple registries},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inverse set estimation and inversion of simultaneous
confidence intervals. <em>JRSSSC</em>, <em>73</em>(4), 1082–1109. (<a
href="https://doi.org/10.1093/jrsssc/qlae027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the questions of risk assessment in climatology (temperature change in North America) and medicine (impact of statin usage and coronavirus disease 2019 on hospitalized patients), we address the problem of estimating the set in the domain of a function whose image equals a predefined subset of the real line. Existing methods require strict assumptions. We generalize the estimation of such sets to dense and nondense domains with protection against inflated Type I error in exploratory data analysis. This is achieved by proving that confidence sets of multiple upper, lower, or interval sets can be simultaneously constructed with the desired confidence nonasymptotically through inverting simultaneous confidence intervals. Nonparametric bootstrap algorithm and code are provided.},
  archive      = {J_JRSSSC},
  author       = {Ren, Junting and Telschow, Fabian J E and Schwartzman, Armin},
  doi          = {10.1093/jrsssc/qlae027},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {8},
  number       = {4},
  pages        = {1082-1109},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Inverse set estimation and inversion of simultaneous confidence intervals},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exponential tilting for zero-inflated interval regression
with applications to cyber security survey data. <em>JRSSSC</em>,
<em>73</em>(4), 1065–1081. (<a
href="https://doi.org/10.1093/jrsssc/qlae026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-negative ordered survey data often exhibit an unusually high frequency of zeros in the first interval. Zero-inflated interval regression models handle the excess of zeros by combining a split probit model and an ordered probit model. In the presence of data violating distributional assumptions, standard inference based on the maximum likelihood method gives biased estimates with large standard errors. In this paper, we consider robust inference based on the exponential tilting methodology for the zero-inflated interval regression model. The application considers data on cyber security to study the relationship between investments in cyber defences and losses from cyber breaches. Robust estimates obtained via tilting clearly show an effect of the investments in reducing the loss amount.},
  archive      = {J_JRSSSC},
  author       = {Roner, Cristian and Di Caterina, Claudia and Ferrari, Davide},
  doi          = {10.1093/jrsssc/qlae026},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {8},
  number       = {4},
  pages        = {1065-1081},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Exponential tilting for zero-inflated interval regression with applications to cyber security survey data},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic factor models for binary data in circular spaces: An
application to the US supreme court. <em>JRSSSC</em>, <em>73</em>(4),
1042–1064. (<a href="https://doi.org/10.1093/jrsssc/qlae025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent factor models are widely used in the social and behavioural sciences as scaling tools to map discrete multivariate outcomes into low-dimensional, continuous scales. In political science, dynamic versions of classical factor models have been widely used to study the evolution of justices’ preferences in multi-judge courts. In this paper, we discuss a new dynamic factor model that relies on a latent circular space that can accommodate voting behaviours in which justices commonly understood to be on opposite ends of the ideological spectrum vote together on a substantial number of otherwise closely divided opinions. We apply this model to data on nonunanimous decisions made by the US Supreme Court between 1937 and 2021, and show that for most of this period, voting patterns can be better described by a circular latent space.},
  archive      = {J_JRSSSC},
  author       = {Lei, Rayleigh and Rodriguez, Abel},
  doi          = {10.1093/jrsssc/qlae025},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {8},
  number       = {4},
  pages        = {1042-1064},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Dynamic factor models for binary data in circular spaces: An application to the US supreme court},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bats monitoring: A classification procedure of bats
behaviours based on hawkes processes. <em>JRSSSC</em>, <em>73</em>(4),
1025–1041. (<a href="https://doi.org/10.1093/jrsssc/qlae024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are interested in the problem of classifying commuting and foraging behaviour of bats at delimited geographical areas, namely sites, throughout France. To predict the majority behaviour on these sites, we use echolocation call data recorded as part of Vigie-Chiro participatory project. As the temporal distribution of calls is a relevant indicator of behaviour, providing an adequate model of this distribution is a matter of great interest. Given the self-exciting dynamics observed in foraging behaviour, we propose to model bat calls by Hawkes processes. Specifically, we consider that the start time of each call emitted on a site is an event of a Hawkes process. Taking advantage of this modelling, we use a suitable procedure that relies on the empirical risk minimization principle to discriminate between the 2 classes. Then, the performance of the procedure is assessed on synthetic data through comprehensive numerical experiments. The overall methodology is evaluated with a goodness-of-fit test. Finally, we present the obtained results on the real data set. The classification results are convincing and show the relevance of our method, which could contribute to a better understanding of behavioural determinants and open up broad perspectives in spatial ecology.},
  archive      = {J_JRSSSC},
  author       = {Denis, Christophe and Dion-Blanc, Charlotte and Lacoste, Romain E and Sansonnet, Laure and Bas, Yves},
  doi          = {10.1093/jrsssc/qlae024},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {8},
  number       = {4},
  pages        = {1025-1041},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Bats monitoring: A classification procedure of bats behaviours based on hawkes processes},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A stochastic gradient relational event additive model for
modelling US patent citations from 1976 to 2022. <em>JRSSSC</em>,
<em>73</em>(4), 1008–1024. (<a
href="https://doi.org/10.1093/jrsssc/qlae023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Until 2022, the US patent citation network contained almost 10 million patents and over 100 million citations, presenting a challenge in analysing such expansive, intricate networks. To overcome limitations in analysing this complex citation network, we propose a stochastic gradient relational event additive model (STREAM) that models the citation relationships between patents as time events. While the structure of this model relies on the relational event model, STREAM offers a more comprehensive interpretation by modelling the effect of each predictor non-linearly. Overall, our model identifies key factors driving patent citations and reveals insights in the citation process.},
  archive      = {J_JRSSSC},
  author       = {Filippi-Mazzola, Edoardo and Wit, Ernst C},
  doi          = {10.1093/jrsssc/qlae023},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {8},
  number       = {4},
  pages        = {1008-1024},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A stochastic gradient relational event additive model for modelling US patent citations from 1976 to 2022},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-parametric bayesian approach for population size
estimation modelling the excess of singletons. <em>JRSSSC</em>,
<em>73</em>(4), 990–1007. (<a
href="https://doi.org/10.1093/jrsssc/qlae022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The phenomenon of one-inflation has received increasing attention in the recent literature on capture–recapture analysis. When data consist of frequencies of number of captures, the phenomenon manifests as an excess of units captured exactly once. We distinguish two possible causes for modelling the excess of singletons, namely, the erroneous inclusion of out-of-scope units, and a behavioural effect preventing subsequent captures after the first one. Accordingly, we propose two families of semi-parametric one-inflated models to estimate the number of uncaptured units. We consider a Bayesian approach by fitting a Dirichlet process mixture model as the base model, and extend this class to include one-inflation. The proposed base model and its two one-inflated counterparts are used to estimate the number of criminals involved in prostitution exploitation activities in Italy. We further assess the performance of the proposed models on three datasets available in the literature, as well as on simulated data.},
  archive      = {J_JRSSSC},
  author       = {Di Cecco, Davide and Tancredi, Andrea and Tuoto, Tiziana},
  doi          = {10.1093/jrsssc/qlae022},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {8},
  number       = {4},
  pages        = {990-1007},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Semi-parametric bayesian approach for population size estimation modelling the excess of singletons},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating disease transmission in a closed population under
repeated testing. <em>JRSSSC</em>, <em>73</em>(4), 972–989. (<a
href="https://doi.org/10.1093/jrsssc/qlae021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article presents a novel statistical framework for COVID-19 transmission monitoring and control, which was developed and deployed at The Ohio State University main campus in Columbus during the Autumn term of 2020. Our approach effectively handles prevalence data with interval censoring and explicitly incorporates changes in transmission dynamics and human behaviour. To illustrate the methodology’s usefulness, we apply it to both synthetic and actual student SARS-CoV-2 testing data collected at the OSU Columbus campus in late 2020.},
  archive      = {J_JRSSSC},
  author       = {Wascher, Matthew and Schnell, Patrick M and KhudaBukhsh, Wasiur R and Quam, Mikkel B M and Tien, Joesph H and Rempała, Grzegorz A},
  doi          = {10.1093/jrsssc/qlae021},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {8},
  number       = {4},
  pages        = {972-989},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Estimating disease transmission in a closed population under repeated testing},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing for distributional structural change with unknown
breaks: Application to pricing crop insurance contracts.
<em>JRSSSC</em>, <em>73</em>(4), 955–971. (<a
href="https://doi.org/10.1093/jrsssc/qlae020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agriculture in developed countries is produced under heavily subsidized insurance. The pricing of these insurance contracts, termed premium rates, directly influences farmers profits, their financial solvency, and indirectly, global food security. Changing climate and technology have likely caused significant shifting of mass in crop yield distributions and, if so, has rendered some of the historical yield data irrelevant for estimating premium rates. Insurance is primarily interested in lower tail probabilities and as such the detection of structural change in tail probabilities or higher moments is of great concern for the efficacy of crop insurance programs. We propose a test for structural change with an unknown break(s) which has power against structural change in any moment and can be tailored to a specific range of the underlying distribution. Simulations demonstrate better finite sample performance relative to existing methods and reasonable performance at identifying the break. The asymptotic distribution is shown to follow the Kolmogorov distribution. Our proposed test finds structural change in most major U.S. field crop yields leading to significant premium rate differences. Results of an out-of-sample premium rating game indicate that incorporating structural change in crop yields leads to more accurate premium rates.},
  archive      = {J_JRSSSC},
  author       = {Lu, Hanjun and Ker, Alan P},
  doi          = {10.1093/jrsssc/qlae020},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {8},
  number       = {4},
  pages        = {955-971},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Testing for distributional structural change with unknown breaks: Application to pricing crop insurance contracts},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bi-endpoint expectation-maximisation algorithm for
re-estimating sample size for the time-to-event endpoint under the blind
condition. <em>JRSSSC</em>, <em>73</em>(4), 935–954. (<a
href="https://doi.org/10.1093/jrsssc/qlae019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The expectation-maximisation (EM) algorithm can be used to adjust the sample size for the time-to-event endpoint without unblinding. Nevertheless, censoring or unreliable initial estimates may render inconsistent estimates by the EM algorithm. To address these limitations, we propose a bi-endpoint EM algorithm that incorporates the time-to-event endpoint and another endpoint, which can encompass various endpoint types and is not limited to efficacy indicators, during the EM iterations. Additionally, we suggest 2 approaches for choosing initial estimates. The application conditions are as follows: (i) at least one endpoint’s initial estimate is reliable and (ii) the influence of this endpoint on the posterior distribution of the latent variable exceeds that of another endpoint.},
  archive      = {J_JRSSSC},
  author       = {Xie, Longshen and Lu, Hui},
  doi          = {10.1093/jrsssc/qlae019},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {8},
  number       = {4},
  pages        = {935-954},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A bi-endpoint expectation-maximisation algorithm for re-estimating sample size for the time-to-event endpoint under the blind condition},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint analysis of multivariate longitudinal, imaging, and
time-to-event data. <em>JRSSSC</em>, <em>73</em>(4), 921–934. (<a
href="https://doi.org/10.1093/jrsssc/qlae018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s (AD) is a progressive neurodegenerative disease frequently associated with memory deficits and cognitive decline. Despite its irreversible once onset, some discoveries revealed the existence of a certain percentage of people who are non-susceptible to AD. This study proposes a joint analysis of multivariate longitudinal data, survival data with a non-susceptible fraction, and ultrahigh-dimensional imaging data. The proposed model comprises three major components. The first component is a mixture proportional hazards cure model with images to examine the potential predictors of the non-susceptible probability and hazards of interest. The second component is a dynamic factor analysis model with images to characterize group-specific latent factors through multiple observed variables. The last component is a semiparametric trajectory model to reveal the change patterns of the dynamic latent factors in the ‘non-susceptible’ and ‘susceptible’ groups. A two-stage approach is developed for statistical inference. The first stage manages the imaging data through high-dimensional functional principal component analysis. The second stage develops a Bayesian approach coupled with penalized splines, data augmentation, and Markov chain Monte Carlo techniques to perform estimation. The application to the Alzheimer’s Disease Neuroimaging Initiative dataset sheds new insight into the pathology of AD.},
  archive      = {J_JRSSSC},
  author       = {Zhou, Xiaoxiao and Song, Xinyuan},
  doi          = {10.1093/jrsssc/qlae018},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {8},
  number       = {4},
  pages        = {921-934},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Joint analysis of multivariate longitudinal, imaging, and time-to-event data},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating the timing of stillbirths in countries worldwide
using a bayesian hierarchical penalized splines regression model.
<em>JRSSSC</em>, <em>73</em>(4), 902–920. (<a
href="https://doi.org/10.1093/jrsssc/qlae017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reducing the global burden of stillbirths is important to improving child and maternal health. Of interest is understanding patterns in the timing of stillbirths—that is, whether they occur before the onset of labour (antepartum) or during labour (intrapartum)—because stillbirths that occur intrapartum are largely preventable. However, data availability on the timing of stillbirths is highly variable across the world, with low- and middle-income countries generally having few reliable observations. In this paper, we develop a Bayesian penalized splines regression framework to estimate the proportion of stillbirths that are intrapartum for all countries worldwide. The model accounts for known relationships with neonatal mortality, pools information across geographic regions, incorporates different errors based on data attributes, and allows for data-driven temporal trends. A weighting procedure is proposed to account for unrepresentative subnational data. Results suggest that the intrapartum proportion is generally decreasing over time, but progress is slower in some regions, particularly Sub-Saharan Africa.},
  archive      = {J_JRSSSC},
  author       = {Chong, Michael Y C and Alexander, Monica},
  doi          = {10.1093/jrsssc/qlae017},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {8},
  number       = {4},
  pages        = {902-920},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Estimating the timing of stillbirths in countries worldwide using a bayesian hierarchical penalized splines regression model},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized functional additive mixed models with
(functional) compositional covariates for areal covid-19 incidence
curves. <em>JRSSSC</em>, <em>73</em>(4), 880–901. (<a
href="https://doi.org/10.1093/jrsssc/qlae016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We extend the generalized functional additive mixed model to include compositional and functional compositional (density) covariates carrying relative information of a whole. Relying on the isometric isomorphism of the Bayes Hilbert space of probability densities with a sub-space of the L 2 ⁠ , we include functional compositions as transformed functional covariates with constrained yet interpretable effect function. The extended model allows for the estimation of linear, non-linear, and time-varying effects of scalar and functional covariates, as well as (correlated) functional random effects, in addition to the compositional effects. We use the model to estimate the effect of the age, sex, and smoking (functional) composition of the population on regional Covid-19 incidence data for Spain, while accounting for climatological and socio-demographic covariate effects and spatial correlation.},
  archive      = {J_JRSSSC},
  author       = {Eckardt, Matthias and Mateu, Jorge and Greven, Sonja},
  doi          = {10.1093/jrsssc/qlae016},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {8},
  number       = {4},
  pages        = {880-901},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Generalized functional additive mixed models with (functional) compositional covariates for areal covid-19 incidence curves},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Population-level task-evoked functional connectivity via
fourier analysis. <em>JRSSSC</em>, <em>73</em>(4), 857–879. (<a
href="https://doi.org/10.1093/jrsssc/qlae015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional magnetic resonance imaging (fMRI) is a noninvasive and in-vivo imaging technique essential for measuring brain activity. Functional connectivity is used to study associations between brain regions, either while study subjects perform tasks or during periods of rest. In this paper, we propose a rigorous definition of task-evoked functional connectivity at the population level (ptFC). Importantly, our proposed ptFC is interpretable in the context of task-fMRI studies. An algorithm for estimating the ptFC is provided. We present the performance of the proposed algorithm compared to existing functional connectivity frameworks using simulations. Lastly, we apply the proposed algorithm to estimate the ptFC in a motor-task study from the Human Connectome Project.},
  archive      = {J_JRSSSC},
  author       = {Meng, Kun and Eloyan, Ani},
  doi          = {10.1093/jrsssc/qlae015},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {8},
  number       = {4},
  pages        = {857-879},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Population-level task-evoked functional connectivity via fourier analysis},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian modelling of effective and functional brain
connectivity using hierarchical vector autoregressions. <em>JRSSSC</em>,
<em>73</em>(4), 835–856. (<a
href="https://doi.org/10.1093/jrsssc/qlae014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysis of brain connectivity is important for understanding how information is processed by the brain. We propose a novel Bayesian vector autoregression hierarchical model for analysing brain connectivity within resting-state functional magnetic resonance imaging, and apply it to simulated data and a real data set with subjects in different groups. Our approach models functional and effective connectivity simultaneously and allows for both group- and single-subject inference. We combine analytical marginalization with Hamiltonian Monte Carlo to obtain highly efficient posterior sampling. We show that our model gives similar inference for effective connectivity compared to models with a common covariance matrix to all subjects, but more accurate inference for functional connectivity between regions compared to models with more restrictive covariance structures. A Stan implementation of our model is available on GitHub.},
  archive      = {J_JRSSSC},
  author       = {Wegmann, Bertil and Lundquist, Anders and Eklund, Anders and Villani, Mattias},
  doi          = {10.1093/jrsssc/qlae014},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {8},
  number       = {4},
  pages        = {835-856},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Bayesian modelling of effective and functional brain connectivity using hierarchical vector autoregressions},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating the size of undeclared work from partially
misclassified survey data via the expectation–maximization algorithm.
<em>JRSSSC</em>, <em>73</em>(3), 816–834. (<a
href="https://doi.org/10.1093/jrsssc/qlae013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Undeclared work (UW) is pervasive in economies. This explains the interest of public authorities in knowing its size and drivers. Unfortunately, this is a very complex task because several issues often arise in the collected data, due to the sensitivity of the topic. In sample surveys, one major problem is misclassification. Without appropriate adjustments, inference would provide biased estimates, the reason being the concealing of undeclared status. In order to overcome such problem, we developed a methodology based on a Expectation–Maximization algorithm that accounts for misclassification due to dishonest answering. Through the proposed approach, we are able to estimate the prevalence of UW and its determinants. The reliability of the methodology is validated through an extensive simulation study. An application to the Special Eurobarometer survey no. 402 on UW is provided.},
  archive      = {J_JRSSSC},
  author       = {Arezzo, Maria Felice and Guagnano, Giuseppina and Vitale, Domenico},
  doi          = {10.1093/jrsssc/qlae013},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {6},
  number       = {3},
  pages        = {816-834},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Estimating the size of undeclared work from partially misclassified survey data via the Expectation–Maximization algorithm},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonlinear network-based quantitative trait prediction from
biological data. <em>JRSSSC</em>, <em>73</em>(3), 796–815. (<a
href="https://doi.org/10.1093/jrsssc/qlae012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantitatively predicting phenotypic variables using biomarkers is a challenging task for several reasons. First, the collected biological observations might be heterogeneous and correspond to different biological mechanisms. Second, the biomarkers used to predict the phenotype are potentially highly correlated since biological entities (genes, proteins, and metabolites) interact through unknown regulatory networks. In this paper, we present a novel approach designed to predict multivariate quantitative traits from biological data which address the 2 issues. The proposed model performs well on prediction but it is also fully parametric, with clusters of individuals and regulatory networks, which facilitates the downstream biological interpretation.},
  archive      = {J_JRSSSC},
  author       = {Blein-Nicolas, Mélisande and Devijver, Emilie and Gallopin, Mélina and Perthame, Emeline},
  doi          = {10.1093/jrsssc/qlae012},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {6},
  number       = {3},
  pages        = {796-815},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Nonlinear network-based quantitative trait prediction from biological data},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A two-sample tree-based test for hierarchically organized
genomic signals. <em>JRSSSC</em>, <em>73</em>(3), 774–795. (<a
href="https://doi.org/10.1093/jrsssc/qlae011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses a common type of data encountered in genomic studies, where a signal along a linear chromosome exhibits a hierarchical organization. We propose a novel framework to assess the significance of dissimilarities between two sets of genomic matrices obtained from distinct biological conditions. Our approach relies on a data representation based on trees. It utilizes tree distances and an aggregation procedure for tests performed at the level of leaf pairs. Numerical experiments demonstrate its statistical validity and its superior accuracy and power compared to alternatives. The method’s effectiveness is illustrated using real-world data from GWAS and Hi-C data.},
  archive      = {J_JRSSSC},
  author       = {Neuvial, Pierre and Randriamihamison, Nathanaël and Chavent, Marie and Foissac, Sylvain and Vialaneix, Nathalie},
  doi          = {10.1093/jrsssc/qlae011},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {6},
  number       = {3},
  pages        = {774-795},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A two-sample tree-based test for hierarchically organized genomic signals},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing unit root non-stationarity in the presence of
missing data in univariate time series of mobile health studies.
<em>JRSSSC</em>, <em>73</em>(3), 755–773. (<a
href="https://doi.org/10.1093/jrsssc/qlae010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of digital devices to collect data in mobile health studies introduces a novel application of time series methods, with the constraint of potential data missing at random or missing not at random (MNAR). In time-series analysis, testing for stationarity is an important preliminary step to inform appropriate subsequent analyses. The Dickey–Fuller test evaluates the null hypothesis of unit root non-stationarity, under no missing data. Beyond recommendations under data missing completely at random for complete case analysis or last observation carry forward imputation, researchers have not extended unit root non-stationarity testing to more complex missing data mechanisms. Multiple imputation with chained equations, Kalman smoothing imputation, and linear interpolation have also been used for time-series data, however such methods impose constraints on the autocorrelation structure and impact unit root testing. We propose maximum likelihood estimation and multiple imputation using state space model approaches to adapt the augmented Dickey–Fuller test to a context with missing data. We further develop sensitivity analyses to examine the impact of MNAR data. We evaluate the performance of existing and proposed methods across missing mechanisms in extensive simulations and in their application to a multi-year smartphone study of bipolar patients.},
  archive      = {J_JRSSSC},
  author       = {Fowler, Charlotte and Cai, Xiaoxuan and Baker, Justin T and Onnela, Jukka-Pekka and Valeri, Linda},
  doi          = {10.1093/jrsssc/qlae010},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {6},
  number       = {3},
  pages        = {755-773},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Testing unit root non-stationarity in the presence of missing data in univariate time series of mobile health studies},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A doubly self-exciting poisson model for describing scoring
levels in NBA basketball. <em>JRSSSC</em>, <em>73</em>(3), 735–754. (<a
href="https://doi.org/10.1093/jrsssc/qlae009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, Poisson time series models are considered to describe the number of field goals made by a basketball team or player at both the game (within-season) and the minute (within-game) level. The model is endowed with a doubly self-exciting structure, following the INGARCH(1,1) specification. To estimate the model at the within-game level, a divide-and-conquer procedure is carried out under a Bayesian framework. Then, we perform a clustering of the players in terms of their similarity according to the corresponding posterior distributions of key model parameters. The model is tested with National Basketball Association (NBA) teams and players from the 2018–2019 season.},
  archive      = {J_JRSSSC},
  author       = {Briz-Redón, Álvaro},
  doi          = {10.1093/jrsssc/qlae009},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {6},
  number       = {3},
  pages        = {735-754},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A doubly self-exciting poisson model for describing scoring levels in NBA basketball},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revisiting the effects of maternal education on adolescents’
academic performance: Doubly robust estimation in a network-based
observational study. <em>JRSSSC</em>, <em>73</em>(3), 715–734. (<a
href="https://doi.org/10.1093/jrsssc/qlae008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many contexts, particularly when study subjects are adolescents, peer effects can invalidate typical statistical requirements in the data. For instance, it is plausible that a student’s academic performance is influenced both by their own mother’s educational level as well as that of their peers. Since the underlying social network is measured, the Add Health study provides a unique opportunity to examine the impact of maternal college education on adolescent school performance, both direct and indirect. However, causal inference on populations embedded in social networks poses technical challenges, since the typical no interference assumption no longer holds. While inverse probability-of-treatment weighted (IPW) estimators have been developed for this setting, they are often highly unstable. Motivated by the question of maternal education, we propose doubly robust (DR) estimators combining models for treatment and outcome that are consistent and asymptotically normal if either model is correctly specified. We present empirical results that illustrate the DR property and the efficiency gain of DR over IPW estimators even when the treatment model is misspecified. Contrary to previous studies, our robust analysis does not provide evidence of an indirect effect of maternal education on academic performance within adolescents’ social circles in Add Health.},
  archive      = {J_JRSSSC},
  author       = {McNealis, Vanessa and Moodie, Erica E M and Dean, Nema},
  doi          = {10.1093/jrsssc/qlae008},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {6},
  number       = {3},
  pages        = {715-734},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Revisiting the effects of maternal education on adolescents’ academic performance: Doubly robust estimation in a network-based observational study},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Macroeconomic and financial mixed frequency factors in a big
data environment. <em>JRSSSC</em>, <em>73</em>(3), 682–714. (<a
href="https://doi.org/10.1093/jrsssc/qlae007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we evaluate the predictive content of 3 new business condition indexes and uncertainty measures that are estimated using high-frequency financial and low-frequency macroeconomic time series data. More specifically, our measures are defined as latent factors that are extracted from a state space model that includes multiple different frequencies of non-parametrically estimated components of quadratic variation, as well as mixed frequency macroeconomic variables. When forecasting growth rates of various monthly financial and macroeconomic variables, use of our new mixed frequency factors is shown to result in significant improvement in predictive performance, relative to a number of benchmark models. Additionally, when used to forecast corporate yields, predictive gains associated with the use of our measures are shown to be monotonically increasing, as one moves from predicting higher to lower rated bonds. This is consistent with the existence of a natural pricing channel wherein financial risk (as measured using our volatility factors) contains more predictive information for lower grade bonds. We also find that a variety of extant risk factors including the Aruoba et al. [(2009a). Real-time measurement of business conditions. Journal of Business &amp; Economic Statistics , 27 (4), 417427] business conditions index also contain marginal predictive content for the variables that we examine, although their inclusion does not reduce the usefulness of our measures.},
  archive      = {J_JRSSSC},
  author       = {Peng, Weijia and Swanson, Norman R and Yang, Xiye and Yao, Chun},
  doi          = {10.1093/jrsssc/qlae007},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {6},
  number       = {3},
  pages        = {682-714},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Macroeconomic and financial mixed frequency factors in a big data environment},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised bayesian classification for models with scalar
and functional covariates. <em>JRSSSC</em>, <em>73</em>(3), 658–681. (<a
href="https://doi.org/10.1093/jrsssc/qlae006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider unsupervised classification by means of a latent multinomial variable which categorizes a scalar response into one of the L components of a mixture model which incorporates scalar and functional covariates. This process can be thought as a hierarchical model with the first level modelling a scalar response according to a mixture of parametric distributions and the second level modelling the mixture probabilities by means of a generalized linear model with functional and scalar covariates. The traditional approach of treating functional covariates as vectors not only suffers from the curse of dimensionality, since functional covariates can be measured at very small intervals leading to a highly parametrized model, but also does not take into account the nature of the data. We use basis expansions to reduce the dimensionality and a Bayesian approach for estimating the parameters while providing predictions of the latent classification vector. The method is motivated by two data examples that are not easily handled by existing methods. The first example concerns identifying placebo responders on a clinical trial (normal mixture model) and the other predicting illness for milking cows (zero-inflated mixture of the Poisson model).},
  archive      = {J_JRSSSC},
  author       = {Garcia, Nancy L and Rodrigues-Motta, Mariana and Migon, Helio S and Petkova, Eva and Tarpey, Thaddeus and Ogden, R Todd and Giordano, Julio O and Perez, Martin M},
  doi          = {10.1093/jrsssc/qlae006},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {6},
  number       = {3},
  pages        = {658-681},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Unsupervised bayesian classification for models with scalar and functional covariates},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal transport-based machine learning to match specific
patterns: Application to the detection of molecular regulation patterns
in omics data. <em>JRSSSC</em>, <em>73</em>(3), 639–657. (<a
href="https://doi.org/10.1093/jrsssc/qlae005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present several algorithms designed to learn a pattern of correspondence between 2 data sets in situations where it is desirable to match elements that exhibit a relationship belonging to a known parametric model. In the motivating case study, the challenge is to better understand micro-RNA regulation in the striatum of Huntington’s disease model mice. The algorithms unfold in 2 stages. First, an optimal transport plan P and an optimal affine transformation are learned, using the Sinkhorn–Knopp algorithm and a mini-batch gradient descent. Second, P is exploited to derive either several co-clusters or several sets of matched elements. A simulation study illustrates how the algorithms work and perform. The real data application further illustrates their applicability and interest.},
  archive      = {J_JRSSSC},
  author       = {Nguyen, Thi Thanh Yen and Harchaoui, Warith and Mégret, Lucile and Mendoza, Cloé and Bouaziz, Olivier and Neri, Christian and Chambaz, Antoine},
  doi          = {10.1093/jrsssc/qlae005},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {6},
  number       = {3},
  pages        = {639-657},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Optimal transport-based machine learning to match specific patterns: Application to the detection of molecular regulation patterns in omics data},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identifying brexit voting patterns in the british house of
commons: An analysis based on bayesian mixture models with flexible
concomitant covariate effects. <em>JRSSSC</em>, <em>73</em>(3), 621–638.
(<a href="https://doi.org/10.1093/jrsssc/qlae004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The results of some divisions related to Brexit held in the House of Commons are investigated. In particular, a new class of mixture models with concomitant covariates is developed to identify groups of members of parliament with similar voting behaviour. The methodological novelty lies in the flexibility introduced by the use of smooth functions to model the effect of concomitant covariates on the component weights of the mixture. Results show this approach allows to quantify the effect of the age of members of parliament, as well as preferences and competitiveness in the constituencies they represent, on their position towards Brexit.},
  archive      = {J_JRSSSC},
  author       = {Berrettini, Marco and Galimberti, Giuliano and Ranciati, Saverio and Murphy, Thomas Brendan},
  doi          = {10.1093/jrsssc/qlae004},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {6},
  number       = {3},
  pages        = {621-638},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Identifying brexit voting patterns in the british house of commons: An analysis based on bayesian mixture models with flexible concomitant covariate effects},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian semi-parametric inference for clustered recurrent
events with zero inflation and a terminal event. <em>JRSSSC</em>,
<em>73</em>(3), 598–620. (<a
href="https://doi.org/10.1093/jrsssc/qlae003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent events are common in clinical studies and are often subject to terminal events. In pragmatic trials, participants are often nested in clinics and can be susceptible or structurally unsusceptible to the recurrent events. We develop a Bayesian shared random effects model to accommodate this complex data structure. To achieve robustness, we consider the Dirichlet processes to model the residual of the accelerated failure time model for the survival process as well as the cluster-specific shared frailty distribution, along with an efficient sampling algorithm for posterior inference. Our method is applied to a recent cluster randomized trial on fall injury prevention.},
  archive      = {J_JRSSSC},
  author       = {Tian, Xinyuan and Ciarleglio, Maria and Cai, Jiachen and Greene, Erich J and Esserman, Denise and Li, Fan and Zhao, Yize},
  doi          = {10.1093/jrsssc/qlae003},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {6},
  number       = {3},
  pages        = {598-620},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Bayesian semi-parametric inference for clustered recurrent events with zero inflation and a terminal event},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian smoothing for time-varying extremal dependence.
<em>JRSSSC</em>, <em>73</em>(3), 581–597. (<a
href="https://doi.org/10.1093/jrsssc/qlae002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Bayesian time-varying model that learns about the dynamics governing joint extreme values over time. Our model relies on dual measures of time-varying extremal dependence, that are modelled via a suitable class of generalized linear models conditional on a large threshold. The simulation study indicates that the proposed methods perform well in a variety of scenarios. The application of the proposed methods to some of the world’s most important stock markets reveals complex patterns of extremal dependence over the last 30 years, including passages from asymptotic dependence to asymptotic independence.},
  archive      = {J_JRSSSC},
  author       = {Lee, Junho and de Carvalho, Miguel and Rua, António and Avila, Julio},
  doi          = {10.1093/jrsssc/qlae002},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {6},
  number       = {3},
  pages        = {581-597},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Bayesian smoothing for time-varying extremal dependence},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prediction for distributional outcomes in high-performance
computing input/output variability. <em>JRSSSC</em>, <em>73</em>(3),
561–580. (<a href="https://doi.org/10.1093/jrsssc/qlae001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although high-performance computing (HPC) systems have been scaled to meet the exponentially growing demand for scientific computing, HPC performance variability remains a major challenge in computer science. Statistically, performance variability can be characterized by a distribution. Predicting performance variability is a critical step in HPC performance variability management. In this article, we propose a new framework to predict performance distributions. The proposed framework is a modified Gaussian process that can predict the distribution function of the input/output (I/O) throughput under a specific HPC system configuration. We also impose a monotonic constraint so that the predicted function is nondecreasing, which is a property of the cumulative distribution function. Additionally, the proposed model can incorporate both quantitative and qualitative input variables. We predict the HPC I/O distribution using the proposed method for the IOzone variability data. Data analysis results show that our framework can generate accurate predictions, and outperform existing methods. We also show how the predicted functional output can be used to generate predictions for a scalar summary of the performance distribution, such as the mean, standard deviation, and quantiles. Our prediction results can further be used for HPC system variability monitoring and optimization. This article has online supplementary materials .},
  archive      = {J_JRSSSC},
  author       = {Xu, Li and Hong, Yili and Morris, Max D and Cameron, Kirk W},
  doi          = {10.1093/jrsssc/qlae001},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {6},
  number       = {3},
  pages        = {561-580},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Prediction for distributional outcomes in high-performance computing input/output variability},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the combination of data smoothing and markov-switching
models. <em>JRSSSC</em>, <em>73</em>(3), 557–560. (<a
href="https://doi.org/10.1093/jrsssc/qlad110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSC},
  author       = {Michels, Rouven and Koslik, Jan-Ole},
  doi          = {10.1093/jrsssc/qlad110},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {6},
  number       = {3},
  pages        = {557-560},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {On the combination of data smoothing and markov-switching models},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Augmenting predictive models in forensic psychiatry with
cultural consensus theory. <em>JRSSSC</em>, <em>73</em>(2), 540–556. (<a
href="https://doi.org/10.1093/jrsssc/qlad109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forensic psychiatric hospitals regularly monitor the mental health and forensic risk factors of their patients. As part of this monitoring, staff score patients on various items. Common practice is to aggregate these scores across staff members. However, this is suboptimal because it assumes that assessors are interchangeable and that patients are independent. An improvement over averaging scores is the use of Cultural Consensus Theory (CCT), which imposes a hierarchical model across patients, staff members, and items. While accounting for differences between patients and staff members, CCT estimates a ‘true’ score for each patient on each item based on the consensus among staff members. Here, we apply a CCT model to data from a Dutch maximum-security forensic psychiatric centre and use the inferences to predict violent behaviour in patients. The CCT model outpredicts several alternatives, such as random forest and boosted regression trees, albeit by a small margin. We discuss practical limitations and directions for how future monitoring of patients could be adapted to maximize the added value of a CCT-based approach.},
  archive      = {J_JRSSSC},
  author       = {van den Bergh, Don and Schuringa, Erwin and Wagenmakers, Eric-Jan},
  doi          = {10.1093/jrsssc/qlad109},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {3},
  number       = {2},
  pages        = {540-556},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Augmenting predictive models in forensic psychiatry with cultural consensus theory},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A pseudo-response approach to constructing confidence
intervals for the subset of patients expected to benefit from a new
treatment. <em>JRSSSC</em>, <em>73</em>(2), 522–539. (<a
href="https://doi.org/10.1093/jrsssc/qlad108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In precision medicine, there is much interest in estimating the expected-to-benefit (EB) subset, i.e. the subset of patients who are expected to benefit from a new treatment based on a collection of baseline characteristics. There are many statistical methods for estimating the EB subset, most of which produce a ‘point estimate’ without a confidence statement to address uncertainty. Confidence intervals for the EB subset have been defined only recently, and their construction is a new area for methodological research. This article proposes a pseudo-response approach to EB subset estimation and confidence interval construction. Compared to existing methods, the pseudo-response approach allows us to focus on modelling a conditional treatment effect function (as opposed to the conditional mean outcome given treatment and baseline covariates) and is able to incorporate information from baseline covariates that are not involved in defining the EB subset. Simulation results show that incorporating such covariates can improve estimation efficiency and reduce the size of the confidence interval for the EB subset. The methodology is applied to a randomized clinical trial comparing two drugs for treating HIV infection.},
  archive      = {J_JRSSSC},
  author       = {Liu, Wei and Zhang, Zhiwei and Hu, Zonghui and Xu, Ping and Cohen, Calvin J},
  doi          = {10.1093/jrsssc/qlad108},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {3},
  number       = {2},
  pages        = {522-539},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A pseudo-response approach to constructing confidence intervals for the subset of patients expected to benefit from a new treatment},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient auxiliary information synthesis for cure rate
model. <em>JRSSSC</em>, <em>73</em>(2), 497–521. (<a
href="https://doi.org/10.1093/jrsssc/qlad106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new auxiliary information synthesis method to utilize subgroup survival information at multiple time points under the semi-parametric mixture cure rate model. After summarizing the auxiliary information via estimating equations, a control variate technique is adopted to reduce the variance efficiently, together with a test statistic to check the homogeneity assumption. Revision using penalization is further considered to adaptively accommodate potential population heterogeneity. Our methods can be adjusted when the uncertainty is not negligible. We establish asymptotic properties of our proposed estimators, and demonstrate their practical performances through extensive simulations and an invasive breast cancer study.},
  archive      = {J_JRSSSC},
  author       = {Ding, Jie and Li, Jialiang and Wang, Xiaoguang},
  doi          = {10.1093/jrsssc/qlad106},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {3},
  number       = {2},
  pages        = {497-521},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Efficient auxiliary information synthesis for cure rate model},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparison of paired ordinal data with mis-classification
and covariates adjustment. <em>JRSSSC</em>, <em>73</em>(2), 478–496. (<a
href="https://doi.org/10.1093/jrsssc/qlad105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop an estimation and testing procedure for comparing matched-pair ordinal outcomes in studies with confounding factors. The classification method for the categories of ordinal outcomes that is accessible for all units may be prone to mis-classification, and thus another error-free classification method that can only be affordable for a fraction of the units are used, resulting in a dataset with partial validation. The distribution of categorical variables is modelled using correlated bivariate Gaussian latent variables, and the confounding factors are adjusted as covariates. The mis-classification of ordinal outcomes is addressed by estimating the mis-classification probabilities through the partial validation structure of the dataset. The mis-classification probabilities and the other parameters are estimated by a two-stage maximum likelihood estimator, and the difference between the matched-pair ordinal outcomes are assessed by a Wald test statistic. Simulation studies were conducted to investigate the accuracy of the estimates of the model parameters, and the type I error rates and power of the proposed testing procedure. The motivating dataset from the Garki Project was analysed to demonstrate the applicability of the proposed approach.},
  archive      = {J_JRSSSC},
  author       = {Han, Yuanyuan and Lu, Zhao-Hua and Li, Yimei and Poon, Wai-Yin},
  doi          = {10.1093/jrsssc/qlad105},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {3},
  number       = {2},
  pages        = {478-496},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Comparison of paired ordinal data with mis-classification and covariates adjustment},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial modelling of infectious diseases with covariate
measurement error. <em>JRSSSC</em>, <em>73</em>(2), 460–477. (<a
href="https://doi.org/10.1093/jrsssc/qlad104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In spatial infectious disease models, it is typical to assume that only the distance between susceptible and infectious individuals is important for modelling, but not the actual spatial locations of the individuals. Recently introduced geographically-dependent individual level models (GD-ILMs) can be used to also consider the effect of spatial locations of individuals and the distance between susceptible and infectious individuals for determining the risk of infection. In these models, it is assumed that the covariates used to predict the occurrence of disease are measured accurately. However, there are many applications in which covariates are prone to measurement error. For instance, to study risk factors for influenza, people with low socio-economic status (SES) are known to be more at risk compared to the rest of the population. However, SES is prone to measurement error. In this paper, we propose a GD-ILM which accounts for measurement error in both individual-level and area-level covariates. A Monte Carlo expectation conditional maximisation algorithm is used for inference. We use models fitted to data to predict areas with high average infectivity rates. We evaluate the performance of the proposed approach through simulation studies and by a real-data application on influenza data in Manitoba, Canada.},
  archive      = {J_JRSSSC},
  author       = {Amiri, Leila and Torabi, Mahmoud and Deardon, Rob},
  doi          = {10.1093/jrsssc/qlad104},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {3},
  number       = {2},
  pages        = {460-477},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Spatial modelling of infectious diseases with covariate measurement error},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive hybrid control design for comparative clinical
trials with historical control data. <em>JRSSSC</em>, <em>73</em>(2),
444–459. (<a href="https://doi.org/10.1093/jrsssc/qlad103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an adaptive hybrid control causal (AHCC) design to leverage historical control data to reduce the sample size demanded by standard randomised controlled trials (RCT). Under the causal inference framework, we define the causal estimand of the average treatment effect and derive the corresponding estimator based on the trial data and historical control data. The AHCC design takes a multistage or group sequential approach. The number of patients randomised to the concurrent control is adaptively adjusted based on the amount of information borrowed from the historical control data. At each stage, based on the interim data, the contribution of the historical control data, quantified by the effective sample size, is updated and used to determine the randomisation ratio between the treatment and control arms for the next stage, with the goal to resemble a standard RCT upon the completion of the trial. Simulation studies show that the AHCC design has desirable operating characteristics. For example, it saves on sample size when substantial information can be borrowed from the historical control, and it maintains power when little information can be borrowed from the historical control.},
  archive      = {J_JRSSSC},
  author       = {Guo, Beibei and Laird, Glen and Song, Yang and Chen, Josh and Yuan, Ying},
  doi          = {10.1093/jrsssc/qlad103},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {3},
  number       = {2},
  pages        = {444-459},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Adaptive hybrid control design for comparative clinical trials with historical control data},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multivariate bayesian structured variable selection for
pharmacogenomic studies. <em>JRSSSC</em>, <em>73</em>(2), 420–443. (<a
href="https://doi.org/10.1093/jrsssc/qlad102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer drug sensitivity screens combined with multi-omics characterisation of the cancer cells have become an important tool to determine the optimal treatment for each patient. We propose a multivariate Bayesian structured variable selection model for sparse identification of multi-omics features associated with multiple correlated drug responses. Our model uses known structure between drugs and their targeted genes via a Markov random field (MRF) prior in sparse seemingly unrelated regression. The use of MRF prior can improve the model performance compared to other common priors. The proposed model is applied to the Genomics of Drug Sensitivity in Cancer data.},
  archive      = {J_JRSSSC},
  author       = {Zhao, Zhi and Banterle, Marco and Lewin, Alex and Zucknick, Manuela},
  doi          = {10.1093/jrsssc/qlad102},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {3},
  number       = {2},
  pages        = {420-443},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Multivariate bayesian structured variable selection for pharmacogenomic studies},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian latent class model for integrating multi-source
longitudinal data: Application to the CHILD cohort study.
<em>JRSSSC</em>, <em>73</em>(2), 398–419. (<a
href="https://doi.org/10.1093/jrsssc/qlad100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-source longitudinal data have become increasingly common. This type of data refers to longitudinal datasets collected from multiple sources describing the same set of individuals. Representing distinct features of the individuals, each data source may consist of multiple longitudinal markers of distinct types and measurement frequencies. Motivated by the CHILD cohort study, we develop a model for joint clustering multi-source longitudinal data. The proposed model allows each data source to follow source-specific clustering, and they are aggregated to yield a global clustering. The proposed model is demonstrated through real-data analysis and simulation study.},
  archive      = {J_JRSSSC},
  author       = {Lu, Zihang and Subbarao, Padmaja and Lou, Wendy},
  doi          = {10.1093/jrsssc/qlad100},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {3},
  number       = {2},
  pages        = {398-419},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A bayesian latent class model for integrating multi-source longitudinal data: Application to the CHILD cohort study},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian design with sampling windows for complex spatial
processes. <em>JRSSSC</em>, <em>73</em>(2), 378–397. (<a
href="https://doi.org/10.1093/jrsssc/qlad099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal design facilitates intelligent data collection. In this paper, we introduce a fully Bayesian design approach for spatial processes with complex covariance structures, like those typically exhibited in natural ecosystems. Coordinate exchange algorithms are commonly used to find optimal design points. However, collecting data at specific points is often infeasible in practice. Currently, there is no provision to allow for flexibility in the choice of design. Accordingly, we also propose an approach to find Bayesian sampling windows, rather than points, via Gaussian process emulation to identify regions of high design efficiency across a multi-dimensional space. These developments are motivated by two ecological case studies: monitoring water temperature in a river network system in the northwestern United States and monitoring submerged coral reefs off the north-west coast of Australia.},
  archive      = {J_JRSSSC},
  author       = {Buchhorn, Katie and Mengersen, Kerrie and Santos-Fernandez, Edgar and Peterson, Erin E and McGree, James M},
  doi          = {10.1093/jrsssc/qlad099},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {3},
  number       = {2},
  pages        = {378-397},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Bayesian design with sampling windows for complex spatial processes},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CRP-tree: A phylogenetic association test for binary traits.
<em>JRSSSC</em>, <em>73</em>(2), 340–377. (<a
href="https://doi.org/10.1093/jrsssc/qlad098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important problem in evolutionary genomics is to investigate whether a certain trait measured on each sample is associated with the sample phylogenetic tree. The phylogenetic tree represents the shared evolutionary history of the samples and it is usually estimated from molecular sequence data at a locus or from other type of genetic data. We propose a model for trait evolution inspired by the Chinese Restaurant Process that includes a parameter that controls the degree of preferential attachment, that is, the tendency of nodes in the tree to subtend from nodes of the same type. This model with no preferential attachment is equivalent to a structured coalescent model with simultaneous migration and coalescence events and serves as a null model. We derive a test for phylogenetic binary trait association with linear computational complexity and empirically demonstrate that it is more powerful than some other methods. We apply our test to study the phylogenetic association of some traits in swordtail fish, breast cancer, yellow fever virus, and influenza A H1N1 virus. R-package implementation of our methods is available at https://github.com/jyzhang27/CRPTree .},
  archive      = {J_JRSSSC},
  author       = {Zhang, Julie and Preising, Gabriel A and Schumer, Molly and Palacios, Julia A},
  doi          = {10.1093/jrsssc/qlad098},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {3},
  number       = {2},
  pages        = {340-377},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {CRP-tree: A phylogenetic association test for binary traits},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian profile regression for clustering analysis
involving a longitudinal response and explanatory variables.
<em>JRSSSC</em>, <em>73</em>(2), 314–339. (<a
href="https://doi.org/10.1093/jrsssc/qlad097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification of sets of co-regulated genes that share a common function is a key question of modern genomics. Bayesian profile regression is a semi-supervised mixture modelling approach that makes use of a response to guide inference toward relevant clusterings. Previous applications of profile regression have considered univariate continuous, categorical, and count outcomes. In this work, we extend Bayesian profile regression to cases where the outcome is longitudinal (or multivariate continuous) and provide PReMiuMlongi, an updated version of PReMiuM, the R package for profile regression. We consider multivariate normal and Gaussian process regression response models and provide proof of principle applications to four simulation studies. The model is applied on budding-yeast data to identify groups of genes co-regulated during the Saccharomyces cerevisiae cell cycle. We identify four distinct groups of genes associated with specific patterns of gene expression trajectories, along with the bound transcriptional factors, likely involved in their co-regulation process.},
  archive      = {J_JRSSSC},
  author       = {Rouanet, Anaïs and Johnson, Rob and Strauss, Magdalena and Richardson, Sylvia and Tom, Brian D and White, Simon R and Kirk, Paul D W},
  doi          = {10.1093/jrsssc/qlad097},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {3},
  number       = {2},
  pages        = {314-339},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Bayesian profile regression for clustering analysis involving a longitudinal response and explanatory variables},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variable selection for individualised treatment rules with
discrete outcomes. <em>JRSSSC</em>, <em>73</em>(2), 298–313. (<a
href="https://doi.org/10.1093/jrsssc/qlad096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An individualised treatment rule (ITR) is a decision rule that aims to improve individuals’ health outcomes by recommending treatments according to subject-specific information. In observational studies, collected data may contain many variables that are irrelevant to treatment decisions. Including all variables in an ITR could yield low efficiency and a complicated treatment rule that is difficult to implement. Thus, selecting variables to improve the treatment rule is crucial. We propose a doubly robust variable selection method for ITRs, and show that it compares favourably with competing approaches. We illustrate the proposed method on data from an adaptive, web-based stress management tool.},
  archive      = {J_JRSSSC},
  author       = {Bian, Zeyu and Moodie, Erica E M and Shortreed, Susan M and Lambert, Sylvie D and Bhatnagar, Sahir},
  doi          = {10.1093/jrsssc/qlad096},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {3},
  number       = {2},
  pages        = {298-313},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Variable selection for individualised treatment rules with discrete outcomes},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combining individual- and population-level data to develop a
bayesian parity-specific fertility projection model. <em>JRSSSC</em>,
<em>73</em>(2), 275–297. (<a
href="https://doi.org/10.1093/jrsssc/qlad095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fertility projections are vital to anticipate demand for maternity and childcare services, among other uses. Models typically use aggregate population-level data alone, ignoring the richness of individual-level data. We hence develop a Bayesian parity-specific projection model combining such data sources. We apply our method to England and Wales, using individual-level data from Understanding Society . Fitting generalised additive models gives smooth projections across age, cohort, and time since last birth. We also incorporate prior beliefs about the relative importance of the data sources. Our approach generates plausible forecasts by individual-level variables including educational qualification, despite their absence in the population-level data.},
  archive      = {J_JRSSSC},
  author       = {Ellison, Joanne and Berrington, Ann and Dodd, Erengul and Forster, Jonathan J},
  doi          = {10.1093/jrsssc/qlad095},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {3},
  number       = {2},
  pages        = {275-297},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Combining individual- and population-level data to develop a bayesian parity-specific fertility projection model},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian kernel machine regression for count data: Modelling
the association between social vulnerability and COVID-19 deaths in
south carolina. <em>JRSSSC</em>, <em>73</em>(1), 257–274. (<a
href="https://doi.org/10.1093/jrsssc/qlad094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic created an unprecedented global health crisis. Recent studies suggest that socially vulnerable communities were disproportionately impacted, although findings are mixed. To quantify social vulnerability in the US, many studies rely on the Social Vulnerability Index (SVI), a county-level measure comprising 15 census variables. Typically, the SVI is modelled in an additive manner, which may obscure non-linear or interactive associations, further contributing to inconsistent findings. As a more robust alternative, we propose a negative binomial Bayesian kernel machine regression (BKMR) model to investigate dynamic associations between social vulnerability and COVID-19 death rates, thus extending BKMR to the count data setting. The model produces a ‘vulnerability effect’ that quantifies the impact of vulnerability on COVID-19 death rates in each county. The method can also identify the relative importance of various SVI variables and make future predictions as county vulnerability profiles evolve. To capture spatio-temporal heterogeneity, the model incorporates spatial effects, county-level covariates, and smooth temporal functions. For Bayesian computation, we propose a tractable data-augmented Gibbs sampler. We conduct a simulation study to highlight the approach and apply the method to a study of COVID-19 deaths in the US state of South Carolina during the 2021 calendar year.},
  archive      = {J_JRSSSC},
  author       = {Mutiso, Fedelis and Li, Hong and Pearce, John L and Benjamin-Neelon, Sara E and Mueller, Noel T and Neelon, Brian},
  doi          = {10.1093/jrsssc/qlad094},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {257-274},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Bayesian kernel machine regression for count data: Modelling the association between social vulnerability and COVID-19 deaths in south carolina},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Penalised semi-parametric copula method for semi-competing
risks data: Application to hip fracture in elderly. <em>JRSSSC</em>,
<em>73</em>(1), 241–256. (<a
href="https://doi.org/10.1093/jrsssc/qlad093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hip fracture is a severe complication in the elderly. The affected people are at a higher risk of second fracture and death occurrence, and the best treatment for hip fractures is still being debated. Aside from the treatment, many factors, such as comorbidity conditions, may be associated with second fracture and death occurrence. This study aims to identify effective treatments and important covariates and estimate their effects on the progression of second fracture and death occurrence in hip fracture elderly patients using the semi-competing risks framework, because death dependently censors a second fracture but not vice versa. Due to the complex semi-competing risks data, performing variable selection simultaneously for second fracture and death occurrence is difficult. We propose a penalised semi-parametric copula method for semi-competing risks data. Specifically, we use separate Cox semi-parametric models for both margins and employ a copula to model the two margins’ dependence. We develop a coordinate-wise optimisation algorithm that takes into account the data structure and copula function’s complexities. Simulations show that the proposed method outperforms the traditional penalised marginal method. We apply the proposed method to a population-based cohort study of hip fracture elderly patients, providing new insights into their treatment and clinical management.},
  archive      = {J_JRSSSC},
  author       = {Sun, Tao and Liang, Weijie and Zhang, Gongzi and Yi, Danhui and Ding, Ying and Zhang, Lihai},
  doi          = {10.1093/jrsssc/qlad093},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {241-256},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Penalised semi-parametric copula method for semi-competing risks data: Application to hip fracture in elderly},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal inference with a functional outcome. <em>JRSSSC</em>,
<em>73</em>(1), 221–240. (<a
href="https://doi.org/10.1093/jrsssc/qlad092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents methods to study the causal effect of a binary treatment on a functional outcome with observational data. We define a Functional Average Treatment Effect (FATE) and develop an outcome regression estimator. We show how to obtain valid inference on the FATE using simultaneous confidence bands, which cover the FATE with a given probability over the entire domain. Simulation experiments illustrate how the simultaneous confidence bands take the multiple comparison problem into account. Finally, we use the methods to infer the effect of early adult location on subsequent income development for one Swedish birth cohort.},
  archive      = {J_JRSSSC},
  author       = {Ecker, Kreske and de Luna, Xavier and Schelin, Lina},
  doi          = {10.1093/jrsssc/qlad092},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {221-240},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Causal inference with a functional outcome},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Horseshoe prior bayesian quantile regression.
<em>JRSSSC</em>, <em>73</em>(1), 193–220. (<a
href="https://doi.org/10.1093/jrsssc/qlad091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper extends the horseshoe prior to Bayesian quantile regression and provides a fast sampling algorithm for computation in high dimensions. Compared to alternative shrinkage priors, our method yields better performance in coefficient bias and forecast error, especially in sparse designs and in estimating extreme quantiles. In a high-dimensional growth-at-risk forecasting application, we forecast tail risks and complete forecast densities using a database covering over 200 macroeconomic variables. Quantile specific and density calibration score functions show that our method provides competitive performance compared to competing Bayesian quantile regression priors, especially at short- and medium-run horizons.},
  archive      = {J_JRSSSC},
  author       = {Kohns, David and Szendrei, Tibor},
  doi          = {10.1093/jrsssc/qlad091},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {193-220},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Horseshoe prior bayesian quantile regression},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical inference for complete and incomplete mobility
trajectories under the flight-pause model. <em>JRSSSC</em>,
<em>73</em>(1), 162–192. (<a
href="https://doi.org/10.1093/jrsssc/qlad090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We formulate a statistical flight-pause model (FPM) for human mobility, represented by a collection of random objects, called motions, appropriate for mobile phone tracking (MPT) data. We develop the statistical machinery for parameter inference and trajectory imputation under various forms of missing data. We show that common assumptions about the missing data mechanism for MPT are not valid for the mechanism governing the random motions underlying the FPM, representing an understudied missing data phenomenon. We demonstrate the consequences of missing data and our proposed adjustments in both simulations and real data, outlining implications for MPT data collection and design.},
  archive      = {J_JRSSSC},
  author       = {Jurek, Marcin and Calder, Catherine A and Zigler, Corwin},
  doi          = {10.1093/jrsssc/qlad090},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {162-192},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Statistical inference for complete and incomplete mobility trajectories under the flight-pause model},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Categorising the world into local climate zones: Towards
quantifying labelling uncertainty for machine learning models.
<em>JRSSSC</em>, <em>73</em>(1), 143–161. (<a
href="https://doi.org/10.1093/jrsssc/qlad089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image classification is often prone to labelling uncertainty. To generate suitable training data, images are labelled according to evaluations of human experts. This can result in ambiguities, which will affect subsequent models. In this work, we aim to model the labelling uncertainty in the context of remote sensing and the classification of satellite images. We construct a multinomial mixture model given the evaluations of multiple experts. This is based on the assumption that there is no ambiguity of the image class, but apparently in the experts’ opinion about it. The model parameters can be estimated by a stochastic expectation maximisation algorithm. Analysing the estimates gives insights into sources of label uncertainty. Here, we focus on the general class ambiguity, the heterogeneity of experts, and the origin city of the images. The results are relevant for all machine learning applications where image classification is pursued and labelling is subject to humans.},
  archive      = {J_JRSSSC},
  author       = {Hechinger, Katharina and Zhu, Xiao Xiang and Kauermann, Göran},
  doi          = {10.1093/jrsssc/qlad089},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {143-161},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Categorising the world into local climate zones: Towards quantifying labelling uncertainty for machine learning models},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modelling change processes in multivariate interrupted time
series data using a multivariate dynamic additive model: An application
to heart rate and blood pressure self-monitoring in heart failure with
drug changes. <em>JRSSSC</em>, <em>73</em>(1), 123–142. (<a
href="https://doi.org/10.1093/jrsssc/qlad088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heart rate (HR) and blood pressure (BP) measured by a patient can be used to monitor response to pharmacologic therapies. Continuously measured HR and BP are multivariate time series as sequences of values at regularly spaced intervals over time and changes in pharmacologic therapies interrupt the multivariate time series and create phases. In the multivariate interrupted time series, there are change processes in multiple phases, such as level changes, linear or non-linear trend changes, and time-varying serial dependence. This paper presents an application of a multivariate dynamic additive model to account for these change processes. In addition, a simulation study is conducted to evaluate the model’s parameter recovery and to demonstrate the consequences of ignoring the time-varying serial dependence in HR and BP when detecting level changes and trend changes. The results of the simulation study show that the accuracy and precision of parameter estimates are satisfactory. Furthermore, the simulation results present that ignoring time-varying serial dependence in the same conditions as those found in the application results in biased estimates and standard errors for the level changes and trend changes.},
  archive      = {J_JRSSSC},
  author       = {Cho, Sun-Joo},
  doi          = {10.1093/jrsssc/qlad088},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {123-142},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Modelling change processes in multivariate interrupted time series data using a multivariate dynamic additive model: An application to heart rate and blood pressure self-monitoring in heart failure with drug changes},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multivariate longitudinal analysis for the association
between brain atrophy and cognitive impairment in prodromal huntington’s
disease subjects. <em>JRSSSC</em>, <em>73</em>(1), 104–122. (<a
href="https://doi.org/10.1093/jrsssc/qlad087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive impairment has been widely accepted as a disease progression measure prior to the onset of Huntington’s disease. We propose a sophisticated measurement error correction method that can handle potentially correlated measurement errors in longitudinally collected exposures and multiple outcomes. The asymptotic theory for the proposed method is developed. A simulation study is conducted to demonstrate the satisfactory performance of the proposed two-stage fitting method and shows that the independent working correlation structure outperforms other alternatives. We conduct a comprehensive longitudinal analysis to assess how brain striatal atrophy affects impairment in various cognitive domains for Huntington’s disease.},
  archive      = {J_JRSSSC},
  author       = {Zheng, Cheng and Tong, Lili and Zhang, Ying},
  doi          = {10.1093/jrsssc/qlad087},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {104-122},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Multivariate longitudinal analysis for the association between brain atrophy and cognitive impairment in prodromal huntington’s disease subjects},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semiparametric m-quantile regression with measurement error
in spatial covariates: An application to housing price modelling.
<em>JRSSSC</em>, <em>73</em>(1), 82–103. (<a
href="https://doi.org/10.1093/jrsssc/qlad086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial data are becoming increasingly accessible to urban scientists, but these data are often prone to measurement error. Motivated by the analysis of the Milan (Italy) apartment market heterogeneity, we propose a semiparametric approach to adjust for the presence of measurement error in the covariates when estimating M-quantile regression. The M-quantile approach helps explain the heterogeneity across individual units, preserving robustness and efficiency in the estimates. The model’s parameters are estimated within a penalised likelihood framework and an analytical expression is proposed to estimate standard errors. Asymptotic properties of estimates are also provided.},
  archive      = {J_JRSSSC},
  author       = {Borgoni, Riccardo and Schirripa Spagnolo, Francesco and Michelangeli, Alessandra and Salvati, Nicola and Carcagnì, Antonella},
  doi          = {10.1093/jrsssc/qlad086},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {82-103},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Semiparametric M-quantile regression with measurement error in spatial covariates: An application to housing price modelling},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-resolution global precipitation downscaling with latent
gaussian models and non-stationary stochastic partial differential
equation structure. <em>JRSSSC</em>, <em>73</em>(1), 65–81. (<a
href="https://doi.org/10.1093/jrsssc/qlad084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining high-resolution maps of precipitation data can provide key insights to stakeholders to assess a sustainable access to water resources at urban scale. Mapping a non-stationary, sparse process such as precipitation at very high spatial resolution requires the interpolation of global datasets at the location where ground stations are available with statistical models able to capture complex non-Gaussian global space–time dependence structures. In this work, we propose a new approach based on capturing the spatially varying anisotropy of a latent Gaussian process via a locally deformed stochastic partial differential equation (SPDE) with a buffer allowing for a different spatial structure across land and sea. The finite volume approximation of the SPDE, coupled with integrated nested Laplace approximation ensures feasible Bayesian inference for tens of millions of observations. The simulation studies showcase the improved predictability of the proposed approach against stationary and no-buffer alternatives. The proposed approach is then used to yield high-resolution simulations of daily precipitation across the United States.},
  archive      = {J_JRSSSC},
  author       = {Zhang, Jiachen and Bonas, Matthew and Bolster, Diogo and Fuglstad, Geir-Arne and Castruccio, Stefano},
  doi          = {10.1093/jrsssc/qlad084},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {65-81},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {High-resolution global precipitation downscaling with latent gaussian models and non-stationary stochastic partial differential equation structure},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient calibration for imperfect epidemic models with
applications to the analysis of COVID-19. <em>JRSSSC</em>,
<em>73</em>(1), 47–64. (<a
href="https://doi.org/10.1093/jrsssc/qlad083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimation of unknown parameters in simulations, also known as calibration, is crucial for practical management of epidemics and prediction of pandemic risk. A simple yet widely used approach is to estimate the parameters by minimising the sum of the squared distances between actual observations and simulation outputs. It is shown in this paper that this method is inefficient, particularly when the epidemic models are developed based on certain simplifications of reality, also known as imperfect models which are commonly used in practice. To address this issue, a new estimator is introduced that is asymptotically consistent, has a smaller estimation variance than the least-squares estimator, and achieves the semiparametric efficiency. Numerical studies are performed to examine the finite sample performance. The proposed method is applied to the analysis of the COVID-19 pandemic for 20 countries based on the susceptible-exposed-infectious-recovered model with both deterministic and stochastic simulations. The estimation of the parameters, including the basic reproduction number and the average incubation period, reveal the risk of disease outbreaks in each country and provide insights to the design of public health interventions.},
  archive      = {J_JRSSSC},
  author       = {Sung, Chih-Li and Hung, Ying},
  doi          = {10.1093/jrsssc/qlad083},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {47-64},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Efficient calibration for imperfect epidemic models with applications to the analysis of COVID-19},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Models and methods for analysing clustered recurrent
hospitalisations in the presence of COVID-19 effects. <em>JRSSSC</em>,
<em>73</em>(1), 28–46. (<a
href="https://doi.org/10.1093/jrsssc/qlad082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent events such as hospitalisations are outcomes that can be used to monitor dialysis facilities’ quality of care. However, current methods are not adequate to analyse data from many facilities with multiple hospitalisations, especially when adjustments are needed for multiple time scales. It is also controversial whether direct or indirect standardisation should be used in comparing facilities. This study is motivated by the need of the Centers for Medicare and Medicaid Services to evaluate US dialysis facilities using Medicare claims, which involve almost 8,000 facilities and over 500,000 dialysis patients. This scope is challenging for current statistical software’s computational power. We propose a method that has a flexible baseline rate function and is computationally efficient. Additionally, the proposed method shares advantages of both indirect and direct standardisation. The method is evaluated under a range of simulation settings and demonstrates substantially improved computational efficiency over the existing R package survival . Finally, we illustrate the method with an important application to monitoring dialysis facilities in the U.S., while making time-dependent adjustments for the effects of COVID-19.},
  archive      = {J_JRSSSC},
  author       = {Ding, Xuemei and He, Kevin and Kalbfleisch, John D},
  doi          = {10.1093/jrsssc/qlad082},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {28-46},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Models and methods for analysing clustered recurrent hospitalisations in the presence of COVID-19 effects},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CAViaR models for value-at-risk and expected shortfall with
long range dependency features. <em>JRSSSC</em>, <em>73</em>(1), 1–27.
(<a href="https://doi.org/10.1093/jrsssc/qlad081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider alternative specifications of conditional autoregressive quantile models to estimate Value-at-Risk and Expected Shortfall. The proposed specifications include a slow moving component in the quantile process, along with aggregate returns from heterogeneous horizons as regressors. Using data for 10 stock indices, we evaluate the performance of the models and find that the proposed features are useful in capturing tail dynamics better.},
  archive      = {J_JRSSSC},
  author       = {Mitrodima, Gelly and Oberoi, Jaideep},
  doi          = {10.1093/jrsssc/qlad081},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {1-27},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {CAViaR models for value-at-risk and expected shortfall with long range dependency features},
  volume       = {73},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
