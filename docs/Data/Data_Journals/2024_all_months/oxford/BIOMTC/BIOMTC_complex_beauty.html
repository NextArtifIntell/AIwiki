<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>BIOMTC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="biomtc---200">BIOMTC - 200</h2>
<ul>
<li><details>
<summary>
(2024). Acknowledgment of referees 2024. <em>BIOMTC</em>,
<em>80</em>(4), ujae162. (<a
href="https://doi.org/10.1093/biomtc/ujae162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  doi          = {10.1093/biomtc/ujae162},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae162},
  shortjournal = {Biometrics},
  title        = {Acknowledgment of referees 2024},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structured feature ranking for genomic marker identification
accommodating multiple types of networks. <em>BIOMTC</em>,
<em>80</em>(4), ujae158. (<a
href="https://doi.org/10.1093/biomtc/ujae158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous statistical methods have been developed to search for genomic markers associated with the development, progression, and response to treatment of complex diseases. Among them, feature ranking plays a vital role due to its intuitive formulation and computational efficiency. However, most of the existing methods are based on the marginal importance of molecular predictors and share the limitation that the dependence (network) structures among predictors are not well accommodated, where a disease phenotype usually reflects various biological processes that interact in a complex network. In this paper, we propose a structured feature ranking method for identifying genomic markers, where such network structures are effectively accommodated using Laplacian regularization. The proposed method innovatively investigates multiple network scenarios, where the networks can be known a priori and data-dependently estimated. In addition, we rigorously explore the noise and uncertainty in the networks and control their impacts with proper selection of tuning parameters. These characteristics make the proposed method enjoy especially broad applicability. Theoretical result of our proposal is rigorously established. Compared to the original marginal measure, the proposed network structured measure can achieve sure screening properties with a faster convergence rate under mild conditions. Extensive simulations and analysis of The Cancer Genome Atlas melanoma data demonstrate the improvement of finite sample performance and practical usefulness of the proposed method.},
  archive      = {J_BIOMTC},
  author       = {Ge, Yeheng and Li, Tao and Feng, Xingdong and Wu, Mengyun and Liu, Hailong},
  doi          = {10.1093/biomtc/ujae158},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae158},
  shortjournal = {Biometrics},
  title        = {Structured feature ranking for genomic marker identification accommodating multiple types of networks},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatially adaptive variable screening in presurgical
functional magnetic resonance imaging data analysis. <em>BIOMTC</em>,
<em>80</em>(4), ujae157. (<a
href="https://doi.org/10.1093/biomtc/ujae157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate delineation of functional brain regions adjacent to tumors is imperative for planning neurosurgery that preserves critical functions. Functional magnetic resonance imaging (fMRI) plays an increasingly pivotal role in presurgical counseling and planning. In the analysis of presurgical fMRI data, the impact of false negatives on patients surpasses that of false positives because failure to identify functional regions and unintentionally resecting critical tissues can result in severe harm to patients. This paper introduces a novel metric, the Bayesian missed discovery rate (BMDR), designed for controlling false negatives within the voxel-specific mixture model. Building on the BMDR metric, we propose a new variable screening procedure that not only ensures effective control of false negatives but also capitalizes on the spatial structure of fMRI data. In comparison to existing statistical methods in fMRI data analysis, our new procedure directly regulates false negatives at a desirable level and is entirely data-driven. Moreover, it significantly differs from current false-negative control procedures by incorporating spatial information. Numerical examples demonstrate that the new method outperforms several state-of-the-art methods in retaining signal voxels, particularly the subtle ones at the boundaries of functional regions, while achieving a cleaner separation of functional regions from background noise. These findings hold promising implications for planning function-preserving neurosurgery.},
  archive      = {J_BIOMTC},
  author       = {Hu, Yifei and Jeng, Xinge Jessie},
  doi          = {10.1093/biomtc/ujae157},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae157},
  shortjournal = {Biometrics},
  title        = {Spatially adaptive variable screening in presurgical functional magnetic resonance imaging data analysis},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). De-biasing the bias: Methods for improving disparity
assessments with noisy group measurements. <em>BIOMTC</em>,
<em>80</em>(4), ujae155. (<a
href="https://doi.org/10.1093/biomtc/ujae155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Health care decisions are increasingly informed by clinical decision support algorithms, but these algorithms may perpetuate or increase racial and ethnic disparities in access to and quality of health care. Further complicating the problem, clinical data often have missing or poor quality racial and ethnic information, which can lead to misleading assessments of algorithmic bias. We present novel statistical methods that allow for the use of probabilities of racial/ethnic group membership in assessments of algorithm performance and quantify the statistical bias that results from error in these imputed group probabilities. We propose a sensitivity analysis approach to estimating the statistical bias that allows practitioners to assess disparities in algorithm performance under a range of assumed levels of group probability error. We also prove theoretical bounds on the statistical bias for a set of commonly used fairness metrics and describe real-world scenarios where our theoretical results are likely to apply. We present a case study using imputed race and ethnicity from the modified Bayesian Improved First and Surname Geocoding algorithm for estimation of disparities in a clinical decision support algorithm used to inform osteoporosis treatment. Our novel methods allow policymakers to understand the range of potential disparities under a given algorithm even when race and ethnicity information is missing and to make informed decisions regarding the implementation of machine learning for clinical decision support.},
  archive      = {J_BIOMTC},
  author       = {Wastvedt, Solvejg and Snoke, Joshua and Agniel, Denis and Lai, Julie and Elliott, Marc N and Martino, Steven C},
  doi          = {10.1093/biomtc/ujae155},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae155},
  shortjournal = {Biometrics},
  title        = {De-biasing the bias: Methods for improving disparity assessments with noisy group measurements},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-parametric sensitivity analysis for trials with
irregular and informative assessment times. <em>BIOMTC</em>,
<em>80</em>(4), ujae154. (<a
href="https://doi.org/10.1093/biomtc/ujae154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many trials are designed to collect outcomes at or around pre-specified times after randomization. If there is variability in the times when participants are actually assessed, this can pose a challenge to learning the effect of treatment, since not all participants have outcome assessments at the times of interest. Furthermore, observed outcome values may not be representative of all participants’ outcomes at a given time. Methods have been developed that account for some types of such irregular and informative assessment times; however, since these methods rely on untestable assumptions, sensitivity analyses are needed. We develop a sensitivity analysis methodology that is benchmarked at the explainable assessment (EA) assumption, under which assessment and outcomes at each time are related only through data collected prior to that time. Our method uses an exponential tilting assumption, governed by a sensitivity analysis parameter, that posits deviations from the EA assumption. Our inferential strategy is based on a new influence function-based, augmented inverse intensity-weighted estimator. Our approach allows for flexible semiparametric modeling of the observed data, which is separated from specification of the sensitivity parameter. We apply our method to a randomized trial of low-income individuals with uncontrolled asthma, and we illustrate implementation of our estimation procedure in detail.},
  archive      = {J_BIOMTC},
  author       = {Smith, Bonnie B and Gao, Yujing and Yang, Shu and Varadhan, Ravi and Apter, Andrea J and Scharfstein, Daniel O},
  doi          = {10.1093/biomtc/ujae154},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae154},
  shortjournal = {Biometrics},
  title        = {Semi-parametric sensitivity analysis for trials with irregular and informative assessment times},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Debiased high-dimensional regression calibration for
errors-in-variables log-contrast models. <em>BIOMTC</em>,
<em>80</em>(4), ujae153. (<a
href="https://doi.org/10.1093/biomtc/ujae153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the challenges in analyzing gut microbiome and metagenomic data, this work aims to tackle the issue of measurement errors in high-dimensional regression models that involve compositional covariates. This paper marks a pioneering effort in conducting statistical inference on high-dimensional compositional data affected by mismeasured or contaminated data. We introduce a calibration approach tailored for the linear log-contrast model. Under relatively lenient conditions regarding the sparsity level of the parameter, we have established the asymptotic normality of the estimator for inference. Numerical experiments and an application in microbiome study have demonstrated the efficacy of our high-dimensional calibration strategy in minimizing bias and achieving the expected coverage rates for confidence intervals. Moreover, the potential application of our proposed methodology extends well beyond compositional data, suggesting its adaptability for a wide range of research contexts.},
  archive      = {J_BIOMTC},
  author       = {Zhao, Huali and Wang, Tianying},
  doi          = {10.1093/biomtc/ujae153},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae153},
  shortjournal = {Biometrics},
  title        = {Debiased high-dimensional regression calibration for errors-in-variables log-contrast models},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive randomization methods for sequential multiple
assignment randomized trials (smarts) via thompson sampling.
<em>BIOMTC</em>, <em>80</em>(4), ujae152. (<a
href="https://doi.org/10.1093/biomtc/ujae152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Response-adaptive randomization (RAR) has been studied extensively in conventional, single-stage clinical trials, where it has been shown to yield ethical and statistical benefits, especially in trials with many treatment arms. However, RAR and its potential benefits are understudied in sequential multiple assignment randomized trials (SMARTs), which are the gold-standard trial design for evaluation of multi-stage treatment regimes. We propose a suite of RAR algorithms for SMARTs based on Thompson Sampling (TS), a widely used RAR method in single-stage trials in which treatment randomization probabilities are aligned with the estimated probability that the treatment is optimal. We focus on two common objectives in SMARTs: (1) comparison of the regimes embedded in the trial and (2) estimation of an optimal embedded regime. We develop valid post-study inferential procedures for treatment regimes under the proposed algorithms. This is nontrivial, as even in single-stage settings standard estimators of an average treatment effect can have nonnormal asymptotic behavior under RAR. Our algorithms are the first for RAR in multi-stage trials that account for non-standard limiting behavior due to RAR. Empirical studies based on real-world SMARTs show that TS can improve in-trial subject outcomes without sacrificing efficiency for post-trial comparisons.},
  archive      = {J_BIOMTC},
  author       = {Norwood, Peter and Davidian, Marie and Laber, Eric},
  doi          = {10.1093/biomtc/ujae152},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae152},
  shortjournal = {Biometrics},
  title        = {Adaptive randomization methods for sequential multiple assignment randomized trials (smarts) via thompson sampling},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graphical model inference with external network data.
<em>BIOMTC</em>, <em>80</em>(4), ujae151. (<a
href="https://doi.org/10.1093/biomtc/ujae151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A frequent challenge when using graphical models in practice is that the sample size is limited relative to the number of parameters. They also become hard to interpret when the number of variables p gets large. We consider applications where one has external data, in the form of networks between variables, that can improve inference and help interpret the fitted model. An example of interest regards the interplay between social media and the co-evolution of the COVID-19 pandemic across USA counties. We develop a spike-and-slab prior framework that depicts how partial correlations depend on the networks, by regressing the edge probabilities, average partial correlations, and their variance on the networks. The goal is to detect when the network data relates to the graphical model and, if so, explain how. We develop computational schemes and software in R and probabilistic programming languages. Our applications show that incorporating network data can improve interpretation, statistical accuracy, and out-of-sample prediction.},
  archive      = {J_BIOMTC},
  author       = {Jewson, Jack and Li, Li and Battaglia, Laura and Hansen, Stephen and Rossell, David and Zwiernik, Piotr},
  doi          = {10.1093/biomtc/ujae151},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae151},
  shortjournal = {Biometrics},
  title        = {Graphical model inference with external network data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Time-dependent prognostic accuracy measures for recurrent
event data. <em>BIOMTC</em>, <em>80</em>(4), ujae150. (<a
href="https://doi.org/10.1093/biomtc/ujae150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many clinical contexts, the event of interest could occur multiple times for the same patient. Considerable advancement has been made on developing recurrent event models based on or that use biomarker information. However, less attention has been given to evaluating the prognostic accuracy of a biomarker or a composite score obtained from a fitted recurrent event-rate model. In this manuscript, we propose novel measures to characterize the prognostic accuracy of a marker measured at baseline in the presence of recurrent events. The proposed estimators are based on a semiparametric frailty model that accounts for the informativeness of a marker and unobserved heterogeneity among patients with respect to the rate of event occurrence. We investigate the asymptotic properties of the proposed accuracy estimators and demonstrate these estimators’ finite sample performance through simulation studies. The proposed estimators have minimal bias and appropriate coverage. The estimators are applied to evaluate the performance of a baseline forced expiratory volume, a measure of lung capacity, for repeated episodes of pulmonary exacerbations in patients with cystic fibrosis.},
  archive      = {J_BIOMTC},
  author       = {Dey, R and Schaubel, D E and Hanley, J A and Saha-Chaudhuri, P},
  doi          = {10.1093/biomtc/ujae150},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae150},
  shortjournal = {Biometrics},
  title        = {Time-dependent prognostic accuracy measures for recurrent event data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient joint model for high dimensional longitudinal
and survival data via generic association features. <em>BIOMTC</em>,
<em>80</em>(4), ujae149. (<a
href="https://doi.org/10.1093/biomtc/ujae149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a prognostic method called FLASH that addresses the problem of joint modeling of longitudinal data and censored durations when a large number of both longitudinal and time-independent features are available. In the literature, standard joint models are either of the shared random effect or joint latent class type. Combining ideas from both worlds and using appropriate regularization techniques, we define a new model with the ability to automatically identify significant prognostic longitudinal features in a high-dimensional context, which is of increasing importance in many areas such as personalized medicine or churn prediction. We develop an estimation methodology based on the expectation–maximization algorithm and provide an efficient implementation. The statistical performance of the method is demonstrated both in extensive Monte Carlo simulation studies and on publicly available medical datasets. Our method significantly outperforms the state-of-the-art joint models in terms of C-index in a so-called “real-time” prediction setting, with a computational speed that is orders of magnitude faster than competing methods. In addition, our model automatically identifies significant features that are relevant from a practical point of view, making it interpretable, which is of the greatest importance for a prognostic algorithm in healthcare.},
  archive      = {J_BIOMTC},
  author       = {Nguyen, Van Tuan and Fermanian, Adeline and Barbieri, Antoine and Zohar, Sarah and Jannot, Anne-Sophie and Bussy, Simon and Guilloux, Agathe},
  doi          = {10.1093/biomtc/ujae149},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae149},
  shortjournal = {Biometrics},
  title        = {An efficient joint model for high dimensional longitudinal and survival data via generic association features},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation of a genetic gaussian network using GWAS summary
data. <em>BIOMTC</em>, <em>80</em>(4), ujae148. (<a
href="https://doi.org/10.1093/biomtc/ujae148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A genetic Gaussian network of multiple phenotypes, constructed through the inverse matrix of the genetic correlation matrix, is informative for understanding the biological dependencies of the phenotypes. However, its estimation may be challenging because the genetic correlation estimates are biased due to estimation errors and idiosyncratic pleiotropy inherent in GWAS summary statistics. Here, we introduce a novel approach called estimation of genetic graph (EGG), which eliminates the estimation error bias and idiosyncratic pleiotropy bias with the same techniques used in multivariable Mendelian randomization. The genetic network estimated by EGG can be interpreted as shared common biological contributions between phenotypes, conditional on others. We use both simulations and real data to demonstrate the superior efficacy of our novel method in comparison with the traditional network estimators.},
  archive      = {J_BIOMTC},
  author       = {Yang, Yihe and Lorincz-Comi, Noah and Zhu, Xiaofeng},
  doi          = {10.1093/biomtc/ujae148},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae148},
  shortjournal = {Biometrics},
  title        = {Estimation of a genetic gaussian network using GWAS summary data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A likelihood approach to incorporating self-report data in
HIV recency classification. <em>BIOMTC</em>, <em>80</em>(4), ujae147.
(<a href="https://doi.org/10.1093/biomtc/ujae147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating new HIV infections is significant yet challenging due to the difficulty in distinguishing between recent and long-term infections. We demonstrate that HIV recency status (recent versus long-term) could be determined from self-report testing history and biomarkers, which are increasingly available in bio-behavioral surveys. HIV recency status is partially observed, given the self-report testing history. For example, people who tested positive for HIV over 1 year ago should have a long-term infection. Based on the nationally representative samples collected by the Population-based HIV Impact Assessment (PHIA) Project, we propose a likelihood-based probabilistic model for HIV recency classification. The model incorporates individuals with known recency status based on testing histories and individuals whose recency status could not be determined and integrates the mechanism of how HIV recency status depends on biomarkers and the mechanism of how HIV recency status, together with the self-report time of the most recent HIV test, impacts the test results. We compare our method to logistic regression and the binary classification tree (current practice) on Malawi PHIA data, as well as on simulated data. Our model obtains more efficient and less biased parameter estimates and is relatively robust to potential reporting error and model misspecification.},
  archive      = {J_BIOMTC},
  author       = {Yang, Wenlong and Liu, Danping and Bao, Le and Li, Runze},
  doi          = {10.1093/biomtc/ujae147},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae147},
  shortjournal = {Biometrics},
  title        = {A likelihood approach to incorporating self-report data in HIV recency classification},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unlocking the power of multi-institutional data: Integrating
and harmonizing genomic data across institutions. <em>BIOMTC</em>,
<em>80</em>(4), ujae146. (<a
href="https://doi.org/10.1093/biomtc/ujae146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer is a complex disease driven by genomic alterations, and tumor sequencing is becoming a mainstay of clinical care for cancer patients. The emergence of multi-institution sequencing data presents a powerful resource for learning real-world evidence to enhance precision oncology. GENIE BPC, led by American Association for Cancer Research, establishes a unique database linking genomic data with clinical information for patients treated at multiple cancer centers. However, leveraging sequencing data from multiple institutions presents significant challenges. Variability in gene panels can lead to loss of information when analyses focus on genes common across panels. Additionally, differences in sequencing techniques and patient heterogeneity across institutions add complexity. High data dimensionality, sparse gene mutation patterns, and weak signals at the individual gene level further complicate matters. Motivated by these real-world challenges, we introduce the Bridge model. It uses a quantile-matched latent variable approach to derive integrated features to preserve information beyond common genes and maximize the utilization of all available data, while leveraging information sharing to enhance both learning efficiency and the model’s capacity to generalize. By extracting harmonized and noise-reduced lower-dimensional latent variables, the true mutation pattern unique to each individual is captured. We assess model’s performance and parameter estimation through extensive simulation studies. The extracted latent features from the Bridge model consistently excel in predicting patient survival across six cancer types in GENIE BPC data.},
  archive      = {J_BIOMTC},
  author       = {Chen, Yuan and Shen, Ronglai and Feng, Xiwen and Panageas, Katherine},
  doi          = {10.1093/biomtc/ujae146},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae146},
  shortjournal = {Biometrics},
  title        = {Unlocking the power of multi-institutional data: Integrating and harmonizing genomic data across institutions},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian framework for causal analysis of recurrent events
with timing misalignment. <em>BIOMTC</em>, <em>80</em>(4), ujae145. (<a
href="https://doi.org/10.1093/biomtc/ujae145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Observational studies of recurrent event rates are common in biomedical statistics. Broadly, the goal is to estimate differences in event rates under 2 treatments within a defined target population over a specified follow-up window. Estimation with observational data is challenging because, while membership in the target population is defined in terms of eligibility criteria, treatment is rarely observed exactly at the time of eligibility. Ad hoc solutions to this timing misalignment can induce bias by incorrectly attributing prior event counts and person-time to treatment. Even if eligibility and treatment are aligned, a terminal event process (eg, death) often stops the recurrent event process of interest. In practice, both processes can be censored so that events are not observed over the entire follow-up window. Our approach addresses misalignment by casting it as a time-varying treatment problem: some patients are on treatment at eligibility while others are off treatment but may switch to treatment at a specified time—if they survive long enough. We define and identify an average causal effect estimand under right-censoring. Estimation is done using a g-computation procedure with a joint semiparametric Bayesian model for the death and recurrent event processes. We apply the method to contrast hospitalization rates among patients with different opioid treatments using Medicare insurance claims data.},
  archive      = {J_BIOMTC},
  author       = {Oganisian, Arman and Girard, Anthony and Steingrimsson, Jon A and Moyo, Patience},
  doi          = {10.1093/biomtc/ujae145},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae145},
  shortjournal = {Biometrics},
  title        = {A bayesian framework for causal analysis of recurrent events with timing misalignment},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust and flexible learning of a high-dimensional
classification rule using auxiliary outcomes. <em>BIOMTC</em>,
<em>80</em>(4), ujae144. (<a
href="https://doi.org/10.1093/biomtc/ujae144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlated outcomes are common in many practical problems. In some settings, one outcome is of particular interest, and others are auxiliary. To leverage information shared by all the outcomes, traditional multi-task learning (MTL) minimizes an averaged loss function over all the outcomes, which may lead to biased estimation for the target outcome, especially when the MTL model is misspecified. In this work, based on a decomposition of estimation bias into two types, within-subspace and against-subspace, we develop a robust transfer learning approach to estimating a high-dimensional linear decision rule for the outcome of interest with the presence of auxiliary outcomes. The proposed method includes an MTL step using all outcomes to gain efficiency and a subsequent calibration step using only the outcome of interest to correct both types of biases. We show that the final estimator can achieve a lower estimation error than the one using only the single outcome of interest. Simulations and real data analysis are conducted to justify the superiority of the proposed method.},
  archive      = {J_BIOMTC},
  author       = {Liang, Muxuan and Park, Jaeyoung and Lu, Qing and Zhong, Xiang},
  doi          = {10.1093/biomtc/ujae144},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae144},
  shortjournal = {Biometrics},
  title        = {Robust and flexible learning of a high-dimensional classification rule using auxiliary outcomes},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian joint model for mediation analysis with
matrix-valued mediators. <em>BIOMTC</em>, <em>80</em>(4), ujae143. (<a
href="https://doi.org/10.1093/biomtc/ujae143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unscheduled treatment interruptions may lead to reduced quality of care in radiation therapy (RT). Identifying the RT prescription dose effects on the outcome of treatment interruptions, mediated through doses distributed into different organs at risk (OARs), can inform future treatment planning. The radiation exposure to OARs can be summarized by a matrix of dose-volume histograms (DVH) for each patient. Although various methods for high-dimensional mediation analysis have been proposed recently, few studies investigated how matrix-valued data can be treated as mediators. In this paper, we propose a novel Bayesian joint mediation model for high-dimensional matrix-valued mediators. In this joint model, latent features are extracted from the matrix-valued data through an adaptation of probabilistic multilinear principal components analysis (MPCA), retaining the inherent matrix structure. We derive and implement a Gibbs sampling algorithm to jointly estimate all model parameters, and introduce a Varimax rotation method to identify active indicators of mediation among the matrix-valued data. Our simulation study finds that the proposed joint model has higher efficiency in estimating causal decomposition effects compared to an alternative two-step method, and demonstrates that the mediation effects can be identified and visualized in the matrix form. We apply the method to study the effect of prescription dose on treatment interruptions in anal canal cancer patients.},
  archive      = {J_BIOMTC},
  author       = {Liu, Zijin and Liu, Zhihui (Amy) and Hosni, Ali and Kim, John and Jiang, Bei and Saarela, Olli},
  doi          = {10.1093/biomtc/ujae143},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae143},
  shortjournal = {Biometrics},
  title        = {A bayesian joint model for mediation analysis with matrix-valued mediators},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint mirror procedure: Controlling false discovery rate for
identifying simultaneous signals. <em>BIOMTC</em>, <em>80</em>(4),
ujae142. (<a href="https://doi.org/10.1093/biomtc/ujae142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many applications, the process of identifying a specific feature of interest often involves testing multiple hypotheses for their joint statistical significance. Examples include mediation analysis, which simultaneously examines the existence of the exposure-mediator and the mediator-outcome effects, and replicability analysis, aiming to identify simultaneous signals that exhibit statistical significance across multiple independent studies. In this work, we present a new approach called the joint mirror (JM) procedure that effectively detects such features while maintaining false discovery rate (FDR) control in finite samples. The JM procedure employs an iterative method that gradually shrinks the rejection region based on progressively revealed information until a conservative estimate of the false discovery proportion is below the target FDR level. Additionally, we introduce a more stringent error measure known as the composite FDR (cFDR), which assigns weights to each false discovery based on its number of null components. We use the leave-one-out technique to prove that the JM procedure controls the cFDR in finite samples. To implement the JM procedure, we propose an efficient algorithm that can incorporate partial ordering information. Through extensive simulations, we show that our procedure effectively controls the cFDR and enhances statistical power across various scenarios, including the case that test statistics are dependent across the features. Finally, we showcase the utility of our method by applying it to real-world mediation and replicability analyses.},
  archive      = {J_BIOMTC},
  author       = {Deng, Linsui and He, Kejun and Zhang, Xianyang},
  doi          = {10.1093/biomtc/ujae142},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae142},
  shortjournal = {Biometrics},
  title        = {Joint mirror procedure: Controlling false discovery rate for identifying simultaneous signals},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive enrichment design using bayesian model averaging
for selection and threshold-identification of predictive variables.
<em>BIOMTC</em>, <em>80</em>(4), ujae141. (<a
href="https://doi.org/10.1093/biomtc/ujae141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precision medicine is transforming healthcare by offering tailored treatments that enhance patient outcomes and reduce costs. As our understanding of complex diseases improves, clinical trials increasingly aim to detect subgroups of patients with enhanced treatment effects. Biomarker-driven adaptive enrichment designs, which initially enroll a broad population and later restrict to treatment-sensitive patients, are gaining popularity. However, current practice often assumes either pre-trial knowledge of biomarkers or a simple, linear relationship between continuous markers and treatment effectiveness. Motivated by a trial studying rheumatoid arthritis treatment, we propose a Bayesian adaptive enrichment design to identify predictive variables from a larger set of candidate biomarkers. Our approach uses a flexible modeling framework where the effects of continuous biomarkers are represented using free knot B-splines. We then estimate key parameters by marginalizing over all possible variable combinations using Bayesian model averaging. At interim analyses, we assess whether a biomarker-defined subgroup has enhanced or reduced treatment effects, allowing for early termination for efficacy or futility and restricting future enrollment to treatment-sensitive patients. We consider both pre-categorized and continuous biomarkers, the latter potentially having complex, nonlinear relationships to the outcome and treatment effect. Through simulations, we derive the operating characteristics of our design and compare its performance to existing methods.},
  archive      = {J_BIOMTC},
  author       = {Maleyeff, Lara and Golchi, Shirin and Moodie, Erica E M and Hudson, Marie},
  doi          = {10.1093/biomtc/ujae141},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae141},
  shortjournal = {Biometrics},
  title        = {An adaptive enrichment design using bayesian model averaging for selection and threshold-identification of predictive variables},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal adaptive SMART designs with binary outcomes.
<em>BIOMTC</em>, <em>80</em>(4), ujae140. (<a
href="https://doi.org/10.1093/biomtc/ujae140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a sequential multiple-assignment randomized trial (SMART), a sequence of treatments is given to a patient over multiple stages. In each stage, randomization may be done to allocate patients to different treatment groups. Even though SMART designs are getting popular among clinical researchers, the methodologies for adaptive randomization at different stages of a SMART are few and not sophisticated enough to handle the complexity of optimal allocation of treatments at every stage of a trial. Lack of optimal allocation methodologies can raise critical concerns about SMART designs from an ethical point of view. In this work, we develop an optimal adaptive allocation procedure using a constrained optimization that minimizes the total expected number of treatment failures for a SMART with a binary primary outcome, subject to a fixed asymptotic variance of a predefined objective function. Issues related to optimal adaptive allocations are explored theoretically with supporting simulations. The applicability of the proposed methodology is demonstrated using a recently conducted SMART study named M-bridge for developing universal and resource-efficient dynamic treatment regimes for incoming first-year college students as a bridge to desirable treatments to address alcohol-related risks.},
  archive      = {J_BIOMTC},
  author       = {Ghosh, Rik and Chakraborty, Bibhas and Nahum-Shani, Inbal and Patrick, Megan E and Ghosh, Palash},
  doi          = {10.1093/biomtc/ujae140},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae140},
  shortjournal = {Biometrics},
  title        = {Optimal adaptive SMART designs with binary outcomes},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A generalized logrank-type test for comparison of treatment
regimes in sequential multiple assignment randomized trials.
<em>BIOMTC</em>, <em>80</em>(4), ujae139. (<a
href="https://doi.org/10.1093/biomtc/ujae139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sequential multiple assignment randomized trial (SMART) is the ideal study design for the evaluation of multistage treatment regimes, which comprise sequential decision rules that recommend treatments for a patient at each of a series of decision points based on their evolving characteristics. A common goal is to compare the set of so-called embedded regimes represented in the design on the basis of a primary outcome of interest. In the study of chronic diseases and disorders, this outcome is often a time to an event, and a goal is to compare the distributions of the time-to-event outcome associated with each regime in the set. We present a general statistical framework in which we develop a logrank-type test for comparison of the survival distributions associated with regimes within a specified set based on the data from a SMART with an arbitrary number of stages that allows incorporation of covariate information to enhance efficiency and can also be used with data from an observational study. The framework provides clarification of the assumptions required to yield a principled test procedure, and the proposed test subsumes or offers an improved alternative to existing methods. We demonstrate performance of the methods in a suite of simulation studies. The methods are applied to a SMART in patients with acute promyelocytic leukemia.},
  archive      = {J_BIOMTC},
  author       = {Tsiatis, Anastasios A and Davidian, Marie},
  doi          = {10.1093/biomtc/ujae139},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae139},
  shortjournal = {Biometrics},
  title        = {A generalized logrank-type test for comparison of treatment regimes in sequential multiple assignment randomized trials},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large-scale survival analysis with a cure fraction.
<em>BIOMTC</em>, <em>80</em>(4), ujae138. (<a
href="https://doi.org/10.1093/biomtc/ujae138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of massive survival data with a cure fraction, large-scale regression for analyzing the effects of risk factors on a general population has become an emerging challenge. This article proposes a new probability-weighted method for estimation and inference for semiparametric cure regression models. We develop a flexible formulation of the mixture cure model consisting of the model-free incidence and the latency assumed by the semiparametric proportional hazards model. The susceptible probability assesses the concordance between the observations and the latency. With the susceptible probability as weight, we propose a weighted estimating equation method in a small-scale setting. Robust nonparametric estimation of the weight permits stable implementation of the estimation of regression parameters. A recursive probability-weighted estimation method based on data blocks with smaller sizes is further proposed, which achieves computational and memory efficiency in a large-scale or online setting. Asymptotic properties of the proposed estimators are established. We conduct simulation studies and a real data application to demonstrate the empirical performance of the proposed method.},
  archive      = {J_BIOMTC},
  author       = {Han, Bo and Wang, Xiaoguang and Sun, Liuquan},
  doi          = {10.1093/biomtc/ujae138},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae138},
  shortjournal = {Biometrics},
  title        = {Large-scale survival analysis with a cure fraction},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cumulative link mixed-effects models in the service of
remote sensing crop progress monitoring. <em>BIOMTC</em>,
<em>80</em>(4), ujae137. (<a
href="https://doi.org/10.1093/biomtc/ujae137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces an innovative cumulative link modeling (CLM) approach to monitor crop progress over large areas using remote sensing data. Two distinct models are developed, a fixed-effects CLM and a mixed-effects one that incorporates annual random effects to capture the inherent inter-seasonal variability. Inference is based on partial-likelihood with two law variations, the standard CLM based on the multinomial distribution and a novel one based on the product binomial distribution. Model performance is evaluated on eight crops, namely corn, oats, sorghum, soybeans, winter wheat, alfalfa, dry beans, and millet, using in-situ data from Nebraska, USA, spanning 20 years. The models utilize the predictive attributes of calendar time, thermal time, and the normalized difference vegetation index. The results demonstrate the wide applicability of this approach to different crops, providing large-scale predictions of crop progress and allowing the estimation of important agronomic parameters. To facilitate reproducibility, an ecosystem of R packages has been developed and made publicly accessible under the name Ages of Man. The packages can be utilized to implement the presented methodology in any area with this type of data, including the USA.},
  archive      = {J_BIOMTC},
  author       = {Oikonomidis, Ioannis and Trevezas, Samis},
  doi          = {10.1093/biomtc/ujae137},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae137},
  shortjournal = {Biometrics},
  title        = {Cumulative link mixed-effects models in the service of remote sensing crop progress monitoring},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multivariate polya tree model for meta-analysis with
event-time distributions. <em>BIOMTC</em>, <em>80</em>(4), ujae136. (<a
href="https://doi.org/10.1093/biomtc/ujae136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a nonparametric Bayesian prior for a family of random probability measures by extending the Polya tree ( ⁠|$\mbox{PT}$|⁠ ) prior to a joint prior for a set of probability measures |$G_1,\dots ,G_n$|⁠ , suitable for meta-analysis with event-time outcomes. In the application to meta-analysis, |$G_i$| is the event-time distribution specific to study |$i$|⁠ . The proposed model defines a regression on study-specific covariates by introducing increased correlation for any pair of studies with similar characteristics. The desired multivariate |$\mbox{PT}$| model is constructed by introducing a hierarchical prior on the conditional splitting probabilities in the |$\mbox{PT}$| construction for each of the |$G_i$|⁠ . The hierarchical prior replaces the independent beta priors for the splitting probability in the PT construction with a Gaussian process prior for corresponding (logit) splitting probabilities across all studies. The Gaussian process is indexed by study-specific covariates, introducing the desired dependence with increased correlation for similar studies. The main feature of the proposed construction is (conditionally) conjugate posterior updating with commonly reported inference summaries for event-time data. The construction is motivated by a meta-analysis over cancer immunotherapy studies.},
  archive      = {J_BIOMTC},
  author       = {Poli, Giovanni and Fountzilas, Elena and Tsimeridou, Apostolia-Maria and Müller, Peter},
  doi          = {10.1093/biomtc/ujae136},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae136},
  shortjournal = {Biometrics},
  title        = {A multivariate polya tree model for meta-analysis with event-time distributions},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating marginal treatment effect in cluster randomized
trials with multi-level missing outcomes. <em>BIOMTC</em>,
<em>80</em>(4), ujae135. (<a
href="https://doi.org/10.1093/biomtc/ujae135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyses of cluster randomized trials (CRTs) can be complicated by informative missing outcome data. Methods such as inverse probability weighted generalized estimating equations have been proposed to account for informative missingness by weighing the observed individual outcome data in each cluster. These existing methods have focused on settings where missingness occurs at the individual level and each cluster has partially or fully observed individual outcomes. In the presence of missing clusters, for example, all outcomes from a cluster are missing due to drop-out of the cluster, these approaches ignore this cluster-level missingness and can lead to biased inference if the cluster-level missingness is informative. Informative missingness at multiple levels can also occur in CRTs with a multi-level structure where study participants are nested in subclusters such as healthcare providers, and the subclusters are nested in clusters such as clinics. In this paper, we propose new estimators for estimating the marginal treatment effect in CRTs accounting for missing outcome data at multiple levels based on weighted generalized estimating equations. We show that the proposed multi-level multiply robust estimator is consistent and asymptotically normally distributed provided that one of the multiple propensity score models postulated at each clustering level is correctly specified. We evaluate the performance of the proposed method through extensive simulations and illustrate its use with a CRT evaluating a Malaria risk-reduction intervention in rural Madagascar.},
  archive      = {J_BIOMTC},
  author       = {Chang, Chia-Rui and Wang, Rui},
  doi          = {10.1093/biomtc/ujae135},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae135},
  shortjournal = {Biometrics},
  title        = {Estimating marginal treatment effect in cluster randomized trials with multi-level missing outcomes},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An exploratory penalized regression to identify combined
effects of temporal variables—application to agri-environmental issues.
<em>BIOMTC</em>, <em>80</em>(4), ujae134. (<a
href="https://doi.org/10.1093/biomtc/ujae134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of sensors is opening new avenues in several fields of activity. Concerning agricultural crops, complex combinations of agri-environmental dynamics, such as soil and climate variables, are now commonly recorded. These new kinds of measurements are an opportunity to improve knowledge of the drivers of crop yield and crop quality at harvest. This involves renewing statistical approaches to account for the combined variations of these dynamic variables, here considered as temporal variables. The objective of the paper is to estimate an interpretable model to study the influence of the two combined inputs on a scalar output. A Sparse and Structured Procedure is proposed to Identify Combined Effects of Formatted temporal Predictors, hereafter denoted S pice FP. The method is based on the transformation of both temporal variables into categorical variables by defining joint modalities, from which a collection of multiple regression models is then derived. The regressors are the frequencies associated with joint class intervals. The class intervals and related regression coefficients are determined using a generalized fused lasso. S pice FP is a generic and exploratory approach. The simulations we performed show that it is flexible enough to select the non-null or influential modalities of values. A motivating example for grape quality is presented.},
  archive      = {J_BIOMTC},
  author       = {Fontez, Bénedicte and Loisel, Patrice and Simonneau, Thierry and Hilgert, Nadine},
  doi          = {10.1093/biomtc/ujae134},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae134},
  shortjournal = {Biometrics},
  title        = {An exploratory penalized regression to identify combined effects of temporal variables—application to agri-environmental issues},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Principles of psychological assessment, with applied
examples in r. By isaac t. Petersen, chapman and hall/CRC, 2024, ISBN:
9781032413068
https://www.routledge.com/principles-of-psychological-assessment-with-applied-examples-in-r/petersen/p/book/9781032413068.
<em>BIOMTC</em>, <em>80</em>(4), ujae133. (<a
href="https://doi.org/10.1093/biomtc/ujae133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Dumenci, Levent},
  doi          = {10.1093/biomtc/ujae133},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae133},
  shortjournal = {Biometrics},
  title        = {Principles of psychological assessment, with applied examples in r. by isaac t. petersen, chapman and Hall/CRC, 2024, ISBN: 9781032413068 https://www.routledge.com/Principles-of-psychological-assessment-with-applied-examples-in-R/Petersen/p/book/9781032413068},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian pathway analysis over brain network mediators for
survival data. <em>BIOMTC</em>, <em>80</em>(4), ujae132. (<a
href="https://doi.org/10.1093/biomtc/ujae132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technological advancements in noninvasive imaging facilitate the construction of whole brain interconnected networks, known as brain connectivity. Existing approaches to analyze brain connectivity frequently disaggregate the entire network into a vector of unique edges or summary measures, leading to a substantial loss of information. Motivated by the need to explore the effect mechanism among genetic exposure, brain connectivity, and time to disease onset with maximum information extraction, we propose a Bayesian approach to model the effect pathway between each of these components while quantifying the mediating role of brain networks. To accommodate the biological architectures of brain connectivity constructed along white matter fiber tracts, we develop a structural model which includes a symmetric matrix-variate accelerated failure time model for disease onset and a symmetric matrix response regression for the network-variate mediator. We further impose within-graph sparsity and between-graph shrinkage to identify informative network configurations and eliminate the interference of noisy components. Simulations are carried out to confirm the advantages of our proposed method over existing alternatives. By applying the proposed method to the landmark Alzheimer’s Disease Neuroimaging Initiative study, we obtain neurobiologically plausible insights that may inform future intervention strategies.},
  archive      = {J_BIOMTC},
  author       = {Tian, Xinyuan and Li, Fan and Shen, Li and Esserman, Denise and Zhao, Yize},
  doi          = {10.1093/biomtc/ujae132},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae132},
  shortjournal = {Biometrics},
  title        = {Bayesian pathway analysis over brain network mediators for survival data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic factor analysis with dependent gaussian processes
for high-dimensional gene expression trajectories. <em>BIOMTC</em>,
<em>80</em>(4), ujae131. (<a
href="https://doi.org/10.1093/biomtc/ujae131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing availability of high-dimensional, longitudinal measures of gene expression can facilitate understanding of biological mechanisms, as required for precision medicine. Biological knowledge suggests that it may be best to describe complex diseases at the level of underlying pathways, which may interact with one another. We propose a Bayesian approach that allows for characterizing such correlation among different pathways through dependent Gaussian processes (DGP) and mapping the observed high-dimensional gene expression trajectories into unobserved low-dimensional pathway expression trajectories via Bayesian sparse factor analysis. Our proposal is the first attempt to relax the classical assumption of independent factors for longitudinal data and has demonstrated a superior performance in recovering the shape of pathway expression trajectories, revealing the relationships between genes and pathways, and predicting gene expressions (closer point estimates and narrower predictive intervals), as demonstrated through simulations and real data analysis. To fit the model, we propose a Monte Carlo expectation maximization (MCEM) scheme that can be implemented conveniently by combining a standard Markov Chain Monte Carlo sampler and an R package GPFDA,which returns the maximum likelihood estimates of DGP hyperparameters. The modular structure of MCEM makes it generalizable to other complex models involving the DGP model component. Our R package DGP4LCF that implements the proposed approach is available on the Comprehensive R Archive Network (CRAN).},
  archive      = {J_BIOMTC},
  author       = {Cai, Jiachen and Goudie, Robert J B and Starr, Colin and Tom, Brian D M},
  doi          = {10.1093/biomtc/ujae131},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae131},
  shortjournal = {Biometrics},
  title        = {Dynamic factor analysis with dependent gaussian processes for high-dimensional gene expression trajectories},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hierarchical random effects state-space model for modeling
brain activities from electroencephalogram data. <em>BIOMTC</em>,
<em>80</em>(4), ujae130. (<a
href="https://doi.org/10.1093/biomtc/ujae130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mental disorders present challenges in diagnosis and treatment due to their complex and heterogeneous nature. Electroencephalogram (EEG) has shown promise as a source of potential biomarkers for these disorders. However, existing methods for analyzing EEG signals have limitations in addressing heterogeneity and capturing complex brain activity patterns between regions. This paper proposes a novel random effects state-space model (RESSM) for analyzing large-scale multi-channel resting-state EEG signals, accounting for the heterogeneity of brain connectivities between groups and individual subjects. We incorporate multi-level random effects for temporal dynamical and spatial mapping matrices and address non-stationarity so that the brain connectivity patterns can vary over time. The model is fitted under a Bayesian hierarchical model framework coupled with a Gibbs sampler. Compared to previous mixed-effects state-space models, we directly model high-dimensional random effects matrices of interest without structural constraints and tackle the challenge of identifiability. Through extensive simulation studies, we demonstrate that our approach yields valid estimation and inference. We apply RESSM to a multi-site clinical trial of major depressive disorder (MDD). Our analysis uncovers significant differences in resting-state brain temporal dynamics among MDD patients compared to healthy individuals. In addition, we show the subject-level EEG features derived from RESSM exhibit a superior predictive value for the heterogeneous treatment effect compared to the EEG frequency band power, suggesting the potential of EEG as a valuable biomarker for MDD.},
  archive      = {J_BIOMTC},
  author       = {Guo, Xingche and Yang, Bin and Loh, Ji Meng and Wang, Qinxia and Wang, Yuanjia},
  doi          = {10.1093/biomtc/ujae130},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae130},
  shortjournal = {Biometrics},
  title        = {A hierarchical random effects state-space model for modeling brain activities from electroencephalogram data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sensitivity analysis for studies transporting prediction
models. <em>BIOMTC</em>, <em>80</em>(4), ujae129. (<a
href="https://doi.org/10.1093/biomtc/ujae129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider estimation of measures of model performance in a target population when covariate and outcome data are available from a source population and covariate data, but not outcome data, are available from the target population. In this setting, identification of measures of model performance is possible under an untestable assumption that the outcome and population (source or target) are independent conditional on covariates. In practice, this assumption is uncertain and, in some cases, controversial. Therefore, sensitivity analysis may be useful for examining the impact of assumption violations on inferences about model performance. Here, we propose an exponential tilt sensitivity analysis model and develop statistical methods to determine how measures of model performance are affected by violations of the assumption of conditional independence between outcome and population. We provide identification results and estimators for the risk in the target population under the sensitivity analysis model, examine the large-sample properties of the estimators, and apply them to data on lung cancer screening.},
  archive      = {J_BIOMTC},
  author       = {Steingrimsson, Jon A and Robertson, Sarah E and Voter, Sarah and Dahabreh, Issa J},
  doi          = {10.1093/biomtc/ujae129},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae129},
  shortjournal = {Biometrics},
  title        = {Sensitivity analysis for studies transporting prediction models},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust model averaging approach by mallows-type criterion.
<em>BIOMTC</em>, <em>80</em>(4), ujae128. (<a
href="https://doi.org/10.1093/biomtc/ujae128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model averaging is an important tool for treating uncertainty from model selection process and fusing information from different models, and has been widely used in various fields. However, the most existing model averaging criteria are proposed based on the methods of ordinary least squares or maximum likelihood, which possess high sensitivity to outliers or violation of certain model assumption. For the mean regression, no optimal robust methods are developed. To fill this gap, in our paper, we propose an outlier-robust model averaging approach by Mallows-type criterion. The idea is that we first construct a generalized M (GM) estimator for each candidate model, and then build robust weighting schemes by the asymptotic expansion of the final prediction error based on the GM-type loss function. So, we can still achieve a trustworthy result even if the dataset is contaminated by outliers in response and/or covariates. Asymptotic properties of the proposed robust model averaging estimators are established under some regularity conditions. The consistency of our weight estimators tending to the theoretically optimal weight vectors is also derived. We prove that our model averaging estimator is robust in terms of having bounded influence function. Further, we define the empirical prediction influence function to evaluate the quantitative robustness of the model averaging estimator. A simulation study and a real data analysis are conducted to demonstrate the finite sample performance of our estimators and compare them with other commonly used model selection and averaging methods.},
  archive      = {J_BIOMTC},
  author       = {Wang, Miaomiao and You, Kang and Zhu, Lixing and Zou, Guohua},
  doi          = {10.1093/biomtc/ujae128},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae128},
  shortjournal = {Biometrics},
  title        = {Robust model averaging approach by mallows-type criterion},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wasserstein regression with empirical measures and density
estimation for sparse data. <em>BIOMTC</em>, <em>80</em>(4), ujae127.
(<a href="https://doi.org/10.1093/biomtc/ujae127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of modeling the relationship between univariate distributions and one or more explanatory variables lately has found increasing interest. Existing approaches proceed by substituting proxy estimated distributions for the typically unknown response distributions. These estimates are obtained from available data but are problematic when for some of the distributions only few data are available. Such situations are common in practice and cannot be addressed with currently available approaches, especially when one aims at density estimates. We show how this and other problems associated with density estimation such as tuning parameter selection and bias issues can be side-stepped when covariates are available. We also introduce a novel version of distribution-response regression that is based on empirical measures. By avoiding the preprocessing step of recovering complete individual response distributions, the proposed approach is applicable when the sample size available for each distribution varies and especially when it is small for some of the distributions but large for others. In this case, one can still obtain consistent distribution estimates even for distributions with only few data by gaining strength across the entire sample of distributions, while traditional approaches where distributions or densities are estimated individually fail, since sparsely sampled densities cannot be consistently estimated. The proposed model is demonstrated to outperform existing approaches through simulations and Environmental Influences on Child Health Outcomes data.},
  archive      = {J_BIOMTC},
  author       = {Zhou, Yidong and Müller, Hans-Georg},
  doi          = {10.1093/biomtc/ujae127},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae127},
  shortjournal = {Biometrics},
  title        = {Wasserstein regression with empirical measures and density estimation for sparse data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fundamentals of probability, fifth edition by saeed
ghahramani, chapman and hall/CRC, 2024, ISBN: 9781032366081,
https://www.routledge.com/fundamentals-of-probability/ghahramani/p/book/9781032366081.
<em>BIOMTC</em>, <em>80</em>(4), ujae126. (<a
href="https://doi.org/10.1093/biomtc/ujae126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Spieker, Andrew J},
  doi          = {10.1093/biomtc/ujae126},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae126},
  shortjournal = {Biometrics},
  title        = {Fundamentals of probability, fifth edition by saeed ghahramani, chapman and Hall/CRC, 2024, ISBN: 9781032366081, https://www.routledge.com/Fundamentals-of-probability/Ghahramani/p/book/9781032366081},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new robust approach for the polytomous logistic regression
model based on rényi’s pseudodistances. <em>BIOMTC</em>, <em>80</em>(4),
ujae125. (<a href="https://doi.org/10.1093/biomtc/ujae125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a robust alternative to the maximum likelihood estimator (MLE) for the polytomous logistic regression model, known as the family of minimum Rènyi Pseudodistance (RP) estimators. The proposed minimum RP estimators are parametrized by a tuning parameter |$\alpha \ge 0$|⁠ , and include the MLE as a special case when |$\alpha =0$|⁠ . These estimators, along with a family of RP-based Wald-type tests, are shown to exhibit superior performance in the presence of misclassification errors. The paper includes an extensive simulation study and a real data example to illustrate the robustness of these proposed statistics.},
  archive      = {J_BIOMTC},
  author       = {Castilla, Elena},
  doi          = {10.1093/biomtc/ujae125},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae125},
  shortjournal = {Biometrics},
  title        = {A new robust approach for the polytomous logistic regression model based on rényi’s pseudodistances},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Clustering computer mouse tracking data with informed
hierarchical shrinkage partition priors. <em>BIOMTC</em>,
<em>80</em>(4), ujae124. (<a
href="https://doi.org/10.1093/biomtc/ujae124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mouse-tracking data, which record computer mouse trajectories while participants perform an experimental task, provide valuable insights into subjects’ underlying cognitive processes. Neuroscientists are interested in clustering the subjects’ responses during computer mouse-tracking tasks to reveal patterns of individual decision-making behaviors and identify population subgroups with similar neurobehavioral responses. These data can be combined with neuroimaging data to provide additional information for personalized interventions. In this article, we develop a novel hierarchical shrinkage partition (HSP) prior for clustering summary statistics derived from the trajectories of mouse-tracking data. The HSP model defines a subjects’ cluster as a set of subjects that gives rise to more similar (rather than identical) nested partitions of the conditions. The proposed model can incorporate prior information about the partitioning of either subjects or conditions to facilitate clustering, and it allows for deviations of the nested partitions within each subject group. These features distinguish the HSP model from other bi-clustering methods that typically create identical nested partitions of conditions within a subject group. Furthermore, it differs from existing nested clustering methods, which define clusters based on common parameters in the sampling model and identify subject groups by different distributions. We illustrate the unique features of the HSP model on a mouse tracking dataset from a pilot study and in simulation studies. Our results show the ability and effectiveness of the proposed exploratory framework in clustering and revealing possible different behavioral patterns across subject groups.},
  archive      = {J_BIOMTC},
  author       = {Song, Ziyi and Shen, Weining and Vannucci, Marina and Baldizon, Alexandria and Cinciripini, Paul M and Versace, Francesco and Guindani, Michele},
  doi          = {10.1093/biomtc/ujae124},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae124},
  shortjournal = {Biometrics},
  title        = {Clustering computer mouse tracking data with informed hierarchical shrinkage partition priors},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How to achieve model-robust inference in stepped wedge
trials with model-based methods? <em>BIOMTC</em>, <em>80</em>(4),
ujae123. (<a href="https://doi.org/10.1093/biomtc/ujae123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A stepped wedge design is an unidirectional crossover design where clusters are randomized to distinct treatment sequences. While model-based analysis of stepped wedge designs is a standard practice to evaluate treatment effects accounting for clustering and adjusting for covariates, their properties under misspecification have not been systematically explored. In this article, we focus on model-based methods, including linear mixed models and generalized estimating equations with an independence, simple exchangeable, or nested exchangeable working correlation structure. We study when a potentially misspecified working model can offer consistent estimation of the marginal treatment effect estimands, which are defined nonparametrically with potential outcomes and may be functions of calendar time and/or exposure time. We prove a central result that consistency for nonparametric estimands usually requires a correctly specified treatment effect structure, but generally not the remaining aspects of the working model (functional form of covariates, random effects, and error distribution), and valid inference is obtained via the sandwich variance estimator. Furthermore, an additional g-computation step is required to achieve model-robust inference under non-identity link functions or for ratio estimands. The theoretical results are illustrated via several simulation experiments and re-analysis of a completed stepped wedge cluster randomized trial.},
  archive      = {J_BIOMTC},
  author       = {Wang, Bingkai and Wang, Xueqi and Li, Fan},
  doi          = {10.1093/biomtc/ujae123},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae123},
  shortjournal = {Biometrics},
  title        = {How to achieve model-robust inference in stepped wedge trials with model-based methods?},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Derivation of outcome-dependent dietary patterns for
low-income women obtained from survey data using a supervised weighted
overfitted latent class analysis. <em>BIOMTC</em>, <em>80</em>(4),
ujae122. (<a href="https://doi.org/10.1093/biomtc/ujae122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Poor diet quality is a key modifiable risk factor for hypertension and disproportionately impacts low-income women. Analyzing diet-driven hypertensive outcomes in this demographic is challenging due to the complexity of dietary data and selection bias when the data come from surveys, a main data source for understanding diet-disease relationships in understudied populations. Supervised Bayesian model-based clustering methods summarize dietary data into latent patterns that holistically capture relationships among foods and a known health outcome but do not sufficiently account for complex survey design. This leads to biased estimation and inference and lack of generalizability of the patterns. To address this, we propose a supervised weighted overfitted latent class analysis (SWOLCA) based on a Bayesian pseudo-likelihood approach that integrates sampling weights into an exposure-outcome model for discrete data. Our model adjusts for stratification, clustering, and informative sampling, and handles modifying effects via interaction terms within a Markov chain Monte Carlo Gibbs sampling algorithm. Simulation studies confirm that the SWOLCA model exhibits good performance in terms of bias, precision, and coverage. Using data from the National Health and Nutrition Examination Survey (2015–2018), we demonstrate the utility of our model by characterizing dietary patterns associated with hypertensive outcomes among low-income women in the United States.},
  archive      = {J_BIOMTC},
  author       = {Wu, Stephanie M and Williams, Matthew R and Savitsky, Terrance D and Stephenson, Briana J K},
  doi          = {10.1093/biomtc/ujae122},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae122},
  shortjournal = {Biometrics},
  title        = {Derivation of outcome-dependent dietary patterns for low-income women obtained from survey data using a supervised weighted overfitted latent class analysis},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling longitudinal skewed functional data.
<em>BIOMTC</em>, <em>80</em>(4), ujae121. (<a
href="https://doi.org/10.1093/biomtc/ujae121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a model for longitudinal functional data analysis that accounts for pointwise skewness. The proposed procedure decouples the marginal pointwise variation from the complex longitudinal and functional dependence using copula methodology. Pointwise variation is described through parametric distribution functions that capture varying skewness and change smoothly both in time and over the functional argument. Joint dependence is quantified through a Gaussian copula with a low-rank approximation-based covariance. The introduced class of models provides a unifying platform for both pointwise quantile estimation and prediction of complete trajectories at new times. We investigate the methods numerically in simulations and discuss their application to a diffusion tensor imaging study of multiple sclerosis patients. This approach is implemented in the R package sLFDA that is publicly available on GitHub.},
  archive      = {J_BIOMTC},
  author       = {Alam, Mohammad Samsul and Staicu, Ana-Maria},
  doi          = {10.1093/biomtc/ujae121},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae121},
  shortjournal = {Biometrics},
  title        = {Modeling longitudinal skewed functional data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Likelihood adaptively incorporated external aggregate
information with uncertainty for survival data. <em>BIOMTC</em>,
<em>80</em>(4), ujae120. (<a
href="https://doi.org/10.1093/biomtc/ujae120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Population-based cancer registry databases are critical resources to bridge the information gap that results from a lack of sufficient statistical power from primary cohort data with small to moderate sample size. Although comprehensive data associated with tumor biomarkers often remain either unavailable or inconsistently measured in these registry databases, aggregate survival information sourced from these repositories has been well documented and publicly accessible. An appealing option is to integrate the aggregate survival information from the registry data with the primary cohort to enhance the evaluation of treatment impacts or prediction of survival outcomes across distinct tumor subtypes. Nevertheless, for rare types of cancer, even the sample sizes of cancer registries remain modest. The variability linked to the aggregated statistics could be non-negligible compared with the sample variation of the primary cohort. In response, we propose an externally informed likelihood approach, which facilitates the linkage between the primary cohort and external aggregate data, with consideration of the variation from aggregate information. We establish the asymptotic properties of the estimators and evaluate the finite sample performance via simulation studies. Through the application of our proposed method, we integrate data from the cohort of inflammatory breast cancer (IBC) patients at the University of Texas MD Anderson Cancer Center with aggregate survival data from the National Cancer Data Base, enabling us to appraise the effect of tri-modality treatment on survival across various tumor subtypes of IBC.},
  archive      = {J_BIOMTC},
  author       = {Chen, Ziqi and Shen, Yu and Qin, Jing and Ning, Jing},
  doi          = {10.1093/biomtc/ujae120},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae120},
  shortjournal = {Biometrics},
  title        = {Likelihood adaptively incorporated external aggregate information with uncertainty for survival data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A formal goodness-of-fit test for spatial binary markov
random field models. <em>BIOMTC</em>, <em>80</em>(4), ujae119. (<a
href="https://doi.org/10.1093/biomtc/ujae119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary spatial observations arise in environmental and ecological studies, where Markov random field (MRF) models are often applied. Despite the prevalence and the long history of MRF models for spatial binary data, appropriate model diagnostics have remained an unresolved issue in practice. A complicating factor is that such models involve neighborhood specifications, which are difficult to assess for binary data. To address this, we propose a formal goodness-of-fit (GOF) test for diagnosing an MRF model for spatial binary values. The test statistic involves a type of conditional Moran’s I based on the fitted conditional probabilities, which can detect departures in model form, including neighborhood structure. Numerical studies show that the GOF test can perform well in detecting deviations from a null model, with a focus on neighborhoods as a difficult issue. We illustrate the spatial test with an application to Besag’s historical endive data as well as the breeding pattern of grasshopper sparrows across Iowa.},
  archive      = {J_BIOMTC},
  author       = {Biswas, Eva and Kaplan, Andee and Kaiser, Mark S and Nordman, Daniel J},
  doi          = {10.1093/biomtc/ujae119},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae119},
  shortjournal = {Biometrics},
  title        = {A formal goodness-of-fit test for spatial binary markov random field models},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging information from secondary endpoints to enhance
dynamic borrowing across subpopulations. <em>BIOMTC</em>,
<em>80</em>(4), ujae118. (<a
href="https://doi.org/10.1093/biomtc/ujae118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized trials seek efficient treatment effect estimation within target populations, yet scientific interest often also centers on subpopulations. Although there are typically too few subjects within each subpopulation to efficiently estimate these subpopulation treatment effects, one can gain precision by borrowing strength across subpopulations, as is the case in a basket trial. While dynamic borrowing has been proposed as an efficient approach to estimating subpopulation treatment effects on primary endpoints, additional efficiency could be gained by leveraging the information found in secondary endpoints. We propose a multisource exchangeability model (MEM) that incorporates secondary endpoints to more efficiently assess subpopulation exchangeability. Across simulation studies, our proposed model almost uniformly reduces the mean squared error when compared to the standard MEM that only considers data from the primary endpoint by gaining efficiency when subpopulations respond similarly to the treatment and reducing the magnitude of bias when the subpopulations are heterogeneous. We illustrate our model’s feasibility using data from a recently completed trial of very low nicotine content cigarettes to estimate the effect on abstinence from smoking within three priority subpopulations. Our proposed model led to increases in the effective sample size two to four times greater than under the standard MEM.},
  archive      = {J_BIOMTC},
  author       = {Wolf, Jack M and Vock, David M and Luo, Xianghua and Hatsukami, Dorothy K and McClernon, F Joseph and Koopmeiners, Joseph S},
  doi          = {10.1093/biomtc/ujae118},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae118},
  shortjournal = {Biometrics},
  title        = {Leveraging information from secondary endpoints to enhance dynamic borrowing across subpopulations},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Case-crossover designs and overdispersion with application
to air pollution epidemiology. <em>BIOMTC</em>, <em>80</em>(4), ujae117.
(<a href="https://doi.org/10.1093/biomtc/ujae117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last three decades, case-crossover designs have found many applications in health sciences, especially in air pollution epidemiology. They are typically used, in combination with partial likelihood techniques, to define a conditional logistic model for the responses, usually health outcomes, conditional on the exposures. Despite the fact that conditional logistic models have been shown equivalent, in typical air pollution epidemiology setups, to specific instances of the well-known Poisson time series model, it is often claimed that they cannot allow for overdispersion. This paper clarifies the relationship between case-crossover designs, the models that ensue from their use, and overdispersion. In particular, we propose to relax the assumption of independence between individuals traditionally made in case-crossover analyses, in order to explicitly introduce overdispersion in the conditional logistic model. As we show, the resulting overdispersed conditional logistic model coincides with the overdispersed, conditional Poisson model, in the sense that their likelihoods are simple re-expressions of one another. We further provide the technical details of a Bayesian implementation of the proposed case-crossover model, which we use to demonstrate, by means of a large simulation study, that standard case-crossover models can lead to dramatically underestimated coverage probabilities, while the proposed models do not. We also perform an illustrative analysis of the association between air pollution and morbidity in Toronto, Canada, which shows that the proposed models are more robust than standard ones to outliers such as those associated with public holidays.},
  archive      = {J_BIOMTC},
  author       = {Perreault, Samuel and Dong, Gracia Y and Stringer, Alex and Shin, Hwashin and Brown, Patrick E},
  doi          = {10.1093/biomtc/ujae117},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae117},
  shortjournal = {Biometrics},
  title        = {Case-crossover designs and overdispersion with application to air pollution epidemiology},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian inference for group-level cortical surface
image-on-scalar regression with gaussian process priors.
<em>BIOMTC</em>, <em>80</em>(4), ujae116. (<a
href="https://doi.org/10.1093/biomtc/ujae116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In regression-based analyses of group-level neuroimage data, researchers typically fit a series of marginal general linear models to image outcomes at each spatially referenced pixel. Spatial regularization of effects of interest is usually induced indirectly by applying spatial smoothing to the data during preprocessing. While this procedure often works well, the resulting inference can be poorly calibrated. Spatial modeling of effects of interest leads to more powerful analyses; however, the number of locations in a typical neuroimage can preclude standard computing methods in this setting. Here, we contribute a Bayesian spatial regression model for group-level neuroimaging analyses. We induce regularization of spatially varying regression coefficient functions through Gaussian process priors. When combined with a simple non-stationary model for the error process, our prior hierarchy can lead to more data-adaptive smoothing than standard methods. We achieve computational tractability through a Vecchia-type approximation of our prior that retains full spatial rank and can be constructed for a wide class of spatial correlation functions. We outline several ways to work with our model in practice and compare performance against standard vertex-wise analyses and several alternatives. Finally, we illustrate our methods in an analysis of cortical surface functional magnetic resonance imaging task contrast data from a large cohort of children enrolled in the adolescent brain cognitive development study.},
  archive      = {J_BIOMTC},
  author       = {Whiteman, Andrew S and Johnson, Timothy D and Kang, Jian},
  doi          = {10.1093/biomtc/ujae116},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae116},
  shortjournal = {Biometrics},
  title        = {Bayesian inference for group-level cortical surface image-on-scalar regression with gaussian process priors},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal generative models for learning heterogeneous group
dynamics of ecological momentary assessment data. <em>BIOMTC</em>,
<em>80</em>(4), ujae115. (<a
href="https://doi.org/10.1093/biomtc/ujae115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the goals of precision psychiatry is to characterize mental disorders in an individualized manner, taking into account the underlying dynamic processes. Recent advances in mobile technologies have enabled the collection of ecological momentary assessments that capture multiple responses in real-time at high frequency. However, ecological momentary assessment data are often multi-dimensional, correlated, and hierarchical. Mixed-effect models are commonly used but may require restrictive assumptions about the fixed and random effects and the correlation structure. The recurrent temporal restricted Boltzmann machine (RTRBM) is a generative neural network that can be used to model temporal data, but most existing RTRBM approaches do not account for the potential heterogeneity of group dynamics within a population based on available covariates. In this paper, we propose a new temporal generative model, the HDRBM, to learn the heterogeneous group dynamics and demonstrate the effectiveness of this approach on simulated and real-world ecological momentary assessment datasets. We show that by incorporating covariates, HDRBM can improve accuracy and interpretability, explore the underlying drivers of the group dynamics of participants, and serve as a generative model for ecological momentary assessment studies.},
  archive      = {J_BIOMTC},
  author       = {Kim, Soohyun and Kim, Young-geun and Wang, Yuanjia},
  doi          = {10.1093/biomtc/ujae115},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae115},
  shortjournal = {Biometrics},
  title        = {Temporal generative models for learning heterogeneous group dynamics of ecological momentary assessment data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Changepoint detection on daily home activity pattern: A
sliced poisson process method. <em>BIOMTC</em>, <em>80</em>(4), ujae114.
(<a href="https://doi.org/10.1093/biomtc/ujae114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of health and care of people is being revolutionized. An important component of that revolution is disease prevention and health improvement from home. A natural approach to the health problem is monitoring changes in people’s behavior or activities. These changes can be indicators of potential health problems. However, due to a person’s daily pattern, changes will be observed throughout each day, with, eg, an increase of events around meal times and fewer events during the night. We do not wish to detect such within-day changes but rather changes in the daily behavior pattern from one day to the next. To this end, we assume the set of event times within a given day as a single observation. We model this observation as the realization of an inhomogeneous Poisson process where the rate function can vary with the time of day. Then, we propose to detect changes in the sequence of inhomogeneous Poisson processes. This approach is appropriate for many phenomena, particularly for home activity data. Our methodology is evaluated on simulated data. Overall, our approach uses local change information to detect changes across days. At the same time, it allows us to visualize and interpret the results, changes, and trends over time, allowing the detection of potential health decline.},
  archive      = {J_BIOMTC},
  author       = {Martínez-Hernández, Israel and Killick, Rebecca},
  doi          = {10.1093/biomtc/ujae114},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae114},
  shortjournal = {Biometrics},
  title        = {Changepoint detection on daily home activity pattern: A sliced poisson process method},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Functional generalized canonical correlation analysis for
studying multiple longitudinal variables. <em>BIOMTC</em>,
<em>80</em>(4), ujae113. (<a
href="https://doi.org/10.1093/biomtc/ujae113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce functional generalized canonical correlation analysis, a new framework for exploring associations between multiple random processes observed jointly. The framework is based on the multiblock regularized generalized canonical correlation analysis framework. It is robust to sparsely and irregularly observed data, making it applicable in many settings. We establish the monotonic property of the solving procedure and introduce a Bayesian approach for estimating canonical components. We propose an extension of the framework that allows the integration of a univariate or multivariate response into the analysis, paving the way for predictive applications. We evaluate the method’s efficiency in simulation studies and present a use case on a longitudinal dataset.},
  archive      = {J_BIOMTC},
  author       = {Sort, Lucas and Le Brusquet, Laurent and Tenenhaus, Arthur},
  doi          = {10.1093/biomtc/ujae113},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae113},
  shortjournal = {Biometrics},
  title        = {Functional generalized canonical correlation analysis for studying multiple longitudinal variables},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On network deconvolution for undirected graphs.
<em>BIOMTC</em>, <em>80</em>(4), ujae112. (<a
href="https://doi.org/10.1093/biomtc/ujae112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network deconvolution (ND) is a method to reconstruct a direct-effect network describing direct (or conditional) effects (or associations) between any two nodes from a given network depicting total (or marginal) effects (or associations). Its key idea is that, in a directed graph, a total effect can be decomposed into the sum of a direct and an indirect effects, with the latter further decomposed as the sum of various products of direct effects. This yields a simple closed-form solution for the direct-effect network, facilitating its important applications to distinguish direct and indirect effects. Despite its application to undirected graphs, it is not well known why the method works, leaving it with skepticism. We first clarify the implicit linear model assumption underlying ND, then derive a surprisingly simple result on the equivalence between ND and use of precision matrices, offering insightful justification and interpretation for the application of ND to undirected graphs. We also establish a formal result to characterize the effect of scaling a total-effect graph. Finally, leveraging large-scale genome-wide association study data, we show a novel application of ND to contrast marginal versus conditional genetic correlations between body height and risk of coronary artery disease; the results align with an inferred causal directed graph using ND. We conclude that ND is a promising approach with its easy and wide applicability to both directed and undirected graphs.},
  archive      = {J_BIOMTC},
  author       = {Lin, Zhaotong and Pan, Isaac and Pan, Wei},
  doi          = {10.1093/biomtc/ujae112},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae112},
  shortjournal = {Biometrics},
  title        = {On network deconvolution for undirected graphs},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian network-guided sparse regression with flexible
varying effects. <em>BIOMTC</em>, <em>80</em>(4), ujae111. (<a
href="https://doi.org/10.1093/biomtc/ujae111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose Varying Effects Regression with Graph Estimation (VERGE), a novel Bayesian method for feature selection in regression. Our model has key aspects that allow it to leverage the complex structure of data sets arising from genomics or imaging studies. We distinguish between the predictors , which are the features utilized in the outcome prediction model, and the subject-level covariates , which modulate the effects of the predictors on the outcome. We construct a varying coefficients modeling framework where we infer a network among the predictor variables and utilize this network information to encourage the selection of related predictors. We employ variable selection spike-and-slab priors that enable the selection of both network-linked predictor variables and covariates that modify the predictor effects. We demonstrate through simulation studies that our method outperforms existing alternative methods in terms of both feature selection and predictive accuracy. We illustrate VERGE with an application to characterizing the influence of gut microbiome features on obesity, where we identify a set of microbial taxa and their ecological dependence relations. We allow subject-level covariates, including sex and dietary intake variables to modify the coefficients of the microbiome predictors, providing additional insight into the interplay between these factors.},
  archive      = {J_BIOMTC},
  author       = {Ren, Yangfan and Peterson, Christine B and Vannucci, Marina},
  doi          = {10.1093/biomtc/ujae111},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae111},
  shortjournal = {Biometrics},
  title        = {Bayesian network-guided sparse regression with flexible varying effects},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal effect estimation in survival analysis with high
dimensional confounders. <em>BIOMTC</em>, <em>80</em>(4), ujae110. (<a
href="https://doi.org/10.1093/biomtc/ujae110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the ever advancing of modern technologies, it has become increasingly common that the number of collected confounders exceeds the number of subjects in a data set. However, matching based methods for estimating causal treatment effect in their original forms are not capable of handling high-dimensional confounders, and their various modified versions lack statistical support and valid inference tools. In this article, we propose a new approach for estimating causal treatment effect, defined as the difference of the restricted mean survival time (RMST) under different treatments in high-dimensional setting for survival data. We combine the factor model and the sufficient dimension reduction techniques to construct propensity score and prognostic score. Based on these scores, we develop a kernel based doubly robust estimator of the RMST difference. We demonstrate its link to matching and establish the consistency and asymptotic normality of the estimator. We illustrate our method by analyzing a dataset from a study aimed at comparing the effects of two alternative treatments on the RMST of patients with diffuse large B cell lymphoma.},
  archive      = {J_BIOMTC},
  author       = {Jiang, Fei and Zhao, Ge and Rodriguez-Monguio, Rosa and Ma, Yanyuan},
  doi          = {10.1093/biomtc/ujae110},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae110},
  shortjournal = {Biometrics},
  title        = {Causal effect estimation in survival analysis with high dimensional confounders},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneity-aware integrative regression for
ancestry-specific association studies. <em>BIOMTC</em>, <em>80</em>(4),
ujae109. (<a href="https://doi.org/10.1093/biomtc/ujae109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ancestry-specific proteome-wide association studies (PWAS) based on genetically predicted protein expression can reveal complex disease etiology specific to certain ancestral groups. These studies require ancestry-specific models for protein expression as a function of SNP genotypes. In order to improve protein expression prediction in ancestral populations historically underrepresented in genomic studies, we propose a new penalized maximum likelihood estimator for fitting ancestry-specific joint protein quantitative trait loci models. Our estimator borrows information across ancestral groups, while simultaneously allowing for heterogeneous error variances and regression coefficients. We propose an alternative parameterization of our model that makes the objective function convex and the penalty scale invariant. To improve computational efficiency, we propose an approximate version of our method and study its theoretical properties. Our method provides a substantial improvement in protein expression prediction accuracy in individuals of African ancestry, and in a downstream PWAS analysis, leads to the discovery of multiple associations between protein expression and blood lipid traits in the African ancestry population.},
  archive      = {J_BIOMTC},
  author       = {Molstad, Aaron J and Cai, Yanwei and Reiner, Alexander P and Kooperberg, Charles and Sun, Wei and Hsu, Li},
  doi          = {10.1093/biomtc/ujae109},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae109},
  shortjournal = {Biometrics},
  title        = {Heterogeneity-aware integrative regression for ancestry-specific association studies},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Group sequential testing of a treatment effect using a
surrogate marker. <em>BIOMTC</em>, <em>80</em>(4), ujae108. (<a
href="https://doi.org/10.1093/biomtc/ujae108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification of surrogate markers is motivated by their potential to make decisions sooner about a treatment effect. However, few methods have been developed to actually use a surrogate marker to test for a treatment effect in a future study. Most existing methods consider combining surrogate marker and primary outcome information to test for a treatment effect, rely on fully parametric methods where strict parametric assumptions are made about the relationship between the surrogate and the outcome, and/or assume the surrogate marker is measured at only a single time point. Recent work has proposed a nonparametric test for a treatment effect using only surrogate marker information measured at a single time point by borrowing information learned from a prior study where both the surrogate and primary outcome were measured. In this paper, we utilize this nonparametric test and propose group sequential procedures that allow for early stopping of treatment effect testing in a setting where the surrogate marker is measured repeatedly over time. We derive the properties of the correlated surrogate-based nonparametric test statistics at multiple time points and compute stopping boundaries that allow for early stopping for a significant treatment effect, or for futility. We examine the performance of our proposed test using a simulation study and illustrate the method using data from two distinct AIDS clinical trials.},
  archive      = {J_BIOMTC},
  author       = {Parast, Layla and Bartroff, Jay},
  doi          = {10.1093/biomtc/ujae108},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae108},
  shortjournal = {Biometrics},
  title        = {Group sequential testing of a treatment effect using a surrogate marker},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Composite dyadic models for spatio-temporal data.
<em>BIOMTC</em>, <em>80</em>(4), ujae107. (<a
href="https://doi.org/10.1093/biomtc/ujae107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mechanistic statistical models are commonly used to study the flow of biological processes. For example, in landscape genetics, the aim is to infer spatial mechanisms that govern gene flow in populations. Existing statistical approaches in landscape genetics do not account for temporal dependence in the data and may be computationally prohibitive. We infer mechanisms with a Bayesian hierarchical dyadic model that scales well with large data sets and that accounts for spatial and temporal dependence. We construct a fully connected network comprising spatio-temporal data for the dyadic model and use normalized composite likelihoods to account for the dependence structure in space and time. We develop a dyadic model to account for physical mechanisms commonly found in physical-statistical models and apply our methods to ancient human DNA data to infer the mechanisms that affected human movement in Bronze Age Europe.},
  archive      = {J_BIOMTC},
  author       = {Schwob, Michael R and Hooten, Mevin B and Narasimhan, Vagheesh},
  doi          = {10.1093/biomtc/ujae107},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae107},
  shortjournal = {Biometrics},
  title        = {Composite dyadic models for spatio-temporal data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semiparametric sensitivity analysis: Unmeasured confounding
in observational studies. <em>BIOMTC</em>, <em>80</em>(4), ujae106. (<a
href="https://doi.org/10.1093/biomtc/ujae106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Establishing cause–effect relationships from observational data often relies on untestable assumptions. It is crucial to know whether, and to what extent, the conclusions drawn from non-experimental studies are robust to potential unmeasured confounding. In this paper, we focus on the average causal effect (ACE) as our target of inference. We generalize the sensitivity analysis approach developed by Robins et al., Franks et al., and Zhou and Yao. We use semiparametric theory to derive the non-parametric efficient influence function of the ACE, for fixed sensitivity parameters. We use this influence function to construct a one-step, split sample, truncated estimator of the ACE. Our estimator depends on semiparametric models for the distribution of the observed data; importantly, these models do not impose any restrictions on the values of sensitivity analysis parameters. We establish sufficient conditions ensuring that our estimator has |$\sqrt{n}$| asymptotics. We use our methodology to evaluate the causal effect of smoking during pregnancy on birth weight. We also evaluate the performance of estimation procedure in a simulation study.},
  archive      = {J_BIOMTC},
  author       = {Nabi, Razieh and Bonvini, Matteo and Kennedy, Edward H and Huang, Ming-Yueh and Smid, Marcela and Scharfstein, Daniel O},
  doi          = {10.1093/biomtc/ujae106},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae106},
  shortjournal = {Biometrics},
  title        = {Semiparametric sensitivity analysis: Unmeasured confounding in observational studies},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ROMI: A randomized two-stage basket trial design to optimize
doses for multiple indications. <em>BIOMTC</em>, <em>80</em>(4),
ujae105. (<a href="https://doi.org/10.1093/biomtc/ujae105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing doses for multiple indications is challenging. The pooled approach of finding a single optimal biological dose (OBD) for all indications ignores that dose-response or dose-toxicity curves may differ between indications, resulting in varying OBDs. Conversely, indication-specific dose optimization often requires a large sample size. To address this challenge, we propose a Randomized two-stage basket trial design that Optimizes doses in Multiple Indications (ROMI). In stage 1, for each indication, response and toxicity are evaluated for a high dose, which may be a previously obtained maximum tolerated dose, with a rule that stops accrual to indications where the high dose is unsafe or ineffective. Indications not terminated proceed to stage 2, where patients are randomized between the high dose and a specified lower dose. A latent-cluster Bayesian hierarchical model is employed to borrow information between indications, while considering the potential heterogeneity of OBD across indications. Indication-specific utilities are used to quantify response-toxicity trade-offs. At the end of stage 2, for each indication with at least one acceptable dose, the dose with highest posterior mean utility is selected as optimal. Two versions of ROMI are presented, one using only stage 2 data for dose optimization and the other optimizing doses using data from both stages. Simulations show that both versions have desirable operating characteristics compared to designs that either ignore indications or optimize dose independently for each indication.},
  archive      = {J_BIOMTC},
  author       = {Wang, Shuqi and Thall, Peter F and Takeda, Kentaro and Yuan, Ying},
  doi          = {10.1093/biomtc/ujae105},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae105},
  shortjournal = {Biometrics},
  title        = {ROMI: A randomized two-stage basket trial design to optimize doses for multiple indications},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A causal inference framework for leveraging external
controls in hybrid trials. <em>BIOMTC</em>, <em>80</em>(4), ujae095. (<a
href="https://doi.org/10.1093/biomtc/ujae095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the challenges associated with causal inference in settings where data from a randomized trial are augmented with control data from an external source to improve efficiency in estimating the average treatment effect (ATE). This question is motivated by the SUNFISH trial, which investigated the effect of risdiplam on motor function in patients with spinal muscular atrophy. While the original analysis used only data generated by the trial, we explore an alternative analysis incorporating external controls from the placebo arm of a historical trial. We cast the setting into a formal causal inference framework and show how these designs are characterized by a lack of full randomization to treatment and heightened dependency on modeling. To address this, we outline sufficient causal assumptions about the exchangeability between the internal and external controls to identify the ATE and establish a connection with novel graphical criteria. Furthermore, we propose estimators, review efficiency bounds, develop an approach for efficient doubly robust estimation even when unknown nuisance models are estimated with flexible machine learning methods, suggest model diagnostics, and demonstrate finite-sample performance of the methods through a simulation study. The ideas and methods are illustrated through their application to the SUNFISH trial, where we find that external controls can increase the efficiency of treatment effect estimation.},
  archive      = {J_BIOMTC},
  author       = {Valancius, Michael and Pang, Herbert and Zhu, Jiawen and Cole, Stephen R and Jonsson Funk, Michele and Kosorok, Michael R},
  doi          = {10.1093/biomtc/ujae095},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujae095},
  shortjournal = {Biometrics},
  title        = {A causal inference framework for leveraging external controls in hybrid trials},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Planning cost-effective operational forest inventories.
<em>BIOMTC</em>, <em>80</em>(3), ujae104. (<a
href="https://doi.org/10.1093/biomtc/ujae104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address a Bayesian two-stage decision problem in operational forestry where the inner stage considers scheduling the harvesting to fulfill demand targets and the outer stage considers selecting the accuracy of pre-harvest inventories that are used to estimate the timber volumes of the forest tracts. The higher accuracy of the inventory enables better scheduling decisions but also implies higher costs. We focus on the outer stage, which we formulate as a maximization of the posterior value of the inventory decision under a budget constraint. The posterior value depends on the solution to the inner stage problem and its computation is analytically intractable, featuring an NP-hard binary optimization problem within a high-dimensional integral. In particular, the binary optimization problem is a special case of a generalized quadratic assignment problem. We present a practical method that solves the outer stage problem with an approximation which combines Monte Carlo sampling with a greedy, randomized method for the binary optimization problem. We derive inventory decisions for a dataset of 100 Swedish forest tracts across a range of inventory budgets and estimate the value of the information to be obtained.},
  archive      = {J_BIOMTC},
  author       = {Karppinen, Santeri and Ene, Liviu and Engberg Sundström, Lovisa and Karvanen, Juha},
  doi          = {10.1093/biomtc/ujae104},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujae104},
  shortjournal = {Biometrics},
  title        = {Planning cost-effective operational forest inventories},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging independence in high-dimensional mixed linear
regression. <em>BIOMTC</em>, <em>80</em>(3), ujae103. (<a
href="https://doi.org/10.1093/biomtc/ujae103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the challenge of estimating regression coefficients and selecting relevant predictors in the context of mixed linear regression in high dimensions, where the number of predictors greatly exceeds the sample size. Recent advancements in this field have centered on incorporating sparsity-inducing penalties into the expectation-maximization (EM) algorithm, which seeks to maximize the conditional likelihood of the response given the predictors. However, existing procedures often treat predictors as fixed or overlook their inherent variability. In this paper, we leverage the independence between the predictor and the latent indicator variable of mixtures to facilitate efficient computation and also achieve synergistic variable selection across all mixture components. We establish the non-asymptotic convergence rate of the proposed fast group-penalized EM estimator to the true regression parameters. The effectiveness of our method is demonstrated through extensive simulations and an application to the Cancer Cell Line Encyclopedia dataset for the prediction of anticancer drug sensitivity.},
  archive      = {J_BIOMTC},
  author       = {Wang, Ning and Deng, Kai and Mai, Qing and Zhang, Xin},
  doi          = {10.1093/biomtc/ujae103},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujae103},
  shortjournal = {Biometrics},
  title        = {Leveraging independence in high-dimensional mixed linear regression},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Handbook of matching and weighting adjustments for causal
inference by josé r. Zubizarreta, elizabeth a. Stuart, dylan s. Small,
and paul r. Rosenbaum, chapman and hall/CRC, 2023, ISBN: 9781003102670,
https://www.routledge.com/handbook-of-matching-and-weighting-adjustments-for-causal-inference/zubizarreta-stuart-small-rosenbaum/p/book/9781003102670.
<em>BIOMTC</em>, <em>80</em>(3), ujae102. (<a
href="https://doi.org/10.1093/biomtc/ujae102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Lee, Youjin},
  doi          = {10.1093/biomtc/ujae102},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujae102},
  shortjournal = {Biometrics},
  title        = {Handbook of matching and weighting adjustments for causal inference by josé r. zubizarreta, elizabeth a. stuart, dylan s. small, and paul r. rosenbaum, chapman and Hall/CRC, 2023, ISBN: 9781003102670, https://www.routledge.com/Handbook-of-matching-and-weighting-adjustments-for-causal-inference/Zubizarreta-stuart-small-rosenbaum/p/book/9781003102670},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Post-selection inference in regression models for group
testing data. <em>BIOMTC</em>, <em>80</em>(3), ujae101. (<a
href="https://doi.org/10.1093/biomtc/ujae101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a methodology for valid inference after variable selection in logistic regression when the responses are partially observed, that is, when one observes a set of error-prone testing outcomes instead of the true values of the responses. Aiming at selecting important covariates while accounting for missing information in the response data, we apply the expectation-maximization algorithm to compute maximum likelihood estimators subject to LASSO penalization. Subsequent to variable selection, we make inferences on the selected covariate effects by extending post-selection inference methodology based on the polyhedral lemma. Empirical evidence from our extensive simulation study suggests that our post-selection inference results are more reliable than those from naive inference methods that use the same data to perform variable selection and inference without adjusting for variable selection.},
  archive      = {J_BIOMTC},
  author       = {Shen, Qinyan and Gregory, Karl and Huang, Xianzheng},
  doi          = {10.1093/biomtc/ujae101},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujae101},
  shortjournal = {Biometrics},
  title        = {Post-selection inference in regression models for group testing data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal inference using multivariate generalized linear
mixed-effects models. <em>BIOMTC</em>, <em>80</em>(3), ujae100. (<a
href="https://doi.org/10.1093/biomtc/ujae100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic prediction of causal effects under different treatment regimens is an essential problem in precision medicine. It is challenging because the actual mechanisms of treatment assignment and effects are unknown in observational studies. We propose a multivariate generalized linear mixed-effects model and a Bayesian g-computation algorithm to calculate the posterior distribution of subgroup-specific intervention benefits of dynamic treatment regimes. Unmeasured time-invariant factors are included as subject-specific random effects in the assumed joint distribution of outcomes, time-varying confounders, and treatment assignments. We identify a sequential ignorability assumption conditional on treatment assignment heterogeneity, that is, analogous to balancing the latent treatment preference due to unmeasured time-invariant factors. We present a simulation study to assess the proposed method’s performance. The method is applied to observational clinical data to investigate the efficacy of continuously using mycophenolate in different subgroups of scleroderma patients.},
  archive      = {J_BIOMTC},
  author       = {Xu, Yizhen and Kim, Ji Soo and Hummers, Laura K and Shah, Ami A and Zeger, Scott L},
  doi          = {10.1093/biomtc/ujae100},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujae100},
  shortjournal = {Biometrics},
  title        = {Causal inference using multivariate generalized linear mixed-effects models},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian nonparametric approach for causal mediation with
a post-treatment confounder. <em>BIOMTC</em>, <em>80</em>(3), ujae099.
(<a href="https://doi.org/10.1093/biomtc/ujae099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new Bayesian nonparametric method for estimating the causal effects of mediation in the presence of a post-treatment confounder. The methodology is motivated by the Rural Lifestyle Intervention Treatment Effectiveness Trial (Rural LITE) for which there is interest in estimating causal mediation effects but is complicated by the presence of a post-treatment confounder. We specify an enriched Dirichlet process mixture (EDPM) to model the joint distribution of the observed data (outcome, mediator, post-treatment confounder, treatment, and baseline confounders). For identifiability, we use the extended version of the standard sequential ignorability (SI) as introduced in Hong et al. along with a Gaussian copula model assumption. The observed data model and causal identification assumptions enable us to estimate and identify the causal effects of mediation, that is, the natural direct effects (NDE) and natural indirect effects (NIE). Our method enables easy computation of NIE and NDE for a subset of confounding variables and addresses missing data through data augmentation under the assumption of ignorable missingness. We conduct simulation studies to assess the performance of our proposed method. Furthermore, we apply this approach to evaluate the causal mediation effect in the Rural LITE trial, finding that there was not strong evidence for the potential mediator.},
  archive      = {J_BIOMTC},
  author       = {Bae, Woojung and Daniels, Michael J and Perri, Michael G},
  doi          = {10.1093/biomtc/ujae099},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujae099},
  shortjournal = {Biometrics},
  title        = {A bayesian nonparametric approach for causal mediation with a post-treatment confounder},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-parametric benchmark dose analysis with monotone
additive models. <em>BIOMTC</em>, <em>80</em>(3), ujae098. (<a
href="https://doi.org/10.1093/biomtc/ujae098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benchmark dose analysis aims to estimate the level of exposure to a toxin associated with a clinically significant adverse outcome and quantifies uncertainty using the lower limit of a confidence interval for this level. We develop a novel framework for benchmark dose analysis based on monotone additive dose-response models. We first introduce a flexible approach for fitting monotone additive models via penalized B-splines and Laplace-approximate marginal likelihood. A reflective Newton method is then developed that employs de Boor’s algorithm for computing splines and their derivatives for efficient estimation of the benchmark dose. Finally, we develop a novel approach for calculating benchmark dose lower limits based on an approximate pivot for the nonlinear equation solved by the estimated benchmark dose. The favorable properties of this approach compared to the Delta method and a parameteric bootstrap are discussed. We apply the new methods to make inferences about the level of prenatal alcohol exposure associated with clinically significant cognitive defects in children using data from six NIH-funded longitudinal cohort studies. Software to reproduce the results in this paper is available online and makes use of the novel semibmd R package, which implements the methods in this paper.},
  archive      = {J_BIOMTC},
  author       = {Stringer, Alex and Akkaya Hocagil, Tugba and Cook, Richard J and Ryan, Louise M and Jacobson, Sandra W and Jacobson, Joseph L},
  doi          = {10.1093/biomtc/ujae098},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujae098},
  shortjournal = {Biometrics},
  title        = {Semi-parametric benchmark dose analysis with monotone additive models},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Designing cancer screening trials for reduction in
late-stage cancer incidence. <em>BIOMTC</em>, <em>80</em>(3), ujae097.
(<a href="https://doi.org/10.1093/biomtc/ujae097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Before implementing a biomarker test for early cancer detection into routine clinical care, the test must demonstrate clinical utility, that is, the test results should lead to clinical actions that positively affect patient-relevant outcomes. Unlike therapeutical trials for patients diagnosed with cancer, designing a randomized controlled trial (RCT) to demonstrate the clinical utility of an early detection biomarker with mortality and related endpoints poses unique challenges. The hurdles stem from the prolonged natural progression of the disease and the lack of information regarding the time-varying screening effect on the target asymptomatic population. To facilitate the study design of screening trials, we propose using a generic multistate disease history model and derive model-based effect sizes. The model links key performance metrics of the test, such as sensitivity, to primary endpoints like the incidence of late-stage cancer. It also incorporates the practical implementation of the biomarker-testing program in real-world scenarios. Based on the chronological time scale aligned with RCT, our method allows the assessment of study powers based on key features of the new program, including the test sensitivity, the length of follow-up, and the number and frequency of repeated tests. The calculation tool from the proposed method will enable practitioners to perform realistic and quick evaluations when strategizing screening trials for specific diseases. We use numerical examples based on the National Lung Screening Trial to demonstrate the method.},
  archive      = {J_BIOMTC},
  author       = {Zhu, Kehao and Zhao, Ying-Qi and Zheng, Yingye},
  doi          = {10.1093/biomtc/ujae097},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujae097},
  shortjournal = {Biometrics},
  title        = {Designing cancer screening trials for reduction in late-stage cancer incidence},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous latent transfer learning in gaussian graphical
models. <em>BIOMTC</em>, <em>80</em>(3), ujae096. (<a
href="https://doi.org/10.1093/biomtc/ujae096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian graphical models (GGMs) are useful for understanding the complex relationships between biological entities. Transfer learning can improve the estimation of GGMs in a target dataset by incorporating relevant information from related source studies. However, biomedical research often involves intrinsic and latent heterogeneity within a study, such as heterogeneous subpopulations. This heterogeneity can make it difficult to identify informative source studies or lead to negative transfer if the source study is improperly used. To address this challenge, we developed a heterogeneous latent transfer learning (Latent-TL) approach that accounts for both within-sample and between-sample heterogeneity. The idea behind this approach is to “learn from the alike” by leveraging the similarities between source and target GGMs within each subpopulation. The Latent-TL algorithm simultaneously identifies common subpopulation structures among samples and facilitates the learning of target GGMs using source samples from the same subpopulation. Through extensive simulations and real data application, we have shown that the proposed method outperforms single-site learning and standard transfer learning that ignores the latent structures. We have also demonstrated the applicability of the proposed algorithm in characterizing gene co-expression networks in breast cancer patients, where the inferred genetic networks identified many biologically meaningful gene–gene interactions.},
  archive      = {J_BIOMTC},
  author       = {Wu, Qiong and Wang, Chi and Chen, Yong},
  doi          = {10.1093/biomtc/ujae096},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujae096},
  shortjournal = {Biometrics},
  title        = {Heterogeneous latent transfer learning in gaussian graphical models},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adjusting for incomplete baseline covariates in randomized
controlled trials: A cross-world imputation framework. <em>BIOMTC</em>,
<em>80</em>(3), ujae094. (<a
href="https://doi.org/10.1093/biomtc/ujae094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In randomized controlled trials, adjusting for baseline covariates is commonly used to improve the precision of treatment effect estimation. However, covariates often have missing values. Recently, Zhao and Ding studied two simple strategies, the single imputation method and missingness-indicator method (MIM), to handle missing covariates and showed that both methods can provide an efficiency gain compared to not adjusting for covariates. To better understand and compare these two strategies, we propose and investigate a novel theoretical imputation framework termed cross-world imputation (CWI). This framework includes both single imputation and MIM as special cases, facilitating the comparison of their efficiency. Through the lens of CWI, we show that MIM implicitly searches for the optimal CWI values and thus achieves optimal efficiency. We also derive conditions under which the single imputation method, by searching for the optimal single imputation values, can achieve the same efficiency as the MIM. We illustrate our findings through simulation studies and a real data analysis based on the Childhood Adenotonsillectomy Trial. We conclude by discussing the practical implications of our findings.},
  archive      = {J_BIOMTC},
  author       = {Song, Yilin and Hughes, James P and Ye, Ting},
  doi          = {10.1093/biomtc/ujae094},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujae094},
  shortjournal = {Biometrics},
  title        = {Adjusting for incomplete baseline covariates in randomized controlled trials: A cross-world imputation framework},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian latent-subgroup platform design for dose
optimization. <em>BIOMTC</em>, <em>80</em>(3), ujae093. (<a
href="https://doi.org/10.1093/biomtc/ujae093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The US Food and Drug Administration launched Project Optimus to reform the dose optimization and dose selection paradigm in oncology drug development, calling for the paradigm shift from finding the maximum tolerated dose to the identification of optimal biological dose (OBD). Motivated by a real-world drug development program, we propose a master-protocol-based platform trial design to simultaneously identify OBDs of a new drug, combined with standards of care or other novel agents, in multiple indications. We propose a Bayesian latent subgroup model to accommodate the treatment heterogeneity across indications, and employ Bayesian hierarchical models to borrow information within subgroups. At each interim analysis, we update the subgroup membership and dose–toxicity and –efficacy estimates, as well as the estimate of the utility for risk-benefit tradeoff, based on the observed data across treatment arms to inform the arm-specific decision of dose escalation and de-escalation and identify the OBD for each arm of a combination partner and an indication. The simulation study shows that the proposed design has desirable operating characteristics, providing a highly flexible and efficient way for dose optimization. The design has great potential to shorten the drug development timeline, save costs by reducing overlapping infrastructure, and speed up regulatory approval.},
  archive      = {J_BIOMTC},
  author       = {Mu, Rongji and Zhan, Xiaojiang and Tang, Rui (Sammi) and Yuan, Ying},
  doi          = {10.1093/biomtc/ujae093},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujae093},
  shortjournal = {Biometrics},
  title        = {A bayesian latent-subgroup platform design for dose optimization},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sensitivity analysis for publication bias in meta-analysis
of sparse data based on exact likelihood. <em>BIOMTC</em>,
<em>80</em>(3), ujae092. (<a
href="https://doi.org/10.1093/biomtc/ujae092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-analysis is a powerful tool to synthesize findings from multiple studies. The normal–normal random-effects model is widely used to account for between-study heterogeneity. However, meta-analyses of sparse data, which may arise when the event rate is low for binary or count outcomes, pose a challenge to the normal–normal random-effects model in the accuracy and stability in inference since the normal approximation in the within-study model may not be good. To reduce bias arising from data sparsity, the generalized linear mixed model can be used by replacing the approximate normal within-study model with an exact model. Publication bias is one of the most serious threats in meta-analysis. Several quantitative sensitivity analysis methods for evaluating the potential impacts of selective publication are available for the normal–normal random-effects model. We propose a sensitivity analysis method by extending the likelihood-based sensitivity analysis with the |$t$| -statistic selection function of Copas to several generalized linear mixed-effects models. Through applications of our proposed method to several real-world meta-analyses and simulation studies, the proposed method was proven to outperform the likelihood-based sensitivity analysis based on the normal–normal model. The proposed method would give useful guidance to address publication bias in the meta-analysis of sparse data.},
  archive      = {J_BIOMTC},
  author       = {Hu, Taojun and Zhou, Yi and Hattori, Satoshi},
  doi          = {10.1093/biomtc/ujae092},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujae092},
  shortjournal = {Biometrics},
  title        = {Sensitivity analysis for publication bias in meta-analysis of sparse data based on exact likelihood},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unit information dirichlet process prior. <em>BIOMTC</em>,
<em>80</em>(3), ujae091. (<a
href="https://doi.org/10.1093/biomtc/ujae091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior distributions, which represent one’s belief in the distributions of unknown parameters before observing the data, impact Bayesian inference in a critical and fundamental way. With the ability to incorporate external information from expert opinions or historical datasets, the priors, if specified appropriately, can improve the statistical efficiency of Bayesian inference. In survival analysis, based on the concept of unit information (UI) under parametric models, we propose the unit information Dirichlet process (UIDP) as a new class of nonparametric priors for the underlying distribution of time-to-event data. By deriving the Fisher information in terms of the differential of the cumulative hazard function, the UIDP prior is formulated to match its prior UI with the weighted average of UI in historical datasets and thus can utilize both parametric and nonparametric information provided by historical datasets. With a Markov chain Monte Carlo algorithm, simulations and real data analysis demonstrate that the UIDP prior can adaptively borrow historical information and improve statistical efficiency in survival analysis.},
  archive      = {J_BIOMTC},
  author       = {Gu, Jiaqi and Yin, Guosheng},
  doi          = {10.1093/biomtc/ujae091},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujae091},
  shortjournal = {Biometrics},
  title        = {Unit information dirichlet process prior},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrating external summary information in the presence of
prior probability shift: An application to assessing essential
hypertension. <em>BIOMTC</em>, <em>80</em>(3), ujae090. (<a
href="https://doi.org/10.1093/biomtc/ujae090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed a rise in the popularity of information integration without sharing of raw data. By leveraging and incorporating summary information from external sources, internal studies can achieve enhanced estimation efficiency and prediction accuracy. However, a noteworthy challenge in utilizing summary-level information is accommodating the inherent heterogeneity across diverse data sources. In this study, we delve into the issue of prior probability shift between two cohorts, wherein the difference of two data distributions depends on the outcome. We introduce a novel semi-parametric constrained optimization-based approach to integrate information within this framework, which has not been extensively explored in existing literature. Our proposed method tackles the prior probability shift by introducing the outcome-dependent selection function and effectively addresses the estimation uncertainty associated with summary information from the external source. Our approach facilitates valid inference even in the absence of a known variance-covariance estimate from the external source. Through extensive simulation studies, we observe the superiority of our method over existing ones, showcasing minimal estimation bias and reduced variance for both binary and continuous outcomes. We further demonstrate the utility of our method through its application in investigating risk factors related to essential hypertension, where the reduced estimation variability is observed after integrating summary information from an external data.},
  archive      = {J_BIOMTC},
  author       = {Chen, Chixiang and Han, Peisong and Chen, Shuo and Shardell, Michelle and Qin, Jing},
  doi          = {10.1093/biomtc/ujae090},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujae090},
  shortjournal = {Biometrics},
  title        = {Integrating external summary information in the presence of prior probability shift: An application to assessing essential hypertension},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visibility graph-based covariance functions for scalable
spatial analysis in non-convex partially euclidean domains.
<em>BIOMTC</em>, <em>80</em>(3), ujae089. (<a
href="https://doi.org/10.1093/biomtc/ujae089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new method for constructing valid covariance functions of Gaussian processes for spatial analysis in irregular, non-convex domains such as bodies of water. Standard covariance functions based on geodesic distances are not guaranteed to be positive definite on such domains, while existing non-Euclidean approaches fail to respect the partially Euclidean nature of these domains where the geodesic distance agrees with the Euclidean distances for some pairs of points. Using a visibility graph on the domain, we propose a class of covariance functions that preserve Euclidean-based covariances between points that are connected in the domain while incorporating the non-convex geometry of the domain via conditional independence relationships. We show that the proposed method preserves the partially Euclidean nature of the intrinsic geometry on the domain while maintaining validity (positive definiteness) and marginal stationarity of the covariance function over the entire parameter space, properties which are not always fulfilled by existing approaches to construct covariance functions on non-convex domains. We provide useful approximations to improve computational efficiency, resulting in a scalable algorithm. We compare the performance of our method with those of competing state-of-the-art methods using simulation studies on synthetic non-convex domains. The method is applied to data regarding acidity levels in the Chesapeake Bay, showing its potential for ecological monitoring in real-world spatial applications on irregular domains.},
  archive      = {J_BIOMTC},
  author       = {Gilbert, Brian and Datta, Abhirup},
  doi          = {10.1093/biomtc/ujae089},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujae089},
  shortjournal = {Biometrics},
  title        = {Visibility graph-based covariance functions for scalable spatial analysis in non-convex partially euclidean domains},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-dimensional multivariate analysis of variance via
geometric median and bootstrapping. <em>BIOMTC</em>, <em>80</em>(3),
ujae088. (<a href="https://doi.org/10.1093/biomtc/ujae088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The geometric median, which is applicable to high-dimensional data, can be viewed as a generalization of the univariate median used in 1-dimensional data. It can be used as a robust estimator for identifying the location of multi-dimensional data and has a wide range of applications in real-world scenarios. This paper explores the problem of high-dimensional multivariate analysis of variance (MANOVA) using the geometric median. A maximum-type statistic that relies on the differences between the geometric medians among various groups is introduced. The distribution of the new test statistic is derived under the null hypothesis using Gaussian approximations, and its consistency under the alternative hypothesis is established. To approximate the distribution of the new statistic in high dimensions, a wild bootstrap algorithm is proposed and theoretically justified. Through simulation studies conducted across a variety of dimensions, sample sizes, and data-generating models, we demonstrate the finite-sample performance of our geometric median-based MANOVA method. Additionally, we implement the proposed approach to analyze a breast cancer gene expression dataset.},
  archive      = {J_BIOMTC},
  author       = {Cheng, Guanghui and Lin, Ruitao and Peng, Liuhua},
  doi          = {10.1093/biomtc/ujae088},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujae088},
  shortjournal = {Biometrics},
  title        = {High-dimensional multivariate analysis of variance via geometric median and bootstrapping},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Rejoinder to the discussion on “LEAP: The latent
exchangeability prior for borrowing information from historical data.”
<em>BIOMTC</em>, <em>80</em>(3), ujae087. (<a
href="https://doi.org/10.1093/biomtc/ujae087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discussions of our paper provide insights into the practical considerations of the latent exchangeability prior while also highlighting further extensions. In this rejoinder, we briefly summarize the discussions and provide comments.},
  archive      = {J_BIOMTC},
  author       = {Alt, Ethan M and Chang, Xiuya and Jiang, Xun and Liu, Qing and Mo, May and Xia, Hong Amy and Ibrahim, Joseph G},
  doi          = {10.1093/biomtc/ujae087},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujae087},
  shortjournal = {Biometrics},
  title        = {Rejoinder to the discussion on “LEAP: The latent exchangeability prior for borrowing information from historical data”},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discussion on “LEAP: The latent exchangeability prior for
borrowing information from historical data” by ethan m. Alt, xiuya
chang, xun jiang, qing liu, may mo, h. Amy xia, and joseph g. ibrahim.
<em>BIOMTC</em>, <em>80</em>(3), ujae086. (<a
href="https://doi.org/10.1093/biomtc/ujae086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This discussion provides commentary on the paper by Ethan M. Alt, Xiuya Chang, Xun Jiang, Qing Liu, May Mo, H. Amy Xia, and Joseph G. Ibrahim entitled “LEAP: the latent exchangeability prior for borrowing information from historical data”. The authors propose a novel method to bridge the incorporation of supplemental information into a study while also identifying potentially exchangeable subgroups to better facilitate information sharing. In this discussion, we highlight the potential relationship with other Bayesian model averaging approaches, such as multisource exchangeability modeling, and provide a brief numeric case study to illustrate how the concepts behind latent exchangeability prior may also improve the performance of existing methods. The results provided by Alt et al. are exciting, and we believe that the method represents a meaningful approach to more efficient information sharing.},
  archive      = {J_BIOMTC},
  author       = {Thomas, Shannon D and Kaizer, Alexander M},
  doi          = {10.1093/biomtc/ujae086},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujae086},
  shortjournal = {Biometrics},
  title        = {Discussion on “LEAP: The latent exchangeability prior for borrowing information from historical data” by ethan m. alt, xiuya chang, xun jiang, qing liu, may mo, h. amy xia, and joseph g. ibrahim},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discussion on “LEAP: The latent exchangeability prior for
borrowing information from historical data” by ethan m. Alt, xiuya
chang, xun jiang, qing liu, may mo, h. Amy xia, and joseph g. ibrahim.
<em>BIOMTC</em>, <em>80</em>(3), ujae085. (<a
href="https://doi.org/10.1093/biomtc/ujae085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the following discussion, we describe the various assumptions of exchangeability that have been made in the context of Bayesian borrowing and related models. In this context, we are able to highlight the difficulty of dynamic Bayesian borrowing under the assumption of individuals in the historical data being exchangeable with the current data and thus the strengths and novel features of the latent exchangeability prior. As borrowing methods are popular within clinical trials to augment the control arm, some potential challenges are identified with the application of the approach in this setting.},
  archive      = {J_BIOMTC},
  author       = {Scott, Darren and Lewin, Alex},
  doi          = {10.1093/biomtc/ujae085},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujae085},
  shortjournal = {Biometrics},
  title        = {Discussion on “LEAP: The latent exchangeability prior for borrowing information from historical data” by ethan m. alt, xiuya chang, xun jiang, qing liu, may mo, h. amy xia, and joseph g. ibrahim},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discussion on “LEAP: The latent exchangeability prior for
borrowing information from historical data” by ethan m. Alt, xiuya
chang, xun jiang, qing liu, may mo, h. Amy xia, and joseph g. ibrahim.
<em>BIOMTC</em>, <em>80</em>(3), ujae084. (<a
href="https://doi.org/10.1093/biomtc/ujae084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We commend Alt et al.’s innovative approach for analysis with a hybrid control arm while offering insights into two key considerations: the necessity for extrapolation and the potential benefits of curating historical control data before analysis.},
  archive      = {J_BIOMTC},
  author       = {Campbell, Harlan and Gustafson, Paul},
  doi          = {10.1093/biomtc/ujae084},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujae084},
  shortjournal = {Biometrics},
  title        = {Discussion on “LEAP: The latent exchangeability prior for borrowing information from historical data” by ethan m. alt, xiuya chang, xun jiang, qing liu, may mo, h. amy xia, and joseph g. ibrahim},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). LEAP: The latent exchangeability prior for borrowing
information from historical data. <em>BIOMTC</em>, <em>80</em>(3),
ujae083. (<a href="https://doi.org/10.1093/biomtc/ujae083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is becoming increasingly popular to elicit informative priors on the basis of historical data. Popular existing priors, including the power prior, commensurate prior, and robust meta-analytic predictive prior, provide blanket discounting. Thus, if only a subset of participants in the historical data are exchangeable with the current data, these priors may not be appropriate. In order to combat this issue, propensity score approaches have been proposed. However, these approaches are only concerned with the covariate distribution, whereas exchangeability is typically assessed with parameters pertaining to the outcome. In this paper, we introduce the latent exchangeability prior (LEAP), where observations in the historical data are classified into exchangeable and non-exchangeable groups. The LEAP discounts the historical data by identifying the most relevant subjects from the historical data. We compare our proposed approach against alternative approaches in simulations and present a case study using our proposed prior to augment a control arm in a phase 3 clinical trial in plaque psoriasis with an unbalanced randomization scheme.},
  archive      = {J_BIOMTC},
  author       = {Alt, Ethan M and Chang, Xiuya and Jiang, Xun and Liu, Qing and Mo, May and Xia, Hong Amy and Ibrahim, Joseph G},
  doi          = {10.1093/biomtc/ujae083},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujae083},
  shortjournal = {Biometrics},
  title        = {LEAP: The latent exchangeability prior for borrowing information from historical data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Summary statistics knockoffs inference with family-wise
error rate control. <em>BIOMTC</em>, <em>80</em>(3), ujae082. (<a
href="https://doi.org/10.1093/biomtc/ujae082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing multiple hypotheses of conditional independence with provable error rate control is a fundamental problem with various applications. To infer conditional independence with family-wise error rate (FWER) control when only summary statistics of marginal dependence are accessible, we adopt GhostKnockoff to directly generate knockoff copies of summary statistics and propose a new filter to select features conditionally dependent on the response. In addition, we develop a computationally efficient algorithm to greatly reduce the computational cost of knockoff copies generation without sacrificing power and FWER control. Experiments on simulated data and a real dataset of Alzheimer’s disease genetics demonstrate the advantage of the proposed method over existing alternatives in both statistical power and computational efficiency.},
  archive      = {J_BIOMTC},
  author       = {Yu, Catherine Xinrui and Gu, Jiaqi and Chen, Zhaomeng and He, Zihuai},
  doi          = {10.1093/biomtc/ujae082},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujae082},
  shortjournal = {Biometrics},
  title        = {Summary statistics knockoffs inference with family-wise error rate control},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric worst-case bounds for publication bias on the
summary receiver operating characteristic curve. <em>BIOMTC</em>,
<em>80</em>(3), ujae080. (<a
href="https://doi.org/10.1093/biomtc/ujae080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The summary receiver operating characteristic (SROC) curve has been recommended as one important meta-analytical summary to represent the accuracy of a diagnostic test in the presence of heterogeneous cutoff values. However, selective publication of diagnostic studies for meta-analysis can induce publication bias (PB) on the estimate of the SROC curve. Several sensitivity analysis methods have been developed to quantify PB on the SROC curve, and all these methods utilize parametric selection functions to model the selective publication mechanism. The main contribution of this article is to propose a new sensitivity analysis approach that derives the worst-case bounds for the SROC curve by adopting nonparametric selection functions under minimal assumptions. The estimation procedures of the worst-case bounds use the Monte Carlo method to approximate the bias on the SROC curves along with the corresponding area under the curves, and then the maximum and minimum values of PB under a range of marginal selection probabilities are optimized by nonlinear programming. We apply the proposed method to real-world meta-analyses to show that the worst-case bounds of the SROC curves can provide useful insights for discussing the robustness of meta-analytical findings on diagnostic test accuracy.},
  archive      = {J_BIOMTC},
  author       = {Zhou, Yi and Huang, Ao and Hattori, Satoshi},
  doi          = {10.1093/biomtc/ujae080},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujae080},
  shortjournal = {Biometrics},
  title        = {Nonparametric worst-case bounds for publication bias on the summary receiver operating characteristic curve},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards automated animal density estimation with acoustic
spatial capture-recapture. <em>BIOMTC</em>, <em>80</em>(3), ujae081. (<a
href="https://doi.org/10.1093/biomtc/ujae081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Passive acoustic monitoring can be an effective way of monitoring wildlife populations that are acoustically active but difficult to survey visually, but identifying target species calls in recordings is non-trivial. Machine learning (ML) techniques can do detection quickly but may miss calls and produce false positives, i.e., misidentify calls from other sources as being from the target species. While abundance estimation methods can address the former issue effectively, methods to deal with false positives are under-investigated. We propose an acoustic spatial capture-recapture (ASCR) method that deals with false positives by treating species identity as a latent variable. Individual-level outputs from ML techniques are treated as random variables whose distributions depend on the latent identity. This gives rise to a mixture model likelihood that we maximize to estimate call density. We compare our method to existing methods by applying it to an ASCR survey of frogs and simulated acoustic surveys of gibbons based on real gibbon acoustic data. Estimates from our method are closer to ASCR applied to the dataset without false positives than those from a widely used false positive “correction factor” method. Simulations show our method to have bias close to zero and accurate coverage probabilities and to perform substantially better than ASCR without accounting for false positives.},
  archive      = {J_BIOMTC},
  author       = {Wang, Yuheng and Ye, Juan and Li, Xiaohui and Borchers, David L},
  doi          = {10.1093/biomtc/ujae081},
  journal      = {Biometrics},
  month        = {8},
  number       = {3},
  pages        = {ujae081},
  shortjournal = {Biometrics},
  title        = {Towards automated animal density estimation with acoustic spatial capture-recapture},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hypothesis tests in ordinal predictive models with optimal
accuracy. <em>BIOMTC</em>, <em>80</em>(3), ujae079. (<a
href="https://doi.org/10.1093/biomtc/ujae079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world applications involving multi-class ordinal discrimination, a common approach is to aggregate multiple predictive variables into a linear combination, aiming to develop a classifier with high prediction accuracy. Assessment of such multi-class classifiers often utilizes the hypervolume under ROC manifolds (HUM). When dealing with a substantial pool of potential predictors and achieving optimal HUM, it becomes imperative to conduct appropriate statistical inference. However, prevalent methodologies in existing literature are computationally expensive. We propose to use the jackknife empirical likelihood method to address this issue. The Wilks’ theorem under moderate conditions is established and the power analysis under the Pitman alternative is provided. We also introduce a novel network-based rapid computation algorithm specifically designed for computing a general multi-sample |$U$| -statistic in our test procedure. To compare our approach against existing approaches, we conduct extensive simulations. Results demonstrate the superior performance of our method in terms of test size, power, and implementation time. Furthermore, we apply our method to analyze a real medical dataset and obtain some new findings.},
  archive      = {J_BIOMTC},
  author       = {Liu, Yuyang and Luo, Shan and Li, Jialiang},
  doi          = {10.1093/biomtc/ujae079},
  journal      = {Biometrics},
  month        = {8},
  number       = {3},
  pages        = {ujae079},
  shortjournal = {Biometrics},
  title        = {Hypothesis tests in ordinal predictive models with optimal accuracy},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Factor-augmented transformation models for interval-censored
failure time data. <em>BIOMTC</em>, <em>80</em>(3), ujae078. (<a
href="https://doi.org/10.1093/biomtc/ujae078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval-censored failure time data frequently arise in various scientific studies where each subject experiences periodical examinations for the occurrence of the failure event of interest, and the failure time is only known to lie in a specific time interval. In addition, collected data may include multiple observed variables with a certain degree of correlation, leading to severe multicollinearity issues. This work proposes a factor-augmented transformation model to analyze interval-censored failure time data while reducing model dimensionality and avoiding multicollinearity elicited by multiple correlated covariates. We provide a joint modeling framework by comprising a factor analysis model to group multiple observed variables into a few latent factors and a class of semiparametric transformation models with the augmented factors to examine their and other covariate effects on the failure event. Furthermore, we propose a nonparametric maximum likelihood estimation approach and develop a computationally stable and reliable expectation-maximization algorithm for its implementation. We establish the asymptotic properties of the proposed estimators and conduct simulation studies to assess the empirical performance of the proposed method. An application to the Alzheimer’s Disease Neuroimaging Initiative (ADNI) study is provided. An R package ICTransCFA is also available for practitioners. Data used in preparation of this article were obtained from the ADNI database.},
  archive      = {J_BIOMTC},
  author       = {Li, Hongxi and Li, Shuwei and Sun, Liuquan and Song, Xinyuan},
  doi          = {10.1093/biomtc/ujae078},
  journal      = {Biometrics},
  month        = {8},
  number       = {3},
  pages        = {ujae078},
  shortjournal = {Biometrics},
  title        = {Factor-augmented transformation models for interval-censored failure time data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing for similarity of multivariate mixed outcomes using
generalized joint regression models with application to
efficacy-toxicity responses. <em>BIOMTC</em>, <em>80</em>(3), ujae077.
(<a href="https://doi.org/10.1093/biomtc/ujae077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common problem in clinical trials is to test whether the effect of an explanatory variable on a response of interest is similar between two groups, for example, patient or treatment groups. In this regard, similarity is defined as equivalence up to a pre-specified threshold that denotes an acceptable deviation between the two groups. This issue is typically tackled by assessing if the explanatory variable’s effect on the response is similar. This assessment is based on, for example, confidence intervals of differences or a suitable distance between two parametric regression models. Typically, these approaches build on the assumption of a univariate continuous or binary outcome variable. However, multivariate outcomes, especially beyond the case of bivariate binary responses, remain underexplored. This paper introduces an approach based on a generalized joint regression framework exploiting the Gaussian copula. Compared to existing methods, our approach accommodates various outcome variable scales, such as continuous, binary, categorical, and ordinal, including mixed outcomes in multi-dimensional spaces. We demonstrate the validity of this approach through a simulation study and an efficacy-toxicity case study, hence highlighting its practical relevance.},
  archive      = {J_BIOMTC},
  author       = {Hagemann, Niklas and Marra, Giampiero and Bretz, Frank and Möllenhoff, Kathrin},
  doi          = {10.1093/biomtc/ujae077},
  journal      = {Biometrics},
  month        = {8},
  number       = {3},
  pages        = {ujae077},
  shortjournal = {Biometrics},
  title        = {Testing for similarity of multivariate mixed outcomes using generalized joint regression models with application to efficacy-toxicity responses},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reduced-rank clustered coefficient regression for addressing
multicollinearity in heterogeneous coefficient estimation.
<em>BIOMTC</em>, <em>80</em>(3), ujae076. (<a
href="https://doi.org/10.1093/biomtc/ujae076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustered coefficient regression (CCR) extends the classical regression model by allowing regression coefficients varying across observations and forming clusters of observations. It has become an increasingly useful tool for modeling the heterogeneous relationship between the predictor and response variables. A typical issue of existing CCR methods is that the estimation and clustering results can be unstable in the presence of multicollinearity. To address the instability issue, this paper introduces a low-rank structure of the CCR coefficient matrix and proposes a penalized non-convex optimization problem with an adaptive group fusion-type penalty tailor-made for this structure. An iterative algorithm is developed to solve this non-convex optimization problem with guaranteed convergence. An upper bound for the coefficient estimation error is also obtained to show the statistical property of the estimator. Empirical studies on both simulated datasets and a COVID-19 mortality rate dataset demonstrate the superiority of the proposed method to existing methods.},
  archive      = {J_BIOMTC},
  author       = {Zhong, Yan and He, Kejun and Li, Gefei},
  doi          = {10.1093/biomtc/ujae076},
  journal      = {Biometrics},
  month        = {7},
  number       = {3},
  pages        = {ujae076},
  shortjournal = {Biometrics},
  title        = {Reduced-rank clustered coefficient regression for addressing multicollinearity in heterogeneous coefficient estimation},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The multivariate bernoulli detector: Change point estimation
in discrete survival analysis. <em>BIOMTC</em>, <em>80</em>(3), ujae075.
(<a href="https://doi.org/10.1093/biomtc/ujae075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-to-event data are often recorded on a discrete scale with multiple, competing risks as potential causes for the event. In this context, application of continuous survival analysis methods with a single risk suffers from biased estimation. Therefore, we propose the multivariate Bernoulli detector for competing risks with discrete times involving a multivariate change point model on the cause-specific baseline hazards. Through the prior on the number of change points and their location, we impose dependence between change points across risks, as well as allowing for data-driven learning of their number. Then, conditionally on these change points, a multivariate Bernoulli prior is used to infer which risks are involved. Focus of posterior inference is cause-specific hazard rates and dependence across risks. Such dependence is often present due to subject-specific changes across time that affect all risks. Full posterior inference is performed through a tailored local-global Markov chain Monte Carlo (MCMC) algorithm, which exploits a data augmentation trick and MCMC updates from nonconjugate Bayesian nonparametric methods. We illustrate our model in simulations and on ICU data, comparing its performance with existing approaches.},
  archive      = {J_BIOMTC},
  author       = {van den Boom, Willem and De Iorio, Maria and Qian, Fang and Guglielmi, Alessandra},
  doi          = {10.1093/biomtc/ujae075},
  journal      = {Biometrics},
  month        = {7},
  number       = {3},
  pages        = {ujae075},
  shortjournal = {Biometrics},
  title        = {The multivariate bernoulli detector: Change point estimation in discrete survival analysis},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semiparametric inference of effective reproduction number
dynamics from wastewater pathogen surveillance data. <em>BIOMTC</em>,
<em>80</em>(3), ujae074. (<a
href="https://doi.org/10.1093/biomtc/ujae074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concentrations of pathogen genomes measured in wastewater have recently become available as a new data source to use when modeling the spread of infectious diseases. One promising use for this data source is inference of the effective reproduction number, the average number of individuals a newly infected person will infect. We propose a model where new infections arrive according to a time-varying immigration rate which can be interpreted as an average number of secondary infections produced by one infectious individual per unit time. This model allows us to estimate the effective reproduction number from concentrations of pathogen genomes, while avoiding difficulty to verify assumptions about the dynamics of the susceptible population. As a byproduct of our primary goal, we also produce a new model for estimating the effective reproduction number from case data using the same framework. We test this modeling framework in an agent-based simulation study with a realistic data generating mechanism which accounts for the time-varying dynamics of pathogen shedding. Finally, we apply our new model to estimating the effective reproduction number of SARS-CoV-2, the causative agent of COVID-19, in Los Angeles, CA, using pathogen RNA concentrations collected from a large wastewater treatment facility.},
  archive      = {J_BIOMTC},
  author       = {Goldstein, Isaac H and Parker, Daniel M and Jiang, Sunny and Minin, Volodymyr M},
  doi          = {10.1093/biomtc/ujae074},
  journal      = {Biometrics},
  month        = {7},
  number       = {3},
  pages        = {ujae074},
  shortjournal = {Biometrics},
  title        = {Semiparametric inference of effective reproduction number dynamics from wastewater pathogen surveillance data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A generalized outcome-adaptive sequential multiple
assignment randomized trial design. <em>BIOMTC</em>, <em>80</em>(3),
ujae073. (<a href="https://doi.org/10.1093/biomtc/ujae073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A dynamic treatment regime (DTR) is a mathematical representation of a multistage decision process. When applied to sequential treatment selection in medical settings, DTRs are useful for identifying optimal therapies for chronic diseases such as AIDs, mental illnesses, substance abuse, and many cancers. Sequential multiple assignment randomized trials (SMARTs) provide a useful framework for constructing DTRs and providing unbiased between-DTR comparisons. A limitation of SMARTs is that they ignore data from past patients that may be useful for reducing the probability of exposing new patients to inferior treatments. In practice, this may result in decreased treatment adherence or dropouts. To address this problem, we propose a generalized outcome-adaptive (GO) SMART design that adaptively unbalances stage-specific randomization probabilities in favor of treatments observed to be more effective in previous patients. To correct for bias induced by outcome adaptive randomization, we propose G-estimators and inverse-probability-weighted estimators of DTR effects embedded in a GO-SMART and show analytically that they are consistent. We report simulation results showing that, compared to a SMART, Response-Adaptive SMART and SMART with adaptive randomization, a GO-SMART design treats significantly more patients with the optimal DTR and achieves a larger number of total responses while maintaining similar or better statistical power.},
  archive      = {J_BIOMTC},
  author       = {Yang, Xue and Cheng, Yu and Thall, Peter F and Wahed, Abdus S},
  doi          = {10.1093/biomtc/ujae073},
  journal      = {Biometrics},
  month        = {7},
  number       = {3},
  pages        = {ujae073},
  shortjournal = {Biometrics},
  title        = {A generalized outcome-adaptive sequential multiple assignment randomized trial design},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving prediction of linear regression models by
integrating external information from heterogeneous populations:
James–stein estimators. <em>BIOMTC</em>, <em>80</em>(3), ujae072. (<a
href="https://doi.org/10.1093/biomtc/ujae072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the setting where (1) an internal study builds a linear regression model for prediction based on individual-level data, (2) some external studies have fitted similar linear regression models that use only subsets of the covariates and provide coefficient estimates for the reduced models without individual-level data, and (3) there is heterogeneity across these study populations. The goal is to integrate the external model summary information into fitting the internal model to improve prediction accuracy. We adapt the James–Stein shrinkage method to propose estimators that are no worse and are oftentimes better in the prediction mean squared error after information integration, regardless of the degree of study population heterogeneity. We conduct comprehensive simulation studies to investigate the numerical performance of the proposed estimators. We also apply the method to enhance a prediction model for patella bone lead level in terms of blood lead level and other covariates by integrating summary information from published literature.},
  archive      = {J_BIOMTC},
  author       = {Han, Peisong and Li, Haoyue and Park, Sung Kyun and Mukherjee, Bhramar and Taylor, Jeremy M G},
  doi          = {10.1093/biomtc/ujae072},
  journal      = {Biometrics},
  month        = {7},
  number       = {3},
  pages        = {ujae072},
  shortjournal = {Biometrics},
  title        = {Improving prediction of linear regression models by integrating external information from heterogeneous populations: James–Stein estimators},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric second-order estimation for spatiotemporal
point patterns. <em>BIOMTC</em>, <em>80</em>(3), ujae071. (<a
href="https://doi.org/10.1093/biomtc/ujae071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many existing methodologies for analyzing spatiotemporal point patterns are developed based on the assumption of stationarity in both space and time for the second-order intensity or pair correlation. In practice, however, such an assumption often lacks validity or proves to be unrealistic. In this paper, we propose a novel and flexible nonparametric approach for estimating the second-order characteristics of spatiotemporal point processes, accommodating non-stationary temporal correlations. Our proposed method employs kernel smoothing and effectively accounts for spatial and temporal correlations differently. Under a spatially increasing-domain asymptotic framework, we establish consistency of the proposed estimators, which can be constructed using different first-order intensity estimators to enhance practicality. Simulation results reveal that our method, in comparison with existing approaches, significantly improves statistical efficiency. An application to a COVID-19 dataset further illustrates the flexibility and interpretability of our procedure.},
  archive      = {J_BIOMTC},
  author       = {Liang, Decai and Liu, Jialing and Shen, Ye and Guan, Yongtao},
  doi          = {10.1093/biomtc/ujae071},
  journal      = {Biometrics},
  month        = {7},
  number       = {3},
  pages        = {ujae071},
  shortjournal = {Biometrics},
  title        = {Nonparametric second-order estimation for spatiotemporal point patterns},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal meta-analysis by integrating multiple observational
studies with multivariate outcomes. <em>BIOMTC</em>, <em>80</em>(3),
ujae070. (<a href="https://doi.org/10.1093/biomtc/ujae070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating multiple observational studies to make unconfounded causal or descriptive comparisons of group potential outcomes in a large natural population is challenging. Moreover, retrospective cohorts, being convenience samples, are usually unrepresentative of the natural population of interest and have groups with unbalanced covariates. We propose a general covariate-balancing framework based on pseudo-populations that extends established weighting methods to the meta-analysis of multiple retrospective cohorts with multiple groups. Additionally, by maximizing the effective sample sizes of the cohorts, we propose a FLEX ible, O ptimized, and R ealistic (FLEXOR) weighting method appropriate for integrative analyses. We develop new weighted estimators for unconfounded inferences on wide-ranging population-level features and estimands relevant to group comparisons of quantitative, categorical, or multivariate outcomes. Asymptotic properties of these estimators are examined. Through simulation studies and meta-analyses of TCGA datasets, we demonstrate the versatility and reliability of the proposed weighting strategy, especially for the FLEXOR pseudo-population.},
  archive      = {J_BIOMTC},
  author       = {Guha, Subharup and Li, Yi},
  doi          = {10.1093/biomtc/ujae070},
  journal      = {Biometrics},
  month        = {7},
  number       = {3},
  pages        = {ujae070},
  shortjournal = {Biometrics},
  title        = {Causal meta-analysis by integrating multiple observational studies with multivariate outcomes},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Propensity weighting plus adjustment in proportional hazards
model is not doubly robust. <em>BIOMTC</em>, <em>80</em>(3), ujae069.
(<a href="https://doi.org/10.1093/biomtc/ujae069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, it has become common for applied works to combine commonly used survival analysis modeling methods, such as the multivariable Cox model and propensity score weighting, with the intention of forming a doubly robust estimator of an exposure effect hazard ratio that is unbiased in large samples when either the Cox model or the propensity score model is correctly specified. This combination does not, in general, produce a doubly robust estimator, even after regression standardization, when there is truly a causal effect. We demonstrate via simulation this lack of double robustness for the semiparametric Cox model, the Weibull proportional hazards model, and a simple proportional hazards flexible parametric model, with both the latter models fit via maximum likelihood. We provide a novel proof that the combination of propensity score weighting and a proportional hazards survival model, fit either via full or partial likelihood, is consistent under the null of no causal effect of the exposure on the outcome under particular censoring mechanisms if either the propensity score or the outcome model is correctly specified and contains all confounders. Given our results suggesting that double robustness only exists under the null, we outline 2 simple alternative estimators that are doubly robust for the survival difference at a given time point (in the above sense), provided the censoring mechanism can be correctly modeled, and one doubly robust method of estimation for the full survival curve. We provide R code to use these estimators for estimation and inference in the supporting information.},
  archive      = {J_BIOMTC},
  author       = {Gabriel, Erin E and Sachs, Michael C and Waernbaum, Ingeborg and Goetghebeur, Els and Blanche, Paul F and Vansteelandt, Stijn and Sjölander, Arvid and Scheike, Thomas},
  doi          = {10.1093/biomtc/ujae069},
  journal      = {Biometrics},
  month        = {7},
  number       = {3},
  pages        = {ujae069},
  shortjournal = {Biometrics},
  title        = {Propensity weighting plus adjustment in proportional hazards model is not doubly robust},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A gaussian-process approximation to a spatial SIR process
using moment closures and emulators. <em>BIOMTC</em>, <em>80</em>(3),
ujae068. (<a href="https://doi.org/10.1093/biomtc/ujae068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dynamics that govern disease spread are hard to model because infections are functions of both the underlying pathogen as well as human or animal behavior. This challenge is increased when modeling how diseases spread between different spatial locations. Many proposed spatial epidemiological models require trade-offs to fit, either by abstracting away theoretical spread dynamics, fitting a deterministic model, or by requiring large computational resources for many simulations. We propose an approach that approximates the complex spatial spread dynamics with a Gaussian process. We first propose a flexible spatial extension to the well-known SIR stochastic process, and then we derive a moment-closure approximation to this stochastic process. This moment-closure approximation yields ordinary differential equations for the evolution of the means and covariances of the susceptibles and infectious through time. Because these ODEs are a bottleneck to fitting our model by MCMC, we approximate them using a low-rank emulator. This approximation serves as the basis for our hierarchical model for noisy, underreported counts of new infections by spatial location and time. We demonstrate using our model to conduct inference on simulated infections from the underlying, true spatial SIR jump process. We then apply our method to model counts of new Zika infections in Brazil from late 2015 through early 2016.},
  archive      = {J_BIOMTC},
  author       = {Trostle, Parker and Guinness, Joseph and Reich, Brian J},
  doi          = {10.1093/biomtc/ujae068},
  journal      = {Biometrics},
  month        = {7},
  number       = {3},
  pages        = {ujae068},
  shortjournal = {Biometrics},
  title        = {A gaussian-process approximation to a spatial SIR process using moment closures and emulators},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint structure learning and causal effect estimation for
categorical graphical models. <em>BIOMTC</em>, <em>80</em>(3), ujae067.
(<a href="https://doi.org/10.1093/biomtc/ujae067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scope of this paper is a multivariate setting involving categorical variables. Following an external manipulation of one variable, the goal is to evaluate the causal effect on an outcome of interest. A typical scenario involves a system of variables representing lifestyle, physical and mental features, symptoms, and risk factors, with the outcome being the presence or absence of a disease. These variables are interconnected in complex ways, allowing the effect of an intervention to propagate through multiple paths. A distinctive feature of our approach is the estimation of causal effects while accounting for uncertainty in both the dependence structure, which we represent through a directed acyclic graph (DAG), and the DAG-model parameters. Specifically, we propose a Markov chain Monte Carlo algorithm that targets the joint posterior over DAGs and parameters, based on an efficient reversible-jump proposal scheme. We validate our method through extensive simulation studies and demonstrate that it outperforms current state-of-the-art procedures in terms of estimation accuracy. Finally, we apply our methodology to analyze a dataset on depression and anxiety in undergraduate students.},
  archive      = {J_BIOMTC},
  author       = {Castelletti, Federico and Consonni, Guido and Della Vedova, Marco L},
  doi          = {10.1093/biomtc/ujae067},
  journal      = {Biometrics},
  month        = {7},
  number       = {3},
  pages        = {ujae067},
  shortjournal = {Biometrics},
  title        = {Joint structure learning and causal effect estimation for categorical graphical models},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An interpretable bayesian clustering approach with feature
selection for analyzing spatially resolved transcriptomics data.
<em>BIOMTC</em>, <em>80</em>(3), ujae066. (<a
href="https://doi.org/10.1093/biomtc/ujae066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent breakthroughs in spatially resolved transcriptomics (SRT) technologies have enabled comprehensive molecular characterization at the spot or cellular level while preserving spatial information. Cells are the fundamental building blocks of tissues, organized into distinct yet connected components. Although many non-spatial and spatial clustering approaches have been used to partition the entire region into mutually exclusive spatial domains based on the SRT high-dimensional molecular profile, most require an ad hoc selection of less interpretable dimensional-reduction techniques. To overcome this challenge, we propose a zero-inflated negative binomial mixture model to cluster spots or cells based on their molecular profiles. To increase interpretability, we employ a feature selection mechanism to provide a low-dimensional summary of the SRT molecular profile in terms of discriminating genes that shed light on the clustering result. We further incorporate the SRT geospatial profile via a Markov random field prior. We demonstrate how this joint modeling strategy improves clustering accuracy, compared with alternative state-of-the-art approaches, through simulation studies and 3 real data applications.},
  archive      = {J_BIOMTC},
  author       = {Li, Huimin and Zhu, Bencong and Jiang, Xi and Guo, Lei and Xie, Yang and Xu, Lin and Li, Qiwei},
  doi          = {10.1093/biomtc/ujae066},
  journal      = {Biometrics},
  month        = {7},
  number       = {3},
  pages        = {ujae066},
  shortjournal = {Biometrics},
  title        = {An interpretable bayesian clustering approach with feature selection for analyzing spatially resolved transcriptomics data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiply robust estimation of marginal structural models in
observational studies subject to covariate-driven observations.
<em>BIOMTC</em>, <em>80</em>(3), ujae065. (<a
href="https://doi.org/10.1093/biomtc/ujae065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic health records and other sources of observational data are increasingly used for drawing causal inferences. The estimation of a causal effect using these data not meant for research purposes is subject to confounding and irregularly-spaced covariate-driven observation times affecting the inference. A doubly-weighted estimator accounting for these features has previously been proposed that relies on the correct specification of two nuisance models used for the weights. In this work, we propose a novel consistent multiply robust estimator and demonstrate analytically and in comprehensive simulation studies that it is more flexible and more efficient than the only alternative estimator proposed for the same setting. It is further applied to data from the Add Health study in the United States to estimate the causal effect of therapy counseling on alcohol consumption in American adolescents.},
  archive      = {J_BIOMTC},
  author       = {Coulombe, Janie and Yang, Shu},
  doi          = {10.1093/biomtc/ujae065},
  journal      = {Biometrics},
  month        = {7},
  number       = {3},
  pages        = {ujae065},
  shortjournal = {Biometrics},
  title        = {Multiply robust estimation of marginal structural models in observational studies subject to covariate-driven observations},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Controlling false discovery rate for mediator selection in
high-dimensional data. <em>BIOMTC</em>, <em>80</em>(3), ujae064. (<a
href="https://doi.org/10.1093/biomtc/ujae064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need to select mediators from a high dimensional data source, such as neuroimaging data and genetic data, arises in much scientific research. In this work, we formulate a multiple-hypothesis testing framework for mediator selection from a high-dimensional candidate set, and propose a method, which extends the recent development in false discovery rate (FDR)-controlled variable selection with knockoff to select mediators with FDR control. We show that the proposed method and algorithm achieved finite sample FDR control. We present extensive simulation results to demonstrate the power and finite sample performance compared with the existing method. Lastly, we demonstrate the method for analyzing the Adolescent Brain Cognitive Development (ABCD) study, in which the proposed method selects several resting-state functional magnetic resonance imaging connectivity markers as mediators for the relationship between adverse childhood events and the crystallized composite score in the NIH toolbox.},
  archive      = {J_BIOMTC},
  author       = {Dai, Ran and Li, Ruiyang and Lee, Seonjoo and Liu, Ying},
  doi          = {10.1093/biomtc/ujae064},
  journal      = {Biometrics},
  month        = {7},
  number       = {3},
  pages        = {ujae064},
  shortjournal = {Biometrics},
  title        = {Controlling false discovery rate for mediator selection in high-dimensional data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric receiver operating characteristic curve
analysis with an imperfect gold standard. <em>BIOMTC</em>,
<em>80</em>(3), ujae063. (<a
href="https://doi.org/10.1093/biomtc/ujae063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the challenge of estimating receiver operating characteristic (ROC) curves and the areas under these curves (AUC) in the context of an imperfect gold standard, a common issue in diagnostic accuracy studies. We delve into the nonparametric identification and estimation of ROC curves and AUCs when the reference standard for disease status is prone to error. Our approach hinges on the known or estimable accuracy of this imperfect reference standard and the conditional independent assumption, under which we demonstrate the identifiability of ROC curves and propose a nonparametric estimation method. In cases where the accuracy of the imperfect reference standard remains unknown, we establish that while ROC curves are unidentifiable, the sign of the difference between two AUCs is identifiable. This insight leads us to develop a hypothesis-testing method for assessing the relative superiority of AUCs. Compared to the existing methods, the proposed methods are nonparametric so that they do not rely on the parametric model assumptions. In addition, they are applicable to both the ROC/AUC analysis of continuous biomarkers and the AUC analysis of ordinal biomarkers. Our theoretical results and simulation studies validate the proposed methods, which we further illustrate through application in two real-world diagnostic studies.},
  archive      = {J_BIOMTC},
  author       = {Sun, Jiarui and Tang, Chao and Xie, Wuxiang and Zhou, Xiao-Hua},
  doi          = {10.1093/biomtc/ujae063},
  journal      = {Biometrics},
  month        = {7},
  number       = {3},
  pages        = {ujae063},
  shortjournal = {Biometrics},
  title        = {Nonparametric receiver operating characteristic curve analysis with an imperfect gold standard},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Absolute risk from double nested case-control designs:
Cause-specific proportional hazards models with and without augmented
estimating equations. <em>BIOMTC</em>, <em>80</em>(3), ujae062. (<a
href="https://doi.org/10.1093/biomtc/ujae062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We estimate relative hazards and absolute risks (or cumulative incidence or crude risk) under cause-specific proportional hazards models for competing risks from double nested case-control (DNCC) data. In the DNCC design, controls are time-matched not only to cases from the cause of primary interest, but also to cases from competing risks (the phase-two sample). Complete covariate data are available in the phase-two sample, but other cohort members only have information on survival outcomes and some covariates. Design-weighted estimators use inverse sampling probabilities computed from Samuelsen-type calculations for DNCC. To take advantage of additional information available on all cohort members, we augment the estimating equations with a term that is unbiased for zero but improves the efficiency of estimates from the cause-specific proportional hazards model. We establish the asymptotic properties of the proposed estimators, including the estimator of absolute risk, and derive consistent variance estimators. We show that augmented design-weighted estimators are more efficient than design-weighted estimators. Through simulations, we show that the proposed asymptotic methods yield nominal operating characteristics in practical sample sizes. We illustrate the methods using prostate cancer mortality data from the Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial Study of the National Cancer Institute.},
  archive      = {J_BIOMTC},
  author       = {Lee, Minjung and Gail, Mitchell H},
  doi          = {10.1093/biomtc/ujae062},
  journal      = {Biometrics},
  month        = {7},
  number       = {3},
  pages        = {ujae062},
  shortjournal = {Biometrics},
  title        = {Absolute risk from double nested case-control designs: Cause-specific proportional hazards models with and without augmented estimating equations},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal refinement of strata to balance covariates.
<em>BIOMTC</em>, <em>80</em>(3), ujae061. (<a
href="https://doi.org/10.1093/biomtc/ujae061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {What is the best way to split one stratum into two to maximally reduce the within-stratum imbalance in many covariates? We formulate this as an integer program and approximate the solution by randomized rounding of a linear program. A linear program may assign a fraction of a person to each refined stratum. Randomized rounding views fractional people as probabilities, assigning intact people to strata using biased coins. Randomized rounding is a well-studied theoretical technique for approximating the optimal solution of certain insoluble integer programs. When the number of people in a stratum is large relative to the number of covariates, we prove the following new results: (i) randomized rounding to split a stratum does very little randomizing, so it closely resembles the linear programming relaxation without splitting intact people; (ii) the linear relaxation and the randomly rounded solution place lower and upper bounds on the unattainable integer programming solution; and because of (i), these bounds are often close, thereby ratifying the usable randomly rounded solution. We illustrate using an observational study that balanced many covariates by forming matched pairs composed of 2016 patients selected from 5735 using a propensity score. Instead, we form 5 propensity score strata and refine them into 10 strata, obtaining excellent covariate balance while retaining all patients. An R package optrefine at CRAN implements the method. Supplementary materials are available online.},
  archive      = {J_BIOMTC},
  author       = {Brumberg, Katherine and Small, Dylan S and Rosenbaum, Paul R},
  doi          = {10.1093/biomtc/ujae061},
  journal      = {Biometrics},
  month        = {7},
  number       = {3},
  pages        = {ujae061},
  shortjournal = {Biometrics},
  title        = {Optimal refinement of strata to balance covariates},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PathGPS: Discover shared genetic architecture using GWAS
summary data. <em>BIOMTC</em>, <em>80</em>(3), ujae060. (<a
href="https://doi.org/10.1093/biomtc/ujae060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing availability and scale of biobanks and “omic” datasets bring new horizons for understanding biological mechanisms. PathGPS is an exploratory data analysis tool to discover genetic architectures using Genome Wide Association Studies (GWAS) summary data. PathGPS is based on a linear structural equation model where traits are regulated by both genetic and environmental pathways. PathGPS decouples the genetic and environmental components by contrasting the GWAS associations of “signal” genes with those of “noise” genes. From the estimated genetic component, PathGPS then extracts genetic pathways via principal component and factor analysis, leveraging the low-rank and sparse properties. In addition, we provide a bootstrap aggregating (“bagging”) algorithm to improve stability under data perturbation and hyperparameter tuning. When applied to a metabolomics dataset and the UK Biobank, PathGPS confirms several known gene–trait clusters and suggests multiple new hypotheses for future investigations.},
  archive      = {J_BIOMTC},
  author       = {Gao, Zijun and Zhao, Qingyuan and Hastie, Trevor},
  doi          = {10.1093/biomtc/ujae060},
  journal      = {Biometrics},
  month        = {7},
  number       = {3},
  pages        = {ujae060},
  shortjournal = {Biometrics},
  title        = {PathGPS: Discover shared genetic architecture using GWAS summary data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian inference for multivariate probit model with latent
envelope. <em>BIOMTC</em>, <em>80</em>(3), ujae059. (<a
href="https://doi.org/10.1093/biomtc/ujae059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The response envelope model proposed by Cook et al. ( 2010 ) is an efficient method to estimate the regression coefficient under the context of the multivariate linear regression model. It improves estimation efficiency by identifying material and immaterial parts of responses and removing the immaterial variation. The response envelope model has been investigated only for continuous response variables. In this paper, we propose the multivariate probit model with latent envelope, in short, the probit envelope model, as a response envelope model for multivariate binary response variables. The probit envelope model takes into account relations between Gaussian latent variables of the multivariate probit model by using the idea of the response envelope model. We address the identifiability of the probit envelope model by employing the essential identifiability concept and suggest a Bayesian method for the parameter estimation. We illustrate the probit envelope model via simulation studies and real-data analysis. The simulation studies show that the probit envelope model has the potential to gain efficiency in estimation compared to the multivariate probit model. The real data analysis shows that the probit envelope model is useful for multi-label classification.},
  archive      = {J_BIOMTC},
  author       = {Lee, Kwangmin and Park, Yeonhee},
  doi          = {10.1093/biomtc/ujae059},
  journal      = {Biometrics},
  month        = {7},
  number       = {3},
  pages        = {ujae059},
  shortjournal = {Biometrics},
  title        = {Bayesian inference for multivariate probit model with latent envelope},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differential recall bias in estimating treatment effects in
observational studies. <em>BIOMTC</em>, <em>80</em>(2), ujae058. (<a
href="https://doi.org/10.1093/biomtc/ujae058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Observational studies are frequently used to estimate the effect of an exposure or treatment on an outcome. To obtain an unbiased estimate of the treatment effect, it is crucial to measure the exposure accurately. A common type of exposure misclassification is recall bias, which occurs in retrospective cohort studies when study subjects may inaccurately recall their past exposure. Particularly challenging is differential recall bias in the context of self-reported binary exposures, where the bias may be directional rather than random and its extent varies according to the outcomes experienced. This paper makes several contributions: (1) it establishes bounds for the average treatment effect even when a validation study is not available; (2) it proposes multiple estimation methods across various strategies predicated on different assumptions; and (3) it suggests a sensitivity analysis technique to assess the robustness of the causal conclusion, incorporating insights from prior research. The effectiveness of these methods is demonstrated through simulation studies that explore various model misspecification scenarios. These approaches are then applied to investigate the effect of childhood physical abuse on mental health in adulthood.},
  archive      = {J_BIOMTC},
  author       = {Bong, Suhwan and Lee, Kwonsang and Dominici, Francesca},
  doi          = {10.1093/biomtc/ujae058},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae058},
  shortjournal = {Biometrics},
  title        = {Differential recall bias in estimating treatment effects in observational studies},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian convolutional neural network-based generalized
linear model. <em>BIOMTC</em>, <em>80</em>(2), ujae057. (<a
href="https://doi.org/10.1093/biomtc/ujae057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) provide flexible function approximations for a wide variety of applications when the input variables are in the form of images or spatial data. Although CNNs often outperform traditional statistical models in prediction accuracy, statistical inference, such as estimating the effects of covariates and quantifying the prediction uncertainty, is not trivial due to the highly complicated model structure and overparameterization. To address this challenge, we propose a new Bayesian approach by embedding CNNs within the generalized linear models (GLMs) framework. We use extracted nodes from the last hidden layer of CNN with Monte Carlo (MC) dropout as informative covariates in GLM. This improves accuracy in prediction and regression coefficient inference, allowing for the interpretation of coefficients and uncertainty quantification. By fitting ensemble GLMs across multiple realizations from MC dropout, we can account for uncertainties in extracting the features. We apply our methods to biological and epidemiological problems, which have both high-dimensional correlated inputs and vector covariates. Specifically, we consider malaria incidence data, brain tumor image data, and fMRI data. By extracting information from correlated inputs, the proposed method can provide an interpretable Bayesian analysis. The algorithm can be broadly applicable to image regressions or correlated data analysis by enabling accurate Bayesian inference quickly.},
  archive      = {J_BIOMTC},
  author       = {Jeon, Yeseul and Chang, Won and Jeong, Seonghyun and Han, Sanghoon and Park, Jaewoo},
  doi          = {10.1093/biomtc/ujae057},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae057},
  shortjournal = {Biometrics},
  title        = {A bayesian convolutional neural network-based generalized linear model},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient testing of the biomarker positive and negative
subgroups in a biomarker-stratified trial. <em>BIOMTC</em>,
<em>80</em>(2), ujae056. (<a
href="https://doi.org/10.1093/biomtc/ujae056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many randomized placebo-controlled trials with a biomarker defined subgroup, it is believed that this subgroup has the same or higher treatment effect compared with its complement. These subgroups are often referred to as the biomarker positive and negative subgroups. Most biomarker-stratified pivotal trials are aimed at demonstrating a significant treatment effect either in the biomarker positive subgroup or in the overall population. A major shortcoming of this approach is that the treatment can be declared effective in the overall population even though it has no effect in the biomarker negative subgroup. We use the isotonic assumption about the treatment effects in the two subgroups to construct an efficient way to test for a treatment effect in both the biomarker positive and negative subgroups. A substantial reduction in the required sample size for such a trial compared with existing methods makes evaluating the treatment effect in both the biomarker positive and negative subgroups feasible in pivotal trials especially when the prevalence of the biomarker positive subgroup is less than 0.5.},
  archive      = {J_BIOMTC},
  author       = {Li, Lang and Ivanova, Anastasia},
  doi          = {10.1093/biomtc/ujae056},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae056},
  shortjournal = {Biometrics},
  title        = {Efficient testing of the biomarker positive and negative subgroups in a biomarker-stratified trial},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Doubly robust proximal synthetic controls. <em>BIOMTC</em>,
<em>80</em>(2), ujae055. (<a
href="https://doi.org/10.1093/biomtc/ujae055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To infer the treatment effect for a single treated unit using panel data, synthetic control (SC) methods construct a linear combination of control units’ outcomes that mimics the treated unit’s pre-treatment outcome trajectory. This linear combination is subsequently used to impute the counterfactual outcomes of the treated unit had it not been treated in the post-treatment period, and used to estimate the treatment effect. Existing SC methods rely on correctly modeling certain aspects of the counterfactual outcome generating mechanism and may require near-perfect matching of the pre-treatment trajectory. Inspired by proximal causal inference, we obtain two novel nonparametric identifying formulas for the average treatment effect for the treated unit: one is based on weighting, and the other combines models for the counterfactual outcome and the weighting function. We introduce the concept of covariate shift to SCs to obtain these identification results conditional on the treatment assignment. We also develop two treatment effect estimators based on these two formulas and generalized method of moments. One new estimator is doubly robust: it is consistent and asymptotically normal if at least one of the outcome and weighting models is correctly specified. We demonstrate the performance of the methods via simulations and apply them to evaluate the effectiveness of a pneumococcal conjugate vaccine on the risk of all-cause pneumonia in Brazil.},
  archive      = {J_BIOMTC},
  author       = {Qiu, Hongxiang and Shi, Xu and Miao, Wang and Dobriban, Edgar and Tchetgen Tchetgen, Eric},
  doi          = {10.1093/biomtc/ujae055},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae055},
  shortjournal = {Biometrics},
  title        = {Doubly robust proximal synthetic controls},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incorporating nonparametric methods for estimating causal
excursion effects in mobile health with zero-inflated count outcomes.
<em>BIOMTC</em>, <em>80</em>(2), ujae054. (<a
href="https://doi.org/10.1093/biomtc/ujae054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mobile health, tailoring interventions for real-time delivery is of paramount importance. Micro-randomized trials have emerged as the “gold-standard” methodology for developing such interventions. Analyzing data from these trials provides insights into the efficacy of interventions and the potential moderation by specific covariates. The “causal excursion effect,” a novel class of causal estimand, addresses these inquiries. Yet, existing research mainly focuses on continuous or binary data, leaving count data largely unexplored. The current work is motivated by the Drink Less micro-randomized trial from the UK, which focuses on a zero-inflated proximal outcome, i.e., the number of screen views in the subsequent hour following the intervention decision point. To be specific, we revisit the concept of causal excursion effect, specifically for zero-inflated count outcomes, and introduce novel estimation approaches that incorporate nonparametric techniques. Bidirectional asymptotics are established for the proposed estimators. Simulation studies are conducted to evaluate the performance of the proposed methods. As an illustration, we also implement these methods to the Drink Less trial data.},
  archive      = {J_BIOMTC},
  author       = {Liu, Xueqing and Qian, Tianchen and Bell, Lauren and Chakraborty, Bibhas},
  doi          = {10.1093/biomtc/ujae054},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae054},
  shortjournal = {Biometrics},
  title        = {Incorporating nonparametric methods for estimating causal excursion effects in mobile health with zero-inflated count outcomes},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Introduction to statistical modelling and inference by
murray aitkin, CRC press, 2023, ISBN: 978-1032105710,
https://www.routledge.com/introduction-to-statistical-modelling-and-inference/aitkin/p/book/9781032105710.
<em>BIOMTC</em>, <em>80</em>(2), ujae053. (<a
href="https://doi.org/10.1093/biomtc/ujae053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Hsiao, Chuhsing Kate},
  doi          = {10.1093/biomtc/ujae053},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae053},
  shortjournal = {Biometrics},
  title        = {Introduction to statistical modelling and inference by murray aitkin, CRC press, 2023, ISBN: 978-1032105710, https://www.routledge.com/Introduction-to-statistical-modelling-and-Inference/Aitkin/p/book/9781032105710},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rejoinder to “on exact randomization-based
covariate-adjusted confidence intervals” by jacob fiksel.
<em>BIOMTC</em>, <em>80</em>(2), ujae052. (<a
href="https://doi.org/10.1093/biomtc/ujae052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Zhu, Ke and Liu, Hanzhong},
  doi          = {10.1093/biomtc/ujae052},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae052},
  shortjournal = {Biometrics},
  title        = {Rejoinder to “On exact randomization-based covariate-adjusted confidence intervals” by jacob fiksel},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On exact randomization-based covariate-adjusted confidence
intervals. <em>BIOMTC</em>, <em>80</em>(2), ujae051. (<a
href="https://doi.org/10.1093/biomtc/ujae051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomization-based inference using the Fisher randomization test allows for the computation of Fisher-exact P- values, making it an attractive option for the analysis of small, randomized experiments with non-normal outcomes. Two common test statistics used to perform Fisher randomization tests are the difference-in-means between the treatment and control groups and the covariate-adjusted version of the difference-in-means using analysis of covariance. Modern computing allows for fast computation of the Fisher-exact P -value, but confidence intervals have typically been obtained by inverting the Fisher randomization test over a range of possible effect sizes. The test inversion procedure is computationally expensive, limiting the usage of randomization-based inference in applied work. A recent paper by Zhu and Liu developed a closed form expression for the randomization-based confidence interval using the difference-in-means statistic. We develop an important extension of Zhu and Liu to obtain a closed form expression for the randomization-based covariate-adjusted confidence interval and give practitioners a sufficiency condition that can be checked using observed data and that guarantees that these confidence intervals have correct coverage. Simulations show that our procedure generates randomization-based covariate-adjusted confidence intervals that are robust to non-normality and that can be calculated in nearly the same time as it takes to calculate the Fisher-exact P -value, thus removing the computational barrier to performing randomization-based inference when adjusting for covariates. We also demonstrate our method on a re-analysis of phase I clinical trial data.},
  archive      = {J_BIOMTC},
  author       = {Fiksel, Jacob},
  doi          = {10.1093/biomtc/ujae051},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae051},
  shortjournal = {Biometrics},
  title        = {On exact randomization-based covariate-adjusted confidence intervals},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dissecting the colocalized GWAS and eQTLs with mediation
analysis for high-dimensional exposures and confounders.
<em>BIOMTC</em>, <em>80</em>(2), ujae050. (<a
href="https://doi.org/10.1093/biomtc/ujae050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To leverage the advancements in genome-wide association studies (GWAS) and quantitative trait loci (QTL) mapping for traits and molecular phenotypes to gain mechanistic understanding of the genetic regulation, biological researchers often investigate the expression QTLs (eQTLs) that colocalize with QTL or GWAS peaks. Our research is inspired by 2 such studies. One aims to identify the causal single nucleotide polymorphisms that are responsible for the phenotypic variation and whose effects can be explained by their impacts at the transcriptomic level in maize. The other study in mouse focuses on uncovering the cis-driver genes that induce phenotypic changes by regulating trans-regulated genes. Both studies can be formulated as mediation problems with potentially high-dimensional exposures, confounders, and mediators that seek to estimate the overall indirect effect (IE) for each exposure. In this paper, we propose MedDiC, a novel procedure to estimate the overall IE based on difference-in-coefficients approach. Our simulation studies find that MedDiC offers valid inference for the IE with higher power, shorter confidence intervals, and faster computing time than competing methods. We apply MedDiC to the 2 aforementioned motivating datasets and find that MedDiC yields reproducible outputs across the analysis of closely related traits, with results supported by external biological evidence. The code and additional information are available on our GitHub page ( https://github.com/QiZhangStat/MedDiC ).},
  archive      = {J_BIOMTC},
  author       = {Zhang, Qi and Yang, Zhikai and Yang, Jinliang},
  doi          = {10.1093/biomtc/ujae050},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae050},
  shortjournal = {Biometrics},
  title        = {Dissecting the colocalized GWAS and eQTLs with mediation analysis for high-dimensional exposures and confounders},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robustness of response-adaptive randomization.
<em>BIOMTC</em>, <em>80</em>(2), ujae049. (<a
href="https://doi.org/10.1093/biomtc/ujae049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Doubly adaptive biased coin design (DBCD), a response-adaptive randomization scheme, aims to skew subject assignment probabilities based on accrued responses for ethical considerations. Recent years have seen substantial advances in understanding DBCD’s theoretical properties, assuming correct model specification for the responses. However, concerns have been raised about the impact of model misspecification on its design and analysis. In this paper, we assess the robustness to both design model misspecification and analysis model misspecification under DBCD. On one hand, we confirm that the consistency and asymptotic normality of the allocation proportions can be preserved, even when the responses follow a distribution other than the one imposed by the design model during the implementation of DBCD. On the other hand, we extensively investigate three commonly used linear regression models for estimating and inferring the treatment effect, namely difference-in-means, analysis of covariance (ANCOVA) I, and ANCOVA II. By allowing these regression models to be arbitrarily misspecified, thereby not reflecting the true data generating process, we derive the consistency and asymptotic normality of the treatment effect estimators evaluated from the three models. The asymptotic properties show that the ANCOVA II model, which takes covariate-by-treatment interaction terms into account, yields the most efficient estimator. These results can provide theoretical support for using DBCD in scenarios involving model misspecification, thereby promoting the widespread application of this randomization procedure.},
  archive      = {J_BIOMTC},
  author       = {Ye, Xiaoqing and Hu, Feifang and Ma, Wei},
  doi          = {10.1093/biomtc/ujae049},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae049},
  shortjournal = {Biometrics},
  title        = {Robustness of response-adaptive randomization},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian semi-parametric model for learning biomarker
trajectories and changepoints in the preclinical phase of alzheimer’s
disease. <em>BIOMTC</em>, <em>80</em>(2), ujae048. (<a
href="https://doi.org/10.1093/biomtc/ujae048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has become consensus that mild cognitive impairment (MCI), one of the early symptoms onset of Alzheimer’s disease (AD), may appear 10 or more years after the emergence of neuropathological abnormalities. Therefore, understanding the progression of AD biomarkers and uncovering when brain alterations begin in the preclinical stage, while patients are still cognitively normal, are crucial for effective early detection and therapeutic development. In this paper, we develop a Bayesian semiparametric framework that jointly models the longitudinal trajectory of the AD biomarker with a changepoint relative to the occurrence of symptoms onset, which is subject to left truncation and right censoring, in a heterogeneous population. Furthermore, unlike most existing methods assuming that everyone in the considered population will eventually develop the disease, our approach accounts for the possibility that some individuals may never experience MCI or AD, even after a long follow-up time. We evaluate the proposed model through simulation studies and demonstrate its clinical utility by examining an important AD biomarker, ptau181, using a dataset from the Biomarkers of Cognitive Decline Among Normal Individuals (BIOCARD) study.},
  archive      = {J_BIOMTC},
  author       = {Wang, Kunbo and Hua, William and Wang, MeiCheng and Xu, Yanxun},
  doi          = {10.1093/biomtc/ujae048},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae048},
  shortjournal = {Biometrics},
  title        = {A bayesian semi-parametric model for learning biomarker trajectories and changepoints in the preclinical phase of alzheimer’s disease},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sequential covariate-adjusted randomization via
hierarchically minimizing mahalanobis distance and marginal imbalance.
<em>BIOMTC</em>, <em>80</em>(2), ujae047. (<a
href="https://doi.org/10.1093/biomtc/ujae047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In comparative studies, covariate balance and sequential allocation schemes have attracted growing academic interest. Although many theoretically justified adaptive randomization methods achieve the covariate balance, they often allocate patients in pairs or groups. To better meet the practical requirements where the clinicians cannot wait for other participants to assign the current patient for some economic or ethical reasons, we propose a method that randomizes patients individually and sequentially. The proposed method conceptually separates the covariate imbalance, measured by the newly proposed modified Mahalanobis distance, and the marginal imbalance, that is the sample size difference between the 2 groups, and it minimizes them with an explicit priority order. Compared with the existing sequential randomization methods, the proposed method achieves the best possible covariate balance while maintaining the marginal balance directly, offering us more control of the randomization process. We demonstrate the superior performance of the proposed method through a wide range of simulation studies and real data analysis, and also establish theoretical guarantees for the proposed method in terms of both the convergence of the imbalance measure and the subsequent treatment effect estimation.},
  archive      = {J_BIOMTC},
  author       = {Yang, Haoyu and Qin, Yichen and Li, Yang and Hu, Feifang},
  doi          = {10.1093/biomtc/ujae047},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae047},
  shortjournal = {Biometrics},
  title        = {Sequential covariate-adjusted randomization via hierarchically minimizing mahalanobis distance and marginal imbalance},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrating randomized and observational studies to estimate
optimal dynamic treatment regimes. <em>BIOMTC</em>, <em>80</em>(2),
ujae046. (<a href="https://doi.org/10.1093/biomtc/ujae046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential multiple assignment randomized trials (SMARTs) are the gold standard for estimating optimal dynamic treatment regimes (DTRs), but are costly and require a large sample size. We introduce the multi-stage augmented Q-learning estimator (MAQE) to improve efficiency of estimation of optimal DTRs by augmenting SMART data with observational data. Our motivating example comes from the Back Pain Consortium, where one of the overarching aims is to learn how to tailor treatments for chronic low back pain to individual patient phenotypes, knowledge which is lacking clinically. The Consortium-wide collaborative SMART and observational studies within the Consortium collect data on the same participant phenotypes, treatments, and outcomes at multiple time points, which can easily be integrated. Previously published single-stage augmentation methods for integration of trial and observational study (OS) data were adapted to estimate optimal DTRs from SMARTs using Q-learning. Simulation studies show the MAQE, which integrates phenotype, treatment, and outcome information from multiple studies over multiple time points, more accurately estimates the optimal DTR, and has a higher average value than a comparable Q-learning estimator without augmentation. We demonstrate this improvement is robust to a wide range of trial and OS sample sizes, addition of noise variables, and effect sizes.},
  archive      = {J_BIOMTC},
  author       = {Batorsky, Anna and Anstrom, Kevin J and Zeng, Donglin},
  doi          = {10.1093/biomtc/ujae046},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae046},
  shortjournal = {Biometrics},
  title        = {Integrating randomized and observational studies to estimate optimal dynamic treatment regimes},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Doubly robust estimation and sensitivity analysis for
marginal structural quantile models. <em>BIOMTC</em>, <em>80</em>(2),
ujae045. (<a href="https://doi.org/10.1093/biomtc/ujae045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The marginal structure quantile model (MSQM) provides a unique lens to understand the causal effect of a time-varying treatment on the full distribution of potential outcomes. Under the semiparametric framework, we derive the efficiency influence function for the MSQM, from which a new doubly robust estimator is proposed for point estimation and inference. We show that the doubly robust estimator is consistent if either of the models associated with treatment assignment or the potential outcome distributions is correctly specified, and is semiparametric efficient if both models are correct. To implement the doubly robust MSQM estimator, we propose to solve a smoothed estimating equation to facilitate efficient computation of the point and variance estimates. In addition, we develop a confounding function approach to investigate the sensitivity of several MSQM estimators when the sequential ignorability assumption is violated. Extensive simulations are conducted to examine the finite-sample performance characteristics of the proposed methods. We apply the proposed methods to the Yale New Haven Health System Electronic Health Record data to study the effect of antihypertensive medications to patients with severe hypertension and assess the robustness of the findings to unmeasured baseline and time-varying confounding.},
  archive      = {J_BIOMTC},
  author       = {Cheng, Chao and Hu, Liangyuan and Li, Fan},
  doi          = {10.1093/biomtc/ujae045},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae045},
  shortjournal = {Biometrics},
  title        = {Doubly robust estimation and sensitivity analysis for marginal structural quantile models},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discussion on “bayesian meta-analysis of penetrance for
cancer risk” by thanthirige lakshika m. Ruberu, danielle braun, giovanni
parmigiani, and swati biswas. <em>BIOMTC</em>, <em>80</em>(2), ujae044.
(<a href="https://doi.org/10.1093/biomtc/ujae044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Gustafson, Paul},
  doi          = {10.1093/biomtc/ujae044},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae044},
  shortjournal = {Biometrics},
  title        = {Discussion on “Bayesian meta-analysis of penetrance for cancer risk” by thanthirige lakshika m. ruberu, danielle braun, giovanni parmigiani, and swati biswas},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discussion on “bayesian meta-analysis of penetrance for
cancer risk” by thanthirige lakshika m. Ruberu, danielle braun, giovanni
parmigiani, and swati biswas. <em>BIOMTC</em>, <em>80</em>(2), ujae043.
(<a href="https://doi.org/10.1093/biomtc/ujae043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We congratulate the authors for the new meta-analysis model that accounts for different outcomes. We discuss the modeling choice and the Bayesian setting, specifically, we point out the connection between the Bayesian hierarchical model and a mixed-effect model formulation to subsequently discuss possible future method extensions.},
  archive      = {J_BIOMTC},
  author       = {Ursino, Moreno and Zohar, Sarah},
  doi          = {10.1093/biomtc/ujae043},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae043},
  shortjournal = {Biometrics},
  title        = {Discussion on “Bayesian meta-analysis of penetrance for cancer risk” by thanthirige lakshika m. ruberu, danielle braun, giovanni parmigiani, and swati biswas},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discussion on “bayesian meta-analysis of penetrance for
cancer risk” by thanthirige lakshika m. Ruberu, danielle braun, giovanni
parmigiani, and swati biswas. <em>BIOMTC</em>, <em>80</em>(2), ujae042.
(<a href="https://doi.org/10.1093/biomtc/ujae042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ruberu et al. (2023) introduce an elegant approach to fit a complicated meta-analysis problem with diverse reporting modalities into the framework of hierarchical Bayesian inference. We discuss issues related to some of the involved parametric model assumptions.},
  archive      = {J_BIOMTC},
  author       = {Müller, Peter and Flores, Bernardo},
  doi          = {10.1093/biomtc/ujae042},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae042},
  shortjournal = {Biometrics},
  title        = {Discussion on “Bayesian meta-analysis of penetrance for cancer risk” by thanthirige lakshika m. ruberu, danielle braun, giovanni parmigiani, and swati biswas},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discussion on “bayesian meta-analysis of penetrance for
cancer risk” by thanthirige lakshika m. Ruberu, danielle braun, giovanni
parmigiani, and swati biswas. <em>BIOMTC</em>, <em>80</em>(2), ujae041.
(<a href="https://doi.org/10.1093/biomtc/ujae041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Baio, Gianluca},
  doi          = {10.1093/biomtc/ujae041},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae041},
  shortjournal = {Biometrics},
  title        = {Discussion on “Bayesian meta-analysis of penetrance for cancer risk” by thanthirige lakshika m. ruberu, danielle braun, giovanni parmigiani, and swati biswas},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Rejoinder to the discussion on “bayesian meta-analysis of
penetrance for cancer risk.” <em>BIOMTC</em>, <em>80</em>(2), ujae040.
(<a href="https://doi.org/10.1093/biomtc/ujae040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The five discussions of our paper provide several modeling alternatives, extensions, and generalizations that can potentially guide future research in meta-analysis. In this rejoinder, we briefly summarize and comment on some of those points.},
  archive      = {J_BIOMTC},
  author       = {Ruberu, Thanthirige Lakshika M and Braun, Danielle and Parmigiani, Giovanni and Biswas, Swati},
  doi          = {10.1093/biomtc/ujae040},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae040},
  shortjournal = {Biometrics},
  title        = {Rejoinder to the discussion on “Bayesian meta-analysis of penetrance for cancer risk”},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discussion on “bayesian meta-analysis of penetrance for
cancer risk” by thanthirige lakshika m. Ruberu, danielle braun, giovanni
parmigiani, and swati biswas. <em>BIOMTC</em>, <em>80</em>(2), ujae039.
(<a href="https://doi.org/10.1093/biomtc/ujae039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Banerjee, Sudipto},
  doi          = {10.1093/biomtc/ujae039},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae039},
  shortjournal = {Biometrics},
  title        = {Discussion on “Bayesian meta-analysis of penetrance for cancer risk” by thanthirige lakshika m. ruberu, danielle braun, giovanni parmigiani, and swati biswas},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Bayesian meta-analysis of penetrance for cancer risk.
<em>BIOMTC</em>, <em>80</em>(2), ujae038. (<a
href="https://doi.org/10.1093/biomtc/ujae038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-gene panel testing allows many cancer susceptibility genes to be tested quickly at a lower cost making such testing accessible to a broader population. Thus, more patients carrying pathogenic germline mutations in various cancer-susceptibility genes are being identified. This creates a great opportunity, as well as an urgent need, to counsel these patients about appropriate risk-reducing management strategies. Counseling hinges on accurate estimates of age-specific risks of developing various cancers associated with mutations in a specific gene, ie, penetrance estimation. We propose a meta-analysis approach based on a Bayesian hierarchical random-effects model to obtain penetrance estimates by integrating studies reporting different types of risk measures (eg, penetrance, relative risk, odds ratio) while accounting for the associated uncertainties. After estimating posterior distributions of the parameters via a Markov chain Monte Carlo algorithm, we estimate penetrance and credible intervals. We investigate the proposed method and compare with an existing approach via simulations based on studies reporting risks for two moderate-risk breast cancer susceptibility genes, ATM and PALB2. Our proposed method is far superior in terms of coverage probability of credible intervals and mean square error of estimates. Finally, we apply our method to estimate the penetrance of breast cancer among carriers of pathogenic mutations in the ATM gene.},
  archive      = {J_BIOMTC},
  author       = {Ruberu, Thanthirige Lakshika M and Braun, Danielle and Parmigiani, Giovanni and Biswas, Swati},
  doi          = {10.1093/biomtc/ujae038},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae038},
  shortjournal = {Biometrics},
  title        = {Bayesian meta-analysis of penetrance for cancer risk},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regression models for average hazard. <em>BIOMTC</em>,
<em>80</em>(2), ujae037. (<a
href="https://doi.org/10.1093/biomtc/ujae037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Limitations of using the traditional Cox’s hazard ratio for summarizing the magnitude of the treatment effect on time-to-event outcomes have been widely discussed, and alternative measures that do not have such limitations are gaining attention. One of the alternative methods recently proposed, in a simple 2-sample comparison setting, uses the average hazard with survival weight (AH), which can be interpreted as the general censoring-free person-time incidence rate on a given time window. In this paper, we propose a new regression analysis approach for the AH with a truncation time τ. We investigate 3 versions of AH regression analysis, assuming (1) independent censoring, (2) group-specific censoring, and (3) covariate-dependent censoring. The proposed AH regression methods are closely related to robust Poisson regression. While the new approach needs to require a truncation time τ explicitly, it can be more robust than Poisson regression in the presence of censoring. With the AH regression approach, one can summarize the between-group treatment difference in both absolute difference and relative terms, adjusting for covariates that are associated with the outcome. This property will increase the likelihood that the treatment effect magnitude is correctly interpreted. The AH regression approach can be a useful alternative to the traditional Cox’s hazard ratio approach for estimating and reporting the magnitude of the treatment effect on time-to-event outcomes.},
  archive      = {J_BIOMTC},
  author       = {Uno, Hajime and Tian, Lu and Horiguchi, Miki and Hattori, Satoshi and Kehl, Kenneth L},
  doi          = {10.1093/biomtc/ujae037},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae037},
  shortjournal = {Biometrics},
  title        = {Regression models for average hazard},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing conditional quantile independence with functional
covariate. <em>BIOMTC</em>, <em>80</em>(2), ujae036. (<a
href="https://doi.org/10.1093/biomtc/ujae036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new non-parametric conditional independence test for a scalar response and a functional covariate over a continuum of quantile levels. We build a Cramer–von Mises type test statistic based on an empirical process indexed by random projections of the functional covariate, effectively avoiding the “curse of dimensionality” under the projected hypothesis, which is almost surely equivalent to the null hypothesis. The asymptotic null distribution of the proposed test statistic is obtained under some mild assumptions. The asymptotic global and local power properties of our test statistic are then investigated. We specifically demonstrate that the statistic is able to detect a broad class of local alternatives converging to the null at the parametric rate. Additionally, we recommend a simple multiplier bootstrap approach for estimating the critical values. The finite-sample performance of our statistic is examined through several Monte Carlo simulation experiments. Finally, an analysis of an EEG data set is used to show the utility and versatility of our proposed test statistic.},
  archive      = {J_BIOMTC},
  author       = {Feng, Yongzhen and Li, Jie and Song, Xiaojun},
  doi          = {10.1093/biomtc/ujae036},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae036},
  shortjournal = {Biometrics},
  title        = {Testing conditional quantile independence with functional covariate},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient data integration under prior probability shift.
<em>BIOMTC</em>, <em>80</em>(2), ujae035. (<a
href="https://doi.org/10.1093/biomtc/ujae035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional supervised learning usually operates under the premise that data are collected from the same underlying population. However, challenges may arise when integrating new data from different populations, resulting in a phenomenon known as dataset shift. This paper focuses on prior probability shift, where the distribution of the outcome varies across datasets but the conditional distribution of features given the outcome remains the same. To tackle the challenges posed by such shift, we propose an estimation algorithm that can efficiently combine information from multiple sources. Unlike existing methods that are restricted to discrete outcomes, the proposed approach accommodates both discrete and continuous outcomes. It also handles high-dimensional covariate vectors through variable selection using an adaptive least absolute shrinkage and selection operator penalty, producing efficient estimates that possess the oracle property. Moreover, a novel semiparametric likelihood ratio test is proposed to check the validity of prior probability shift assumptions by embedding the null conditional density function into Neyman’s smooth alternatives (Neyman, 1937 ) and testing study-specific parameters. We demonstrate the effectiveness of our proposed method through extensive simulations and a real data example. The proposed methods serve as a useful addition to the repertoire of tools for dealing dataset shifts.},
  archive      = {J_BIOMTC},
  author       = {Huang, Ming-Yueh and Qin, Jing and Huang, Chiung-Yu},
  doi          = {10.1093/biomtc/ujae035},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae035},
  shortjournal = {Biometrics},
  title        = {Efficient data integration under prior probability shift},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data science for infectious disease data analytics: An
introduction with r, by lily wang, CRC press, 2022 ISBN-13:
978-1032187426,
https://www.routledge.com/data-science-for-infectious-disease-data-analytics-an-introduction-with-r/wang/p/book/9781032187426.
<em>BIOMTC</em>, <em>80</em>(2), ujae034. (<a
href="https://doi.org/10.1093/biomtc/ujae034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Cheng, Gillian and Akhmetzhanov, Andrei R},
  doi          = {10.1093/biomtc/ujae034},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae034},
  shortjournal = {Biometrics},
  title        = {Data science for infectious disease data analytics: an introduction with r, by lily wang, CRC press, 2022 ISBN-13: 978-1032187426, https://www.routledge.com/Data-science-for-infectious-disease-data-analytics-an-introduction-with-R/Wang/p/book/9781032187426},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identifying temporal pathways using biomarkers in the
presence of latent non-gaussian components. <em>BIOMTC</em>,
<em>80</em>(2), ujae033. (<a
href="https://doi.org/10.1093/biomtc/ujae033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-series data collected from a network of random variables are useful for identifying temporal pathways among the network nodes. Observed measurements may contain multiple sources of signals and noises, including Gaussian signals of interest and non-Gaussian noises, including artifacts, structured noise, and other unobserved factors (eg, genetic risk factors, disease susceptibility). Existing methods, including vector autoregression (VAR) and dynamic causal modeling do not account for unobserved non-Gaussian components. Furthermore, existing methods cannot effectively distinguish contemporaneous relationships from temporal relations. In this work, we propose a novel method to identify latent temporal pathways using time-series biomarker data collected from multiple subjects. The model adjusts for the non-Gaussian components and separates the temporal network from the contemporaneous network. Specifically, an independent component analysis (ICA) is used to extract the unobserved non-Gaussian components, and residuals are used to estimate the contemporaneous and temporal networks among the node variables based on method of moments. The algorithm is fast and can easily scale up. We derive the identifiability and the asymptotic properties of the temporal and contemporaneous networks. We demonstrate superior performance of our method by extensive simulations and an application to a study of attention-deficit/hyperactivity disorder (ADHD), where we analyze the temporal relationships between brain regional biomarkers. We find that temporal network edges were across different brain regions, while most contemporaneous network edges were bilateral between the same regions and belong to a subset of the functional connectivity network.},
  archive      = {J_BIOMTC},
  author       = {Xie, Shanghong and Zeng, Donglin and Wang, Yuanjia},
  doi          = {10.1093/biomtc/ujae033},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae033},
  shortjournal = {Biometrics},
  title        = {Identifying temporal pathways using biomarkers in the presence of latent non-gaussian components},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Direct and indirect treatment effects in the presence of
semicompeting risks. <em>BIOMTC</em>, <em>80</em>(2), ujae032. (<a
href="https://doi.org/10.1093/biomtc/ujae032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semicompeting risks refer to the phenomenon that the terminal event (such as death) can censor the nonterminal event (such as disease progression) but not vice versa. The treatment effect on the terminal event can be delivered either directly following the treatment or indirectly through the nonterminal event. We consider 2 strategies to decompose the total effect into a direct effect and an indirect effect under the framework of mediation analysis in completely randomized experiments by adjusting the prevalence and hazard of nonterminal events, respectively. They require slightly different assumptions on cross-world quantities to achieve identifiability. We establish asymptotic properties for the estimated counterfactual cumulative incidences and decomposed treatment effects. We illustrate the subtle difference between these 2 decompositions through simulation studies and two real-data applications in the Supplementary Materials.},
  archive      = {J_BIOMTC},
  author       = {Deng, Yuhao and Wang, Yi and Zhou, Xiao-Hua},
  doi          = {10.1093/biomtc/ujae032},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae032},
  shortjournal = {Biometrics},
  title        = {Direct and indirect treatment effects in the presence of semicompeting risks},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-dimensional covariate-augmented overdispersed poisson
factor model. <em>BIOMTC</em>, <em>80</em>(2), ujae031. (<a
href="https://doi.org/10.1093/biomtc/ujae031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current Poisson factor models often assume that the factors are unknown, which overlooks the explanatory potential of certain observable covariates. This study focuses on high dimensional settings, where the number of the count response variables and/or covariates can diverge as the sample size increases. A covariate-augmented overdispersed Poisson factor model is proposed to jointly perform a high-dimensional Poisson factor analysis and estimate a large coefficient matrix for overdispersed count data. A group of identifiability conditions is provided to theoretically guarantee computational identifiability. We incorporate the interdependence of both response variables and covariates by imposing a low-rank constraint on the large coefficient matrix. To address the computation challenges posed by nonlinearity, two high-dimensional latent matrices, and the low-rank constraint, we propose a novel variational estimation scheme that combines Laplace and Taylor approximations. We also develop a criterion based on a singular value ratio to determine the number of factors and the rank of the coefficient matrix. Comprehensive simulation studies demonstrate that the proposed method outperforms the state-of-the-art methods in estimation accuracy and computational efficiency. The practical merit of our method is demonstrated by an application to the CITE-seq dataset. A flexible implementation of our proposed method is available in the R package COAP .},
  archive      = {J_BIOMTC},
  author       = {Liu, Wei and Zhong, Qingzhi},
  doi          = {10.1093/biomtc/ujae031},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae031},
  shortjournal = {Biometrics},
  title        = {High-dimensional covariate-augmented overdispersed poisson factor model},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Topical hidden genome: Discovering latent cancer mutational
topics using a bayesian multilevel context-learning approach.
<em>BIOMTC</em>, <em>80</em>(2), ujae030. (<a
href="https://doi.org/10.1093/biomtc/ujae030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring the cancer-type specificities of ultra-rare, genome-wide somatic mutations is an open problem. Traditional statistical methods cannot handle such data due to their ultra-high dimensionality and extreme data sparsity. To harness information in rare mutations, we have recently proposed a formal multilevel multilogistic “hidden genome” model. Through its hierarchical layers, the model condenses information in ultra-rare mutations through meta-features embodying mutation contexts to characterize cancer types. Consistent, scalable point estimation of the model can incorporate 10s of millions of variants across thousands of tumors and permit impressive prediction and attribution. However, principled statistical inference is infeasible due to the volume, correlation, and noninterpretability of mutation contexts. In this paper, we propose a novel framework that leverages topic models from computational linguistics to effectuate dimension reduction of mutation contexts producing interpretable, decorrelated meta-feature topics. We propose an efficient MCMC algorithm for implementation that permits rigorous full Bayesian inference at a scale that is orders of magnitude beyond the capability of existing out-of-the-box inferential high-dimensional multi-class regression methods and software. Applying our model to the Pan Cancer Analysis of Whole Genomes dataset reveals interesting biological insights including somatic mutational topics associated with UV exposure in skin cancer, aging in colorectal cancer, and strong influence of epigenome organization in liver cancer. Under cross-validation, our model demonstrates highly competitive predictive performance against blackbox methods of random forest and deep learning.},
  archive      = {J_BIOMTC},
  author       = {Chakraborty, Saptarshi and Guan, Zoe and Begg, Colin B and Shen, Ronglai},
  doi          = {10.1093/biomtc/ujae030},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae030},
  shortjournal = {Biometrics},
  title        = {Topical hidden genome: Discovering latent cancer mutational topics using a bayesian multilevel context-learning approach},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Addressing age measurement errors in fish growth estimation
from length-stratified samples. <em>BIOMTC</em>, <em>80</em>(2),
ujae029. (<a href="https://doi.org/10.1093/biomtc/ujae029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fish growth models are crucial for fisheries stock assessments and are commonly estimated using fish length-at-age data. This data is widely collected using length-stratified age sampling (LSAS), a cost-effective two-phase response-selective sampling method. The data may contain age measurement errors (MEs). We propose a methodology that accounts for both LSAS and age MEs to accurately estimate fish growth. The proposed methods use empirical proportion likelihood methodology for LSAS and the structural errors in variables methodology for age MEs. We provide a measure of uncertainty for parameter estimates and standardized residuals for model validation. To model the age distribution, we employ a continuation ratio-logit model that is consistent with the random nature of the true age distribution. We also apply a discretization approach for age and length distributions, which significantly improves computational efficiency and is consistent with the discrete age and length data typically encountered in practice. Our simulation study shows that neglecting age MEs can lead to significant bias in growth estimation, even with small but non-negligible age MEs. However, our new approach performs well regardless of the magnitude of age MEs and accurately estimates SEs of parameter estimators. Real data analysis demonstrates the effectiveness of the proposed model validation device. Computer codes to implement the methodology are provided.},
  archive      = {J_BIOMTC},
  author       = {Zheng, Nan and Kheirollahi, Atefeh and Yilmaz, Yildiz},
  doi          = {10.1093/biomtc/ujae029},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae029},
  shortjournal = {Biometrics},
  title        = {Addressing age measurement errors in fish growth estimation from length-stratified samples},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal inference for time-to-event data with a cured
subpopulation. <em>BIOMTC</em>, <em>80</em>(2), ujae028. (<a
href="https://doi.org/10.1093/biomtc/ujae028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When studying the treatment effect on time-to-event outcomes, it is common that some individuals never experience failure events, which suggests that they have been cured. However, the cure status may not be observed due to censoring which makes it challenging to define treatment effects. Current methods mainly focus on estimating model parameters in various cure models, ultimately leading to a lack of causal interpretations. To address this issue, we propose 2 causal estimands, the timewise risk difference and mean survival time difference, in the always-uncured based on principal stratification as a complement to the treatment effect on cure rates. These estimands allow us to study the treatment effects on failure times in the always-uncured subpopulation. We show the identifiability using a substitutional variable for the potential cure status under ignorable treatment assignment mechanism, these 2 estimands are identifiable. We also provide estimation methods using mixture cure models. We applied our approach to an observational study that compared the leukemia-free survival rates of different transplantation types to cure acute lymphoblastic leukemia. Our proposed approach yielded insightful results that can be used to inform future treatment decisions.},
  archive      = {J_BIOMTC},
  author       = {Wang, Yi and Deng, Yuhao and Zhou, Xiao-Hua},
  doi          = {10.1093/biomtc/ujae028},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae028},
  shortjournal = {Biometrics},
  title        = {Causal inference for time-to-event data with a cured subpopulation},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Single proxy control. <em>BIOMTC</em>, <em>80</em>(2),
ujae027. (<a href="https://doi.org/10.1093/biomtc/ujae027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Negative control variables are sometimes used in nonexperimental studies to detect the presence of confounding by hidden factors. A negative control outcome (NCO) is an outcome that is influenced by unobserved confounders of the exposure effects on the outcome in view, but is not causally impacted by the exposure. Tchetgen Tchetgen ( 2013 ) introduced the Control Outcome Calibration Approach (COCA) as a formal NCO counterfactual method to detect and correct for residual confounding bias. For identification, COCA treats the NCO as an error-prone proxy of the treatment-free counterfactual outcome of interest, and involves regressing the NCO on the treatment-free counterfactual, together with a rank-preserving structural model, which assumes a constant individual-level causal effect. In this work, we establish nonparametric COCA identification for the average causal effect for the treated, without requiring rank-preservation, therefore accommodating unrestricted effect heterogeneity across units. This nonparametric identification result has important practical implications, as it provides single-proxy confounding control, in contrast to recently proposed proximal causal inference, which relies for identification on a pair of confounding proxies. For COCA estimation we propose 3 separate strategies: (i) an extended propensity score approach, (ii) an outcome bridge function approach, and (iii) a doubly-robust approach. Finally, we illustrate the proposed methods in an application evaluating the causal impact of a Zika virus outbreak on birth rate in Brazil.},
  archive      = {J_BIOMTC},
  author       = {Park, Chan and Richardson, David B and Tchetgen Tchetgen, Eric J},
  doi          = {10.1093/biomtc/ujae027},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae027},
  shortjournal = {Biometrics},
  title        = {Single proxy control},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Well-spread samples with dynamic sample sizes.
<em>BIOMTC</em>, <em>80</em>(2), ujae026. (<a
href="https://doi.org/10.1093/biomtc/ujae026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A spatial sampling design determines where sample locations are placed in a study area so that population parameters can be estimated with relatively high precision. If the response variable has spatial trends, spatially balanced or well-spread designs give precise results for commonly used estimators. This article proposes a new method that draws well-spread samples over arbitrary auxiliary spaces and can be used for master sampling applications. All we require is a measure of the distance between population units. Numerical results show that the method generates well-spread samples and compares favorably with existing designs. We provide an example application using several auxiliary variables to estimate total aboveground biomass over a large study area in Eastern Amazonia, Brazil. Multipurpose surveys are also considered, where the totals of aboveground biomass, primary production, and clay content (3 responses) are estimated from a single well-spread sample over the auxiliary space.},
  archive      = {J_BIOMTC},
  author       = {Robertson, Blair and Price, Chris and Reale, Marco},
  doi          = {10.1093/biomtc/ujae026},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae026},
  shortjournal = {Biometrics},
  title        = {Well-spread samples with dynamic sample sizes},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Confounder-dependent bayesian mixture model: Characterizing
heterogeneity of causal effects in air pollution epidemiology.
<em>BIOMTC</em>, <em>80</em>(2), ujae025. (<a
href="https://doi.org/10.1093/biomtc/ujae025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several epidemiological studies have provided evidence that long-term exposure to fine particulate matter ( pm 2.5 ) increases mortality rate. Furthermore, some population characteristics (e.g., age, race, and socioeconomic status) might play a crucial role in understanding vulnerability to air pollution. To inform policy, it is necessary to identify groups of the population that are more or less vulnerable to air pollution. In causal inference literature, the group average treatment effect (GATE) is a distinctive facet of the conditional average treatment effect. This widely employed metric serves to characterize the heterogeneity of a treatment effect based on some population characteristics. In this paper, we introduce a novel Confounder-Dependent Bayesian Mixture Model (CDBMM) to characterize causal effect heterogeneity. More specifically, our method leverages the flexibility of the dependent Dirichlet process to model the distribution of the potential outcomes conditionally to the covariates and the treatment levels, thus enabling us to: (i) identify heterogeneous and mutually exclusive population groups defined by similar GATEs in a data-driven way, and (ii) estimate and characterize the causal effects within each of the identified groups. Through simulations, we demonstrate the effectiveness of our method in uncovering key insights about treatment effects heterogeneity. We apply our method to claims data from Medicare enrollees in Texas. We found six mutually exclusive groups where the causal effects of pm 2.5 on mortality rate are heterogeneous.},
  archive      = {J_BIOMTC},
  author       = {Zorzetto, Dafne and Bargagli-Stoffi, Falco J and Canale, Antonio and Dominici., Francesca},
  doi          = {10.1093/biomtc/ujae025},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae025},
  shortjournal = {Biometrics},
  title        = {Confounder-dependent bayesian mixture model: Characterizing heterogeneity of causal effects in air pollution epidemiology},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep partially linear cox model for current status data.
<em>BIOMTC</em>, <em>80</em>(2), ujae024. (<a
href="https://doi.org/10.1093/biomtc/ujae024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has continuously attained huge success in diverse fields, while its application to survival data analysis remains limited and deserves further exploration. For the analysis of current status data, a deep partially linear Cox model is proposed to circumvent the curse of dimensionality. Modeling flexibility is attained by using deep neural networks (DNNs) to accommodate nonlinear covariate effects and monotone splines to approximate the baseline cumulative hazard function. We establish the convergence rate of the proposed maximum likelihood estimators. Moreover, we derive that the finite-dimensional estimator for treatment covariate effects is |$\sqrt{n}$| -consistent, asymptotically normal, and attains semiparametric efficiency. Finally, we demonstrate the performance of our procedures through extensive simulation studies and application to real-world data on news popularity.},
  archive      = {J_BIOMTC},
  author       = {Wu, Qiang and Tong, Xingwei and Zhao, Xingqiu},
  doi          = {10.1093/biomtc/ujae024},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae024},
  shortjournal = {Biometrics},
  title        = {Deep partially linear cox model for current status data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Behavioral carry-over effect and power consideration in
crossover trials. <em>BIOMTC</em>, <em>80</em>(2), ujae023. (<a
href="https://doi.org/10.1093/biomtc/ujae023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A crossover trial is an efficient trial design when there is no carry-over effect. To reduce the impact of the biological carry-over effect, a washout period is often designed. However, the carry-over effect remains an outstanding concern when a washout period is unethical or cannot sufficiently diminish the impact of the carry-over effect. The latter can occur in comparative effectiveness research, where the carry-over effect is often non-biological but behavioral. In this paper, we investigate the crossover design under a potential outcomes framework with and without the carry-over effect. We find that when the carry-over effect exists and satisfies a sign condition, the basic estimator underestimates the treatment effect, which does not inflate the type I error of one-sided tests but negatively impacts the power. This leads to a power trade-off between the crossover design and the parallel-group design, and we derive the condition under which the crossover design does not lead to type I error inflation and is still more powerful than the parallel-group design. We also develop covariate adjustment methods for crossover trials. We evaluate the performance of cross-over design and covariate adjustment using data from the MTN-034/REACH study.},
  archive      = {J_BIOMTC},
  author       = {Shi, Danni and Ye, Ting},
  doi          = {10.1093/biomtc/ujae023},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae023},
  shortjournal = {Biometrics},
  title        = {Behavioral carry-over effect and power consideration in crossover trials},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flagging unusual clusters based on linear mixed models using
weighted and self-calibrated predictors. <em>BIOMTC</em>,
<em>80</em>(2), ujae022. (<a
href="https://doi.org/10.1093/biomtc/ujae022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical models incorporating cluster-specific intercepts are commonly used in hierarchical settings, for example, observations clustered within patients or patients clustered within hospitals. Predicted values of these intercepts are often used to identify or “flag” extreme or outlying clusters, such as poorly performing hospitals or patients with rapid declines in their health. We consider a variety of flagging rules, assessing different predictors, and using different accuracy measures. Using theoretical calculations and comprehensive numerical evaluation, we show that previously proposed rules based on the 2 most commonly used predictors, the usual best linear unbiased predictor and fixed effects predictor, perform extremely poorly: the incorrect flagging rates are either unacceptably high (approaching 0.5 in the limit) or overly conservative (eg, much &lt;0.05 for reasonable parameter values, leading to very low correct flagging rates). We develop novel methods for flagging extreme clusters that can control the incorrect flagging rates, including very simple-to-use versions that we call “self-calibrated.” The new methods have substantially higher correct flagging rates than previously proposed methods for flagging extreme values, while controlling the incorrect flagging rates. We illustrate their application using data on length of stay in pediatric hospitals for children admitted for asthma diagnoses.},
  archive      = {J_BIOMTC},
  author       = {McCulloch, Charles E and Neuhaus, John M and Boylan, Ross D},
  doi          = {10.1093/biomtc/ujae022},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae022},
  shortjournal = {Biometrics},
  title        = {Flagging unusual clusters based on linear mixed models using weighted and self-calibrated predictors},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-dimensional multisubject time series transition matrix
inference with application to brain connectivity analysis.
<em>BIOMTC</em>, <em>80</em>(2), ujae021. (<a
href="https://doi.org/10.1093/biomtc/ujae021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain-effective connectivity analysis quantifies directed influence of one neural element or region over another, and it is of great scientific interest to understand how effective connectivity pattern is affected by variations of subject conditions. Vector autoregression (VAR) is a useful tool for this type of problems. However, there is a paucity of solutions when there is measurement error, when there are multiple subjects, and when the focus is the inference of the transition matrix. In this article, we study the problem of transition matrix inference under the high-dimensional VAR model with measurement error and multiple subjects. We propose a simultaneous testing procedure, with three key components: a modified expectation-maximization (EM) algorithm, a test statistic based on the tensor regression of a bias-corrected estimator of the lagged auto-covariance given the covariates, and a properly thresholded simultaneous test. We establish the uniform consistency for the estimators of our modified EM, and show that the subsequent test achieves both a consistent false discovery control, and its power approaches one asymptotically. We demonstrate the efficacy of our method through both simulations and a brain connectivity study of task-evoked functional magnetic resonance imaging.},
  archive      = {J_BIOMTC},
  author       = {Lyu, Xiang and Kang, Jian and Li, Lexin},
  doi          = {10.1093/biomtc/ujae021},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae021},
  shortjournal = {Biometrics},
  title        = {High-dimensional multisubject time series transition matrix inference with application to brain connectivity analysis},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Case weighted power priors for hybrid control analyses with
time-to-event data. <em>BIOMTC</em>, <em>80</em>(2), ujae019. (<a
href="https://doi.org/10.1093/biomtc/ujae019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a method for hybrid analyses that uses external controls to augment internal control arms in randomized controlled trials (RCTs) where the degree of borrowing is determined based on similarity between RCT and external control patients to account for systematic differences (e.g., unmeasured confounders). The method represents a novel extension of the power prior where discounting weights are computed separately for each external control based on compatibility with the randomized control data. The discounting weights are determined using the predictive distribution for the external controls derived via the posterior distribution for time-to-event parameters estimated from the RCT. This method is applied using a proportional hazards regression model with piecewise constant baseline hazard. A simulation study and a real-data example are presented based on a completed trial in non-small cell lung cancer. It is shown that the case weighted power prior provides robust inference under various forms of incompatibility between the external controls and RCT population.},
  archive      = {J_BIOMTC},
  author       = {Kwiatkowski, Evan and Zhu, Jiawen and Li, Xiao and Pang, Herbert and Lieberman, Grazyna and Psioda, Matthew A},
  doi          = {10.1093/biomtc/ujae019},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae019},
  shortjournal = {Biometrics},
  title        = {Case weighted power priors for hybrid control analyses with time-to-event data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating the size of a closed population by modeling
latent and observed heterogeneity. <em>BIOMTC</em>, <em>80</em>(2),
ujae017. (<a href="https://doi.org/10.1093/biomtc/ujae017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper extends the empirical likelihood (EL) approach of Liu et al. to a new and very flexible family of latent class models for capture-recapture data also allowing for serial dependence on previous capture history, conditionally on latent type and covariates. The EL approach allows to estimate the overall population size directly rather than by adding estimates conditional to covariate configurations. A Fisher-scoring algorithm for maximum likelihood estimation is proposed and a more efficient alternative to the traditional EL approach for estimating the non-parametric component is introduced; this allows us to show that the mapping between the non-parametric distribution of the covariates and the probabilities of being never captured is one-to-one and strictly increasing. Asymptotic results are outlined, and a procedure for constructing profile likelihood confidence intervals for the population size is presented. Two examples based on real data are used to illustrate the proposed approach and a simulation study indicates that, when estimating the overall undercount, the method proposed here is substantially more efficient than the one based on conditional maximum likelihood estimation, especially when the sample size is not sufficiently large.},
  archive      = {J_BIOMTC},
  author       = {Bartolucci, Francesco and Forcina, Antonio},
  doi          = {10.1093/biomtc/ujae017},
  journal      = {Biometrics},
  month        = {3},
  number       = {2},
  pages        = {ujae017},
  shortjournal = {Biometrics},
  title        = {Estimating the size of a closed population by modeling latent and observed heterogeneity},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conditional modeling of panel count data with partly
interval-censored failure event. <em>BIOMTC</em>, <em>80</em>(1),
ujae020. (<a href="https://doi.org/10.1093/biomtc/ujae020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In longitudinal follow-up studies, panel count data arise from discrete observations on recurrent events. We investigate a more general situation where a partly interval-censored failure event is informative to recurrent events. The existing methods for the informative failure event are based on the latent variable model, which provides indirect interpretation for the effect of failure event. To solve this problem, we propose a failure-time-dependent proportional mean model with panel count data through an unspecified link function. For estimation of model parameters, we consider a conditional expectation of least squares function to overcome the challenges from partly interval-censoring, and develop a two-stage estimation procedure by treating the distribution function of the failure time as a functional nuisance parameter and using the B-spline functions to approximate unknown baseline mean and link functions. Furthermore, we derive the overall convergence rate of the proposed estimators and establish the asymptotic normality of finite-dimensional estimator and functionals of infinite-dimensional estimator. The proposed estimation procedure is evaluated by extensive simulation studies, in which the finite-sample performances coincide with the theoretical results. We further illustrate our method with a longitudinal healthy longevity study and draw some insightful conclusions.},
  archive      = {J_BIOMTC},
  author       = {Hu, Xiangbin and Su, Wen and Ye, Zhisheng and Zhao, Xingqiu},
  doi          = {10.1093/biomtc/ujae020},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujae020},
  shortjournal = {Biometrics},
  title        = {Conditional modeling of panel count data with partly interval-censored failure event},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fitting the cox proportional hazards model to big data.
<em>BIOMTC</em>, <em>80</em>(1), ujae018. (<a
href="https://doi.org/10.1093/biomtc/ujae018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The semiparametric Cox proportional hazards model, together with the partial likelihood principle, has been widely used to study the effects of potentially time-dependent covariates on a possibly censored event time. We propose a computationally efficient method for fitting the Cox model to big data involving millions of study subjects. Specifically, we perform maximum partial likelihood estimation on a small subset of the whole data and improve the initial estimator by incorporating the remaining data through one-step estimation with estimated efficient score functions. We show that the final estimator has the same asymptotic distribution as the conventional maximum partial likelihood estimator using the whole dataset but requires only a small fraction of computation time. We demonstrate the usefulness of the proposed method through extensive simulation studies and an application to the UK Biobank data.},
  archive      = {J_BIOMTC},
  author       = {Wang, Jianqiao and Zeng, Donglin and Lin, Dan-Yu},
  doi          = {10.1093/biomtc/ujae018},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujae018},
  shortjournal = {Biometrics},
  title        = {Fitting the cox proportional hazards model to big data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient computation of high-dimensional penalized
generalized linear mixed models by latent factor modeling of the random
effects. <em>BIOMTC</em>, <em>80</em>(1), ujae016. (<a
href="https://doi.org/10.1093/biomtc/ujae016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern biomedical datasets are increasingly high-dimensional and exhibit complex correlation structures. Generalized linear mixed models (GLMMs) have long been employed to account for such dependencies. However, proper specification of the fixed and random effects in GLMMs is increasingly difficult in high dimensions, and computational complexity grows with increasing dimension of the random effects. We present a novel reformulation of the GLMM using a factor model decomposition of the random effects, enabling scalable computation of GLMMs in high dimensions by reducing the latent space from a large number of random effects to a smaller set of latent factors. We also extend our prior work to estimate model parameters using a modified Monte Carlo Expectation Conditional Minimization algorithm, allowing us to perform variable selection on both the fixed and random effects simultaneously. We show through simulation that through this factor model decomposition, our method can fit high-dimensional penalized GLMMs faster than comparable methods and more easily scale to larger dimensions not previously seen in existing approaches.},
  archive      = {J_BIOMTC},
  author       = {Heiling, Hillary M and Rashid, Naim U and Li, Quefeng and Peng, Xianlu L and Yeh, Jen Jen and Ibrahim, Joseph G},
  doi          = {10.1093/biomtc/ujae016},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujae016},
  shortjournal = {Biometrics},
  title        = {Efficient computation of high-dimensional penalized generalized linear mixed models by latent factor modeling of the random effects},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Asymptotic uncertainty of false discovery proportion.
<em>BIOMTC</em>, <em>80</em>(1), ujae015. (<a
href="https://doi.org/10.1093/biomtc/ujae015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple testing has been a prominent topic in statistical research. Despite extensive work in this area, controlling false discoveries remains a challenging task, especially when the test statistics exhibit dependence. Various methods have been proposed to estimate the false discovery proportion (FDP) under arbitrary dependencies among the test statistics. One key approach is to transform arbitrary dependence into weak dependence and subsequently establish the strong consistency of FDP and false discovery rate under weak dependence. As a result, FDPs converge to the same asymptotic limit within the framework of weak dependence. However, we have observed that the asymptotic variance of FDP can be significantly influenced by the dependence structure of the test statistics, even when they exhibit only weak dependence. Quantifying this variability is of great practical importance, as it serves as an indicator of the quality of FDP estimation from the data. To the best of our knowledge, there is limited research on this aspect in the literature. In this paper, we aim to fill in this gap by quantifying the variation of FDP, assuming that the test statistics exhibit weak dependence and follow normal distributions. We begin by deriving the asymptotic expansion of the FDP and subsequently investigate how the asymptotic variance of the FDP is influenced by different dependence structures. Based on the insights gained from this study, we recommend that in multiple testing procedures utilizing FDP, reporting both the mean and variance estimates of FDP can provide a more comprehensive assessment of the study’s outcomes.},
  archive      = {J_BIOMTC},
  author       = {Mei, Meng and Yu, Tao and Jiang, Yuan},
  doi          = {10.1093/biomtc/ujae015},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujae015},
  shortjournal = {Biometrics},
  title        = {Asymptotic uncertainty of false discovery proportion},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bias correction models for electronic health records data in
the presence of non-random sampling. <em>BIOMTC</em>, <em>80</em>(1),
ujae014. (<a href="https://doi.org/10.1093/biomtc/ujae014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic health records (EHRs) contain rich clinical information for millions of patients and are increasingly used for public health research. However, non-random inclusion of subjects in EHRs can result in selection bias, with factors such as demographics, socioeconomic status, healthcare referral patterns, and underlying health status playing a role. While this issue has been well documented, little work has been done to develop or apply bias-correction methods, often due to the fact that most of these factors are unavailable in EHRs. To address this gap, we propose a series of Heckman type bias correction methods by incorporating social determinants of health selection covariates to model the EHR non-random sampling probability. Through simulations under various settings, we demonstrate the effectiveness of our proposed method in correcting biases in both the association coefficient and the outcome mean. Our method augments the utility of EHRs for public health inferences, as we show by estimating the prevalence of cardiovascular disease and its correlation with risk factors in the New York City network of EHRs.},
  archive      = {J_BIOMTC},
  author       = {Kim, Jiyu and Anthopolos, Rebecca and Zhong, Judy},
  doi          = {10.1093/biomtc/ujae014},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujae014},
  shortjournal = {Biometrics},
  title        = {Bias correction models for electronic health records data in the presence of non-random sampling},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Soft classification and regression analysis of audiometric
phenotypes of age-related hearing loss. <em>BIOMTC</em>, <em>80</em>(1),
ujae013. (<a href="https://doi.org/10.1093/biomtc/ujae013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Age-related hearing loss has a complex etiology. Researchers have made efforts to classify relevant audiometric phenotypes, aiming to enhance medical interventions and improve hearing health. We leveraged existing pattern analyses of age-related hearing loss and implemented the phenotype classification via quadratic discriminant analysis (QDA). We herein propose a method for analyzing the exposure effects on the soft classification probabilities of the phenotypes via estimating equations. Under reasonable assumptions, the estimating equations are unbiased and lead to consistent estimators. The resulting estimator had good finite sample performances in simulation studies. As an illustrative example, we applied our proposed methods to assess the association between a dietary intake pattern, assessed as adherence scores for the dietary approaches to stop hypertension diet calculated using validated food-frequency questionnaires, and audiometric phenotypes (older-normal, metabolic, sensory, and metabolic plus sensory), determined based on data obtained in the Nurses’ Health Study II Conservation of Hearing Study, the Audiology Assessment Arm. Our findings suggested that participants with a more healthful dietary pattern were less likely to develop the metabolic plus sensory phenotype of age-related hearing loss.},
  archive      = {J_BIOMTC},
  author       = {Yang, Ce and Langworthy, Benjamin and Curhan, Sharon and Vaden, Kenneth I and Curhan, Gary and Dubno, Judy R and Wang, Molin},
  doi          = {10.1093/biomtc/ujae013},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujae013},
  shortjournal = {Biometrics},
  title        = {Soft classification and regression analysis of audiometric phenotypes of age-related hearing loss},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accounting for network noise in graph-guided bayesian
modeling of structured high-dimensional data. <em>BIOMTC</em>,
<em>80</em>(1), ujae012. (<a
href="https://doi.org/10.1093/biomtc/ujae012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing body of literature on knowledge-guided statistical learning methods for analysis of structured high-dimensional data (such as genomic and transcriptomic data) that can incorporate knowledge of underlying networks derived from functional genomics and functional proteomics. These methods have been shown to improve variable selection and prediction accuracy and yield more interpretable results. However, these methods typically use graphs extracted from existing databases or rely on subject matter expertise, which are known to be incomplete and may contain false edges. To address this gap, we propose a graph-guided Bayesian modeling framework to account for network noise in regression models involving structured high-dimensional predictors. Specifically, we use 2 sources of network information, including the noisy graph extracted from existing databases and the estimated graph from observed predictors in the dataset at hand, to inform the model for the true underlying network via a latent scale modeling framework. This model is coupled with the Bayesian regression model with structured high-dimensional predictors involving an adaptive structured shrinkage prior. We develop an efficient Markov chain Monte Carlo algorithm for posterior sampling. We demonstrate the advantages of our method over existing methods in simulations, and through analyses of a genomics dataset and another proteomics dataset for Alzheimer’s disease.},
  archive      = {J_BIOMTC},
  author       = {Li, Wenrui and Chang, Changgee and Kundu, Suprateek and Long, Qi},
  doi          = {10.1093/biomtc/ujae012},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujae012},
  shortjournal = {Biometrics},
  title        = {Accounting for network noise in graph-guided bayesian modeling of structured high-dimensional data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian two-stage modeling of longitudinal and
time-to-event data with an integrated fractional brownian motion
covariance structure. <em>BIOMTC</em>, <em>80</em>(1), ujae011. (<a
href="https://doi.org/10.1093/biomtc/ujae011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is difficult to characterize complex variations of biological processes, often longitudinally measured using biomarkers that yield noisy data. While joint modeling with a longitudinal submodel for the biomarker measurements and a survival submodel for assessing the hazard of events can alleviate measurement error issues, the continuous longitudinal submodel often uses random intercepts and slopes to estimate both between- and within-patient heterogeneity in biomarker trajectories. To overcome longitudinal submodel challenges, we replace random slopes with scaled integrated fractional Brownian motion (IFBM). As a more generalized version of integrated Brownian motion, IFBM reasonably depicts noisily measured biological processes. From this longitudinal IFBM model, we derive novel target functions to monitor the risk of rapid disease progression as real-time predictive probabilities. Predicted biomarker values from the IFBM submodel are used as inputs in a Cox submodel to estimate event hazard. This two-stage approach to fit the submodels is performed via Bayesian posterior computation and inference. We use the proposed approach to predict dynamic lung disease progression and mortality in women with a rare disease called lymphangioleiomyomatosis who were followed in a national patient registry. We compare our approach to those using integrated Ornstein-Uhlenbeck or conventional random intercepts-and-slopes terms for the longitudinal submodel. In the comparative analysis, the IFBM model consistently demonstrated superior predictive performance.},
  archive      = {J_BIOMTC},
  author       = {Palipana, Anushka and Song, Seongho and Gupta, Nishant and Szczesniak, Rhonda},
  doi          = {10.1093/biomtc/ujae011},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujae011},
  shortjournal = {Biometrics},
  title        = {Bayesian two-stage modeling of longitudinal and time-to-event data with an integrated fractional brownian motion covariance structure},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A boosting method to select the random effects in linear
mixed models. <em>BIOMTC</em>, <em>80</em>(1), ujae010. (<a
href="https://doi.org/10.1093/biomtc/ujae010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel likelihood-based boosting method for the selection of the random effects in linear mixed models. The nonconvexity of the objective function to minimize, which is the negative profile log-likelihood, requires the adoption of new solutions. In this respect, our optimization approach also employs the directions of negative curvature besides the usual Newton directions. A simulation study and a real-data application show the good performance of the proposal.},
  archive      = {J_BIOMTC},
  author       = {Battauz, Michela and Vidoni, Paolo},
  doi          = {10.1093/biomtc/ujae010},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujae010},
  shortjournal = {Biometrics},
  title        = {A boosting method to select the random effects in linear mixed models},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparing two spatial variables with the probability of
agreement. <em>BIOMTC</em>, <em>80</em>(1), ujae009. (<a
href="https://doi.org/10.1093/biomtc/ujae009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing the agreement between 2 continuous sequences is of great interest in statistics when comparing 2 instruments or one instrument with a gold standard. The probability of agreement quantifies the similarity between 2 variables of interest, and it is useful for determining what constitutes a practically important difference. In this article, we introduce a generalization of the PA for the treatment of spatial variables. Our proposal makes the PA dependent on the spatial lag. We establish the conditions for which the PA decays as a function of the distance lag for isotropic stationary and nonstationary spatial processes. Estimation is addressed through a first-order approximation that guarantees the asymptotic normality of the sample version of the PA. The sensitivity of the PA with respect to the covariance parameters is studied for finite sample size. The new method is described and illustrated with real data involving autumnal changes in the green chromatic coordinate ( G cc ), an index of “greenness” that captures the phenological stage of tree leaves, is associated with carbon flux from ecosystems, and is estimated from repeated images of forest canopies.},
  archive      = {J_BIOMTC},
  author       = {Acosta, Jonathan and Vallejos, Ronny and Ellison, Aaron M and Osorio, Felipe and de Castro, Mário},
  doi          = {10.1093/biomtc/ujae009},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujae009},
  shortjournal = {Biometrics},
  title        = {Comparing two spatial variables with the probability of agreement},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A scalar-on-quantile-function approach for estimating
short-term health effects of environmental exposures. <em>BIOMTC</em>,
<em>80</em>(1), ujae008. (<a
href="https://doi.org/10.1093/biomtc/ujae008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Environmental epidemiologic studies routinely utilize aggregate health outcomes to estimate effects of short-term (eg, daily) exposures that are available at increasingly fine spatial resolutions. However, areal averages are typically used to derive population-level exposure, which cannot capture the spatial variation and individual heterogeneity in exposures that may occur within the spatial and temporal unit of interest (eg, within a day or ZIP code). We propose a general modeling approach to incorporate within-unit exposure heterogeneity in health analyses via exposure quantile functions. Furthermore, by viewing the exposure quantile function as a functional covariate, our approach provides additional flexibility in characterizing associations at different quantile levels. We apply the proposed approach to an analysis of air pollution and emergency department (ED) visits in Atlanta over 4 years. The analysis utilizes daily ZIP code-level distributions of personal exposures to 4 traffic-related ambient air pollutants simulated from the Stochastic Human Exposure and Dose Simulator. Our analyses find that effects of carbon monoxide on respiratory and cardiovascular disease ED visits are more pronounced with changes in lower quantiles of the population’s exposure. Software for implement is provided in the R package nbRegQF .},
  archive      = {J_BIOMTC},
  author       = {Zhang, Yuzi and Chang, Howard H and Warren, Joshua L and Ebelt, Stefanie T},
  doi          = {10.1093/biomtc/ujae008},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujae008},
  shortjournal = {Biometrics},
  title        = {A scalar-on-quantile-function approach for estimating short-term health effects of environmental exposures},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diagnostics for regression models with semicontinuous
outcomes. <em>BIOMTC</em>, <em>80</em>(1), ujae007. (<a
href="https://doi.org/10.1093/biomtc/ujae007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semicontinuous outcomes commonly arise in a wide variety of fields, such as insurance claims, healthcare expenditures, rainfall amounts, and alcohol consumption. Regression models, including Tobit, Tweedie, and two-part models, are widely employed to understand the relationship between semicontinuous outcomes and covariates. Given the potential detrimental consequences of model misspecification, after fitting a regression model, it is of prime importance to check the adequacy of the model. However, due to the point mass at zero, standard diagnostic tools for regression models (eg, deviance and Pearson residuals) are not informative for semicontinuous data. To bridge this gap, we propose a new type of residuals for semicontinuous outcomes that is applicable to general regression models. Under the correctly specified model, the proposed residuals converge to being uniformly distributed, and when the model is misspecified, they significantly depart from this pattern. In addition to in-sample validation, the proposed methodology can also be employed to evaluate predictive distributions. We demonstrate the effectiveness of the proposed tool using health expenditure data from the US Medical Expenditure Panel Survey.},
  archive      = {J_BIOMTC},
  author       = {Yang, Lu},
  doi          = {10.1093/biomtc/ujae007},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujae007},
  shortjournal = {Biometrics},
  title        = {Diagnostics for regression models with semicontinuous outcomes},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Changing interim monitoring in response to internal clinical
trial data. <em>BIOMTC</em>, <em>80</em>(1), ujae006. (<a
href="https://doi.org/10.1093/biomtc/ujae006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing clinical trials for emerging infectious diseases such as COVID-19 is challenging because information needed for proper planning may be lacking. Pre-specified adaptive designs can be attractive options, but what happens if a trial with no such design needs to be modified? For example, unexpectedly high efficacy (approximately 95%) in two COVID-19 vaccine trials might cause investigators in other COVID-19 vaccine trials to increase the number of interim analyses to allow earlier stopping for efficacy. If such a decision is based solely on external data, there are no issues, but what if internal trial data by arm are also examined? Fortunately, the conditional error principle of Müller and Schäfer ( 2004 ) can be used to ensure no inflation of the type 1 error rate, even if no interim analyses were planned. We study the properties, including limitations, of this method. We provide a shiny app to evaluate changes in timing of interim analyses in response to outcome data by arm in clinical trials.},
  archive      = {J_BIOMTC},
  author       = {Proschan, Michael A and Nason, Martha and Ortega-Villa, Ana M and Wang, Jing},
  doi          = {10.1093/biomtc/ujae006},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujae006},
  shortjournal = {Biometrics},
  title        = {Changing interim monitoring in response to internal clinical trial data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation of the causal effects of time-varying treatments
in nested case-control studies using marginal structural cox models.
<em>BIOMTC</em>, <em>80</em>(1), ujae005. (<a
href="https://doi.org/10.1093/biomtc/ujae005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When estimating the causal effects of time-varying treatments on survival in nested case-control (NCC) studies, marginal structural Cox models (Cox-MSMs) with inverse probability weights (IPWs) are a natural approach. However, calculating IPWs from the cases and controls is difficult because they are not random samples from the full cohort, and the number of subjects may be insufficient for calculation. To overcome these difficulties, we propose a method for calculating IPWs to fit Cox-MSMs to NCC sampling data. We estimate the IPWs using a pseudo-likelihood estimation method with an inverse probability of sampling weight using NCC samples, and additional samples of subjects who experience treatment changes and subjects whose follow-up is censored are required to calculate the weights. Our method only requires covariate histories for the samples. The confidence intervals are calculated from the robust variance estimator for the NCC sampling data. We also derive the asymptotic properties of the estimator of Cox-MSM under NCC sampling. The proposed methods will allow researchers to apply several case-control matching methods to improve statistical efficiency. A simulation study was conducted to evaluate the finite sample performance of the proposed method. We also applied our method to a motivating pharmacoepidemiological study examining the effect of statins on the incidence of coronary heart disease. The proposed method may be useful for estimating the causal effects of time-varying treatments in NCC studies.},
  archive      = {J_BIOMTC},
  author       = {Takeuchi, Yoshinori and Hagiwawa, Yasuhiro and Komukai, Sho and Matsuyama, Yutaka},
  doi          = {10.1093/biomtc/ujae005},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujae005},
  shortjournal = {Biometrics},
  title        = {Estimation of the causal effects of time-varying treatments in nested case-control studies using marginal structural cox models},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Report of the editors—2023. <em>BIOMTC</em>, <em>80</em>(1),
ujae004. (<a href="https://doi.org/10.1093/biomtc/ujae004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  doi          = {10.1093/biomtc/ujae004},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujae004},
  shortjournal = {Biometrics},
  title        = {Report of the editors—2023},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Merging or ensembling: Integrative analysis in multiple
neuroimaging studies. <em>BIOMTC</em>, <em>80</em>(1), ujae003. (<a
href="https://doi.org/10.1093/biomtc/ujae003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this paper is to systematically investigate merging and ensembling methods for spatially varying coefficient mixed effects models (SVCMEM) in order to carry out integrative learning of neuroimaging data obtained from multiple biomedical studies. The ”merged” approach involves training a single learning model using a comprehensive dataset that encompasses information from all the studies. Conversely, the ”ensemble” approach involves creating a weighted average of distinct learning models, each developed from an individual study. We systematically investigate the prediction accuracy of the merged and ensemble learners under the presence of different degrees of interstudy heterogeneity. Additionally, we establish asymptotic guidelines for making strategic decisions about when to employ either of these models in different scenarios, along with deriving optimal weights for the ensemble learner. To validate our theoretical results, we perform extensive simulation studies. The proposed methodology is also applied to 3 large-scale neuroimaging studies.},
  archive      = {J_BIOMTC},
  author       = {Shan, Yue and Huang, Chao and Li, Yun and Zhu, Hongtu},
  doi          = {10.1093/biomtc/ujae003},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujae003},
  shortjournal = {Biometrics},
  title        = {Merging or ensembling: Integrative analysis in multiple neuroimaging studies},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semisupervised transfer learning for evaluation of model
classification performance. <em>BIOMTC</em>, <em>80</em>(1), ujae002.
(<a href="https://doi.org/10.1093/biomtc/ujae002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many modern machine learning applications, changes in covariate distributions and difficulty in acquiring outcome information have posed challenges to robust model training and evaluation. Numerous transfer learning methods have been developed to robustly adapt the model itself to some unlabeled target populations using existing labeled data in a source population. However, there is a paucity of literature on transferring performance metrics, especially receiver operating characteristic (ROC) parameters, of a trained model. In this paper, we aim to evaluate the performance of a trained binary classifier on unlabeled target population based on ROC analysis. We proposed S emisupervised T ransfer l E arning of A ccuracy M easures (STEAM), an efficient three-step estimation procedure that employs (1) double-index modeling to construct calibrated density ratio weights and (2) robust imputation to leverage the large amount of unlabeled data to improve estimation efficiency. We establish the consistency and asymptotic normality of the proposed estimator under the correct specification of either the density ratio model or the outcome model. We also correct for potential overfitting bias in the estimators in finite samples with cross-validation. We compare our proposed estimators to existing methods and show reductions in bias and gains in efficiency through simulations. We illustrate the practical utility of the proposed method on evaluating prediction performance of a phenotyping model for rheumatoid arthritis (RA) on a temporally evolving EHR cohort.},
  archive      = {J_BIOMTC},
  author       = {Wang, Linshanshan and Wang, Xuan and Liao, Katherine P and Cai, Tianxi},
  doi          = {10.1093/biomtc/ujae002},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujae002},
  shortjournal = {Biometrics},
  title        = {Semisupervised transfer learning for evaluation of model classification performance},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From local to global gene co-expression estimation using
single-cell RNA-seq data. <em>BIOMTC</em>, <em>80</em>(1), ujae001. (<a
href="https://doi.org/10.1093/biomtc/ujae001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In genomics studies, the investigation of gene relationships often brings important biological insights. Currently, the large heterogeneous datasets impose new challenges for statisticians because gene relationships are often local. They change from one sample point to another, may only exist in a subset of the sample, and can be nonlinear or even nonmonotone. Most previous dependence measures do not specifically target local dependence relationships, and the ones that do are computationally costly. In this paper, we explore a state-of-the-art network estimation technique that characterizes gene relationships at the single cell level, under the name of cell-specific gene networks . We first show that averaging the cell-specific gene relationship over a population gives a novel univariate dependence measure, the averaged Local Density Gap (aLDG), that accumulates local dependence and can detect any nonlinear, nonmonotone relationship. Together with a consistent nonparametric estimator, we establish its robustness on both the population and empirical levels. Then, we show that averaging the cell-specific gene relationship over mini-batches determined by some external structure information (eg, spatial or temporal factor) better highlights meaningful local structure change points. We explore the application of aLDG and its minibatch variant in many scenarios, including pairwise gene relationship estimation, bifurcating point detection in cell trajectory, and spatial transcriptomics structure visualization. Both simulations and real data analysis show that aLDG outperforms existing ones.},
  archive      = {J_BIOMTC},
  author       = {Tian, Jinjin and Lei, Jing and Roeder, Kathryn},
  doi          = {10.1093/biomtc/ujae001},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujae001},
  shortjournal = {Biometrics},
  title        = {From local to global gene co-expression estimation using single-cell RNA-seq data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-dimensional sparse vine copula regression with
application to genomic prediction. <em>BIOMTC</em>, <em>80</em>(1),
ujad042. (<a href="https://doi.org/10.1093/biomtc/ujad042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional data sets are often available in genome-enabled predictions. Such data sets include nonlinear relationships with complex dependence structures. For such situations, vine copula-based (quantile) regression is an important tool. However, the current vine copula-based regression approaches do not scale up to high and ultra-high dimensions. To perform high-dimensional sparse vine copula-based regression, we propose 2 methods. First, we show their superiority regarding computational complexity over the existing methods. Second, we define relevant, irrelevant, and redundant explanatory variables for quantile regression. Then, we show our method’s power in selecting relevant variables and prediction accuracy in high-dimensional sparse data sets via simulation studies. Next, we apply the proposed methods to the high-dimensional real data, aiming at the genomic prediction of maize traits. Some data processing and feature extraction steps for the real data are further discussed. Finally, we show the advantage of our methods over linear models and quantile regression forests in simulation studies and real data applications.},
  archive      = {J_BIOMTC},
  author       = {Sahin, Özge and Czado, Claudia},
  doi          = {10.1093/biomtc/ujad042},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad042},
  shortjournal = {Biometrics},
  title        = {High-dimensional sparse vine copula regression with application to genomic prediction},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simultaneous variable selection and estimation in
semiparametric regression of mixed panel count data. <em>BIOMTC</em>,
<em>80</em>(1), ujad041. (<a
href="https://doi.org/10.1093/biomtc/ujad041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed panel count data represent a common complex data structure in longitudinal survey studies. A major challenge in analyzing such data is variable selection and estimation while efficiently incorporating both the panel count and panel binary data components. Analyses in the medical literature have often ignored the panel binary component and treated it as missing with the unknown panel counts, while obviously such a simplification does not effectively utilize the original data information. In this research, we put forward a penalized likelihood variable selection and estimation procedure under the proportional mean model. A computationally efficient EM algorithm is developed that ensures sparse estimation for variable selection, and the resulting estimator is shown to have the desirable oracle property. Simulation studies assessed and confirmed the good finite-sample properties of the proposed method, and the method is applied to analyze a motivating dataset from the Health and Retirement Study.},
  archive      = {J_BIOMTC},
  author       = {Ge, Lei and Hu, Tao and Li, Yang},
  doi          = {10.1093/biomtc/ujad041},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad041},
  shortjournal = {Biometrics},
  title        = {Simultaneous variable selection and estimation in semiparametric regression of mixed panel count data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse ordinal discriminant analysis. <em>BIOMTC</em>,
<em>80</em>(1), ujad040. (<a
href="https://doi.org/10.1093/biomtc/ujad040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ordinal class labels are frequently observed in classification studies across various fields. In medical science, patients’ responses to a drug can be arranged in the natural order, reflecting their recovery postdrug administration. The severity of the disease is often recorded using an ordinal scale, such as cancer grades or tumor stages. We propose a method based on the linear discriminant analysis (LDA) that generates a sparse, low-dimensional discriminant subspace reflecting the class orders. Unlike existing approaches that focus on predictors marginally associated with ordinal labels, our proposed method selects variables that collectively contribute to the ordinal labels. We employ the optimal scoring approach for LDA as a regularization framework, applying an ordinality penalty to the optimal scores and a sparsity penalty to the coefficients for the predictors. We demonstrate the effectiveness of our approach using a glioma dataset, where we predict cancer grades based on gene expression. A simulation study with various settings validates the competitiveness of our classification performance and demonstrates the advantages of our approach in terms of the interpretability of the estimated classifier with respect to the ordinal class labels.},
  archive      = {J_BIOMTC},
  author       = {Han, Sangil and Kim, Minwoo and Jung, Sungkyu and Ahn, Jeongyoun},
  doi          = {10.1093/biomtc/ujad040},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad040},
  shortjournal = {Biometrics},
  title        = {Sparse ordinal discriminant analysis},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inferring a directed acyclic graph of phenotypes from GWAS
summary statistics. <em>BIOMTC</em>, <em>80</em>(1), ujad039. (<a
href="https://doi.org/10.1093/biomtc/ujad039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating phenotype networks is a growing field in computational biology. It deepens the understanding of disease etiology and is useful in many applications. In this study, we present a method that constructs a phenotype network by assuming a Gaussian linear structure model embedding a directed acyclic graph (DAG). We utilize genetic variants as instrumental variables and show how our method only requires access to summary statistics from a genome-wide association study (GWAS) and a reference panel of genotype data. Besides estimation, a distinct feature of the method is its summary statistics-based likelihood ratio test on directed edges. We applied our method to estimate a causal network of 29 cardiovascular-related proteins and linked the estimated network to Alzheimer’s disease (AD). A simulation study was conducted to demonstrate the effectiveness of this method. An R package sumdag implementing the proposed method, all relevant code, and a Shiny application are available.},
  archive      = {J_BIOMTC},
  author       = {Zilinskas, Rachel and Li, Chunlin and Shen, Xiaotong and Pan, Wei and Yang, Tianzhong},
  doi          = {10.1093/biomtc/ujad039},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad039},
  shortjournal = {Biometrics},
  title        = {Inferring a directed acyclic graph of phenotypes from GWAS summary statistics},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-phase designs with failure time processes subject to
nonsusceptibility. <em>BIOMTC</em>, <em>80</em>(1), ujad038. (<a
href="https://doi.org/10.1093/biomtc/ujad038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epidemiological studies based on 2-phase designs help ensure efficient use of limited resources in situations where certain covariates are prohibitively expensive to measure for a full cohort. Typically, these designs involve 2 steps: In phase I, data on an outcome and inexpensive covariates are acquired, and in phase II, a subsample is chosen in which the costly variable of interest is measured. For right-censored data, 2-phase designs have been primarily based on the Cox model. We develop efficient 2-phase design strategies for settings involving a fraction of long-term survivors due to nonsusceptibility. Using mixture models accommodating a nonsusceptible fraction, we consider 3 regression frameworks, including (a) a logistic “cure” model, (b) a proportional hazards model for those who are susceptible, and (c) regression models for susceptibility and failure time in those susceptible. Importantly, we introduce a novel class of bivariate residual-dependent designs to address the unique challenges presented in scenario (c), which involves 2 parameters of interest. Extensive simulation studies demonstrate the superiority of our approach over various phase II subsampling schemes. We illustrate the method through applications to the Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial.},
  archive      = {J_BIOMTC},
  author       = {Mao, Fangya and Cheung, Li C and Cook, Richard J},
  doi          = {10.1093/biomtc/ujad038},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad038},
  shortjournal = {Biometrics},
  title        = {Two-phase designs with failure time processes subject to nonsusceptibility},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using instrumental variables to address unmeasured
confounding in causal mediation analysis. <em>BIOMTC</em>,
<em>80</em>(1), ujad037. (<a
href="https://doi.org/10.1093/biomtc/ujad037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis is a strategy for understanding the mechanisms by which interventions affect later outcomes. However, unobserved confounding concerns may be compounded in mediation analyses, as there may be unobserved exposure-outcome, exposure-mediator, and mediator-outcome confounders. Instrumental variables (IVs) are a popular identification strategy in the presence of unobserved confounding. However, in contrast to the rich literature on the use of IV methods to identify and estimate a total effect of a non-randomized exposure, there has been almost no research into using IV as an identification strategy to identify mediational indirect effects. In response, we define and nonparametrically identify novel estimands—double complier interventional direct and indirect effects—when 2, possibly related, IVs are available, one for the exposure and another for the mediator. We propose nonparametric, robust, efficient estimators for these effects and apply them to a housing voucher experiment.},
  archive      = {J_BIOMTC},
  author       = {Rudolph, Kara E and Williams, Nicholas and Díaz, Iván},
  doi          = {10.1093/biomtc/ujad037},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad037},
  shortjournal = {Biometrics},
  title        = {Using instrumental variables to address unmeasured confounding in causal mediation analysis},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiply robust estimators in longitudinal studies with
missing data under control-based imputation. <em>BIOMTC</em>,
<em>80</em>(1), ujad036. (<a
href="https://doi.org/10.1093/biomtc/ujad036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal studies are often subject to missing data. The recent guidance from regulatory agencies, such as the ICH E9(R1) addendum addresses the importance of defining a treatment effect estimand with the consideration of intercurrent events. Jump-to-reference (J2R) is one classical control-based scenario for the treatment effect evaluation, where the participants in the treatment group after intercurrent events are assumed to have the same disease progress as those with identical covariates in the control group. We establish new estimators to assess the average treatment effect based on a proposed potential outcomes framework under J2R. Various identification formulas are constructed, motivating estimators that rely on different parts of the observed data distribution. Moreover, we obtain a novel estimator inspired by the efficient influence function, with multiple robustness in the sense that it achieves n 1/2 -consistency if any pairs of multiple nuisance functions are correctly specified, or if the nuisance functions converge at a rate not slower than n −1/4 when using flexible modeling approaches. The finite-sample performance of the proposed estimators is validated in simulation studies and an antidepressant clinical trial.},
  archive      = {J_BIOMTC},
  author       = {Liu, Siyi and Yang, Shu and Zhang, Yilong and Liu, Guanghan (Frank)},
  doi          = {10.1093/biomtc/ujad036},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad036},
  shortjournal = {Biometrics},
  title        = {Multiply robust estimators in longitudinal studies with missing data under control-based imputation},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A rank-based approach to evaluate a surrogate marker in a
small sample setting. <em>BIOMTC</em>, <em>80</em>(1), ujad035. (<a
href="https://doi.org/10.1093/biomtc/ujad035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical studies of chronic diseases, the effectiveness of an intervention is often assessed using “high cost” outcomes that require long-term patient follow-up and/or are invasive to obtain. While much progress has been made in the development of statistical methods to identify surrogate markers, that is, measurements that could replace such costly outcomes, they are generally not applicable to studies with a small sample size. These methods either rely on nonparametric smoothing which requires a relatively large sample size or rely on strict model assumptions that are unlikely to hold in practice and empirically difficult to verify with a small sample size. In this paper, we develop a novel rank-based nonparametric approach to evaluate a surrogate marker in a small sample size setting. The method developed in this paper is motivated by a small study of children with nonalcoholic fatty liver disease (NAFLD), a diagnosis for a range of liver conditions in individuals without significant history of alcohol intake. Specifically, we examine whether change in alanine aminotransferase (ALT; measured in blood) is a surrogate marker for change in NAFLD activity score (obtained by biopsy) in a trial, which compared Vitamin E ( ⁠|$n=50$|⁠ ) versus placebo ( ⁠|$n=46$|⁠ ) among children with NAFLD.},
  archive      = {J_BIOMTC},
  author       = {Parast, Layla and Cai, Tianxi and Tian, Lu},
  doi          = {10.1093/biomtc/ujad035},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad035},
  shortjournal = {Biometrics},
  title        = {A rank-based approach to evaluate a surrogate marker in a small sample setting},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive selection of the optimal strategy to improve
precision and power in randomized trials. <em>BIOMTC</em>,
<em>80</em>(1), ujad034. (<a
href="https://doi.org/10.1093/biomtc/ujad034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benkeser et al. demonstrate how adjustment for baseline covariates in randomized trials can meaningfully improve precision for a variety of outcome types. Their findings build on a long history, starting in 1932 with R.A. Fisher and including more recent endorsements by the U.S. Food and Drug Administration and the European Medicines Agency. Here, we address an important practical consideration: how to select the adjustment approach—which variables and in which form—to maximize precision, while maintaining Type-I error control. Balzer et al. previously proposed Adaptive Pre-specification within TMLE to flexibly and automatically select, from a prespecified set, the approach that maximizes empirical efficiency in small trials ( N &lt; 40). To avoid overfitting with few randomized units, selection was previously limited to working generalized linear models, adjusting for a single covariate. Now, we tailor Adaptive Pre-specification to trials with many randomized units. Using V -fold cross-validation and the estimated influence curve-squared as the loss function, we select from an expanded set of candidates, including modern machine learning methods adjusting for multiple covariates. As assessed in simulations exploring a variety of data-generating processes, our approach maintains Type-I error control (under the null) and offers substantial gains in precision—equivalent to 20%-43% reductions in sample size for the same statistical power. When applied to real data from ACTG Study 175, we also see meaningful efficiency improvements overall and within subgroups.},
  archive      = {J_BIOMTC},
  author       = {Balzer, Laura B and Cai, Erica and Godoy Garraza, Lucas and Amaranath, Pracheta},
  doi          = {10.1093/biomtc/ujad034},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad034},
  shortjournal = {Biometrics},
  title        = {Adaptive selection of the optimal strategy to improve precision and power in randomized trials},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Rejoinder to the discussion on “the central role of the
identifying assumption in population size estimation.” <em>BIOMTC</em>,
<em>80</em>(1), ujad033. (<a
href="https://doi.org/10.1093/biomtc/ujad033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We organize the discussants’ major comments into the following categories: sensitivity analyses, zero counts, model selection, the marginal no-highest-order interaction (NHOI) assumption, and the usefulness of our proposed framework.},
  archive      = {J_BIOMTC},
  author       = {Aleshin-Guendel, Serge and Sadinle, Mauricio and Wakefield, Jon},
  doi          = {10.1093/biomtc/ujad033},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad033},
  shortjournal = {Biometrics},
  title        = {Rejoinder to the discussion on “The central role of the identifying assumption in population size estimation”},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discussion on “the central role of the identifying
assumption in population size estimation” by serge aleshin-guendel,
mauricio sadinle, and jon wakefield. <em>BIOMTC</em>, <em>80</em>(1),
ujad032. (<a href="https://doi.org/10.1093/biomtc/ujad032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this discussion response, we consider some practical implications of the authors’ consideration of the no-highest-order interaction (NHOI) model for multiple systems estimation, which permits the authors to derive the explicit (albeit untestable) identifying assumption related to the unobserved (or missing) individuals. In particular, we discuss several aspects, from the standard process of model selection to potential poor predictive performance due to over-fitting and the implications of data reduction. We discuss these aspects in relation to the case study presented by the authors relating to the number of civilian casualties within the Kosovo war, and conduct further preliminary simulations to investigate these issues further. The results suggest that the NHOI models considered, despite having a potentially useful theoretical result in relation to the underlying identifying assumption, may perform poorly in practice.},
  archive      = {J_BIOMTC},
  author       = {King, Ruth and McCrea, Rachel and Overstall, Antony},
  doi          = {10.1093/biomtc/ujad032},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad032},
  shortjournal = {Biometrics},
  title        = {Discussion on “The central role of the identifying assumption in population size estimation” by serge aleshin-guendel, mauricio sadinle, and jon wakefield},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discussion on “the central role of the identifying
assumption in population size estimation” by serge aleshin-guendel,
mauricio sadinle, and jon wakefield. <em>BIOMTC</em>, <em>80</em>(1),
ujad031. (<a href="https://doi.org/10.1093/biomtc/ujad031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Whitehead, John},
  doi          = {10.1093/biomtc/ujad031},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad031},
  shortjournal = {Biometrics},
  title        = {Discussion on “The central role of the identifying assumption in population size estimation” by serge aleshin-guendel, mauricio sadinle, and jon wakefield},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discussion on “the central role of the identifying
assumption in population size estimation” by serge aleshin-guendel,
mauricio sadinle, and jon wakefield. <em>BIOMTC</em>, <em>80</em>(1),
ujad030. (<a href="https://doi.org/10.1093/biomtc/ujad030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Manrique-Vallier, Daniel},
  doi          = {10.1093/biomtc/ujad030},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad030},
  shortjournal = {Biometrics},
  title        = {Discussion on “The central role of the identifying assumption in population size estimation” by serge aleshin-guendel, mauricio sadinle, and jon wakefield},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discussion on “the central role of the identifying
assumption in population size estimation” by serge aleshin-guendel,
mauricio sadinle, and jon wakefield. <em>BIOMTC</em>, <em>80</em>(1),
ujad029. (<a href="https://doi.org/10.1093/biomtc/ujad029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Zhang, Li-Chun},
  doi          = {10.1093/biomtc/ujad029},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad029},
  shortjournal = {Biometrics},
  title        = {Discussion on “The central role of the identifying assumption in population size estimation” by serge aleshin-guendel, mauricio sadinle, and jon wakefield},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). The central role of the identifying assumption in
population size estimation. <em>BIOMTC</em>, <em>80</em>(1), ujad028.
(<a href="https://doi.org/10.1093/biomtc/ujad028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of estimating the size of a population based on a subset of individuals observed across multiple data sources is often referred to as capture-recapture or multiple-systems estimation. This is fundamentally a missing data problem, where the number of unobserved individuals represents the missing data. As with any missing data problem, multiple-systems estimation requires users to make an untestable identifying assumption in order to estimate the population size from the observed data. If an appropriate identifying assumption cannot be found for a data set, no estimate of the population size should be produced based on that data set, as models with different identifying assumptions can produce arbitrarily different population size estimates—even with identical observed data fits. Approaches to multiple-systems estimation often do not explicitly specify identifying assumptions. This makes it difficult to decouple the specification of the model for the observed data from the identifying assumption and to provide justification for the identifying assumption. We present a re-framing of the multiple-systems estimation problem that leads to an approach that decouples the specification of the observed-data model from the identifying assumption, and discuss how common models fit into this framing. This approach takes advantage of existing software and facilitates various sensitivity analyses. We demonstrate our approach in a case study estimating the number of civilian casualties in the Kosovo war.},
  archive      = {J_BIOMTC},
  author       = {Aleshin-Guendel, Serge and Sadinle, Mauricio and Wakefield, Jon},
  doi          = {10.1093/biomtc/ujad028},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad028},
  shortjournal = {Biometrics},
  title        = {The central role of the identifying assumption in population size estimation},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating the effect of latent time-varying count exposures
using multiple lists. <em>BIOMTC</em>, <em>80</em>(1), ujad027. (<a
href="https://doi.org/10.1093/biomtc/ujad027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A major challenge in longitudinal built-environment health studies is the accuracy of commercial business databases that are used to characterize dynamic food environments. Different databases often provide conflicting exposure measures on the same subject due to different source credibilities. As on-site verification is not feasible for historical data, we suggest combining multiple databases to correct the bias in health effect estimates due to measurement error in any 1 datasource. We propose a joint model for the time-varying health outcomes, observed count exposures, and latent true count exposures. Our model estimates the time-specific quality of sources and incorporates time dependence of true count exposure by Poisson integer-valued first-order autoregressive process. We take a Bayesian nonparametric approach to flexibly account for location-specific exposures. By resolving the discordance between different databases, our method reduces the bias in the longitudinal health effect of the true exposures. Our method is demonstrated with childhood obesity data in California public schools with respect to convenience store exposures in school neighborhoods from 2001 to 2008.},
  archive      = {J_BIOMTC},
  author       = {Won, Jung Yeon and Elliott, Michael R and Sanchez-Vaznaugh, Emma V and Sánchez, Brisa N},
  doi          = {10.1093/biomtc/ujad027},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad027},
  shortjournal = {Biometrics},
  title        = {Estimating the effect of latent time-varying count exposures using multiple lists},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Bayesian nonparametric for causal inference and missing
data by michael j. Daniels, antonio linero, and jason roy, CRC press,
2023 ISBN-13: 978-0367341008,
https://www.routledge.com/bayesian-nonparametrics-for-causal-inference-and-missing-data/daniels-linero-roy/p/book/9780367341008.
<em>BIOMTC</em>, <em>80</em>(1), ujad026. (<a
href="https://doi.org/10.1093/biomtc/ujad026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Chen, Li-Pang},
  doi          = {10.1093/biomtc/ujad026},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad026},
  shortjournal = {Biometrics},
  title        = {Bayesian nonparametric for causal inference and missing data by michael j. daniels, antonio linero, and jason roy, CRC press, 2023 ISBN-13: 978-0367341008, https://www.routledge.com/Bayesian-nonparametrics-for-causal-inference-and-missing-Data/Daniels-linero-roy/p/book/9780367341008},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Sparse graphical modeling for high dimensional data: A
paradigm of conditional independence tests by faming liang and bochao
jia, CRC press, 2023 ISBN-13: 978-0367183738,
https://www.routledge.com/sparse-graphical-modeling-for-high-dimensional-data-a-paradigm-of-conditional/liang-jia/p/book/9780367183738.
<em>BIOMTC</em>, <em>80</em>(1), ujad025. (<a
href="https://doi.org/10.1093/biomtc/ujad025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Chen, Li-Pang},
  doi          = {10.1093/biomtc/ujad025},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad025},
  shortjournal = {Biometrics},
  title        = {Sparse graphical modeling for high dimensional data: a paradigm of conditional independence tests by faming liang and bochao jia, CRC press, 2023 ISBN-13: 978-0367183738, https://www.routledge.com/Sparse-graphical-modeling-for-high-dimensional-data-A-paradigm-of-Conditional/Liang-jia/p/book/9780367183738},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Penalized deep partially linear cox models with application
to CT scans of lung cancer patients. <em>BIOMTC</em>, <em>80</em>(1),
ujad024. (<a href="https://doi.org/10.1093/biomtc/ujad024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer is a leading cause of cancer mortality globally, highlighting the importance of understanding its mortality risks to design effective patient-centered therapies. The National Lung Screening Trial (NLST) employed computed tomography texture analysis, which provides objective measurements of texture patterns on CT scans, to quantify the mortality risks of lung cancer patients. Partially linear Cox models have gained popularity for survival analysis by dissecting the hazard function into parametric and nonparametric components, allowing for the effective incorporation of both well-established risk factors (such as age and clinical variables) and emerging risk factors (eg, image features) within a unified framework. However, when the dimension of parametric components exceeds the sample size, the task of model fitting becomes formidable, while nonparametric modeling grapples with the curse of dimensionality. We propose a novel Penalized Deep Partially Linear Cox Model (Penalized DPLC), which incorporates the smoothly clipped absolute deviation (SCAD) penalty to select important texture features and employs a deep neural network to estimate the nonparametric component of the model. We prove the convergence and asymptotic properties of the estimator and compare it to other methods through extensive simulation studies, evaluating its performance in risk prediction and feature selection. The proposed method is applied to the NLST study dataset to uncover the effects of key clinical and imaging risk factors on patients’ survival. Our findings provide valuable insights into the relationship between these factors and survival outcomes.},
  archive      = {J_BIOMTC},
  author       = {Sun, Yuming and Kang, Jian and Haridas, Chinmay and Mayne, Nicholas and Potter, Alexandra and Yang, Chi-Fu and Christiani, David C and Li, Yi},
  doi          = {10.1093/biomtc/ujad024},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad024},
  shortjournal = {Biometrics},
  title        = {Penalized deep partially linear cox models with application to CT scans of lung cancer patients},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric predictive model for sparse and irregular
longitudinal data. <em>BIOMTC</em>, <em>80</em>(1), ujad023. (<a
href="https://doi.org/10.1093/biomtc/ujad023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a kernel-based estimator to predict the mean response trajectory for sparse and irregularly measured longitudinal data. The kernel estimator is constructed by imposing weights based on the subject-wise similarity on L 2 metric space between predictor trajectories, where we assume that an analogous fashion in predictor trajectories over time would result in a similar trend in the response trajectory among subjects. In order to deal with the curse of dimensionality caused by the multiple predictors, we propose an appealing multiplicative model with multivariate Gaussian kernels. This model is capable of achieving dimension reduction as well as selecting functional covariates with predictive significance. The asymptotic properties of the proposed nonparametric estimator are investigated under mild regularity conditions. We illustrate the robustness and flexibility of our proposed method via extensive simulation studies and an application to the Framingham Heart Study.},
  archive      = {J_BIOMTC},
  author       = {Wang, Shixuan and Kim, Seonjin and Ryan Cho, Hyunkeun and Chang, Won},
  doi          = {10.1093/biomtc/ujad023},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad023},
  shortjournal = {Biometrics},
  title        = {Nonparametric predictive model for sparse and irregular longitudinal data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A generalized phase 1-2-3 design integrating dose
optimization with confirmatory treatment comparison. <em>BIOMTC</em>,
<em>80</em>(1), ujad022. (<a
href="https://doi.org/10.1093/biomtc/ujad022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A generalized phase 1-2-3 design, Gen 1-2-3, that includes all phases of clinical treatment evaluation is proposed. The design extends and modifies the design of Chapple and Thall (2019), denoted by CT. Both designs begin with a phase 1-2 trial including dose acceptability and optimality criteria, and both select an optimal dose for phase 3. The Gen 1-2-3 design has the following key differences. In stage 1, it uses phase 1-2 criteria to identify a set of candidate doses rather than 1 dose. In stage 2, which is intermediate between phase 1-2 and phase 3, it randomizes additional patients fairly among the candidate doses and an active control treatment arm and uses survival time data from both stage 1 and stage 2 patients to select an optimal dose. It then makes a Go/No Go decision of whether or not to conduct phase 3 based on the predictive probability that the selected optimal dose will provide a specified substantive improvement in survival time over the control. A simulation study shows that the Gen 1-2-3 design has desirable operating characteristics compared to the CT design and 2 conventional designs.},
  archive      = {J_BIOMTC},
  author       = {Zang, Yong and Thall, Peter F and Yuan, Ying},
  doi          = {10.1093/biomtc/ujad022},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad022},
  shortjournal = {Biometrics},
  title        = {A generalized phase 1-2-3 design integrating dose optimization with confirmatory treatment comparison},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Clustering blood donors via mixtures of product partition
models with covariates. <em>BIOMTC</em>, <em>80</em>(1), ujad021. (<a
href="https://doi.org/10.1093/biomtc/ujad021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the problem of accurately predicting gap times between successive blood donations, we present here a general class of Bayesian nonparametric models for clustering. These models allow for the prediction of new recurrences, accommodating covariate information that describes the personal characteristics of the sample individuals. We introduce a prior for the random partition of the sample individuals, which encourages two individuals to be co-clustered if they have similar covariate values. Our prior generalizes product partition models with covariates (PPMx) models in the literature, which are defined in terms of cohesion and similarity functions. We assume cohesion functions that yield mixtures of PPMx models, while our similarity functions represent the denseness of a cluster. We show that including covariate information in the prior specification improves the posterior predictive performance and helps interpret the estimated clusters in terms of covariates in the blood donation application.},
  archive      = {J_BIOMTC},
  author       = {Argiento, Raffaele and Corradin, Riccardo and Guglielmi, Alessandra and Lanzarone, Ettore},
  doi          = {10.1093/biomtc/ujad021},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad021},
  shortjournal = {Biometrics},
  title        = {Clustering blood donors via mixtures of product partition models with covariates},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). That’s not the mona lisa! How to interpret spatial
capture-recapture density surface estimates. <em>BIOMTC</em>,
<em>80</em>(1), ujad020. (<a
href="https://doi.org/10.1093/biomtc/ujad020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial capture-recapture methods are often used to produce density surfaces, and these surfaces are often misinterpreted. In particular, spatial change in density is confused with spatial change in uncertainty about density. We illustrate correct and incorrect inference visually by treating a grayscale image of the Mona Lisa as an activity center intensity or density surface and simulating spatial capture-recapture survey data from it. Inferences can be drawn about the intensity of the point process generating activity centers, and about the likely locations of activity centers associated with the capture histories obtained from a single survey of a single realization of this process. We show that treating probabilistic predictions of activity center locations as estimates of the intensity of the process results in invalid and misleading ecological inferences, and that predictions are highly dependent on where the detectors are placed and how much survey effort is used. Estimates of the activity center density surface should be obtained by estimating the intensity of a point process model for activity centers. Practitioners should state explicitly whether they are estimating the intensity or making predictions of activity center location, and predictions of activity center locations should not be confused with estimates of the intensity.},
  archive      = {J_BIOMTC},
  author       = {Durbach, Ian and Chopara, Rishika and Borchers, David L and Phillip, Rachel and Sharma, Koustubh and Stevenson, Ben C},
  doi          = {10.1093/biomtc/ujad020},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad020},
  shortjournal = {Biometrics},
  title        = {That’s not the mona lisa! how to interpret spatial capture-recapture density surface estimates},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A flexible framework for spatial capture-recapture with
unknown identities. <em>BIOMTC</em>, <em>80</em>(1), ujad019. (<a
href="https://doi.org/10.1093/biomtc/ujad019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camera traps or acoustic recorders are often used to sample wildlife populations. When animals can be individually identified, these data can be used with spatial capture-recapture (SCR) methods to assess populations. However, obtaining animal identities is often labor-intensive and not always possible for all detected animals. To address this problem, we formulate SCR, including acoustic SCR, as a marked Poisson process, comprising a single counting process for the detections of all animals and a mark distribution for what is observed (eg, animal identity, detector location). The counting process applies equally when it is animals appearing in front of camera traps and when vocalizations are captured by microphones, although the definition of a mark changes. When animals cannot be uniquely identified, the observed marks arise from a mixture of mark distributions defined by the animal activity centers and additional characteristics. Our method generalizes existing latent identity SCR models and provides an integrated framework that includes acoustic SCR. We apply our method to estimate density from a camera trap study of fisher ( Pekania pennanti ) and an acoustic survey of Cape Peninsula moss frog ( Arthroleptella lightfooti ). We also test it through simulation. We find latent identity SCR with additional marks such as sex or time of arrival to be a reliable method for estimating animal density.},
  archive      = {J_BIOMTC},
  author       = {van Dam-Bates, Paul and Papathomas, Michail and Stevenson, Ben C and Fewster, Rachel M and Turek, Daniel and Stewart, Frances E C and Borchers, David L},
  doi          = {10.1093/biomtc/ujad019},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad019},
  shortjournal = {Biometrics},
  title        = {A flexible framework for spatial capture-recapture with unknown identities},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantifying the HIV reservoir with dilution assays and deep
viral sequencing. <em>BIOMTC</em>, <em>80</em>(1), ujad018. (<a
href="https://doi.org/10.1093/biomtc/ujad018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People living with HIV on antiretroviral therapy often have undetectable virus levels by standard assays, but “latent” HIV still persists in viral reservoirs. Eliminating these reservoirs is the goal of HIV cure research. The quantitative viral outgrowth assay (QVOA) is commonly used to estimate the reservoir size, that is, the infectious units per million (IUPM) of HIV-persistent resting CD4+ T cells. A new variation of the QVOA, the ultra deep sequencing assay of the outgrowth virus (UDSA), was recently developed that further quantifies the number of viral lineages within a subset of infected wells. Performing the UDSA on a subset of wells provides additional information that can improve IUPM estimation. This paper considers statistical inference about the IUPM from combined dilution assay (QVOA) and deep viral sequencing (UDSA) data, even when some deep sequencing data are missing. Methods are proposed to accommodate assays with wells sequenced at multiple dilution levels and with imperfect sensitivity and specificity, and a novel bias-corrected estimator is included for small samples. The proposed methods are evaluated in a simulation study, applied to data from the University of North Carolina HIV Cure Center, and implemented in the open-source R package SLDeepAssay .},
  archive      = {J_BIOMTC},
  author       = {Lotspeich, Sarah C and Richardson, Brian D and Baldoni, Pedro L and Enders, Kimberly P and Hudgens, Michael G},
  doi          = {10.1093/biomtc/ujad018},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad018},
  shortjournal = {Biometrics},
  title        = {Quantifying the HIV reservoir with dilution assays and deep viral sequencing},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiobjective tree-based reinforcement learning for
estimating tolerant dynamic treatment regimes. <em>BIOMTC</em>,
<em>80</em>(1), ujad017. (<a
href="https://doi.org/10.1093/biomtc/ujad017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A dynamic treatment regime (DTR) is a sequence of treatment decision rules that dictate individualized treatments based on evolving treatment and covariate history. It provides a vehicle for optimizing a clinical decision support system and fits well into the broader paradigm of personalized medicine. However, many real-world problems involve multiple competing priorities, and decision rules differ when trade-offs are present. Correspondingly, there may be more than one feasible decision that leads to empirically sufficient optimization. In this paper, we propose a concept of “tolerant regime,” which provides a set of individualized feasible decision rules under a prespecified tolerance rate. A multiobjective tree-based reinforcement learning (MOT-RL) method is developed to directly estimate the tolerant DTR (tDTR) that optimizes multiple objectives in a multistage multitreatment setting. At each stage, MOT-RL constructs an unsupervised decision tree by modeling the counterfactual mean outcome of each objective via semiparametric regression and maximizing a purity measure constructed by the scalarized augmented inverse probability weighted estimators (SAIPWE). The algorithm is implemented in a backward inductive manner through multiple decision stages, and it estimates the optimal DTR and tDTR depending on the decision-maker’s preferences. Multiobjective tree-based reinforcement learning is robust, efficient, easy-to-interpret, and flexible to different settings. We apply MOT-RL to evaluate 2-stage chemotherapy regimes that reduce disease burden and prolong survival for advanced prostate cancer patients using a dataset collected at MD Anderson Cancer Center.},
  archive      = {J_BIOMTC},
  author       = {Song, Yao and Wang, Lu},
  doi          = {10.1093/biomtc/ujad017},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad017},
  shortjournal = {Biometrics},
  title        = {Multiobjective tree-based reinforcement learning for estimating tolerant dynamic treatment regimes},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Principal stratification analysis of noncompliance with
time-to-event outcomes. <em>BIOMTC</em>, <em>80</em>(1), ujad016. (<a
href="https://doi.org/10.1093/biomtc/ujad016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Post-randomization events, also known as intercurrent events, such as treatment noncompliance and censoring due to a terminal event, are common in clinical trials. Principal stratification is a framework for causal inference in the presence of intercurrent events. The existing literature on principal stratification lacks generally applicable and accessible methods for time-to-event outcomes. In this paper, we focus on the noncompliance setting. We specify 2 causal estimands for time-to-event outcomes in principal stratification and provide a nonparametric identification formula. For estimation, we adopt the latent mixture modeling approach and illustrate the general strategy with a mixture of Bayesian parametric Weibull-Cox proportional hazards model for the outcome. We utilize the Stan programming language to obtain automatic posterior sampling of the model parameters. We provide analytical forms of the causal estimands as functions of the model parameters and an alternative numerical method when analytical forms are not available. We apply the proposed method to the ADAPTABLE ( A spirin D osing: A P atient-Centric T rial A ssessing B enefits and L ong-Term E ffectiveness) trial to evaluate the causal effect of taking 81 versus 325 mg aspirin on the risk of major adverse cardiovascular events. We develop the corresponding R package PStrata .},
  archive      = {J_BIOMTC},
  author       = {Liu, Bo and Wruck, Lisa and Li, Fan},
  doi          = {10.1093/biomtc/ujad016},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad016},
  shortjournal = {Biometrics},
  title        = {Principal stratification analysis of noncompliance with time-to-event outcomes},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inferring HIV transmission patterns from viral deep-sequence
data via latent typed point processes. <em>BIOMTC</em>, <em>80</em>(1),
ujad015. (<a href="https://doi.org/10.1093/biomtc/ujad015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Viral deep-sequencing data play a crucial role toward understanding disease transmission network flows, providing higher resolution compared to standard Sanger sequencing. To more fully utilize these rich data and account for the uncertainties in outcomes from phylogenetic analyses, we propose a spatial Poisson process model to uncover human immunodeficiency virus (HIV) transmission flow patterns at the population level. We represent pairings of individuals with viral sequence data as typed points, with coordinates representing covariates such as gender and age and point types representing the unobserved transmission statuses (linkage and direction). Points are associated with observed scores on the strength of evidence for each transmission status that are obtained through standard deep-sequence phylogenetic analysis. Our method is able to jointly infer the latent transmission statuses for all pairings and the transmission flow surface on the source-recipient covariate space. In contrast to existing methods, our framework does not require preclassification of the transmission statuses of data points, and instead learns them probabilistically through a fully Bayesian inference scheme. By directly modeling continuous spatial processes with smooth densities, our method enjoys significant computational advantages compared to previous methods that rely on discretization of the covariate space. We demonstrate that our framework can capture age structures in HIV transmission at high resolution, bringing valuable insights in a case study on viral deep-sequencing data from Southern Uganda.},
  archive      = {J_BIOMTC},
  author       = {Bu, Fan and Kagaayi, Joseph and Grabowski, Mary Kate and Ratmann, Oliver and Xu, Jason},
  doi          = {10.1093/biomtc/ujad015},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad015},
  shortjournal = {Biometrics},
  title        = {Inferring HIV transmission patterns from viral deep-sequence data via latent typed point processes},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incorporating graph information in bayesian factor analysis
with robust and adaptive shrinkage priors. <em>BIOMTC</em>,
<em>80</em>(1), ujad014. (<a
href="https://doi.org/10.1093/biomtc/ujad014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been an increasing interest in decomposing high-dimensional multi-omics data into a product of low-rank and sparse matrices for the purpose of dimension reduction and feature engineering. Bayesian factor models achieve such low-dimensional representation of the original data through different sparsity-inducing priors. However, few of these models can efficiently incorporate the information encoded by the biological graphs, which has been already proven to be useful in many analysis tasks. In this work, we propose a Bayesian factor model with novel hierarchical priors, which incorporate the biological graph knowledge as a tool of identifying a group of genes functioning collaboratively. The proposed model therefore enables sparsity within networks by allowing each factor loading to be shrunk adaptively and by considering additional layers to relate individual shrinkage parameters to the underlying graph information, both of which yield a more accurate structure recovery of factor loadings. Further, this new priors overcome the phase transition phenomenon, in contrast to existing graph-incorporated approaches, so that it is robust to noisy edges that are inconsistent with the actual sparsity structure of the factor loadings. Finally, our model can handle both continuous and discrete data types. The proposed method is shown to outperform several existing factor analysis methods through simulation experiments and real data analyses.},
  archive      = {J_BIOMTC},
  author       = {Zhang, Qiyiwen and Chang, Changgee and Shen, Li and Long, Qi},
  doi          = {10.1093/biomtc/ujad014},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad014},
  shortjournal = {Biometrics},
  title        = {Incorporating graph information in bayesian factor analysis with robust and adaptive shrinkage priors},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Randomized phase II selection design with order constrained
strata. <em>BIOMTC</em>, <em>80</em>(1), ujad013. (<a
href="https://doi.org/10.1093/biomtc/ujad013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exploratory nature of phase II trials makes it quite common to include heterogeneous patient subgroups with different prognoses in the same trial. Incorporating such patient heterogeneity or stratification into statistical calculation for sample size can improve efficiency and reduce sample sizes in single-arm phase II trials with binary outcomes. However, such consideration is lacking in randomized phase II trials. In this paper, we propose methods that can utilize some natural order constraints that may exist in stratified population to gain statistical efficiency for randomized phase II designs. For thoroughness and simplicity, we focus on the randomized phase II selection designs in this paper, although our method can be easily generalized to the randomized phase II screening designs. We consider both binary and time-to-event outcomes in our development. Compared with methods that do not use order constraints, our method is shown to improve the probabilities of correct selection or reduce sample size in our simulation and real examples.},
  archive      = {J_BIOMTC},
  author       = {Chen, Yi and Yu, Menggang},
  doi          = {10.1093/biomtc/ujad013},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad013},
  shortjournal = {Biometrics},
  title        = {Randomized phase II selection design with order constrained strata},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Individualized treatment rule characterization via a value
function surrogate. <em>BIOMTC</em>, <em>80</em>(1), ujad012. (<a
href="https://doi.org/10.1093/biomtc/ujad012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precision medicine is a promising framework for generating evidence to improve health and health care. Yet, a gap persists between the ever-growing number of statistical precision medicine strategies for evidence generation and implementation in real-world clinical settings, and the strategies for closing this gap will likely be context-dependent. In this paper, we consider the specific context of partial compliance to wound management among patients with peripheral artery disease. Using a Gaussian process surrogate for the value function, we show the feasibility of using Bayesian optimization to learn optimal individualized treatment rules. Further, we expand beyond the common precision medicine task of learning an optimal individualized treatment rule to the characterization of classes of individualized treatment rules and show how those findings can be translated into clinical contexts.},
  archive      = {J_BIOMTC},
  author       = {Freeman, Nikki L B and Browder, Sydney E and McGinigle, Katharine L and Kosorok, Michael R},
  doi          = {10.1093/biomtc/ujad012},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad012},
  shortjournal = {Biometrics},
  title        = {Individualized treatment rule characterization via a value function surrogate},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Proportional rates models for multivariate panel count data.
<em>BIOMTC</em>, <em>80</em>(1), ujad011. (<a
href="https://doi.org/10.1093/biomtc/ujad011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate panel count data arise when there are multiple types of recurrent events, and the observation for each study subject consists of the number of recurrent events of each type between two successive examinations. We formulate the effects of potentially time-dependent covariates on multiple types of recurrent events through proportional rates models, while leaving the dependence structures of the related recurrent events completely unspecified. We employ nonparametric maximum pseudo-likelihood estimation under the working assumptions that all types of events are independent and each type of event is a nonhomogeneous Poisson process, and we develop a simple and stable EM-type algorithm. We show that the resulting estimators of the regression parameters are consistent and asymptotically normal, with a covariance matrix that can be estimated consistently by a sandwich estimator. In addition, we develop a class of graphical and numerical methods for checking the adequacy of the fitted model. Finally, we evaluate the performance of the proposed methods through simulation studies and analysis of a skin cancer clinical trial.},
  archive      = {J_BIOMTC},
  author       = {Xu, Yangjianchen and Zeng, Donglin and Lin, Dan-Yu},
  doi          = {10.1093/biomtc/ujad011},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad011},
  shortjournal = {Biometrics},
  title        = {Proportional rates models for multivariate panel count data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient designs and analysis of two-phase studies with
longitudinal binary data. <em>BIOMTC</em>, <em>80</em>(1), ujad010. (<a
href="https://doi.org/10.1093/biomtc/ujad010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers interested in understanding the relationship between a readily available longitudinal binary outcome and a novel biomarker exposure can be confronted with ascertainment costs that limit sample size. In such settings, two-phase studies can be cost-effective solutions that allow researchers to target informative individuals for exposure ascertainment and increase estimation precision for time-varying and/or time-fixed exposure coefficients. In this paper, we introduce a novel class of residual-dependent sampling (RDS) designs that select informative individuals using data available on the longitudinal outcome and inexpensive covariates. Together with the RDS designs, we propose a semiparametric analysis approach that efficiently uses all data to estimate the parameters. We describe a numerically stable and computationally efficient EM algorithm to maximize the semiparametric likelihood. We examine the finite sample operating characteristics of the proposed approaches through extensive simulation studies, and compare the efficiency of our designs and analysis approach with existing ones. We illustrate the usefulness of the proposed RDS designs and analysis method in practice by studying the association between a genetic marker and poor lung function among patients enrolled in the Lung Health Study (Connett et al, 1993).},
  archive      = {J_BIOMTC},
  author       = {Di Gravio, Chiara and Schildcrout, Jonathan S and Tao, Ran},
  doi          = {10.1093/biomtc/ujad010},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad010},
  shortjournal = {Biometrics},
  title        = {Efficient designs and analysis of two-phase studies with longitudinal binary data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian survival treed hazards model using latent
gaussian processes. <em>BIOMTC</em>, <em>80</em>(1), ujad009. (<a
href="https://doi.org/10.1093/biomtc/ujad009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survival models are used to analyze time-to-event data in a variety of disciplines. Proportional hazard models provide interpretable parameter estimates, but proportional hazard assumptions are not always appropriate. Non-parametric models are more flexible but often lack a clear inferential framework. We propose a Bayesian treed hazards partition model that is both flexible and inferential. Inference is obtained through the posterior tree structure and flexibility is preserved by modeling the log-hazard function in each partition using a latent Gaussian process. An efficient reversible jump Markov chain Monte Carlo algorithm is accomplished by marginalizing the parameters in each partition element via a Laplace approximation. Consistency properties for the estimator are established. The method can be used to help determine subgroups as well as prognostic and/or predictive biomarkers in time-to-event data. The method is compared with some existing methods on simulated data and a liver cirrhosis dataset.},
  archive      = {J_BIOMTC},
  author       = {Payne, Richard D and Guha, Nilabja and Mallick, Bani K},
  doi          = {10.1093/biomtc/ujad009},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad009},
  shortjournal = {Biometrics},
  title        = {A bayesian survival treed hazards model using latent gaussian processes},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient estimation for left-truncated competing risks
regression for case-cohort studies. <em>BIOMTC</em>, <em>80</em>(1),
ujad008. (<a href="https://doi.org/10.1093/biomtc/ujad008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The case-cohort study design provides a cost-effective study design for a large cohort study with competing risk outcomes. The proportional subdistribution hazards model is widely used to estimate direct covariate effects on the cumulative incidence function for competing risk data. In biomedical studies, left truncation often occurs and brings extra challenges to the analysis. Existing inverse probability weighting methods for case-cohort studies with competing risk data not only have not addressed left truncation, but also are inefficient in regression parameter estimation for fully observed covariates. We propose an augmented inverse probability-weighted estimating equation for left-truncated competing risk data to address these limitations of the current literature. We further propose a more efficient estimator when extra information from the other causes is available. The proposed estimators are consistent and asymptotically normally distributed. Simulation studies show that the proposed estimator is unbiased and leads to estimation efficiency gain in the regression parameter estimation. We analyze the Atherosclerosis Risk in Communities study data using the proposed methods.},
  archive      = {J_BIOMTC},
  author       = {Fang, Xi and Ahn, Kwang Woo and Cai, Jianwen and Kim, Soyoung},
  doi          = {10.1093/biomtc/ujad008},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad008},
  shortjournal = {Biometrics},
  title        = {Efficient estimation for left-truncated competing risks regression for case-cohort studies},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive sequential surveillance with network and temporal
dependence. <em>BIOMTC</em>, <em>80</em>(1), ujad007. (<a
href="https://doi.org/10.1093/biomtc/ujad007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strategic test allocation is important for control of both emerging and existing pandemics (eg, COVID-19, HIV). It supports effective epidemic control by (1) reducing transmission via identifying cases and (2) tracking outbreak dynamics to inform targeted interventions. However, infectious disease surveillance presents unique statistical challenges. For instance, the true outcome of interest (positive infection status) is often a latent variable. In addition, presence of both network and temporal dependence reduces data to a single observation. In this work, we study an adaptive sequential design, which allows for unspecified dependence among individuals and across time. Our causal parameter is the mean latent outcome we would have obtained, if, starting at time t given the observed past, we had carried out a stochastic intervention that maximizes the outcome under a resource constraint. The key strength of the method is that we do not have to model network and time dependence: a short-term performance Online Super Learner is used to select among dependence models and randomization schemes. The proposed strategy learns the optimal choice of testing over time while adapting to the current state of the outbreak and learning across samples, through time, or both. We demonstrate the superior performance of the proposed strategy in an agent-based simulation modeling a residential university environment during the COVID-19 pandemic.},
  archive      = {J_BIOMTC},
  author       = {Malenica, Ivana and Coyle, Jeremy R and van der Laan, Mark J and Petersen, Maya L},
  doi          = {10.1093/biomtc/ujad007},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad007},
  shortjournal = {Biometrics},
  title        = {Adaptive sequential surveillance with network and temporal dependence},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Longitudinal varying coefficient single-index model with
censored covariates. <em>BIOMTC</em>, <em>80</em>(1), ujad006. (<a
href="https://doi.org/10.1093/biomtc/ujad006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is of interest to health policy research to estimate the population-averaged longitudinal medical cost trajectory from initial cancer diagnosis to death, and understand how the trajectory curve is affected by patient characteristics. This research question leads to a number of statistical challenges because the longitudinal cost data are often non-normally distributed with skewness, zero-inflation, and heteroscedasticity. The trajectory is nonlinear, and its length and shape depend on survival, which are subject to censoring. Modeling the association between multiple patient characteristics and nonlinear cost trajectory curves of varying lengths should take into consideration parsimony, flexibility, and interpretation. We propose a novel longitudinal varying coefficient single-index model. Multiple patient characteristics are summarized in a single-index, representing a patient&#39;s overall propensity for healthcare use. The effects of this index on various segments of the cost trajectory depend on both time and survival, which is flexibly modeled by a bivariate varying coefficient function. The model is estimated by generalized estimating equations with an extended marginal mean structure to accommodate censored survival time as a covariate. We established the pointwise confidence interval of the varying coefficient and a test for the covariate effect. The numerical performance was extensively studied in simulations. We applied the proposed methodology to medical cost data of prostate cancer patients from the Surveillance, Epidemiology, and End Results-Medicare-Linked Database.},
  archive      = {J_BIOMTC},
  author       = {Wang, Shikun and Ning, Jing and Xu, Ying and Shih, Ya-Chen Tina and Shen, Yu and Li, Liang},
  doi          = {10.1093/biomtc/ujad006},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad006},
  shortjournal = {Biometrics},
  title        = {Longitudinal varying coefficient single-index model with censored covariates},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust data integration from multiple external sources for
generalized linear models with binary outcomes. <em>BIOMTC</em>,
<em>80</em>(1), ujad005. (<a
href="https://doi.org/10.1093/biomtc/ujad005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We aim to estimate parameters in a generalized linear model (GLM) for a binary outcome when, in addition to the raw data from the internal study, more than 1 external study provides summary information in the form of parameter estimates from fitting GLMs with varying subsets of the internal study covariates. We propose an adaptive penalization method that exploits the external summary information and gains efficiency for estimation, and that is both robust and computationally efficient. The robust property comes from exploiting the relationship between parameters of a GLM and parameters of a GLM with omitted covariates and from downweighting external summary information that is less compatible with the internal data through a penalization. The computational burden associated with searching for the optimal tuning parameter for the penalization is reduced by using adaptive weights and by using an information criterion when searching for the optimal tuning parameter. Simulation studies show that the proposed estimator is robust against various types of population distribution heterogeneity and also gains efficiency compared to direct maximum likelihood estimation. The method is applied to improve a logistic regression model that predicts high-grade prostate cancer making use of parameter estimates from 2 external models.},
  archive      = {J_BIOMTC},
  author       = {Choi, Kyuseong and Taylor, Jeremy M G and Han, Peisong},
  doi          = {10.1093/biomtc/ujad005},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad005},
  shortjournal = {Biometrics},
  title        = {Robust data integration from multiple external sources for generalized linear models with binary outcomes},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incorporating participants’ welfare into sequential multiple
assignment randomized trials. <em>BIOMTC</em>, <em>80</em>(1), ujad004.
(<a href="https://doi.org/10.1093/biomtc/ujad004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic treatment regimes (DTRs) are sequences of decision rules that recommend treatments based on patients’ time-varying clinical conditions. The sequential, multiple assignment, randomized trial (SMART) is an experimental design that can provide high-quality evidence for constructing optimal DTRs. In a conventional SMART, participants are randomized to available treatments at multiple stages with balanced randomization probabilities. Despite its relative simplicity of implementation and desirable performance in comparing embedded DTRs, the conventional SMART faces inevitable ethical issues, including assigning many participants to the empirically inferior treatment or the treatment they dislike, which might slow down the recruitment procedure and lead to higher attrition rates, ultimately leading to poor internal and external validities of the trial results. In this context, we propose a SMART under the Experiment-as-Market framework (SMART-EXAM), a novel SMART design that holds the potential to improve participants’ welfare by incorporating their preferences and predicted treatment effects into the randomization procedure. We describe the steps of conducting a SMART-EXAM and evaluate its performance compared to the conventional SMART. The results indicate that the SMART-EXAM can improve the welfare of the participants enrolled in the trial, while also achieving a desirable ability to construct an optimal DTR when the experimental parameters are suitably specified. We finally illustrate the practical potential of the SMART-EXAM design using data from a SMART for children with attention-deficit/hyperactivity disorder.},
  archive      = {J_BIOMTC},
  author       = {Wang, Xinru and Deliu, Nina and Narita, Yusuke and Chakraborty, Bibhas},
  doi          = {10.1093/biomtc/ujad004},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad004},
  shortjournal = {Biometrics},
  title        = {Incorporating participants’ welfare into sequential multiple assignment randomized trials},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Personalized treatment selection via product partition
models with covariates. <em>BIOMTC</em>, <em>80</em>(1), ujad003. (<a
href="https://doi.org/10.1093/biomtc/ujad003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precision medicine is an approach for disease treatment that defines treatment strategies based on the individual characteristics of the patients. Motivated by an open problem in cancer genomics, we develop a novel model that flexibly clusters patients with similar predictive characteristics and similar treatment responses; this approach identifies, via predictive inference, which one among a set of treatments is better suited for a new patient. The proposed method is fully model based, avoiding uncertainty underestimation attained when treatment assignment is performed by adopting heuristic clustering procedures, and belongs to the class of product partition models with covariates, here extended to include the cohesion induced by the normalized generalized gamma process. The method performs particularly well in scenarios characterized by considerable heterogeneity of the predictive covariates in simulation studies. A cancer genomics case study illustrates the potential benefits in terms of treatment response yielded by the proposed approach. Finally, being model based, the approach allows estimating clusters’ specific response probabilities and then identifying patients more likely to benefit from personalized treatment.},
  archive      = {J_BIOMTC},
  author       = {Pedone, Matteo and Argiento, Raffaele and Stingo, Francesco C},
  doi          = {10.1093/biomtc/ujad003},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad003},
  shortjournal = {Biometrics},
  title        = {Personalized treatment selection via product partition models with covariates},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple augmented reduced rank regression for pan-cancer
analysis. <em>BIOMTC</em>, <em>80</em>(1), ujad002. (<a
href="https://doi.org/10.1093/biomtc/ujad002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical approaches that successfully combine multiple datasets are more powerful, efficient, and scientifically informative than separate analyses. To address variation architectures correctly and comprehensively for high-dimensional data across multiple sample sets (ie, cohorts), we propose multiple augmented reduced rank regression (maRRR), a flexible matrix regression and factorization method to concurrently learn both covariate-driven and auxiliary structured variations. We consider a structured nuclear norm objective that is motivated by random matrix theory, in which the regression or factorization terms may be shared or specific to any number of cohorts. Our framework subsumes several existing methods, such as reduced rank regression and unsupervised multimatrix factorization approaches, and includes a promising novel approach to regression and factorization of a single dataset (aRRR) as a special case. Simulations demonstrate substantial gains in power from combining multiple datasets, and from parsimoniously accounting for all structured variations. We apply maRRR to gene expression data from multiple cancer types (ie, pan-cancer) from The Cancer Genome Atlas, with somatic mutations as covariates. The method performs well with respect to prediction and imputation of held-out data, and provides new insights into mutation-driven and auxiliary variations that are shared or specific to certain cancer types.},
  archive      = {J_BIOMTC},
  author       = {Wang, Jiuzhou and Lock, Eric F},
  doi          = {10.1093/biomtc/ujad002},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad002},
  shortjournal = {Biometrics},
  title        = {Multiple augmented reduced rank regression for pan-cancer analysis},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Homogeneity pursuit and variable selection in regression
models for multivariate abundance data. <em>BIOMTC</em>, <em>80</em>(1),
ujad001. (<a href="https://doi.org/10.1093/biomtc/ujad001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When building regression models for multivariate abundance data in ecology, it is important to allow for the fact that the species are correlated with each other. Moreover, there is often evidence species exhibit some degree of homogeneity in their responses to each environmental predictor, and that most species are informed by only a subset of predictors. We propose a generalized estimating equation (GEE) approach for simultaneous homogeneity pursuit (ie, grouping species with similar coefficient values while allowing differing groups for different covariates) and variable selection in regression models for multivariate abundance data. Using GEEs allows us to straightforwardly account for between-response correlations through a (reduced-rank) working correlation matrix. We augment the GEE with both adaptive fused lasso- and adaptive lasso-type penalties, which aim to cluster the species-specific coefficients within each covariate and encourage differing levels of sparsity across the covariates, respectively. Numerical studies demonstrate the strong finite sample performance of the proposed method relative to several existing approaches for modeling multivariate abundance data. Applying the proposed method to presence–absence records collected along the Great Barrier Reef in Australia reveals both a substantial degree of homogeneity and sparsity in species-environmental relationships. We show this leads to a more parsimonious model for understanding the environmental drivers of seabed biodiversity, and results in stronger out-of-sample predictive performance relative to methods that do not accommodate such features.},
  archive      = {J_BIOMTC},
  author       = {Hui, Francis K C and Maestrini, Luca and Welsh, Alan H},
  doi          = {10.1093/biomtc/ujad001},
  journal      = {Biometrics},
  month        = {1},
  number       = {1},
  pages        = {ujad001},
  shortjournal = {Biometrics},
  title        = {Homogeneity pursuit and variable selection in regression models for multivariate abundance data},
  volume       = {80},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
