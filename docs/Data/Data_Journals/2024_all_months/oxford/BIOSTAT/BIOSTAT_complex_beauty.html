<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>BIOSTAT_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="biostat---116">BIOSTAT - 116</h2>
<ul>
<li><details>
<summary>
(2024). Random forest for dynamic risk prediction of recurrent
events: A pseudo-observation approach. <em>BIOSTAT</em>, <em>26</em>(1),
kxaf007. (<a
href="https://doi.org/10.1093/biostatistics/kxaf007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent events are common in clinical, healthcare, social, and behavioral studies, yet methods for dynamic risk prediction of these events are limited. To overcome some long-standing challenges in analyzing censored recurrent event data, a recent regression analysis framework constructs a censored longitudinal dataset consisting of times to the first recurrent event in multiple pre-specified follow-up windows of length |$ \tau $| (XMT models). Traditional regression models struggle with nonlinear and multiway interactions, with success depending on the skill of the statistical programmer. With a staggering number of potential predictors being generated from genetic, -omic, and electronic health records sources, machine learning approaches such as the random forest regression are growing in popularity, as they can nonparametrically incorporate information from many predictors with nonlinear and multiway interactions involved in prediction. In this article, we (i) develop a random forest approach for dynamically predicting probabilities of remaining event-free during a subsequent |$ \tau $| -duration follow-up period from a reconstructed censored longitudinal data set, (ii) modify the XMT regression approach to predict these same probabilities, subject to the limitations that traditional regression models typically have, and (iii) demonstrate how to incorporate patient-specific history of recurrent events for prediction in settings where this information may be partially missing. We show the increased ability of our random forest algorithm for predicting the probability of remaining event-free over a |$ \tau $| -duration follow-up window when compared to our modified XMT method for prediction in settings where association between predictors and recurrent event outcomes is complex in nature. We also show the importance of incorporating past recurrent event history in prediction algorithms when event times are correlated within a subject. The proposed random forest algorithm is demonstrated using recurrent exacerbation data from the trial of Azithromycin for the Prevention of Exacerbations of Chronic Obstructive Pulmonary Disease.},
  archive      = {J_BIOSTAT},
  author       = {Loe, Abigail and Murray, Susan and Wu, Zhenke},
  doi          = {10.1093/biostatistics/kxaf007},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxaf007},
  shortjournal = {Biostatistics},
  title        = {Random forest for dynamic risk prediction of recurrent events: A pseudo-observation approach},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Covariate-adjusted estimators of diagnostic accuracy in
randomized trials. <em>BIOSTAT</em>, <em>26</em>(1), kxaf005. (<a
href="https://doi.org/10.1093/biostatistics/kxaf005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized controlled trials evaluating the diagnostic accuracy of a marker frequently collect information on baseline covariates in addition to information on the marker and the reference standard. However, standard estimators of sensitivity and specificity do not use data on baseline covariates and restrict the analysis to data from participants with a positive reference standard in the intervention arm being evaluated. Covariate-adjusted estimators for marginal treatment effects have been developed and been advocated for by regulatory agencies because they can improve power compared to unadjusted estimators. Despite this, similar covariate-adjusted estimators for marginal sensitivity and specificity have not yet been developed. In this manuscript, we address this gap by developing covariate-adjusted estimators for marginal sensitivity and specificity of a diagnostic test that leverage baseline covariate information. The estimators also use data from all participants, not just participants with a positive reference standard in the intervention arm being evaluated. We derive the asymptotic properties of the estimators and evaluate the finite sample properties of the estimators using simulations and by analyzing data on lung cancer screening.},
  archive      = {J_BIOSTAT},
  author       = {Steingrimsson, Jon A},
  doi          = {10.1093/biostatistics/kxaf005},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxaf005},
  shortjournal = {Biostatistics},
  title        = {Covariate-adjusted estimators of diagnostic accuracy in randomized trials},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mediation analysis with graph mediator. <em>BIOSTAT</em>,
<em>26</em>(1), kxaf004. (<a
href="https://doi.org/10.1093/biostatistics/kxaf004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a mediation analysis framework when the mediator is a graph. A Gaussian covariance graph model is assumed for graph presentation. Causal estimands and assumptions are discussed under this presentation. With a covariance matrix as the mediator, a low-rank representation is introduced and parametric mediation models are considered under the structural equation modeling framework. Assuming Gaussian random errors, likelihood-based estimators are introduced to simultaneously identify the low-rank representation and causal parameters. An efficient computational algorithm is proposed and asymptotic properties of the estimators are investigated. Via simulation studies, the performance of the proposed approach is evaluated. Applying to a resting-state fMRI study, a brain network is identified within which functional connectivity mediates the sex difference in the performance of a motor task.},
  archive      = {J_BIOSTAT},
  author       = {Xu, Yixi and Zhao, Yi},
  doi          = {10.1093/biostatistics/kxaf004},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxaf004},
  shortjournal = {Biostatistics},
  title        = {Mediation analysis with graph mediator},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable randomized kernel methods for multiview data
integration and prediction with application to coronavirus disease.
<em>BIOSTAT</em>, <em>26</em>(1), kxaf001. (<a
href="https://doi.org/10.1093/biostatistics/kxaf001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is still more to learn about the pathobiology of coronavirus disease (COVID-19) despite 4 years of the pandemic. A multiomics approach offers a comprehensive view of the disease and has the potential to yield deeper insight into the pathogenesis of the disease. Previous multiomics integrative analysis and prediction studies for COVID-19 severity and status have assumed simple relationships (ie linear relationships) between omics data and between omics and COVID-19 outcomes. However, these linear methods do not account for the inherent underlying nonlinear structure associated with these different types of data. The motivation behind this work is to model nonlinear relationships in multiomics and COVID-19 outcomes, and to determine key multidimensional molecules associated with the disease. Toward this goal, we develop scalable randomized kernel methods for jointly associating data from multiple sources or views and simultaneously predicting an outcome or classifying a unit into one of 2 or more classes. We also determine variables or groups of variables that best contribute to the relationships among the views. We use the idea that random Fourier bases can approximate shift-invariant kernel functions to construct nonlinear mappings of each view and we use these mappings and the outcome variable to learn view-independent low-dimensional representations. We demonstrate the effectiveness of the proposed methods through extensive simulations. When the proposed methods were applied to gene expression, metabolomics, proteomics, and lipidomics data pertaining to COVID-19, we identified several molecular signatures for COVID-19 status and severity. Our results agree with previous findings and suggest potential avenues for future research. Our algorithms are implemented in Pytorch and interfaced in R and available at: https://github.com/lasandrall/RandMVLearn .},
  archive      = {J_BIOSTAT},
  author       = {Safo, Sandra E and Lu, Han},
  doi          = {10.1093/biostatistics/kxaf001},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxaf001},
  shortjournal = {Biostatistics},
  title        = {Scalable randomized kernel methods for multiview data integration and prediction with application to coronavirus disease},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unlocking the power of time-since-infection models: Data
augmentation for improved instantaneous reproduction number estimation.
<em>BIOSTAT</em>, <em>26</em>(1), kxae054. (<a
href="https://doi.org/10.1093/biostatistics/kxae054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The time-since-infection (TSI) models, which use disease surveillance data to model infectious diseases, have become increasingly popular due to their flexibility and capacity to address complex disease control questions. However, a notable limitation of TSI models is their primary reliance on incidence data. Even when hospitalization data are available, existing TSI models have not been crafted to improve the estimation of disease transmission or to estimate hospitalization-related parameters—metrics crucial for understanding a pandemic and planning hospital resources. Moreover, their dependence on reported infection data makes them vulnerable to variations in data quality. In this study, we advance TSI models by integrating hospitalization data, marking a significant step forward in modeling with TSI models. We introduce hospitalization propensity parameters to jointly model incidence and hospitalization data. We use a composite likelihood function to accommodate complex data structure and a Monte Carlo expectation–maximization algorithm to estimate model parameters. We analyze COVID-19 data to estimate disease transmission, assess risk factor impacts, and calculate hospitalization propensity. Our model improves the accuracy of estimating the instantaneous reproduction number in TSI models, particularly when hospitalization data is of higher quality than incidence data. It enables the estimation of key infectious disease parameters without relying on contact tracing data and provides a foundation for integrating TSI models with other infectious disease models.},
  archive      = {J_BIOSTAT},
  author       = {Shi, Jiasheng and Zhou, Yizhao and Huang, Jing},
  doi          = {10.1093/biostatistics/kxae054},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae054},
  shortjournal = {Biostatistics},
  title        = {Unlocking the power of time-since-infection models: Data augmentation for improved instantaneous reproduction number estimation},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recurrent events modeling based on a reflected brownian
motion with application to hypoglycemia. <em>BIOSTAT</em>,
<em>26</em>(1), kxae053. (<a
href="https://doi.org/10.1093/biostatistics/kxae053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patients with type 2 diabetes need to closely monitor blood sugar levels as their routine diabetes self-management. Although many treatment agents aim to tightly control blood sugar, hypoglycemia often stands as an adverse event. In practice, patients can observe hypoglycemic events more easily than hyperglycemic events due to the perception of neurogenic symptoms. We propose to model each patient’s observed hypoglycemic event as a lower boundary crossing event for a reflected Brownian motion with an upper reflection barrier. The lower boundary is set by clinical standards. To capture patient heterogeneity and within-patient dependence, covariates and a patient level frailty are incorporated into the volatility and the upper reflection barrier. This framework provides quantification for the underlying glucose level variability, patients heterogeneity, and risk factors’ impact on glucose. We make inferences based on a Bayesian framework using Markov chain Monte Carlo. Two model comparison criteria, the deviance information criterion and the logarithm of the pseudo-marginal likelihood, are used for model selection. The methodology is validated in simulation studies. In analyzing a dataset from the diabetic patients in the DURABLE trial, our model provides adequate fit, generates data similar to the observed data, and offers insights that could be missed by other models.},
  archive      = {J_BIOSTAT},
  author       = {Xie, Yingfa and Fu, Haoda and Huang, Yuan and Pozdnyakov, Vladimir and Yan, Jun},
  doi          = {10.1093/biostatistics/kxae053},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae053},
  shortjournal = {Biostatistics},
  title        = {Recurrent events modeling based on a reflected brownian motion with application to hypoglycemia},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding the opioid syndemic in north carolina: A novel
approach to modeling and identifying factors. <em>BIOSTAT</em>,
<em>26</em>(1), kxae052. (<a
href="https://doi.org/10.1093/biostatistics/kxae052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The opioid epidemic is a significant public health challenge in North Carolina, but limited data restrict our understanding of its complexity. Examining trends and relationships among different outcomes believed to reflect opioid misuse provides an alternative perspective to understand the opioid epidemic. We use a Bayesian dynamic spatial factor model to capture the interrelated dynamics within six different county-level outcomes, such as illicit opioid overdose deaths, emergency department visits related to drug overdose, treatment counts for opioid use disorder, patients receiving prescriptions for buprenorphine, and newly diagnosed cases of acute and chronic hepatitis C virus and human immunodeficiency virus. We design the factor model to yield meaningful interactions among predefined subsets of these outcomes, causing a departure from the conventional lower triangular structure in the loadings matrix and leading to familiar identifiability issues. To address this challenge, we propose a novel approach that involves decomposing the loadings matrix within a Markov chain Monte Carlo algorithm, allowing us to estimate the loadings and factors uniquely. As a result, we gain a better understanding of the spatio-temporal dynamics of the opioid epidemic in North Carolina.},
  archive      = {J_BIOSTAT},
  author       = {Murphy, Eva and Kline, David and Egan, Kathleen L and Lancaster, Kathryn E and Miller, William C and Waller, Lance A and Hepler, Staci A},
  doi          = {10.1093/biostatistics/kxae052},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae052},
  shortjournal = {Biostatistics},
  title        = {Understanding the opioid syndemic in north carolina: A novel approach to modeling and identifying factors},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bipartite interference and air pollution transport:
Estimating health effects of power plant interventions.
<em>BIOSTAT</em>, <em>26</em>(1), kxae051. (<a
href="https://doi.org/10.1093/biostatistics/kxae051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating air quality interventions is confronted with the challenge of interference since interventions at a particular pollution source likely impact air quality and health at distant locations, and air quality and health at any given location are likely impacted by interventions at many sources. The structure of interference in this context is dictated by complex atmospheric processes governing how pollution emitted from a particular source is transformed and transported across space and can be cast with a bipartite structure reflecting the two distinct types of units: (i) interventional units on which treatments are applied or withheld to change pollution emissions; and (ii) outcome units on which outcomes of primary interest are measured. We propose new estimands for bipartite causal inference with interference that construe two components of treatment: a “key-associated” (or “individual”) treatment and an “upwind” (or “neighborhood”) treatment. Estimation is carried out using a covariate adjustment approach based on a joint propensity score. A reduced-complexity atmospheric model characterizes the structure of the interference network by modeling the movement of air parcels through time and space. The new methods are deployed to evaluate the effectiveness of installing flue-gas desulfurization scrubbers on 472 coal-burning power plants (the interventional units) in reducing Medicare hospitalizations among 21,577,552 Medicare beneficiaries residing across 25,553 ZIP codes in the United States (the outcome units).},
  archive      = {J_BIOSTAT},
  author       = {Zigler, Corwin and Liu, Vera and Mealli, Fabrizia and Forastiere, Laura},
  doi          = {10.1093/biostatistics/kxae051},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae051},
  shortjournal = {Biostatistics},
  title        = {Bipartite interference and air pollution transport: Estimating health effects of power plant interventions},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Correction to: Scalable kernel balancing weights in a
nationwide observational study of hospital profit status and heart
attack outcomes. <em>BIOSTAT</em>, <em>26</em>(1), kxae050. (<a
href="https://doi.org/10.1093/biostatistics/kxae050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOSTAT},
  doi          = {10.1093/biostatistics/kxae050},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae050},
  shortjournal = {Biostatistics},
  title        = {Correction to: Scalable kernel balancing weights in a nationwide observational study of hospital profit status and heart attack outcomes},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unveiling schizophrenia: A study with generalized functional
linear mixed model via the investigation of functional random effects.
<em>BIOSTAT</em>, <em>26</em>(1), kxae049. (<a
href="https://doi.org/10.1093/biostatistics/kxae049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous studies have identified attenuated pre-speech activity and speech sound suppression in individuals with Schizophrenia, with similar patterns observed in basic tasks entailing button-pressing to perceive a tone. However, it remains unclear whether these patterns are uniform across individuals or vary from person to person. Motivated by electroencephalographic (EEG) data from a Schizophrenia study, we develop a generalized functional linear mixed model (GFLMM) for repeated measurements by incorporating subject-specific functional random effects associated with multiple functional predictors. To assess the significance of these functional effects, we employ two different multivariate functional principal component analysis methods, which transform the GFLMM into a conventional generalized linear mixed model, thereby facilitating its implementation with standard software. Furthermore, we introduce a cutting-edge testing approach utilizing working responses to detect both subject-specific and predictor-specific functional random effects. Monte Carlo simulation studies demonstrate the effectiveness of our proposed testing method. Application of the proposed methods to the Schizophrenia data reveals significant subject-specific effects of human brain activity in the frontal zone (Fz) and the central zone (Cz), providing valuable insights into the potential variations among individuals, from healthy controls to those diagnosed with Schizophrenia.},
  archive      = {J_BIOSTAT},
  author       = {Rui, Rongxiang and Xiong, Wei and Pan, Jianxin and Tian, Maozai},
  doi          = {10.1093/biostatistics/kxae049},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae049},
  shortjournal = {Biostatistics},
  title        = {Unveiling schizophrenia: A study with generalized functional linear mixed model via the investigation of functional random effects},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian thresholded modeling for integrating brain node and
network predictors. <em>BIOSTAT</em>, <em>26</em>(1), kxae048. (<a
href="https://doi.org/10.1093/biostatistics/kxae048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Progress in neuroscience has provided unprecedented opportunities to advance our understanding of brain alterations and their correspondence to phenotypic profiles. With data collected from various imaging techniques, studies have integrated different types of information ranging from brain structure, function, or metabolism. More recently, an emerging way to categorize imaging traits is through a metric hierarchy, including localized node-level measurements and interactive network-level metrics. However, limited research has been conducted to integrate these different hierarchies and achieve a better understanding of the neurobiological mechanisms and communications. In this work, we address this literature gap by proposing a Bayesian regression model under both vector-variate and matrix-variate predictors. To characterize the interplay between different predicting components, we propose a set of biologically plausible prior models centered on an innovative joint thresholded prior. This captures the coupling and grouping effect of signal patterns, as well as their spatial contiguity across brain anatomy. By developing a posterior inference, we can identify and quantify the uncertainty of signaling node- and network-level neuromarkers, as well as their predictive mechanism for phenotypic outcomes. Through extensive simulations, we demonstrate that our proposed method outperforms the alternative approaches substantially in both out-of-sample prediction and feature selection. By implementing the model to study children’s general mental abilities, we establish a powerful predictive mechanism based on the identified task contrast traits and resting-state sub-networks.},
  archive      = {J_BIOSTAT},
  author       = {Sun, Zhe and Xu, Wanwan and Li, Tianxi and Kang, Jian and Alanis-Lobato, Gregorio and Zhao, Yize},
  doi          = {10.1093/biostatistics/kxae048},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae048},
  shortjournal = {Biostatistics},
  title        = {Bayesian thresholded modeling for integrating brain node and network predictors},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BAMITA: Bayesian multiple imputation for tensor arrays.
<em>BIOSTAT</em>, <em>26</em>(1), kxae047. (<a
href="https://doi.org/10.1093/biostatistics/kxae047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data increasingly take the form of a multi-way array, or tensor, in several biomedical domains. Such tensors are often incompletely observed. For example, we are motivated by longitudinal microbiome studies in which several timepoints are missing for several subjects. There is a growing literature on missing data imputation for tensors. However, existing methods give a point estimate for missing values without capturing uncertainty. We propose a multiple imputation approach for tensors in a flexible Bayesian framework, that yields realistic simulated values for missing entries and can propagate uncertainty through subsequent analyses. Our model uses efficient and widely applicable conjugate priors for a CANDECOMP/PARAFAC (CP) factorization, with a separable residual covariance structure. This approach is shown to perform well with respect to both imputation accuracy and uncertainty calibration, for scenarios in which either single entries or entire fibers of the tensor are missing. For two microbiome applications, it is shown to accurately capture uncertainty in the full microbiome profile at missing timepoints and used to infer trends in species diversity for the population. Documented R code to perform our multiple imputation approach is available at https://github.com/lockEF/MultiwayImputation .},
  archive      = {J_BIOSTAT},
  author       = {Jiang, Ziren and Li, Gen and Lock, Eric F},
  doi          = {10.1093/biostatistics/kxae047},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae047},
  shortjournal = {Biostatistics},
  title        = {BAMITA: Bayesian multiple imputation for tensor arrays},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing for a difference in means of a single feature after
clustering. <em>BIOSTAT</em>, <em>26</em>(1), kxae046. (<a
href="https://doi.org/10.1093/biostatistics/kxae046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many applications, it is critical to interpret and validate groups of observations obtained via clustering. A common interpretation and validation approach involves testing differences in feature means between observations in two estimated clusters. In this setting, classical hypothesis tests lead to an inflated Type I error rate. To overcome this problem, we propose a new test for the difference in means in a single feature between a pair of clusters obtained using hierarchical or k -means clustering. The test controls the selective Type I error rate in finite samples and can be efficiently computed. We further illustrate the validity and power of our proposal in simulation and demonstrate its use on single-cell RNA-sequencing data.},
  archive      = {J_BIOSTAT},
  author       = {Chen, Yiqun T and Gao, Lucy L},
  doi          = {10.1093/biostatistics/kxae046},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae046},
  shortjournal = {Biostatistics},
  title        = {Testing for a difference in means of a single feature after clustering},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian subtyping for multi-state brain functional
connectome with application on preadolescent brain cognition.
<em>BIOSTAT</em>, <em>26</em>(1), kxae045. (<a
href="https://doi.org/10.1093/biostatistics/kxae045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Converging evidence indicates that the heterogeneity of cognitive profiles may arise through detectable alternations in brain functional connectivity. Despite an unprecedented opportunity to uncover neurobiological subtypes through clustering or subtyping analyses on multi-state functional connectivity, few existing approaches are applicable to accommodate the network topology and unique biological architecture. To address this issue, we propose an innovative Bayesian nonparametric network-variate clustering analysis to uncover subgroups of individuals with homogeneous brain functional network patterns under multiple cognitive states. In light of the existing neuroscience literature, we assume there are unknown state-specific modular structures within functional connectivity. Concurrently, we identify informative network features essential for defining subtypes. To further facilitate practical use, we develop a computationally efficient variational inference algorithm to approximate posterior inference with satisfactory estimation accuracy. Extensive simulations show the superiority of our method. We apply the method to the Adolescent Brain Cognitive Development (ABCD) study, and identify neurodevelopmental subtypes and brain sub-network phenotypes under each state to signal neurobiological heterogeneity, suggesting promising directions for further exploration and investigation in neuroscience.},
  archive      = {J_BIOSTAT},
  author       = {Chen, Tianqi and Zhao, Hongyu and Tan, Chichun and Constable, Todd and Yip, Sarah and Zhao, Yize},
  doi          = {10.1093/biostatistics/kxae045},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae045},
  shortjournal = {Biostatistics},
  title        = {Bayesian subtyping for multi-state brain functional connectome with application on preadolescent brain cognition},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recoverability of causal effects under presence of missing
data: A longitudinal case study. <em>BIOSTAT</em>, <em>26</em>(1),
kxae044. (<a
href="https://doi.org/10.1093/biostatistics/kxae044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data in multiple variables is a common issue. We investigate the applicability of the framework of graphical models for handling missing data to a complex longitudinal pharmacological study of children with HIV treated with an efavirenz-based regimen as part of the CHAPAS-3 trial. Specifically, we examine whether the causal effects of interest, defined through static interventions on multiple continuous variables, can be recovered (estimated consistently) from the available data only. So far, no general algorithms are available to decide on recoverability, and decisions have to be made on a case-by-case basis. We emphasize the sensitivity of recoverability to even the smallest changes in the graph structure, and present recoverability results for three plausible missingness-directed acyclic graphs (m-DAGs) in the CHAPAS-3 study, informed by clinical knowledge. Furthermore, we propose the concept of a “closed missingness mechanism”: if missing data are generated based on this mechanism, an available case analysis is admissible for consistent estimation of any statistical or causal estimand, even if data are missing not at random. Both simulations and theoretical considerations demonstrate how, in the assumed MNAR setting of our study, a complete or available case analysis can be superior to multiple imputation, and estimation results vary depending on the assumed missingness DAG. Our analyses demonstrate an innovative application of missingness DAGs to complex longitudinal real-world data, while highlighting the sensitivity of the results with respect to the assumed causal model.},
  archive      = {J_BIOSTAT},
  author       = {Holovchak, Anastasiia and McIlleron, Helen and Denti, Paolo and Schomaker, Michael},
  doi          = {10.1093/biostatistics/kxae044},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae044},
  shortjournal = {Biostatistics},
  title        = {Recoverability of causal effects under presence of missing data: A longitudinal case study},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast standard error estimation for joint models of
longitudinal and time-to-event data based on stochastic EM algorithms.
<em>BIOSTAT</em>, <em>26</em>(1), kxae043. (<a
href="https://doi.org/10.1093/biostatistics/kxae043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maximum likelihood inference can often become computationally intensive when performing joint modeling of longitudinal and time-to-event data, due to the intractable integrals in the joint likelihood function. The computational challenges escalate further when modeling HIV-1 viral load data, owing to the nonlinear trajectories and the presence of left-censored data resulting from the assay’s lower limit of quantification. In this paper, for a joint model comprising a nonlinear mixed-effect model and a Cox Proportional Hazards model, we develop a computationally efficient Stochastic EM (StEM) algorithm for parameter estimation. Furthermore, we propose a novel technique for fast standard error estimation, which directly estimates standard errors from the results of StEM iterations and is broadly applicable to various joint modeling settings, such as those containing generalized linear mixed-effect models, parametric survival models, or joint models with more than two submodels. We evaluate the performance of the proposed methods through simulation studies and apply them to HIV-1 viral load data from six AIDS Clinical Trials Group studies to characterize viral rebound trajectories following the interruption of antiretroviral therapy (ART), accounting for the informative duration of off-ART periods.},
  archive      = {J_BIOSTAT},
  author       = {Yu, Tingting and Wu, Lang and Bosch, Ronald J and Smith, Davey M and Wang, Rui},
  doi          = {10.1093/biostatistics/kxae043},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae043},
  shortjournal = {Biostatistics},
  title        = {Fast standard error estimation for joint models of longitudinal and time-to-event data based on stochastic EM algorithms},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The impact of coarsening an exposure on partial
identifiability in instrumental variable settings. <em>BIOSTAT</em>,
<em>26</em>(1), kxae042. (<a
href="https://doi.org/10.1093/biostatistics/kxae042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In instrumental variable (IV) settings, such as imperfect randomized trials and observational studies with Mendelian randomization, one may encounter a continuous exposure, the causal effect of which is not of true interest. Instead, scientific interest may lie in a coarsened version of this exposure. Although there is a lengthy literature on the impact of coarsening of an exposure with several works focusing specifically on IV settings, all methods proposed in this literature require parametric assumptions. Instead, just as in the standard IV setting, one can consider partial identification via bounds making no parametric assumptions. This was first pointed out in Alexander Balke’s PhD dissertation. We extend and clarify his work and derive novel bounds in several settings, including for a three-level IV, which will most likely be the case in Mendelian randomization. We demonstrate our findings in two real data examples, a randomized trial for peanut allergy in infants and a Mendelian randomization setting investigating the effect of homocysteine on cardiovascular disease.},
  archive      = {J_BIOSTAT},
  author       = {Gabriel, Erin E and Sachs, Michael C and Sjölander, Arvid},
  doi          = {10.1093/biostatistics/kxae042},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae042},
  shortjournal = {Biostatistics},
  title        = {The impact of coarsening an exposure on partial identifiability in instrumental variable settings},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shared parameter modeling of longitudinal data allowing for
possibly informative visiting process and terminal event.
<em>BIOSTAT</em>, <em>26</em>(1), kxae041. (<a
href="https://doi.org/10.1093/biostatistics/kxae041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint modeling of longitudinal and time-to-event data, particularly through shared parameter models (SPMs), is a common approach for handling longitudinal marker data with an informative terminal event. A critical but often neglected assumption in this context is that the visiting/observation process is noninformative, depending solely on past marker values and visit times. When this assumption fails, the visiting process becomes informative, resulting potentially to biased SPM estimates. Existing methods generally rely on a conditional independence assumption, positing that the marker model, visiting process, and time-to-event model are independent given shared or correlated random effects. Moreover, they are typically built on an intensity-based visiting process using calendar time. This study introduces a unified approach for jointly modeling a normally distributed marker, the visiting process, and time-to-event data in the form of competing risks. Our model conditions on the history of observed marker values, prior visit times, the marker’s random effects, and possibly a frailty term independent of the random effects. While our approach aligns with the shared-parameter framework, it does not presume conditional independence between the processes. Additionally, the visiting process can be defined on either a gap time scale, via proportional hazard models, or a calendar time scale, via proportional intensity models. Through extensive simulation studies, we assess the performance of our proposed methodology. We demonstrate that disregarding an informative visiting process can yield significantly biased marker estimates. However, misspecification of the visiting process can also lead to biased estimates. The gap time formulation exhibits greater robustness compared to the intensity-based model when the visiting process is misspecified. In general, enriching the visiting process with prior visit history enhances performance. We further apply our methodology to real longitudinal data from HIV, where visit frequency varies substantially among individuals.},
  archive      = {J_BIOSTAT},
  author       = {Thomadakis, Christos and Meligkotsidou, Loukia and Pantazis, Nikos and Touloumi, Giota},
  doi          = {10.1093/biostatistics/kxae041},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae041},
  shortjournal = {Biostatistics},
  title        = {Shared parameter modeling of longitudinal data allowing for possibly informative visiting process and terminal event},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Functional quantile principal component analysis.
<em>BIOSTAT</em>, <em>26</em>(1), kxae040. (<a
href="https://doi.org/10.1093/biostatistics/kxae040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces functional quantile principal component analysis (FQPCA), a dimensionality reduction technique that extends the concept of functional principal components analysis (FPCA) to the examination of participant-specific quantiles curves. Our approach borrows strength across participants to estimate patterns in quantiles, and uses participant-level data to estimate loadings on those patterns. As a result, FQPCA is able to capture shifts in the scale and distribution of data that affect participant-level quantile curves, and is also a robust methodology suitable for dealing with outliers, heteroscedastic data or skewed data. The need for such methodology is exemplified by physical activity data collected using wearable devices. Participants often differ in the timing and intensity of physical activity behaviors, and capturing information beyond the participant-level expected value curves produced by FPCA is necessary for a robust quantification of diurnal patterns of activity. We illustrate our methods using accelerometer data from the National Health and Nutrition Examination Survey, and produce participant-level 10%, 50%, and 90% quantile curves over 24 h of activity. The proposed methodology is supported by simulation results, and is available as an R package.},
  archive      = {J_BIOSTAT},
  author       = {Méndez-Civieta, Álvaro and Wei, Ying and Diaz, Keith M. and Goldsmith, Jeff},
  doi          = {10.1093/biostatistics/kxae040},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae040},
  shortjournal = {Biostatistics},
  title        = {Functional quantile principal component analysis},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Selection processes, transportability, and failure time
analysis in life history studies. <em>BIOSTAT</em>, <em>26</em>(1),
kxae039. (<a
href="https://doi.org/10.1093/biostatistics/kxae039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In life history analysis of data from cohort studies, it is important to address the process by which participants are identified and selected. Many health studies select or enrol individuals based on whether they have experienced certain health related events, for example, disease diagnosis or some complication from disease. Standard methods of analysis rely on assumptions concerning the independence of selection and a person’s prospective life history process, given their prior history. Violations of such assumptions are common, however, and can bias estimation of process features. This has implications for the internal and external validity of cohort studies, and for the transportabilty of results to a population. In this paper, we study failure time analysis by proposing a joint model for the cohort selection process and the failure process of interest. This allows us to address both independence assumptions and the transportability of study results. It is shown that transportability cannot be guaranteed in the absence of auxiliary information on the population. Conditions that produce dependent selection and types of auxiliary data are discussed and illustrated in numerical studies. The proposed framework is applied to a study of the risk of psoriatic arthritis in persons with psoriasis.},
  archive      = {J_BIOSTAT},
  author       = {Cook, Richard J and Lawless, Jerald F},
  doi          = {10.1093/biostatistics/kxae039},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae039},
  shortjournal = {Biostatistics},
  title        = {Selection processes, transportability, and failure time analysis in life history studies},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A scalable two-stage bayesian approach accounting for
exposure measurement error in environmental epidemiology.
<em>BIOSTAT</em>, <em>26</em>(1), kxae038. (<a
href="https://doi.org/10.1093/biostatistics/kxae038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accounting for exposure measurement errors has been recognized as a crucial problem in environmental epidemiology for over two decades. Bayesian hierarchical models offer a coherent probabilistic framework for evaluating associations between environmental exposures and health effects, which take into account exposure measurement errors introduced by uncertainty in the estimated exposure as well as spatial misalignment between the exposure and health outcome data. While two-stage Bayesian analyses are often regarded as a good alternative to fully Bayesian analyses when joint estimation is not feasible, there has been minimal research on how to properly propagate uncertainty from the first-stage exposure model to the second-stage health model, especially in the case of a large number of participant locations along with spatially correlated exposures. We propose a scalable two-stage Bayesian approach, called a sparse multivariate normal (sparse MVN) prior approach, based on the Vecchia approximation for assessing associations between exposure and health outcomes in environmental epidemiology. We compare its performance with existing approaches through simulation. Our sparse MVN prior approach shows comparable performance with the fully Bayesian approach, which is a gold standard but is impossible to implement in some cases. We investigate the association between source-specific exposures and pollutant (nitrogen dioxide [NO 2 ])-specific exposures and birth weight of full-term infants born in 2012 in Harris County, Texas, using several approaches, including the newly developed method.},
  archive      = {J_BIOSTAT},
  author       = {Lee, Changwoo J and Symanski, Elaine and Rammah, Amal and Kang, Dong Hun and Hopke, Philip K and Park, Eun Sug},
  doi          = {10.1093/biostatistics/kxae038},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae038},
  shortjournal = {Biostatistics},
  title        = {A scalable two-stage bayesian approach accounting for exposure measurement error in environmental epidemiology},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Speeding up interval estimation for r2-based mediation
effect of high-dimensional mediators via cross-fitting.
<em>BIOSTAT</em>, <em>26</em>(1), kxae037. (<a
href="https://doi.org/10.1093/biostatistics/kxae037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis is a useful tool in investigating how molecular phenotypes such as gene expression mediate the effect of exposure on health outcomes. However, commonly used mean-based total mediation effect measures may suffer from cancellation of component-wise mediation effects in opposite directions in the presence of high-dimensional omics mediators. To overcome this limitation, we recently proposed a variance-based R-squared total mediation effect measure that relies on the computationally intensive nonparametric bootstrap for confidence interval estimation. In the work described herein, we formulated a more efficient two-stage, cross-fitted estimation procedure for the R 2 measure. To avoid potential bias, we performed iterative Sure Independence Screening (iSIS) in two subsamples to exclude the non-mediators, followed by ordinary least squares regressions for the variance estimation. We then constructed confidence intervals based on the newly derived closed-form asymptotic distribution of the R 2 measure. Extensive simulation studies demonstrated that this proposed procedure is much more computationally efficient than the resampling-based method, with comparable coverage probability. Furthermore, when applied to the Framingham Heart Study, the proposed method replicated the established finding of gene expression mediating age-related variation in systolic blood pressure and identified the role of gene expression profiles in the relationship between sex and high-density lipoprotein cholesterol level. The proposed estimation procedure is implemented in R package CFR2M .},
  archive      = {J_BIOSTAT},
  author       = {Xu, Zhichao and Li, Chunlin and Chi, Sunyi and Yang, Tianzhong and Wei, Peng},
  doi          = {10.1093/biostatistics/kxae037},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae037},
  shortjournal = {Biostatistics},
  title        = {Speeding up interval estimation for r2-based mediation effect of high-dimensional mediators via cross-fitting},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic and concordance-assisted learning for risk
stratification with application to alzheimer’s disease.
<em>BIOSTAT</em>, <em>26</em>(1), kxae036. (<a
href="https://doi.org/10.1093/biostatistics/kxae036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic prediction models capable of retaining accuracy by evolving over time could play a significant role for monitoring disease progression in clinical practice. In biomedical studies with long-term follow up, participants are often monitored through periodic clinical visits with repeat measurements until an occurrence of the event of interest (e.g. disease onset) or the study end. Acknowledging the dynamic nature of disease risk and clinical information contained in the longitudinal markers, we propose an innovative concordance-assisted learning algorithm to derive a real-time risk stratification score. The proposed approach bypasses the need to fit regression models, such as joint models of the longitudinal markers and time-to-event outcome, and hence enjoys the desirable property of model robustness. Simulation studies confirmed that the proposed method has satisfactory performance in dynamically monitoring the risk of developing disease and differentiating high-risk and low-risk population over time. We apply the proposed method to the Alzheimer’s Disease Neuroimaging Initiative data and develop a dynamic risk score of Alzheimer’s Disease for patients with mild cognitive impairment using multiple longitudinal markers and baseline prognostic factors.},
  archive      = {J_BIOSTAT},
  author       = {Li, Wen and Li, Ruosha and Feng, Ziding and Ning, Jing and For the Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1093/biostatistics/kxae036},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae036},
  shortjournal = {Biostatistics},
  title        = {Dynamic and concordance-assisted learning for risk stratification with application to alzheimer’s disease},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the addams family of discrete frailty distributions for
modeling multivariate case i interval-censored data. <em>BIOSTAT</em>,
<em>26</em>(1), kxae035. (<a
href="https://doi.org/10.1093/biostatistics/kxae035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random effect models for time-to-event data, also known as frailty models, provide a conceptually appealing way of quantifying association between survival times and of representing heterogeneities resulting from factors which may be difficult or impossible to measure. In the literature, the random effect is usually assumed to have a continuous distribution. However, in some areas of application, discrete frailty distributions may be more appropriate. The present paper is about the implementation and interpretation of the Addams family of discrete frailty distributions. We propose methods of estimation for this family of densities in the context of shared frailty models for the hazard rates for case I interval-censored data. Our optimization framework allows for stratification of random effect distributions by covariates. We highlight interpretational advantages of the Addams family of discrete frailty distributions and the K -point distribution as compared to other frailty distributions. A unique feature of the Addams family and the K -point distribution is that the support of the frailty distribution depends on its parameters. This feature is best exploited by imposing a model on the distributional parameters, resulting in a model with non-homogeneous covariate effects that can be analyzed using standard measures such as the hazard ratio. Our methods are illustrated with applications to multivariate case I interval-censored infection data.},
  archive      = {J_BIOSTAT},
  author       = {Bardo, Maximilian and Hens, Niel and Unkel, Steffen},
  doi          = {10.1093/biostatistics/kxae035},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae035},
  shortjournal = {Biostatistics},
  title        = {On the addams family of discrete frailty distributions for modeling multivariate case i interval-censored data},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian pharmacokinetics integrated phase i–II design to
optimize dose-schedule regimes. <em>BIOSTAT</em>, <em>26</em>(1),
kxae034. (<a
href="https://doi.org/10.1093/biostatistics/kxae034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The schedule of administering a drug has profound impact on the toxicity and efficacy profiles of the drug through changing its pharmacokinetics (PK). PK is an innate and indispensable component of the dose-schedule optimization. Motivated by this, we propose a Bayesian PK integrated dose-schedule finding (PKIDS) design to identify the optimal dose-schedule regime by integrating PK, toxicity, and efficacy data. Based on the causal pathway that dose and schedule affect PK, which in turn affects efficacy and toxicity, we jointly model the three endpoints by first specifying a Bayesian hierarchical model for the marginal distribution of the longitudinal dose-concentration process. Conditional on the drug concentration in plasma, we jointly model toxicity and efficacy as a function of the concentration. We quantify the risk-benefit of regimes using utility—continuously updating the estimates of PK, toxicity, and efficacy based on interim data—and make adaptive decisions to assign new patients to appropriate dose-schedule regimes via adaptive randomization. The simulation study shows that the PKIDS design has desirable operating characteristics.},
  archive      = {J_BIOSTAT},
  author       = {Lu, Mengyi and Yuan, Ying and Liu, Suyu},
  doi          = {10.1093/biostatistics/kxae034},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae034},
  shortjournal = {Biostatistics},
  title        = {A bayesian pharmacokinetics integrated phase I–II design to optimize dose-schedule regimes},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HMM for discovering decision-making dynamics using
reinforcement learning experiments. <em>BIOSTAT</em>, <em>26</em>(1),
kxae033. (<a
href="https://doi.org/10.1093/biostatistics/kxae033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Major depressive disorder (MDD), a leading cause of years of life lived with disability, presents challenges in diagnosis and treatment due to its complex and heterogeneous nature. Emerging evidence indicates that reward processing abnormalities may serve as a behavioral marker for MDD. To measure reward processing, patients perform computer-based behavioral tasks that involve making choices or responding to stimulants that are associated with different outcomes, such as gains or losses in the laboratory. Reinforcement learning (RL) models are fitted to extract parameters that measure various aspects of reward processing (e.g. reward sensitivity) to characterize how patients make decisions in behavioral tasks. Recent findings suggest the inadequacy of characterizing reward learning solely based on a single RL model; instead, there may be a switching of decision-making processes between multiple strategies. An important scientific question is how the dynamics of strategies in decision-making affect the reward learning ability of individuals with MDD. Motivated by the probabilistic reward task within the Establishing Moderators and Biosignatures of Antidepressant Response in Clinical Care (EMBARC) study, we propose a novel RL-HMM (hidden Markov model) framework for analyzing reward-based decision-making. Our model accommodates decision-making strategy switching between two distinct approaches under an HMM: subjects making decisions based on the RL model or opting for random choices. We account for continuous RL state space and allow time-varying transition probabilities in the HMM. We introduce a computationally efficient Expectation-maximization (EM) algorithm for parameter estimation and use a nonparametric bootstrap for inference. Extensive simulation studies validate the finite-sample performance of our method. We apply our approach to the EMBARC study to show that MDD patients are less engaged in RL compared to the healthy controls, and engagement is associated with brain activities in the negative affect circuitry during an emotional conflict task.},
  archive      = {J_BIOSTAT},
  author       = {Guo, Xingche and Zeng, Donglin and Wang, Yuanjia},
  doi          = {10.1093/biostatistics/kxae033},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae033},
  shortjournal = {Biostatistics},
  title        = {HMM for discovering decision-making dynamics using reinforcement learning experiments},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pooling controls from nested case–control studies with the
proportional risks model. <em>BIOSTAT</em>, <em>26</em>(1), kxae032. (<a
href="https://doi.org/10.1093/biostatistics/kxae032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The standard approach to regression modeling for cause-specific hazards with prospective competing risks data specifies separate models for each failure type. An alternative proposed by Lunn and McNeil (1995) assumes the cause-specific hazards are proportional across causes. This may be more efficient than the standard approach, and allows the comparison of covariate effects across causes. In this paper, we extend Lunn and McNeil (1995) to nested case–control studies, accommodating scenarios with additional matching and non-proportionality. We also consider the case where data for different causes are obtained from different studies conducted in the same cohort. It is demonstrated that while only modest gains in efficiency are possible in full cohort analyses, substantial gains may be attained in nested case–control analyses for failure types that are relatively rare. Extensive simulation studies are conducted and real data analyses are provided using the Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial (PLCO) study.},
  archive      = {J_BIOSTAT},
  author       = {Chang, Yen and Ivanova, Anastasia and Albanes, Demetrius and Fine, Jason P and Shin, Yei Eun},
  doi          = {10.1093/biostatistics/kxae032},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae032},
  shortjournal = {Biostatistics},
  title        = {Pooling controls from nested case–control studies with the proportional risks model},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exposure proximal immune correlates analysis.
<em>BIOSTAT</em>, <em>26</em>(1), kxae031. (<a
href="https://doi.org/10.1093/biostatistics/kxae031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immune response decays over time, and vaccine-induced protection often wanes. Understanding how vaccine efficacy changes over time is critical to guiding the development and application of vaccines in preventing infectious diseases. The objective of this article is to develop statistical methods that assess the effect of decaying immune responses on the risk of disease and on vaccine efficacy, within the context of Cox regression with sparse sampling of immune responses, in a baseline-naive population. We aim to further disentangle the various aspects of the time-varying vaccine effect, whether direct on disease or mediated through immune responses. Based on time-to-event data from a vaccine efficacy trial and sparse sampling of longitudinal immune responses, we propose a weighted estimated induced likelihood approach that models the longitudinal immune response trajectory and the time to event separately. This approach assesses the effects of the decaying immune response, the peak immune response, and/or the waning vaccine effect on the risk of disease. The proposed method is applicable not only to standard randomized trial designs but also to augmented vaccine trial designs that re-vaccinate uninfected placebo recipients at the end of the standard trial period. We conducted simulation studies to evaluate the performance of our method and applied the method to analyze immune correlates from a phase III SARS-CoV-2 vaccine trial.},
  archive      = {J_BIOSTAT},
  author       = {Huang, Ying and Follmann, Dean},
  doi          = {10.1093/biostatistics/kxae031},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae031},
  shortjournal = {Biostatistics},
  title        = {Exposure proximal immune correlates analysis},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive gaussian markov random fields for child mortality
estimation. <em>BIOSTAT</em>, <em>26</em>(1), kxae030. (<a
href="https://doi.org/10.1093/biostatistics/kxae030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The under-5 mortality rate (U5MR), a critical health indicator, is typically estimated from household surveys in lower and middle income countries. Spatio-temporal disaggregation of household survey data can lead to highly variable estimates of U5MR, necessitating the usage of smoothing models which borrow information across space and time. The assumptions of common smoothing models may be unrealistic when certain time periods or regions are expected to have shocks in mortality relative to their neighbors, which can lead to oversmoothing of U5MR estimates. In this paper, we develop a spatial and temporal smoothing approach based on Gaussian Markov random field models which incorporate knowledge of these expected shocks in mortality. We demonstrate the potential for these models to improve upon alternatives not incorporating knowledge of expected shocks in a simulation study. We apply these models to estimate U5MR in Rwanda at the national level from 1985 to 2019, a time period which includes the Rwandan civil war and genocide.},
  archive      = {J_BIOSTAT},
  author       = {Aleshin-Guendel, Serge and Wakefield, Jon},
  doi          = {10.1093/biostatistics/kxae030},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae030},
  shortjournal = {Biostatistics},
  title        = {Adaptive gaussian markov random fields for child mortality estimation},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction. <em>BIOSTAT</em>, <em>26</em>(1), kxae029. (<a
href="https://doi.org/10.1093/biostatistics/kxae029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOSTAT},
  doi          = {10.1093/biostatistics/kxae029},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae029},
  shortjournal = {Biostatistics},
  title        = {Correction},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incorporating prior information in gene expression
network-based cancer heterogeneity analysis. <em>BIOSTAT</em>,
<em>26</em>(1), kxae028. (<a
href="https://doi.org/10.1093/biostatistics/kxae028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer is molecularly heterogeneous, with seemingly similar patients having different molecular landscapes and accordingly different clinical behaviors. In recent studies, gene expression networks have been shown as more effective/informative for cancer heterogeneity analysis than some simpler measures. Gene interconnections can be classified as “direct” and “indirect,” where the latter can be caused by shared genomic regulators (such as transcription factors, microRNAs, and other regulatory molecules) and other mechanisms. It has been suggested that incorporating the regulators of gene expressions in network analysis and focusing on the direct interconnections can lead to a deeper understanding of the more essential gene interconnections. Such analysis can be seriously challenged by the large number of parameters (jointly caused by network analysis, incorporation of regulators, and heterogeneity) and often weak signals. To effectively tackle this problem, we propose incorporating prior information contained in the published literature. A key challenge is that such prior information can be partial or even wrong. We develop a two-step procedure that can flexibly accommodate different levels of prior information quality. Simulation demonstrates the effectiveness of the proposed approach and its superiority over relevant competitors. In the analysis of a breast cancer dataset, findings different from the alternatives are made, and the identified sample subgroups have important clinical differences.},
  archive      = {J_BIOSTAT},
  author       = {Li, Rong and Xu, Shaodong and Li, Yang and Tang, Zuojian and Feng, Di and Cai, James and Ma, Shuangge},
  doi          = {10.1093/biostatistics/kxae028},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae028},
  shortjournal = {Biostatistics},
  title        = {Incorporating prior information in gene expression network-based cancer heterogeneity analysis},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Direct estimation and inference of higher-level correlations
from lower-level measurements with applications in gene-pathway and
proteomics studies. <em>BIOSTAT</em>, <em>26</em>(1), kxae027. (<a
href="https://doi.org/10.1093/biostatistics/kxae027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper tackles the challenge of estimating correlations between higher-level biological variables (e.g. proteins and gene pathways) when only lower-level measurements are directly observed (e.g. peptides and individual genes). Existing methods typically aggregate lower-level data into higher-level variables and then estimate correlations based on the aggregated data. However, different data aggregation methods can yield varying correlation estimates as they target different higher-level quantities. Our solution is a latent factor model that directly estimates these higher-level correlations from lower-level data without the need for data aggregation. We further introduce a shrinkage estimator to ensure the positive definiteness and improve the accuracy of the estimated correlation matrix. Furthermore, we establish the asymptotic normality of our estimator, enabling efficient computation of P -values for the identification of significant correlations. The effectiveness of our approach is demonstrated through comprehensive simulations and the analysis of proteomics and gene expression datasets. We develop the R package highcor for implementing our method.},
  archive      = {J_BIOSTAT},
  author       = {Wang, Yue and Shi, Haoran},
  doi          = {10.1093/biostatistics/kxae027},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae027},
  shortjournal = {Biostatistics},
  title        = {Direct estimation and inference of higher-level correlations from lower-level measurements with applications in gene-pathway and proteomics studies},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regression and alignment for functional data and network
topology. <em>BIOSTAT</em>, <em>26</em>(1), kxae026. (<a
href="https://doi.org/10.1093/biostatistics/kxae026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the brain, functional connections form a network whose topological organization can be described by graph-theoretic network diagnostics. These include characterizations of the community structure, such as modularity and participation coefficient, which have been shown to change over the course of childhood and adolescence. To investigate if such changes in the functional network are associated with changes in cognitive performance during development, network studies often rely on an arbitrary choice of preprocessing parameters, in particular the proportional threshold of network edges. Because the choice of parameter can impact the value of the network diagnostic, and therefore downstream conclusions, we propose to circumvent that choice by conceptualizing the network diagnostic as a function of the parameter. As opposed to a single value, a network diagnostic curve describes the connectome topology at multiple scales—from the sparsest group of the strongest edges to the entire edge set. To relate these curves to executive function and other covariates, we use scalar-on-function regression, which is more flexible than previous functional data-based models used in network neuroscience. We then consider how systematic differences between networks can manifest in misalignment of diagnostic curves, and consequently propose a supervised curve alignment method that incorporates auxiliary information from other variables. Our algorithm performs both functional regression and alignment via an iterative, penalized, and nonlinear likelihood optimization. The illustrated method has the potential to improve the interpretability and generalizability of neuroscience studies where the goal is to study heterogeneity among a mixture of function- and scalar-valued measures.},
  archive      = {J_BIOSTAT},
  author       = {Tu, Danni and Wrobel, Julia and Satterthwaite, Theodore D and Goldsmith, Jeff and Gur, Ruben C and Gur, Raquel E and Gertheiss, Jan and Bassett, Dani S and Shinohara, Russell T},
  doi          = {10.1093/biostatistics/kxae026},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae026},
  shortjournal = {Biostatistics},
  title        = {Regression and alignment for functional data and network topology},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating causal effects for binary outcomes using
per-decision inverse probability weighting. <em>BIOSTAT</em>,
<em>26</em>(1), kxae025. (<a
href="https://doi.org/10.1093/biostatistics/kxae025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-randomized trials are commonly conducted for optimizing mobile health interventions such as push notifications for behavior change. In analyzing such trials, causal excursion effects are often of primary interest, and their estimation typically involves inverse probability weighting (IPW). However, in a micro-randomized trial, additional treatments can often occur during the time window over which an outcome is defined, and this can greatly inflate the variance of the causal effect estimator because IPW would involve a product of numerous weights. To reduce variance and improve estimation efficiency, we propose two new estimators using a modified version of IPW, which we call “per-decision IPW.” The second estimator further improves efficiency using the projection idea from the semiparametric efficiency theory. These estimators are applicable when the outcome is binary and can be expressed as the maximum of a series of sub-outcomes defined over sub-intervals of time. We establish the estimators’ consistency and asymptotic normality. Through simulation studies and real data applications, we demonstrate substantial efficiency improvement of the proposed estimator over existing estimators. The new estimators can be used to improve the precision of primary and secondary analyses for micro-randomized trials with binary outcomes.},
  archive      = {J_BIOSTAT},
  author       = {Bao, Yihan and Bell, Lauren and Williamson, Elizabeth and Garnett, Claire and Qian, Tianchen},
  doi          = {10.1093/biostatistics/kxae025},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae025},
  shortjournal = {Biostatistics},
  title        = {Estimating causal effects for binary outcomes using per-decision inverse probability weighting},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian estimation of covariate assisted principal
regression for brain functional connectivity. <em>BIOSTAT</em>,
<em>26</em>(1), kxae023. (<a
href="https://doi.org/10.1093/biostatistics/kxae023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a Bayesian reformulation of covariate-assisted principal regression for covariance matrix outcomes to identify low-dimensional components in the covariance associated with covariates. By introducing a geometric approach to the covariance matrices and leveraging Euclidean geometry, we estimate dimension reduction parameters and model covariance heterogeneity based on covariates. This method enables joint estimation and uncertainty quantification of relevant model parameters associated with heteroscedasticity. We demonstrate our approach through simulation studies and apply it to analyze associations between covariates and brain functional connectivity using data from the Human Connectome Project.},
  archive      = {J_BIOSTAT},
  author       = {Park, Hyung G},
  doi          = {10.1093/biostatistics/kxae023},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae023},
  shortjournal = {Biostatistics},
  title        = {Bayesian estimation of covariate assisted principal regression for brain functional connectivity},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A modeling framework for detecting and leveraging node-level
information in bayesian network inference. <em>BIOSTAT</em>,
<em>26</em>(1), kxae021. (<a
href="https://doi.org/10.1093/biostatistics/kxae021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian graphical models are powerful tools to infer complex relationships in high dimension, yet are often fraught with computational and statistical challenges. If exploited in a principled way, the increasing information collected alongside the data of primary interest constitutes an opportunity to mitigate these difficulties by guiding the detection of dependence structures. For instance, gene network inference may be informed by the use of publicly available summary statistics on the regulation of genes by genetic variants. Here we present a novel Gaussian graphical modeling framework to identify and leverage information on the centrality of nodes in conditional independence graphs. Specifically, we consider a fully joint hierarchical model to simultaneously infer (i) sparse precision matrices and (ii) the relevance of node-level information for uncovering the sought-after network structure. We encode such information as candidate auxiliary variables using a spike-and-slab submodel on the propensity of nodes to be hubs, which allows hypothesis-free selection and interpretation of a sparse subset of relevant variables. As efficient exploration of large posterior spaces is needed for real-world applications, we develop a variational expectation conditional maximization algorithm that scales inference to hundreds of samples, nodes and auxiliary variables. We illustrate and exploit the advantages of our approach in simulations and in a gene network study which identifies hub genes involved in biological pathways relevant to immune-mediated diseases.},
  archive      = {J_BIOSTAT},
  author       = {Xi, Xiaoyue and Ruffieux, Hélène},
  doi          = {10.1093/biostatistics/kxae021},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae021},
  shortjournal = {Biostatistics},
  title        = {A modeling framework for detecting and leveraging node-level information in bayesian network inference},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-based multifacet clustering with high-dimensional
omics applications. <em>BIOSTAT</em>, <em>26</em>(1), kxae020. (<a
href="https://doi.org/10.1093/biostatistics/kxae020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional omics data often contain intricate and multifaceted information, resulting in the coexistence of multiple plausible sample partitions based on different subsets of selected features. Conventional clustering methods typically yield only one clustering solution, limiting their capacity to fully capture all facets of cluster structures in high-dimensional data. To address this challenge, we propose a model-based multifacet clustering (MFClust) method based on a mixture of Gaussian mixture models, where the former mixture achieves facet assignment for gene features and the latter mixture determines cluster assignment of samples. We demonstrate superior facet and cluster assignment accuracy of MFClust through simulation studies. The proposed method is applied to three transcriptomic applications from postmortem brain and lung disease studies. The result captures multifacet clustering structures associated with critical clinical variables and provides intriguing biological insights for further hypothesis generation and discovery.},
  archive      = {J_BIOSTAT},
  author       = {Zong, Wei and Li, Danyang and Seney, Marianne L and Mcclung, Colleen A and Tseng, George C},
  doi          = {10.1093/biostatistics/kxae020},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae020},
  shortjournal = {Biostatistics},
  title        = {Model-based multifacet clustering with high-dimensional omics applications},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A marginal structural model for normal tissue complication
probability. <em>BIOSTAT</em>, <em>26</em>(1), kxae019. (<a
href="https://doi.org/10.1093/biostatistics/kxae019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of radiation therapy for cancer is to deliver prescribed radiation dose to the tumor while minimizing dose to the surrounding healthy tissues. To evaluate treatment plans, the dose distribution to healthy organs is commonly summarized as dose-volume histograms (DVHs). Normal tissue complication probability (NTCP) modeling has centered around making patient-level risk predictions with features extracted from the DVHs, but few have considered adapting a causal framework to evaluate the safety of alternative treatment plans. We propose causal estimands for NTCP based on deterministic and stochastic interventions, as well as propose estimators based on marginal structural models that impose bivariable monotonicity between dose, volume, and toxicity risk. The properties of these estimators are studied through simulations, and their use is illustrated in the context of radiotherapy treatment of anal canal cancer patients.},
  archive      = {J_BIOSTAT},
  author       = {Tang, Thai-Son and Liu, Zhihui and Hosni, Ali and Kim, John and Saarela, Olli},
  doi          = {10.1093/biostatistics/kxae019},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae019},
  shortjournal = {Biostatistics},
  title        = {A marginal structural model for normal tissue complication probability},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stochastic EM algorithm for partially observed stochastic
epidemics with individual heterogeneity. <em>BIOSTAT</em>,
<em>26</em>(1), kxae018. (<a
href="https://doi.org/10.1093/biostatistics/kxae018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a stochastic epidemic model progressing over dynamic networks, where infection rates are heterogeneous and may vary with individual-level covariates. The joint dynamics are modeled as a continuous-time Markov chain such that disease transmission is constrained by the contact network structure, and network evolution is in turn influenced by individual disease statuses. To accommodate partial epidemic observations commonly seen in real-world data, we propose a stochastic EM algorithm for inference, introducing key innovations that include efficient conditional samplers for imputing missing infection and recovery times which respect the dynamic contact network. Experiments on both synthetic and real datasets demonstrate that our inference method can accurately and efficiently recover model parameters and provide valuable insight at the presence of unobserved disease episodes in epidemic data.},
  archive      = {J_BIOSTAT},
  author       = {Bu, Fan and Aiello, Allison E and Volfovsky, Alexander and Xu, Jason},
  doi          = {10.1093/biostatistics/kxae018},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae018},
  shortjournal = {Biostatistics},
  title        = {Stochastic EM algorithm for partially observed stochastic epidemics with individual heterogeneity},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simultaneous clustering and estimation of networks in
multiple graphical models. <em>BIOSTAT</em>, <em>26</em>(1), kxae015.
(<a href="https://doi.org/10.1093/biostatistics/kxae015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian graphical models are widely used to study the dependence structure among variables. When samples are obtained from multiple conditions or populations, joint analysis of multiple graphical models are desired due to their capacity to borrow strength across populations. Nonetheless, existing methods often overlook the varying levels of similarity between populations, leading to unsatisfactory results. Moreover, in many applications, learning the population-level clustering structure itself is of particular interest. In this article, we develop a novel method, called S imultaneous C lustering and E stimation of N etworks via T ensor decomposition (SCENT), that simultaneously clusters and estimates graphical models from multiple populations. Precision matrices from different populations are uniquely organized as a three-way tensor array, and a low-rank sparse model is proposed for joint population clustering and network estimation. We develop a penalized likelihood method and an augmented Lagrangian algorithm for model fitting. We also establish the clustering accuracy and norm consistency of the estimated precision matrices. We demonstrate the efficacy of the proposed method with comprehensive simulation studies. The application to the Genotype-Tissue Expression multi-tissue gene expression data provides important insights into tissue clustering and gene coexpression patterns in multiple brain tissues.},
  archive      = {J_BIOSTAT},
  author       = {Li, Gen and Wang, Miaoyan},
  doi          = {10.1093/biostatistics/kxae015},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae015},
  shortjournal = {Biostatistics},
  title        = {Simultaneous clustering and estimation of networks in multiple graphical models},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A joint normal-ordinal (probit) model for ordinal and
continuous longitudinal data. <em>BIOSTAT</em>, <em>26</em>(1), kxae014.
(<a href="https://doi.org/10.1093/biostatistics/kxae014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomedical studies, continuous and ordinal longitudinal variables are frequently encountered. In many of these studies it is of interest to estimate the effect of one of these longitudinal variables on the other. Time-dependent covariates have, however, several limitations; they can, for example, not be included when the data is not collected at fixed intervals. The issues can be circumvented by implementing joint models, where two or more longitudinal variables are treated as a response and modeled with a correlated random effect. Next, by conditioning on these response(s), we can study the effect of one or more longitudinal variables on another. We propose a normal-ordinal(probit) joint model. First, we derive closed-form formulas to estimate the model-based correlations between the responses on their original scale. In addition, we derive the marginal model, where the interpretation is no longer conditional on the random effects. As a consequence, we can make predictions for a subvector of one response conditional on the other response and potentially a subvector of the history of the response. Next, we extend the approach to a high-dimensional case with more than two ordinal and/or continuous longitudinal variables. The methodology is applied to a case study where, among others, a longitudinal ordinal response is predicted with a longitudinal continuous variable.},
  archive      = {J_BIOSTAT},
  author       = {Delporte, Margaux and Molenberghs, Geert and Fieuws, Steffen and Verbeke, Geert},
  doi          = {10.1093/biostatistics/kxae014},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae014},
  shortjournal = {Biostatistics},
  title        = {A joint normal-ordinal (probit) model for ordinal and continuous longitudinal data},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A semiparametric gaussian mixture model for chest CT-based
3D blood vessel reconstruction. <em>BIOSTAT</em>, <em>26</em>(1),
kxae013. (<a
href="https://doi.org/10.1093/biostatistics/kxae013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computed tomography (CT) has been a powerful diagnostic tool since its emergence in the 1970s. Using CT data, 3D structures of human internal organs and tissues, such as blood vessels, can be reconstructed using professional software. This 3D reconstruction is crucial for surgical operations and can serve as a vivid medical teaching example. However, traditional 3D reconstruction heavily relies on manual operations, which are time-consuming, subjective, and require substantial experience. To address this problem, we develop a novel semiparametric Gaussian mixture model tailored for the 3D reconstruction of blood vessels. This model extends the classical Gaussian mixture model by enabling nonparametric variations in the component-wise parameters of interest according to voxel positions. We develop a kernel-based expectation–maximization algorithm for estimating the model parameters, accompanied by a supporting asymptotic theory. Furthermore, we propose a novel regression method for optimal bandwidth selection. Compared to the conventional cross-validation-based (CV) method, the regression method outperforms the CV method in terms of computational and statistical efficiency. In application, this methodology facilitates the fully automated reconstruction of 3D blood vessel structures with remarkable accuracy.},
  archive      = {J_BIOSTAT},
  author       = {Zeng, Qianhan and Zhou, Jing and Ji, Ying and Wang, Hansheng},
  doi          = {10.1093/biostatistics/kxae013},
  journal      = {Biostatistics},
  month        = {12},
  number       = {1},
  pages        = {kxae013},
  shortjournal = {Biostatistics},
  title        = {A semiparametric gaussian mixture model for chest CT-based 3D blood vessel reconstruction},
  volume       = {26},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Correction to: Exponential family measurement error models
for single-cell CRISPR screens. <em>BIOSTAT</em>, <em>25</em>(4), 1273.
(<a href="https://doi.org/10.1093/biostatistics/kxae022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOSTAT},
  doi          = {10.1093/biostatistics/kxae022},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {1273},
  shortjournal = {Biostatistics},
  title        = {Correction to: Exponential family measurement error models for single-cell CRISPR screens},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exponential family measurement error models for single-cell
CRISPR screens. <em>BIOSTAT</em>, <em>25</em>(4), 1254–1272. (<a
href="https://doi.org/10.1093/biostatistics/kxae010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {CRISPR genome engineering and single-cell RNA sequencing have accelerated biological discovery. Single-cell CRISPR screens unite these two technologies, linking genetic perturbations in individual cells to changes in gene expression and illuminating regulatory networks underlying diseases. Despite their promise, single-cell CRISPR screens present considerable statistical challenges. We demonstrate through theoretical and real data analyses that a standard method for estimation and inference in single-cell CRISPR screens—“thresholded regression”—exhibits attenuation bias and a bias-variance tradeoff as a function of an intrinsic, challenging-to-select tuning parameter. To overcome these difficulties, we introduce GLM-EIV (“GLM-based errors-in-variables”), a new method for single-cell CRISPR screen analysis. GLM-EIV extends the classical errors-in-variables model to responses and noisy predictors that are exponential family-distributed and potentially impacted by the same set of confounding variables. We develop a computational infrastructure to deploy GLM-EIV across hundreds of processors on clouds (e.g. Microsoft Azure) and high-performance clusters. Leveraging this infrastructure, we apply GLM-EIV to analyze two recent, large-scale, single-cell CRISPR screen datasets, yielding several new insights.},
  archive      = {J_BIOSTAT},
  author       = {Barry, Timothy and Roeder, Kathryn and Katsevich, Eugene},
  doi          = {10.1093/biostatistics/kxae010},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {1254-1272},
  shortjournal = {Biostatistics},
  title        = {Exponential family measurement error models for single-cell CRISPR screens},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tree-informed bayesian multi-source domain adaptation:
Cross-population probabilistic cause-of-death assignment using verbal
autopsy. <em>BIOSTAT</em>, <em>25</em>(4), 1233–1253. (<a
href="https://doi.org/10.1093/biostatistics/kxae005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining causes of deaths (CODs) occurred outside of civil registration and vital statistics systems is challenging. A technique called verbal autopsy (VA) is widely adopted to gather information on deaths in practice. A VA consists of interviewing relatives of a deceased person about symptoms of the deceased in the period leading to the death, often resulting in multivariate binary responses. While statistical methods have been devised for estimating the cause-specific mortality fractions (CSMFs) for a study population, continued expansion of VA to new populations (or “domains”) necessitates approaches that recognize between-domain differences while capitalizing on potential similarities. In this article, we propose such a domain-adaptive method that integrates external between-domain similarity information encoded by a prespecified rooted weighted tree. Given a cause, we use latent class models to characterize the conditional distributions of the responses that may vary by domain. We specify a logistic stick-breaking Gaussian diffusion process prior along the tree for class mixing weights with node-specific spike-and-slab priors to pool information between the domains in a data-driven way. The posterior inference is conducted via a scalable variational Bayes algorithm. Simulation studies show that the domain adaptation enabled by the proposed method improves CSMF estimation and individual COD assignment. We also illustrate and evaluate the method using a validation dataset. The article concludes with a discussion of limitations and future directions.},
  archive      = {J_BIOSTAT},
  author       = {Wu, Zhenke and Li, Zehang R and Chen, Irena and Li, Mengbing},
  doi          = {10.1093/biostatistics/kxae005},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {1233-1253},
  shortjournal = {Biostatistics},
  title        = {Tree-informed bayesian multi-source domain adaptation: Cross-population probabilistic cause-of-death assignment using verbal autopsy},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neuroimaging meta regression for coordinate based meta
analysis data with a spatial model. <em>BIOSTAT</em>, <em>25</em>(4),
1210–1232. (<a
href="https://doi.org/10.1093/biostatistics/kxae024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coordinate-based meta-analysis combines evidence from a collection of neuroimaging studies to estimate brain activation. In such analyses, a key practical challenge is to find a computationally efficient approach with good statistical interpretability to model the locations of activation foci. In this article, we propose a generative coordinate-based meta-regression (CBMR) framework to approximate a smooth activation intensity function and investigate the effect of study-level covariates (e.g. year of publication, sample size). We employ a spline parameterization to model the spatial structure of brain activation and consider four stochastic models for modeling the random variation in foci. To examine the validity of CBMR, we estimate brain activation on 20 meta-analytic datasets, conduct spatial homogeneity tests at the voxel level, and compare the results to those generated by existing kernel-based and model-based approaches. Keywords: generalized linear models; meta-analysis; spatial statistics; statistical modeling.},
  archive      = {J_BIOSTAT},
  author       = {Yu, Yifan and Lobo, Rosario Pintos and Riedel, Michael Cody and Bottenhorn, Katherine and Laird, Angela R and Nichols, Thomas E},
  doi          = {10.1093/biostatistics/kxae024},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {1210-1232},
  shortjournal = {Biostatistics},
  title        = {Neuroimaging meta regression for coordinate based meta analysis data with a spatial model},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian mixed model inference for genetic association under
related samples with brain network phenotype. <em>BIOSTAT</em>,
<em>25</em>(4), 1195–1209. (<a
href="https://doi.org/10.1093/biostatistics/kxae008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genetic association studies for brain connectivity phenotypes have gained prominence due to advances in noninvasive imaging techniques and quantitative genetics. Brain connectivity traits, characterized by network configurations and unique biological structures, present distinct challenges compared to other quantitative phenotypes. Furthermore, the presence of sample relatedness in the most imaging genetics studies limits the feasibility of adopting existing network-response modeling. In this article, we fill this gap by proposing a Bayesian network-response mixed-effect model that considers a network-variate phenotype and incorporates population structures including pedigrees and unknown sample relatedness. To accommodate the inherent topological architecture associated with the genetic contributions to the phenotype, we model the effect components via a set of effect network configurations and impose an inter-network sparsity and intra-network shrinkage to dissect the phenotypic network configurations affected by the risk genetic variant. A Markov chain Monte Carlo (MCMC) algorithm is further developed to facilitate uncertainty quantification. We evaluate the performance of our model through extensive simulations. By further applying the method to study, the genetic bases for brain structural connectivity using data from the Human Connectome Project with excessive family structures, we obtain plausible and interpretable results. Beyond brain connectivity genetic studies, our proposed model also provides a general linear mixed-effect regression framework for network-variate outcomes.},
  archive      = {J_BIOSTAT},
  author       = {Tian, Xinyuan and Wang, Yiting and Wang, Selena and Zhao, Yi and Zhao, Yize},
  doi          = {10.1093/biostatistics/kxae008},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {1195-1209},
  shortjournal = {Biostatistics},
  title        = {Bayesian mixed model inference for genetic association under related samples with brain network phenotype},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Functional support vector machine. <em>BIOSTAT</em>,
<em>25</em>(4), 1178–1194. (<a
href="https://doi.org/10.1093/biostatistics/kxae007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear and generalized linear scalar-on-function modeling have been commonly used to understand the relationship between a scalar response variable (e.g. continuous, binary outcomes) and functional predictors. Such techniques are sensitive to model misspecification when the relationship between the response variable and the functional predictors is complex. On the other hand, support vector machines (SVMs) are among the most robust prediction models but do not take account of the high correlations between repeated measurements and cannot be used for irregular data. In this work, we propose a novel method to integrate functional principal component analysis with SVM techniques for classification and regression to account for the continuous nature of functional data and the nonlinear relationship between the scalar response variable and the functional predictors. We demonstrate the performance of our method through extensive simulation experiments and two real data applications: the classification of alcoholics using electroencephalography signals and the prediction of glucobrassicin concentration using near-infrared reflectance spectroscopy. Our methods especially have more advantages when the measurement errors in functional predictors are relatively large.},
  archive      = {J_BIOSTAT},
  author       = {Xie, Shanghong and Ogden, R Todd},
  doi          = {10.1093/biostatistics/kxae007},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {1178-1194},
  shortjournal = {Biostatistics},
  title        = {Functional support vector machine},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Projection-based two-sample inference for sparsely observed
multivariate functional data. <em>BIOSTAT</em>, <em>25</em>(4),
1156–1177. (<a
href="https://doi.org/10.1093/biostatistics/kxae004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern longitudinal studies collect multiple outcomes as the primary endpoints to understand the complex dynamics of the diseases. Oftentimes, especially in clinical trials, the joint variation among the multidimensional responses plays a significant role in assessing the differential characteristics between two or more groups, rather than drawing inferences based on a single outcome. We develop a projection-based two-sample significance test to identify the population-level difference between the multivariate profiles observed under a sparse longitudinal design. The methodology is built upon widely adopted multivariate functional principal component analysis to reduce the dimension of the infinite-dimensional multi-modal functions while preserving the dynamic correlation between the components. The test applies to a wide class of (non-stationary) covariance structures of the response, and it detects a significant group difference based on a single p-value, thereby overcoming the issue of adjusting for multiple p-values that arise due to comparing the means in each of components separately. Finite-sample numerical studies demonstrate that the test maintains the type-I error, and is powerful to detect significant group differences, compared to the state-of-the-art testing procedures. The test is carried out on two significant longitudinal studies for Alzheimer’s disease and Parkinson’s disease (PD) patients, namely, TOMMORROW study of individuals at high risk of mild cognitive impairment to detect differences in the cognitive test scores between the pioglitazone and the placebo groups, and Azillect study to assess the efficacy of rasagiline as a potential treatment to slow down the progression of PD.},
  archive      = {J_BIOSTAT},
  author       = {Koner, Salil and Luo, Sheng},
  doi          = {10.1093/biostatistics/kxae004},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {1156-1177},
  shortjournal = {Biostatistics},
  title        = {Projection-based two-sample inference for sparsely observed multivariate functional data},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating dynamic and predictive discrimination for
recurrent event models: Use of a time-dependent c-index.
<em>BIOSTAT</em>, <em>25</em>(4), 1140–1155. (<a
href="https://doi.org/10.1093/biostatistics/kxad031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interest in analyzing recurrent event data has increased over the past few decades. One essential aspect of a risk prediction model for recurrent event data is to accurately distinguish individuals with different risks of developing a recurrent event. Although the concordance index (C-index) effectively evaluates the overall discriminative ability of a regression model for recurrent event data, a local measure is also desirable to capture dynamic performance of the regression model over time. Therefore, in this study, we propose a time-dependent C-index measure for inferring the model’s discriminative ability locally. We formulated the C-index as a function of time using a flexible parametric model and constructed a concordance-based likelihood for estimation and inference. We adapted a perturbation-resampling procedure for variance estimation. Extensive simulations were conducted to investigate the proposed time-dependent C-index’s finite-sample performance and estimation procedure. We applied the time-dependent C-index to three regression models of a study of re-hospitalization in patients with colorectal cancer to evaluate the models’ discriminative capability.},
  archive      = {J_BIOSTAT},
  author       = {Wang, Jian and Jiang, Xinyang and Ning, Jing},
  doi          = {10.1093/biostatistics/kxad031},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {1140-1155},
  shortjournal = {Biostatistics},
  title        = {Evaluating dynamic and predictive discrimination for recurrent event models: Use of a time-dependent C-index},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Similarity-based multimodal regression. <em>BIOSTAT</em>,
<em>25</em>(4), 1122–1139. (<a
href="https://doi.org/10.1093/biostatistics/kxad033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To better understand complex human phenotypes, large-scale studies have increasingly collected multiple data modalities across domains such as imaging, mobile health, and physical activity. The properties of each data type often differ substantially and require either separate analyses or extensive processing to obtain comparable features for a combined analysis. Multimodal data fusion enables certain analyses on matrix-valued and vector-valued data, but it generally cannot integrate modalities of different dimensions and data structures. For a single data modality, multivariate distance matrix regression provides a distance-based framework for regression accommodating a wide range of data types. However, no distance-based method exists to handle multiple complementary types of data. We propose a novel distance-based regression model, which we refer to as Similarity-based Multimodal Regression (SiMMR), that enables simultaneous regression of multiple modalities through their distance profiles. We demonstrate through simulation, imaging studies, and longitudinal mobile health analyses that our proposed method can detect associations between clinical variables and multimodal data of differing properties and dimensionalities, even with modest sample sizes. We perform experiments to evaluate several different test statistics and provide recommendations for applying our method across a broad range of scenarios.},
  archive      = {J_BIOSTAT},
  author       = {Chen, Andrew A and Weinstein, Sarah M and Adebimpe, Azeez and Gur, Ruben C and Gur, Raquel E and Merikangas, Kathleen R and Satterthwaite, Theodore D and Shinohara, Russell T and Shou, Haochang},
  doi          = {10.1093/biostatistics/kxad033},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {1122-1139},
  shortjournal = {Biostatistics},
  title        = {Similarity-based multimodal regression},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Signal detection statistics of adverse drug events in
hierarchical structure for matched case–control data. <em>BIOSTAT</em>,
<em>25</em>(4), 1112–1121. (<a
href="https://doi.org/10.1093/biostatistics/kxad029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tree-based scan statistic is a data mining method used to identify signals of adverse drug reactions in a database of spontaneous reporting systems. It is particularly beneficial when dealing with hierarchical data structures. One may use a retrospective case–control study design from spontaneous reporting systems (SRS) to investigate whether a specific adverse event of interest is associated with certain drugs. However, the existing Bernoulli model of the tree-based scan statistic may not be suitable as it fails to adequately account for dependencies within matched pairs. In this article, we propose signal detection statistics for matched case–control data based on McNemar’s test, Wald test for conditional logistic regression, and the likelihood ratio test for a multinomial distribution. Through simulation studies, we demonstrate that our proposed methods outperform the existing approach in terms of the type I error rate, power, sensitivity, and false detection rate. To illustrate our proposed approach, we applied the three methods and the existing method to detect drug signals for dizziness-related adverse events related to antihypertensive drugs using the database of the Korea Adverse Event Reporting System.},
  archive      = {J_BIOSTAT},
  author       = {Heo, Seok-Jae and Jeong, Sohee and Jung, Dagyeom and Jung, Inkyung},
  doi          = {10.1093/biostatistics/kxad029},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {1112-1121},
  shortjournal = {Biostatistics},
  title        = {Signal detection statistics of adverse drug events in hierarchical structure for matched case–control data},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identifying predictors of resilience to stressors in
single-arm studies of pre–post change. <em>BIOSTAT</em>, <em>25</em>(4),
1094–1111. (<a
href="https://doi.org/10.1093/biostatistics/kxad018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many older adults experience a major stressor at some point in their lives. The ability to recover well after a major stressor is known as resilience. An important goal of geriatric research is to identify factors that influence resilience to stressors. Studies of resilience in older adults are typically conducted with a single-arm where everyone experiences the stressor. The simplistic approach of regressing change versus baseline yields biased estimates due to mathematical coupling and regression to the mean (RTM). We develop a method to correct the bias. We extend the method to include covariates. Our approach considers a counterfactual control group and involves sensitivity analyses to evaluate different settings of control group parameters. Only minimal distributional assumptions are required. Simulation studies demonstrate the validity of the method. We illustrate the method using a large, registry of older adults ( N = 7239) who underwent total knee replacement (TKR). We demonstrate how external data can be utilized to constrain the sensitivity analysis. Naive analyses implicated several treatment effect modifiers including baseline function, age, body-mass index (BMI), gender, number of comorbidities, income, and race. Corrected analysis revealed that baseline (pre-stressor) function was not strongly linked to recovery after TKR and among the covariates, only age and number of comorbidities were consistently and negatively associated with post-stressor recovery in all functional domains. Correction of mathematical coupling and RTM is necessary for drawing valid inferences regarding the effect of covariates and baseline status on pre–post change. Our method provides a simple estimator to this end.},
  archive      = {J_BIOSTAT},
  author       = {Varadhan, Ravi and Zhu, Jiafeng and Bandeen-Roche, Karen},
  doi          = {10.1093/biostatistics/kxad018},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {1094-1111},
  shortjournal = {Biostatistics},
  title        = {Identifying predictors of resilience to stressors in single-arm studies of pre–post change},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DifferentialRegulation: A bayesian hierarchical approach to
identify differentially regulated genes. <em>BIOSTAT</em>,
<em>25</em>(4), 1079–1093. (<a
href="https://doi.org/10.1093/biostatistics/kxae017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although transcriptomics data is typically used to analyze mature spliced mRNA, recent attention has focused on jointly investigating spliced and unspliced (or precursor-) mRNA, which can be used to study gene regulation and changes in gene expression production. Nonetheless, most methods for spliced/unspliced inference (such as RNA velocity tools) focus on individual samples, and rarely allow comparisons between groups of samples (e.g. healthy vs. diseased). Furthermore, this kind of inference is challenging, because spliced and unspliced mRNA abundance is characterized by a high degree of quantification uncertainty, due to the prevalence of multi-mapping reads, ie reads compatible with multiple transcripts (or genes), and/or with both their spliced and unspliced versions. Here, we present DifferentialRegulation , a Bayesian hierarchical method to discover changes between experimental conditions with respect to the relative abundance of unspliced mRNA (over the total mRNA). We model the quantification uncertainty via a latent variable approach, where reads are allocated to their gene/transcript of origin, and to the respective splice version. We designed several benchmarks where our approach shows good performance, in terms of sensitivity and error control, vs. state-of-the-art competitors. Importantly, our tool is flexible, and works with both bulk and single-cell RNA-sequencing data. DifferentialRegulation is distributed as a Bioconductor R package.},
  archive      = {J_BIOSTAT},
  author       = {Tiberi, Simone and Meili, Joël and Cai, Peiying and Soneson, Charlotte and He, Dongze and Sarkar, Hirak and Avalos-Pacheco, Alejandra and Patro, Rob and Robinson, Mark D},
  doi          = {10.1093/biostatistics/kxae017},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {1079-1093},
  shortjournal = {Biostatistics},
  title        = {DifferentialRegulation: A bayesian hierarchical approach to identify differentially regulated genes},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast matrix completion in epigenetic methylation studies
with informative covariates. <em>BIOSTAT</em>, <em>25</em>(4),
1062–1078. (<a
href="https://doi.org/10.1093/biostatistics/kxae016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DNA methylation is an important epigenetic mark that modulates gene expression through the inhibition of transcriptional proteins binding to DNA. As in many other omics experiments, the issue of missing values is an important one, and appropriate imputation techniques are important in avoiding an unnecessary sample size reduction as well as to optimally leverage the information collected. We consider the case where relatively few samples are processed via an expensive high-density whole genome bisulfite sequencing (WGBS) strategy and a larger number of samples is processed using more affordable low-density, array-based technologies. In such cases, one can impute the low-coverage (array-based) methylation data using the high-density information provided by the WGBS samples. In this paper, we propose an efficient Linear Model of Coregionalisation with informative Covariates (LMCC) to predict missing values based on observed values and covariates. Our model assumes that at each site, the methylation vector of all samples is linked to the set of fixed factors (covariates) and a set of latent factors. Furthermore, we exploit the functional nature of the data and the spatial correlation across sites by assuming some Gaussian processes on the fixed and latent coefficient vectors, respectively. Our simulations show that the use of covariates can significantly improve the accuracy of imputed values, especially in cases where missing data contain some relevant information about the explanatory variable. We also showed that our proposed model is particularly efficient when the number of columns is much greater than the number of rows—which is usually the case in methylation data analysis. Finally, we apply and compare our proposed method with alternative approaches on two real methylation datasets, showing how covariates such as cell type, tissue type or age can enhance the accuracy of imputed values.},
  archive      = {J_BIOSTAT},
  author       = {Ribaud, Mélina and Labbe, Aurélie and Fouda, Khaled and Oualkacha, Karim},
  doi          = {10.1093/biostatistics/kxae016},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {1062-1078},
  shortjournal = {Biostatistics},
  title        = {Fast matrix completion in epigenetic methylation studies with informative covariates},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic models augmented by hierarchical data: An
application of estimating HIV epidemics at sub-national level.
<em>BIOSTAT</em>, <em>25</em>(4), 1049–1061. (<a
href="https://doi.org/10.1093/biostatistics/kxae003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic models have been successfully used in producing estimates of HIV epidemics at the national level due to their epidemiological nature and their ability to estimate prevalence, incidence, and mortality rates simultaneously. Recently, HIV interventions and policies have required more information at sub-national levels to support local planning, decision-making and resource allocation. Unfortunately, many areas lack sufficient data for deriving stable and reliable results, and this is a critical technical barrier to more stratified estimates. One solution is to borrow information from other areas within the same country. However, directly assuming hierarchical structures within the HIV dynamic models is complicated and computationally time-consuming. In this article, we propose a simple and innovative way to incorporate hierarchical information into the dynamical systems by using auxiliary data. The proposed method efficiently uses information from multiple areas within each country without increasing the computational burden. As a result, the new model improves predictive ability and uncertainty assessment.},
  archive      = {J_BIOSTAT},
  author       = {Le, Bao and Niu, Xiaoyue and Brown, Tim and Imai-Eaton, Jeffrey W},
  doi          = {10.1093/biostatistics/kxae003},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {1049-1061},
  shortjournal = {Biostatistics},
  title        = {Dynamic models augmented by hierarchical data: An application of estimating HIV epidemics at sub-national level},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian approach for investigating the pharmacogenetics
of combination antiretroviral therapy in people with HIV.
<em>BIOSTAT</em>, <em>25</em>(4), 1034–1048. (<a
href="https://doi.org/10.1093/biostatistics/kxae001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combination antiretroviral therapy (ART) with at least three different drugs has become the standard of care for people with HIV (PWH) due to its exceptional effectiveness in viral suppression. However, many ART drugs have been reported to associate with neuropsychiatric adverse effects including depression, especially when certain genetic polymorphisms exist. Pharmacogenetics is an important consideration for administering combination ART as it may influence drug efficacy and increase risk for neuropsychiatric conditions. Large-scale longitudinal HIV databases provide researchers opportunities to investigate the pharmacogenetics of combination ART in a data-driven manner. However, with more than 30 FDA-approved ART drugs, the interplay between the large number of possible ART drug combinations and genetic polymorphisms imposes statistical modeling challenges. We develop a Bayesian approach to examine the longitudinal effects of combination ART and their interactions with genetic polymorphisms on depressive symptoms in PWH. The proposed method utilizes a Gaussian process with a composite kernel function to capture the longitudinal combination ART effects by directly incorporating individuals’ treatment histories, and a Bayesian classification and regression tree to account for individual heterogeneity. Through both simulation studies and an application to a dataset from the Women’s Interagency HIV Study, we demonstrate the clinical utility of the proposed approach in investigating the pharmacogenetics of combination ART and assisting physicians to make effective individualized treatment decisions that can improve health outcomes for PWH.},
  archive      = {J_BIOSTAT},
  author       = {Jin, Wei and Ni, Yang and Spence, Amanda B and Rubin, Leah H and Xu, Yanxun},
  doi          = {10.1093/biostatistics/kxae001},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {1034-1048},
  shortjournal = {Biostatistics},
  title        = {A bayesian approach for investigating the pharmacogenetics of combination antiretroviral therapy in people with HIV},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mendelian randomization analysis using multiple biomarkers
of an underlying common exposure. <em>BIOSTAT</em>, <em>25</em>(4),
1015–1033. (<a
href="https://doi.org/10.1093/biostatistics/kxae006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mendelian randomization (MR) analysis is increasingly popular for testing the causal effect of exposures on disease outcomes using data from genome-wide association studies. In some settings, the underlying exposure, such as systematic inflammation, may not be directly observable, but measurements can be available on multiple biomarkers or other types of traits that are co-regulated by the exposure. We propose a method for MR analysis on latent exposures (MRLE), which tests the significance for, and the direction of, the effect of a latent exposure by leveraging information from multiple related traits. The method is developed by constructing a set of estimating functions based on the second-order moments of GWAS summary association statistics for the observable traits, under a structural equation model where genetic variants are assumed to have indirect effects through the latent exposure and potentially direct effects on the traits. Simulation studies show that MRLE has well-controlled type I error rates and enhanced power compared to single-trait MR tests under various types of pleiotropy. Applications of MRLE using genetic association statistics across five inflammatory biomarkers (CRP, IL-6, IL-8, TNF- α ⁠ , and MCP-1) provide evidence for potential causal effects of inflammation on increasing the risk of coronary artery disease, colorectal cancer, and rheumatoid arthritis, while standard MR analysis for individual biomarkers fails to detect consistent evidence for such effects.},
  archive      = {J_BIOSTAT},
  author       = {Jin, Jin and Qi, Guanghao and Yu, Zhi and Chatterjee, Nilanjan},
  doi          = {10.1093/biostatistics/kxae006},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {1015-1033},
  shortjournal = {Biostatistics},
  title        = {Mendelian randomization analysis using multiple biomarkers of an underlying common exposure},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Practical causal mediation analysis: Extending nonparametric
estimators to accommodate multiple mediators and multiple intermediate
confounders. <em>BIOSTAT</em>, <em>25</em>(4), 997–1014. (<a
href="https://doi.org/10.1093/biostatistics/kxae012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis is appealing for its ability to improve understanding of the mechanistic drivers of causal effects, but real-world data complexities challenge its successful implementation, including (i) the existence of post-exposure variables that also affect mediators and outcomes (thus, confounding the mediator-outcome relationship), that may also be (ii) multivariate, and (iii) the existence of multivariate mediators. All three challenges are present in the mediation analysis we consider here, where our goal is to estimate the indirect effects of receiving a Section 8 housing voucher as a young child on the risk of developing a psychiatric mood disorder in adolescence that operate through mediators related to neighborhood poverty, the school environment, and instability of the neighborhood and school environments, considered together and separately. Interventional direct and indirect effects (IDE/IIE) accommodate post-exposure variables that confound the mediator–outcome relationship, but currently, no readily implementable nonparametric estimator for IDE/IIE exists that allows for both multivariate mediators and multivariate post-exposure intermediate confounders. The absence of such an IDE/IIE estimator that can easily accommodate both multivariate mediators and post-exposure confounders represents a significant limitation for real-world analyses, because when considering each mediator subgroup separately, the remaining mediator subgroups (or a subset of them) become post-exposure intermediate confounders. We address this gap by extending a recently developed nonparametric estimator for the IDE/IIE to allow for easy incorporation of multivariate mediators and multivariate post-exposure confounders simultaneously. We apply the proposed estimation approach to our analysis, including walking through a strategy to account for other, possibly co-occurring intermediate variables when considering each mediator subgroup separately.},
  archive      = {J_BIOSTAT},
  author       = {Rudolph, Kara E and Williams, Nicholas T and Diaz, Ivan},
  doi          = {10.1093/biostatistics/kxae012},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {997-1014},
  shortjournal = {Biostatistics},
  title        = {Practical causal mediation analysis: Extending nonparametric estimators to accommodate multiple mediators and multiple intermediate confounders},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identification of complier and noncomplier average causal
effects in the presence of latent missing-at-random (LMAR) outcomes: A
unifying view and choices of assumptions. <em>BIOSTAT</em>,
<em>25</em>(4), 978–996. (<a
href="https://doi.org/10.1093/biostatistics/kxae011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of treatment effects is often complicated by noncompliance and missing data. In the one-sided noncompliance setting where of interest are the complier and noncomplier average causal effects, we address outcome missingness of the latent missing at random type (LMAR, also known as latent ignorability ). That is, conditional on covariates and treatment assigned, the missingness may depend on compliance type. Within the instrumental variable (IV) approach to noncompliance, methods have been proposed for handling LMAR outcome that additionally invoke an exclusion restriction-type assumption on missingness, but no solution has been proposed for when a non-IV approach is used. This article focuses on effect identification in the presence of LMAR outcomes, with a view to flexibly accommodate different principal identification approaches. We show that under treatment assignment ignorability and LMAR only, effect nonidentifiability boils down to a set of two connected mixture equations involving unidentified stratum-specific response probabilities and outcome means. This clarifies that (except for a special case) effect identification generally requires two additional assumptions: a specific missingness mechanism assumption and a principal identification assumption. This provides a template for identifying effects based on separate choices of these assumptions. We consider a range of specific missingness assumptions, including those that have appeared in the literature and some new ones. Incidentally, we find an issue in the existing assumptions, and propose a modification of the assumptions to avoid the issue. Results under different assumptions are illustrated using data from the Baltimore Experience Corps Trial.},
  archive      = {J_BIOSTAT},
  author       = {Nguyen, Trang Quynh and Carlson, Michelle C and Stuart, Elizabeth A},
  doi          = {10.1093/biostatistics/kxae011},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {978-996},
  shortjournal = {Biostatistics},
  title        = {Identification of complier and noncomplier average causal effects in the presence of latent missing-at-random (LMAR) outcomes: A unifying view and choices of assumptions},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian joint modeling of multivariate longitudinal and
survival outcomes using gaussian copulas. <em>BIOSTAT</em>,
<em>25</em>(4), 962–977. (<a
href="https://doi.org/10.1093/biostatistics/kxae009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is an increasing interest in the use of joint models for the analysis of longitudinal and survival data. While random effects models have been extensively studied, these models can be hard to implement and the fixed effect regression parameters must be interpreted conditional on the random effects. Copulas provide a useful alternative framework for joint modeling. One advantage of using copulas is that practitioners can directly specify marginal models for the outcomes of interest. We develop a joint model using a Gaussian copula to characterize the association between multivariate longitudinal and survival outcomes. Rather than using an unstructured correlation matrix in the copula model to characterize dependence structure as is common, we propose a novel decomposition that allows practitioners to impose structure (e.g., auto-regressive) which provides efficiency gains in small to moderate sample sizes and reduces computational complexity. We develop a Markov chain Monte Carlo model fitting procedure for estimation. We illustrate the method’s value using a simulation study and present a real data analysis of longitudinal quality of life and disease-free survival data from an International Breast Cancer Study Group trial.},
  archive      = {J_BIOSTAT},
  author       = {Cho, Seoyoon and Psioda, Matthew A and Ibrahim, Joseph G},
  doi          = {10.1093/biostatistics/kxae009},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {962-977},
  shortjournal = {Biostatistics},
  title        = {Bayesian joint modeling of multivariate longitudinal and survival outcomes using gaussian copulas},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian semiparametric model for sequential treatment
decisions with informative timing. <em>BIOSTAT</em>, <em>25</em>(4),
947–961. (<a
href="https://doi.org/10.1093/biostatistics/kxad035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a Bayesian semiparametric model for the impact of dynamic treatment rules on survival among patients diagnosed with pediatric acute myeloid leukemia (AML). The data consist of a subset of patients enrolled in a phase III clinical trial in which patients move through a sequence of four treatment courses. At each course, they undergo treatment that may or may not include anthracyclines (ACT). While ACT is known to be effective at treating AML, it is also cardiotoxic and can lead to early death for some patients. Our task is to estimate the potential survival probability under hypothetical dynamic ACT treatment strategies, but there are several impediments. First, since ACT is not randomized, its effect on survival is confounded over time. Second, subjects initiate the next course depending on when they recover from the previous course, making timing potentially informative of subsequent treatment and survival. Third, patients may die or drop out before ever completing the full treatment sequence. We develop a generative Bayesian semiparametric model based on Gamma Process priors to address these complexities. At each treatment course, the model captures subjects’ transition to subsequent treatment or death in continuous time. G-computation is used to compute a posterior over potential survival probability that is adjusted for time-varying confounding. Using our approach, we estimate the efficacy of hypothetical treatment rules that dynamically modify ACT based on evolving cardiac function.},
  archive      = {J_BIOSTAT},
  author       = {Oganisian, Arman and Getz, Kelly D and Alonzo, Todd A and Aplenc, Richard and Roy, Jason A},
  doi          = {10.1093/biostatistics/kxad035},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {947-961},
  shortjournal = {Biostatistics},
  title        = {Bayesian semiparametric model for sequential treatment decisions with informative timing},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation of optimal treatment regimes with electronic
medical record data using the residual life value estimator.
<em>BIOSTAT</em>, <em>25</em>(4), 933–946. (<a
href="https://doi.org/10.1093/biostatistics/kxae002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinicians and patients must make treatment decisions at a series of key decision points throughout disease progression. A dynamic treatment regime is a set of sequential decision rules that return treatment decisions based on accumulating patient information, like that commonly found in electronic medical record (EMR) data. When applied to a patient population, an optimal treatment regime leads to the most favorable outcome on average. Identifying optimal treatment regimes that maximize residual life is especially desirable for patients with life-threatening diseases such as sepsis, a complex medical condition that involves severe infections with organ dysfunction. We introduce the residual life value estimator (ReLiVE), an estimator for the expected value of cumulative restricted residual life under a fixed treatment regime. Building on ReLiVE, we present a method for estimating an optimal treatment regime that maximizes expected cumulative restricted residual life. Our proposed method, ReLiVE-Q, conducts estimation via the backward induction algorithm Q-learning. We illustrate the utility of ReLiVE-Q in simulation studies, and we apply ReLiVE-Q to estimate an optimal treatment regime for septic patients in the intensive care unit using EMR data from the Multiparameter Intelligent Monitoring Intensive Care database. Ultimately, we demonstrate that ReLiVE-Q leverages accumulating patient information to estimate personalized treatment regimes that optimize a clinically meaningful function of residual life.},
  archive      = {J_BIOSTAT},
  author       = {Rhodes, Grace and Davidian, Marie and Lu, Wenbin},
  doi          = {10.1093/biostatistics/kxae002},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {933-946},
  shortjournal = {Biostatistics},
  title        = {Estimation of optimal treatment regimes with electronic medical record data using the residual life value estimator},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian nonparametric approach for multiple mediators
with applications in mental health studies. <em>BIOSTAT</em>,
<em>25</em>(3), 919–932. (<a
href="https://doi.org/10.1093/biostatistics/kxad038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis with contemporaneously observed multiple mediators is a significant area of causal inference. Recent approaches for multiple mediators are often based on parametric models and thus may suffer from model misspecification. Also, much of the existing literature either only allow estimation of the joint mediation effect or estimate the joint mediation effect just as the sum of individual mediator effects, ignoring the interaction among the mediators. In this article, we propose a novel Bayesian nonparametric method that overcomes the two aforementioned drawbacks. We model the joint distribution of the observed data (outcome, mediators, treatment, and confounders) flexibly using an enriched Dirichlet process mixture with three levels. We use standardization (g-computation) to compute all possible mediation effects, including pairwise and all other possible interaction among the mediators. We thoroughly explore our method via simulations and apply our method to a mental health data from Wisconsin Longitudinal Study, where we estimate how the effect of births from unintended pregnancies on later life mental depression (CES-D) among the mothers is mediated through lack of self-acceptance and autonomy, employment instability, lack of social participation, and increased family stress. Our method identified significant individual mediators, along with some significant pairwise effects.},
  archive      = {J_BIOSTAT},
  author       = {Roy, Samrat and Daniels, Michael J and Roy, Jason},
  doi          = {10.1093/biostatistics/kxad038},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {919-932},
  shortjournal = {Biostatistics},
  title        = {A bayesian nonparametric approach for multiple mediators with applications in mental health studies},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian nonparametric approach to correct for
underreporting in count data. <em>BIOSTAT</em>, <em>25</em>(3), 904–918.
(<a href="https://doi.org/10.1093/biostatistics/kxad027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a nonparametric compound Poisson model for underreported count data that introduces a latent clustering structure for the reporting probabilities. The latter are estimated with the model’s parameters based on experts’ opinion and exploiting a proxy for the reporting process. The proposed model is used to estimate the prevalence of chronic kidney disease in Apulia, Italy, based on a unique statistical database covering information on m = 258 municipalities obtained by integrating multisource register information. Accurate prevalence estimates are needed for monitoring, surveillance, and management purposes; yet, counts are deemed to be considerably underreported, especially in some areas of Apulia, one of the most deprived and heterogeneous regions in Italy. Our results agree with previous findings and highlight interesting geographical patterns of the disease. We compare our model to existing approaches in the literature using simulated as well as real data on early neonatal mortality risk in Brazil, described in previous research: the proposed approach proves to be accurate and particularly suitable when partial information about data quality is available.},
  archive      = {J_BIOSTAT},
  author       = {Arima, Serena and Polettini, Silvia and Pasculli, Giuseppe and Gesualdo, Loreto and Pesce, Francesco and Procaccini, Deni-Aldo},
  doi          = {10.1093/biostatistics/kxad027},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {904-918},
  shortjournal = {Biostatistics},
  title        = {A bayesian nonparametric approach to correct for underreporting in count data},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved fMRI-based pain prediction using bayesian
group-wise functional registration. <em>BIOSTAT</em>, <em>25</em>(3),
885–903. (<a
href="https://doi.org/10.1093/biostatistics/kxad026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the field of neuroimaging has undergone a paradigm shift, moving away from the traditional brain mapping approach towards the development of integrated, multivariate brain models that can predict categories of mental events. However, large interindividual differences in both brain anatomy and functional localization after standard anatomical alignment remain a major limitation in performing this type of analysis, as it leads to feature misalignment across subjects in subsequent predictive models. This article addresses this problem by developing and validating a new computational technique for reducing misalignment across individuals in functional brain systems by spatially transforming each subject’s functional data to a common latent template map. Our proposed Bayesian functional group-wise registration approach allows us to assess differences in brain function across subjects and individual differences in activation topology. We achieve the probabilistic registration with inverse-consistency by utilizing the generalized Bayes framework with a loss function for the symmetric group-wise registration. It models the latent template with a Gaussian process, which helps capture spatial features in the template, producing a more precise estimation. We evaluate the method in simulation studies and apply it to data from an fMRI study of thermal pain, with the goal of using functional brain activity to predict physical pain. We find that the proposed approach allows for improved prediction of reported pain scores over conventional approaches. Received on 2 January 2017. Editorial decision on 8 June 2021},
  archive      = {J_BIOSTAT},
  author       = {Wang, Guoqing and Datta, Abhirup and Lindquist, Martin A},
  doi          = {10.1093/biostatistics/kxad026},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {885-903},
  shortjournal = {Biostatistics},
  title        = {Improved fMRI-based pain prediction using bayesian group-wise functional registration},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian multivariate factor analysis model for causal
inference using time-series observational data on mixed outcomes.
<em>BIOSTAT</em>, <em>25</em>(3), 867–884. (<a
href="https://doi.org/10.1093/biostatistics/kxad030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessing the impact of an intervention by using time-series observational data on multiple units and outcomes is a frequent problem in many fields of scientific research. Here, we propose a novel Bayesian multivariate factor analysis model for estimating intervention effects in such settings and develop an efficient Markov chain Monte Carlo algorithm to sample from the high-dimensional and nontractable posterior of interest. The proposed method is one of the few that can simultaneously deal with outcomes of mixed type (continuous, binomial, count), increase efficiency in the estimates of the causal effects by jointly modeling multiple outcomes affected by the intervention, and easily provide uncertainty quantification for all causal estimands of interest. Using the proposed approach, we evaluate the impact that Local Tracing Partnerships had on the effectiveness of England’s Test and Trace programme for COVID-19.},
  archive      = {J_BIOSTAT},
  author       = {Samartsidis, Pantelis and Seaman, Shaun R and Harrison, Abbie and Alexopoulos, Angelos and Hughes, Gareth J and Rawlinson, Christopher and Anderson, Charlotte and Charlett, André and Oliver, Isabel and De Angelis, Daniela},
  doi          = {10.1093/biostatistics/kxad030},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {867-884},
  shortjournal = {Biostatistics},
  title        = {A bayesian multivariate factor analysis model for causal inference using time-series observational data on mixed outcomes},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian joint models for multi-regional clinical trials.
<em>BIOSTAT</em>, <em>25</em>(3), 852–866. (<a
href="https://doi.org/10.1093/biostatistics/kxad023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, multi-regional clinical trials (MRCTs) have increased in popularity in the pharmaceutical industry due to their ability to accelerate the global drug development process. To address potential challenges with MRCTs, the International Council for Harmonisation released the E17 guidance document which suggests the use of statistical methods that utilize information borrowing across regions if regional sample sizes are small. We develop an approach that allows for information borrowing via Bayesian model averaging in the context of a joint analysis of survival and longitudinal data from MRCTs. In this novel application of joint models to MRCTs, we use Laplace’s method to integrate over subject-specific random effects and to approximate posterior distributions for region-specific treatment effects on the time-to-event outcome. Through simulation studies, we demonstrate that the joint modeling approach can result in an increased rejection rate when testing the global treatment effect compared with methods that analyze survival data alone. We then apply the proposed approach to data from a cardiovascular outcomes MRCT.},
  archive      = {J_BIOSTAT},
  author       = {Bean, Nathan W and Ibrahim, Joseph G and Psioda, Matthew A},
  doi          = {10.1093/biostatistics/kxad023},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {852-866},
  shortjournal = {Biostatistics},
  title        = {Bayesian joint models for multi-regional clinical trials},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty directed factorial clinical trials.
<em>BIOSTAT</em>, <em>25</em>(3), 833–851. (<a
href="https://doi.org/10.1093/biostatistics/kxad036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development and evaluation of novel treatment combinations is a key component of modern clinical research. The primary goals of factorial clinical trials of treatment combinations range from the estimation of intervention-specific effects, or the discovery of potential synergies, to the identification of combinations with the highest response probabilities. Most factorial studies use balanced or block randomization, with an equal number of patients assigned to each treatment combination, irrespective of the specific goals of the trial. Here, we introduce a class of Bayesian response-adaptive designs for factorial clinical trials with binary outcomes. The study design was developed using Bayesian decision-theoretic arguments and adapts the randomization probabilities to treatment combinations during the enrollment period based on the available data. Our approach enables the investigator to specify a utility function representative of the aims of the trial, and the Bayesian response-adaptive randomization algorithm aims to maximize this utility function. We considered several utility functions and factorial designs tailored to them. Then, we conducted a comparative simulation study to illustrate relevant differences of key operating characteristics across the resulting designs. We also investigated the asymptotic behavior of the proposed adaptive designs. We also used data summaries from three recent factorial trials in perioperative care, smoking cessation, and infectious disease prevention to define realistic simulation scenarios and illustrate advantages of the introduced trial designs compared to other study designs.},
  archive      = {J_BIOSTAT},
  author       = {Kotecha, Gopal and Ventz, Steffen and Fortini, Sandra and Trippa, Lorenzo},
  doi          = {10.1093/biostatistics/kxad036},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {833-851},
  shortjournal = {Biostatistics},
  title        = {Uncertainty directed factorial clinical trials},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DP2LM: Leveraging deep learning approach for estimation and
hypothesis testing on mediation effects with high-dimensional mediators
and complex confounders. <em>BIOSTAT</em>, <em>25</em>(3), 818–832. (<a
href="https://doi.org/10.1093/biostatistics/kxad037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional linear mediation analysis has inherent limitations when it comes to handling high-dimensional mediators. Particularly, accurately estimating and rigorously inferring mediation effects is challenging, primarily due to the intertwined nature of the mediator selection issue. Despite recent developments, the existing methods are inadequate for addressing the complex relationships introduced by confounders. To tackle these challenges, we propose a novel approach called DP2LM (Deep neural network-based Penalized Partially Linear Mediation). This approach incorporates deep neural network techniques to account for nonlinear effects in confounders and utilizes the penalized partially linear model to accommodate high dimensionality. Unlike most existing works that concentrate on mediator selection, our method prioritizes estimation and inference on mediation effects. Specifically, we develop test procedures for testing the direct and indirect mediation effects. Theoretical analysis shows that the tests maintain the Type-I error rate. In simulation studies, DP2LM demonstrates its superior performance as a modeling tool for complex data, outperforming existing approaches in a wide range of settings and providing reliable estimation and inference in scenarios involving a considerable number of mediators. Further, we apply DP2LM to investigate the mediation effect of DNA methylation on cortisol stress reactivity in individuals who experienced childhood trauma, uncovering new insights through a comprehensive analysis.},
  archive      = {J_BIOSTAT},
  author       = {Wang, Shuoyang and Huang, Yuan},
  doi          = {10.1093/biostatistics/kxad037},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {818-832},
  shortjournal = {Biostatistics},
  title        = {DP2LM: Leveraging deep learning approach for estimation and hypothesis testing on mediation effects with high-dimensional mediators and complex confounders},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantification and statistical modeling of droplet-based
single-nucleus RNA-sequencing data. <em>BIOSTAT</em>, <em>25</em>(3),
801–817. (<a
href="https://doi.org/10.1093/biostatistics/kxad010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In complex tissues containing cells that are difficult to dissociate, single-nucleus RNA-sequencing (snRNA-seq) has become the preferred experimental technology over single-cell RNA-sequencing (scRNA-seq) to measure gene expression. To accurately model these data in downstream analyses, previous work has shown that droplet-based scRNA-seq data are not zero-inflated, but whether droplet-based snRNA-seq data follow the same probability distributions has not been systematically evaluated. Using pseudonegative control data from nuclei in mouse cortex sequenced with the 10x Genomics Chromium system and mouse kidney sequenced with the DropSeq system, we found that droplet-based snRNA-seq data follow a negative binomial distribution, suggesting that parametric statistical models applied to scRNA-seq are transferable to snRNA-seq. Furthermore, we found that the quantification choices in adapting quantification mapping strategies from scRNA-seq to snRNA-seq can play a significant role in downstream analyses and biological interpretation. In particular, reference transcriptomes that do not include intronic regions result in significantly smaller library sizes and incongruous cell type classifications. We also confirmed the presence of a gene length bias in snRNA-seq data, which we show is present in both exonic and intronic reads, and investigate potential causes for the bias.},
  archive      = {J_BIOSTAT},
  author       = {Kuo, Albert and Hansen, Kasper D and Hicks, Stephanie C},
  doi          = {10.1093/biostatistics/kxad010},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {801-817},
  shortjournal = {Biostatistics},
  title        = {Quantification and statistical modeling of droplet-based single-nucleus RNA-sequencing data},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analyzing microbial evolution through gene and genome
phylogenies. <em>BIOSTAT</em>, <em>25</em>(3), 786–800. (<a
href="https://doi.org/10.1093/biostatistics/kxad025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microbiome scientists critically need modern tools to explore and analyze microbial evolution. Often this involves studying the evolution of microbial genomes as a whole. However, different genes in a single genome can be subject to different evolutionary pressures, which can result in distinct gene-level evolutionary histories. To address this challenge, we propose to treat estimated gene-level phylogenies as data objects, and present an interactive method for the analysis of a collection of gene phylogenies. We use a local linear approximation of phylogenetic tree space to visualize estimated gene trees as points in low-dimensional Euclidean space, and address important practical limitations of existing related approaches, allowing an intuitive visualization of complex data objects. We demonstrate the utility of our proposed approach through microbial data analyses, including by identifying outlying gene histories in strains of Prevotella , and by contrasting Streptococcus phylogenies estimated using different gene sets. Our method is available as an open-source R package, and assists with estimating, visualizing, and interacting with a collection of bacterial gene phylogenies.},
  archive      = {J_BIOSTAT},
  author       = {Teichman, Sarah and Lee, Michael D and Willis, Amy D},
  doi          = {10.1093/biostatistics/kxad025},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {786-800},
  shortjournal = {Biostatistics},
  title        = {Analyzing microbial evolution through gene and genome phylogenies},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An integrative latent class model of heterogeneous data
modalities for diagnosing kidney obstruction. <em>BIOSTAT</em>,
<em>25</em>(3), 769–785. (<a
href="https://doi.org/10.1093/biostatistics/kxad020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radionuclide imaging plays a critical role in the diagnosis and management of kidney obstruction. However, most practicing radiologists in US hospitals have insufficient time and resources to acquire training and experience needed to interpret radionuclide images, leading to increased diagnostic errors. To tackle this problem, Emory University embarked on a study that aims to develop a computer-assisted diagnostic (CAD) tool for kidney obstruction by mining and analyzing patient data comprised of renogram curves, ordinal expert ratings on the obstruction status, pharmacokinetic variables, and demographic information. The major challenges here are the heterogeneity in data modes and the lack of gold standard for determining kidney obstruction. In this article, we develop a statistically principled CAD tool based on an integrative latent class model that leverages heterogeneous data modalities available for each patient to provide accurate prediction of kidney obstruction. Our integrative model consists of three sub-models (multilevel functional latent factor regression model, probit scalar-on-function regression model, and Gaussian mixture model), each of which is tailored to the specific data mode and depends on the unknown obstruction status (latent class). An efficient MCMC algorithm is developed to train the model and predict kidney obstruction with associated uncertainty. Extensive simulations are conducted to evaluate the performance of the proposed method. An application to an Emory renal study demonstrates the usefulness of our model as a CAD tool for kidney obstruction.},
  archive      = {J_BIOSTAT},
  author       = {Jang, Jeong Hoon and Chang, Changgee and Manatunga, Amita K and Taylor, Andrew T and Long, Qi},
  doi          = {10.1093/biostatistics/kxad020},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {769-785},
  shortjournal = {Biostatistics},
  title        = {An integrative latent class model of heterogeneous data modalities for diagnosing kidney obstruction},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint modeling in presence of informative censoring on the
retrospective time scale with application to palliative care research.
<em>BIOSTAT</em>, <em>25</em>(3), 754–768. (<a
href="https://doi.org/10.1093/biostatistics/kxad028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint modeling of longitudinal data such as quality of life data and survival data is important for palliative care researchers to draw efficient inferences because it can account for the associations between those two types of data. Modeling quality of life on a retrospective from death time scale is useful for investigators to interpret the analysis results of palliative care studies which have relatively short life expectancies. However, informative censoring remains a complex challenge for modeling quality of life on the retrospective time scale although it has been addressed for joint models on the prospective time scale. To fill this gap, we develop a novel joint modeling approach that can address the challenge by allowing informative censoring events to be dependent on patients’ quality of life and survival through a random effect. There are two sub-models in our approach: a linear mixed effect model for the longitudinal quality of life and a competing-risk model for the death time and dropout time that share the same random effect as the longitudinal model. Our approach can provide unbiased estimates for parameters of interest by appropriately modeling the informative censoring time. Model performance is assessed with a simulation study and compared with existing approaches. A real-world study is presented to illustrate the application of the new approach.},
  archive      = {J_BIOSTAT},
  author       = {Wu, Quran and Daniels, Michael and El-Jawahri, Areej and Bakitas, Marie and Li, Zhigang},
  doi          = {10.1093/biostatistics/kxad028},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {754-768},
  shortjournal = {Biostatistics},
  title        = {Joint modeling in presence of informative censoring on the retrospective time scale with application to palliative care research},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable kernel balancing weights in a nationwide
observational study of hospital profit status and heart attack outcomes.
<em>BIOSTAT</em>, <em>25</em>(3), 736–753. (<a
href="https://doi.org/10.1093/biostatistics/kxad032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weighting is a general and often-used method for statistical adjustment. Weighting has two objectives: first, to balance covariate distributions, and second, to ensure that the weights have minimal dispersion and thus produce a more stable estimator. A recent, increasingly common approach directly optimizes the weights toward these two objectives. However, this approach has not yet been feasible in large-scale datasets when investigators wish to flexibly balance general basis functions in an extended feature space. To address this practical problem, we describe a scalable and flexible approach to weighting that integrates a basis expansion in a reproducing kernel Hilbert space with state-of-the-art convex optimization techniques. Specifically, we use the rank-restricted Nyström method to efficiently compute a kernel basis for balancing in nearly linear time and space, and then use the specialized first-order alternating direction method of multipliers to rapidly find the optimal weights. In an extensive simulation study, we provide new insights into the performance of weighting estimators in large datasets, showing that the proposed approach substantially outperforms others in terms of accuracy and speed. Finally, we use this weighting approach to conduct a national study of the relationship between hospital profit status and heart attack outcomes in a comprehensive dataset of 1.27 million patients. We find that for-profit hospitals use interventional cardiology to treat heart attacks at similar rates as other hospitals but have higher mortality and readmission rates.},
  archive      = {J_BIOSTAT},
  author       = {Kim, Kwangho and Niknam, Bijan A and Zubizarreta, José R},
  doi          = {10.1093/biostatistics/kxad032},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {736-753},
  shortjournal = {Biostatistics},
  title        = {Scalable kernel balancing weights in a nationwide observational study of hospital profit status and heart attack outcomes},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multivariate spatiotemporal functional principal component
analysis for modeling hospitalization and mortality rates in the
dialysis population. <em>BIOSTAT</em>, <em>25</em>(3), 718–735. (<a
href="https://doi.org/10.1093/biostatistics/kxad013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dialysis patients experience frequent hospitalizations and a higher mortality rate compared to other Medicare populations, in whom hospitalizations are a major contributor to morbidity, mortality, and healthcare costs. Patients also typically remain on dialysis for the duration of their lives or until kidney transplantation. Hence, there is growing interest in studying the spatiotemporal trends in the correlated outcomes of hospitalization and mortality among dialysis patients as a function of time starting from transition to dialysis across the United States Utilizing national data from the United States Renal Data System (USRDS), we propose a novel multivariate spatiotemporal functional principal component analysis model to study the joint spatiotemporal patterns of hospitalization and mortality rates among dialysis patients. The proposal is based on a multivariate Karhunen–Loéve expansion that describes leading directions of variation across time and induces spatial correlations among region-specific scores. An efficient estimation procedure is proposed using only univariate principal components decompositions and a Markov Chain Monte Carlo framework for targeting the spatial correlations. The finite sample performance of the proposed method is studied through simulations. Novel applications to the USRDS data highlight hot spots across the United States with higher hospitalization and/or mortality rates and time periods of elevated risk.},
  archive      = {J_BIOSTAT},
  author       = {Qian, Qi and Nguyen, Danh V and Telesca, Donatello and Kurum, Esra and Rhee, Connie M and Banerjee, Sudipto and Li, Yihao and Senturk, Damla},
  doi          = {10.1093/biostatistics/kxad013},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {718-735},
  shortjournal = {Biostatistics},
  title        = {Multivariate spatiotemporal functional principal component analysis for modeling hospitalization and mortality rates in the dialysis population},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An intersectional framework for counterfactual fairness in
risk prediction. <em>BIOSTAT</em>, <em>25</em>(3), 702–717. (<a
href="https://doi.org/10.1093/biostatistics/kxad021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Along with the increasing availability of health data has come the rise of data-driven models to inform decision making and policy. These models have the potential to benefit both patients and health care providers but can also exacerbate health inequities. Existing “algorithmic fairness” methods for measuring and correcting model bias fall short of what is needed for health policy in two key ways. First, methods typically focus on a single grouping along which discrimination may occur rather than considering multiple, intersecting groups. Second, in clinical applications, risk prediction is typically used to guide treatment, creating distinct statistical issues that invalidate most existing techniques. We present novel unfairness metrics that address both challenges. We also develop a complete framework of estimation and inference tools for our metrics, including the unfairness value (“ u -value”), used to determine the relative extremity of unfairness, and standard errors and confidence intervals employing an alternative to the standard bootstrap. We demonstrate application of our framework to a COVID-19 risk prediction model deployed in a major Midwestern health system.},
  archive      = {J_BIOSTAT},
  author       = {Wastvedt, Solvejg and Huling, Jared D and Wolfson, Julian},
  doi          = {10.1093/biostatistics/kxad021},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {702-717},
  shortjournal = {Biostatistics},
  title        = {An intersectional framework for counterfactual fairness in risk prediction},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A scalable approach for continuous time markov models with
covariates. <em>BIOSTAT</em>, <em>25</em>(3), 681–701. (<a
href="https://doi.org/10.1093/biostatistics/kxad012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing methods for fitting continuous time Markov models (CTMM) in the presence of covariates suffer from scalability issues due to high computational cost of matrix exponentials calculated for each observation. In this article, we propose an optimization technique for CTMM which uses a stochastic gradient descent algorithm combined with differentiation of the matrix exponential using a Padé approximation. This approach makes fitting large scale data feasible. We present two methods for computing standard errors, one novel approach using the Padé expansion and the other using power series expansion of the matrix exponential. Through simulations, we find improved performance relative to existing CTMM methods, and we demonstrate the method on the large-scale multiple sclerosis NO.MS data set.},
  archive      = {J_BIOSTAT},
  author       = {Hatami, Farhad and Ocampo, Alex and Graham, Gordon and Nichols, Thomas E and Ganjgahi, Habib},
  doi          = {10.1093/biostatistics/kxad012},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {681-701},
  shortjournal = {Biostatistics},
  title        = {A scalable approach for continuous time markov models with covariates},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Covariate-guided bayesian mixture of spline experts for the
analysis of multivariate high-density longitudinal data.
<em>BIOSTAT</em>, <em>25</em>(3), 666–680. (<a
href="https://doi.org/10.1093/biostatistics/kxad034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With rapid development of techniques to measure brain activity and structure, statistical methods for analyzing modern brain-imaging data play an important role in the advancement of science. Imaging data that measure brain function are usually multivariate high-density longitudinal data and are heterogeneous across both imaging sources and subjects, which lead to various statistical and computational challenges. In this article, we propose a group-based method to cluster a collection of multivariate high-density longitudinal data via a Bayesian mixture of smoothing splines. Our method assumes each multivariate high-density longitudinal trajectory is a mixture of multiple components with different mixing weights. Time-independent covariates are assumed to be associated with the mixture components and are incorporated via logistic weights of a mixture-of-experts model. We formulate this approach under a fully Bayesian framework using Gibbs sampling where the number of components is selected based on a deviance information criterion. The proposed method is compared to existing methods via simulation studies and is applied to a study on functional near-infrared spectroscopy, which aims to understand infant emotional reactivity and recovery from stress. The results reveal distinct patterns of brain activity, as well as associations between these patterns and selected covariates.},
  archive      = {J_BIOSTAT},
  author       = {Fu, Haoyi and Tang, Lu and Rosen, Ori and Hipwell, Alison E and Huppert, Theodore J and Krafty, Robert T},
  doi          = {10.1093/biostatistics/kxad034},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {666-680},
  shortjournal = {Biostatistics},
  title        = {Covariate-guided bayesian mixture of spline experts for the analysis of multivariate high-density longitudinal data},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian semiparametric markov renewal mixed models for
vocalization syntax. <em>BIOSTAT</em>, <em>25</em>(3), 648–665. (<a
href="https://doi.org/10.1093/biostatistics/kxac050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech and language play an important role in human vocal communication. Studies have shown that vocal disorders can result from genetic factors. In the absence of high-quality data on humans, mouse vocalization experiments in laboratory settings have been proven useful in providing valuable insights into mammalian vocal development, including especially the impact of certain genetic mutations. Such data sets usually consist of categorical syllable sequences along with continuous intersyllable interval (ISI) times for mice of different genotypes vocalizing under different contexts. ISIs are of particular importance as increased ISIs can be an indication of possible vocal impairment. Statistical methods for properly analyzing ISIs along with the transition probabilities have however been lacking. In this article, we propose a class of novel Markov renewal mixed models that capture the stochastic dynamics of both state transitions and ISI lengths. Specifically, we model the transition dynamics and the ISIs using Dirichlet and gamma mixtures, respectively, allowing the mixture probabilities in both cases to vary flexibly with fixed covariate effects as well as random individual-specific effects. We apply our model to analyze the impact of a mutation in the Foxp2 gene on mouse vocal behavior. We find that genotypes and social contexts significantly affect the length of ISIs but, compared to previous analyses, the influences of genotype and social context on the syllable transition dynamics are weaker.},
  archive      = {J_BIOSTAT},
  author       = {Wu, Yutong and Jarvis, Erich D and Sarkar, Abhra},
  doi          = {10.1093/biostatistics/kxac050},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {648-665},
  shortjournal = {Biostatistics},
  title        = {Bayesian semiparametric markov renewal mixed models for vocalization syntax},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variable selection in high dimensions for discrete-outcome
individualized treatment rules: Reducing severity of depression
symptoms. <em>BIOSTAT</em>, <em>25</em>(3), 633–647. (<a
href="https://doi.org/10.1093/biostatistics/kxad022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite growing interest in estimating individualized treatment rules, little attention has been given the binary outcome setting. Estimation is challenging with nonlinear link functions, especially when variable selection is needed. We use a new computational approach to solve a recently proposed doubly robust regularized estimating equation to accomplish this difficult task in a case study of depression treatment. We demonstrate an application of this new approach in combination with a weighted and penalized estimating equation to this challenging binary outcome setting. We demonstrate the double robustness of the method and its effectiveness for variable selection. The work is motivated by and applied to an analysis of treatment for unipolar depression using a population of patients treated at Kaiser Permanente Washington.},
  archive      = {J_BIOSTAT},
  author       = {Moodie, Erica E M and Bian, Zeyu and Coulombe, Janie and Lian, Yi and Yang, Archer Y and Shortreed, Susan M},
  doi          = {10.1093/biostatistics/kxad022},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {633-647},
  shortjournal = {Biostatistics},
  title        = {Variable selection in high dimensions for discrete-outcome individualized treatment rules: Reducing severity of depression symptoms},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised mixture multi-source exchangeability model
for leveraging real-world data in clinical trials. <em>BIOSTAT</em>,
<em>25</em>(3), 617–632. (<a
href="https://doi.org/10.1093/biostatistics/kxad024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional trial paradigm is often criticized as being slow, inefficient, and costly. Statistical approaches that leverage external trial data have emerged to make trials more efficient by augmenting the sample size. However, these approaches assume that external data are from previously conducted trials, leaving a rich source of untapped real-world data (RWD) that cannot yet be effectively leveraged. We propose a semi-supervised mixture (SS-MIX) multisource exchangeability model (MEM); a flexible, two-step Bayesian approach for incorporating RWD into randomized controlled trial analyses. The first step is a SS-MIX model on a modified propensity score and the second step is a MEM. The first step targets a representative subgroup of individuals from the trial population and the second step avoids borrowing when there are substantial differences in outcomes among the trial sample and the representative observational sample. When comparing the proposed approach to competing borrowing approaches in a simulation study, we find that our approach borrows efficiently when the trial and RWD are consistent, while mitigating bias when the trial and external data differ on either measured or unmeasured covariates. We illustrate the proposed approach with an application to a randomized controlled trial investigating intravenous hyperimmune immunoglobulin in hospitalized patients with influenza, while leveraging data from an external observational study to supplement a subgroup analysis by influenza subtype.},
  archive      = {J_BIOSTAT},
  author       = {Haine, Lillian M F and Murry, Thomas A and Nahra, Raquel and Touloumi, Giota and Fernández-Cruz, Eduardo and Petoumenos, Kathy and Koopmeiners, Joseph S and for the Insight FLU 003 Plus and FLU-IVIG Study Groups},
  doi          = {10.1093/biostatistics/kxad024},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {617-632},
  shortjournal = {Biostatistics},
  title        = {Semi-supervised mixture multi-source exchangeability model for leveraging real-world data in clinical trials},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Blurring cluster randomized trials and observational
studies: Two-stage TMLE for subsampling, missingness, and few
independent units. <em>BIOSTAT</em>, <em>25</em>(3), 599–616. (<a
href="https://doi.org/10.1093/biostatistics/kxad015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster randomized trials (CRTs) often enroll large numbers of participants; yet due to resource constraints, only a subset of participants may be selected for outcome assessment, and those sampled may not be representative of all cluster members. Missing data also present a challenge: if sampled individuals with measured outcomes are dissimilar from those with missing outcomes, unadjusted estimates of arm-specific endpoints and the intervention effect may be biased. Further, CRTs often enroll and randomize few clusters, limiting statistical power and raising concerns about finite sample performance. Motivated by SEARCH-TB, a CRT aimed at reducing incident tuberculosis infection, we demonstrate interlocking methods to handle these challenges. First, we extend Two-Stage targeted minimum loss-based estimation to account for three sources of missingness: (i) subsampling; (ii) measurement of baseline status among those sampled; and (iii) measurement of final status among those in the incidence cohort (persons known to be at risk at baseline). Second, we critically evaluate the assumptions under which subunits of the cluster can be considered the conditionally independent unit, improving precision and statistical power but also causing the CRT to behave like an observational study. Our application to SEARCH-TB highlights the real-world impact of different assumptions on measurement and dependence; estimates relying on unrealistic assumptions suggested the intervention increased the incidence of TB infection by 18% (risk ratio [RR] = 1.18, 95% confidence interval [CI]: 0.85–1.63), while estimates accounting for the sampling scheme, missingness, and within community dependence found the intervention decreased the incident TB by 27% (RR = 0.73, 95% CI: 0.57–0.92).},
  archive      = {J_BIOSTAT},
  author       = {Nugent, Joshua R and for the SEARCH COLLABORATION and Marquez, Carina and Charlebois, Edwin D and Abbott, Rachel and Balzer, Laura B},
  doi          = {10.1093/biostatistics/kxad015},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {599-616},
  shortjournal = {Biostatistics},
  title        = {Blurring cluster randomized trials and observational studies: Two-stage TMLE for subsampling, missingness, and few independent units},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction to: A transformation perspective on marginal and
conditional models. <em>BIOSTAT</em>, <em>25</em>(2), 597. (<a
href="https://doi.org/10.1093/biostatistics/kxad017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOSTAT},
  doi          = {10.1093/biostatistics/kxad017},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {597},
  shortjournal = {Biostatistics},
  title        = {Correction to: A transformation perspective on marginal and conditional models},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling biomarker variability in joint analysis of
longitudinal and time-to-event data. <em>BIOSTAT</em>, <em>25</em>(2),
577–596. (<a
href="https://doi.org/10.1093/biostatistics/kxad009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The role of visit-to-visit variability of a biomarker in predicting related disease has been recognized in medical science. Existing measures of biological variability are criticized for being entangled with random variability resulted from measurement error or being unreliable due to limited measurements per individual. In this article, we propose a new measure to quantify the biological variability of a biomarker by evaluating the fluctuation of each individual-specific trajectory behind longitudinal measurements. Given a mixed-effects model for longitudinal data with the mean function over time specified by cubic splines, our proposed variability measure can be mathematically expressed as a quadratic form of random effects. A Cox model is assumed for time-to-event data by incorporating the defined variability as well as the current level of the underlying longitudinal trajectory as covariates, which, together with the longitudinal model, constitutes the joint modeling framework in this article. Asymptotic properties of maximum likelihood estimators are established for the present joint model. Estimation is implemented via an Expectation-Maximization (EM) algorithm with fully exponential Laplace approximation used in E-step to reduce the computation burden due to the increase of the random effects dimension. Simulation studies are conducted to reveal the advantage of the proposed method over the two-stage method, as well as a simpler joint modeling approach which does not take into account biomarker variability. Finally, we apply our model to investigate the effect of systolic blood pressure variability on cardiovascular events in the Medical Research Council elderly trial, which is also the motivating example for this article.},
  archive      = {J_BIOSTAT},
  author       = {Wang, Chunyu and Shen, Jiaming and Charalambous, Christiana and Pan, Jianxin},
  doi          = {10.1093/biostatistics/kxad009},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {577-596},
  shortjournal = {Biostatistics},
  title        = {Modeling biomarker variability in joint analysis of longitudinal and time-to-event data},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differential transcript usage analysis incorporating
quantification uncertainty via compositional measurement error
regression modeling. <em>BIOSTAT</em>, <em>25</em>(2), 559–576. (<a
href="https://doi.org/10.1093/biostatistics/kxad008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential transcript usage (DTU) occurs when the relative expression of multiple transcripts arising from the same gene changes between different conditions. Existing approaches to detect DTU often rely on computational procedures that can have speed and scalability issues as the number of samples increases. Here we propose a new method, CompDTU , that uses compositional regression to model the relative abundance proportions of each transcript that are of interest in DTU analyses. This procedure leverages fast matrix-based computations that make it ideally suited for DTU analysis with larger sample sizes. This method also allows for the testing of and adjustment for multiple categorical or continuous covariates. Additionally, many existing approaches for DTU ignore quantification uncertainty in the expression estimates for each transcript in RNA-seq data. We extend our CompDTU method to incorporate quantification uncertainty leveraging common output from RNA-seq expression quantification tool in a novel method CompDTUme . Through several power analyses, we show that CompDTU has excellent sensitivity and reduces false positive results relative to existing methods. Additionally, CompDTUme results in further improvements in performance over CompDTU with sufficient sample size for genes with high levels of quantification uncertainty, while also maintaining favorable speed and scalability. We motivate our methods using data from the Cancer Genome Atlas Breast Invasive Carcinoma data set, specifically using RNA-seq data from primary tumors for 740 patients with breast cancer. We show greatly reduced computation time from our new methods as well as the ability to detect several novel genes with significant DTU across different breast cancer subtypes.},
  archive      = {J_BIOSTAT},
  author       = {Young, Amber M and Van Buren, Scott and Rashid, Naim U},
  doi          = {10.1093/biostatistics/kxad008},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {559-576},
  shortjournal = {Biostatistics},
  title        = {Differential transcript usage analysis incorporating quantification uncertainty via compositional measurement error regression modeling},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identifying covariate-related subnetworks for whole-brain
connectome analysis. <em>BIOSTAT</em>, <em>25</em>(2), 541–558. (<a
href="https://doi.org/10.1093/biostatistics/kxad007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whole-brain connectome data characterize the connections among distributed neural populations as a set of edges in a large network, and neuroscience research aims to systematically investigate associations between brain connectome and clinical or experimental conditions as covariates. A covariate is often related to a number of edges connecting multiple brain areas in an organized structure. However, in practice, neither the covariate-related edges nor the structure is known. Therefore, the understanding of underlying neural mechanisms relies on statistical methods that are capable of simultaneously identifying covariate-related connections and recognizing their network topological structures. The task can be challenging because of false-positive noise and almost infinite possibilities of edges combining into subnetworks. To address these challenges, we propose a new statistical approach to handle multivariate edge variables as outcomes and output covariate-related subnetworks. We first study the graph properties of covariate-related subnetworks from a graph and combinatorics perspective and accordingly bridge the inference for individual connectome edges and covariate-related subnetworks. Next, we develop efficient algorithms to exact covariate-related subnetworks from the whole-brain connectome data with an |$\ell_0$| norm penalty. We validate the proposed methods based on an extensive simulation study, and we benchmark our performance against existing methods. Using our proposed method, we analyze two separate resting-state functional magnetic resonance imaging data sets for schizophrenia research and obtain highly replicable disease-related subnetworks.},
  archive      = {J_BIOSTAT},
  author       = {Chen, Shuo and Zhang, Yuan and Wu, Qiong and Bi, Chuan and Kochunov, Peter and Hong, L Elliot},
  doi          = {10.1093/biostatistics/kxad007},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {541-558},
  shortjournal = {Biostatistics},
  title        = {Identifying covariate-related subnetworks for whole-brain connectome analysis},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cohort-based smoothing methods for age-specific contact
rates. <em>BIOSTAT</em>, <em>25</em>(2), 521–540. (<a
href="https://doi.org/10.1093/biostatistics/kxad005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of social contact rates is widespread in infectious disease modeling since it has been shown that they are key driving forces of important epidemiological parameters. Quantification of contact patterns is crucial to parameterize dynamic transmission models and to provide insights on the (basic) reproduction number. Information on social interactions can be obtained from population-based contact surveys, such as the European Commission project POLYMOD. Estimation of age-specific contact rates from these studies is often done using a piecewise constant approach or bivariate smoothing techniques. For the latter, typically, smoothness is introduced in the dimensions of the respondent’s and contact’s age (i.e., the rows and columns of the social contact matrix). We propose a smoothing constrained approach—taking into account the reciprocal nature of contacts—introducing smoothness over the diagonal (including all subdiagonals) of the social contact matrix. This modeling approach is justified assuming that when people age their contact behavior changes smoothly. We call this smoothing from a cohort perspective. Two approaches that allow for smoothing over social contact matrix diagonals are proposed, namely (i) reordering of the diagonal components of the contact matrix and (ii) reordering of the penalty matrix ensuring smoothness over the contact matrix diagonals. Parameter estimation is done in the likelihood framework by using constrained penalized iterative reweighted least squares. A simulation study underlines the benefits of cohort-based smoothing. Finally, the proposed methods are illustrated on the Belgian POLYMOD data of 2006. Code to reproduce the results of the article can be downloaded on this GitHub repository https://github.com/oswaldogressani/Cohort_smoothing .},
  archive      = {J_BIOSTAT},
  author       = {Vandendijck, Yannick and Gressani, Oswaldo and Faes, Christel and Camarda, Carlo G and Hens, Niel},
  doi          = {10.1093/biostatistics/kxad005},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {521-540},
  shortjournal = {Biostatistics},
  title        = {Cohort-based smoothing methods for age-specific contact rates},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-trait analysis of gene-by-environment interactions in
large-scale genetic studies. <em>BIOSTAT</em>, <em>25</em>(2), 504–520.
(<a href="https://doi.org/10.1093/biostatistics/kxad004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying genotype-by-environment interaction (GEI) is challenging because the GEI analysis generally has low power. Large-scale consortium-based studies are ultimately needed to achieve adequate power for identifying GEI. We introduce Multi-Trait Analysis of Gene–Environment Interactions (MTAGEI), a powerful, robust, and computationally efficient framework to test gene–environment interactions on multiple traits in large data sets, such as the UK Biobank (UKB). To facilitate the meta-analysis of GEI studies in a consortium, MTAGEI efficiently generates summary statistics of genetic associations for multiple traits under different environmental conditions and integrates the summary statistics for GEI analysis. MTAGEI enhances the power of GEI analysis by aggregating GEI signals across multiple traits and variants that would otherwise be difficult to detect individually. MTAGEI achieves robustness by combining complementary tests under a wide spectrum of genetic architectures. We demonstrate the advantages of MTAGEI over existing single-trait-based GEI tests through extensive simulation studies and the analysis of the whole exome sequencing data from the UKB.},
  archive      = {J_BIOSTAT},
  author       = {Luo, Lan and Mehrotra, Devan V and Shen, Judong and Tang, Zheng-Zheng},
  doi          = {10.1093/biostatistics/kxad004},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {504-520},
  shortjournal = {Biostatistics},
  title        = {Multi-trait analysis of gene-by-environment interactions in large-scale genetic studies},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating the overall fraction of phenotypic variance
attributed to high-dimensional predictors measured with error.
<em>BIOSTAT</em>, <em>25</em>(2), 486–503. (<a
href="https://doi.org/10.1093/biostatistics/kxad001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In prospective genomic studies (e.g., DNA methylation, metagenomics, and transcriptomics), it is crucial to estimate the overall fraction of phenotypic variance (OFPV) attributed to the high-dimensional genomic variables, a concept similar to heritability analyses in genome-wide association studies (GWAS). Unlike genetic variants in GWAS, these genomic variables are typically measured with error due to technical limitation and temporal instability. While the existing methods developed for GWAS can be used, ignoring measurement error may severely underestimate OFPV and mislead the design of future studies. Assuming that measurement error variances are distributed similarly between causal and noncausal variables, we show that the asymptotic attenuation factor equals to the average intraclass correlation coefficients of all genomic variables, which can be estimated based on a pilot study with repeated measurements. We illustrate the method by estimating the contribution of microbiome taxa to body mass index and multiple allergy traits in the American Gut Project. Finally, we show that measurement error does not cause meaningful bias when estimating the correlation of effect sizes for two traits.},
  archive      = {J_BIOSTAT},
  author       = {Mandal, Soutrik and Kim, Do Hyun and Hua, Xing and Li, Shilan and Shi, Jianxin},
  doi          = {10.1093/biostatistics/kxad001},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {486-503},
  shortjournal = {Biostatistics},
  title        = {Estimating the overall fraction of phenotypic variance attributed to high-dimensional predictors measured with error},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DeLIVR: A deep learning approach to IV regression for
testing nonlinear causal effects in transcriptome-wide association
studies. <em>BIOSTAT</em>, <em>25</em>(2), 468–485. (<a
href="https://doi.org/10.1093/biostatistics/kxac051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transcriptome-wide association studies (TWAS) have been increasingly applied to identify (putative) causal genes for complex traits and diseases. TWAS can be regarded as a two-sample two-stage least squares method for instrumental variable (IV) regression for causal inference. The standard TWAS (called TWAS-L) only considers a linear relationship between a gene’s expression and a trait in stage 2, which may lose statistical power when not true. Recently, an extension of TWAS (called TWAS-LQ) considers both the linear and quadratic effects of a gene on a trait, which however is not flexible enough due to its parametric nature and may be low powered for nonquadratic nonlinear effects. On the other hand, a deep learning (DL) approach, called DeepIV, has been proposed to nonparametrically model a nonlinear effect in IV regression. However, it is both slow and unstable due to the ill-posed inverse problem of solving an integral equation with Monte Carlo approximations. Furthermore, in the original DeepIV approach, statistical inference, that is, hypothesis testing, was not studied. Here, we propose a novel DL approach, called DeLIVR, to overcome the major drawbacks of DeepIV, by estimating a related but different target function and including a hypothesis testing framework. We show through simulations that DeLIVR was both faster and more stable than DeepIV. We applied both parametric and DL approaches to the GTEx and UK Biobank data, showcasing that DeLIVR detected additional 8 and 7 genes nonlinearly associated with high-density lipoprotein (HDL) cholesterol and low-density lipoprotein (LDL) cholesterol, respectively, all of which would be missed by TWAS-L, TWAS-LQ, and DeepIV; these genes include BUD13 associated with HDL, SLC44A2 and GMIP with LDL, all supported by previous studies.},
  archive      = {J_BIOSTAT},
  author       = {He, Ruoyu and Liu, Mingyang and Lin, Zhaotong and Zhuang, Zhong and Shen, Xiaotong and Pan, Wei},
  doi          = {10.1093/biostatistics/kxac051},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {468-485},
  shortjournal = {Biostatistics},
  title        = {DeLIVR: A deep learning approach to IV regression for testing nonlinear causal effects in transcriptome-wide association studies},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Characterizing quantile-varying covariate effects under the
accelerated failure time model. <em>BIOSTAT</em>, <em>25</em>(2),
449–467. (<a
href="https://doi.org/10.1093/biostatistics/kxac052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important task in survival analysis is choosing a structure for the relationship between covariates of interest and the time-to-event outcome. For example, the accelerated failure time (AFT) model structures each covariate effect as a constant multiplicative shift in the outcome distribution across all survival quantiles. Though parsimonious, this structure cannot detect or capture effects that differ across quantiles of the distribution, a limitation that is analogous to only permitting proportional hazards in the Cox model. To address this, we propose a general framework for quantile-varying multiplicative effects under the AFT model. Specifically, we embed flexible regression structures within the AFT model and derive a novel formula for interpretable effects on the quantile scale. A regression standardization scheme based on the g-formula is proposed to enable the estimation of both covariate-conditional and marginal effects for an exposure of interest. We implement a user-friendly Bayesian approach for the estimation and quantification of uncertainty while accounting for left truncation and complex censoring. We emphasize the intuitive interpretation of this model through numerical and graphical tools and illustrate its performance through simulation and application to a study of Alzheimer’s disease and dementia.},
  archive      = {J_BIOSTAT},
  author       = {Reeder, Harrison T and Lee, Kyu Ha and Haneuse, Sebastien},
  doi          = {10.1093/biostatistics/kxac052},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {449-467},
  shortjournal = {Biostatistics},
  title        = {Characterizing quantile-varying covariate effects under the accelerated failure time model},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast and flexible inference for joint models of multivariate
longitudinal and survival data using integrated nested laplace
approximations. <em>BIOSTAT</em>, <em>25</em>(2), 429–448. (<a
href="https://doi.org/10.1093/biostatistics/kxad019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling longitudinal and survival data jointly offers many advantages such as addressing measurement error and missing data in the longitudinal processes, understanding and quantifying the association between the longitudinal markers and the survival events, and predicting the risk of events based on the longitudinal markers. A joint model involves multiple submodels (one for each longitudinal/survival outcome) usually linked together through correlated or shared random effects. Their estimation is computationally expensive (particularly due to a multidimensional integration of the likelihood over the random effects distribution) so that inference methods become rapidly intractable, and restricts applications of joint models to a small number of longitudinal markers and/or random effects. We introduce a Bayesian approximation based on the integrated nested Laplace approximation algorithm implemented in the R package R-INLA to alleviate the computational burden and allow the estimation of multivariate joint models with fewer restrictions. Our simulation studies show that R-INLA substantially reduces the computation time and the variability of the parameter estimates compared with alternative estimation strategies. We further apply the methodology to analyze five longitudinal markers (3 continuous, 1 count, 1 binary, and 16 random effects) and competing risks of death and transplantation in a clinical trial on primary biliary cholangitis. R-INLA provides a fast and reliable inference technique for applying joint models to the complex multivariate data encountered in health research.},
  archive      = {J_BIOSTAT},
  author       = {Rustand, Denis and van Niekerk, Janet and Krainski, Elias Teixeira and Rue, Håvard and Proust-Lima, Cécile},
  doi          = {10.1093/biostatistics/kxad019},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {429-448},
  shortjournal = {Biostatistics},
  title        = {Fast and flexible inference for joint models of multivariate longitudinal and survival data using integrated nested laplace approximations},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A transformation perspective on marginal and conditional
models. <em>BIOSTAT</em>, <em>25</em>(2), 402–428. (<a
href="https://doi.org/10.1093/biostatistics/kxac048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustered observations are ubiquitous in controlled and observational studies and arise naturally in multicenter trials or longitudinal surveys. We present a novel model for the analysis of clustered observations where the marginal distributions are described by a linear transformation model and the correlations by a joint multivariate normal distribution. The joint model provides an analytic formula for the marginal distribution. Owing to the richness of transformation models, the techniques are applicable to any type of response variable, including bounded, skewed, binary, ordinal, or survival responses. We demonstrate how the common normal assumption for reaction times can be relaxed in the sleep deprivation benchmark data set and report marginal odds ratios for the notoriously difficult toe nail data. We furthermore discuss the analysis of two clinical trials aiming at the estimation of marginal treatment effects. In the first trial, pain was repeatedly assessed on a bounded visual analog scale and marginal proportional-odds models are presented. The second trial reported disease-free survival in rectal cancer patients, where the marginal hazard ratio from Weibull and Cox models is of special interest. An empirical evaluation compares the performance of the novel approach to general estimation equations for binary responses and to conditional mixed-effects models for continuous responses. An implementation is available in the tram add-on package to the R system and was benchmarked against established models in the literature.},
  archive      = {J_BIOSTAT},
  author       = {Barbanti, Luisa and Hothorn, Torsten},
  doi          = {10.1093/biostatistics/kxac048},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {402-428},
  shortjournal = {Biostatistics},
  title        = {A transformation perspective on marginal and conditional models},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Longitudinal regression of covariance matrix outcomes.
<em>BIOSTAT</em>, <em>25</em>(2), 385–401. (<a
href="https://doi.org/10.1093/biostatistics/kxac045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a longitudinal regression model for covariance matrix outcomes is introduced. The proposal considers a multilevel generalized linear model for regressing covariance matrices on (time-varying) predictors. This model simultaneously identifies covariate-associated components from covariance matrices, estimates regression coefficients, and captures the within-subject variation in the covariance matrices. Optimal estimators are proposed for both low-dimensional and high-dimensional cases by maximizing the (approximated) hierarchical-likelihood function. These estimators are proved to be asymptotically consistent, where the proposed covariance matrix estimator is the most efficient under the low-dimensional case and achieves the uniformly minimum quadratic loss among all linear combinations of the identity matrix and the sample covariance matrix under the high-dimensional case. Through extensive simulation studies, the proposed approach achieves good performance in identifying the covariate-related components and estimating the model parameters. Applying to a longitudinal resting-state functional magnetic resonance imaging data set from the Alzheimer’s Disease (AD) Neuroimaging Initiative, the proposed approach identifies brain networks that demonstrate the difference between males and females at different disease stages. The findings are in line with existing knowledge of AD and the method improves the statistical power over the analysis of cross-sectional data.},
  archive      = {J_BIOSTAT},
  author       = {Zhao, Yi and For the Alzheimer’s Disease Neuroimaging Initiative and Caffo, Brian S and Luo, Xi},
  doi          = {10.1093/biostatistics/kxac045},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {385-401},
  shortjournal = {Biostatistics},
  title        = {Longitudinal regression of covariance matrix outcomes},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian approach to estimating COVID-19 incidence and
infection fatality rates. <em>BIOSTAT</em>, <em>25</em>(2), 354–384. (<a
href="https://doi.org/10.1093/biostatistics/kxad003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Naive estimates of incidence and infection fatality rates (IFR) of coronavirus disease 2019 suffer from a variety of biases, many of which relate to preferential testing. This has motivated epidemiologists from around the globe to conduct serosurveys that measure the immunity of individuals by testing for the presence of SARS-CoV-2 antibodies in the blood. These quantitative measures (titer values) are then used as a proxy for previous or current infection. However, statistical methods that use this data to its full potential have yet to be developed. Previous researchers have discretized these continuous values, discarding potentially useful information. In this article, we demonstrate how multivariate mixture models can be used in combination with post-stratification to estimate cumulative incidence and IFR in an approximate Bayesian framework without discretization. In doing so, we account for uncertainty from both the estimated number of infections and incomplete deaths data to provide estimates of IFR. This method is demonstrated using data from the Action to Beat Coronavirus erosurvey in Canada.},
  archive      = {J_BIOSTAT},
  author       = {Slater, Justin J and Bansal, Aiyush and Campbell, Harlan and Rosenthal, Jeffrey S and Gustafson, Paul and Brown, Patrick E},
  doi          = {10.1093/biostatistics/kxad003},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {354-384},
  shortjournal = {Biostatistics},
  title        = {A bayesian approach to estimating COVID-19 incidence and infection fatality rates},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A joint bayesian hierarchical model for estimating
SARS-CoV-2 genomic and subgenomic RNA viral dynamics and seroconversion.
<em>BIOSTAT</em>, <em>25</em>(2), 336–353. (<a
href="https://doi.org/10.1093/biostatistics/kxad016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the viral dynamics of and natural immunity to the severe acute respiratory syndrome coronavirus 2 is crucial for devising better therapeutic and prevention strategies for coronavirus disease 2019 (COVID-19). Here, we present a Bayesian hierarchical model that jointly estimates the genomic RNA viral load, the subgenomic RNA (sgRNA) viral load (correlated to active viral replication), and the rate and timing of seroconversion (correlated to presence of antibodies). Our proposed method accounts for the dynamical relationship and correlation structure between the two types of viral load, allows for borrowing of information between viral load and antibody data, and identifies potential correlates of viral load characteristics and propensity for seroconversion. We demonstrate the features of the joint model through application to the COVID-19 post-exposure prophylaxis study and conduct a cross-validation exercise to illustrate the model’s ability to impute the sgRNA viral trajectories for people who only had genomic RNA viral load data.},
  archive      = {J_BIOSTAT},
  author       = {Dong, Tracy Q and Brown, Elizabeth R},
  doi          = {10.1093/biostatistics/kxad016},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {336-353},
  shortjournal = {Biostatistics},
  title        = {A joint bayesian hierarchical model for estimating SARS-CoV-2 genomic and subgenomic RNA viral dynamics and seroconversion},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tree-based subgroup discovery using electronic health record
data: Heterogeneity of treatment effects for DTG-containing therapies.
<em>BIOSTAT</em>, <em>25</em>(2), 323–335. (<a
href="https://doi.org/10.1093/biostatistics/kxad014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rich longitudinal individual level data available from electronic health records (EHRs) can be used to examine treatment effect heterogeneity. However, estimating treatment effects using EHR data poses several challenges, including time-varying confounding, repeated and temporally non-aligned measurements of covariates, treatment assignments and outcomes, and loss-to-follow-up due to dropout. Here, we develop the subgroup discovery for longitudinal data algorithm, a tree-based algorithm for discovering subgroups with heterogeneous treatment effects using longitudinal data by combining the generalized interaction tree algorithm, a general data-driven method for subgroup discovery, with longitudinal targeted maximum likelihood estimation. We apply the algorithm to EHR data to discover subgroups of people living with human immunodeficiency virus who are at higher risk of weight gain when receiving dolutegravir (DTG)-containing antiretroviral therapies (ARTs) versus when receiving non-DTG-containing ARTs.},
  archive      = {J_BIOSTAT},
  author       = {Yang, Jiabei and Mwangi, Ann W and Kantor, Rami and Dahabreh, Issa J and Nyambura, Monicah and Delong, Allison and Hogan, Joseph W and Steingrimsson, Jon A},
  doi          = {10.1093/biostatistics/kxad014},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {323-335},
  shortjournal = {Biostatistics},
  title        = {Tree-based subgroup discovery using electronic health record data: Heterogeneity of treatment effects for DTG-containing therapies},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple imputation of more than one environmental exposure
with nondifferential measurement error. <em>BIOSTAT</em>,
<em>25</em>(2), 306–322. (<a
href="https://doi.org/10.1093/biostatistics/kxad011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measurement error is common in environmental epidemiologic studies, but methods for correcting measurement error in regression models with multiple environmental exposures as covariates have not been well investigated. We consider a multiple imputation approach, combining external or internal calibration samples that contain information on both true and error-prone exposures with the main study data of multiple exposures measured with error. We propose a constrained chained equations multiple imputation (CEMI) algorithm that places constraints on the imputation model parameters in the chained equations imputation based on the assumptions of strong nondifferential measurement error. We also extend the constrained CEMI method to accommodate nondetects in the error-prone exposures in the main study data. We estimate the variance of the regression coefficients using the bootstrap with two imputations of each bootstrapped sample. The constrained CEMI method is shown by simulations to outperform existing methods, namely the method that ignores measurement error, classical calibration, and regression prediction, yielding estimated regression coefficients with smaller bias and confidence intervals with coverage close to the nominal level. We apply the proposed method to the Neighborhood Asthma and Allergy Study to investigate the associations between the concentrations of multiple indoor allergens and the fractional exhaled nitric oxide level among asthmatic children in New York City. The constrained CEMI method can be implemented by imposing constraints on the imputation matrix using the mice and bootImpute packages in R.},
  archive      = {J_BIOSTAT},
  author       = {Yu, Yuanzhi and Little, Roderick J and Perzanowski, Matthew and Chen, Qixuan},
  doi          = {10.1093/biostatistics/kxad011},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {306-322},
  shortjournal = {Biostatistics},
  title        = {Multiple imputation of more than one environmental exposure with nondifferential measurement error},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Systematically missing data in causally interpretable
meta-analysis. <em>BIOSTAT</em>, <em>25</em>(2), 289–305. (<a
href="https://doi.org/10.1093/biostatistics/kxad006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causally interpretable meta-analysis combines information from a collection of randomized controlled trials to estimate treatment effects in a target population in which experimentation may not be possible but from which covariate information can be obtained. In such analyses, a key practical challenge is the presence of systematically missing data when some trials have collected data on one or more baseline covariates, but other trials have not, such that the covariate information is missing for all participants in the latter. In this article, we provide identification results for potential (counterfactual) outcome means and average treatment effects in the target population when covariate data are systematically missing from some of the trials in the meta-analysis. We propose three estimators for the average treatment effect in the target population, examine their asymptotic properties, and show that they have good finite-sample performance in simulation studies. We use the estimators to analyze data from two large lung cancer screening trials and target population data from the National Health and Nutrition Examination Survey (NHANES). To accommodate the complex survey design of the NHANES, we modify the methods to incorporate survey sampling weights and allow for clustering.},
  archive      = {J_BIOSTAT},
  author       = {Steingrimsson, Jon A and Barker, David H and Bie, Ruofan and Dahabreh, Issa J},
  doi          = {10.1093/biostatistics/kxad006},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {289-305},
  shortjournal = {Biostatistics},
  title        = {Systematically missing data in causally interpretable meta-analysis},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inference after latent variable estimation for single-cell
RNA sequencing data. <em>BIOSTAT</em>, <em>25</em>(1), 270–287. (<a
href="https://doi.org/10.1093/biostatistics/kxac047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the analysis of single-cell RNA sequencing data, researchers often characterize the variation between cells by estimating a latent variable, such as cell type or pseudotime, representing some aspect of the cell’s state. They then test each gene for association with the estimated latent variable. If the same data are used for both of these steps, then standard methods for computing p-values in the second step will fail to achieve statistical guarantees such as Type 1 error control. Furthermore, approaches such as sample splitting that can be applied to solve similar problems in other settings are not applicable in this context. In this article, we introduce count splitting , a flexible framework that allows us to carry out valid inference in this setting, for virtually any latent variable estimation technique and inference approach, under a Poisson assumption. We demonstrate the Type 1 error control and power of count splitting in a simulation study and apply count splitting to a data set of pluripotent stem cells differentiating to cardiomyocytes.},
  archive      = {J_BIOSTAT},
  author       = {Neufeld, Anna and Gao, Lucy L and Popp, Joshua and Battle, Alexis and Witten, Daniela},
  doi          = {10.1093/biostatistics/kxac047},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {270-287},
  shortjournal = {Biostatistics},
  title        = {Inference after latent variable estimation for single-cell RNA sequencing data},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CGR-CUSUM: A continuous time generalized rapid response
cumulative sum chart. <em>BIOSTAT</em>, <em>25</em>(1), 253–269. (<a
href="https://doi.org/10.1093/biostatistics/kxac041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapidly detecting problems in the quality of care is of utmost importance for the well-being of patients. Without proper inspection schemes, such problems can go undetected for years. Cumulative sum (CUSUM) charts have proven to be useful for quality control, yet available methodology for survival outcomes is limited. The few available continuous time inspection charts usually require the researcher to specify an expected increase in the failure rate in advance, thereby requiring prior knowledge about the problem at hand. Misspecifying parameters can lead to false positive alerts and large detection delays. To solve this problem, we take a more general approach to derive the new Continuous time Generalized Rapid response CUSUM (CGR-CUSUM) chart. We find an expression for the approximate average run length (average time to detection) and illustrate the possible gain in detection speed by using the CGR-CUSUM over other commonly used monitoring schemes on a real-life data set from the Dutch Arthroplasty Register as well as in simulation studies. Besides the inspection of medical procedures, the CGR-CUSUM can also be used for other real-time inspection schemes such as industrial production lines and quality control of services.},
  archive      = {J_BIOSTAT},
  author       = {Gomon, Daniel and Putter, Hein and Nelissen, Rob G H H and Van Der Pas, Stéphanie},
  doi          = {10.1093/biostatistics/kxac041},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {253-269},
  shortjournal = {Biostatistics},
  title        = {CGR-CUSUM: A continuous time generalized rapid response cumulative sum chart},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive clinical trial designs with blinded selection of
binary composite endpoints and sample size reassessment.
<em>BIOSTAT</em>, <em>25</em>(1), 237–252. (<a
href="https://doi.org/10.1093/biostatistics/kxac040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For randomized clinical trials where a single, primary, binary endpoint would require unfeasibly large sample sizes, composite endpoints (CEs) are widely chosen as the primary endpoint. Despite being commonly used, CEs entail challenges in designing and interpreting results. Given that the components may be of different relevance and have different effect sizes, the choice of components must be made carefully. Especially, sample size calculations for composite binary endpoints depend not only on the anticipated effect sizes and event probabilities of the composite components but also on the correlation between them. However, information on the correlation between endpoints is usually not reported in the literature which can be an obstacle for designing future sound trials. We consider two-arm randomized controlled trials with a primary composite binary endpoint and an endpoint that consists only of the clinically more important component of the CE. We propose a trial design that allows an adaptive modification of the primary endpoint based on blinded information obtained at an interim analysis. Especially, we consider a decision rule to select between a CE and its most relevant component as primary endpoint. The decision rule chooses the endpoint with the lower estimated required sample size. Additionally, the sample size is reassessed using the estimated event probabilities and correlation, and the expected effect sizes of the composite components. We investigate the statistical power and significance level under the proposed design through simulations. We show that the adaptive design is equally or more powerful than designs without adaptive modification on the primary endpoint. Besides, the targeted power is achieved even if the correlation is misspecified at the planning stage while maintaining the type 1 error. All the computations are implemented in R and illustrated by means of a peritoneal dialysis trial.},
  archive      = {J_BIOSTAT},
  author       = {Roig, Marta Bofill and Melis, Guadalupe Gómez and Posch, Martin and Koenig, Franz},
  doi          = {10.1093/biostatistics/kxac040},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {237-252},
  shortjournal = {Biostatistics},
  title        = {Adaptive clinical trial designs with blinded selection of binary composite endpoints and sample size reassessment},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible evaluation of surrogacy in platform studies.
<em>BIOSTAT</em>, <em>25</em>(1), 220–236. (<a
href="https://doi.org/10.1093/biostatistics/kxac053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trial-level surrogates are useful tools for improving the speed and cost effectiveness of trials but surrogates that have not been properly evaluated can cause misleading results. The evaluation procedure is often contextual and depends on the type of trial setting. There have been many proposed methods for trial-level surrogate evaluation, but none, to our knowledge, for the specific setting of platform studies. As platform studies are becoming more popular, methods for surrogate evaluation using them are needed. These studies also offer a rich data resource for surrogate evaluation that would not normally be possible. However, they also offer a set of statistical issues including heterogeneity of the study population, treatments, implementation, and even potentially the quality of the surrogate. We propose the use of a hierarchical Bayesian semiparametric model for the evaluation of potential surrogates using nonparametric priors for the distribution of true effects based on Dirichlet process mixtures. The motivation for this approach is to flexibly model relationships between the treatment effect on the surrogate and the treatment effect on the outcome and also to identify potential clusters with differential surrogate value in a data-driven manner so that treatment effects on the surrogate can be used to reliably predict treatment effects on the clinical outcome. In simulations, we find that our proposed method is superior to a simple, but fairly standard, hierarchical Bayesian method. We demonstrate how our method can be used in a simulated illustrative example (based on the ProBio trial), in which we are able to identify clusters where the surrogate is, and is not useful. We plan to apply our method to the ProBio trial, once it is completed.},
  archive      = {J_BIOSTAT},
  author       = {Sachs, Michael C and Gabriel, Erin E and Crippa, Alessio and Daniels, Michael J},
  doi          = {10.1093/biostatistics/kxac053},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {220-236},
  shortjournal = {Biostatistics},
  title        = {Flexible evaluation of surrogacy in platform studies},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multilayer exponential family factor models for integrative
analysis and learning disease progression. <em>BIOSTAT</em>,
<em>25</em>(1), 203–219. (<a
href="https://doi.org/10.1093/biostatistics/kxac042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current diagnosis of neurological disorders often relies on late-stage clinical symptoms, which poses barriers to developing effective interventions at the premanifest stage. Recent research suggests that biomarkers and subtle changes in clinical markers may occur in a time-ordered fashion and can be used as indicators of early disease. In this article, we tackle the challenges to leverage multidomain markers to learn early disease progression of neurological disorders. We propose to integrate heterogeneous types of measures from multiple domains (e.g., discrete clinical symptoms, ordinal cognitive markers, continuous neuroimaging, and blood biomarkers) using a hierarchical Multilayer Exponential Family Factor (MEFF) model, where the observations follow exponential family distributions with lower-dimensional latent factors. The latent factors are decomposed into shared factors across multiple domains and domain-specific factors, where the shared factors provide robust information to perform extensive phenotyping and partition patients into clinically meaningful and biologically homogeneous subgroups. Domain-specific factors capture remaining unique variations for each domain. The MEFF model also captures nonlinear trajectory of disease progression and orders critical events of neurodegeneration measured by each marker. To overcome computational challenges, we fit our model by approximate inference techniques for large-scale data. We apply the developed method to Parkinson’s Progression Markers Initiative data to integrate biological, clinical, and cognitive markers arising from heterogeneous distributions. The model learns lower-dimensional representations of Parkinson’s disease (PD) and the temporal ordering of the neurodegeneration of PD.},
  archive      = {J_BIOSTAT},
  author       = {Wang, Qinxia and Wang, Yuanjia},
  doi          = {10.1093/biostatistics/kxac042},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {203-219},
  shortjournal = {Biostatistics},
  title        = {Multilayer exponential family factor models for integrative analysis and learning disease progression},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A scalable and unbiased discordance metric with h+.
<em>BIOSTAT</em>, <em>25</em>(1), 188–202. (<a
href="https://doi.org/10.1093/biostatistics/kxac035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A standard unsupervised analysis is to cluster observations into discrete groups using a dissimilarity measure, such as Euclidean distance. If there does not exist a ground-truth label for each observation necessary for external validity metrics, then internal validity metrics, such as the tightness or separation of the clusters, are often used. However, the interpretation of these internal metrics can be problematic when using different dissimilarity measures as they have different magnitudes and ranges of values that they span. To address this problem, previous work introduced the “scale-agnostic” |$G_{+}$| discordance metric; however, this internal metric is slow to calculate for large data. Furthermore, in the setting of unsupervised clustering with |$k$| groups, we show that |$G_{+}$| varies as a function of the proportion of observations assigned to each of the groups (or clusters), referred to as the group balance, which is an undesirable property. To address this problem, we propose a modification of |$G_{+}$|⁠ , referred to as |$H_{+}$|⁠ , and demonstrate that |$H_{+}$| does not vary as a function of group balance using a simulation study and with public single-cell RNA-sequencing data. Finally, we provide scalable approaches to estimate |$H_{+}$|⁠ , which are available in the |$\mathtt{fasthplus}$| R package.},
  archive      = {J_BIOSTAT},
  author       = {Dyjack, Nathan and Baker, Daniel N and Braverman, Vladimir and Langmead, Ben and Hicks, Stephanie C},
  doi          = {10.1093/biostatistics/kxac035},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {188-202},
  shortjournal = {Biostatistics},
  title        = {A scalable and unbiased discordance metric with h+},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differences in set-based tests for sparse alternatives when
testing sets of outcomes compared to sets of explanatory factors in
genetic association studies. <em>BIOSTAT</em>, <em>25</em>(1), 171–187.
(<a href="https://doi.org/10.1093/biostatistics/kxac036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Set-based association tests are widely popular in genetic association settings for their ability to aggregate weak signals and reduce multiple testing burdens. In particular, a class of set-based tests including the Higher Criticism, Berk–Jones, and other statistics have recently been popularized for reaching a so-called detection boundary when signals are rare and weak. Such tests have been applied in two subtly different settings: (a) associating a genetic variant set with a single phenotype and (b) associating a single genetic variant with a phenotype set. A significant issue in practice is the choice of test, especially when deciding between innovated and generalized type methods for detection boundary tests. Conflicting guidance is present in the literature. This work describes how correlation structures generate marked differences in relative operating characteristics for settings (a) and (b). The implications for study design are significant. We also develop novel power bounds that facilitate the aforementioned calculations and allow for analysis of individual testing settings. In more concrete terms, our investigation is motivated by translational expression quantitative trait loci (eQTL) studies in lung cancer. These studies involve both testing for groups of variants associated with a single gene expression (multiple explanatory factors) and testing whether a single variant is associated with a group of gene expressions (multiple outcomes). Results are supported by a collection of simulation studies and illustrated through lung cancer eQTL examples.},
  archive      = {J_BIOSTAT},
  author       = {Sun, Ryan and Shi, Andy and Lin, Xihong},
  doi          = {10.1093/biostatistics/kxac036},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {171-187},
  shortjournal = {Biostatistics},
  title        = {Differences in set-based tests for sparse alternatives when testing sets of outcomes compared to sets of explanatory factors in genetic association studies},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoCoA: Conditional correlation models with association size.
<em>BIOSTAT</em>, <em>25</em>(1), 154–170. (<a
href="https://doi.org/10.1093/biostatistics/kxac032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many scientific questions can be formulated as hypotheses about conditional correlations. For instance, in tests of cognitive and physical performance, the trade-off between speed and accuracy motivates study of the two variables together. A natural question is whether speed–accuracy coupling depends on other variables, such as sustained attention. Classical regression techniques, which posit models in terms of covariates and outcomes, are insufficient to investigate the effect of a third variable on the symmetric relationship between speed and accuracy. In response, we propose a conditional correlation model with association size, a likelihood-based statistical framework to estimate the conditional correlation between speed and accuracy as a function of additional variables. We propose novel measures of the association size, which are analogous to effect sizes on the correlation scale while adjusting for confound variables. In simulation studies, we compare likelihood-based estimators of conditional correlation to semiparametric estimators adapted from genomic studies and find that the former achieves lower bias and variance under both ideal settings and model assumption misspecification. Using neurocognitive data from the Philadelphia Neurodevelopmental Cohort, we demonstrate that greater sustained attention is associated with stronger speed–accuracy coupling in a complex reasoning task while controlling for age. By highlighting conditional correlations as the outcome of interest, our model provides complementary insights to traditional regression modeling and partitioned correlation analyses.},
  archive      = {J_BIOSTAT},
  author       = {Tu, Danni and Mahony, Bridget and Moore, Tyler M and Bertolero, Maxwell A and Alexander-Bloch, Aaron F and Gur, Ruben and Bassett, Dani S and Satterthwaite, Theodore D and Raznahan, Armin and Shinohara, Russell T},
  doi          = {10.1093/biostatistics/kxac032},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {154-170},
  shortjournal = {Biostatistics},
  title        = {CoCoA: Conditional correlation models with association size},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An online framework for survival analysis: Reframing cox
proportional hazards model for large data sets and neural networks.
<em>BIOSTAT</em>, <em>25</em>(1), 134–153. (<a
href="https://doi.org/10.1093/biostatistics/kxac039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many biomedical applications, outcome is measured as a “time-to-event” (e.g., disease progression or death). To assess the connection between features of a patient and this outcome, it is common to assume a proportional hazards model and fit a proportional hazards regression (or Cox regression). To fit this model, a log-concave objective function known as the “partial likelihood” is maximized. For moderate-sized data sets, an efficient Newton–Raphson algorithm that leverages the structure of the objective function can be employed. However, in large data sets this approach has two issues: (i) The computational tricks that leverage structure can also lead to computational instability; (ii) The objective function does not naturally decouple: Thus, if the data set does not fit in memory, the model can be computationally expensive to fit. This additionally means that the objective is not directly amenable to stochastic gradient-based optimization methods. To overcome these issues, we propose a simple, new framing of proportional hazards regression: This results in an objective function that is amenable to stochastic gradient descent. We show that this simple modification allows us to efficiently fit survival models with very large data sets. This also facilitates training complex, for example, neural-network-based, models with survival data.},
  archive      = {J_BIOSTAT},
  author       = {Tarkhan, Aliasghar and Simon, Noah},
  doi          = {10.1093/biostatistics/kxac039},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {134-153},
  shortjournal = {Biostatistics},
  title        = {An online framework for survival analysis: Reframing cox proportional hazards model for large data sets and neural networks},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An imputation approach for a time-to-event analysis subject
to missing outcomes due to noncoverage in disease registries.
<em>BIOSTAT</em>, <em>25</em>(1), 117–133. (<a
href="https://doi.org/10.1093/biostatistics/kxac049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disease incidence data in a national-based cohort study would ideally be obtained through a national disease registry. Unfortunately, no such registry currently exists in the United States. Instead, the results from individual state registries need to be combined to ascertain certain disease diagnoses in the United States. The National Cancer Institute has initiated a program to assemble all state registries to provide a complete assessment of all cancers in the United States. Unfortunately, not all registries have agreed to participate. In this article, we develop an imputation-based approach that uses self-reported cancer diagnosis from longitudinally collected questionnaires to impute cancer incidence not covered by the combined registry. We propose a two-step procedure, where in the first step a mover–stayer model is used to impute a participant’s registry coverage status when it is only reported at the time of the questionnaires given at 10-year intervals and the time of the last-alive vital status and death. In the second step, we propose a semiparametric working model, fit using an imputed coverage area sample identified from the mover–stayer model, to impute registry-based survival outcomes for participants in areas not covered by the registry. The simulation studies show the approach performs well as compared with alternative ad hoc approaches for dealing with this problem. We illustrate the methodology with an analysis that links the United States Radiologic Technologists study cohort with the combined registry that includes 32 of the 50 states.},
  archive      = {J_BIOSTAT},
  author       = {Shih, Joanna H and Albert, Paul S and Fine, Jason and Liu, Danping},
  doi          = {10.1093/biostatistics/kxac049},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {117-133},
  shortjournal = {Biostatistics},
  title        = {An imputation approach for a time-to-event analysis subject to missing outcomes due to noncoverage in disease registries},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Time-to-event surrogate endpoint validation using mediation
analysis and meta-analytic data. <em>BIOSTAT</em>, <em>25</em>(1),
98–116. (<a
href="https://doi.org/10.1093/biostatistics/kxac044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the ongoing development of treatments and the resulting increase in survival in oncology, clinical trials based on endpoints such as overall survival may require long follow-up periods to observe sufficient events and ensure adequate statistical power. This increase in follow-up time may compromise the feasibility of the study. The use of surrogate endpoints instead of final endpoints may be attractive for these studies. However, before a surrogate can be used in a clinical trial, it must be statistically validated. In this article, we propose an approach to validate surrogates when both the surrogate and final endpoints are censored event times. This approach is developed for meta-analytic data and uses a mediation analysis to decompose the total effect of the treatment on the final endpoint as a direct effect and an indirect effect through the surrogate. The meta-analytic nature of the data is accounted for in a joint model with random effects at the trial level. The proportion of the indirect effect over the total effect of the treatment on the final endpoint can be computed from the parameters of the model and used as a measure of surrogacy. We applied this method to investigate time-to-relapse as a surrogate endpoint for overall survival in resectable gastric cancer.},
  archive      = {J_BIOSTAT},
  author       = {Le Coënt, Quentin and Legrand, Catherine and Rondeau, Virginie},
  doi          = {10.1093/biostatistics/kxac044},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {98-116},
  shortjournal = {Biostatistics},
  title        = {Time-to-event surrogate endpoint validation using mediation analysis and meta-analytic data},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint modeling of longitudinal and competing-risk data using
cumulative incidence functions for the failure submodels accounting for
potential failure cause misclassification through double sampling.
<em>BIOSTAT</em>, <em>25</em>(1), 80–97. (<a
href="https://doi.org/10.1093/biostatistics/kxac043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the literature on joint modeling of longitudinal and competing-risk data is based on cause-specific hazards, although modeling of the cumulative incidence function (CIF) is an easier and more direct approach to evaluate the prognosis of an event. We propose a flexible class of shared parameter models to jointly model a normally distributed marker over time and multiple causes of failure using CIFs for the survival submodels, with CIFs depending on the “true” marker value over time (i.e., removing the measurement error). The generalized odds rate transformation is applied, thus a proportional subdistribution hazards model is a special case. The requirement that the all-cause CIF should be bounded by 1 is formally considered. The proposed models are extended to account for potential failure cause misclassification, where the true failure causes are available in a small random sample of individuals. We also provide a multistate representation of the whole population by defining mutually exclusive states based on the marker values and the competing risks. Based solely on the assumed joint model, we derive fully Bayesian posterior samples for state occupation and transition probabilities. The proposed approach is evaluated in a simulation study and, as an illustration, it is fitted to real data from people with HIV.},
  archive      = {J_BIOSTAT},
  author       = {Thomadakis, Christos and Meligkotsidou, Loukia and Yiannoutsos, Constantin T and Touloumi, Giota},
  doi          = {10.1093/biostatistics/kxac043},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {80-97},
  shortjournal = {Biostatistics},
  title        = {Joint modeling of longitudinal and competing-risk data using cumulative incidence functions for the failure submodels accounting for potential failure cause misclassification through double sampling},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessing the causal effects of a stochastic intervention in
time series data: Are heat alerts effective in preventing deaths and
hospitalizations? <em>BIOSTAT</em>, <em>25</em>(1), 57–79. (<a
href="https://doi.org/10.1093/biostatistics/kxad002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The methodological development of this article is motivated by the need to address the following scientific question: does the issuance of heat alerts prevent adverse health effects? Our goal is to address this question within a causal inference framework in the context of time series data. A key challenge is that causal inference methods require the overlap assumption to hold: each unit (i.e., a day) must have a positive probability of receiving the treatment (i.e., issuing a heat alert on that day). In our motivating example, the overlap assumption is often violated: the probability of issuing a heat alert on a cooler day is near zero. To overcome this challenge, we propose a stochastic intervention for time series data which is implemented via an incremental time-varying propensity score (ItvPS). The ItvPS intervention is executed by multiplying the probability of issuing a heat alert on day |$t$| —conditional on past information up to day |$t$| —by an odds ratio |$\delta_t$|⁠ . First, we introduce a new class of causal estimands, which relies on the ItvPS intervention. We provide theoretical results to show that these causal estimands can be identified and estimated under a weaker version of the overlap assumption. Second, we propose nonparametric estimators based on the ItvPS and derive an upper bound for the variances of these estimators. Third, we extend this framework to multisite time series using a spatial meta-analysis approach. Fourth, we show that the proposed estimators perform well in terms of bias and root mean squared error via simulations. Finally, we apply our proposed approach to estimate the causal effects of increasing the probability of issuing heat alerts on each warm-season day in reducing deaths and hospitalizations among Medicare enrollees in 2837 US counties.},
  archive      = {J_BIOSTAT},
  author       = {Wu, Xiao and Weinberger, Kate R and Wellenius, Gregory A and Dominici, Francesca and Braun, Danielle},
  doi          = {10.1093/biostatistics/kxad002},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {57-79},
  shortjournal = {Biostatistics},
  title        = {Assessing the causal effects of a stochastic intervention in time series data: Are heat alerts effective in preventing deaths and hospitalizations?},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatiotemporal varying coefficient model for respiratory
disease mapping in taiwan. <em>BIOSTAT</em>, <em>25</em>(1), 40–56. (<a
href="https://doi.org/10.1093/biostatistics/kxac046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Respiratory diseases have been global public health problems for a long time. In recent years, air pollutants as important risk factors have drawn lots of attention. In this study, we investigate the influence of |$\pm2.5$| (particulate matters in diameter less than 2.5 |${\rm{\mu }} m$|⁠ ) on hospital visit rates for respiratory diseases in Taiwan. To reveal the spatiotemporal pattern of data, we propose a Bayesian disease mapping model with spatially varying coefficients and a parametric temporal trend. Model fitting is conducted using the integrated nested Laplace approximation, which is a widely applied technique for large-scale data sets due to its high computational efficiency. The finite sample performance of the proposed method is studied through a series of simulations. As demonstrated by simulations, the proposed model can improve both the parameter estimation performance and the prediction performance. We apply the proposed model on the respiratory disease data in 328 third-level administrative regions in Taiwan and find significant associations between hospital visit rates and |$\pm2.5$|⁠ .},
  archive      = {J_BIOSTAT},
  author       = {Wang, Feifei and Duan, Congyuan and Li, Yang and Huang, Hui and Shia, Ben-Chang},
  doi          = {10.1093/biostatistics/kxac046},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {40-56},
  shortjournal = {Biostatistics},
  title        = {Spatiotemporal varying coefficient model for respiratory disease mapping in taiwan},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian framework for incorporating exposure uncertainty
into health analyses with application to air pollution and stillbirth.
<em>BIOSTAT</em>, <em>25</em>(1), 20–39. (<a
href="https://doi.org/10.1093/biostatistics/kxac034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies of the relationships between environmental exposures and adverse health outcomes often rely on a two-stage statistical modeling approach, where exposure is modeled/predicted in the first stage and used as input to a separately fit health outcome analysis in the second stage. Uncertainty in these predictions is frequently ignored, or accounted for in an overly simplistic manner when estimating the associations of interest. Working in the Bayesian setting, we propose a flexible kernel density estimation (KDE) approach for fully utilizing posterior output from the first stage modeling/prediction to make accurate inference on the association between exposure and health in the second stage, derive the full conditional distributions needed for efficient model fitting, detail its connections with existing approaches, and compare its performance through simulation. Our KDE approach is shown to generally have improved performance across several settings and model comparison metrics. Using competing approaches, we investigate the association between lagged daily ambient fine particulate matter levels and stillbirth counts in New Jersey (2011–2015), observing an increase in risk with elevated exposure 3 days prior to delivery. The newly developed methods are available in the R package KDExp .},
  archive      = {J_BIOSTAT},
  author       = {Comess, Saskia and Chang, Howard H and Warren, Joshua L},
  doi          = {10.1093/biostatistics/kxac034},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {20-39},
  shortjournal = {Biostatistics},
  title        = {A bayesian framework for incorporating exposure uncertainty into health analyses with application to air pollution and stillbirth},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple exposure distributed lag models with variable
selection. <em>BIOSTAT</em>, <em>25</em>(1), 1–19. (<a
href="https://doi.org/10.1093/biostatistics/kxac038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed lag models are useful in environmental epidemiology as they allow the user to investigate critical windows of exposure, defined as the time periods during which exposure to a pollutant adversely affects health outcomes. Recent studies have focused on estimating the health effects of a large number of environmental exposures, or an environmental mixture, on health outcomes. In such settings, it is important to understand which environmental exposures affect a particular outcome, while acknowledging the possibility that different exposures have different critical windows. Further, in studies of environmental mixtures, it is important to identify interactions among exposures and to account for the fact that this interaction may occur between two exposures having different critical windows. Exposure to one exposure early in time could cause an individual to be more or less susceptible to another exposure later in time. We propose a Bayesian model to estimate the temporal effects of a large number of exposures on an outcome. We use spike-and-slab priors and semiparametric distributed lag curves to identify important exposures and exposure interactions and discuss extensions with improved power to detect harmful exposures. We then apply these methods to estimate the effects of exposure to multiple air pollutants during pregnancy on birthweight from vital records in Colorado.},
  archive      = {J_BIOSTAT},
  author       = {Antonelli, Joseph and Wilson, Ander and Coull, Brent A},
  doi          = {10.1093/biostatistics/kxac038},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Biostatistics},
  title        = {Multiple exposure distributed lag models with variable selection},
  volume       = {25},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
