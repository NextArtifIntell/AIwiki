<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>BIOMET_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="biomet---90">BIOMET - 90</h2>
<ul>
<li><details>
<summary>
(2024). Inference for partial correlations of a multivariate
gaussian time series. <em>BIOMET</em>, <em>111</em>(4), 1437–1444. (<a
href="https://doi.org/10.1093/biomet/asae012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive an asymptotic joint distribution and novel covariance estimator for the partial correlations of a multivariate Gaussian time series under mild regularity conditions. Using our derived asymptotic distribution, we develop a Wald confidence interval and testing procedure for inference of individual partial correlations for time series data. Through simulation we demonstrate that our proposed confidence interval attains higher coverage rates and our testing procedure achieves false positive rates closer to the nominal levels than approaches that assume independent observations when autocorrelation is present.},
  archive      = {J_BIOMET},
  author       = {Dilernia, A S and Fiecas, M and Zhang, L},
  doi          = {10.1093/biomet/asae012},
  journal      = {Biometrika},
  month        = {12},
  number       = {4},
  pages        = {1437-1444},
  shortjournal = {Biometrika},
  title        = {Inference for partial correlations of a multivariate gaussian time series},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sharp symbolic nonparametric bounds for measures of benefit
in observational and imperfect randomized studies with ordinal outcomes.
<em>BIOMET</em>, <em>111</em>(4), 1429–1436. (<a
href="https://doi.org/10.1093/biomet/asae020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The probability of benefit can be a valuable and meaningful measure of treatment effect. Particularly for an ordinal outcome, it can have an intuitive interpretation. Unfortunately, this measure, and variations of it, are not identifiable even in randomized trials with perfect compliance. There is, for this reason, a long literature on nonparametric bounds for unidentifiable measures of benefit. These have primarily focused on perfect randomized trial settings and one or two specific estimands. We expand these bounds to observational settings with unmeasured confounders and imperfect randomized trials for all three estimands considered in the literature: the probability of benefit, the probability of no harm and the relative treatment effect.},
  archive      = {J_BIOMET},
  author       = {Gabriel, Erin E and Sachs, Michael C and Jensen, Andreas Kryger},
  doi          = {10.1093/biomet/asae020},
  journal      = {Biometrika},
  month        = {12},
  number       = {4},
  pages        = {1429-1436},
  shortjournal = {Biometrika},
  title        = {Sharp symbolic nonparametric bounds for measures of benefit in observational and imperfect randomized studies with ordinal outcomes},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On propensity score matching with a diverging number of
matches. <em>BIOMET</em>, <em>111</em>(4), 1421–1428. (<a
href="https://doi.org/10.1093/biomet/asae026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper re-examines the work of Abadie &amp; Imbens (2016) on propensity score matching for average treatment effect estimation. We explore the asymptotic behaviour of these estimators when the number of nearest neighbours, M , grows with the sample size. It is shown, while not surprising, but technically nontrivial, that the modified estimators can improve upon the original fixed M -estimators in terms of efficiency. Additionally, we demonstrate the potential to attain the semiparametric efficiency lower bound when the propensity score admits some special structures, echoing the insight of Hahn (1998) .},
  archive      = {J_BIOMET},
  author       = {He, Yihui and Han, Fang},
  doi          = {10.1093/biomet/asae026},
  journal      = {Biometrika},
  month        = {12},
  number       = {4},
  pages        = {1421-1428},
  shortjournal = {Biometrika},
  title        = {On propensity score matching with a diverging number of matches},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Covariate adjustment in randomized experiments with missing
outcomes and covariates. <em>BIOMET</em>, <em>111</em>(4), 1413–1420.
(<a href="https://doi.org/10.1093/biomet/asae017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covariate adjustment can improve precision in analysing randomized experiments. With fully observed data, regression adjustment and propensity score weighting are asymptotically equivalent in improving efficiency over unadjusted analysis. When some outcomes are missing, we consider combining these two adjustment methods with the inverse probability of observation weighting for handling missing outcomes, and show that the equivalence between the two methods breaks down. Regression adjustment no longer ensures efficiency gain over unadjusted analysis unless the true outcome model is linear in covariates or the outcomes are missing completely at random. Propensity score weighting, in contrast, still guarantees efficiency over unadjusted analysis, and including more covariates in adjustment never harms asymptotic efficiency. Moreover, we establish the value of using partially observed covariates to secure additional efficiency by the missingness indicator method, which imputes all missing covariates by zero and uses the union of the completed covariates and corresponding missingness indicators as the new, fully observed covariates. Based on these findings, we recommend using regression adjustment in combination with the missingness indicator method if the linear outcome model or missing-completely-at-random assumption is plausible and using propensity score weighting with the missingness indicator method otherwise.},
  archive      = {J_BIOMET},
  author       = {Zhao, Anqi and Ding, Peng and Li, Fan},
  doi          = {10.1093/biomet/asae017},
  journal      = {Biometrika},
  month        = {12},
  number       = {4},
  pages        = {1413-1420},
  shortjournal = {Biometrika},
  title        = {Covariate adjustment in randomized experiments with missing outcomes and covariates},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). More power by using fewer permutations. <em>BIOMET</em>,
<em>111</em>(4), 1405–1412. (<a
href="https://doi.org/10.1093/biomet/asae031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is conventionally believed that permutation-based testing methods should ideally use all permutations. We challenge this by showing that we can sometimes obtain dramatically more power by using a tiny subgroup. As the subgroup is tiny, this also comes at a much lower computational cost. Moreover, the method remains valid for the same hypotheses. We exploit this to improve the popular permutation-based Westfall and Young MaxT multiple testing method. We analyse the relative efficiency in a Gaussian location model, and find the largest gain in high dimensions.},
  archive      = {J_BIOMET},
  author       = {Koning, Nick W},
  doi          = {10.1093/biomet/asae031},
  journal      = {Biometrika},
  month        = {12},
  number       = {4},
  pages        = {1405-1412},
  shortjournal = {Biometrika},
  title        = {More power by using fewer permutations},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inference for possibly misspecified generalized linear
models with nonpolynomial-dimensional nuisance parameters.
<em>BIOMET</em>, <em>111</em>(4), 1387–1404. (<a
href="https://doi.org/10.1093/biomet/asae024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is routine practice in statistical modelling to first select variables and then make inference for the selected model as in stepwise regression. Such inference is made upon the assumption that the selected model is true. However, without this assumption, one would not know the validity of the inference. Similar problems also exist in high-dimensional regression with regularization. To address these problems, we propose a dimension-reduced generalized likelihood ratio test for generalized linear models with nonpolynomial dimensionality, based on quasilikelihood estimation that allows for misspecification of the conditional variance. The test has nearly oracle performance when using the correct amount of shrinkage and has robust performance against the choice of regularization parameter across a large range. We further develop an adaptive data-driven dimension-reduced generalized likelihood ratio test and prove that, with probability going to one, it is an oracle generalized likelihood ratio test. However, in ultrahigh-dimensional models the penalized estimation may produce spuriously important variables that deteriorate the performance of the test. To tackle this problem, we introduce a cross-fitted dimension-reduced generalized likelihood ratio test, which is not only free of spurious effects, but robust against the choice of regularization parameter. We establish limiting distributions of the proposed tests. Their advantages are highlighted via theoretical and empirical comparisons to some competitive tests. An application to breast cancer data illustrates the use of our proposed methodology.},
  archive      = {J_BIOMET},
  author       = {Hong, Shaoxin and Jiang, Jiancheng and Jiang, Xuejun and Wang, Haofeng},
  doi          = {10.1093/biomet/asae024},
  journal      = {Biometrika},
  month        = {12},
  number       = {4},
  pages        = {1387-1404},
  shortjournal = {Biometrika},
  title        = {Inference for possibly misspecified generalized linear models with nonpolynomial-dimensional nuisance parameters},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A model-free variable screening method for optimal treatment
regimes with high-dimensional survival data. <em>BIOMET</em>,
<em>111</em>(4), 1369–1386. (<a
href="https://doi.org/10.1093/biomet/asae022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a model-free variable screening method for the optimal treatment regime with high-dimensional survival data. The proposed screening method provides a unified framework to select the active variables in a prespecified target population, including the treated group as a special case. Based on this framework, the optimal treatment regime is exactly the optimal classifier that minimizes a weighted misclassification error rate, with weights associated with survival outcome variables, the censoring distribution and a prespecified target population. Our main contribution involves reformulating the weighted classification problem into a classification problem within a hypothetical population, where the observed data can be viewed as a sample obtained from outcome-dependent sampling, with the selection probability inversely proportional to the weights. Consequently, we introduce the weighted Kolmogorov–Smirnov approach for selecting active variables in the optimal treatment regime, extending the conventional Kolmogorov–Smirnov method for binary classification. Additionally, the proposed screening method exhibits two levels of robustness. The first level of robustness is achieved because the proposed method does not require any model assumptions for the survival outcome on treatment and covariates, whereas the other is attained as the form of treatment regimes is allowed to be unspecified even without requiring convex surrogate loss, such as logit loss or hinge loss. As a result, the proposed screening method is robust to model misspecifications, and nonparametric learning methods such as random forests and boosting can be applied to those selected variables for further analysis. The theoretical properties of the proposed method are established. The performance of the proposed method is examined through simulation studies and illustrated by a lung cancer dataset.},
  archive      = {J_BIOMET},
  author       = {Yang, Cheng-Han and Cheng, Yu-Jen},
  doi          = {10.1093/biomet/asae022},
  journal      = {Biometrika},
  month        = {12},
  number       = {4},
  pages        = {1369-1386},
  shortjournal = {Biometrika},
  title        = {A model-free variable screening method for optimal treatment regimes with high-dimensional survival data},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sensitivity analysis for matched observational studies with
continuous exposures and binary outcomes. <em>BIOMET</em>,
<em>111</em>(4), 1349–1368. (<a
href="https://doi.org/10.1093/biomet/asae021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matching is one of the most widely used study designs for adjusting for measured confounders in observational studies. However, unmeasured confounding may exist and cannot be removed by matching. Therefore, a sensitivity analysis is typically needed to assess a causal conclusion’s sensitivity to unmeasured confounding. Sensitivity analysis frameworks for binary exposures have been well established for various matching designs and are commonly used in various studies. However, unlike the binary exposure case, there still lacks valid and general sensitivity analysis methods for continuous exposures, except in some special cases such as pair matching. To fill this gap in the binary outcome case, we develop a sensitivity analysis framework for general matching designs with continuous exposures and binary outcomes. First, we use probabilistic lattice theory to show that our sensitivity analysis approach is finite population exact under Fisher’s sharp null. Second, we prove a novel design sensitivity formula as a powerful tool for asymptotically evaluating the performance of our sensitivity analysis approach. Third, to allow effect heterogeneity with binary outcomes, we introduce a framework for conducting asymptotically exact inference and sensitivity analysis on generalized attributable effects with binary outcomes via mixed-integer programming. Fourth, for the continuous outcome case, we show that conducting an asymptotically exact sensitivity analysis in matched observational studies when both the exposures and outcomes are continuous is generally NP-hard, except in some special cases such as pair matching. As a real data application, we apply our new methods to study the effect of early-life lead exposure on juvenile delinquency. An implementation of the methods in this work is available in the R package doseSens .},
  archive      = {J_BIOMET},
  author       = {Zhang, Jeffrey and Small, Dylan S and Heng, Siyu},
  doi          = {10.1093/biomet/asae021},
  journal      = {Biometrika},
  month        = {12},
  number       = {4},
  pages        = {1349-1368},
  shortjournal = {Biometrika},
  title        = {Sensitivity analysis for matched observational studies with continuous exposures and binary outcomes},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bootstrap test procedure for variance components in
nonlinear mixed effects models in the presence of nuisance parameters
and a singular fisher information matrix. <em>BIOMET</em>,
<em>111</em>(4), 1331–1348. (<a
href="https://doi.org/10.1093/biomet/asae025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine the problem of variance component testing in general mixed effects models using the likelihood ratio test. We account for the presence of nuisance parameters, ie, the fact that some untested variances might also be equal to zero. Two main issues arise in this context, leading to a nonregular setting. First, under the null hypothesis, the true parameter value lies on the boundary of the parameter space. Moreover, due to the presence of nuisance parameters, the exact locations of these boundary points are not known, which prevents the use of classical asymptotic theory of maximum likelihood estimation. Then, in the specific context of nonlinear mixed effects models, the Fisher information matrix is singular at the true parameter value. We address these two points by proposing a shrunk parametric bootstrap procedure, which is straightforward to apply even for nonlinear models. We show that the procedure is consistent, solving both the boundary and the singularity issues, and we provide a verifiable criterion for the applicability of our theoretical results. We show through a simulation study that, compared to the asymptotic approach, our procedure has a better small sample performance and is more robust to the presence of nuisance parameters. A real data application on bird growth rates is also provided.},
  archive      = {J_BIOMET},
  author       = {Guédon, T and Baey, C and Kuhn, E},
  doi          = {10.1093/biomet/asae025},
  journal      = {Biometrika},
  month        = {12},
  number       = {4},
  pages        = {1331-1348},
  shortjournal = {Biometrika},
  title        = {Bootstrap test procedure for variance components in nonlinear mixed effects models in the presence of nuisance parameters and a singular fisher information matrix},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Debiasing welch’s method for spectral density estimation.
<em>BIOMET</em>, <em>111</em>(4), 1313–1329. (<a
href="https://doi.org/10.1093/biomet/asae033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Welch’s method provides an estimator of the power spectral density that is statistically consistent. This is achieved by averaging over periodograms calculated from overlapping segments of a time series. For a finite-length time series, while the variance of the estimator decreases as the number of segments increases, the magnitude of the estimator’s bias increases: a bias-variance trade-off ensues when setting the segment number. We address this issue by providing a novel method for debiasing Welch’s method that maintains the computational complexity and asymptotic consistency, and leads to improved finite-sample performance. Theoretical results are given for fourth-order stationary processes with finite fourth-order moments and an absolutely convergent fourth-order cumulant function. The significant bias reduction is demonstrated with numerical simulation and an application to real-world data. Our estimator also permits irregular spacing over frequency and we demonstrate how this may be employed for signal compression and further variance reduction. The code accompanying this work is available in R and python .},
  archive      = {J_BIOMET},
  author       = {Astfalck, Lachlan C and Sykulski, Adam M and Cripps, Edward J},
  doi          = {10.1093/biomet/asae033},
  journal      = {Biometrika},
  month        = {12},
  number       = {4},
  pages        = {1313-1329},
  shortjournal = {Biometrika},
  title        = {Debiasing welch’s method for spectral density estimation},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing serial dependence or cross dependence for time
series with underreporting. <em>BIOMET</em>, <em>111</em>(4), 1293–1312.
(<a href="https://doi.org/10.1093/biomet/asae027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practice, it is common for collected data to be underreported, an issue that is particularly prevalent in fields such as the social sciences, ecology and epidemiology. Drawing inferences from such data using conventional statistical methods can lead to incorrect conclusions. In this paper, we study tests for serial or cross dependence in time series data that are subject to underreporting. We introduce new test statistics, develop corresponding group-of-blocks bootstrap techniques and establish their consistency. The methods are shown via simulation studies to be efficient and are used to identify key factors responsible for the spread of dengue fever and the occurrence of cardiovascular disease.},
  archive      = {J_BIOMET},
  author       = {Wei, Keyao and Wang, Lengyang and Xia, Yingcun},
  doi          = {10.1093/biomet/asae027},
  journal      = {Biometrika},
  month        = {12},
  number       = {4},
  pages        = {1293-1312},
  shortjournal = {Biometrika},
  title        = {Testing serial dependence or cross dependence for time series with underreporting},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Difference-based covariance matrix estimation in time series
nonparametric regression with application to specification tests.
<em>BIOMET</em>, <em>111</em>(4), 1277–1292. (<a
href="https://doi.org/10.1093/biomet/asae013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-run covariance matrix estimation is the building block of time series inference. The corresponding difference-based estimator, which avoids detrending, has attracted considerable interest due to its robustness to both smooth and abrupt structural breaks and its competitive finite sample performance. However, existing methods mainly focus on estimators for the univariate process, while their direct and multivariate extensions for most linear models are asymptotically biased. We propose a novel difference-based and debiased long-run covariance matrix estimator for functional linear models with time-varying regression coefficients, allowing time series nonstationarity, long-range dependence, state heteroscedasticity and combinations thereof. We apply the new estimator to (i) the structural stability test, overcoming the notorious nonmonotonic power phenomena caused by piecewise smooth alternatives for regression coefficients, and (ii) the nonparametric residual-based tests for long memory, improving the performance via the residual-free formula of the proposed estimator. The effectiveness of the proposed method is justified theoretically and demonstrated by superior performance in simulation studies, while its usefulness is elaborated via real data analysis. Our method is implemented in the R package mlrv .},
  archive      = {J_BIOMET},
  author       = {Bai, Lujia and Wu, Weichi},
  doi          = {10.1093/biomet/asae013},
  journal      = {Biometrika},
  month        = {12},
  number       = {4},
  pages        = {1277-1292},
  shortjournal = {Biometrika},
  title        = {Difference-based covariance matrix estimation in time series nonparametric regression with application to specification tests},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Individualized dynamic latent factor model for
multi-resolutional data with application to mobile health.
<em>BIOMET</em>, <em>111</em>(4), 1257–1275. (<a
href="https://doi.org/10.1093/biomet/asae015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile health has emerged as a major success for tracking individual health status, due to the popularity and power of smartphones and wearable devices. This has also brought great challenges in handling heterogeneous, multi-resolution data that arise ubiquitously in mobile health due to irregular multivariate measurements collected from individuals. In this paper, we propose an individualized dynamic latent factor model for irregular multi-resolution time series data to interpolate unsampled measurements of time series with low resolution. One major advantage of the proposed method is the capability to integrate multiple irregular time series and multiple subjects by mapping the multi-resolution data to the latent space. In addition, the proposed individualized dynamic latent factor model is applicable to capturing heterogeneous longitudinal information through individualized dynamic latent factors. Our theory provides a bound on the integrated interpolation error and the convergence rate for B -spline approximation methods. Both the simulation studies and the application to smartwatch data demonstrate the superior performance of the proposed method compared to existing methods.},
  archive      = {J_BIOMET},
  author       = {Zhang, J and Xue, F and Xu, Q and Lee, J and Qu, A},
  doi          = {10.1093/biomet/asae015},
  journal      = {Biometrika},
  month        = {12},
  number       = {4},
  pages        = {1257-1275},
  shortjournal = {Biometrika},
  title        = {Individualized dynamic latent factor model for multi-resolutional data with application to mobile health},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Skip sampling: Subsampling in the frequency domain.
<em>BIOMET</em>, <em>111</em>(4), 1241–1256. (<a
href="https://doi.org/10.1093/biomet/asae039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last 35 years, several bootstrap methods for time series have been proposed. Popular time domain methods include the block bootstrap, the stationary bootstrap, the linear process bootstrap, among others; subsampling for time series is also available, and is closely related to the block bootstrap. The frequency domain bootstrap has been performed either by resampling the periodogram ordinates or by resampling the ordinates of the discrete Fourier transform. The paper at hand proposes a novel construction of subsampling the discrete Fourier transform ordinates, and investigates its theoretical properties and realm of applicability. Numerical studies show that the new method performs comparably to the frequency domain bootstrap for linear spectral means and ratio statistics, while at the same time yielding significant computational savings as well as numerical stability. Some key words: Discrete Fourier transform; Spectral density, Time series.},
  archive      = {J_BIOMET},
  author       = {McElroy, Tucker and Politis, Dimitris N},
  doi          = {10.1093/biomet/asae039},
  journal      = {Biometrika},
  month        = {12},
  number       = {4},
  pages        = {1241-1256},
  shortjournal = {Biometrika},
  title        = {Skip sampling: Subsampling in the frequency domain},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Network-adjusted covariates for community detection.
<em>BIOMET</em>, <em>111</em>(4), 1221–1240. (<a
href="https://doi.org/10.1093/biomet/asae011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection is a crucial task in network analysis that can be significantly improved by incorporating subject-level information, ie, covariates. Existing methods have shown the effectiveness of using covariates on the low-degree nodes, but rarely discuss the case where communities have significantly different density levels, ie, multiscale networks. In this paper, we introduce a novel method that addresses this challenge by constructing network-adjusted covariates, which leverage the network connections and covariates with a node-specific weight for each node. This weight can be calculated without tuning parameters. We present novel theoretical results on the strong consistency of our method under degree-corrected stochastic blockmodels with covariates, even in the presence of misspecification and multiple sparse communities. Additionally, we establish a general lower bound for the community detection problem when both the network and covariates are present, and it shows that our method is optimal for connection intensity up to a constant factor. Our method outperforms existing approaches in simulations and a LastFM app user network. We then compare our method with others on a statistics publication citation network where 30 % of nodes are isolated, and our method produces reasonable and balanced results. Our method is implemented in the R package NAC .},
  archive      = {J_BIOMET},
  author       = {Hu, Y and Wang, W},
  doi          = {10.1093/biomet/asae011},
  journal      = {Biometrika},
  month        = {12},
  number       = {4},
  pages        = {1221-1240},
  shortjournal = {Biometrika},
  title        = {Network-adjusted covariates for community detection},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On some algorithms for estimation in gaussian graphical
models. <em>BIOMET</em>, <em>111</em>(4), 1201–1219. (<a
href="https://doi.org/10.1093/biomet/asae028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Gaussian graphical models, the likelihood equations must typically be solved iteratively. This paper investigates two algorithms: a version of iterative proportional scaling, which avoids inversion of large matrices, and an algorithm based on convex duality and operating on the covariance matrix by neighbourhood coordinate descent, which corresponds to the graphical lasso with zero penalty. For large, sparse graphs, the iterative proportional scaling algorithm appears feasible and has simple convergence properties. The algorithm based on neighbourhood coordinate descent is extremely fast and less dependent on sparsity, but needs a positive-definite starting value to converge. We provide an algorithm for finding such a starting value for graphs with low colouring number. As a consequence, we also obtain a simplified proof of existence of the maximum likelihood estimator in such cases.},
  archive      = {J_BIOMET},
  author       = {Højsgaard, S and Lauritzen, S},
  doi          = {10.1093/biomet/asae028},
  journal      = {Biometrika},
  month        = {12},
  number       = {4},
  pages        = {1201-1219},
  shortjournal = {Biometrika},
  title        = {On some algorithms for estimation in gaussian graphical models},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing independence for sparse longitudinal data.
<em>BIOMET</em>, <em>111</em>(4), 1187–1199. (<a
href="https://doi.org/10.1093/biomet/asae035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advance of science and technology, more and more data are collected in the form of functions. A fundamental question for a pair of random functions is to test whether they are independent. This problem becomes quite challenging when the random trajectories are sampled irregularly and sparsely for each subject. In other words, each random function is only sampled at a few time-points, and these time-points vary with subjects. Furthermore, the observed data may contain noise. To the best of our knowledge, there exists no consistent test in the literature to test the independence of sparsely observed functional data. We show in this work that testing pointwise independence simultaneously is feasible. The test statistics are constructed by integrating pointwise distance covariances ( Székely et al., 2007 ) and are shown to converge, at a certain rate, to their corresponding population counterparts, which characterize the simultaneous pointwise independence of two random functions. The performance of the proposed methods is further verified by Monte Carlo simulations and analysis of real data.},
  archive      = {J_BIOMET},
  author       = {Zhu, Changbo and Yao, Junwen and Wang, Jane-Ling},
  doi          = {10.1093/biomet/asae035},
  journal      = {Biometrika},
  month        = {12},
  number       = {4},
  pages        = {1187-1199},
  shortjournal = {Biometrika},
  title        = {Testing independence for sparse longitudinal data},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A rank-based sequential test of independence.
<em>BIOMET</em>, <em>111</em>(4), 1169–1186. (<a
href="https://doi.org/10.1093/biomet/asae023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of independence testing for two univariate random variables in a sequential setting. By leveraging recent developments on safe, anytime-valid inference, we propose a test with time-uniform Type-I error control and derive explicit bounds on the finite-sample performance of the test. We demonstrate the empirical performance of the procedure in comparison to existing sequential and nonsequential independence tests. Furthermore, since the proposed test is distribution-free under the null hypothesis, we empirically simulate the gap due to Ville’s inequality, the supermartingale analogue of Markov’s inequality, that is commonly applied to control Type-I error in anytime-valid inference, and apply this to construct a truncated sequential test.},
  archive      = {J_BIOMET},
  author       = {Henzi, Alexander and Law, Michael},
  doi          = {10.1093/biomet/asae023},
  journal      = {Biometrika},
  month        = {12},
  number       = {4},
  pages        = {1169-1186},
  shortjournal = {Biometrika},
  title        = {A rank-based sequential test of independence},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Radial neighbours for provably accurate scalable
approximations of gaussian processes. <em>BIOMET</em>, <em>111</em>(4),
1151–1167. (<a href="https://doi.org/10.1093/biomet/asae029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In geostatistical problems with massive sample size, Gaussian processes can be approximated using sparse directed acyclic graphs to achieve scalable O ( n ) computational complexity. In these models, data at each location are typically assumed conditionally dependent on a small set of parents that usually include a subset of the nearest neighbours. These methodologies often exhibit excellent empirical performance, but the lack of theoretical validation leads to unclear guidance in specifying the underlying graphical model and sensitivity to graph choice. We address these issues by introducing radial-neighbour Gaussian processes, a class of Gaussian processes based on directed acyclic graphs in which directed edges connect every location to all of its neighbours within a predetermined radius. We prove that any radial-neighbour Gaussian process can accurately approximate the corresponding unrestricted Gaussian process in the Wasserstein-2 distance, with an error rate determined by the approximation radius, the spatial covariance function and the spatial dispersion of samples. We offer further empirical validation of our approach via applications on simulated and real-world data, showing excellent performance in both prior and posterior approximations to the original Gaussian process.},
  archive      = {J_BIOMET},
  author       = {Zhu, Yichen and Peruzzi, Michele and Li, Cheng and Dunson, David B},
  doi          = {10.1093/biomet/asae029},
  journal      = {Biometrika},
  month        = {12},
  number       = {4},
  pages        = {1151-1167},
  shortjournal = {Biometrika},
  title        = {Radial neighbours for provably accurate scalable approximations of gaussian processes},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible control of the median of the false discovery
proportion. <em>BIOMET</em>, <em>111</em>(4), 1129–1150. (<a
href="https://doi.org/10.1093/biomet/asae018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a multiple testing procedure that controls the median of the proportion of false discoveries in a flexible way. The procedure requires only a vector of p -values as input and is comparable to the Benjamini–Hochberg method, which controls the mean of the proportion of false discoveries. Our method allows free choice of one or several values of |$ \alpha $| after seeing the data, unlike the Benjamini–Hochberg procedure, which can be very anti-conservative when |$ \alpha $| is chosen post hoc. We prove these claims and illustrate them with simulations. The proposed procedure is inspired by a popular estimator of the total number of true hypotheses. We adapt this estimator to provide simultaneously median unbiased estimators of the proportion of false discoveries, valid for finite samples. This simultaneity allows for the claimed flexibility. Our approach does not assume independence. The time complexity of our method is linear in the number of hypotheses, after sorting the p -values.},
  archive      = {J_BIOMET},
  author       = {Hemerik, Jesse and Solari, Aldo and Goeman, Jelle J},
  doi          = {10.1093/biomet/asae018},
  journal      = {Biometrika},
  month        = {12},
  number       = {4},
  pages        = {1129-1150},
  shortjournal = {Biometrika},
  title        = {Flexible control of the median of the false discovery proportion},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exact selective inference with randomization.
<em>BIOMET</em>, <em>111</em>(4), 1109–1127. (<a
href="https://doi.org/10.1093/biomet/asae019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a pivot for exact selective inference with randomization. Not only does our pivot lead to exact inference in Gaussian regression models, but it is also available in closed form. We reduce this problem to inference for a bivariate truncated Gaussian variable. By doing so, we give up some power that is achieved with approximate maximum likelihood estimation in Panigrahi &amp; Taylor (2023) . Yet our pivot always produces narrower confidence intervals than a closely related data-splitting procedure. We investigate the trade-off between power and exact selective inference on simulated datasets and an HIV drug resistance dataset.},
  archive      = {J_BIOMET},
  author       = {Panigrahi, Snigdha and Fry, Kevin and Taylor, Jonathan},
  doi          = {10.1093/biomet/asae019},
  journal      = {Biometrika},
  month        = {12},
  number       = {4},
  pages        = {1109-1127},
  shortjournal = {Biometrika},
  title        = {Exact selective inference with randomization},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal regimes for algorithm-assisted human
decision-making. <em>BIOMET</em>, <em>111</em>(4), 1089–1108. (<a
href="https://doi.org/10.1093/biomet/asae016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider optimal regimes for algorithm-assisted human decision-making. Such regimes are decision functions of measured pre-treatment variables and, by leveraging natural treatment values, enjoy a superoptimality property whereby they are guaranteed to outperform conventional optimal regimes. When there is unmeasured confounding, the benefit of using superoptimal regimes can be considerable. When there is no unmeasured confounding, superoptimal regimes are identical to conventional optimal regimes. Furthermore, identification of the expected outcome under superoptimal regimes in nonexperimental studies requires the same assumptions as identification of value functions under conventional optimal regimes when the treatment is binary. To illustrate the utility of superoptimal regimes, we derive identification and estimation results in a common instrumental variable setting. We use these derivations to analyse examples from the optimal regimes literature, including a case study of the effect of prompt intensive care treatment on survival.},
  archive      = {J_BIOMET},
  author       = {Stensrud, M J and Laurendeau, J D and Sarvet, A L},
  doi          = {10.1093/biomet/asae016},
  journal      = {Biometrika},
  month        = {12},
  number       = {4},
  pages        = {1089-1108},
  shortjournal = {Biometrika},
  title        = {Optimal regimes for algorithm-assisted human decision-making},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Correction to: “Selective machine learning of doubly robust
functionals.” <em>BIOMET</em>, <em>111</em>(3), 1087. (<a
href="https://doi.org/10.1093/biomet/asae032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  doi          = {10.1093/biomet/asae032},
  journal      = {Biometrika},
  month        = {9},
  number       = {3},
  pages        = {1087},
  shortjournal = {Biometrika},
  title        = {Correction to: ‘Selective machine learning of doubly robust functionals’},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Correction to: “Simplified integrated nested laplace
approximation.” <em>BIOMET</em>, <em>111</em>(3), 1085. (<a
href="https://doi.org/10.1093/biomet/asae030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  doi          = {10.1093/biomet/asae030},
  journal      = {Biometrika},
  month        = {9},
  number       = {3},
  pages        = {1085},
  shortjournal = {Biometrika},
  title        = {Correction to: ‘Simplified integrated nested laplace approximation’},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Second term improvement to generalized linear mixed model
asymptotics. <em>BIOMET</em>, <em>111</em>(3), 1077–1084. (<a
href="https://doi.org/10.1093/biomet/asad072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recent article by Jiang et al. (2022) on generalized linear mixed model asymptotics derived the rates of convergence for the asymptotic variances of maximum likelihood estimators. If m denotes the number of groups and n is the average within-group sample size then the asymptotic variances have orders m − 1 and ( m n ) − 1 ⁠ , depending on the parameter. We extend this theory to provide explicit forms of the ( m n ) − 1 second terms of the asymptotically harder-to-estimate parameters. Improved accuracy of statistical inference and planning are consequences of our theory.},
  archive      = {J_BIOMET},
  author       = {Maestrini, Luca and Bhaskaran, Aishwarya and Wand, Matt P},
  doi          = {10.1093/biomet/asad072},
  journal      = {Biometrika},
  month        = {9},
  number       = {3},
  pages        = {1077-1084},
  shortjournal = {Biometrika},
  title        = {Second term improvement to generalized linear mixed model asymptotics},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A note on minimax robustness of designs against correlated
or heteroscedastic responses. <em>BIOMET</em>, <em>111</em>(3),
1071–1075. (<a href="https://doi.org/10.1093/biomet/asae001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a result according to which certain functions of covariance matrices are maximized at scalar multiples of the identity matrix. This is used to show that experimental designs that are optimal under an assumption of independent, homoscedastic responses can be minimax robust, in broad classes of alternate covariance structures. In particular, it can justify the common practice of disregarding possible dependence, or heteroscedasticity, at the design stage of an experiment.},
  archive      = {J_BIOMET},
  author       = {Wiens, D P},
  doi          = {10.1093/biomet/asae001},
  journal      = {Biometrika},
  month        = {9},
  number       = {3},
  pages        = {1071-1075},
  shortjournal = {Biometrika},
  title        = {A note on minimax robustness of designs against correlated or heteroscedastic responses},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the failure of the bootstrap for chatterjee’s rank
correlation. <em>BIOMET</em>, <em>111</em>(3), 1063–1070. (<a
href="https://doi.org/10.1093/biomet/asae004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While researchers commonly use the bootstrap to quantify the uncertainty of an estimator, it has been noticed that the standard bootstrap, in general, does not work for Chatterjee’s rank correlation. In this paper, we provide proof of this issue under an additional independence assumption, and complement our theory with simulation evidence for general settings. Chatterjee’s rank correlation thus falls into a category of statistics that are asymptotically normal, but bootstrap inconsistent. Valid inferential methods in this case are Chatterjee’s original proposal for testing independence and the analytic asymptotic variance estimator of Lin &amp; Han (2022) for more general purposes. [ Received on 5 April 2023. Editorial decision on 10 January 2024]},
  archive      = {J_BIOMET},
  author       = {Lin, Zhexiao and Han, Fang},
  doi          = {10.1093/biomet/asae004},
  journal      = {Biometrika},
  month        = {9},
  number       = {3},
  pages        = {1063-1070},
  shortjournal = {Biometrika},
  title        = {On the failure of the bootstrap for chatterjee’s rank correlation},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regression analysis of group-tested current status data.
<em>BIOMET</em>, <em>111</em>(3), 1047–1061. (<a
href="https://doi.org/10.1093/biomet/asae006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group testing is an effective way to reduce the time and cost associated with conducting large-scale screening for infectious diseases. Benefits are realized through testing pools formed by combining specimens, such as blood or urine, from different individuals. In some studies, individuals are assessed only once and a time-to-event endpoint is recorded, for example, the time until infection. Combining group testing with this type of endpoint results in group-tested current status data ( Petito &amp; Jewell, 2016 ). To analyse these complex data, we propose methods that estimate a proportional hazard regression model based on test outcomes from measuring the pools. A sieve maximum likelihood estimation approach is developed that approximates the cumulative baseline hazard function with a piecewise constant function. To identify the sieve estimator, a computationally efficient expectation-maximization algorithm is derived by using data augmentation. Asymptotic properties of both the parametric and nonparametric components of the sieve estimator are then established by applying modern empirical process theory. Numerical results from simulation studies show that our proposed method performs nominally and has advantages over the corresponding estimation method based on individual testing results. We illustrate our work by analysing a chlamydia dataset collected by the State Hygienic Laboratory at the University of Iowa.},
  archive      = {J_BIOMET},
  author       = {Li, Shuwei and Hu, Tao and Wang, Lianming and McMahan, Christopher S and Tebbs, Joshua M},
  doi          = {10.1093/biomet/asae006},
  journal      = {Biometrika},
  month        = {9},
  number       = {3},
  pages        = {1047-1061},
  shortjournal = {Biometrika},
  title        = {Regression analysis of group-tested current status data},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Familial inference: Tests for hypotheses on a family of
centres. <em>BIOMET</em>, <em>111</em>(3), 1029–1045. (<a
href="https://doi.org/10.1093/biomet/asad074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical hypotheses are translations of scientific hypotheses into statements about one or more distributions, often concerning their centre. Tests that assess statistical hypotheses of centre implicitly assume a specific centre, e.g., the mean or median. Yet, scientific hypotheses do not always specify a particular centre. This ambiguity leaves the possibility for a gap between scientific theory and statistical practice that can lead to rejection of a true null. In the face of replicability crises in many scientific disciplines, significant results of this kind are concerning. Rather than testing a single centre, this paper proposes testing a family of plausible centres, such as that induced by the Huber loss function. Each centre in the family generates a testing problem, and the resulting family of hypotheses constitutes a familial hypothesis. A Bayesian nonparametric procedure is devised to test familial hypotheses, enabled by a novel pathwise optimization routine to fit the Huber family. The favourable properties of the new test are demonstrated theoretically and experimentally. Two examples from psychology serve as real-world case studies.},
  archive      = {J_BIOMET},
  author       = {Thompson, Ryan and Forbes, Catherine S and MacEachern, Steven N and Peruggia, Mario},
  doi          = {10.1093/biomet/asad074},
  journal      = {Biometrika},
  month        = {9},
  number       = {3},
  pages        = {1029-1045},
  shortjournal = {Biometrika},
  title        = {Familial inference: Tests for hypotheses on a family of centres},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Projective independence tests in high dimensions: The curses
and the cures. <em>BIOMET</em>, <em>111</em>(3), 1013–1027. (<a
href="https://doi.org/10.1093/biomet/asad070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing independence between high-dimensional random vectors is fundamentally different from testing independence between univariate random variables. Taking the projection correlation as an example, it suffers from at least three problems. First, it has a high computational complexity of O { n 3 ( p + q ) } ⁠ , where n , p and q are the sample size and dimensions of the random vectors; this limits its usefulness substantially when n is extremely large. Second, the asymptotic null distribution of the projection correlation test is rarely tractable; therefore, random permutations are often suggested as a means of approximating the asymptotic null distribution, which further increases the complexity of implementing independence tests. Third, the power performance of the projection correlation test deteriorates in high dimensions. To address these issues, the projection correlation is improved by using a modified weight function, which reduces the complexity to O { n 2 ( p + q ) } ⁠ . We estimate the improved projection correlation with U -statistic theory. Importantly, its asymptotic null distribution is standard normal, thanks to the high dimesnionality of the random vectors. This expedites the implementation of independence tests substantially. To enhance the power performance in high dimensions, we propose incorporating a cross-validation procedure with feature screening into the projection correlation test. The implementation efficacy and power enhancement are confirmed through extensive numerical studies.},
  archive      = {J_BIOMET},
  author       = {Zhang, Yaowu and Zhu, Liping},
  doi          = {10.1093/biomet/asad070},
  journal      = {Biometrika},
  month        = {9},
  number       = {3},
  pages        = {1013-1027},
  shortjournal = {Biometrika},
  title        = {Projective independence tests in high dimensions: The curses and the cures},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On inference in high-dimensional logistic regression models
with separated data. <em>BIOMET</em>, <em>111</em>(3), 989–1011. (<a
href="https://doi.org/10.1093/biomet/asad065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Direct use of the likelihood function typically produces severely biased estimates when the dimension of the parameter vector is large relative to the effective sample size. With linearly separable data generated from a logistic regression model, the loglikelihood function asymptotes and the maximum likelihood estimator does not exist. We show that an exact analysis for each regression coefficient produces half-infinite confidence sets for some parameters when the data are separable. Such conclusions are not vacuous, but an honest portrayal of the limitations of the data. Finite confidence sets are only achievable when additional, perhaps implicit, assumptions are made. Under a notional double-asymptotic regime in which the dimension of the logistic coefficient vector increases with the sample size, the present paper considers the implications of enforcing a natural constraint on the vector of logistic transformed probabilities. We derive a relationship between the logistic coefficients and a notional parameter obtained as a probability limit of an ordinary least-squares estimator. The latter exists even when the data are separable. Consistency is ascertained under weak conditions on the design matrix.},
  archive      = {J_BIOMET},
  author       = {Lewis, R M and Battey, H S},
  doi          = {10.1093/biomet/asad065},
  journal      = {Biometrika},
  month        = {9},
  number       = {3},
  pages        = {989-1011},
  shortjournal = {Biometrika},
  title        = {On inference in high-dimensional logistic regression models with separated data},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Maximum likelihood estimation for semiparametric regression
models with interval-censored multistate data. <em>BIOMET</em>,
<em>111</em>(3), 971–988. (<a
href="https://doi.org/10.1093/biomet/asad073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval-censored multistate data arise in many studies of chronic diseases, where the health status of a subject can be characterized by a finite number of disease states and the transition between any two states is only known to occur over a broad time interval. We relate potentially time-dependent covariates to multistate processes through semiparametric proportional intensity models with random effects. We study nonparametric maximum likelihood estimation under general interval censoring and develop a stable expectation-maximization algorithm. We show that the resulting parameter estimators are consistent and that the finite-dimensional components are asymptotically normal with a covariance matrix that attains the semiparametric efficiency bound and can be consistently estimated through profile likelihood. In addition, we demonstrate through extensive simulation studies that the proposed numerical and inferential procedures perform well in realistic settings. Finally, we provide an application to a major epidemiologic cohort study.},
  archive      = {J_BIOMET},
  author       = {Gu, Yu and Zeng, Donglin and Heiss, Gerardo and Lin, D Y},
  doi          = {10.1093/biomet/asad073},
  journal      = {Biometrika},
  month        = {9},
  number       = {3},
  pages        = {971-988},
  shortjournal = {Biometrika},
  title        = {Maximum likelihood estimation for semiparametric regression models with interval-censored multistate data},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric priors with full-range borrowing of
information. <em>BIOMET</em>, <em>111</em>(3), 945–969. (<a
href="https://doi.org/10.1093/biomet/asad063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modelling of the dependence structure across heterogeneous data is crucial for Bayesian inference, since it directly impacts the borrowing of information. Despite extensive advances over the past two decades, most available methods only allow for nonnegative correlations. We derive a new class of dependent nonparametric priors that can induce correlations of any sign, thus introducing a new and more flexible idea of borrowing of information. This is achieved thanks to a novel concept, which we term hyper-tie, and represents a direct and simple measure of dependence. We investigate prior and posterior distributional properties of the model and develop algorithms to perform posterior inference. Illustrative examples on simulated and real data show that the proposed method outperforms alternatives in terms of prediction and clustering.},
  archive      = {J_BIOMET},
  author       = {Ascolani, F and Franzolini, B and Lijoi, A and Prünster, I},
  doi          = {10.1093/biomet/asad063},
  journal      = {Biometrika},
  month        = {9},
  number       = {3},
  pages        = {945-969},
  shortjournal = {Biometrika},
  title        = {Nonparametric priors with full-range borrowing of information},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing serial independence of object-valued time series.
<em>BIOMET</em>, <em>111</em>(3), 925–944. (<a
href="https://doi.org/10.1093/biomet/asad069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel method for testing serial independence of object-valued time series in metric spaces, which are more general than Euclidean or Hilbert spaces. The proposed method is fully nonparametric, free of tuning parameters and can capture all nonlinear pairwise dependence. The key concept used in this paper is the distance covariance in metric spaces, which is extended to the autodistance covariance for object-valued time series. Furthermore, we propose a generalized spectral density function to account for pairwise dependence at all lags and construct a Cramér–von Mises-type test statistic. New theoretical arguments are developed to establish the asymptotic behaviour of the test statistic. A wild bootstrap is also introduced to obtain the critical values of the nonpivotal limiting null distribution. Extensive numerical simulations and two real data applications on cumulative intraday returns and human mortality data are conducted to illustrate the effectiveness and versatility of our proposed test.},
  archive      = {J_BIOMET},
  author       = {Jiang, Feiyu and Gao, Hanjia and Shao, Xiaofeng},
  doi          = {10.1093/biomet/asad069},
  journal      = {Biometrika},
  month        = {9},
  number       = {3},
  pages        = {925-944},
  shortjournal = {Biometrika},
  title        = {Testing serial independence of object-valued time series},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Phylogenetic association analysis with conditional rank
correlation. <em>BIOMET</em>, <em>111</em>(3), 881–902. (<a
href="https://doi.org/10.1093/biomet/asad075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phylogenetic association analysis plays a crucial role in investigating the correlation between microbial compositions and specific outcomes of interest in microbiome studies. However, existing methods for testing such associations have limitations related to the assumption of a linear association in high-dimensional settings and the handling of confounding effects. Hence, there is a need for methods capable of characterizing complex associations, including nonmonotonic relationships. This article introduces a novel phylogenetic association analysis framework and associated tests to address these challenges by employing conditional rank correlation as a measure of association. The proposed tests account for confounders in a fully nonparametric manner, ensuring robustness against outliers and the ability to detect diverse dependencies. The proposed framework aggregates conditional rank correlations for subtrees using weighted sum and maximum approaches to capture both dense and sparse signals. The significance level of the test statistics is determined by calibration through a nearest-neighbour bootstrapping method, which is straightforward to implement and can accommodate additional datasets when these are available. The practical advantages of the proposed framework are demonstrated through numerical experiments using both simulated and real microbiome datasets.},
  archive      = {J_BIOMET},
  author       = {Wang, Shulei and Yuan, Bo and Tony Cai, T and Li, Hongzhe},
  doi          = {10.1093/biomet/asad075},
  journal      = {Biometrika},
  month        = {9},
  number       = {3},
  pages        = {881-902},
  shortjournal = {Biometrika},
  title        = {Phylogenetic association analysis with conditional rank correlation},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the optimality of score-driven models. <em>BIOMET</em>,
<em>111</em>(3), 865–880. (<a
href="https://doi.org/10.1093/biomet/asad067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Score-driven models have recently been introduced as a general framework to specify time-varying parameters of conditional densities. The score enjoys stochastic properties that make these models easy to implement and convenient to apply in several contexts, ranging from biostatistics to finance. Score-driven parameter updates have been shown to be optimal in terms of locally reducing a local version of the Kullback–Leibler divergence between the true conditional density and the postulated density of the model. A key limitation of such an optimality property is that it holds only locally both in the parameter space and sample space, yielding to a definition of local Kullback–Leibler divergence that is in fact not a divergence measure. The current paper shows that score-driven updates satisfy stronger optimality properties that are based on a global definition of Kullback–Leibler divergence. In particular, it is shown that score-driven updates reduce the distance between the expected updated parameter and the pseudo-true parameter. Furthermore, depending on the conditional density and the scaling of the score, the optimality result can hold globally over the parameter space, which can be viewed as a generalization of the monotonicity property of the stochastic gradient descent scheme. Several examples illustrate how the results derived in the paper apply to specific models under different easy-to-check assumptions, and provide a formal method to select the link function and the scaling of the score.},
  archive      = {J_BIOMET},
  author       = {Gorgi, P and Lauria, C S A and Luati, A},
  doi          = {10.1093/biomet/asad067},
  journal      = {Biometrika},
  month        = {9},
  number       = {3},
  pages        = {865-880},
  shortjournal = {Biometrika},
  title        = {On the optimality of score-driven models},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graphical tools for selecting conditional instrumental sets.
<em>BIOMET</em>, <em>111</em>(3), 771–788. (<a
href="https://doi.org/10.1093/biomet/asad066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the efficient estimation of total causal effects in the presence of unmeasured confounding using conditional instrumental sets. Specifically, we consider the two-stage least-squares estimator in the setting of a linear structural equation model with correlated errors that is compatible with a known acyclic directed mixed graph. To set the stage for our results, we characterize the class of linearly valid conditional instrumental sets that yield consistent two-stage least-squares estimators for the target total effect and derive a new asymptotic variance formula for these estimators. Equipped with these results, we provide three graphical tools for selecting more efficient linearly valid conditional instrumental sets: first, a graphical criterion that, for certain pairs of linearly valid conditional instrumental sets, identifies which of the two corresponding estimators has the smaller asymptotic variance second, an algorithm that greedily adds covariates that reduce the asymptotic variance to a given linearly valid conditional instrumental set and, third, a linearly valid conditional instrumental set for which the corresponding estimator has the smallest asymptotic variance that can be ensured with a graphical criterion.},
  archive      = {J_BIOMET},
  author       = {Henckel, L and Buttenschoen, M and Maathuis, M H},
  doi          = {10.1093/biomet/asad066},
  journal      = {Biometrika},
  month        = {9},
  number       = {3},
  pages        = {771-788},
  shortjournal = {Biometrika},
  title        = {Graphical tools for selecting conditional instrumental sets},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized kernel two-sample tests. <em>BIOMET</em>,
<em>111</em>(3), 755–770. (<a
href="https://doi.org/10.1093/biomet/asad068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel two-sample tests have been widely used for multivariate data to test equality of distributions. However, existing tests based on mapping distributions into a reproducing kernel Hilbert space mainly target specific alternatives and do not work well for some scenarios when the dimension of the data is moderate to high due to the curse of dimensionality. We propose a new test statistic that makes use of a common pattern under moderate and high dimensions and achieves substantial power improvements over existing kernel two-sample tests for a wide range of alternatives. We also propose alternative testing procedures that maintain high power with low computational cost, offering easy off-the-shelf tools for large datasets. The new approaches are compared to other state-of-the-art tests under various settings and show good performance. We showcase the new approaches through two applications: the comparison of musks and nonmusks using the shape of molecules, and the comparison of taxi trips starting from John F. Kennedy airport in consecutive months. All proposed methods are implemented in an R package kerTests .},
  archive      = {J_BIOMET},
  author       = {Song, Hoseung and Chen, Hao},
  doi          = {10.1093/biomet/asad068},
  journal      = {Biometrika},
  month        = {9},
  number       = {3},
  pages        = {755-770},
  shortjournal = {Biometrika},
  title        = {Generalized kernel two-sample tests},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Central limit theorems for local network statistics.
<em>BIOMET</em>, <em>111</em>(3), 743–754. (<a
href="https://doi.org/10.1093/biomet/asad080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subgraph counts, in particular the number of occurrences of small shapes such as triangles, characterize properties of random networks. As a result, they have seen wide use as network summary statistics. Subgraphs are typically counted globally, making existing approaches unable to describe vertex-specific characteristics. In contrast, rooted subgraphs focus on vertex neighbourhoods, and are fundamental descriptors of local network properties. We derive the asymptotic joint distribution of rooted subgraph counts in inhomogeneous random graphs, a model that generalizes most statistical network models. This result enables a shift in the statistical analysis of graphs, from estimating network summaries to estimating models linking local network structure and vertex-specific covariates. As an example, we consider a school friendship network and show that gender and race are significant predictors of local friendship patterns.},
  archive      = {J_BIOMET},
  author       = {Maugis, P A},
  doi          = {10.1093/biomet/asad080},
  journal      = {Biometrika},
  month        = {9},
  number       = {3},
  pages        = {743-754},
  shortjournal = {Biometrika},
  title        = {Central limit theorems for local network statistics},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Network community detection using higher-order structures.
<em>BIOMET</em>, <em>111</em>(3), 903–923. (<a
href="https://doi.org/10.1093/biomet/asae014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real-world networks, it is often observed that subgraphs or higher-order structures of certain configurations, e.g., triangles and by-fans, are overly abundant compared to standard randomly generated networks ( Milo et al., 2002 ). However, statistical models accounting for this phenomenon are limited, especially when community structure is of interest. This limitation is coupled with a lack of community detection methods that leverage subgraphs or higher-order structures. In this paper, we propose a new community detection method that effectively uses higher-order structures in a network. Furthermore, for the community detection accuracy, under an edge-dependent network model that consists of both community and triangle structures, we develop a finite-sample error bound characterized by the expected triangle degree, which leads to the consistency of the proposed method. To the best of our knowledge, this is the first statistical error bound and consistency result for community detection of a single network considering a network model with dependent edges. We also show, in both simulation studies and a real-world data example, that our method unveils network communities that are otherwise invisible to methods that ignore higher-order structures.},
  archive      = {J_BIOMET},
  author       = {Yu, X and Zhu, J},
  doi          = {10.1093/biomet/asae014},
  journal      = {Biometrika},
  month        = {3},
  number       = {3},
  pages        = {903-923},
  shortjournal = {Biometrika},
  title        = {Network community detection using higher-order structures},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Asymptotically constant risk estimator of the time-average
variance constant. <em>BIOMET</em>, <em>111</em>(3), 825–842. (<a
href="https://doi.org/10.1093/biomet/asae003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of the time-average variance constant is important for statistical analyses involving dependent data. This problem is difficult as it relies on a bandwidth parameter. Specifically, the optimal choices of the bandwidths of all existing estimators depend on the estimand itself and another unknown parameter that is very difficult to estimate. Thus, optimal variance estimation is unachievable. In this paper, we introduce a concept of converging flat-top kernels for constructing variance estimators whose optimal bandwidths are free of unknown parameters asymptotically and hence can be computed easily. We prove that the new estimator has an asymptotically constant risk and is locally asymptotically minimax.},
  archive      = {J_BIOMET},
  author       = {Chan, K W and Yau, C Y},
  doi          = {10.1093/biomet/asae003},
  journal      = {Biometrika},
  month        = {2},
  number       = {3},
  pages        = {825-842},
  shortjournal = {Biometrika},
  title        = {Asymptotically constant risk estimator of the time-average variance constant},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explicit solutions for the asymptotically optimal bandwidth
in cross-validation. <em>BIOMET</em>, <em>111</em>(3), 809–823. (<a
href="https://doi.org/10.1093/biomet/asae007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that least-squares cross-validation methods share a common structure that has an explicit asymptotic solution, when the chosen kernel is asymptotically separable in bandwidth and data. For density estimation with a multivariate Student-t ( ν ) kernel, the cross-validation criterion becomes asymptotically equivalent to a polynomial of only three terms. Our bandwidth formulae are simple and noniterative, thus leading to very fast computations, their integrated squared-error dominates traditional cross-validation implementations, they alleviate the notorious sample variability of cross-validation and overcome its breakdown in the case of repeated observations. We illustrate our method with univariate and bivariate applications, of density estimation and nonparametric regressions, to a large dataset of Michigan State University academic wages and experience.},
  archive      = {J_BIOMET},
  author       = {Abadir, Karim M and Lubrano, Michel},
  doi          = {10.1093/biomet/asae007},
  journal      = {Biometrika},
  month        = {2},
  number       = {3},
  pages        = {809-823},
  shortjournal = {Biometrika},
  title        = {Explicit solutions for the asymptotically optimal bandwidth in cross-validation},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Doubly robust estimation under covariate-induced dependent
left truncation. <em>BIOMET</em>, <em>111</em>(3), 789–808. (<a
href="https://doi.org/10.1093/biomet/asae005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In prevalent cohort studies with follow-up, the time-to-event outcome is subject to left truncation leading to selection bias. For estimation of the distribution of the time to event, conventional methods adjusting for left truncation tend to rely on the quasi-independence assumption that the truncation time and the event time are independent on the observed region. This assumption is violated when there is dependence between the truncation time and the event time, possibly induced by measured covariates. Inverse probability of truncation weighting can be used in this case, but it is sensitive to misspecification of the truncation model. In this work, we apply semiparametric theory to find the efficient influence curve of the expectation of an arbitrarily transformed survival time in the presence of covariate-induced dependent left truncation. We then use it to construct estimators that are shown to enjoy double-robustness properties. Our work represents the first attempt to construct doubly robust estimators in the presence of left truncation, which does not fall under the established framework of coarsened data where doubly robust approaches were developed. We provide technical conditions for the asymptotic properties that appear to not have been carefully examined in the literature for time-to-event data, and study the estimators via extensive simulation. We apply the estimators to two datasets from practice, with different right-censoring patterns.},
  archive      = {J_BIOMET},
  author       = {Wang, Yuyao and Ying, Andrew and Xu, Ronghui},
  doi          = {10.1093/biomet/asae005},
  journal      = {Biometrika},
  month        = {2},
  number       = {3},
  pages        = {789-808},
  shortjournal = {Biometrika},
  title        = {Doubly robust estimation under covariate-induced dependent left truncation},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Selective conformal inference with false coverage-statement
rate control. <em>BIOMET</em>, <em>111</em>(3), 727–742. (<a
href="https://doi.org/10.1093/biomet/asae010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conformal inference is a popular tool for constructing prediction intervals. We consider here the scenario of post-selection/selective conformal inference, that is, prediction intervals are reported only for individuals selected from unlabelled test data. To account for multiplicity, we develop a general split conformal framework to construct selective prediction intervals with the false coverage-statement rate control. We first investigate the false coverage rate–adjusted method of Benjamini &amp; Yekutieli (2005) in the present setting, and show that it is able to achieve false coverage-statement rate control, but yields uniformly inflated prediction intervals. We then propose a novel solution to the problem called selective conditional conformal prediction . Our method performs selection procedures on both the calibration set and test set, and then constructs conformal prediction intervals for the selected test candidates with the aid of the conditional empirical distribution obtained by the post-selection calibration set. When the selection rule is exchangeable, we show that our proposed method can exactly control the false coverage-statement rate in a model-free and distribution-free guarantee. For nonexchangeable selection procedures involving the calibration set, we provide non-asymptotic bounds for the false coverage-statement rate under mild distributional assumptions. Numerical results confirm the effectiveness and robustness of our method under false coverage-statement rate control and show that it achieves more narrowed prediction intervals over existing methods across various settings.},
  archive      = {J_BIOMET},
  author       = {Bao, Yajie and Huo, Yuyang and Ren, Haojie and Zou, Changliang},
  doi          = {10.1093/biomet/asae010},
  journal      = {Biometrika},
  month        = {2},
  number       = {3},
  pages        = {727-742},
  shortjournal = {Biometrika},
  title        = {Selective conformal inference with false coverage-statement rate control},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient nonparametric estimation of toeplitz covariance
matrices. <em>BIOMET</em>, <em>111</em>(3), 843–864. (<a
href="https://doi.org/10.1093/biomet/asae002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new efficient nonparametric estimator for Toeplitz covariance matrices is proposed. This estimator is based on a data transformation that translates the problem of Toeplitz covariance matrix estimation to the problem of mean estimation in an approximate Gaussian regression. The resulting Toeplitz covariance matrix estimator is positive definite by construction, fully data driven and computationally very fast. Moreover, this estimator is shown to be minimax optimal under the spectral norm for a large class of Toeplitz matrices. These results are readily extended to estimation of inverses of Toeplitz covariance matrices. Also, an alternative version of the Whittle likelihood for the spectral density based on the discrete cosine transform is proposed.},
  archive      = {J_BIOMET},
  author       = {Klockmann, K and Krivobokova, T},
  doi          = {10.1093/biomet/asae002},
  journal      = {Biometrika},
  month        = {1},
  number       = {3},
  pages        = {843-864},
  shortjournal = {Biometrika},
  title        = {Efficient nonparametric estimation of toeplitz covariance matrices},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Correction to: “Nonparametric efficient causal mediation
with intermediate confounders.” <em>BIOMET</em>, <em>111</em>(2),
723–726. (<a href="https://doi.org/10.1093/biomet/asae009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  doi          = {10.1093/biomet/asae009},
  journal      = {Biometrika},
  month        = {5},
  number       = {2},
  pages        = {723-726},
  shortjournal = {Biometrika},
  title        = {Correction to: ‘Nonparametric efficient causal mediation with intermediate confounders’},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kernel interpolation generalizes poorly. <em>BIOMET</em>,
<em>111</em>(2), 715–722. (<a
href="https://doi.org/10.1093/biomet/asad048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most interesting problems in the recent renaissance of the studies in kernel regression might be whether kernel interpolation can generalize well, since it may help us understand the ‘benign overfitting phenomenon’ reported in the literature on deep networks. In this paper, under mild conditions, we show that, for any ε &gt; 0 ⁠ , the generalization error of kernel interpolation is lower bounded by Ω ( n − ε ) ⁠ . In other words, the kernel interpolation generalizes poorly for a large class of kernels. As a direct corollary, we can show that overfitted wide neural networks defined on the sphere generalize poorly.},
  archive      = {J_BIOMET},
  author       = {Li, Yicheng and Zhang, Haobo and Lin, Qian},
  doi          = {10.1093/biomet/asad048},
  journal      = {Biometrika},
  month        = {5},
  number       = {2},
  pages        = {715-722},
  shortjournal = {Biometrika},
  title        = {Kernel interpolation generalizes poorly},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep kronecker network. <em>BIOMET</em>, <em>111</em>(2),
707–714. (<a href="https://doi.org/10.1093/biomet/asad049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a novel framework for the analysis of medical imaging data, including magnetic resonance imaging, functional magnetic resonance imaging, computed tomography and more. Medical imaging data differ from general images in two main aspects: (i) the sample size is often considerably smaller and (ii) the interpretation of the model is usually more crucial than predicting the outcome. As a result, standard methods such as convolutional neural networks cannot be directly applied to medical imaging analysis. Therefore, we propose the deep Kronecker network , which can adapt to the low sample size constraint and offer the desired model interpretation. Our approach is versatile, as it works for both matrix- and tensor-represented image data and can be applied to discrete and continuous outcomes. The deep Kronecker network is built upon a Kronecker product structure, which implicitly enforces a piecewise smooth property on coefficients. Moreover, our approach resembles a fully convolutional network as the Kronecker structure can be expressed in a convolutional form. Interestingly, our approach also has strong connections to the tensor regression framework proposed by Zhou et al. (2013) , which imposes a canonical low-rank structure on tensor coefficients. We conduct both classification and regression analyses using real magnetic resonance imaging data from the Alzheimer’s Disease Neuroimaging Initiative to demonstrate the effectiveness of our approach.},
  archive      = {J_BIOMET},
  author       = {Feng, Long and Yang, Guang},
  doi          = {10.1093/biomet/asad049},
  journal      = {Biometrika},
  month        = {5},
  number       = {2},
  pages        = {707-714},
  shortjournal = {Biometrika},
  title        = {Deep kronecker network},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Covariate-adjusted log-rank test: Guaranteed efficiency gain
and universal applicability. <em>BIOMET</em>, <em>111</em>(2), 691–705.
(<a href="https://doi.org/10.1093/biomet/asad045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonparametric covariate adjustment is considered for log-rank-type tests of the treatment effect with right-censored time-to-event data from clinical trials applying covariate-adaptive randomization. Our proposed covariate-adjusted log-rank test has a simple explicit formula and a guaranteed efficiency gain over the unadjusted test. We also show that our proposed test achieves universal applicability in the sense that the same formula of test can be universally applied to simple randomization and all commonly used covariate-adaptive randomization schemes such as the stratified permuted block and the Pocock–Simon minimization, which is not a property enjoyed by the unadjusted log-rank test. Our method is supported by novel asymptotic theory and empirical results for Type-I error and power of tests.},
  archive      = {J_BIOMET},
  author       = {Ye, Ting and Shao, Jun and Yi, Yanyao},
  doi          = {10.1093/biomet/asad045},
  journal      = {Biometrika},
  month        = {5},
  number       = {2},
  pages        = {691-705},
  shortjournal = {Biometrika},
  title        = {Covariate-adjusted log-rank test: Guaranteed efficiency gain and universal applicability},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An anomaly arising in the analysis of processes with more
than one source of variability. <em>BIOMET</em>, <em>111</em>(2),
677–689. (<a href="https://doi.org/10.1093/biomet/asad044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is frequently observed in practice that the Wald statistic gives a poor assessment of the statistical significance of a variance component. This paper provides detailed analytic insight into the phenomenon by way of two simple models, which point to an atypical geometry as the source of the aberration. The latter can in principle be checked numerically to cover situations of arbitrary complexity, such as those arising from elaborate forms of blocking in an experimental context, or models for longitudinal or clustered data. The salient point, echoing Dickey (2020) , is that a suitable likelihood-ratio test should always be used for the assessment of variance components.},
  archive      = {J_BIOMET},
  author       = {Battey, H S and McCullagh, Peter},
  doi          = {10.1093/biomet/asad044},
  journal      = {Biometrika},
  month        = {5},
  number       = {2},
  pages        = {677-689},
  shortjournal = {Biometrika},
  title        = {An anomaly arising in the analysis of processes with more than one source of variability},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An eigenvector-assisted estimation framework for
signal-plus-noise matrix models. <em>BIOMET</em>, <em>111</em>(2),
661–676. (<a href="https://doi.org/10.1093/biomet/asad058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop an eigenvector-assisted estimation framework for a collection of signal-plus-noise matrix models arising in high-dimensional statistics and many applications. The framework is built upon a novel asymptotically unbiased estimating equation using the leading eigenvectors of the data matrix. However, the estimator obtained by directly solving the estimating equation could be numerically unstable in practice and lacks robustness against model misspecification. We propose to use the quasi-posterior distribution by exponentiating a criterion function whose maximizer coincides with the estimating equation estimator. The proposed framework can incorporate heteroskedastic variance information, but does not require the complete specification of the sampling distribution and is also robust to the potential misspecification of the distribution of the noise matrix. Computationally, the quasi-posterior distribution can be obtained via a Markov chain Monte Carlo sampler, which exhibits superior numerical stability over some of the existing optimization-based estimators and is straightforward for uncertainty quantification. Under mild regularity conditions, we establish the large sample properties of the quasi-posterior distributions. In particular, the quasi-posterior credible sets have the correct frequentist nominal coverage probability provided that the criterion function is carefully selected. The validity and usefulness of the proposed framework are demonstrated through the analysis of synthetic datasets and the real-world ENZYMES network datasets.},
  archive      = {J_BIOMET},
  author       = {Xie, Fangzheng and Wu, Dingbo},
  doi          = {10.1093/biomet/asad058},
  journal      = {Biometrika},
  month        = {5},
  number       = {2},
  pages        = {661-676},
  shortjournal = {Biometrika},
  title        = {An eigenvector-assisted estimation framework for signal-plus-noise matrix models},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation of prediction error in time series.
<em>BIOMET</em>, <em>111</em>(2), 643–660. (<a
href="https://doi.org/10.1093/biomet/asad053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate estimation of prediction errors in time series is an important problem, which has immediate implications for the accuracy of prediction intervals as well as the quality of a number of widely used time series model selection criteria such as the Akaike information criterion. Except for simple cases, however, it is difficult or even impossible to obtain exact analytical expressions for one-step and multi-step predictions. This may be one of the reasons that, unlike in the independent case (see Efron, 2004 ), up to now there has been no fully established methodology for time series prediction error estimation. Starting from an approximation to the bias-variance decomposition of the squared prediction error, a method for accurate estimation of prediction errors in both univariate and multivariate stationary time series is developed in this article. In particular, several estimates are derived for a general class of predictors that includes most of the popular linear, nonlinear, parametric and nonparametric time series models used in practice, with causal invertible autoregressive moving average and nonparametric autoregressive processes discussed as lead examples. Simulations demonstrate that the proposed estimators perform quite well in finite samples. The estimates may also be used for model selection when the purpose of modelling is prediction.},
  archive      = {J_BIOMET},
  author       = {Aue, Alexander and Burman, Prabir},
  doi          = {10.1093/biomet/asad053},
  journal      = {Biometrika},
  month        = {5},
  number       = {2},
  pages        = {643-660},
  shortjournal = {Biometrika},
  title        = {Estimation of prediction error in time series},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A cross-validation-based statistical theory for point
processes. <em>BIOMET</em>, <em>111</em>(2), 625–641. (<a
href="https://doi.org/10.1093/biomet/asad041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the general ability of cross-validation to reduce overfitting and mean square error, we develop a cross-validation-based statistical theory for general point processes. It is based on the combination of two novel concepts for general point processes: cross-validation and prediction errors. Our cross-validation approach uses thinning to split a point process/pattern into pairs of training and validation sets, while our prediction errors measure discrepancy between two point processes. The new statistical approach, which may be used to model different distributional characteristics, exploits the prediction errors to measure how well a given model predicts validation sets using associated training sets. Having indicated that our new framework generalizes many existing statistical approaches, we then establish different theoretical properties for it, including large sample properties. We further recognize that nonparametric intensity estimation is an instance of Papangelou conditional intensity estimation, which we exploit to apply our new statistical theory to kernel intensity estimation. Using independent thinning-based cross-validation, we numerically show that the new approach substantially outperforms the state-of-the-art in bandwidth selection. Finally, we carry out intensity estimation for a dataset in forestry and a dataset in neurology.},
  archive      = {J_BIOMET},
  author       = {Cronie, Ottmar and Moradi, Mehdi and Biscio, Christophe A N},
  doi          = {10.1093/biomet/asad041},
  journal      = {Biometrika},
  month        = {5},
  number       = {2},
  pages        = {625-641},
  shortjournal = {Biometrika},
  title        = {A cross-validation-based statistical theory for point processes},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On varimax asymptotics in network models and spectral
methods for dimensionality reduction. <em>BIOMET</em>, <em>111</em>(2),
609–623. (<a href="https://doi.org/10.1093/biomet/asad061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Varimax factor rotations, while popular among practitioners in psychology and statistics since being introduced by Kaiser (1958) , have historically been viewed with skepticism and suspicion by some theoreticians and mathematical statisticians. Now, work by Rohe &amp; Zeng (2023) provides new, fundamental insight: varimax rotations provably perform statistical estimation in certain classes of latent variable models when paired with spectral-based matrix truncations for dimensionality reduction. We build on this new-found understanding of varimax rotations by developing further connections to network analysis and spectral methods rooted in entrywise matrix perturbation analysis. Concretely, this paper establishes the asymptotic multivariate normality of vectors in varimax-transformed Euclidean point clouds that represent low-dimensional node embeddings in certain latent space random graph models. We address related concepts including network sparsity, data denoising and the role of matrix rank in latent variable parameterizations. Collectively, these findings, at the confluence of classical and contemporary multivariate analysis, reinforce methodology and inference procedures grounded in matrix factorization-based techniques. Numerical examples illustrate our findings and supplement our discussion.},
  archive      = {J_BIOMET},
  author       = {Cape, J},
  doi          = {10.1093/biomet/asad061},
  journal      = {Biometrika},
  month        = {5},
  number       = {2},
  pages        = {609-623},
  shortjournal = {Biometrika},
  title        = {On varimax asymptotics in network models and spectral methods for dimensionality reduction},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Likelihood-based inference under nonconvex boundary
constraints. <em>BIOMET</em>, <em>111</em>(2), 591–607. (<a
href="https://doi.org/10.1093/biomet/asad062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Likelihood-based inference under nonconvex constraints on model parameters has become increasingly common in biomedical research. In this paper, we establish large-sample properties of the maximum likelihood estimator when the true parameter value lies at the boundary of a nonconvex parameter space. We further derive the asymptotic distribution of the likelihood ratio test statistic under nonconvex constraints on model parameters. A general Monte Carlo procedure for generating the limiting distribution is provided. The theoretical results are demonstrated by five examples in Anderson’s stereotype logistic regression model, genetic association studies, gene-environment interaction tests, cost-constrained linear regression and fairness-constrained linear regression.},
  archive      = {J_BIOMET},
  author       = {Wang, J Y and Ye, Z S and Chen, Y},
  doi          = {10.1093/biomet/asad062},
  journal      = {Biometrika},
  month        = {5},
  number       = {2},
  pages        = {591-607},
  shortjournal = {Biometrika},
  title        = {Likelihood-based inference under nonconvex boundary constraints},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Retrospective causal inference with multiple effect
variables. <em>BIOMET</em>, <em>111</em>(2), 573–589. (<a
href="https://doi.org/10.1093/biomet/asad056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As highlighted in Dawid (2000) and Pearl &amp; Mackenzie (2018) , deducing the causes of given effects is a more challenging problem than evaluating the effects of causes in causal inference. Lu et al. (2023) proposed an approach for deducing causes of a single effect variable based on posterior causal effects. In many applications, there are multiple effect variables, and they can be used simultaneously to more accurately deduce the causes. To retrospectively deduce causes from multiple effects, we propose multivariate posterior total, intervention and direct causal effects conditional on the observed evidence. We describe the assumptions of no confounding and monotonicity, under which we prove identifiability of the multivariate posterior causal effects and provide their identification equations. The proposed approach can be applied for causal attributions, medical diagnosis, blame and responsibility in various studies with multiple effect or outcome variables. Two examples are used to illustrate the proposed approach.},
  archive      = {J_BIOMET},
  author       = {Li, Wei and Lu, Zitong and Jia, Jinzhu and Xie, Min and Geng, Zhi},
  doi          = {10.1093/biomet/asad056},
  journal      = {Biometrika},
  month        = {5},
  number       = {2},
  pages        = {573-589},
  shortjournal = {Biometrika},
  title        = {Retrospective causal inference with multiple effect variables},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Order-based structure learning without score equivalence.
<em>BIOMET</em>, <em>111</em>(2), 551–572. (<a
href="https://doi.org/10.1093/biomet/asad052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an empirical Bayes formulation of the structure learning problem, where the prior specification assumes that all node variables have the same error variance, an assumption known to ensure the identifiability of the underlying causal directed acyclic graph. To facilitate efficient posterior computation, we approximate the posterior probability of each ordering by that of a best directed acyclic graph model, which naturally leads to an order-based Markov chain Monte Carlo algorithm. Strong selection consistency for our model in high-dimensional settings is proved under a condition that allows heterogeneous error variances, and the mixing behaviour of our sampler is theoretically investigated. Furthermore, we propose a new iterative top-down algorithm, which quickly yields an approximate solution to the structure learning problem and can be used to initialize the Markov chain Monte Carlo sampler. We demonstrate that our method outperforms other state-of-the-art algorithms under various simulation settings, and conclude the paper with a single-cell real-data study illustrating practical advantages of the proposed method.},
  archive      = {J_BIOMET},
  author       = {Chang, Hyunwoong and Cai, James J and Zhou, Quan},
  doi          = {10.1093/biomet/asad052},
  journal      = {Biometrika},
  month        = {5},
  number       = {2},
  pages        = {551-572},
  shortjournal = {Biometrika},
  title        = {Order-based structure learning without score equivalence},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Promises of parallel outcomes. <em>BIOMET</em>,
<em>111</em>(2), 537–550. (<a
href="https://doi.org/10.1093/biomet/asae008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key challenge in causal inference from observational studies is the identification and estimation of causal effects in the presence of unmeasured confounding. In this paper, we introduce a novel approach for causal inference that leverages information in multiple outcomes to deal with unmeasured confounding. An important assumption in our approach is conditional independence among multiple outcomes. In contrast to existing proposals in the literature, the roles of multiple outcomes in the conditional independence assumption are symmetric; hence, the name parallel outcomes. We show nonparametric identifiability with at least three parallel outcomes and provide parametric estimation tools under a set of linear structural equation models. Our proposal is evaluated through a set of synthetic and real data analyses.},
  archive      = {J_BIOMET},
  author       = {Zhou, Ying and Tang, Dingke and Kong, Dehan and Wang, Linbo},
  doi          = {10.1093/biomet/asae008},
  journal      = {Biometrika},
  month        = {5},
  number       = {2},
  pages        = {537-550},
  shortjournal = {Biometrika},
  title        = {Promises of parallel outcomes},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Selective machine learning of doubly robust functionals.
<em>BIOMET</em>, <em>111</em>(2), 517–535. (<a
href="https://doi.org/10.1093/biomet/asad055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While model selection is a well-studied topic in parametric and nonparametric regression or density estimation, selection of possibly high-dimensional nuisance parameters in semiparametric problems is far less developed. In this paper, we propose a selective machine learning framework for making inferences about a finite-dimensional functional defined on a semiparametric model, when the latter admits a doubly robust estimating function and several candidate machine learning algorithms are available for estimating the nuisance parameters. We introduce a new selection criterion aimed at bias reduction in estimating the functional of interest based on a novel definition of pseudo risk inspired by the double robustness property. Intuitively, the proposed criterion selects a pair of learners with the smallest pseudo risk, so that the estimated functional is least sensitive to perturbations of a nuisance parameter. We establish an oracle property for a multi-fold cross-validation version of the new selection criterion that states that our empirical criterion performs nearly as well as an oracle with a priori knowledge of the pseudo risk for each pair of candidate learners. Finally, we apply the approach to model selection of a semiparametric estimator of average treatment effect given an ensemble of candidate machine learners to account for confounding in an observational study that we illustrate in simulations and a data application.},
  archive      = {J_BIOMET},
  author       = {Cui, Y and Tchetgen Tchetgen, E J},
  doi          = {10.1093/biomet/asad055},
  journal      = {Biometrika},
  month        = {5},
  number       = {2},
  pages        = {517-535},
  shortjournal = {Biometrika},
  title        = {Selective machine learning of doubly robust functionals},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kernel methods for causal functions: Dose, heterogeneous and
incremental response curves. <em>BIOMET</em>, <em>111</em>(2), 497–516.
(<a href="https://doi.org/10.1093/biomet/asad042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose estimators based on kernel ridge regression for nonparametric causal functions such as dose, heterogeneous and incremental response curves. The treatment and covariates may be discrete or continuous in general spaces. Because of a decomposition property specific to the reproducing kernel Hilbert space, our estimators have simple closed-form solutions. We prove uniform consistency with finite sample rates via an original analysis of generalized kernel ridge regression. We extend our main results to counterfactual distributions and to causal functions identified by front and back door criteria. We achieve state-of-the-art performance in nonlinear simulations with many covariates, and conduct a policy evaluation of the US Job Corps training programme for disadvantaged youths.},
  archive      = {J_BIOMET},
  author       = {Singh, R and Xu, L and Gretton, A},
  doi          = {10.1093/biomet/asad042},
  journal      = {Biometrika},
  month        = {5},
  number       = {2},
  pages        = {497-516},
  shortjournal = {Biometrika},
  title        = {Kernel methods for causal functions: Dose, heterogeneous and incremental response curves},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Τ-censored weighted benjamini–hochberg procedures under
independence. <em>BIOMET</em>, <em>111</em>(2), 479–496. (<a
href="https://doi.org/10.1093/biomet/asad047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of multiple hypothesis testing, auxiliary information can be leveraged to enhance the efficiency of test procedures. A common way to make use of auxiliary information is by weighting p -values. However, when the weights are learned from data, controlling the finite-sample false discovery rate becomes challenging, and most existing weighted procedures only guarantee false discovery rate control in an asymptotic limit. In a recent study conducted by Ignatiadis &amp; Huber (2021) , a novel τ -censored weighted Benjamini–Hochberg procedure was proposed to control the finite-sample false discovery rate. The authors employed the cross-weighting approach to learn weights for the p -values. This approach randomly splits the data into several folds and constructs a weight for each p -value P i using the p -values outside the fold containing P i . Cross-weighting does not exploit the p -value information inside the fold and only balances the weights within each fold, which may result in a loss of power. In this article, we introduce two methods for constructing data-driven weights for τ -censored weighted Benjamini–Hochberg procedures under independence. They provide new insight into masking p -values to prevent overfitting in multiple testing. The first method utilizes a leave-one-out technique, where all but one of the p -values are used to learn a weight for each p -value. This technique masks the information of a p -value in its weight by calculating the infimum of the weight with respect to the p -value. The second method uses partial information from each p -value to construct weights and utilizes the conditional distributions of the null p -values to establish false discovery rate control. Additionally, we propose two methods for estimating the null proportion and demonstrate how to integrate null-proportion adaptivity into the proposed weights to improve power.},
  archive      = {J_BIOMET},
  author       = {Zhao, Haibing and Zhou, Huijuan},
  doi          = {10.1093/biomet/asad047},
  journal      = {Biometrika},
  month        = {5},
  number       = {2},
  pages        = {479-496},
  shortjournal = {Biometrika},
  title        = {τ-censored weighted Benjamini–Hochberg procedures under independence},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conformalized survival analysis with adaptive cut-offs.
<em>BIOMET</em>, <em>111</em>(2), 459–477. (<a
href="https://doi.org/10.1093/biomet/asad076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces an assumption-lean method that constructs valid and efficient lower predictive bounds for survival times with censored data. We build on recent work by Candès et al. (2023) , whose approach first subsets the data to discard any data points with early censoring times and then uses a reweighting technique, namely, weighted conformal inference ( Tibshirani et al., 2019 ), to correct for the distribution shift introduced by this subsetting procedure. For our new method, instead of constraining to a fixed threshold for the censoring time when subsetting the data, we allow for a covariate-dependent and data-adaptive subsetting step, which is better able to capture the heterogeneity of the censoring mechanism. As a result, our method can lead to lower predictive bounds that are less conservative and give more accurate information. We show that in the Type-I right-censoring setting, if either the censoring mechanism or the conditional quantile of the survival time is well estimated, our proposed procedure achieves nearly exact marginal coverage, where in the latter case we additionally have approximate conditional coverage. We evaluate the validity and efficiency of our proposed algorithm in numerical experiments, illustrating its advantage when compared with other competing methods. Finally, our method is applied to a real dataset to generate lower predictive bounds for users’ active times on a mobile app.},
  archive      = {J_BIOMET},
  author       = {Gui, Yu and Hore, Rohan and Ren, Zhimei and Barber, Rina Foygel},
  doi          = {10.1093/biomet/asad076},
  journal      = {Biometrika},
  month        = {5},
  number       = {2},
  pages        = {459-477},
  shortjournal = {Biometrika},
  title        = {Conformalized survival analysis with adaptive cut-offs},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). More efficient exact group invariance testing: Using a
representative subgroup. <em>BIOMET</em>, <em>111</em>(2), 441–458. (<a
href="https://doi.org/10.1093/biomet/asad050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider testing invariance of a distribution under an algebraic group of transformations, such as permutations or sign flips. As such groups are typically huge, tests based on the full group are often computationally infeasible. Hence, it is standard practice to use a random subset of transformations. We improve upon this by replacing the random subset with a strategically chosen, fixed subgroup of transformations. In a generalized location model, we show that the resulting tests are often consistent for lower signal-to-noise ratios. Moreover, we establish an analogy between the power improvement and switching from a t -test to a Z -test under normality. Importantly, in permutation-based multiple testing, the efficiency gain with our approach can be huge, since we attain the same power with many fewer permutations.},
  archive      = {J_BIOMET},
  author       = {Koning, N W and Hemerik, J},
  doi          = {10.1093/biomet/asad050},
  journal      = {Biometrika},
  month        = {5},
  number       = {2},
  pages        = {441-458},
  shortjournal = {Biometrika},
  title        = {More efficient exact group invariance testing: Using a representative subgroup},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). E-values as unnormalized weights in multiple testing.
<em>BIOMET</em>, <em>111</em>(2), 417–439. (<a
href="https://doi.org/10.1093/biomet/asad057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study how to combine p -values and e -values, and design multiple testing procedures where both p -values and e -values are available for every hypothesis. Our results provide a new perspective on multiple testing with data-driven weights: while standard weighted multiple testing methods require the weights to deterministically add up to the number of hypotheses being tested, we show that this normalization is not required when the weights are e -values that are independent of the p -values. Such e -values can be obtained in meta-analysis where a primary dataset is used to compute p -values, and an independent secondary dataset is used to compute e -values. Going beyond meta-analysis, we showcase settings wherein independent e -values and p -values can be constructed on a single dataset itself. Our procedures can result in a substantial increase in power, especially if the nonnull hypotheses have e -values much larger than one.},
  archive      = {J_BIOMET},
  author       = {Ignatiadis, Nikolaos and Wang, Ruodu and Ramdas, Aaditya},
  doi          = {10.1093/biomet/asad057},
  journal      = {Biometrika},
  month        = {5},
  number       = {2},
  pages        = {417-439},
  shortjournal = {Biometrika},
  title        = {E-values as unnormalized weights in multiple testing},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On selection and conditioning in multiple testing and
selective inference. <em>BIOMET</em>, <em>111</em>(2), 393–416. (<a
href="https://doi.org/10.1093/biomet/asad078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate a class of methods for selective inference that condition on a selection event. Such methods follow a two-stage process. First, a data-driven collection of hypotheses is chosen from some large universe of hypotheses. Subsequently, inference takes place within this data-driven collection, conditioned on the information that was used for the selection. Examples of such methods include basic data splitting as well as modern data-carving methods and post-selection inference methods for lasso coefficients based on the polyhedral lemma. In this article, we take a holistic view of such methods, considering the selection, conditioning and final error control steps together as a single method. From this perspective, we demonstrate that multiple testing methods defined directly on the full universe of hypotheses are always at least as powerful as selective inference methods based on selection and conditioning. This result holds true even when the universe is potentially infinite and only implicitly defined, such as in the case of data splitting. We provide general theory and intuition before investigating in detail several case studies where a shift to a nonselective or unconditional perspective can yield a power gain.},
  archive      = {J_BIOMET},
  author       = {Goeman, Jelle J and Solari, Aldo},
  doi          = {10.1093/biomet/asad078},
  journal      = {Biometrika},
  month        = {5},
  number       = {2},
  pages        = {393-416},
  shortjournal = {Biometrika},
  title        = {On selection and conditioning in multiple testing and selective inference},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The state of cumulative sum sequential changepoint testing
70 years after page. <em>BIOMET</em>, <em>111</em>(2), 367–391. (<a
href="https://doi.org/10.1093/biomet/asad079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quality control charts aim at raising an alarm as soon as sequentially obtained observations of an underlying random process no longer seem to be within stochastic fluctuations prescribed by an in-control scenario. Such random processes can often be modelled using the concept of stationarity, or even independence as in most classical works. An important out-of-control scenario is the changepoint alternative, for which the distribution of the process changes at an unknown point in time. In his seminal 1954 Biometrika paper, E. S. Page introduced the famous cumulative sum control charts for changepoint monitoring. Innovatively, decision rules based on cumulative sum procedures took the full history of the process into account, whereas previous procedures were based only on a fixed and typically small number of the most recent observations. The extreme case of using only the most recent observation, often referred to as the Shewhart chart, is more akin to serial outlier than changepoint detection. Page’s cumulative sum approach, introduced seven decades ago, is ubiquitous in modern changepoint analysis, and his original paper has led to a multitude of follow-up papers in different research communities. This review is focused on a particular subfield of this research, namely nonparametric sequential, or online, changepoint tests that are constructed to maintain a desired Type-1 error as opposed to the more traditional approach seeking to minimize the average run length of the procedures. Such tests have originated at the intersection of econometrics and statistics. We trace the development of these tests and highlight their properties, mostly using a simple location model for clarity of exposition, but we also review more complex situations such as regression and time series models.},
  archive      = {J_BIOMET},
  author       = {Aue, Alexander and Kirch, Claudia},
  doi          = {10.1093/biomet/asad079},
  journal      = {Biometrika},
  month        = {5},
  number       = {2},
  pages        = {367-391},
  shortjournal = {Biometrika},
  title        = {The state of cumulative sum sequential changepoint testing 70 years after page},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction to: “A cross-validation-based statistical theory
for point processes.” <em>BIOMET</em>, <em>111</em>(1), 365. (<a
href="https://doi.org/10.1093/biomet/asad077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  doi          = {10.1093/biomet/asad077},
  journal      = {Biometrika},
  month        = {2},
  number       = {1},
  pages        = {365},
  shortjournal = {Biometrika},
  title        = {Correction to: ‘A cross-validation-based statistical theory for point processes’},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Power and sample size calculations for rerandomization.
<em>BIOMET</em>, <em>111</em>(1), 355–363. (<a
href="https://doi.org/10.1093/biomet/asad027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power analyses are an important aspect of experimental design, because they help determine how experiments are implemented in practice. It is common to specify a desired level of power and compute the sample size necessary to obtain that power. Such calculations are well known for completely randomized experiments, but there can be many benefits to using other experimental designs. For example, it has recently been established that rerandomization, where subjects are randomized until covariate balance is obtained, increases the precision of causal effect estimators. This work establishes the power of rerandomized treatment-control experiments, thereby allowing for sample size calculators. We find the surprising result that, while power is often greater under rerandomization than complete randomization, the opposite can occur for very small treatment effects. The reason is that inference under rerandomization can be relatively more conservative, in the sense that it can have a lower Type-I error at the same nominal significance level, and this additional conservativeness adversely affects power. This surprising result is due to treatment effect heterogeneity, a quantity often ignored in power analyses. We find that heterogeneity increases power for large effect sizes, but decreases power for small effect sizes.},
  archive      = {J_BIOMET},
  author       = {Branson, Zach and Li, Xinran and Ding, Peng},
  doi          = {10.1093/biomet/asad027},
  journal      = {Biometrika},
  month        = {2},
  number       = {1},
  pages        = {355-363},
  shortjournal = {Biometrika},
  title        = {Power and sample size calculations for rerandomization},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable subsampling: Computation, aggregation and
inference. <em>BIOMET</em>, <em>111</em>(1), 347–354. (<a
href="https://doi.org/10.1093/biomet/asad021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subsampling has seen a resurgence in the big data era where the standard, full-resample size bootstrap can be infeasible to compute. Nevertheless, even choosing a single random subsample of size b can be computationally challenging with both b and the sample size n being very large. This paper shows how a set of appropriately chosen, nonrandom subsamples can be used to conduct effective, and computationally feasible, subsampling distribution estimation. Furthermore, the same set of subsamples can be used to yield a procedure for subsampling aggregation, also known as subagging, that is scalable with big data. Interestingly, the scalable subagging estimator can be tuned to have the same, or better, rate of convergence than that of θ ^ n ⁠ . Statistical inference could then be based on the scalable subagging estimator instead of the original θ ^ n ⁠ .},
  archive      = {J_BIOMET},
  author       = {Politis, Dimitris N},
  doi          = {10.1093/biomet/asad021},
  journal      = {Biometrika},
  month        = {2},
  number       = {1},
  pages        = {347-354},
  shortjournal = {Biometrika},
  title        = {Scalable subsampling: Computation, aggregation and inference},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Characterizing m-estimators. <em>BIOMET</em>,
<em>111</em>(1), 339–346. (<a
href="https://doi.org/10.1093/biomet/asad026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We characterize the full classes of M -estimators for semiparametric models of general functionals by formally connecting the theory of consistent loss functions from forecast evaluation with the theory of M -estimation. This novel characterization result allows us to leverage existing results on loss functions known from the literature on forecast evaluation in estimation theory. We exemplify advantageous implications for the fields of robust, efficient, equivariant and Pareto-optimal M -estimation.},
  archive      = {J_BIOMET},
  author       = {Dimitriadis, Timo and Fissler, Tobias and Ziegel, Johanna},
  doi          = {10.1093/biomet/asad026},
  journal      = {Biometrika},
  month        = {2},
  number       = {1},
  pages        = {339-346},
  shortjournal = {Biometrika},
  title        = {Characterizing M-estimators},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). No-harm calibration for generalized oaxaca–blinder
estimators. <em>BIOMET</em>, <em>111</em>(1), 331–338. (<a
href="https://doi.org/10.1093/biomet/asad036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In randomized experiments, adjusting for observed features when estimating treatment effects has been proposed as a way to improve asymptotic efficiency. However, among parametric methods, only linear regression has been proven to form an estimate of the average treatment effect that is asymptotically no less efficient than the treated-minus-control difference in means regardless of the true data generating process. Randomized treatment assignment provides this do-no-harm property, with neither truth of a linear model nor a generative model for the outcomes being required. We present a general calibration method that confers the same no-harm property onto estimators leveraging a broad class of nonlinear models. This recovers the usual regression-adjusted estimator when ordinary least squares is used, and further provides noninferior treatment effect estimators using methods such as logistic and Poisson regression. The resulting estimators are noninferior to both the difference-in-means estimator and to treatment effect estimators that have not undergone calibration. We show that our estimator is asymptotically equivalent to an inverse-probability-weighted estimator using a logit link with predicted potential outcomes as covariates. In a simulation study, we demonstrate that common nonlinear estimators without our calibration procedure may perform markedly worse than both the calibrated estimator and the unadjusted difference in means.},
  archive      = {J_BIOMET},
  author       = {Cohen, P L and Fogarty, C B},
  doi          = {10.1093/biomet/asad036},
  journal      = {Biometrika},
  month        = {2},
  number       = {1},
  pages        = {331-338},
  shortjournal = {Biometrika},
  title        = {No-harm calibration for generalized Oaxaca–Blinder estimators},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust sample weighting to facilitate individualized
treatment rule learning for a target population. <em>BIOMET</em>,
<em>111</em>(1), 309–329. (<a
href="https://doi.org/10.1093/biomet/asad038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning individualized treatment rules is an important topic in precision medicine. Current literature mainly focuses on deriving individualized treatment rules from a single source population. We consider the observational data setting when the source population differs from a target population of interest. Compared with causal generalization for the average treatment effect that is a scalar quantity, individualized treatment rule generalization poses new challenges due to the need to model and generalize the rules based on a prespecified class of functions that may not contain the unrestricted true optimal individualized treatment rule. The aim of this paper is to develop a weighting framework to mitigate the impact of such misspecification, and thus facilitate the generalizability of optimal individualized treatment rules from a source population to a target population. Our method seeks covariate balance over a nonparametric function class characterized by a reproducing kernel Hilbert space and can improve many individualized treatment rule learning methods that rely on weights. We show that the proposed method encompasses importance weights and overlap weights as two extreme cases, allowing for a better bias-variance trade-off in between. Numerical examples demonstrate that the use of our weighting method can greatly improve individualized treatment rule estimation for the target population compared with other weighting methods.},
  archive      = {J_BIOMET},
  author       = {Chen, Rui and Huling, Jared D and Chen, Guanhua and Yu, Menggang},
  doi          = {10.1093/biomet/asad038},
  journal      = {Biometrika},
  month        = {2},
  number       = {1},
  pages        = {309-329},
  shortjournal = {Biometrika},
  title        = {Robust sample weighting to facilitate individualized treatment rule learning for a target population},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interpolating discriminant functions in high-dimensional
gaussian latent mixtures. <em>BIOMET</em>, <em>111</em>(1), 291–308. (<a
href="https://doi.org/10.1093/biomet/asad037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers binary classification of high-dimensional features under a postulated model with a low-dimensional latent Gaussian mixture structure and nonvanishing noise. A generalized least-squares estimator is used to estimate the direction of the optimal separating hyperplane. The estimated hyperplane is shown to interpolate on the training data. While the direction vector can be consistently estimated, as could be expected from recent results in linear regression, a naive plug-in estimate fails to consistently estimate the intercept. A simple correction, which requires an independent hold-out sample, renders the procedure minimax optimal in many scenarios. The interpolation property of the latter procedure can be retained, but surprisingly depends on the way the labels are encoded.},
  archive      = {J_BIOMET},
  author       = {Bing, Xin and Wegkamp, Marten},
  doi          = {10.1093/biomet/asad037},
  journal      = {Biometrika},
  month        = {2},
  number       = {1},
  pages        = {291-308},
  shortjournal = {Biometrika},
  title        = {Interpolating discriminant functions in high-dimensional gaussian latent mixtures},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On geometric convergence for the metropolis-adjusted
langevin algorithm under simple conditions. <em>BIOMET</em>,
<em>111</em>(1), 273–289. (<a
href="https://doi.org/10.1093/biomet/asad060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the Metropolis-adjusted Langevin algorithm is a popular and widely used Markov chain Monte Carlo method, very few papers derive conditions that ensure its convergence. In particular, to the authors’ knowledge, assumptions that are both easy to verify and guarantee geometric convergence, are still missing. In this work, we establish V -uniformly geometric convergence for the Metropolis-adjusted Langevin algorithm under mild assumptions about the target distribution. Unlike previous work, we only consider tail and smoothness conditions for the potential associated with the target distribution. These conditions are quite common in the Markov chain Monte Carlo literature. Finally, we pay special attention to the dependence of the bounds we derive on the step size of the Euler–Maruyama discretization, which corresponds to the proposed Markov kernel of the Metropolis-adjusted Langevin algorithm.},
  archive      = {J_BIOMET},
  author       = {Oliviero-Durmus, Alain and Moulines, Éric},
  doi          = {10.1093/biomet/asad060},
  journal      = {Biometrika},
  month        = {2},
  number       = {1},
  pages        = {273-289},
  shortjournal = {Biometrika},
  title        = {On geometric convergence for the metropolis-adjusted langevin algorithm under simple conditions},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A mark-specific quantile regression model. <em>BIOMET</em>,
<em>111</em>(1), 255–272. (<a
href="https://doi.org/10.1093/biomet/asad039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantile regression has become a widely used tool for analysing competing risk data. However, quantile regression for competing risk data with a continuous mark is still scarce. The mark variable is an extension of cause of failure in a classical competing risk model where cause of failure is replaced by a continuous mark only observed at uncensored failure times. An example of the continuous mark variable is the genetic distance that measures dissimilarity between the infecting virus and the virus contained in the vaccine construct. In this article, we propose a novel mark-specific quantile regression model. The proposed estimation method borrows strength from data in a neighbourhood of a mark and is based on an induced smoothed estimation equation, which is very different from the existing methods for competing risk data with discrete causes. The asymptotic properties of the resulting estimators are established across mark and quantile continuums. In addition, a mark-specific quantile-type vaccine efficacy is proposed and its statistical inference procedures are developed. Simulation studies are conducted to evaluate the finite sample performances of the proposed estimation and hypothesis testing procedures. An application to the first HIV vaccine efficacy trial is provided.},
  archive      = {J_BIOMET},
  author       = {Qu, Lianqiang and Sun, Liuquan and Sun, Yanqing},
  doi          = {10.1093/biomet/asad039},
  journal      = {Biometrika},
  month        = {2},
  number       = {1},
  pages        = {255-272},
  shortjournal = {Biometrika},
  title        = {A mark-specific quantile regression model},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Treatment effect quantiles in stratified randomized
experiments and matched observational studies. <em>BIOMET</em>,
<em>111</em>(1), 235–254. (<a
href="https://doi.org/10.1093/biomet/asad030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating the treatment effect has become an important topic for many applications. However, most existing literature focuses mainly on average treatment effects. When the individual effects are heavy tailed or have outlier values, not only may the average effect not be appropriate for summarizing treatment effects, but also the conventional inference for it can be sensitive and possibly invalid due to poor large-sample approximations. In this paper we focus on quantiles of individual treatment effects, which can be more robust in the presence of extreme individual effects. Moreover, our inference for them is purely randomization based, avoiding any distributional assumptions on the units. We first consider inference in stratified randomized experiments, extending the recent work by Caughey et al. (2021) . We show that the computation of valid p -values for testing null hypotheses on quantiles of individual effects can be transformed into instances of the multiple-choice knapsack problem, which can be efficiently solved exactly or slightly conservatively. We then extend our approach to matched observational studies and propose a sensitivity analysis to investigate to what extent our inference on quantiles of individual effects is robust to unmeasured confounding. The proposed randomization inference and sensitivity analysis are simul- taneously valid for all quantiles of individual effects, noting that the analysis for the maximum or minimum individual effect coincides with the conventional analysis assuming constant treatment effects.},
  archive      = {J_BIOMET},
  author       = {Su, Yongchang and Li, Xinran},
  doi          = {10.1093/biomet/asad030},
  journal      = {Biometrika},
  month        = {2},
  number       = {1},
  pages        = {235-254},
  shortjournal = {Biometrika},
  title        = {Treatment effect quantiles in stratified randomized experiments and matched observational studies},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tailored inference for finite populations: Conditional
validity and transfer across distributions. <em>BIOMET</em>,
<em>111</em>(1), 215–233. (<a
href="https://doi.org/10.1093/biomet/asad022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameters of subpopulations can be more relevant than those of superpopulations. For example, a healthcare provider may be interested in the effect of a treatment plan for a specific subset of their patients; policymakers may be concerned with the impact of a policy in a particular state within a given population. In these cases, the focus is on a specific finite population, as opposed to an infinite superpopulation. Such a population can be characterized by fixing some attributes that are intrinsic to them, leaving unexplained variations like measurement error as random. Inference for a population with fixed attributes can then be modelled as inferring parameters of a conditional distribution. Accordingly, it is desirable that confidence intervals are conditionally valid for the realized population, instead of marginalizing over many possible draws of populations. We provide a statistical inference framework for parameters of finite populations with known attributes. Leveraging the attribute information, our estimators and confidence intervals closely target a specific finite population. When the data are from the population of interest, our confidence intervals attain asymptotic conditional validity, given the attributes, and are shorter than those for superpopulation inference. In addition, we develop procedures to infer parameters of new populations with differing covariate distributions; the confidence intervals are also conditionally valid for the new populations under mild conditions. Our methods extend to situations where the fixed information has a weaker structure or is only partially observed. We demonstrate the validity and applicability of our methods using simulated data and a real-word dataset for predicting car prices.},
  archive      = {J_BIOMET},
  author       = {Jin, Ying and Rothenhäusler, Dominik},
  doi          = {10.1093/biomet/asad022},
  journal      = {Biometrika},
  month        = {2},
  number       = {1},
  pages        = {215-233},
  shortjournal = {Biometrika},
  title        = {Tailored inference for finite populations: Conditional validity and transfer across distributions},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian learning of network structures from interventional
experimental data. <em>BIOMET</em>, <em>111</em>(1), 195–214. (<a
href="https://doi.org/10.1093/biomet/asad032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directed acyclic graphs provide an effective framework for learning causal relationships among variables given multivariate observations. Under pure observational data, directed acyclic graphs encoding the same conditional independencies cannot be distinguished and are collected into Markov equivalence classes. In many contexts, however, observational measurements are supplemented by interventional data that improve directed acyclic graph identifiability and enhance causal effect estimation. We propose a Bayesian framework for multivariate data partially generated after stochastic interventions. To this end, we introduce an effective prior elicitation procedure leading to a closed-form expression for the directed acyclic graph marginal likelihood and guaranteeing score equivalence among directed acyclic graphs that are Markov equivalent post intervention. Under the Gaussian setting, we show, in terms of posterior ratio consistency, that the true network will be asymptotically recovered, regardless of the specific distribution of the intervened variables and of the relative asymptotic dominance between observational and interventional measurements. We validate our theoretical results via simulation and we implement a Markov chain Monte Carlo sampler for posterior inference on the space of directed acyclic graphs on both synthetic and biological protein expression data.},
  archive      = {J_BIOMET},
  author       = {Castelletti, F and Peluso, S},
  doi          = {10.1093/biomet/asad032},
  journal      = {Biometrika},
  month        = {2},
  number       = {1},
  pages        = {195-214},
  shortjournal = {Biometrika},
  title        = {Bayesian learning of network structures from interventional experimental data},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical summaries of unlabelled evolutionary trees.
<em>BIOMET</em>, <em>111</em>(1), 171–193. (<a
href="https://doi.org/10.1093/biomet/asad025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rooted and ranked phylogenetic trees are mathematical objects that are useful in modelling hierarchical data and evolutionary relationships with applications to many fields such as evolutionary biology and genetic epidemiology. Bayesian phylogenetic inference usually explores the posterior distribution of trees via Markov chain Monte Carlo methods. However, assessing uncertainty and summarizing distributions remains challenging for these types of structures. While labelled phylogenetic trees have been extensively studied, relatively less literature exists for unlabelled trees that are increasingly useful, for example when one seeks to summarize samples of trees obtained with different methods, or from different samples and environments, and wishes to assess the stability and generalizability of these summaries. In our paper, we exploit recently proposed distance metrics of unlabelled ranked binary trees and unlabelled ranked genealogies, or trees equipped with branch lengths, to define the Fréchet mean, variance and interquartile sets as summaries of these tree distributions. We provide an efficient combinatorial optimization algorithm for computing the Fréchet mean of a sample or of distributions on unlabelled ranked tree shapes and unlabelled ranked genealogies. We show the applicability of our summary statistics for studying popular tree distributions and for comparing the SARS-CoV-2 evolutionary trees across different locations during the COVID-19 epidemic in 2020. Our current implementations are publicly available at https://github.com/RSamyak/fmatrix .},
  archive      = {J_BIOMET},
  author       = {Samyak, Rajanala and Palacios, Julia A},
  doi          = {10.1093/biomet/asad025},
  journal      = {Biometrika},
  month        = {2},
  number       = {1},
  pages        = {171-193},
  shortjournal = {Biometrika},
  title        = {Statistical summaries of unlabelled evolutionary trees},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Populations of unlabelled networks: Graph space geometry and
generalized geodesic principal components. <em>BIOMET</em>,
<em>111</em>(1), 147–170. (<a
href="https://doi.org/10.1093/biomet/asad024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical analysis for populations of networks is widely applicable, but challenging, as networks have strongly non-Euclidean behaviour. Graph space is an exhaustive framework for studying populations of unlabelled networks that are weighted or unweighted, uni- or multilayered, directed or undirected. Viewing graph space as the quotient of a Euclidean space with respect to a finite group action, we show that it is not a manifold, and that its curvature is unbounded from above. Within this geometrical framework we define generalized geodesic principal components, and we introduce the align-all-and-compute algorithms, all of which allow for the computation of statistics on graph space. The statistics and algorithms are compared with existing methods and empirically validated on three real datasets, showcasing the potential utility of the framework. The whole framework is implemented within the geomstats Python package.},
  archive      = {J_BIOMET},
  author       = {Calissano, Anna and Feragen, Aasa and Vantini, Simone},
  doi          = {10.1093/biomet/asad024},
  journal      = {Biometrika},
  month        = {2},
  number       = {1},
  pages        = {147-170},
  shortjournal = {Biometrika},
  title        = {Populations of unlabelled networks: Graph space geometry and generalized geodesic principal components},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). One-step targeted maximum likelihood estimation for
targeting cause-specific absolute risks and survival curves.
<em>BIOMET</em>, <em>111</em>(1), 129–145. (<a
href="https://doi.org/10.1093/biomet/asad033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the one-step targeted maximum likelihood estimation methodology for multi-dimensional causal parameters in general survival and competing risk settings where event times take place on the positive real line and are subject to right censoring. We focus on effects of baseline treatment decisions possibly confounded by pretreatment covariates, but remark that our work generalizes to settings with time-varying treatment regimes and time-dependent confounding. We point out two overall contributions of our work. First, our methods can be used to obtain simultaneous inference for treatment effects on multiple absolute risks in competing risk settings. Second, our methods can be used to achieve inference for the full survival curve, or a full absolute risk curve, across time. The one-step targeted maximum likelihood procedure is based on a one-dimensional universal least favourable submodel for each cause-specific hazard that we implement in recursive steps along a corresponding nonuniversal multivariate least favourable submodel. Our empirical study demonstrates the practical use of the methods.},
  archive      = {J_BIOMET},
  author       = {Rytgaard, H C W and van der Laan, M J},
  doi          = {10.1093/biomet/asad033},
  journal      = {Biometrika},
  month        = {2},
  number       = {1},
  pages        = {129-145},
  shortjournal = {Biometrika},
  title        = {One-step targeted maximum likelihood estimation for targeting cause-specific absolute risks and survival curves},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid confidence intervals for informative uniform
asymptotic inference after model selection. <em>BIOMET</em>,
<em>111</em>(1), 109–127. (<a
href="https://doi.org/10.1093/biomet/asad023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I propose a new type of confidence interval for correct asymptotic inference after using data to select a model of interest without assuming any model is correctly specified. This hybrid confidence interval is constructed by combining techniques from the selective inference and post-selection inference literatures to yield a short confidence interval across a wide range of data realizations. I show that hybrid confidence intervals have correct asymptotic coverage, uniformly over a large class of probability distributions that do not bound scaled model parameters. I illustrate the use of these confidence intervals in the problem of inference after using the lasso objective function to select a regression model of interest and provide evidence of their desirable length and coverage properties in small samples via a set of Monte Carlo experiments that entail a variety of different data distributions as well as an empirical application to the predictors of diabetes disease progression.},
  archive      = {J_BIOMET},
  author       = {McCloskey, A},
  doi          = {10.1093/biomet/asad023},
  journal      = {Biometrika},
  month        = {2},
  number       = {1},
  pages        = {109-127},
  shortjournal = {Biometrika},
  title        = {Hybrid confidence intervals for informative uniform asymptotic inference after model selection},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online inference with debiased stochastic gradient descent.
<em>BIOMET</em>, <em>111</em>(1), 93–108. (<a
href="https://doi.org/10.1093/biomet/asad046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a debiased stochastic gradient descent algorithm for online statistical inference with high-dimensional data. Our approach combines the debiasing technique developed in high-dimensional statistics with the stochastic gradient descent algorithm. It can be used to construct confidence intervals efficiently in an online fashion. Our proposed algorithm has several appealing aspects: as a one-pass algorithm, it reduces the time complexity; in addition, each update step requires only the current data together with the previous estimate, which reduces the space complexity. We establish the asymptotic normality of the proposed estimator under mild conditions on the sparsity level of the parameter and the data distribution. Numerical experiments demonstrate that the proposed debiased stochastic gradient descent algorithm attains nominal coverage probability. Furthermore, we illustrate our method with analysis of a high-dimensional text dataset.},
  archive      = {J_BIOMET},
  author       = {Han, Ruijian and Luo, Lan and Lin, Yuanyuan and Huang, Jian},
  doi          = {10.1093/biomet/asad046},
  journal      = {Biometrika},
  month        = {2},
  number       = {1},
  pages        = {93-108},
  shortjournal = {Biometrika},
  title        = {Online inference with debiased stochastic gradient descent},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Universal robust regression via maximum mean discrepancy.
<em>BIOMET</em>, <em>111</em>(1), 71–92. (<a
href="https://doi.org/10.1093/biomet/asad031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many modern datasets are collected automatically and are thus easily contaminated by outliers. This has led to a renewed interest in robust estimation, including new notions of robustness such as robustness to adversarial contamination of the data. However, most robust estimation methods are designed for a specific model. Notably, many methods were proposed recently to obtain robust estimators in linear models, or generalized linear models, and a few were developed for very specific settings, for example beta regression or sample selection models. In this paper we develop a new approach for robust estimation in arbitrary regression models, based on maximum mean discrepancy minimization. We build two estimators that are both proven to be robust to Huber-type contamination. For one of them, we obtain a non-asymptotic error bound and show that it is also robust to adversarial contamination, but this estimator is computationally more expensive to use in practice than the other one. As a by-product of our theoretical analysis of the proposed estimators, we derive new results on kernel conditional mean embedding of distributions that are of independent interest.},
  archive      = {J_BIOMET},
  author       = {Alquier, P and Gerber, M},
  doi          = {10.1093/biomet/asad031},
  journal      = {Biometrika},
  month        = {2},
  number       = {1},
  pages        = {71-92},
  shortjournal = {Biometrika},
  title        = {Universal robust regression via maximum mean discrepancy},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient evaluation of natural stochastic policies in
off-line reinforcement learning. <em>BIOMET</em>, <em>111</em>(1),
51–69. (<a href="https://doi.org/10.1093/biomet/asad059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the efficient off-policy evaluation of natural stochastic policies, which are defined in terms of deviations from the unknown behaviour policy. This is a departure from the literature on off-policy evaluation that largely considers the evaluation of explicitly specified policies. Crucially, off-line reinforcement learning with natural stochastic policies can help alleviate issues of weak overlap, lead to policies that build upon current practice and improve policies’ implementability in practice. Compared with the classic case of a prespecified evaluation policy, when evaluating natural stochastic policies, the efficiency bound, which measures the best-achievable estimation error, is inflated since the evaluation policy itself is unknown. In this paper we derive the efficiency bounds of two major types of natural stochastic policies: tilting policies and modified treatment policies. We then propose efficient nonparametric estimators that attain the efficiency bounds under lax conditions and enjoy a partial double robustness property.},
  archive      = {J_BIOMET},
  author       = {Kallus, Nathan and Uehara, Masatoshi},
  doi          = {10.1093/biomet/asad059},
  journal      = {Biometrika},
  month        = {2},
  number       = {1},
  pages        = {51-69},
  shortjournal = {Biometrika},
  title        = {Efficient evaluation of natural stochastic policies in off-line reinforcement learning},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A linear adjustment-based approach to posterior drift in
transfer learning. <em>BIOMET</em>, <em>111</em>(1), 31–50. (<a
href="https://doi.org/10.1093/biomet/asad029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present new models and methods for the posterior drift problem where the regression function in the target domain is modelled as a linear adjustment, on an appropriate scale, of that in the source domain, and study the theoretical properties of our proposed estimators in the binary classification problem. The core idea of our model inherits the simplicity and the usefulness of generalized linear models and accelerated failure time models from the classical statistics literature. Our approach is shown to be flexible and applicable in a variety of statistical settings, and can be adopted for transfer learning problems in various domains including epidemiology, genetics and biomedicine. As concrete applications, we illustrate the power of our approach (i) through mortality prediction for British Asians by borrowing strength from similar data from the larger pool of British Caucasians, using the UK Biobank data, and (ii) in overcoming a spurious correlation present in the source domain of the Waterbirds dataset.},
  archive      = {J_BIOMET},
  author       = {Maity, Subha and Dutta, Diptavo and Terhorst, Jonathan and Sun, Yuekai and Banerjee, Moulinath},
  doi          = {10.1093/biomet/asad029},
  journal      = {Biometrika},
  month        = {2},
  number       = {1},
  pages        = {31-50},
  shortjournal = {Biometrika},
  title        = {A linear adjustment-based approach to posterior drift in transfer learning},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Rejoinder: Causal inference with misspecified exposure
mappings: Separating definitions and assumptions. <em>BIOMET</em>,
<em>111</em>(1), 25–29. (<a
href="https://doi.org/10.1093/biomet/asad071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  author       = {Sävje, F},
  doi          = {10.1093/biomet/asad071},
  journal      = {Biometrika},
  month        = {2},
  number       = {1},
  pages        = {25-29},
  shortjournal = {Biometrika},
  title        = {Rejoinder: causal inference with misspecified exposure mappings: separating definitions and assumptions},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discussion of “causal inference with misspecified exposure
mappings: Separating definitions and assumptions.” <em>BIOMET</em>,
<em>111</em>(1), 21–24. (<a
href="https://doi.org/10.1093/biomet/asad054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  author       = {Auerbach, Eric and Auerbach, Jonathan and Tabord-Meehan, Max},
  doi          = {10.1093/biomet/asad054},
  journal      = {Biometrika},
  month        = {2},
  number       = {1},
  pages        = {21-24},
  shortjournal = {Biometrika},
  title        = {Discussion of ‘Causal inference with misspecified exposure mappings: Separating definitions and assumptions’},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discussion of “causal inference with misspecified exposure
mappings: Separating definitions and assumptions.” <em>BIOMET</em>,
<em>111</em>(1), 17–20. (<a
href="https://doi.org/10.1093/biomet/asad040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  author       = {Leung, Michael P},
  doi          = {10.1093/biomet/asad040},
  journal      = {Biometrika},
  month        = {2},
  number       = {1},
  pages        = {17-20},
  shortjournal = {Biometrika},
  title        = {Discussion of ‘Causal inference with misspecified exposure mappings: Separating definitions and assumptions’},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Causal inference with misspecified exposure mappings:
Separating definitions and assumptions. <em>BIOMET</em>,
<em>111</em>(1), 1–15. (<a
href="https://doi.org/10.1093/biomet/asad019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exposure mappings facilitate investigations of complex causal effects when units interact in experiments. Current methods require experimenters to use the same exposure mappings to define the effect of interest and to impose assumptions on the interference structure. However, the two roles rarely coincide in practice, and experimenters are forced to make the often questionable assumption that their exposures are correctly specified. This paper argues that the two roles exposure mappings currently serve can, and typically should, be separated, so that exposures are used to define effects without necessarily assuming that they are capturing the complete causal structure in the experiment. The paper shows that this approach is practically viable by providing conditions under which exposure effects can be precisely estimated when the exposures are misspecified. Some important questions remain open.},
  archive      = {J_BIOMET},
  author       = {Sävje, F},
  doi          = {10.1093/biomet/asad019},
  journal      = {Biometrika},
  month        = {2},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Biometrika},
  title        = {Causal inference with misspecified exposure mappings: Separating definitions and assumptions},
  volume       = {111},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
