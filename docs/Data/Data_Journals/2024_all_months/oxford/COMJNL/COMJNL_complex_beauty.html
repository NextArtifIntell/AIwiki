<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COMJNL_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="comjnl---256">COMJNL - 256</h2>
<ul>
<li><details>
<summary>
(2024). GRNet: A graph reasoning network for enhanced multi-modal
learning in scene text recognition. <em>COMJNL</em>, <em>67</em>(12),
3239–3250. (<a href="https://doi.org/10.1093/comjnl/bxae085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in scene text recognition have predominantly focused on leveraging textual semantics. However, an over-reliance on linguistic priors can impede a model’s ability to handle irregular text scenes, including non-standard word usage, occlusions, severe distortions, or stretching. The key challenges lie in effectively localizing occlusions, perceiving multi-scale text, and inferring text based on scene context. To address these challenges and enhance visual capabilities, we introduce the Graph Reasoning Model (GRM). The GRM employs a novel feature fusion method to align spatial context information across different scales, beginning with a feature aggregation stage that extracts rich spatial contextual information from various feature maps. Visual reasoning representations are then obtained through graph convolution. We integrate the GRM module with a language model to form a two-stream architecture called GRNet. This architecture combines pure visual predictions with joint visual-linguistic predictions to produce the final recognition results. Additionally, we propose a dynamic iteration refinement for the language model to prevent over-correction of prediction results, ensuring a balanced contribution from both visual and linguistic cues. Extensive experiments demonstrate that GRNet achieves state-of-the-art average recognition accuracy across six mainstream benchmarks. These results highlight the efficacy of our multi-modal approach in scene text recognition, particularly in challenging scenarios where visual reasoning plays a crucial role.},
  archive      = {J_COMJNL},
  author       = {Jia, Zeguang and Wang, Jianming and Jin, Rize},
  doi          = {10.1093/comjnl/bxae085},
  journal      = {The Computer Journal},
  month        = {12},
  number       = {12},
  pages        = {3239-3250},
  shortjournal = {Comput. J.},
  title        = {GRNet: A graph reasoning network for enhanced multi-modal learning in scene text recognition},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). P-TIMA: A framework of t witter threat intelligence mining
and analysis based on a prompt-learning NER model. <em>COMJNL</em>,
<em>67</em>(12), 3221–3238. (<a
href="https://doi.org/10.1093/comjnl/bxae084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-source information platforms such as Twitter continuously provide the latest threat intelligence, including new vulnerabilities and in-the-wild exploitations of advanced persistent threat (APT) groups. Automated extraction of threat intelligence from Twitter has become crucial for defenders to access up-to-date threat knowledge. However, existing studies mainly rely on supervised learning methods to extract threat intelligence knowledge, such as entities, which require a large amount of annotated data. This paper presents Threat Intelligence Mining and Analysis based on Prompt Learning (P-TIMA), a framework specifically crafted for extracting and analyzing threat intelligence from Twitter. P-TIMA employs our innovative few-shot entity recognition method, SecEntPrompt (SEP), built on prompt learning, to extract vulnerability intelligence from Twitter. Additionally, P-TIMA analyzes and profiles the overarching vulnerability intelligence obtained from Twitter, along with in-the-wild exploitation intelligence of APT groups. The SEP improves the average entity recognition F1 score by 3.62-4.40 compared with the best-performing comparison model and outperforms the method based on the large language model on recognition performance and inference time. To validate our framework, we apply P-TIMA to extract vulnerability-related threat intelligence from real Twitter data. Through case studies, we then analyze trends in vulnerability threats and the exploitation capabilities of APT groups. In conclusion, our framework provides a more efficient and accurate method for extracting threat intelligence from Twitter, enabling defenders to stay up-to-date with the latest threat trends and helping them improve their defense strategies against cyber attacks.},
  archive      = {J_COMJNL},
  author       = {You, Yizhe and Jiang, Zhengwei and Yang, Peian and Jiang, Jun and Zhang, Kai and Wang, Xuren and Tu, Chenpeng and Feng, Huamin},
  doi          = {10.1093/comjnl/bxae084},
  journal      = {The Computer Journal},
  month        = {12},
  number       = {12},
  pages        = {3221-3238},
  shortjournal = {Comput. J.},
  title        = {P-TIMA: A framework of t witter threat intelligence mining and analysis based on a prompt-learning NER model},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Structure connectivity and substructure connectivity of
möbius cubes. <em>COMJNL</em>, <em>67</em>(12), 3207–3220. (<a
href="https://doi.org/10.1093/comjnl/bxae083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The connectivity is an important measurement for the fault-tolerance of networks. To provide more accurate measures for the fault-tolerance of networks than the connectivity, some generalizations of connectivity have been introduced. Substructure connectivity and structure connectivity are two extended concepts of classical connectivity. As a variant of the popular network hypercube, the Möbius cubes is also a famous interconnection network in parallel and distributed systems. In this article, we calculate H -substructure connectivity and H -structure connectivity of Möbius cubes when H is isomorphic to P m , C m and K 1 , m ⁠ .},
  archive      = {J_COMJNL},
  author       = {Zhao, Xiaojun and Xue, Shudan and Deng, Qingying and Li, Pingshan},
  doi          = {10.1093/comjnl/bxae083},
  journal      = {The Computer Journal},
  month        = {12},
  number       = {12},
  pages        = {3207-3220},
  shortjournal = {Comput. J.},
  title        = {Structure connectivity and substructure connectivity of möbius cubes},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Efficient object detector via dynamic prior and dynamic
feature fusion. <em>COMJNL</em>, <em>67</em>(12), 3196–3206. (<a
href="https://doi.org/10.1093/comjnl/bxae082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse R-CNN is a new paradigm of object detection, which predicts objects in a sparse way. However, there are some limitations in Sparse R-CNN. One is the presence of weak prior information caused by fixed learnable proposal boxes and features across different images, necessitating excessive iterations for the model to refine its predictions; the other is the inadequate exploitation of multi-scale information, leading to the sub-optimal detection performance. Thus, building upon Sparse R-CNN, we propose an efficient detector that incorporates dynamic prior and dynamic feature fusion, called D 2 -Det. In particular, for the dynamic prior part, a prior information generator module dynamically generates proposal features and boxes as the dynamic prior for different images to alleviate the inference-inefficient iterative refinement process of predictions, and we further propose the class scores decoupling method to reduce the computation overhead. Furthermore, for the dynamic feature fusion part, we develop a novel lightweight multi-scale feature fusion module, which dynamically aggregates features from all layers for each proposal box, enabling adaptive feature fusion and improving detection precision by nearly 2 AP. Experiments show that D 2 -Det can achieve 46.6 AP on COCO 2017 with fewer computations for the backbone ResNet50, surpassing most of the state-of-the-art detectors.},
  archive      = {J_COMJNL},
  author       = {Zhang, Zihang and Liu, Yuling and Zhou, Zhili and Yang, Gaobo and Wu, Q M Jonathan},
  doi          = {10.1093/comjnl/bxae082},
  journal      = {The Computer Journal},
  month        = {12},
  number       = {12},
  pages        = {3196-3206},
  shortjournal = {Comput. J.},
  title        = {Efficient object detector via dynamic prior and dynamic feature fusion},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FDP-FL: Differentially private federated learning with
flexible privacy budget allocation. <em>COMJNL</em>, <em>67</em>(12),
3180–3195. (<a href="https://doi.org/10.1093/comjnl/bxae081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) as a privacy-preserving technology enables multiple clients to collaboratively train models on decentralized data. However, transmitting model parameters between local clients and the central server can potentially result in information leakage. Differentially private federated learning (DPFL) has emerged as a promising solution to enhance privacy. Nevertheless, existing DPFL schemes suffer from two issues: (i) most schemes that aim to achieve desired model accuracy may incur a high privacy budget. (ii) several schemes that consider the trade-off between privacy and accuracy by utilizing linear clipping bound may distort numerous model parameters. In this paper, we first propose FDP-FL, a flexible differential privacy approach for FL. FDP-FL introduces a novel series sum privacy budget allocation instead of uniform allocation and enables adaptive and nonlinear noise scale decay. In this way, a tight bound for cumulative privacy loss can be achieved while optimizing model accuracy. Then in order to mitigate gradient leakages caused by honest-but-curious clients and server, we further design client-level FDP-FL and record-level FDP-FL, respectively. Experimental results demonstrate that our FDP-FL improves model accuracy by ∼ 13.3% compared with the basic DP-FL under a fixed privacy budget and outperforms existing trade-off schemes with the same hyperparameter setting.},
  archive      = {J_COMJNL},
  author       = {Qian, Wenjun and Shen, Qingni and Chen, Xiaoyi and Li, Cong and Fang, Yuejian and Wu, Zhonghai},
  doi          = {10.1093/comjnl/bxae081},
  journal      = {The Computer Journal},
  month        = {12},
  number       = {12},
  pages        = {3180-3195},
  shortjournal = {Comput. J.},
  title        = {FDP-FL: Differentially private federated learning with flexible privacy budget allocation},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research on blind reversible database watermarking algorithm
based on dual embedding strategy. <em>COMJNL</em>, <em>67</em>(12),
3169–3179. (<a href="https://doi.org/10.1093/comjnl/bxae080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Massive databases encounter security such as data theft, illegal copying, and copyright infringement during creating, transmitting, and sharing of big data. The reversible data watermarking technology can effectively solve these problems, which can extract the watermark information accurately and recover the original carrier data without any distortion. However, most existing methods extract watermark information non-blindly and cannot effectively achieve a balance between watermark embedding capacity and data distortion. This paper proposes a blind reversible database watermarking method based on dual embedding, which combines histogram shifting and distortion-free watermarking methods to achieve an adaptive selection of histogram bins, blind extraction of watermark information, and carrier data recovery. The proposed method preprocesses the database tuples by scrambling them and constructs a prediction error histogram using first-layer tuples in square prediction within each group. The watermark information is embedded through adaptive selection and expansion of histogram bins, while the distortion-free watermarking method is used in another layer to assist in the recovery of original carrier data. The experimental results show that the proposed method can achieve an embedding capacity of more than three times the capacity of existing methods. It can also achieve blind watermark extraction and outperform some other state-of-the-art methods.},
  archive      = {J_COMJNL},
  author       = {Qi, Wenfa and Li, Cheng and Han, Xinhui},
  doi          = {10.1093/comjnl/bxae080},
  journal      = {The Computer Journal},
  month        = {12},
  number       = {12},
  pages        = {3169-3179},
  shortjournal = {Comput. J.},
  title        = {Research on blind reversible database watermarking algorithm based on dual embedding strategy},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph neural network based phishing account detection in
ethereum. <em>COMJNL</em>, <em>67</em>(12), 3160–3168. (<a
href="https://doi.org/10.1093/comjnl/bxae079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  author       = {Ratra, Siftee and Ghosh, Mohona and Baliyan, Niyati and Rashmitha Mohan, Jinka and Singh, Sanjana},
  doi          = {10.1093/comjnl/bxae079},
  journal      = {The Computer Journal},
  month        = {12},
  number       = {12},
  pages        = {3160-3168},
  shortjournal = {Comput. J.},
  title        = {Graph neural network based phishing account detection in ethereum},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supervisor synthesis for opacity enforcement in partially
observed discrete event systems. <em>COMJNL</em>, <em>67</em>(12),
3151–3159. (<a href="https://doi.org/10.1093/comjnl/bxae077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Opacity is an important system property that is particularly relevant in the context of system security and privacy. A system is considered opaque if the predefined secret behavior of the system is not leaked to an external intruder. In this work, the opacity property is studied in the framework of labeled Petri nets (LPNs). The secret in an LPN system is characterized by a subset of reachable markings. Firstly, an opacity basis reachability graph (OBRG) containing opacity information of the system is developed to denote a system’s reachability set without computing all reachable states. Then the observer of the OBRG is computed, based on which a necessary and sufficient condition is derived to verify the opacity of the LPN system. Finally, given an LPN that does not satisfy the opacity, a maximally permissive supervisor is introduced to guarantee that the controlled system is opaque.},
  archive      = {J_COMJNL},
  author       = {Xie, Huawei and Liu, Jing and Li, Na},
  doi          = {10.1093/comjnl/bxae077},
  journal      = {The Computer Journal},
  month        = {12},
  number       = {12},
  pages        = {3151-3159},
  shortjournal = {Comput. J.},
  title        = {Supervisor synthesis for opacity enforcement in partially observed discrete event systems},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoinFA: An efficient coin mixing scheme with flexible
amounts. <em>COMJNL</em>, <em>67</em>(12), 3141–3150. (<a
href="https://doi.org/10.1093/comjnl/bxae076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coin mixing is an efficient anonymization technology of cryptocurrency used to eliminate the linkability of transaction parties by hiding their addresses in an anonymous set. However, a common weakness with most existing coin mixing schemes is that the amount of mixed coins must be the same for all requests within a mixing cycle, otherwise it is easy for an attacker to restore the linkability of transaction parties. In this paper, we design a stage-payable puzzle solution mechanism, named CoinFA, which reverses the control of the requesting amounts to the users for flexible mixing amounts. In our design, the payee (with an output address) first requests a puzzle from the mixers and the latter are the only ones who know the solution of the puzzle. If the payee solves the puzzle successfully, he can be rewarded with the corresponding Bitcoins. The payer (allowed to have multiple input addresses) then requests the solution by paying in installments. We achieve better security by weakening the rights of the involved third parties, while the hierarchical structure allows our solution to have better efficiency and robustness. We perform a security analysis on CoinFA based on the standard Rivest-Shamir-Adleman (RSA) assumption and Elliptic Curve Digital Signature Algorithm unforgeability. We also analyze the performance of CoinFA by comparing it with two related schemes, and the results show that our CoinFA scheme has a greater advantage when the mixing amount is relatively small.},
  archive      = {J_COMJNL},
  author       = {Yang, Xin and Zeng, Peng and Raymond Choo, Kim-Kwang and Li, Chengju and Yang, Yanzhao},
  doi          = {10.1093/comjnl/bxae076},
  journal      = {The Computer Journal},
  month        = {12},
  number       = {12},
  pages        = {3141-3150},
  shortjournal = {Comput. J.},
  title        = {CoinFA: An efficient coin mixing scheme with flexible amounts},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Practical and secure policy-based chameleon hash for
redactable blockchains. <em>COMJNL</em>, <em>67</em>(11), 3128–3139. (<a
href="https://doi.org/10.1093/comjnl/bxae075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Policy-based chameleon hash functions have been widely proposed for its use in blockchain rewriting systems. They allow anyone to create a mutable transaction associated with an access policy, while an authorized user who possesses sufficient rewriting privileges from a trusted authority satisfying the access policy can rewrite the mutable transaction. However, existing chameleon hash functions lack certain fundamental security guarantees, including forward security and backward security. In this paper, we introduce a new primitive called forward/backward-secure policy-based chameleon hash (FB-PCH for short). We present a practical instantiation. We prove that the proposed scheme achieves forward/backward-secure collision-resistance, and show its practicality through implementation and evaluation analysis.},
  archive      = {J_COMJNL},
  author       = {Li, Nan and Li, Yingjiu and Manulis, Mark and Tian, Yangguang and Yang, Guomin},
  doi          = {10.1093/comjnl/bxae075},
  journal      = {The Computer Journal},
  month        = {11},
  number       = {11},
  pages        = {3128-3139},
  shortjournal = {Comput. J.},
  title        = {Practical and secure policy-based chameleon hash for redactable blockchains},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Defog YOLO for road object detection in foggy weather.
<em>COMJNL</em>, <em>67</em>(11), 3115–3127. (<a
href="https://doi.org/10.1093/comjnl/bxae074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection research predominantly focuses on clear weather conditions, often overlooking the challenges posed by foggy weather. Fog impairs the vision of onboard cameras, creating significant obstacles for autonomous vehicles. To tackle these issues, we present the Defog YOLO algorithm, specifically designed for road object detection in foggy conditions. Our approach integrates an enhanced U-Net framework for visual defogging, where the encoder leverages super-resolution back projection to combine multi-layer features. The decoder employs a back projection feedback mechanism to improve image restoration. Additionally, we augment the Feature Pyramid Network with a noise-aware attention mechanism, allowing the network to emphasize critical channel and spatial information while mitigating noise. Given the scarcity of labeled foggy images, we introduce a fog addition module to generate a more diverse training dataset. We validate our method using a synthesized FOG-TRAINVAL dataset, derived from the VOC dataset, demonstrating its robustness in foggy scenarios. Experimental results show that our proposed method achieves an mAP score of 60% on the Real-world Task-driven Testing Set foggy weather test set, with a precision of 86.7% and a recall of 54.2%. These findings underscore the effectiveness and improved generalizability of our approach for object detection in adverse weather conditions.},
  archive      = {J_COMJNL},
  author       = {Shi, Xiaolong and Song, Anjun},
  doi          = {10.1093/comjnl/bxae074},
  journal      = {The Computer Journal},
  month        = {11},
  number       = {11},
  pages        = {3115-3127},
  shortjournal = {Comput. J.},
  title        = {Defog YOLO for road object detection in foggy weather},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Customization effort estimation of microsoft ERP projects.
<em>COMJNL</em>, <em>67</em>(11), 3105–3114. (<a
href="https://doi.org/10.1093/comjnl/bxae073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The enterprise resource planning (ERP) software market is rapidly growing due to its potential in aiding organizations to streamline their business processes and enhance workforce productivity. However, ERP customization projects often encounter delays in schedules and exceed budgets mainly because estimations of project customization effort and duration are typically carried out using ad-hoc methods. This study aims to construct and validate a customization effort estimation (CEE) model tailored for ERP projects employing Microsoft Dynamics 365 (D365) customization. To achieve this, six critical factors that impact CEE in D365 projects are identified. Subsequently, an industry-wide survey is conducted to determine the relative significance of these factors and to establish their weights for both simple and complex variants. The proposed model D365CustomizePro is created through the application of stepwise multiple linear regression and then validated through the leave-one-out cross-validation technique based on data collected from 21 real-life D365 projects undertaken at a software house. The estimation accuracy of D365CustomizePro is compared with the estimation accuracy achieved by using two different well-known effort estimation techniques, i.e. expert judgment and COCOMO II (algorithmic estimation model). Results reveal that D365CustomizePro exhibits better estimation accuracy vis-á-vis both expert judgment and COCOMO II.},
  archive      = {J_COMJNL},
  author       = {Yousaf, Tameem and Afzal Malik, Ali and Daud, Marriam},
  doi          = {10.1093/comjnl/bxae073},
  journal      = {The Computer Journal},
  month        = {11},
  number       = {11},
  pages        = {3105-3114},
  shortjournal = {Comput. J.},
  title        = {Customization effort estimation of microsoft ERP projects},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DGA domain embedding with deep metric learning.
<em>COMJNL</em>, <em>67</em>(11), 3094–3104. (<a
href="https://doi.org/10.1093/comjnl/bxae072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Botnets currently use domain-generation algorithms to produce fast-flux domains that enable them to evade detection. Accurately categorizing these botnet domains is crucial to develop cybersecurity solutions against botnet threats. However, existing methods, requiring labeled data, are ineffective against new botnets. To address this issue, we propose Domain2Vec, a metric learning-based approach that can explore new botnets. Domain2Vec integrates a framework of metric learning, which uses individual domains from known botnets for categorization of unknown botnet domains. The training involves an attention-based encoder, and it includes a constraint to ensure that samples with the same labels are closer in the embedding space. The categorization uses the encoder to project domain names into appropriate representations (numerical vectors), even for domains from new botnets. Finally, Domain2Vec uses numerical vectors to explore botnets. Experiments showed that Domain2Vec performs well on domain retrieval and clustering tasks without labeled data, outperforming the state of the art by 13% and 100%, respectively. Real-world tests demonstrate that Domain2Vec can effectively identify unreported malicious domains and monitor botnet activities.},
  archive      = {J_COMJNL},
  author       = {Yang, Yifan and Li, Xionglve and Yang, Tao and Hou, Bingnan and Zeng, Lingbin and Cai, Zhiping and Kuang, Wenyuan},
  doi          = {10.1093/comjnl/bxae072},
  journal      = {The Computer Journal},
  month        = {11},
  number       = {11},
  pages        = {3094-3104},
  shortjournal = {Comput. J.},
  title        = {DGA domain embedding with deep metric learning},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flower pollination-enhanced CNN for lung disease diagnosis.
<em>COMJNL</em>, <em>67</em>(11), 3080–3093. (<a
href="https://doi.org/10.1093/comjnl/bxae071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The utilization of automated software tools is imperative to enhance the efficiency of lung diseases through the analysis of X-ray images. The main objective of this study is to employ an analysis of chest X-ray images to diagnose lung disease. This study presents an Optimized Convolutional Neural Network (CNNFPA) designed to automate the diagnosis of lung disease. The Flower pollination technique is employed to optimize the hyperparameters associated with the training of the layers of the Convolutional Neural Network (CNN). In this paper, a novel model called RCNNFPA model is proposed, which makes use of a pre-trained ResNet50 with its layers frozen. Subsequently, CNNFPA architecture is integrated on top of the frozen ResNet-50 layers. This approach allowed us to leverage the knowledge captured by the ResNet-50 model on a large-scale dataset. To assess the efficacy of the proposed model and perform a comparison study using several classification methodologies, various publicly available datasets comprising images of COVID-19, Viral Pneumonia, Normal, and Tuberculosis are employed. As optimized and elaborated upon in this study, the CNN model is juxtaposed with existing state-of-the-art models. The proposed novel RCNNFPA model demonstrates considerable potential in facilitating the automated screening of individuals affected by different lung diseases.},
  archive      = {J_COMJNL},
  author       = {Khate, Kevisino and Bahadur Sinha, Bam and Neelima, Arambam},
  doi          = {10.1093/comjnl/bxae071},
  journal      = {The Computer Journal},
  month        = {11},
  number       = {11},
  pages        = {3080-3093},
  shortjournal = {Comput. J.},
  title        = {Flower pollination-enhanced CNN for lung disease diagnosis},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). K-nearest neighbor smart contract classification with
semantic feature enhancement. <em>COMJNL</em>, <em>67</em>(11),
3067–3079. (<a href="https://doi.org/10.1093/comjnl/bxae070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to quickly and accurately retrieve relevant smart contracts from a huge amount of smart contracts has become an urgent need for users. The classification of smart contracts offers a solution by narrowing down the search space. Existing smart contract classification methods suffer from incomplete semantic feature extraction and a lack of consideration of the existence of rich semantics in existing smart contracts of the same class. To address the above problems, we propose a contrast learning and semantic feature embedding approach to enhance K-Nearest Neighbor (CL-SFE-IKNN). Our method fuses local features, global features, and account transaction features of the smart contract source code to perfect the semantics of the contract. Our method adopts KNN to retrieve multiple instances of contracts in the same class and assigns weights to the model output based on their labels. Meanwhile, we introduce contrastive learning and semantic feature embedding to enhance KNN retrieval to high-quality nearest neighbors of the same class. Experimental results show that by combining a KNN classifier with a traditional linear classifier, our model achieves the best performance compared with other baseline models.},
  archive      = {J_COMJNL},
  author       = {Tian, Gang and Zhao, Guangxin and Wang, Rui and Wang, Jiachang and He, Cheng},
  doi          = {10.1093/comjnl/bxae070},
  journal      = {The Computer Journal},
  month        = {11},
  number       = {11},
  pages        = {3067-3079},
  shortjournal = {Comput. J.},
  title        = {K-nearest neighbor smart contract classification with semantic feature enhancement},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FormatAEG: A framework for bypassing ASLR defense and
automated exploitation of format string vulnerability. <em>COMJNL</em>,
<em>67</em>(11), 3056–3066. (<a
href="https://doi.org/10.1093/comjnl/bxae069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The format string vulnerability is a common software vulnerability. A well-constructed format string can read and modify arbitrary memory addresses, causing serious system problems. Existing automated exploit generation solutions for format string vulnerability are unable to cope with the limitations imposed by the vulnerability defense mechanism Address Space Layout Randomization (ASLR) and the program itself on vulnerability exploitation. In this paper, to address the above challenges, we propose FormatAEG, the first automatic exploitation framework for format string vulnerabilities that can bypass ASLR defense and the program&#39;s own constraints. Specifically, we first proposed an arbitrary address reading and writing method based on a format string vulnerability, which can modify the target address data by directly arranging the target address or automatically searching and utilizing the pointer chain in the stack. Then, we propose a vulnerability reentry method based on global offset table (GOT) hijacking, which hijacks the program control flow by modifying function addresses in the GOT, making the vulnerability reentrant. In the experimental section, we evaluated FormatAEG using 20 Capture The Flag programs from top international tournaments and two real-world programs with format string vulnerabilities. The evaluation results show that with ASLR defense turned on, FormatAEG successfully detects format string vulnerability in 19 of these programs and generates exploit code for 15 of them. Compared with existing tools, FormatAEG detected 11 more format string vulnerabilities and generated 13 more exploit codes.},
  archive      = {J_COMJNL},
  author       = {Xu, Shenglin and Jiang, Zhiyuan and Wang, Yongjun and Xie, Peidai},
  doi          = {10.1093/comjnl/bxae069},
  journal      = {The Computer Journal},
  month        = {11},
  number       = {11},
  pages        = {3056-3066},
  shortjournal = {Comput. J.},
  title        = {FormatAEG: A framework for bypassing ASLR defense and automated exploitation of format string vulnerability},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A generic multi-level framework for building term-weighting
schemes in text classification. <em>COMJNL</em>, <em>67</em>(11),
3042–3055. (<a href="https://doi.org/10.1093/comjnl/bxae068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Term weighting is essential for text classification tasks, and thus various supervised term-weighting (STW) methods have been designed and presented in recent years, such as TF (term frequency)-IG (information gain), TF-MI (mutual information), TF-RF (relevance frequency), and TF-IDF (inverse document frequency)-ICSDF (inverse class space density frequency). Unlike other schemes, TF-IDF-ICSDF considers not only the local factor (i.e. TF) and the category factor (i.e. ICSDF) but also the global factor (i.e. IDF) in the weighting process. Hence, a natural question is whether IDF is really useful for improving the classification performance of STW schemes. To explore this issue, a generic multi-level framework composed of term-level, text-level, and category-level is first established, which corresponds to local factor, global factor, and category factor, respectively. Based on the generic multi-level framework, a new two-level STW method, TF-ICSDF, can be generated by removing the IDF from the TF-IDF-ICSDF scheme. Conversely, we also integrated the IDF with other two-level STW schemes (e.g. TF-IG, TF-MI, TF-RF) to obtain several three-level STW schemes. We verified the general classification performance of our proposed STW schemes on three open benchmark datasets. The results manifest that performance can usually be boosted if IDF is incorporated into the STW schemes, indicating that weighting terms utilizing the IDF factor could provide better text representation. Therefore, the generic multi-level framework and STW schemes we proposed are effective.},
  archive      = {J_COMJNL},
  author       = {Tang, Zhong},
  doi          = {10.1093/comjnl/bxae068},
  journal      = {The Computer Journal},
  month        = {11},
  number       = {11},
  pages        = {3042-3055},
  shortjournal = {Comput. J.},
  title        = {A generic multi-level framework for building term-weighting schemes in text classification},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image retrieval based on auto-encoder and clustering with
centroid update. <em>COMJNL</em>, <em>67</em>(11), 3031–3041. (<a
href="https://doi.org/10.1093/comjnl/bxae067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper suggests a revolutionary deep learning method using a dynamic deep auto-encoder for improving the performance of indexing the feature vectors of images by centroid updation. Feature vectors such as color, semantic, and spatial local binary pattern are extracted from the images for content-based image retrieval. The owner encrypts the images for protection using elliptic curve cryptography before uploading them to the cloud. A black hole entropic fuzzy clustering with Tversky indexing is used to retrieve similar information. When the new training image is matched with any of the centroid then the centroid gets updated by using dynamic deep auto-encoder. During the auto-encoder phase, the conflicted data points are dedicated to reconstruction and the reliable data points are helpful to centroid updation. The suggested BHE fuzzy clustering with dynamic deep auto-encoder approach fared better than the current methods, achieving the best accuracy of 97.605%, the highest |$\boldsymbol{F_{1}}$| score of 90.210%, better precision of 90.001%, and the highest recall of 95.149%.},
  archive      = {J_COMJNL},
  author       = {Nalini Sujantha Bel, K and Sam, I Shatheesh},
  doi          = {10.1093/comjnl/bxae067},
  journal      = {The Computer Journal},
  month        = {11},
  number       = {11},
  pages        = {3031-3041},
  shortjournal = {Comput. J.},
  title        = {Image retrieval based on auto-encoder and clustering with centroid update},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Face recognition using deep learning on raspberry pi.
<em>COMJNL</em>, <em>67</em>(10), 3020–3030. (<a
href="https://doi.org/10.1093/comjnl/bxae066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial recognition on resource-limited devices such as the Raspberry Pi poses a challenge due to inherent processing limitations. For real-time applications, finding efficient and reliable solutions is critical. This study investigated the feasibility of using transfer learning for facial recognition tasks on the Raspberry Pi and evaluated transfer learning that leverages knowledge from previously trained models. We compared two well-known deep learning (DL) architectures, InceptionV3 and MobileNetV2, adapted to face recognition datasets. MobileNetV2 outperformed InceptionV3, achieving a training accuracy of 98.20% and an F1 score of 98%, compared to InceptionV3’s training accuracy of 86.80% and an F1 score of 91%. As a result, MobileNetV2 emerges as a more powerful architecture for facial recognition tasks on the Raspberry Pi when integrated with transfer learning. These results point to a promising direction for deploying efficient DL applications on edge devices, reducing latency, and enabling real-time processing.},
  archive      = {J_COMJNL},
  author       = {Ahmed Ali Aboluhom, Abdulatif and Kandilli, Ismet},
  doi          = {10.1093/comjnl/bxae066},
  journal      = {The Computer Journal},
  month        = {10},
  number       = {10},
  pages        = {3020-3030},
  shortjournal = {Comput. J.},
  title        = {Face recognition using deep learning on raspberry pi},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pornographic video detection based on semantic and image
enhancement. <em>COMJNL</em>, <em>67</em>(10), 3009–3019. (<a
href="https://doi.org/10.1093/comjnl/bxae065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pornographic video detection is of significant importance in curbing the proliferation of pornographic information on online video platforms. However, existing works often employ generic frame extraction methods that ignore the low-latency requirements of detection scenarios and the characteristics of pornographic videos. Additionally, existing detection methods have difficulties in detail characterization and semantic understanding, resulting in low accuracy. Therefore, this paper proposes an efficient pornographic video detection framework based on semantic and image enhancement. Firstly, a keyframe extraction method tailored for pornographic video detection is proposed to select representative frames. Secondly, a light enhancement method is introduced to facilitate accurate capture of pornographic visual cues. Moreover, a compression-reconstruction network is employed to eliminate adversarial perturbations, enabling models to obtain reliable features. Subsequently, YOLOv5 is introduced to locate and crop human targets in keyframes, reducing background interference and enhancing the expression of human semantic information. Finally, MobileNetV3 is employed to determine if the human targets contain pornographic content. The proposed framework is validated on the publicly available NPDI dataset, achieving an accuracy of 95.9%, surpassing existing baseline methods.},
  archive      = {J_COMJNL},
  author       = {Zeng, Junhao and Liang, Gang and Ma, Yixin and Yang, Xinyan and Chen, Cheng},
  doi          = {10.1093/comjnl/bxae065},
  journal      = {The Computer Journal},
  month        = {10},
  number       = {10},
  pages        = {3009-3019},
  shortjournal = {Comput. J.},
  title        = {Pornographic video detection based on semantic and image enhancement},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DRL-tomo: A deep reinforcement learning-based approach to
augmented data generation for network tomography. <em>COMJNL</em>,
<em>67</em>(10), 2995–3008. (<a
href="https://doi.org/10.1093/comjnl/bxae064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and current comprehension of network status is crucial for efficient network management. Nevertheless, direct network measurement strategies entail substantial traffic overhead and demand intricate coordination among network entities, making them impractical. Network tomography, an indirect measurement approach, utilizes insights garnered from measured parts to deduce characteristics of the entire network. Past studies frequently depend on acquiring challenging-to-access information, such as the complete network topology or support from specialized protocols. Unfortunately, these constraints pose challenges in non-cooperative scenarios where obtaining such information is difficult. Recent endeavors pursue emancipating tomography from dependence on copious information, striving to predict unmeasured path performance using limited data. Nevertheless, the disparity between the measured data and actual performance has hindered the accuracy. In response, we introduce an innovative tomography framework named DRL-Tomo, designed to alleviate potential biases. DRL-Tomo initiates by generating augmented data through deep reinforcement learning, gradually approximating the genuine performance of unmeasured paths. Subsequently, a neural network model is trained using this augmented data, enabling precise inferences. Our experiments, encompassing both real-world and synthetic datasets, vividly demonstrate DRL-Tomo’s remarkable enhancement. Specifically, it achieves a substantial 10%–67% improvement in path delay prediction and an impressive 30%–98% enhancement in path loss rate prediction.},
  archive      = {J_COMJNL},
  author       = {Hou, Changsheng and Hou, Bingnan and Li, Xionglve and Zhou, Tongqing and Chen, Yingwen and Cai, Zhiping},
  doi          = {10.1093/comjnl/bxae064},
  journal      = {The Computer Journal},
  month        = {10},
  number       = {10},
  pages        = {2995-3008},
  shortjournal = {Comput. J.},
  title        = {DRL-tomo: A deep reinforcement learning-based approach to augmented data generation for network tomography},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic compact data structure for temporal reachability
with unsorted contact insertions. <em>COMJNL</em>, <em>67</em>(10),
2984–2994. (<a href="https://doi.org/10.1093/comjnl/bxae063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal graphs represent interactions between entities over time. Deciding whether entities can reach each other through temporal paths is useful for various applications such as in communication networks and epidemiology. Previous works have studied the scenario in which addition of new interactions can happen at any point in time. A known strategy maintains, incrementally, a Timed Transitive Closure using a dynamic data structure composed of |$O(n^{2})$| binary search trees containing non-nested time intervals. However, space usage for storing these trees grows rapidly as more interactions are inserted. In this paper, we introduce a compact data structure that represents each tree as two dynamic bit-vectors. Furthermore, we present two variants of this data structure: one representing bits in dynamic bit-vectors explicitly and the other representing only the active bits by encoding their consecutive distances. In our experiments, we observed that our data structure improves space usage while having similar time performance for incremental updates when comparing with the previous strategy. The first variant of our data structure gives the best space improvement when constructing Time Transitive Closures for temporally dense temporal graphs, and the second variant uses less space for temporally sparse temporal graphs.},
  archive      = {J_COMJNL},
  author       = {Fernando Afra Brito, Luiz and Keese Albertini, Marcelo and Augusto Nassif Travençolo, Bruno and Navarro, Gonzalo},
  doi          = {10.1093/comjnl/bxae063},
  journal      = {The Computer Journal},
  month        = {10},
  number       = {10},
  pages        = {2984-2994},
  shortjournal = {Comput. J.},
  title        = {Dynamic compact data structure for temporal reachability with unsorted contact insertions},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VQCNN: Variational quantum convolutional neural networks
based on quantum filters and fully connected layers. <em>COMJNL</em>,
<em>67</em>(10), 2970–2983. (<a
href="https://doi.org/10.1093/comjnl/bxae062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical machine learning is more susceptible to adversarial examples due to its linear and non-robust nature, which results in a severe degradation of the recognition accuracy of classical machine learning models. Quantum techniques are shown to have a higher robustness advantage and are more resistant to attacks from adversarial examples than classical machine learning. Inspired by the robustness advantage of quantum computing and the feature extraction advantage of convolutional neural networks, this paper proposes a novel variational quantum convolutional neural network model (VQCNN), whose quantum fully connected layer consists of a combination of a quantum filter and a variational quantum neural network to increase the model’s adversarial robustness. The network intrusion detection model based on VQCNN is verified on KDD CUP99 and UNSW-NB datasets. The results show that under the attack of Fast Gradient Sign Method, the decline values of accuracy, precision, and recall rate of the intrusion detection model based on VQCNN are less than those of the other four models, and it has higher adversarial robustness.},
  archive      = {J_COMJNL},
  author       = {Qi, Han and Wang, Jingtong and Cui, Yufan},
  doi          = {10.1093/comjnl/bxae062},
  journal      = {The Computer Journal},
  month        = {10},
  number       = {10},
  pages        = {2970-2983},
  shortjournal = {Comput. J.},
  title        = {VQCNN: Variational quantum convolutional neural networks based on quantum filters and fully connected layers},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global-local graph attention: Unifying global and local
attention for node classification. <em>COMJNL</em>, <em>67</em>(10),
2959–2969. (<a href="https://doi.org/10.1093/comjnl/bxae060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) are deep learning models specifically designed for analyzing graph-structured data, capturing complex relationships and structures to improve analysis and prediction. A common task in GNNs is node classification, where each node in the graph is assigned a predefined category. The Graph Attention Network (GAT) is a popular variant of GNNs known for its ability to capture complex dependencies by assigning importance weights to nodes during information aggregation. However, the GAT’s reliance on local attention mechanisms limits its effectiveness in capturing global information and long-range dependencies. To address this limitation, we propose a new attention mechanism called Global-Local Graph Attention (GLGA). Our mechanism enables the GAT to capture long-range dependencies and global graph structures while maintaining its ability to focus on local interactions. We evaluate our algorithm on three citation datasets (Cora, Citeseer, and Pubmed) using multiple metrics, demonstrating its superiority over other baseline models. The proposed GLGA mechanism has been proven to be an effective solution for improving node classification tasks.},
  archive      = {J_COMJNL},
  author       = {Lin, Keao and Xie, Xiaozhu and Weng, Wei and Du, Xiaofeng},
  doi          = {10.1093/comjnl/bxae060},
  journal      = {The Computer Journal},
  month        = {10},
  number       = {10},
  pages        = {2959-2969},
  shortjournal = {Comput. J.},
  title        = {Global-local graph attention: Unifying global and local attention for node classification},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Security analysis of the MAP-t IPv6 transition technology.
<em>COMJNL</em>, <em>67</em>(10), 2945–2958. (<a
href="https://doi.org/10.1093/comjnl/bxae059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on one of the most prominent IPv6 transition technologies, namely Mapping of Address and Port using Translation (MAP-T), and we give attention to Mapping of Address and Port with Encapsulation (MAP-E) as well. We emphasize the uniqueness of MAP-T and MAP-E, and we discuss the differences between those two technologies, including their topology, functionality, and security vulnerabilities. We apply a threat modeling technique, Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, and Elevation of Privilege (STRIDE), to assess potential vulnerabilities in the MAP-T infrastructure. Furthermore, we build a testbed for MAP-T using the open-source software, Jool, and we conduct testing on the translation process capabilities of Jool and its port allocation per subscriber. Finally, we present various attacking scenarios against the main routers of MAP-T, such as IP address spoofing, information disclosure, and source port exhaustion, and we propose mitigation methods for several attacks.},
  archive      = {J_COMJNL},
  author       = {Al-Azzawi, Ameen and Lencse, Gábor},
  doi          = {10.1093/comjnl/bxae059},
  journal      = {The Computer Journal},
  month        = {10},
  number       = {10},
  pages        = {2945-2958},
  shortjournal = {Comput. J.},
  title        = {Security analysis of the MAP-T IPv6 transition technology},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). COVID-19 virus mutation prediction with LSTM and attention
mechanisms. <em>COMJNL</em>, <em>67</em>(10), 2934–2944. (<a
href="https://doi.org/10.1093/comjnl/bxae058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus disease 2019 (COVID-19), caused by Severe Acute Respiratory Syndrome Coronavirus 2, is an emerging and rapidly spreading type of coronavirus. One of the most important reasons for the rapid spread of the COVID-19 virus are the frequent mutations of the COVID-19 virus. One of the most important methods to overcome mutations of the COVID-19 virus is to predict these mutations before they occur. In this study, we propose a robust HyperMixer and long short-term memory based model with attention mechanisms, HyperAttCov, for COVID-19 virus mutation prediction. The proposed HyperAttCov model outperforms several state-of-the-art methods. Experimental results have showed that the proposed HyperAttCov model reached accuracy 70.0%, precision 92.0%, MCC 46.5% on the COVID-19 testing dataset. Similarly, the proposed HyperAttCov model reached accuracy 70.2%, precision 90.4%, MCC 46.2% on the COVID-19 testing dataset with an average of 10 random trail. Besides, When the proposed HyperAttCov model with 10 random trail has been compared with compared to the study in the literature, the average of performance values has been increased by accuracy 7.18%, precision 37.39%, MCC 49.51% on the testing dataset. As a result, the proposed HyperAttCov can successfully predict mutations occurring on the COVID-19 dataset in the 2022 year.},
  archive      = {J_COMJNL},
  author       = {Burukanli, Mehmet and Yumuşak, Nejat},
  doi          = {10.1093/comjnl/bxae058},
  journal      = {The Computer Journal},
  month        = {10},
  number       = {10},
  pages        = {2934-2944},
  shortjournal = {Comput. J.},
  title        = {COVID-19 virus mutation prediction with LSTM and attention mechanisms},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TPRT: A trajectory publishing scheme for the internet of
vehicles based on radix tree. <em>COMJNL</em>, <em>67</em>(10),
2920–2933. (<a href="https://doi.org/10.1093/comjnl/bxae057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The location and trajectory information generated in the Internet of Vehicles (IoV) provides sufficient data support for road network planning. However, there is much sensitive information in these data, and publishing them directly will lead to serious privacy leakage risks. Therefore, this paper proposes a trajectory publishing scheme for the Internet of Vehicles based on radix tree (TPRT). Firstly, the trajectory data are divided into multiple location planes based on timestamps and processed using clustering and generalization. This approach addresses the issue of real-life trajectory data having almost no identical prefixes, making it more suitable for storage in radix tree. Then, the directed graph and the shortest path algorithm are utilized to synthesize a new trajectory dataset for publication. Subsequently, a radix tree structure that satisfies differential privacy is defined. In comparison to the research method employing a prefix tree, the radix tree not only captures the spatiotemporal characteristics of the trajectories but also reduces space consumption. Finally, a novel method of noise addition is proposed. In contrast to the traditional layer-by-layer noise addition approach, our method reduces the cost of noise addition and enhances data availability. Experimental results demonstrate that TPRT exhibits superior data availability compared to the baseline methods.},
  archive      = {J_COMJNL},
  author       = {Tian, Junfeng and Zhu, Qi and Shen, Jia wei and Xu, Tengfei},
  doi          = {10.1093/comjnl/bxae057},
  journal      = {The Computer Journal},
  month        = {10},
  number       = {10},
  pages        = {2920-2933},
  shortjournal = {Comput. J.},
  title        = {TPRT: A trajectory publishing scheme for the internet of vehicles based on radix tree},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DHCL-BR: Dual hypergraph contrastive learning for bundle
recommendation. <em>COMJNL</em>, <em>67</em>(10), 2906–2919. (<a
href="https://doi.org/10.1093/comjnl/bxae056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an extension of conventional top-K item recommendation solution, bundle recommendation has aroused increasingly attention. However, because of the extreme sparsity of user-bundle (UB) interactions, the existing top-K item recommendation methods suffer from poor performance when applied to bundle recommendation. While some graph-based approaches have been proposed for bundle recommendation, these approaches primarily leverage the bipartite graph to model the UB interactions, resulting in suboptimal performance. In this paper, a dual hypergraph contrastive learning model is proposed for bundle recommendation. First, we model the direct and indirect UB interactions as hypergraphs to represent the higher-order UB relations. Second, we utilize the hypergraph convolution networks to learn the user and bundle embeddings from the hypergraphs, and improve the learned embeddings through a bidirectional contrastive learning strategy. Finally, we adopt a joint loss that combines the InfoBPR loss supporting multiple negative samples and the contrastive losses to optimize model parameters for prediction. Experiments on the real-world datasets indicate that our model performs better than the state-of-the-art baseline methods.},
  archive      = {J_COMJNL},
  author       = {Zhang, Peng and Niu, Zhendong and Ma, Ru and Zhang, Fuzhi},
  doi          = {10.1093/comjnl/bxae056},
  journal      = {The Computer Journal},
  month        = {10},
  number       = {10},
  pages        = {2906-2919},
  shortjournal = {Comput. J.},
  title        = {DHCL-BR: Dual hypergraph contrastive learning for bundle recommendation},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Approximation algorithms for maximum weighted internal
spanning trees in regular graphs and subdivisions of graphs.
<em>COMJNL</em>, <em>67</em>(10), 2898–2905. (<a
href="https://doi.org/10.1093/comjnl/bxae055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let |$G$| be a vertex-weighted connected graph of |$n$| vertices and let |$T$| be a spanning tree of |$G$|⁠ . We call |$T$| a maximum weighted internal spanning tree of |$G$| if the sum of the weights of the internal vertices of |$T$| is the maximum over all spanning trees of |$G$|⁠ . The maximum weighted internal spanning tree (MaxwIST) problem asks to find such a spanning tree |$T$| of |$G$|⁠ . The problem is NP-hard. We give an |$O(dn)$| time approximation algorithm for |$d$| -regular graphs of |$n=|V|$| vertices that computes a spanning tree with total weight of the internal vertices is at least |$\frac{\beta _{d}}{\beta _{d} +d-2} - \epsilon $| of the total weight of all the vertices of the graph for any |$\epsilon&gt;0$|⁠ , where |$\beta _{d} = (d-1)H_{d-1}$|⁠ , and |$H_{d-1} = \sum _{i=1}^{d-1} i^{-1}$| is the |$(d-1)$| th harmonic number. For every |$d \geq 3$| and |$n_{0} \geq 1$|⁠ , we show the construction of a |$d$| -regular graph of at least |$n_{0}$| vertices, such that for any of its spanning trees, |$\frac{w(I)}{w(V)}\le \frac{d}{d+1}$| holds. We give an |$O(dn)$| time approximation algorithm for subdivisions of |$d$| -regular graphs, where the ratio of the internal weight of the spanning tree with the total vertex weight of the graph is at least |$\frac{d-1}{2d-3} - \epsilon $| for |$\epsilon&gt;0$|⁠ . We extend our study to |$x$| -subdivisions of Hamiltonian and hypoHamiltonian graphs, where each edge of the original Hamiltonian or hypoHamiltonian graph has been subdivided at least |$x$| times. For those two graph classes, we show that there exists a spanning tree with internal vertex weight at least |$1-\frac{2}{x-1}$| of the total vertex weight of the graph. Furthermore, we give |$O(n)$| time algorithm for |$x$| -subdivisions of biconnected outerplanar graphs and |$4$| -connected planar graphs to achieve the above bound.},
  archive      = {J_COMJNL},
  author       = {Hakim, Sheikh Azizul and Nishat, Rahnuma Islam and Rahman, Md Saidur},
  doi          = {10.1093/comjnl/bxae055},
  journal      = {The Computer Journal},
  month        = {10},
  number       = {10},
  pages        = {2898-2905},
  shortjournal = {Comput. J.},
  title        = {Approximation algorithms for maximum weighted internal spanning trees in regular graphs and subdivisions of graphs},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fault tolerance of hierarchical cubic networks based on
cluster fault pattern. <em>COMJNL</em>, <em>67</em>(10), 2890–2897. (<a
href="https://doi.org/10.1093/comjnl/bxae054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Connectivity is a meaningful metric parameter and indicator for estimating network reliability and evaluating network fault tolerance. However, the traditional connectivity and current conditional connectivity do not take into account the association between a certain node and its neighboring nodes. In fact, adjacent nodes are easily influenced by each other so that the failing probability of adjacent nodes around a faulty node is high. Therefore, cluster and super cluster connectivities are proposed to more intuitively measure the fault tolerance of the network. In this paper, we mainly explore the cluster connectivity and super cluster connectivity of the hierarchical cubic network |$HCN_{n}$|⁠ . In detail, we show that |$\kappa (HCN_{n}\mid K_{1, 0}(K_{1, 0}^{*}))=n+1$|⁠ , |$\kappa (HCN_{n}\mid K_{1, 1}(K_{1, 1}^{*}))=\kappa ^{\prime}(HCN_{n}\mid K_{1, 1}(K_{1, 1}^{*}))=n+1$|⁠ , |$\kappa (HCN_{n}\mid K_{1, m}(K_{1, m}^{*}))=\lceil n/2\rceil +1$| ( ⁠|$2\leq m\leq 4$|⁠ ), |$\kappa ^{\prime}(HCN_{n}\mid K_{1, 0}(K_{1, 0}^{*}))=2n$|⁠ , and |$\kappa ^{\prime}(HCN_{n}\mid K_{1, m}(K_{1, m}^{*}))=n+1$| ( ⁠|$2\leq m\leq 3$|⁠ ) if |$n$| is odd and |$\kappa ^{\prime}(HCN_{n}\mid K_{1, m}(K_{1, m}^{*}))=n$| ( ⁠|$2\leq m\leq 3$|⁠ ) if |$n$| is even, where |$n\geq 4$|⁠ .},
  archive      = {J_COMJNL},
  author       = {Lv, Mengjie and Fan, Weibei and Dong, Hui and Wang, Guijuan},
  doi          = {10.1093/comjnl/bxae054},
  journal      = {The Computer Journal},
  month        = {10},
  number       = {10},
  pages        = {2890-2897},
  shortjournal = {Comput. J.},
  title        = {Fault tolerance of hierarchical cubic networks based on cluster fault pattern},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IPOD2: An irrecoverable and verifiable deletion scheme for
outsourced data. <em>COMJNL</em>, <em>67</em>(10), 2877–2889. (<a
href="https://doi.org/10.1093/comjnl/bxae053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To alleviate the burden of data storage and management, there is a growing trend of outsourcing data to the cloud that enables users to remotely manage their data flexibly. However, this shift also raises concerns regarding outsourced data deletion, as users lose physical control over their outsourced data and are unable to verify its proper eradication. To address this issue, cloud service providers are required to provide a scheme that guarantees the effective deletion of outsourced data. Existing schemes, including key management-based and overwriting-based schemes, fail to ensure both the irrecoverability of deleted data and the verifiability of the deletion process. In this paper, we propose IPOD2, an irrecoverable and verifiable deletion scheme for outsourced data. Specifically, IPOD2 utilizes the overwriting-based deletion method to implement outsourced data deletion and extends the Integrity Measurement Architecture to measure the operations in the deletion process. The measurement results are protected by the Trusted Platform Module and verifiable for users. To demonstrate the viability of IPOD2, we implement a prototype of IPOD2 on the Linux kernel 5.4.120. Experimental results show that, compared with the three existing schemes, IPOD2 has the minimum overhead in both deletion and verification processes.},
  archive      = {J_COMJNL},
  author       = {Zhang, Xiaolei and Chen, Zhaoyu and Zhang, Xin and Shen, Qingni and Wu, Zhonghai},
  doi          = {10.1093/comjnl/bxae053},
  journal      = {The Computer Journal},
  month        = {10},
  number       = {10},
  pages        = {2877-2889},
  shortjournal = {Comput. J.},
  title        = {IPOD2: An irrecoverable and verifiable deletion scheme for outsourced data},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A roman urdu corpus for sentiment analysis. <em>COMJNL</em>,
<em>67</em>(9), 2864–2876. (<a
href="https://doi.org/10.1093/comjnl/bxae052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis is a dynamic field focused on understanding and predicting emotional sentiments in text or images. With the prevalence of smartphones, e-commerce and social networks, individuals readily express opinions, aiding businesses, political analysts and organizations in decision-making. Despite extensive research in sentiment analysis for various languages, challenges persist in low-resource languages like Roman Urdu. Roman Urdu, the use of Roman script to write Urdu, has gained popularity, yet limited linguistic resources hinder sentiment analysis research. This study addresses this gap by developing a bidirectional long short-term memory network with FastText embeddings and additional layers. A large Roman Urdu corpus for sentiment analysis, consisting of over 51 000 reviews, is crated and the proposed model is trained and compared with 14 other models, demonstrating an accuracy of 0.854 and an F1-score of 0.84.},
  archive      = {J_COMJNL},
  author       = {Khan, Marwa and Naseer, Asma and Wali, Aamir and Tamoor, Maria},
  doi          = {10.1093/comjnl/bxae052},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {2864-2876},
  shortjournal = {Comput. J.},
  title        = {A roman urdu corpus for sentiment analysis},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An improved DNN model for WLAN intrusion detection.
<em>COMJNL</em>, <em>67</em>(9), 2854–2863. (<a
href="https://doi.org/10.1093/comjnl/bxae051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrusion detection represents an efficacious approach for addressing security concerns. However, given the substantial volume and high-dimensional nature of WLAN dataset features, existing methods exhibit limited effectiveness in feature extraction, thereby impacting classification performance. To address above problems, an improved deep neural network (DNN) model for WLAN intrusion detection was proposed. Firstly, the activation function and loss function of a single sparse autoencoders (SAE) were determined through experiments, followed by the addition of regularization terms to the autoencoder, to prevent the model from overfitting. Subsequently, multiple SAEs were employed for a stacked architecture. This configuration served the purpose of feature dimension reduction and facilitated the selection of suitable feature dimensions for training the dataset. The chosen features were then utilized as the input layer for a DNN, with a SoftMax classifier serving as the output layer. Secondly, to obtain better DNN model parameters, the grid search method was adopted to optimize the parameters of the DNN model, namely activation, epochs, batch_size, init_mode, and optimizer. The results were visualized for assessment and analysis. Finally, the receiver operating characteristic curves were generated to assess the performance of various models, the analysis results show that the model exhibited better classifier performance.},
  archive      = {J_COMJNL},
  author       = {Wang, Haizhen and Cui, Zhiqing and Lian, Zuozheng and Yan, Jinying},
  doi          = {10.1093/comjnl/bxae051},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {2854-2863},
  shortjournal = {Comput. J.},
  title        = {An improved DNN model for WLAN intrusion detection},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An improved sensing data cleaning scheme for object
localization in edge computing environment. <em>COMJNL</em>,
<em>67</em>(9), 2838–2853. (<a
href="https://doi.org/10.1093/comjnl/bxae050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio frequency identification (RFID) is widely applied due to its fast identification speed and non-contact detection. However, the identification process of RFID tags is susceptible to interference from other tags and environmental factors, resulting in inaccurate identification data. To overcome these problem, this paper proposes an improved sensing data cleaning scheme for object localization in edge computing environment. In tag level data cleaning, we use adaptive sliding window and further consider dynamic tags and read rate in continuous reading cycle to adjust the window size timely and appropriately. In the reader level data cleaning, we estimate the tag number based on Chebyshev’s inequality through Markov chain for cyclic control and optimize different time slot lengths to improve the recognition rate. We build an edge computing environment and combine the proposed tag-level cleaning method and reader-level cleaning method to form a comprehensive RFID data cleaning process. Comparative experimental results show that the RFID data cleaning method proposed in this paper can effectively reduce redundant and missing data and improve the accuracy of tag recognition.},
  archive      = {J_COMJNL},
  author       = {Tang, Fang and Du, Nengsheng and Zhengwei, Zhong and Li, Chunlin and Luo, Youlong},
  doi          = {10.1093/comjnl/bxae050},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {2838-2853},
  shortjournal = {Comput. J.},
  title        = {An improved sensing data cleaning scheme for object localization in edge computing environment},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discriminative subspace learning with adaptive graph
regularization. <em>COMJNL</em>, <em>67</em>(9), 2823–2837. (<a
href="https://doi.org/10.1093/comjnl/bxae049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many subspace learning methods based on low-rank representation employ the nearest neighborhood graph to preserve the local structure. However, in these methods, the nearest neighborhood graph is a binary matrix, which fails to precisely capture the similarity between distinct samples. Additionally, these methods need to manually select an appropriate number of neighbors, and they cannot adaptively update the similarity graph during projection learning. To tackle these issues, we introduce Discriminative Subspace Learning with Adaptive Graph Regularization (DSL_AGR), an innovative unsupervised subspace learning method that integrates low-rank representation, adaptive graph learning and nonnegative representation into a framework. DSL_AGR introduces a low-rank constraint to capture the global structure of the data and extract more discriminative information. Furthermore, a novel graph regularization term in DSL_AGR is guided by nonnegative representations to enhance the capability of capturing the local structure. Since closed-form solutions for the proposed method are not easily obtained, we devise an iterative optimization algorithm for its resolution. We also analyze the computational complexity and convergence of DSL_AGR. Extensive experiments on real-world datasets demonstrate that the proposed method achieves competitive performance compared with other state-of-the-art methods.},
  archive      = {J_COMJNL},
  author       = {Huang, Zhuojie and Zhao, Shuping and Liang, Zien and Wu, Jigang},
  doi          = {10.1093/comjnl/bxae049},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {2823-2837},
  shortjournal = {Comput. J.},
  title        = {Discriminative subspace learning with adaptive graph regularization},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the security of a novel construction of certificateless
aggregate signature scheme for healthcare wireless medical sensor
networks. <em>COMJNL</em>, <em>67</em>(9), 2819–2822. (<a
href="https://doi.org/10.1093/comjnl/bxae048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Qiao et al . proposed a novel construction of certificateless aggregate signature (CLAS) scheme to ensure the integrity and authenticity of medical data in healthcare wireless medical sensor networks (HWMSNs). They first created an underlying certificateless signature (CLS) scheme, and then proposed a CLAS scheme from the underlying CLS scheme by adding an aggregation algorithm and a verification algorithm. In this paper, we point out that their CLS scheme is insecure because the Type I adversary can forge valid signatures. That is, the unforgeability is not actually captured by their CLS scheme. Finally, we map our cryptanalysis to the practical application. That is, in the practical application of HWMSNs, the attacker can launch real attack to their CLS scheme using our cryptanalysis to forge signatures. Therefore, Qiao et al .’s CLS scheme can be totally broken.},
  archive      = {J_COMJNL},
  author       = {Yan, Zhen and Qu, Haipeng and Lin, Xi-Jun},
  doi          = {10.1093/comjnl/bxae048},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {2819-2822},
  shortjournal = {Comput. J.},
  title        = {On the security of a novel construction of certificateless aggregate signature scheme for healthcare wireless medical sensor networks},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 2L-LSH: A locality-sensitive hash function-based method for
rapid point cloud indexing. <em>COMJNL</em>, <em>67</em>(9), 2809–2818.
(<a href="https://doi.org/10.1093/comjnl/bxae047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of 3D scanning technology has enabled the acquisition of massive point cloud models with diverse structures and large scales, thereby presenting significant challenges in point cloud processing. Fast neighboring points search is one of the most common problems, which is frequently used in model reconstruction, classification, retrieval and feature visualization. Hash function is well known for its high-speed and accurate performance in searching high-dimensional data, which is also the core of the proposed 2L-LSH. Specifically, the 2L-LSH algorithm adopts a two-step hash function strategy, in which the popular step divides the bounding box of the point cloud model and the second step constructs a generalized table-based data structure. The proposed 2L-LSH offers a highly efficient and accurate solution for fast neighboring points search in large-scale 3D point cloud models, making it a promising technique for various applications in the field. The proposed algorithm is compared with the well-known methods including Kd-tree and Octree; the obtained results demonstrated that the proposed method outperforms Kd-tree and Octree in terms of speed, i.e. the time consumption of k NN search can be 51.111% and 94.159% lower than Kd-tree and Octree, respectively. And the RN search time can be 54.519% and 41.840% lower than Kd-tree and Octree, respectively.},
  archive      = {J_COMJNL},
  author       = {Wang, Shurui and Zhang, Yuhe and Guo, Ruizhe and Zhang, Yaning and Xie, Yifei and Zhou, Xinyu},
  doi          = {10.1093/comjnl/bxae047},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {2809-2818},
  shortjournal = {Comput. J.},
  title        = {2L-LSH: A locality-sensitive hash function-based method for rapid point cloud indexing},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coverless image steganography using content-based image
patch retrieval. <em>COMJNL</em>, <em>67</em>(9), 2799–2808. (<a
href="https://doi.org/10.1093/comjnl/bxae045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image steganography is the process of concealing secret information within a cover image. The main challenge of steganography is to ensure that the embedding process does not significantly alter the cover file. In this paper, instead of modifying a cover image to carry information, steganography is performed using a set of images. These images are selected from a dataset of natural images. Each image in the dataset is divided into a number of non-overlapping patches. Then, indexing of the patches is performed based on their features. The secret image is also divided into a set of non-overlapping patches. Similar versions of the patches in the secret image are searched in the dataset to identify candidate patches. The final candidate is selected by calculating the minimum distance between the feature vector of the patches in the secret image and the patches in the dataset. Finally, the receiver retrieves the secret image using the pieces of selected images. Since, instead of embedding information in a cover image, a set of patches from natural images are selected without any changes, this approach can resist change-tracking tools, as demonstrated by experimental results, and also offers the advantage of high embedding capacity.},
  archive      = {J_COMJNL},
  author       = {Taheri, Fatemeh and Rahbar, Kambiz},
  doi          = {10.1093/comjnl/bxae045},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {2799-2808},
  shortjournal = {Comput. J.},
  title        = {Coverless image steganography using content-based image patch retrieval},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Algorithms for cold-start game recommendation based on GNN
pre-training model. <em>COMJNL</em>, <em>67</em>(9), 2787–2798. (<a
href="https://doi.org/10.1093/comjnl/bxae044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the absence of sufficient user behavior data, game recommendation systems face the cold-start problem. To address this issue, this paper proposes a solution based on the Graph Neural Network pre-training model to alleviate the cold-start problem. The proposed model directly reconstructs cold-start user/game embeddings using a meta-learning setup based on dataset training simulations and uses an adaptive neighbor sampler to improve user interaction relations and thereby to improve game recommendation performance. Experimental results demonstrate the effectiveness and practicality of the recommendation model proposed in this study. Moreover, the proposed model is embedded in the game recommendation system to visualize the recommendation results.},
  archive      = {J_COMJNL},
  author       = {Yang, Hongjuan and Tian, Gang and Xu, Chengrui and Wang, Rui},
  doi          = {10.1093/comjnl/bxae044},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {2787-2798},
  shortjournal = {Comput. J.},
  title        = {Algorithms for cold-start game recommendation based on GNN pre-training model},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intermittent fault diagnosis of product network based on PMC
model. <em>COMJNL</em>, <em>67</em>(9), 2777–2786. (<a
href="https://doi.org/10.1093/comjnl/bxae043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault diagnosis of processors plays a critical role in assessing the reliability of multiprocessor systems. The interconnection network’s diagnosability is an important metric for measuring its self-diagnostic capability which has been extensively studied in many novel mutiprocessor systems. Permanent fault diagnosability for many mutiprocessor systems has been determined; however intermittent fault diagnosability is hard to obtain due to its crypticity. In this paper, we focus on the problem pertaining to the diagnosability in the intermittent fault situation. First, by learning the characteristics of intermittent fault diagnosis in PMC model, we propose some theorems and lemmas for intermittent fault diagnosability. Secondly, we give the range of intermittent fault diagnosability of product network. Lastly, by adopting the theorems, we propose an Auto-IFD algorithm to find the intermittent faults of the hypercube network and conduct experiments to verify the theorems.},
  archive      = {J_COMJNL},
  author       = {Feng, Hao and Wu, Jiong and Chen, Lin},
  doi          = {10.1093/comjnl/bxae043},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {2777-2786},
  shortjournal = {Comput. J.},
  title        = {Intermittent fault diagnosis of product network based on PMC model},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CAIMP: Cross-architecture IoT malware detection and
prediction based on static feature. <em>COMJNL</em>, <em>67</em>(9),
2763–2776. (<a href="https://doi.org/10.1093/comjnl/bxae042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IoT malware and cross-platform malware are currently the top threats to information systems. This paper proposes a robust cross-architecture IoT malware detection and prediction model based on machine learning and opcode features using a novel approach. In our method, a feature opcode transformation model between chip architecture platforms is proposed to facilitate the process of building a detection model for cross-architecture malware on IoT devices. The feature transformation model is capable of converting opcodes between different architecture platforms using an unsupervised machine learning approach. In our approach, a machine learning model is used for the detection of cross-platform malware based on the proposed opcode features. Experiments have demonstrated that our method is effective in detecting and predicting cross-platform malware with an accuracy of up to 99.4% and an F1-score of 99.3%. The method is capable of learning on one architecture platform and detecting malware on a different architecture platform. Therefore, the method can be used to develop cross-architecture detection and zero-day malware prediction solutions on IoT devices.},
  archive      = {J_COMJNL},
  author       = {The Dung, Luong and Ngoc Toan, Nguyen and Nghi Phu, Tran},
  doi          = {10.1093/comjnl/bxae042},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {2763-2776},
  shortjournal = {Comput. J.},
  title        = {CAIMP: Cross-architecture IoT malware detection and prediction based on static feature},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vulnerability localization based on intermediate code
representation and feature fusion. <em>COMJNL</em>, <em>67</em>(9),
2749–2762. (<a href="https://doi.org/10.1093/comjnl/bxae041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vulnerability localization can assist security professionals in vulnerability validation and analysis. This study proposes an intelligent vulnerability localization method based on fine-grained program representation and feature fusion. Firstly, we generate efficient fine-grained program representations of the program. This involves transforming the source code into intermediate code. We use abstract syntax tree characteristics to correspond to the points of interest of the intermediate code. We slice the intermediate code file based on the point of interest and program dependency relationships. Subsequently, we use the word2vec model to the vectorization of the intermediate code slices. Then, we propose a vulnerability localization framework based on a feature fusion method, which can better combine the advantages of bidirectional gate recurrent unit and convolutional neural network to capture the syntax and semantics of program representation. Through comparing different program representations, we have discovered that the fine-grained representation based on intermediate code in this study provides a more accurate portrayal of program semantics. By comparing various methods, the proposed feature fusion approach in this paper improves vulnerability localization. We also conducted a visualization display of vulnerability localization. Furthermore, we have validated the effectiveness of this method in localizing vulnerabilities across five common vulnerability types.},
  archive      = {J_COMJNL},
  author       = {Zhu, Chenguang and Wei, Renzheng and Chen, Liwei and Wu, Tongshuai and Du, Gewangzi and Shi, Gang},
  doi          = {10.1093/comjnl/bxae041},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {2749-2762},
  shortjournal = {Comput. J.},
  title        = {Vulnerability localization based on intermediate code representation and feature fusion},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A certificateless linearly homomorphic signature scheme
based on lattice for network coding. <em>COMJNL</em>, <em>67</em>(9),
2739–2748. (<a href="https://doi.org/10.1093/comjnl/bxae040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Homomorphic signature is an extremely important public key authentication technique for network coding to defend against pollution attacks. However, there are many problems with previous homomorphic signature schemes which require key escrow, cannot resist malicious key generation center (KGC), and are insecure in the post-quantum era. Therefore, we propose a lattice-based certificateless linearly homomorphic signature scheme. In our scheme, certificateless structure can avoid key escrow and malicious KGC. The lattice structure ensures that our scheme is secure in the post-quantum era. The bimodal Gaussian distribution is used to improve the security and the efficiency. Compared with the previous schemes, our scheme has smaller storage space (no key escrow), can avoid malicious KGC, is more secure in the post-quantum era, and has higher signature efficiency. At the same time, our scheme is more suitable for network coding. Finally, under random oracle model, we proved that our scheme is weakly context hiding and existentially unforgeable against adaptive chosen message attacks against external attackers and the internal KGC.},
  archive      = {J_COMJNL},
  author       = {Dong, Songshou and Yao, Yanqing and Zhou, Yihua and Yang, Yuguang},
  doi          = {10.1093/comjnl/bxae040},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {2739-2748},
  shortjournal = {Comput. J.},
  title        = {A certificateless linearly homomorphic signature scheme based on lattice for network coding},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anomaly metrics on class variations for face anti-spoofing.
<em>COMJNL</em>, <em>67</em>(9), 2725–2738. (<a
href="https://doi.org/10.1093/comjnl/bxae039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In face anti-spoofing tasks, distinguishing between live and spoof faces across different data domains presents challenges due to inter-class similarities, intra-class variations and unknown spoof patterns. This hampers generalization in real-world applications. To address this, we propose a novel convolutional neural network framework that utilizes spatial-frequency cues for 2D and 3D attacks. Furthermore, we introduce compact anomaly metrics and design three anomaly metrics-based supervisions from the perspective of Reed-Xiaoli anomaly detection, aiming to tackle the challenge posed by unknown attacks. Thanks to our proposed spatial frequency factorization network and its frequency-related supervisions, the spoofing cues are significantly enhanced, resulting in remarkable improvements in our experimental results. These outcomes demonstrate that our proposed framework achieves state-of-the-art performance on both monocular and multi-spectral benchmark datasets.},
  archive      = {J_COMJNL},
  author       = {Liu, Weihua and Gong, Bing and Che, Kai and Ma, Jieming and Pan, Yushan},
  doi          = {10.1093/comjnl/bxae039},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {2725-2738},
  shortjournal = {Comput. J.},
  title        = {Anomaly metrics on class variations for face anti-spoofing},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Thematic editorial: Edge computing, fog computing, and
internet of things. <em>COMJNL</em>, <em>67</em>(9), 2721–2724. (<a
href="https://doi.org/10.1093/comjnl/bxae097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  author       = {Fernández Anta, Antonio},
  doi          = {10.1093/comjnl/bxae097},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {2721-2724},
  shortjournal = {Comput. J.},
  title        = {Thematic editorial: Edge computing, fog computing, and internet of things},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Failing to hash into supersingular isogeny graphs.
<em>COMJNL</em>, <em>67</em>(8), 2702–2719. (<a
href="https://doi.org/10.1093/comjnl/bxae038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important open problem in supersingular isogeny-based cryptography is to produce, without a trusted authority, concrete examples of ‘hard supersingular curves’ that is equations for supersingular curves for which computing the endomorphism ring is as difficult as it is for random supersingular curves. A related open problem is to produce a hash function to the vertices of the supersingular |$\ell $| -isogeny graph, which does not reveal the endomorphism ring, or a path to a curve of known endomorphism ring. Such a hash function would open up interesting cryptographic applications. In this paper, we document a number of (thus far) failed attempts to solve this problem, in the hope that we may spur further research, and shed light on the challenges and obstacles to this endeavour. The mathematical approaches contained in this article include: (i) iterative root-finding for the supersingular polynomial; (ii) gcd’s of specialized modular polynomials; (iii) using division polynomials to create small systems of equations; (iv) taking random walks in the isogeny graph of abelian surfaces, and applying Kummer surfaces and (v) using quantum random walks.},
  archive      = {J_COMJNL},
  author       = {Booher, Jeremy and Bowden, Ross and Doliskani, Javad and Boris Fouotsa, Tako and Galbraith, Steven D and Kunzweiler, Sabrina and Merz, Simon-Philipp and Petit, Christophe and Smith, Benjamin and Stange, Katherine E and Ti, Yan Bo and Vincent, Christelle and Voloch, José Felipe and Weitkämper, Charlotte and Zobernig, Lukas},
  doi          = {10.1093/comjnl/bxae038},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {2702-2719},
  shortjournal = {Comput. J.},
  title        = {Failing to hash into supersingular isogeny graphs},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Tor trace in images: A novel multi-tab website
fingerprinting attack with object detection. <em>COMJNL</em>,
<em>67</em>(8), 2690–2701. (<a
href="https://doi.org/10.1093/comjnl/bxae037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Website fingerprinting (WF) attacks on Tor enable a passive adversary to predict the encrypted web browsing activity of a victim by matching the eavesdropped traffic with pretrained classifiers. Nowadays, deep learning-based methods have led to significant achievements in the single-tab Tor WF attack. However, the practical implementation of these single-tab methods is challenging due to most real-world Tor traffic involving multiple tabs. Existing single-tab methods hardly identify multi-tab WF due to the overlapping areas of the traffic trace confusing the original features. In this paper, we propose a Trace Image-based Object Detection model named TIOD as a novel multi-tab attacking model. Specifically, we model the traffic overlap in Tor as the object overlap in object detection tasks from the computer vision field. Besides, we propose a special S-matrix scheme to convert a trace into an image: (i) Retaining the original features by keeping the direction and order of the cells and (ii) Bringing cells of the same page closer together in space. We then utilize a specially designed object detection model for trace images, WF R-CNN, to extract features and identify potential destination websites within multi-tab traces. Comparative analysis with other multi-tab attacks is conducted, and the empirical results consistently underscore the superior performance of the proposed trace image model across diverse datasets. In the two-tab setting, TIOD achieves the best accuracy with more than 85% on both the first and second tabs.},
  archive      = {J_COMJNL},
  author       = {Xu, Yifan and Wang, Liangmin and Chen, Jie and Zhou, Qiang},
  doi          = {10.1093/comjnl/bxae037},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {2690-2701},
  shortjournal = {Comput. J.},
  title        = {Tor trace in images: A novel multi-tab website fingerprinting attack with object detection},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prediction of student performance using random forest
combined with naïve bayes. <em>COMJNL</em>, <em>67</em>(8), 2677–2689.
(<a href="https://doi.org/10.1093/comjnl/bxae036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random forest is a powerful ensemble learning technique celebrated for its heightened predictive performance and robustness in handling complex datasets; nevertheless, it is criticized for its computational expense, particularly with a large number of trees in the ensemble. Moreover, the model’s interpretability diminishes as the ensemble’s complexity increases, presenting challenges in understanding the decision-making process. Although various pruning techniques have been proposed by researchers to tackle these issues, achieving a consensus on the optimal strategy across diverse datasets remains elusive. In response to these challenges, this paper introduces an innovative machine learning algorithm that integrates random forest with Naïve Bayes to predict student performance. The proposed method employs the Naïve Bayes formula to evaluate random forest branches, classifying data by prioritizing branches based on importance and assigning each example to a single branch for classification. The algorithm is utilized on two sets of student data and is evaluated against seven alternative machine-learning algorithms. The results confirm its strong performance, characterized by a minimal number of branches.},
  archive      = {J_COMJNL},
  author       = {Manzali, Youness and Akhiat, Yassine and Abdoulaye Barry, Khalidou and Akachar, Elyazid and El Far, Mohamed},
  doi          = {10.1093/comjnl/bxae036},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {2677-2689},
  shortjournal = {Comput. J.},
  title        = {Prediction of student performance using random forest combined with naïve bayes},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy-preserving breast cancer prediction based on
logistic regression. <em>COMJNL</em>, <em>67</em>(8), 2667–2676. (<a
href="https://doi.org/10.1093/comjnl/bxae035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing strain on today’s healthcare resources, there is a growing demand for pre-diagnosis testing. In response, researchers have suggested diverse machine learning models for disease prediction, among which logistic regression stands out as one of the most effective models. Its objective is to enhance the accuracy and efficiency of pre-diagnosis testing, thereby alleviating the burden on healthcare resources. However, when multiple medical institutions collaborate to train models, the untrusted cloud server may pose a risk of private data leakage, enabling participants to steal data from one another. Existing privacy-preserving methods often suffer from drawbacks such as high communication costs, long training times and lack of security proofs. Therefore, it is imperative to jointly train an excellent model collaboratively and uphold data privacy. In this paper, we develop a highly optimized two-party logistic regression algorithm based on CKKS scheme. The algorithm optimizes ciphertext operations by employing ciphertext segmentation and minimizing the multiplication depth, resulting in time savings. Furthermore, it utilizes least squares to approximate sigmoid functions within specific intervals that cannot be handled by homomorphic encryption. Finally, the proposed algorithm is evaluated on a breast cancer dataset, and simulation experiments demonstrate that the model’s prediction accuracy, after machine learning training, exceeds 96% for two-sided encrypted data.},
  archive      = {J_COMJNL},
  author       = {Chen, Shuangquan and Li, Jinguo and Zhang, Kai and Di, Aoran and Lu, Mengli},
  doi          = {10.1093/comjnl/bxae035},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {2667-2676},
  shortjournal = {Comput. J.},
  title        = {Privacy-preserving breast cancer prediction based on logistic regression},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hardware addition over finite fields based on
booth–karatsuba algorithm. <em>COMJNL</em>, <em>67</em>(8), 2643–2666.
(<a href="https://doi.org/10.1093/comjnl/bxae034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two algorithms, both based around multiplication, one defined by Andrew Donald Booth in 1950 and the other defined by Anatoly Alexeevitch Karatsuba in 1960 can be applied to other types of operations. We know from recent results that to perform some algebraic transformations it is more efficient to calculate an inverse effect of a smaller magnitude together with an original action with a higher rank, than the initial operation, reaching the final element with less transitions. In this paper, we present an addition algorithm on |$\mathbb {Z}/m\mathbb {Z}$| of big-integer numbers based on these concepts, using an alternative to traditional hardware implementation for binary addition based on FULL-ADDER cells, allowing the reduction of space complexity compared with other techniques, such as carry-lookahead, letting us calculate a modular addition in an optimal order complexity of |$\mathcal {O}(n)$| without adding more complexity due to reduction operations.},
  archive      = {J_COMJNL},
  author       = {Perez, J Ayuso},
  doi          = {10.1093/comjnl/bxae034},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {2643-2666},
  shortjournal = {Comput. J.},
  title        = {Hardware addition over finite fields based on Booth–Karatsuba algorithm},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning model for tamil part-of-speech tagging.
<em>COMJNL</em>, <em>67</em>(8), 2633–2642. (<a
href="https://doi.org/10.1093/comjnl/bxae033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Part-of-Speech (POS) tagging is one of the popular Natural Language Processing (NLP) tasks. It is also considered to be a preliminary task for other NLP applications such as speech recognition, machine translation, and sentiment analysis. A few works have been published on POS tagging for the Tamil language. However, the performance of the POS tagger with unknown words is not explored in the literature. The appearance of unknown words is a frequently occurring problem in POS tagging and makes it a challenging task. In this paper, we propose a deep learning-based POS tagger for Tamil using Bi-directional Long Short Term Memory (BLSTM). The performance of the POS tagger was evaluated using known and unknown words. The POS tagger with regular word-level embeddings produces 99.83 and 92.46% accuracies for all known and 63.21% unknown words. It clearly shows that the accuracy decreases when the number of unknown words increases. To improve the performance of the POS tagger with unknown words, the proposed BLSTM model that uses word-level, character-level and pre-trained word embeddings. Test results of this model show a 2.57% improvement for 63.21% of unknown words, with an accuracy of 95.03%.},
  archive      = {J_COMJNL},
  author       = {Visuwalingam, Hemakasiny and Sakuntharaj, Ratnasingam and Alawatugoda, Janaka and Ragel, Roshan},
  doi          = {10.1093/comjnl/bxae033},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {2633-2642},
  shortjournal = {Comput. J.},
  title        = {Deep learning model for tamil part-of-speech tagging},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GNN-based multimodal named entity recognition.
<em>COMJNL</em>, <em>67</em>(8), 2622–2632. (<a
href="https://doi.org/10.1093/comjnl/bxae030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Multimodal Named Entity Recognition (MNER) task enhances the text representations and improves the accuracy and robustness of named entity recognition by leveraging visual information from images. However, previous methods have two limitations: (i) the semantic mismatch between text and image modalities makes it challenging to establish accurate internal connections between words and visual representations. Besides, the limited number of characters in social media posts leads to semantic and contextual ambiguity, further exacerbating the semantic mismatch between modalities. (ii) Existing methods employ cross-modal attention mechanisms to facilitate interaction and fusion between different modalities, overlooking fine-grained correspondences between semantic units of text and images. To alleviate these issues, we propose a graph neural network approach for MNER (GNN-MNER), which promotes fine-grained alignment and interaction between semantic units of different modalities. Specifically, to mitigate the issue of semantic mismatch between modalities, we construct corresponding graph structures for text and images, and leverage graph convolutional networks to augment text and visual representations. For the second issue, we propose a multimodal interaction graph to explicitly represent the fine-grained semantic correspondences between text and visual objects. Based on this graph, we implement deep-level feature fusion between modalities utilizing graph attention networks. Compared with existing methods, our approach is the first to extend graph deep learning throughout the MNER task. Extensive experiments on the Twitter multimodal datasets validate the effectiveness of our GNN-MNER.},
  archive      = {J_COMJNL},
  author       = {Gong, Yunchao and Lv, Xueqiang and Yuan, Zhu and You, Xindong and Hu, Feng and Chen, Yuzhong},
  doi          = {10.1093/comjnl/bxae030},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {2622-2632},
  shortjournal = {Comput. J.},
  title        = {GNN-based multimodal named entity recognition},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-domain recommendation to cold-start users via
categorized preference transfer. <em>COMJNL</em>, <em>67</em>(8),
2610–2621. (<a href="https://doi.org/10.1093/comjnl/bxae029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing cross-domain recommendation (CDR) systems apply the embedding and mapping idea to tackle the cold-start user problem and, to this end, they learn a common bridge function to transfer the user preferences from the source domain into the target domain. However, sharing a bridge function for all users inevitably leads to biased recommendations. This paper proposes a novel method, named CDR to cold-start users via categorized preference transfer (CDRCPT), to overcome the shortcomings of existing approaches. First, the embeddings of users and items in both the source and target domain are learned through pretraining and we utilize preference encoder to obtain the preference embeddings of users in the source domain. Second, mini-batch clustering is applied in the source domain to group users according to their preferences; here, each cluster identifies a specific class of users, and each cluster is represented by its center. Finally, the general representation is fed into a meta network to learn a bridge function for each available class of users. Experiments on two real data sets show that our CDRCPT method is effective in improving the accuracy and robustness of recommendations.},
  archive      = {J_COMJNL},
  author       = {Liu, Xiaoyang and Fu, Xiaoyang and De Meo, Pasquale and Fiumara, Giacomo},
  doi          = {10.1093/comjnl/bxae029},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {2610-2621},
  shortjournal = {Comput. J.},
  title        = {Cross-domain recommendation to cold-start users via categorized preference transfer},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human activity recognition based on video summarization and
deep convolutional neural network. <em>COMJNL</em>, <em>67</em>(8),
2601–2609. (<a href="https://doi.org/10.1093/comjnl/bxae028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this technological era, human activity recognition (HAR) plays a significant role in several applications like surveillance, health services, Internet of Things, etc. Recent advancements in deep learning and video summarization have motivated us to integrate these techniques for HAR. This paper introduces a computationally efficient HAR technique based on a deep learning framework, which works well in realistic and multi-view environments. Deep convolutional neural networks (DCNNs) normally suffer from different constraints, including data size dependencies, computational complexity, overfitting, training challenges and vanishing gradients. Additionally, with the use of advanced mobile vision devices, the demand for computationally efficient HAR algorithms with the requirement of limited computational resources is high. To address these issues, we used integration of DCNN with video summarization using keyframes. The proposed technique offers a solution that enhances performance with efficient resource utilization. For this, first, we designed a lightweight and computationally efficient deep learning architecture based on the concept of identity skip connections (features reusability), which preserves the gradient loss attenuation and can handle the enormous complexity of activity classes. Subsequently, we employed an efficient keyframe extraction technique to minimize redundancy and succinctly encapsulate the entire video content in a lesser number of frames. To evaluate the efficacy of the proposed method, we performed the experimentation on several publicly available datasets. The performance of the proposed method is measured in terms of evaluation parameters Precision, Recall, F-Measure and Classification Accuracy. The experimental results demonstrated the superiority of the presented algorithm over other existing state-of-the-art methods.},
  archive      = {J_COMJNL},
  author       = {Kushwaha, Arati and Khare, Manish and Bommisetty, Reddy Mounika and Khare, Ashish},
  doi          = {10.1093/comjnl/bxae028},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {2601-2609},
  shortjournal = {Comput. J.},
  title        = {Human activity recognition based on video summarization and deep convolutional neural network},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Liveness attacks on HotStuff: The vulnerability of timer
doubling mechanism. <em>COMJNL</em>, <em>67</em>(8), 2586–2600. (<a
href="https://doi.org/10.1093/comjnl/bxae027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Byzantine fault-tolerant (BFT) consensus protocols are essential in distributed computing. Most partially synchronous BFT protocols proceed in views and rely on a view synchronizer module to guarantee liveness by synchronizing honest replicas to the same view. HotStuff is a leading BFT consensus protocol known for achieving linear view change and optimistic responsiveness . To achieve these desirable properties, HotStuff relies on a candidate solution for the view synchronizer based on a recomposed timer doubling mechanism. However, a formal analysis of this mechanism is currently lacking. This paper delves into HotStuff with the recomposed timer doubling mechanism. To facilitate accurate analysis, we introduce a new specification for the view synchronizer, incorporating two paths for view switching as in HotStuff’s setting. Surprisingly, we observe that the adversary can disrupt the view synchronization and launch a liveness attack, stalling the confirmation process. Besides, the adversary can further recover or control the confirmation process at will . A repairment that retains the desirable feature of HotStuff is also presented. We simulate the liveness attack and the repairment, demonstrating their effectiveness. Specifically, the liveness attack can cause HotStuff’s throughput to drop and remain at 0. When equipped with our repairment, HotStuff can resist the attack and retain the throughput performance.},
  archive      = {J_COMJNL},
  author       = {Guo, Kaiwen and Hu, Kexin and Zhang, Zhenfeng},
  doi          = {10.1093/comjnl/bxae027},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {2586-2600},
  shortjournal = {Comput. J.},
  title        = {Liveness attacks on HotStuff: The vulnerability of timer doubling mechanism},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SDTA: Secure decentralized trading alliance for electronic
medical data. <em>COMJNL</em>, <em>67</em>(8), 2573–2585. (<a
href="https://doi.org/10.1093/comjnl/bxae026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Massive medical data are indispensable for training diagnostic models to provide high-quality health monitoring services. The methods for sharing data in existing works involve securely and essentially copying data but often overlook the integration and efficiency of data storage, exchange and application. In this paper, we propose a Secure Decentralized Trading Alliance (SDTA) to encompass the entire process holistically. With monetary incentives, we formulate a chain-net structure for recording data digests and authentic transactions, thereby transforming data sharing into data trading without duplicating data storage. Data privacy is promised by encryption. To manage and employ encrypted medical data, users can update and search their encrypted data using an index and keywords, subsequently retrieving data within the SDTA framework. It is realized by a novel dynamic searchable symmetric encryption (SSE) with an |$l$| -level access strategy, which confines users to data pertinent solely to them, thus circumventing unnecessary data leakage. We scrutinize the storage efficiency and prove the fairness and security of SDTA. Finally, we generate datasets of varying sizes, where the time required to search for a single keyword is approximately 0.04 s with 1 000 000 (keyword, identifier) pairs, showing it quite acceptable.},
  archive      = {J_COMJNL},
  author       = {Zhang, Xi and Su, Ye and Qin, Jing and Sun, Jiameng},
  doi          = {10.1093/comjnl/bxae026},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {2573-2585},
  shortjournal = {Comput. J.},
  title        = {SDTA: Secure decentralized trading alliance for electronic medical data},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Software failure prediction based on program state and
first-error characteristics. <em>COMJNL</em>, <em>67</em>(8), 2559–2572.
(<a href="https://doi.org/10.1093/comjnl/bxae025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating program reliability against software faults involves the work of analyzing program failure against software faults as behavior on failures is fundamental for assessing reliability. Similar failure behavior reflects similar reliability assessment against software faults of same time–space distribution. Since program failure is determined by fault manifestations, working on similarities on fault manifestations helps figuring out failures in similarity as well. In this paper, we propose a novel method to characterize program behavior by defining fine-grained runtime states at assembly-level of code, aiming to capture the very first abnormal manifestation—first-error—after software fault being activated during program run. Failure prediction model and measurement is presented based on similarities reflected by the runtime behaviors obtained. Fault injection experiments are conducted to verify the prediction measurement by utilizing software faults of Orthogonal Defect Classification to inject and MiBench programs to perform. Over 15 000+ times of fault injection by considering type and location of fault were conducted into each program in order to quantify and analyze the sensitivity on first-error and similarity on failure accordingly. The results show that programs having particular structural features (e.g. massive operations with regards to calculations, nested function-calls, case structures and loop structures, etc.) are well characterized toward first-error behaviors by extracting fine-grained states. Similarities on first-error sensitivity can be represented better for these programs as well. Same trend is seen on failure behavior and its prediction measurement.},
  archive      = {J_COMJNL},
  author       = {Zhu, Lina and Zhang, Zuochang},
  doi          = {10.1093/comjnl/bxae025},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {2559-2572},
  shortjournal = {Comput. J.},
  title        = {Software failure prediction based on program state and first-error characteristics},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning-based cyberbullying detection in kurdish
language. <em>COMJNL</em>, <em>67</em>(7), 2548–2558. (<a
href="https://doi.org/10.1093/comjnl/bxae024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyberbullying is a significant concern in this digital age due to its harmful effects on individuals and society. Sadly, social media platforms have only exacerbated the problem, making it imperative to find effective ways to identify and prevent offensive content. While previous research has extensively focused on English and explored machine learning techniques to tackle this issue. To address this gap, this paper introduces a new hybrid deep learning model called Gray Wolf Algorithm-convolutional neural network (GWA-CNN), explicitly designed to detect cyberbullying in the Kurdish language on Twitter. The proposed model combines the CNN framework with an optimised GWA version to improve CNN’s parameters and reduce training time. We evaluated GWA-CNN thoroughly utilizing the first-ever manually annotated Kurdish dataset of 30k tweets that have been meticulously curated and divided into three categories, namely sexism, racism and neutral expressions, and compared its performance to those of state-of-the-art algorithms such as Naïve Bayes, K-Nearest Neighbors, Recurrent Neural Networks, Gated Recurrent Units and attention-based transformer. The experimental results demonstrate that GWA-CNN exhibited superior performance in all scenarios, outperforming other approaches in detecting cyberbullying on Twitter.},
  archive      = {J_COMJNL},
  author       = {Badawi, Soran},
  doi          = {10.1093/comjnl/bxae024},
  journal      = {The Computer Journal},
  month        = {7},
  number       = {7},
  pages        = {2548-2558},
  shortjournal = {Comput. J.},
  title        = {Deep learning-based cyberbullying detection in kurdish language},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Concept drift–based intrusion detection for evolving data
stream classification in IDS: Approaches and comparative study.
<em>COMJNL</em>, <em>67</em>(7), 2529–2547. (<a
href="https://doi.org/10.1093/comjnl/bxae023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Static machine and deep learning algorithms are commonly used in intrusion detection systems (IDSs). However, their effectiveness is constrained by the evolving data distribution and the obsolescence of the static data sources used for model training. Consequently, static classifiers lose efficacy, necessitating expensive model retraining with time. The aim is to develop a dynamic and adaptable IDS that mitigates the limitations of static models, ensuring real-time threat detection and reducing the need for frequent, resource-intensive model retraining. This research proposes an approach that amalgamates the adaptive random forest (ARF) classifier with Hoeffding’s bounds and a moving average test for the early and accurate detection of network intrusions. The ARF can adapt in real time to shifting network conditions and evolving attack patterns, constantly refining its intrusion detection capabilities. Furthermore, the inclusion of Hoeffding’s bounds and the moving average test adds a dimension of statistical rigor to the system, facilitating the timely recognition of concept drift and distinguishing benign network variations from potential intrusions. The synergy of these techniques results in reduced false positives and false negatives, thereby enhancing the overall detection rate. The proposed method delivers outstanding results, with 99.95% accuracy and an impressive 99.96% recall rate on the latest CIC-IDS 2018 dataset, outperforming the results of existing approaches.},
  archive      = {J_COMJNL},
  author       = {Seth, Sugandh and Chahal, Kuljit Kaur and Singh, Gurvinder},
  doi          = {10.1093/comjnl/bxae023},
  journal      = {The Computer Journal},
  month        = {7},
  number       = {7},
  pages        = {2529-2547},
  shortjournal = {Comput. J.},
  title        = {Concept Drift–Based intrusion detection for evolving data stream classification in IDS: Approaches and comparative study},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An improved density peaks clustering algorithm based on
density ratio. <em>COMJNL</em>, <em>67</em>(7), 2515–2528. (<a
href="https://doi.org/10.1093/comjnl/bxae022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Density peaks clustering (DPC) is a relatively new density clustering algorithm. It is based on the idea that cluster centers always have relatively high local densities and are relatively far from the points with higher densities. With the aforementioned idea, a decision graph can be drawn, and cluster centers will be chosen easily with the aid of the decision graph. However, the algorithm has its own weaknesses. Because the algorithm calculates local density and allocates points based on the distances between certain points, the algorithm has difficulty in classifying points into proper groups with varying densities or nested structures. This paper proposes an improved density peaks clustering algorithm called Dratio-DPC to overcome this weakness. First, Dratio-DPC adjusts the original local density with a coefficient calculated with the density ratio. Second, Dratio-DPC takes density similarity into consideration to calculate the distances between one point and other points with higher local densities. We design and perform experiments on different benchmark datasets and compare the clustering results of Dratio-DPC, traditional clustering algorithms and three improved DPC algorithms. Comparison results show that Dratio-DPC is effective and applicable to a wider range of scenarios.},
  archive      = {J_COMJNL},
  author       = {Zou, Yujuan and Wang, Zhijian and Xu, Pengfei and Lv, Taizhi},
  doi          = {10.1093/comjnl/bxae022},
  journal      = {The Computer Journal},
  month        = {7},
  number       = {7},
  pages        = {2515-2528},
  shortjournal = {Comput. J.},
  title        = {An improved density peaks clustering algorithm based on density ratio},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ELSM: Evidence-based line segment merging. <em>COMJNL</em>,
<em>67</em>(7), 2498–2514. (<a
href="https://doi.org/10.1093/comjnl/bxae021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing line segment detectors break perceptually contiguous linear structures into multiple line segments. This can be offset by re-merging the segments, but existing merging algorithms over-merge and produce globally incorrect segments. Geometric cues are necessary but not sufficient for deciding whether to merge two segments or not. By restricting the result of any merging decision to have underlying image support, we reduce over-merging and globally incorrect segments. We propose a novel measure for evaluating merged segments based on line segment Hausdorff distance. On images from YorkUrbanDB, we show that our algorithm improves both qualitative and quantitative results obtained from four existing line segment detection methods and is better than two existing line segment merging methods. Our method does not suffer from inconsistent results produced by four recent deep learning-based models. The method is easily customisable to work for line drawings such as hand-drawn maps to obtain vectorized representations.},
  archive      = {J_COMJNL},
  author       = {Hamid, Naila and Khan, Nazar and Akram, Arbish},
  doi          = {10.1093/comjnl/bxae021},
  journal      = {The Computer Journal},
  month        = {7},
  number       = {7},
  pages        = {2498-2514},
  shortjournal = {Comput. J.},
  title        = {ELSM: Evidence-based line segment merging},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BotGSL: Twitter bot detection with graph structure learning.
<em>COMJNL</em>, <em>67</em>(7), 2486–2497. (<a
href="https://doi.org/10.1093/comjnl/bxae020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Twitter bot detection is an important and meaningful task. Existing methods can be bypassed by the latest bots that disguise themselves as genuine users and evade detection by mimicking them. These methods also fail to leverage the clustering tendencies of users, which is the most important feature for detecting bots at the community level. Moreover, they neglect the implicit relations between users that contain crucial clues for detection. Furthermore, the user relation graphs, which are essential for graph-based methods, may be unreliable due to noise and incompleteness in datasets. To address these issues, a bot detection framework with graph structure learning is proposed. The framework constructs a heterogeneous graph with users and their relations, extracts multiple features to characterise user intent and establishes a feature similarity graph using metric learning. Implicit relations are discovered to derive an implicit relation graph. Additionally, a semantic relation graph is generated by aggregating relation semantics among users. The graphs are then fused and embedded into a Graph Transformer for training with partially known user labels. The framework demonstrated a 91.92% average detection accuracy on three real-world benchmark, outperforming state-of-the-art methods, while also showcasing the effectiveness and necessity of each module.},
  archive      = {J_COMJNL},
  author       = {Wei, Chuancheng and Liang, Gang and Yan, Kexiang},
  doi          = {10.1093/comjnl/bxae020},
  journal      = {The Computer Journal},
  month        = {7},
  number       = {7},
  pages        = {2486-2497},
  shortjournal = {Comput. J.},
  title        = {BotGSL: Twitter bot detection with graph structure learning},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Service function placement optimization for cloud service
with end-to-end delay constraints. <em>COMJNL</em>, <em>67</em>(7),
2473–2485. (<a href="https://doi.org/10.1093/comjnl/bxae019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network function virtualization (NFV) has been proposed to enable flexible management and deployment of the network service in cloud. In NFV architecture, a network service needs to invoke several service functions (SFs) in a particular order following the service chain function. The placement of SFs has significant impact on the performance of network services. However, stochastic nature of the network service arrivals and departures as well as meeting the end-to-end Quality of Service(QoS) makes the SFs placement problem even more challenging. In this paper, we firstly provide a system architecture for the SFs placement of cloud service with end-to-end QoS deadline. We then formulate the end-to-end service placement as a Markov decision process (MDP) which aims to minimize the placement cost and the end-to-end delay. In our MDP, the end-to-end delay of active services in the network is considered to be the state of the system, and the placement ( nonplacement or placement ) of SF is considered as the action. Also, we discuss the rationality of our analytical model by analyzing the Markov stochastic property of the end-to-end service placement. To obtain the optimal placement policy, we then propose an algorithm (Algorithm 1) for dynamic SFs placement based on our model and use successive approximations, i.e. |$\epsilon $| -iteration algorithm (Algorithm 2) to obtain action distribution. Finally, we evaluate the proposed MDP by comparing our optimal method with DDQP, DRL-QOR, MinPath and MinDelay for QoS optimization, including acceptance probability, average delay, resource utilization, load-balancing and reliability.},
  archive      = {J_COMJNL},
  author       = {Yan, Guofeng and Su, Zhengwen and Tan, Hengliang and Du, Jiao},
  doi          = {10.1093/comjnl/bxae019},
  journal      = {The Computer Journal},
  month        = {7},
  number       = {7},
  pages        = {2473-2485},
  shortjournal = {Comput. J.},
  title        = {Service function placement optimization for cloud service with end-to-end delay constraints},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hybrid BERT-CNN approach for depression detection on
social media using multimodal data. <em>COMJNL</em>, <em>67</em>(7),
2453–2472. (<a href="https://doi.org/10.1093/comjnl/bxae018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the absence of early facilities, a large population is dealing with stress, anxiety, and depression issues, which may have disastrous consequences, including suicide. Past studies revealed a direct relationship between the high engagement with social media and the increasing depression rate. This research initially creates a dataset with text, emoticons and image data, and then preprocessing is performed using diverse techniques. The proposed model in the research consists of three parts: first is textual bidirectional encoder representations from transformers (BERT), which is trained on only text data and also emoticons are converted into a textual form for easy processing; second is convolutional neural network (CNN), which is trained only on image data; and the third is the combination of best-performing models, i.e. hybrid of BERT and CNN (BERT-CNN), to work on both the text and images with enhanced accuracy. The results show the best accuracy with BERT, i.e. 97% for text data; for image data, CNN has attained the highest accuracy of 89%. Finally, the hybrid approach is compared with other combinations and previous studies; it achieved the best accuracy of 99% in the categorization of users into depressive and non-depressive based on multimodal data.},
  archive      = {J_COMJNL},
  author       = {Beniwal, Rohit and Saraswat, Pavi},
  doi          = {10.1093/comjnl/bxae018},
  journal      = {The Computer Journal},
  month        = {7},
  number       = {7},
  pages        = {2453-2472},
  shortjournal = {Comput. J.},
  title        = {A hybrid BERT-CNN approach for depression detection on social media using multimodal data},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CM-UTC: A cost-sensitive matrix based method for unknown
encrypted traffic classification. <em>COMJNL</em>, <em>67</em>(7),
2441–2452. (<a href="https://doi.org/10.1093/comjnl/bxae017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been widely adopted in the field of network traffic classification due to its unique advantages in handling encrypted network traffic. However, most existing deep learning models can only classify known encrypted traffic that has been sampled and labeled. In this paper, we propose CM-UTC, a cost-sensitive matrix-based method for classifying unknown encrypted traffic. CM-UTC explores the probability distribution of the DNN output layer to filter out the unknown classes and further designs a cost-sensitive matrix to address the class imbalance problem. Additionally, we propose the utilization of the Harris Hawk optimization algorithm to modify the model parameters and improve its performance. The experiments are validated on two different datasets, and the results demonstrate that CM-UTC not only outperforms existing methods in terms of overall performance but also exhibits superior capability in correctly identifying samples from the minority class.},
  archive      = {J_COMJNL},
  author       = {Gao, Zhiyuan and Li, Jinguo and Wang, Liangliang and He, Yin and Yuan, Peichun},
  doi          = {10.1093/comjnl/bxae017},
  journal      = {The Computer Journal},
  month        = {7},
  number       = {7},
  pages        = {2441-2452},
  shortjournal = {Comput. J.},
  title        = {CM-UTC: A cost-sensitive matrix based method for unknown encrypted traffic classification},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pattern models: A dynamic epistemic logic for distributed
systems. <em>COMJNL</em>, <em>67</em>(7), 2421–2440. (<a
href="https://doi.org/10.1093/comjnl/bxae016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce pattern models , a dynamic epistemic logic for analyzing distributed systems. First, we present a version of pattern models where the full-information protocol , widely studied in distributed computability, is static in the product definition of pattern models. Next, we parametrize such a logic so as to add the capability to model dynamics of arbitrary deterministic protocols. We thus give a systematic construction of pattern models for a large variety of distributed-computing models called dynamic-network models . Using pattern models, the epistemic dynamics of a proper subclass of dynamic-network models called oblivious can be described using a static pattern model, hence using constant space. For this case, we present a sufficient unsolvability condition for the consensus task that can be easily verified analyzing the structure of the initial epistemic model and the pattern model for a given oblivious dynamic-network model.},
  archive      = {J_COMJNL},
  author       = {Castañeda, Armando and van Ditmarsch, Hans and Rosenblueth, David A and Velázquez, Diego A},
  doi          = {10.1093/comjnl/bxae016},
  journal      = {The Computer Journal},
  month        = {7},
  number       = {7},
  pages        = {2421-2440},
  shortjournal = {Comput. J.},
  title        = {Pattern models: A dynamic epistemic logic for distributed systems},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Borehole depth recognition based on improved YOLOX
detection. <em>COMJNL</em>, <em>67</em>(7), 2408–2420. (<a
href="https://doi.org/10.1093/comjnl/bxae015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a method for recognizing the drill depth in low-light underground environments, with the aim of addressing the issues of low efficiency and susceptibility to manual changes in the current methods. The method is based on an improved You Only Look Once X model. Initially, image data undergo enhancement and annotation. Secondly, it incorporates an attention mechanism to improve the feature extraction capability. The feature pyramid is utilized to minimize feature loss and facilitate better multi-scale feature fusion. Additionally, the loss function is optimized to enhance the localization ability of the prediction box. The enhanced model achieves an accuracy of 91.3 |$\%$|⁠ , representing a 4.4 |$\%$| increase compared to the pre-improvement performance, and demonstrates improved positioning accuracy. Successful drilling depth measurements were carried out with the acquired positioning information.},
  archive      = {J_COMJNL},
  author       = {Ren, Dawei and Meng, Lingwei and Wang, Rui},
  doi          = {10.1093/comjnl/bxae015},
  journal      = {The Computer Journal},
  month        = {7},
  number       = {7},
  pages        = {2408-2420},
  shortjournal = {Comput. J.},
  title        = {Borehole depth recognition based on improved YOLOX detection},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Thematic editorial: Sentiment analysis. <em>COMJNL</em>,
<em>67</em>(7), 2403–2407. (<a
href="https://doi.org/10.1093/comjnl/bxae061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  author       = {Manolopoulos, Yannis},
  doi          = {10.1093/comjnl/bxae061},
  journal      = {The Computer Journal},
  month        = {7},
  number       = {7},
  pages        = {2403-2407},
  shortjournal = {Comput. J.},
  title        = {Thematic editorial: Sentiment analysis},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SEDD: Robust blind image watermarking with single encoder
and dual decoders. <em>COMJNL</em>, <em>67</em>(6), 2390–2402. (<a
href="https://doi.org/10.1093/comjnl/bxae014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind image watermarking is regarded as a vital technology to provide copyright of digital images. Due to the rapid growth of deep neural networks, deep learning-based watermarking methods have been widely studied. However, most existing methods which adopt simple embedding and extraction structures cannot fully utilize the image features. In this paper, we propose a novel Single-Encoder-Dual-Decoder (SEDD) watermarking architecture to achieve high imperceptibility and strong robustness. Precisely, the single encoder utilizes normalizing flow to realize watermark embedding, which can effectively fuse the watermark and cover image. For watermark extraction, we introduce a parallel dual-decoder to improve the imperceptibility and extracting ability. Extensive experiments demonstrate that better watermark robustness and imperceptibility are obtained by SEDD architecture. Our method achieves a bit error rate less than 0.1% under most attacks such as JPEG compression, Gaussian blur and crop. Besides, the proposed method also obtains strong robustness under combined attacks and social platform processing.},
  archive      = {J_COMJNL},
  author       = {Xiang, Yuyuan and Wang, Hongxia and Yang, Ling and He, Mingze and Zhang, Fei},
  doi          = {10.1093/comjnl/bxae014},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2390-2402},
  shortjournal = {Comput. J.},
  title        = {SEDD: Robust blind image watermarking with single encoder and dual decoders},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discrete-time quantum walks community detection in
multi-domain networks. <em>COMJNL</em>, <em>67</em>(6), 2379–2389. (<a
href="https://doi.org/10.1093/comjnl/bxae013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of detecting communities in real-world networks has been extensively studied in the past, but most of the existing approaches work on single-domain networks , i.e. they consider only one type of relationship between nodes. Single-domain networks may contain noisy edges and they may lack some important information. Thus, some authors have proposed to consider the multiple relationships that connect the nodes of a network, thus obtaining multi-domain networks . However, most community detection approaches are limited to multi-layer networks , i.e. networks generated from the superposition of several single-domain networks (called layers ) that are regarded as independent of each other. In addition to being computationally expensive, multi-layer approaches might yield inaccurate results because they ignore potential dependencies between layers. This paper proposes a multi-domain discrete-time quantum walks (MDQW) model for multi-domain networks. First, the walking space of network nodes in multi-domain network is constructed. Second, the quantum permutation circuit of the coin state is designed based on the coded particle state. Then, using different coin states, the shift operator performs several quantum walks on the particles. Finally, the corresponding update rule is selected to move the node according to the measurement result of the quantum state. With continuous update iteration, the shift operator automatically optimizes the discovered community structure. We experimentally compared our MDQW method with four state-of-the-art competitors on five real datasets. We used the normalized mutual information (NMI) to compare clustering quality, and we report an increase in NMI of up to 3.51 of our MDQW method in comparison with the second-best performing competitor. The MDQW method is much faster than its competitors, allowing us to conclude that MDQW is a useful tool in the analysis of large real-life multi-domain networks. Finally, we illustrate the usefulness of our approach on two real-world case studies.},
  archive      = {J_COMJNL},
  author       = {Liu, Xiaoyang and Ding, Nan and Wu, Yudie and Fiumara, Giacomo and De Meo, Pasquale},
  doi          = {10.1093/comjnl/bxae013},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2379-2389},
  shortjournal = {Comput. J.},
  title        = {Discrete-time quantum walks community detection in multi-domain networks},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EFTA: An efficient and fault-tolerant data aggregation
scheme without TTP in smart grid. <em>COMJNL</em>, <em>67</em>(6),
2368–2378. (<a href="https://doi.org/10.1093/comjnl/bxae012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid construction and implementation of smart grid, lots of studies have been conducted to explore how to ensure the security of information privacy. At present, most privacy-preserving data aggregation schemes in smart grid achieve privacy data protection through homomorphically encrypted data aggregation. However, these data aggregation schemes tend to rely on a trusted third party (TTP), and fail to efficiently handle the case of a meter failure. Besides, they are less flexible for overall user management, and resistance to collusion attacks needs to be improved. In this paper, we propose an efficient and robust privacy-preserving data aggregation scheme without TTP, called EFTA. Overall, the scheme eliminates the reliance on a TTP, combines with Shamir threshold secret sharing scheme to increase overall fault tolerance, supports flexible and dynamic user management, and effectively defends against entity initiated collusion attacks. According to security and performance analysis results, the scheme proposed in this paper meets the multiple security requirements of smart grid, and is more efficient in terms of overall overhead compared to the existing privacy-preserving data aggregation schemes.},
  archive      = {J_COMJNL},
  author       = {Mei, Xianyun and Wang, Liangliang and Qin, Baodong and Zhang, Kai and Long, Yu},
  doi          = {10.1093/comjnl/bxae012},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2368-2378},
  shortjournal = {Comput. J.},
  title        = {EFTA: An efficient and fault-tolerant data aggregation scheme without TTP in smart grid},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual intrusion detection based on CBAM-capsule networks.
<em>COMJNL</em>, <em>67</em>(6), 2357–2367. (<a
href="https://doi.org/10.1093/comjnl/bxae011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrusion detection has become a research focus in internet information security, with deep learning algorithms playing a crucial role in its development. Typically, intrusion detection data are transformed into a two-dimensional matrix by segmenting, stacking and padding them with zeros for input into deep learning models. However, this method consumes computational resources and fails to consider the correlation between features. In this paper, we transform the data into images through visualization operations and propose an information entropy weighted scheme to optimize the collision element problem during the transformation process. This method enhances the correlation between pixel frame features, leading to approximately 2% improvement in accuracy of the classification model when using the generated image samples for detection in experiments. To address the issues of insensitivity to target feature locations and incomplete feature extraction in traditional neural networks, this paper introduces a new network model called CBAM-CapsNet, which combines the advantages of the lightweight Convolutional Block Attention Module and capsule networks. Experimental results on the UNSW-NB15 and IDS-2017 datasets demonstrate that the proposed model achieves accuracies of 92.94% and 99.72%, respectively. The F1 scores obtained are 91.83% and 99.56%, indicating a high level of detection.},
  archive      = {J_COMJNL},
  author       = {Yang, Zhongjun and Huang, Qing and Wang, Qi and Zong, Xuejun and Ao, Ran},
  doi          = {10.1093/comjnl/bxae011},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2357-2367},
  shortjournal = {Comput. J.},
  title        = {Visual intrusion detection based on CBAM-capsule networks},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LiteMixer: Cauliflower disease diagnosis based on a novel
lightweight neural network. <em>COMJNL</em>, <em>67</em>(6), 2346–2356.
(<a href="https://doi.org/10.1093/comjnl/bxae010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cauliflower, a globally cultivated and nutritionally rich crop, confronts significant challenges in quality and yield due to the rising prevalence of diseases. Traditional manual detection methods, suitable for empiricists or plant pathologists, prove inefficient. Furthermore, existing automated disease identification methods in cauliflower often neglect crucial computational performance metrics within computer vision algorithms, such as complexity, inference speed and training time. This study introduces LiteMixer, a novel lightweight model designed to address these challenges. The Lightweight Mixed-Domain Feature Extraction module (LMFE) meticulously captures global image features, followed by a maximum pooling layer that downscales the resulting multidimensional feature matrix. The Plug-and-Play Multi-Scale Lightweight Convolutional Attention Fusion module (MLCAF) integrates multichannel spatial features, connecting to fully connected layers for the final classification. Ablation experiments highlight the effectiveness of the LMFE module coupled with the MLCAF module. Comparative analyses against state-of-the-art and other lightweight models demonstrate LiteMixer achieving the highest accuracy in identifying cauliflower diseases at 99.86%. Notably, LiteMixer exhibits optimal computational performance, featuring minimal storage costs (4.02M) and the lowest parameter count, resulting in cost-effective computational expenses (16.78M). LiteMixer also boasts the fastest inference time (4.69 ms) and the shortest training time (865 s). This study positions LiteMixer as an advanced solution for diagnosing cauliflower leaf diseases in agricultural settings, underscoring its efficacy and practicality in overcoming the unique challenges associated with cauliflower disease detection within the realm of computer vision algorithms.},
  archive      = {J_COMJNL},
  author       = {Zhong, Yi and Teng, Zihan and Tong, Mengjun},
  doi          = {10.1093/comjnl/bxae010},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2346-2356},
  shortjournal = {Comput. J.},
  title        = {LiteMixer: Cauliflower disease diagnosis based on a novel lightweight neural network},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint alignment networks for few-shot website fingerprinting
attack. <em>COMJNL</em>, <em>67</em>(6), 2331–2345. (<a
href="https://doi.org/10.1093/comjnl/bxae009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Website fingerprinting (WF) attacks based on deep neural networks pose a significant threat to the privacy of anonymous network users. However, training a deep WF model requires many labeled traces, which can be labor-intensive and time-consuming, and models trained on the originally collected traces cannot be directly used for the classification of newly collected traces due to the concept drift caused by the time gap in the data collection. Few-shot WF attacks are proposed for using the originally and few-shot newly collected labeled traces to facilitate anonymous trace classification. However, existing few-shot WF attacks ignore the fine-grained feature alignment to eliminate the concept drift in the model training, which fails to fully use the knowledge of labeled traces. We propose a novel few-shot WF attack called Joint Alignment Networks (JAN), which conducts fine-grained feature alignment at both semantic-level and feature-level. Specifically, JAN minimizes a distribution distance between originally and newly collected traces in the feature space for feature-level alignment, and utilizes two task-specific classifiers to detect unaligned traces and force these traces mapped within decision boundaries for semantic-level alignment. Extensive experiments on public datasets show that JAN outperforms the state-of-the-art few-shot WF methods, especially in the difficult 1-shot tasks.},
  archive      = {J_COMJNL},
  author       = {Zhou, Qiang and Wang, Liangmin and Zhu, Huijuan and Lu, Tong and Song, Heping},
  doi          = {10.1093/comjnl/bxae009},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2331-2345},
  shortjournal = {Comput. J.},
  title        = {Joint alignment networks for few-shot website fingerprinting attack},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An intelligent security system using enhanced anomaly-based
detection scheme. <em>COMJNL</em>, <em>67</em>(6), 2317–2330. (<a
href="https://doi.org/10.1093/comjnl/bxae008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring the security of computer networks is of utmost importance, and intrusion detection plays a vital role in safeguarding these systems. Traditional intrusion detection systems (IDSs) often suffer from drawbacks like reliance on outdated rules and centralized architectures, limiting their performance in the face of evolving threats and large-scale data networks. To address these challenges, we present an advanced anomaly detection-based IDS that utilizes a decentralized communicative multi-agent reinforcement learning (MARL). In our approach, multiple reinforcement learning agents collaborate in intrusion detection, effectively mitigating the non-stationarity problem and introducing a specialized secure communication method. We further enhance the learning process by incorporating external knowledge. Our approach is evaluated through extensive experiments conducted on the benchmark NSL Knowledge Discovery and Data Mining dataset. These experiments encompass diverse scenarios, involving varying numbers of agents to prove scalability feature. The results underscore the effectiveness of our method, which surpasses the performance of existing state-of-the-art solutions based on MARL, achieving a high accuracy rate of 97.80%.},
  archive      = {J_COMJNL},
  author       = {Louati, Faten and Barika Ktata, Farah and Amous, Ikram},
  doi          = {10.1093/comjnl/bxae008},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2317-2330},
  shortjournal = {Comput. J.},
  title        = {An intelligent security system using enhanced anomaly-based detection scheme},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Color patterns and enhanced texture learning for detecting
computer-generated images. <em>COMJNL</em>, <em>67</em>(6), 2303–2316.
(<a href="https://doi.org/10.1093/comjnl/bxae007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detection of computer-generated (CG) images can reveal the authenticity and originality of digital images. However, recent cutting-edge image generation methods make it very difficult to distinguish CG images from natural photographs. In this paper, a novel method based on color patterns and enhanced texture learning is proposed to tackle this problem. We designed and implemented the backbone network with a separation-fusion learning strategy by constructing a multi-branch neural network. The luminance and chrominance patterns in dual-color spaces (RGB and YCbCr) are leveraged to achieve a robust representation of image differences. A channel-spatial attention module and a global texture enhancement module are also integrated into a backbone network to enhance the learning of inherent traces. Experiments on several commonly used benchmark datasets and a newly constructed dataset with more realistic and diverse images demonstrate that the proposed algorithm outperforms state-of-the-art competitors by a large margin.},
  archive      = {J_COMJNL},
  author       = {Xu, Qiang and Xu, Dongmei and Wang, Hao and Yuan, Jianye and Wang, Zhe},
  doi          = {10.1093/comjnl/bxae007},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2303-2316},
  shortjournal = {Comput. J.},
  title        = {Color patterns and enhanced texture learning for detecting computer-generated images},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Link residual closeness of graphs with fixed parameters.
<em>COMJNL</em>, <em>67</em>(6), 2286–2302. (<a
href="https://doi.org/10.1093/comjnl/bxae006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link residual closeness is a newly proposed measure for network vulnerability. In this model, vertices are perfectly reliable and the links fail independently of each other. It measures the vulnerability even when the removal of links does not disconnect the graph. In this paper, we characterize those graphs that maximize the link residual closeness over the connected graphs with fixed order and one additional parameter such as connectivity, edge connectivity, bipartiteness, independence number, matching number, chromatic number, number of cut vertices and number of cut edges.},
  archive      = {J_COMJNL},
  author       = {Xu, Leyou and Li, Chengli and Zhou, Bo},
  doi          = {10.1093/comjnl/bxae006},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2286-2302},
  shortjournal = {Comput. J.},
  title        = {Link residual closeness of graphs with fixed parameters},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A light weight depthwise separable layer optimized CNN
architecture for object-based forgery detection in surveillance videos.
<em>COMJNL</em>, <em>67</em>(6), 2270–2285. (<a
href="https://doi.org/10.1093/comjnl/bxae005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present era is at the peak of technological advancement in image and video processing techniques, with user-friendly accessible tools/techniques. This immersive technology development makes video forensics enormously challenging. Specifically, the passive approaches to object-based forgeries in videos are crucial for legal and judicial matters. Hence, to ensure the integrity of the videos, a scientific, statistical and passive investigation of videos is required to maintain the spatial and temporal information content. This paper aims to develop a passive approach for digging out the forgery traces by applying the motion residue windowing technique for object removal forgery in surveillance videos. The novel max averaging windowing techniques improve visual imprints of the object removal forgery in the videos from the existing methods in the literature. A deep learning approach is the next step for achieving forgery detection in surveillance videos. The proposed lightweight depth-separable layer-optimized CNN has fast execution speed, optimized in terms of parameters without compromising the desired accuracy. This network is trained at a frame level with 98.60% testing accuracy, followed by a pipeline architecture of the proposed model for detection of forgery at video level with 99.01% accuracy. The suggested model works better than current models regarding post-processing operations, compression rates, forged video detection accuracy, precision, recall and F1 score.},
  archive      = {J_COMJNL},
  author       = {Sandhya, Sandhya and Kashyap, Abhishek},
  doi          = {10.1093/comjnl/bxae005},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2270-2285},
  shortjournal = {Comput. J.},
  title        = {A light weight depthwise separable layer optimized CNN architecture for object-based forgery detection in surveillance videos},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing aspect category detection through hybridised
contextualised neural language models: A case study in multi-label text
classification. <em>COMJNL</em>, <em>67</em>(6), 2257–2269. (<a
href="https://doi.org/10.1093/comjnl/bxae004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the field of Natural Language Processing (NLP) has made significant progress with the evolution of Contextualised Neural Language Models (CNLMs) and the emergence of large LMs. Traditional and static language models exhibit limitations in tasks demanding contextual comprehension due to their reliance on fixed representations. CNLMs such as BERT and Semantic Folding aim to produce feature-rich representations by considering a broader linguistic context. In this paper, Deep Learning-based Aspect Category Detection approaches are introduced to perform text classification. The study extensively assesses classification model performance, emphasising enhanced representativeness and optimised feature extraction resolution using CNLMs and their hybridised variants. The effectiveness of the proposed approaches is evaluated on benchmark datasets of 4500 reviews from the laptop and restaurant domains. The results show that the proposed approaches using hybridised CNLMs outperform state-of-the-art methods with an f-score of 0.85 for the laptop and f-scores higher than 0.90 for the restaurant dataset. This study represents a pioneering work as one of the initial research efforts aiming to jointly evaluate the representation performance of CNLMs with different architectures to determine their classification capabilities. The findings indicate that the proposed approaches can enable the development of more effective classification models in various NLP tasks.},
  archive      = {J_COMJNL},
  author       = {Karaoglan, Kursat Mustafa and Findik, Oguz},
  doi          = {10.1093/comjnl/bxae004},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2257-2269},
  shortjournal = {Comput. J.},
  title        = {Enhancing aspect category detection through hybridised contextualised neural language models: A case study in multi-label text classification},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CNN-LSTM base station traffic prediction based on dual
attention mechanism and timing application. <em>COMJNL</em>,
<em>67</em>(6), 2246–2256. (<a
href="https://doi.org/10.1093/comjnl/bxae003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy consumption in 5G base stations remains consistently high, even during periods of low traffic loads, thereby resulting in unnecessary inefficiencies. To address this problem, this paper presents a novel approach by proposing a convolutional neural network (CNN)-long short-term memory (LSTM) traffic prediction model with a dual attention mechanism, coupled with the particle swarm optimization k-means algorithm for intelligent switch timing. The proposed CNN-LSTM model leverages a dual channel attention mechanism to bolster key feature information for long-term traffic data predictions. Specifically, a temporal attention mechanism is added to the LSTM to enhance the importance of temporal information. Moreover, the particle swarm optimization K-Means algorithm is proposed in order to cluster the traffic prediction results, output the corresponding time points of the lower traffic value and to obtain the optimal switch-off periods of the base station. Extensive experiments across multiple base stations over an extended period of time have validated our approach. The results show that this method offers accurate traffic prediction with minimal average errors in traffic prediction and the on/off timings of the base stations are in line with the “tide effect” of traffic, thereby achieving the goal of energy savings.},
  archive      = {J_COMJNL},
  author       = {Jia, Hairong and Wang, Suying and Ren, Zelong},
  doi          = {10.1093/comjnl/bxae003},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2246-2256},
  shortjournal = {Comput. J.},
  title        = {CNN-LSTM base station traffic prediction based on dual attention mechanism and timing application},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal sentiment analysis based on composite
hierarchical fusion. <em>COMJNL</em>, <em>67</em>(6), 2230–2245. (<a
href="https://doi.org/10.1093/comjnl/bxae002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of multimodal sentiment analysis, it is an important research task to fully extract modal features and perform efficient fusion. In response to the problems of insufficient semantic information and poor cross-modal fusion effect of traditional sentiment classification models, this paper proposes a composite hierarchical feature fusion method combined with prior knowledge. Firstly, the ALBERT (A Lite BERT) model and the improved ResNet model are constructed for feature extraction of text and image, respectively, and high-dimensional feature vectors are obtained. Secondly, to solve the problem of insufficient semantic information expression in cross-scene, a prior knowledge enhancement model is proposed to enrich the data characteristics of each modality. Finally, to solve the problem of poor cross-modal fusion effect, a composite hierarchical fusion model is proposed, which combines the temporal convolutional network and the attention mechanism to fuse the sequence features of each modality information and realizes the information interaction between different modalities. Experiments on MVSA-Single and MVSA-Multi datasets show that the proposed model is superior to a series of comparison models and has good adaptability in new scenarios.},
  archive      = {J_COMJNL},
  author       = {Lei, Yu and Qu, Keshuai and Zhao, Yifan and Han, Qing and Wang, Xuguang},
  doi          = {10.1093/comjnl/bxae002},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2230-2245},
  shortjournal = {Comput. J.},
  title        = {Multimodal sentiment analysis based on composite hierarchical fusion},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A general blockchain-based automatic audit scheme for proofs
of retrievability. <em>COMJNL</em>, <em>67</em>(6), 2219–2229. (<a
href="https://doi.org/10.1093/comjnl/bxae001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud storage has been widely used in remote data management, although correct storage of the outsourced file is still challenging in practice. Proofs of Retrievability (PoRs), a storage-oriented cryptographic tool, support integrity checking and efficient retrieval of the file. However, due to the lack of a fully credible oversight mechanism or a serious dependence on a trusted third party, most PoRs are incapable of achieving essential and straightforward trust between participants (i.e. the client and server). While blockchain shows promise in solving this trust issue, existing blockchain-based storage systems are scenario-constrained as they require private/permissioned or special-construct blockchains. Consequently, none of these systems provide robust and decentralized trustworthiness. We propose a general Blockchain-based Automatic Audit (BAA) scheme for PoR without limitations based on specific blockchain types. Specifically, we present BAA via stitching together a carefully designed or chosen array of sub-components such as storage proofs and Turing-complete smart contracts. We also integrate BAA with specific PoR models to prove its strong generality and availability. To our best knowledge, our proposal is the first blockchain-based approach that enhances traditional PoR models with both automatic audit and fair payment. The final analysis and implemented prototype on Ethereum demonstrate the utility of BAA.},
  archive      = {J_COMJNL},
  author       = {Chen, Xiuyuan and Lin, Chao and Wu, Wei and He, Debiao},
  doi          = {10.1093/comjnl/bxae001},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2219-2229},
  shortjournal = {Comput. J.},
  title        = {A general blockchain-based automatic audit scheme for proofs of retrievability},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CFAuditChain: Audit BlockChain based on cuckoo filter.
<em>COMJNL</em>, <em>67</em>(6), 2208–2218. (<a
href="https://doi.org/10.1093/comjnl/bxad133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Log blockchain can be used to ensure the integrity of log data. However, current methods are facing the problems of throughput mismatch and rough audit granularity. Packaging multiple logs to generate integrity proofs improves throughput but reduces audit granularity, and the auditor can only locate the tampering log packet instead of specific records. This paper proposes CFAuditChain, an audit blockchain based on a cuckoo filter, where the proof of existence (PoE) of the single log is calculated while calculating the integrity proof of packet logs. The PoE is saved in the cuckoo filter and stored using blockchain for immutability. Therefore, the auditor can verify the credibility of each log based on the filter when the integrity proof of a log packet does not match. The theoretical analysis and experimental results show that CFAuditChain provides the granularity of audit logs down to the records level at an acceptable cost.},
  archive      = {J_COMJNL},
  author       = {Liu, Kang and Lu, Yang and Tan, Shiyi and Liang, Wei and Sun, Huiping and Chen, Zhong},
  doi          = {10.1093/comjnl/bxad133},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2208-2218},
  shortjournal = {Comput. J.},
  title        = {CFAuditChain: Audit BlockChain based on cuckoo filter},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Partitioned 2D set-pruning segment trees with compressed
buckets for multi-dimensional packet classification. <em>COMJNL</em>,
<em>67</em>(6), 2189–2207. (<a
href="https://doi.org/10.1093/comjnl/bxad132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-dimensional packet classification is one of the most important functions to support various services in next generation routers. Both the memory-efficient data structure to support larger rule tables and the hardware architecture to achieve a higher throughput are desired. In this paper, we propose a parallel and pipelined architecture called Set-Pruning Segment Trees with Buckets (SPSTwB) for multi-dimensional packet classification. SPSTwB significantly reduces rule duplication based on a novel partitioning scheme and an efficient bucket merging scheme. The key feature of our proposed architecture is that memory consumption is reduced significantly regardless of the characteristics of various rule tables. In addition, the logic complexity of each pipeline stage is simplified by not storing rule IDs and priorities and thus it can run at a high clock rate. The proposed scheme needs less than 20 bytes per rule for various 100 K rule tables generated by ClassBench. In addition, the proposed scheme supports fast incremental rule update. The proposed pipelined architecture can achieve a throughput of 134 Gbps from the implementation on Xilinx Virtex-7 FPGA device with dual-ported Block RAM.},
  archive      = {J_COMJNL},
  author       = {Chang, Yeim-Kuan and Chen, Hsin-Mao},
  doi          = {10.1093/comjnl/bxad132},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2189-2207},
  shortjournal = {Comput. J.},
  title        = {Partitioned 2D set-pruning segment trees with compressed buckets for multi-dimensional packet classification},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid ITÖ algorithm for maximum scatter colored traveling
salesman problem. <em>COMJNL</em>, <em>67</em>(6), 2172–2188. (<a
href="https://doi.org/10.1093/comjnl/bxad131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research presents a new problem maximum scatter colored traveling salesman problem (MSCTSP), the objective of MSCTSP is to find Hamiltonian cycles with the minimal edge as max as possible, it is used to simulate the real-world applications of network and transport. Since MSCTSP has been proved to be a NP-hard problem, population-based algorithms can be used for solving it. However, the performances are not satisfactory. Thus, it is necessary to develop novel algorithms to obtain high quality feasible solution. Based on the above reason, the paper proposes a novel hybrid ITÖ (HITÖ) algorithm, which integrates the two new strategies: crossover operator and mutation strategy, into the standard ITÖ. In the iteration course of HITÖ, the dual-chromosome coding is used to code a feasible solution of MSCTSP, and the stochastic drift and volatility operators are used to explore and exploit new unknown region. During the process, drift operator is performed by crossover operator, volatility operator is carried out by mutation strategy, and they are both affected by activity intensity of particles which functionally depends on the radius and temperature. Experiments display HITÖ shows an improvement over comparative algorithms on solution quality.},
  archive      = {J_COMJNL},
  author       = {Dong, Xueshi and Lin, Qing and Wang, Wei},
  doi          = {10.1093/comjnl/bxad131},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2172-2188},
  shortjournal = {Comput. J.},
  title        = {Hybrid ITÖ algorithm for maximum scatter colored traveling salesman problem},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Video hashing with tensor robust PCA and histogram of
optical flow for copy detection. <em>COMJNL</em>, <em>67</em>(6),
2162–2171. (<a href="https://doi.org/10.1093/comjnl/bxad130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel video hashing with tensor robust Principal Component Analysis (PCA) and Histogram of Optical Flow (HOF) for copy detection. In the proposed hashing, a video is divided into some video groups. For each video group, a low-rank secondary frame is constructed from the low-rank component decomposed by applying tensor robust PCA to the video group. Since the low-rank component can well indicate spatial-temporal intrinsic structure of the video group and it is slightly disturbed by digital operations, feature extraction from the low-rank secondary frames is discriminative and stable. Next, spatial features and temporal features are extracted from low-rank secondary frames by Charlier moments and HOF, respectively. Since the Charlier moments are robust to geometric transform and they can efficiently distinguish video frames with different contents, the use of Charlier moments can make robust and discriminative spatial features. As the HOF can measure the distribution of motion information between frames, the temporal features formed by HOFs can provide good discrimination. Hash is ultimately determined by quantizing the spatial and temporal features and concatenating the quantized results. Numerous experiments on open video datasets indicate that the proposed hashing is superior to some hashing baseline schemes in terms of classification and copy detection.},
  archive      = {J_COMJNL},
  author       = {Yu, Mengzhu and Tang, Zhenjun and Zhang, Hanyun and Liang, Xiaoping and Zhang, Xianquan},
  doi          = {10.1093/comjnl/bxad130},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2162-2171},
  shortjournal = {Comput. J.},
  title        = {Video hashing with tensor robust PCA and histogram of optical flow for copy detection},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From stars to diamonds: Counting and listing almost complete
subgraphs in large networks. <em>COMJNL</em>, <em>67</em>(6), 2151–2161.
(<a href="https://doi.org/10.1093/comjnl/bxad129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Listing dense subgraphs is a fundamental task with a variety of network analytics applications. A lot of research has been done focusing on |$k$| -cliques, i.e. complete subgraphs on |$k$| nodes. However, requiring complete connectivity between the nodes of a subgraph may be too restrictive in many real applications. Hence, in this paper, we consider a natural relaxation of cliques, called |$k$| -diamonds and defined as cliques of size |$k$| with one missing edge. We first provide a sequential algorithm that, in |$O(nm^{(k-1)/2})$| time, counts and lists all the |$k$| -diamonds in large graphs, for any constant |$k \geq 4$|⁠ . A parallel extension of the sequential algorithm is then proposed and analyzed in a MapReduce-style model, achieving the same local and total space usage of the state-of-the-art algorithms for |$k$| -cliques. The running time is optimal on dense graphs and |$O(\sqrt{m})$| larger than |$k$| -clique counting if the graph is sparse. Our algorithms compute induced diamonds by analyzing the structure of directed stars formed by the graph nodes and their neighbors.},
  archive      = {J_COMJNL},
  author       = {Finocchi, Irene and Garcia, Renan Leon and Sinaimeri, Blerina},
  doi          = {10.1093/comjnl/bxad129},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2151-2161},
  shortjournal = {Comput. J.},
  title        = {From stars to diamonds: Counting and listing almost complete subgraphs in large networks},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Influence maximization in social networks using an improved
multi-objective particle swarm optimization. <em>COMJNL</em>,
<em>67</em>(6), 2137–2150. (<a
href="https://doi.org/10.1093/comjnl/bxad128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The influence maximization (IM) problem has received great attention in the field of social network analysis, and its analysis results can provide reliable basis for decision makers when promoting products or political viewpoints. IM problem aims to select a set of seed users from social networks and maximize the number of users expected to be influenced. Most previous studies on the IM problem focused only on the single-objective problem of maximizing the influence spread of the seed set, ignoring the cost of the seed set, which causes decision makers to be unable to develop effective management strategies. In this work, the IM problem is formulated as a multi-objective IM problem that considers the cost of the seed set. An improved multi-objective particle swarm optimization (IMOPSO) algorithm is proposed to solve this problem. In the IMOPSO algorithm, the initialization strategy of Levy flight based on degree value is used to improve the quality of the initial solution, and the local search strategy based on greedy mechanism is designed to improve the Pareto Frontier distribution and promote algorithm convergence. Experimental results on six real social networks demonstrate that the proposed IMOPSO algorithm is effective, reducing runtime while providing competitive solutions.},
  archive      = {J_COMJNL},
  author       = {Wang, Ping and Zhang, Ruisheng},
  doi          = {10.1093/comjnl/bxad128},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2137-2150},
  shortjournal = {Comput. J.},
  title        = {Influence maximization in social networks using an improved multi-objective particle swarm optimization},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ensemble of deep features for breast cancer
histopathological image classification. <em>COMJNL</em>, <em>67</em>(6),
2126–2136. (<a href="https://doi.org/10.1093/comjnl/bxad127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysis of histopathological images (HIs) is crucial for detecting breast cancer (BR). However, because they vary, it is still very difficult to extract well-designed elements. Deep learning (DL) is a recent development that is used to extract high-level features. However, DL techniques continue to confront several difficult problems, such as the need for sufficient training data for DL models, which reduces the classification findings. In this study, an ensemble deep transfer convolutional neural network is presented to address this problem. The pre-trained models (ResNet50 and MobileNet) are employed to extract high-level features by freezing the front layer parameters while fine-tuning the last layers. In the proposed ensemble framework, KNN, SVM, logistic regression and neural networks are used as base classifiers. The majority vote and product approaches are used to integrate the predictions of each separate classifier. In the benchmark BreaKHis dataset, the suggested ensemble model is compared to some current approaches. It demonstrates that while the ensemble model obtains a considerable accuracy of 97.72% for the multiclass classification test, it achieves an accuracy of 99.2% for the binary task. The suggested ensemble model’s effectiveness in extracting useful features for BR images is demonstrated by comparison with existing cutting-edge models.},
  archive      = {J_COMJNL},
  author       = {Atwan, Jaffar and Almansour, Nedaa and Hashem Ryalat, Mohammad and Sahran, Shahnorbanun and Aldabbas, Hamza and Albashish, Dheeb},
  doi          = {10.1093/comjnl/bxad127},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2126-2136},
  shortjournal = {Comput. J.},
  title        = {Ensemble of deep features for breast cancer histopathological image classification},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Menger-type connectivity of line graphs of generalized
hypercubes with faulty edges. <em>COMJNL</em>, <em>67</em>(6),
2118–2125. (<a href="https://doi.org/10.1093/comjnl/bxad126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A connected graph |$G$| is called strongly Menger edge connected if |$G$| has min{deg |$_{G}(x)$|⁠ , deg |$_{G}(y)$| } edge-disjoint paths between any two distinct vertices |$x$| and |$y$| in |$G$|⁠ . In this paper, we consider two types of the strongly Menger edge connectivity of the line graphs of generalized |$n$| -dimensional hypercubes with faulty edges, namely the |$m$| -edge-fault-tolerant and |$m$| -conditional edge-fault-tolerant strongly Menger edge connectivity. We show that the line graphs of all generalized |$n$| -dimensional hypercubes are |$(2n-4)$| -edge-fault-tolerant strongly Menger edge connected for |$n\geq 3$| and |$(4n-10)$| -conditional edge-fault-tolerant strongly Menger edge connected for |$n\geq 4$|⁠ . The two bounds for the maximum numbers of faulty edges are best possible.},
  archive      = {J_COMJNL},
  author       = {Jia, Huanshen and Qian, Jianguo},
  doi          = {10.1093/comjnl/bxad126},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2118-2125},
  shortjournal = {Comput. J.},
  title        = {Menger-type connectivity of line graphs of generalized hypercubes with faulty edges},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WiPoTS: An application layer protocol with network protocol
stack enhancements for wireless power transfer networks.
<em>COMJNL</em>, <em>67</em>(6), 2108–2117. (<a
href="https://doi.org/10.1093/comjnl/bxad125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, a far-field wireless power transfer (WPT) system aims to deliver wireless power over a distance of a few meters. Communication among devices for the purpose of WPT in the far-field WPT system is unique as its purpose is to establish, maintain and monitor a WPT session among devices in the system. For proper functionality of a WPT system, a number of communication-, control- and management-related challenges need to be addressed. Hence, here an application layer protocol specifically designed for a WPT system is presented. The protocol provides essential control, management and communication functionalities to establish, maintain and monitor a WPT session. Some features of a WPT system require low-latency communication, hence enhancements for a networking protocol stack are also proposed. To validate the effectiveness of the presented application layer protocol along with the enhancements proposed for the networking protocol stack, a set of experiments was carried out over a real WPT system. The experimental results demonstrate that the proposed protocol possesses the features essential for a WPT system, and the proposed enhancements for the networking protocol stack provide low-latency communication along with lower control overhead compared with an existing state-of-the-art network protocol stack.},
  archive      = {J_COMJNL},
  author       = {Omer Farooq, Muhammad},
  doi          = {10.1093/comjnl/bxad125},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2108-2117},
  shortjournal = {Comput. J.},
  title        = {WiPoTS: An application layer protocol with network protocol stack enhancements for wireless power transfer networks},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Laws of timed state machines. <em>COMJNL</em>,
<em>67</em>(6), 2066–2107. (<a
href="https://doi.org/10.1093/comjnl/bxad124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State machines are widely used in industry and academia to capture behavioural models of control. They are included in popular notations, such as UML and its variants, and used (sometimes informally) to describe computational artefacts. In this paper, we present laws for state machines that we prove sound with respect to a process algebraic semantics for refinement, and complete, in that they are sufficient to reduce an arbitrary model to a normal form that isolates basic (action and control) elements. We consider two variants of UML-like state machines, both enriched with facilities to deal with time budgets, timeouts and deadlines over triggers and actions. In the first variant, machines are self-contained components, declaring all the variables, events and operations that they require or define. In contrast, in the second variant, machines are open, like in UML for instance. Laws for open state machines do not depend on a specific context of variables, events and operations, and normalization uses a novel operator for open-machine (de)composition. Our laws can be used in behaviour-preservation transformation techniques. Their applications are automated by a model-transformation engine.},
  archive      = {J_COMJNL},
  author       = {Cavalcanti, Ana and Conserva Filho, Madiel and Ribeiro, Pedro and Sampaio, Augusto},
  doi          = {10.1093/comjnl/bxad124},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2066-2107},
  shortjournal = {Comput. J.},
  title        = {Laws of timed state machines},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detection of e-commerce fraud review via self-paced graph
contrast learning. <em>COMJNL</em>, <em>67</em>(6), 2054–2065. (<a
href="https://doi.org/10.1093/comjnl/bxad123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, graph neural networks (GNNs) have been widely used for e-commerce review fraud detection by aggregating the neighborhood information of nodes in various relationships to highlight the suspiciousness of nodes. However, existing GNN-based detection methods are susceptible to sample class imbalance and fraud camouflage problems, resulting in poor quality of constructed graph structures and inability to learn reliable node embeddings. To address the above problems, we propose a novel e-commerce review fraud detection method based on self-paced graph contrast learning (SPCL-GNN). Firstly, the method constructs a subgraph by initially selecting nodes through a labeled balanced extractor. Secondly, the subgraph connections are filtered and complemented by combining self-paced graph contrast learning and an adaptive neighbor sampler to obtain an optimized graph structure. Again, an attention mechanism is introduced in intra- and inter-relationship aggregation to focus on the importance of aggregation under different relationships. Finally, the quality of the node embedding representation is further improved by maximizing the mutual information between the local and global representations. Experimental results on the Amazon and YelpChi datasets show that SPCL-GNN significantly outperforms the baseline.},
  archive      = {J_COMJNL},
  author       = {Zhao, WeiDong and Liu, XiaoTong},
  doi          = {10.1093/comjnl/bxad123},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2054-2065},
  shortjournal = {Comput. J.},
  title        = {Detection of E-commerce fraud review via self-paced graph contrast learning},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A sketch framework for fast, accurate and fine-grained
analysis of application traffic. <em>COMJNL</em>, <em>67</em>(6),
2039–2053. (<a href="https://doi.org/10.1093/comjnl/bxad122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, with the continuous increase in internet traffic, the demand for real-time and high-speed traffic analysis has grown significantly. However, existing traffic analysis technologies are either limited by specific applications or data, unable to expand for widespread implementation, or in offline mode are unable to keep up with dynamic adjustments required in certain network management scenarios. A promising approach is to utilize sketch technology to enhance real-time traffic analysis. Unfortunately, existing technologies suffer from defects, such as overly coarse-grained statistics that cannot perform precise application-level traffic analysis, and irreversibility, which cannot support real-time queries in a friendly way. To achieve real-time fine-grained application traffic analysis in general scenarios, we propose AppSketch, a real-time network traffic measurement tool. AppSketch adopts a one-pass approach to classify and label the application information of each packet in the network flows. It then hashes the flow, identified with the application tag, into a carefully designed multiple-key sketch, for gathering application-specific statistics. We conducted extensive experiments using a real-world network traffic dataset collected on a university campus. The results showed that AppSketch achieved high accuracy while requiring less update time than other alternatives. Moreover, AppSketch occupies limited memory ( ⁠|$ {\leq }$| 64KB), making it suitable for online network devices.},
  archive      = {J_COMJNL},
  author       = {Hou, Changsheng and Jia, Chunbo and Hou, Bingnan and Zhou, Tongqing and Chen, Yingwen and Cai, Zhiping},
  doi          = {10.1093/comjnl/bxad122},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2039-2053},
  shortjournal = {Comput. J.},
  title        = {A sketch framework for fast, accurate and fine-grained analysis of application traffic},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Strongly menger connectedness of a class of recursive
networks. <em>COMJNL</em>, <em>67</em>(6), 2030–2038. (<a
href="https://doi.org/10.1093/comjnl/bxad121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to Menger’s theorem, connectivity and edge connectivity are closely related to node-disjoint paths and edge-disjoint paths, respectively. Node- and edge-disjoint paths can keep the effective transmission of information and confidentiality. Therefore, node-disjoint paths and edge-disjoint paths are two important parameters to measure the reliability of a network. For a faulty node (resp. edge) set |$S\subset V$| (resp. |$S\subset E$|⁠ ), a connected graph |$G=(V,E)$| is |$S$| -strongly Menger-node-connected (resp. Menger-edge-connected) if any two distinct nodes |$x$| and |$y$| in |$G-S$| are connected by |$\min \{\deg _{G-S}(x),\deg _{G-S}(y)\}$| internally node-disjoint (resp. edge-disjoint) paths in |$G-S$|⁠ , where |$\deg _{G-S}(x)$| and |$\deg _{G-S}(y)$| are the degrees of |$x$| and |$y$| in |$G-S$|⁠ , respectively. And most of the previous studies are based on networks that are triangle-free. In this paper, we consider the strongly Menger (edge) connectedness of a class of |$r$| -dimensional recursive networks RNCG |$G_{r}$| with triangles. Moreover, we show that |$G_{r}$| is |$(rl-l-1)$| -strongly Menger-node-connected. And then we show that |$G_{r}$| is |$[k+(r-1)l-2]$| -strongly Menger-edge-connected of order 1 and |$[2k+2(r-1)l-6]$| -strongly Menger-edge-connected of order 2. Since the class of |$r$| -dimensional recursive networks RNCG |$G_{r}$| includes not only data center networks DCell and generalized DCell but also interconnection network dragonfly, etc., all the results are appropriate for these networks.},
  archive      = {J_COMJNL},
  author       = {Wang, Yihong and Cheng, Baolei and Wang, Yan and Yu, Jia and Fan, Jianxi},
  doi          = {10.1093/comjnl/bxad121},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2030-2038},
  shortjournal = {Comput. J.},
  title        = {Strongly menger connectedness of a class of recursive networks},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Closeness centralities of lollipop graphs. <em>COMJNL</em>,
<em>67</em>(6), 2020–2029. (<a
href="https://doi.org/10.1093/comjnl/bxad120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Closeness is one of the most studied characteristics of networks. Residual closeness is a very sensitive measure of graphs robustness. Additional closeness is a measure of growth potentials of networks. In this article, we calculate the closeness, vertex residual closeness, link residual closeness and additional closeness of lollipop graphs.},
  archive      = {J_COMJNL},
  author       = {Dangalchev, Chavdar},
  doi          = {10.1093/comjnl/bxad120},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2020-2029},
  shortjournal = {Comput. J.},
  title        = {Closeness centralities of lollipop graphs},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Panoptic segmentation with convex object representation.
<em>COMJNL</em>, <em>67</em>(6), 2009–2019. (<a
href="https://doi.org/10.1093/comjnl/bxad119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate representation of objects holds pivotal significance in the realm of panoptic segmentation. Presently, prevalent object representation methodologies, including box-based, keypoint-based and query-based techniques, encounter a challenge known as the ‘representation confusion’ issue in specific scenarios, often resulting in the mislabeling of instances. In response, this paper introduces Convex Object Representation (COR), a straightforward yet highly effective approach to address this problem. COR leverages a CNN-based Euclidean Distance Transform to convert the target instance into a convex heatmap. Simultaneously, it offers a parallel embedding method for encoding the object. Subsequently, COR characterizes objects based on the distinctive embedding vectors of their convex vertices. This paper seamlessly integrates COR into a state-of-the-art query-based panoptic segmentation framework. Experimental findings validate that COR successfully mitigates the representation confusion predicament, enhancing segmentation accuracy. The COR-augmented methods exhibit notable improvements of +1.3 and +0.7 points in PQ on the Cityscapes validation and MS COCO panoptic 2017 validation datasets, respectively.},
  archive      = {J_COMJNL},
  author       = {Yao, Zhicheng and Wang, Sa and Zhu, Jinbin and Bao, Yungang},
  doi          = {10.1093/comjnl/bxad119},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {6},
  pages        = {2009-2019},
  shortjournal = {Comput. J.},
  title        = {Panoptic segmentation with convex object representation},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Correction to: Automatic diagnosis of diabetic retinopathy
from retinal abnormalities: Improved jaya-based feature selection and
recurrent neural network. <em>COMJNL</em>, <em>67</em>(5), 2007. (<a
href="https://doi.org/10.1093/comjnl/bxad108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  doi          = {10.1093/comjnl/bxad108},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {2007},
  shortjournal = {Comput. J.},
  title        = {Correction to: automatic diagnosis of diabetic retinopathy from retinal abnormalities: improved jaya-based feature selection and recurrent neural network},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Clinical dataset classification using feature ranking and
satin bower bird optimized SVMs. <em>COMJNL</em>, <em>67</em>(5),
1993–2006. (<a href="https://doi.org/10.1093/comjnl/bxad118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A clinical decision support system is a computer-based system that is designed to assist healthcare providers with clinical decision-making by analyzing electronic health records and other healthcare information systems to provide real-time support to clinicians at the point of care. A novel classification framework for clinical datasets in which the relevant features are selected by ranking them based on Fisher’s Score and a wrapper-based Satin Bower Bird Optimization algorithm with the combination of accuracy, G-mean and F-Score measured by support vector machine (SVM) as the fitness function is proposed. The classification is performed using an SVM classifier in which the hyperparameters of the SVM classifier are optimized using the Satin Bower Bird Optimization that improves the classification performance. In the context of statistical analysis, the research undergoes a non-parametric Friedman Test. This study selects relevant attributes from three clinical datasets from the Machine Learning Repository maintained by the University of California Irvine and achieved an accuracy of 86% for the Breast Cancer Wisconsin (Diagnostic) dataset, 89% for the Diabetic Retinopathy Debrecen dataset, and 91% for the EEG Eye State dataset respectively. When compared with other machine learning classifiers the proposed approach performed well with feature selection compared with other machine learning classifiers.},
  archive      = {J_COMJNL},
  author       = {K S, Navin and Nehemiah H, Khanna and Jane, Nancy Y and Arputharaj, Kannan},
  doi          = {10.1093/comjnl/bxad118},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1993-2006},
  shortjournal = {Comput. J.},
  title        = {Clinical dataset classification using feature ranking and satin bower bird optimized SVMs},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy-preserving image retrieval with multi-modal query.
<em>COMJNL</em>, <em>67</em>(5), 1979–1992. (<a
href="https://doi.org/10.1093/comjnl/bxad117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ever-growing multi-modal images pose great challenges to local image storage and retrieval systems. Cloud computing provides a solution to large-scale image data storage but suffers from privacy issues and lacks the support for multi-modal image retrieval. To address these, a searchable encryption-empowered privacy-preserving multi-modal image retrieval method is proposed. First, we design a hybrid image retrieval framework that fuses visual features and textual features at a decision level and further supports similar image retrieval and multi-keyword image retrieval. Second, we construct a new hybrid inverted index structure to distinguish high-frequency terms from low-frequency terms and index them through hierarchical index trees and data blocks, respectively, which greatly improves query efficiency. Third, we design a prime encoding-based multi-keyword query method that converts mapping operations in bloom filters into inner product calculations, and further implements secure multi-keyword image query. Experiments against the Baseline schemes are conducted to verify the performance of the scheme in terms of high efficiency.},
  archive      = {J_COMJNL},
  author       = {Zhou, Fucai and Zhang, Zongye and Hou, Ruiwei},
  doi          = {10.1093/comjnl/bxad117},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1979-1992},
  shortjournal = {Comput. J.},
  title        = {Privacy-preserving image retrieval with multi-modal query},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GUI: A geolocation method for unreachable IP.
<em>COMJNL</em>, <em>67</em>(5), 1963–1978. (<a
href="https://doi.org/10.1093/comjnl/bxad116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IP geolocation technology based on network measurement can provide IP geographic location capability without user assistance. In the case that the IP target device or intermediate route device does not respond to the detection message, the existing geolocation methods have reduced the accuracy of city-level geolocation, not to mention that the target cannot be located within the city. To solve this problem, a geolocation method for unreachable IP (called GUI) is proposed. GUI first extracts the target city metropolitan area network (MAN) topology, obtains the path to the IP target and compares it with the MAN topology to achieve the city-level geolocation of the target, and then looks for landmarks in the database that belong to the same /24 subnet and the same city-level location with the target. If the above landmarks exist, GUI uses the coordinate set corresponding to the landmarks to solve the minimum covering circle, so as to judge the location of the target. If the above landmarks do not exist, GUI searches for the intersection nodes from the target detection path and MAN topology, enumerates the connected landmarks, calculates the minimum covering circle according to the coordinates and uses it for the target’s urban internal geolocation. The experimental results of 15 cities in China and the USA show that GUI can effectively geolocate IP targets. The city-level geolocation success rate of reachable IP can reach 99.41%, that of unreachable IP can reach 89.37% and the minimum median error of urban internal unreachable IP geolocation can reach 4.99km. All indicators are significantly better than the existing typical geolocation methods, such as PoP-Based Geolocation, Street-Level Geolocation, Ranking Nodes Based Geolocation, and the existing typical IP geographic location databases such as IPIP and MaxMind.},
  archive      = {J_COMJNL},
  author       = {Zu, Shuodi and Luo, Xiangyang and Wang, Liang and Zhang, Fan},
  doi          = {10.1093/comjnl/bxad116},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1963-1978},
  shortjournal = {Comput. J.},
  title        = {GUI: A geolocation method for unreachable IP},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy-preserving confidential reporting system with
designated reporters. <em>COMJNL</em>, <em>67</em>(5), 1951–1962. (<a
href="https://doi.org/10.1093/comjnl/bxad114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A confidential reporting system (CRS) allows reporters to report concerns or problems in confidence without the fear of blame or reprisals. Nevertheless, privacy has been the primary concern of reporters. In this paper, we propose a privacy-preserving confidential reporting system with designated reporters (PPCRS-DR) to protect the privacy of reporters and the confidentiality of reports. Our PPCRS-DR provides the following interesting features: (1) for an event, an auditor can designate a reporter to report; (2) an auditor can neither see the report nor know the reporter’s identity from an encrypted report if the reporter is not the designated one; (3) when an auditor is unavailable, he/she can temporarily designate a delegatee to collect and review reports on behalf of him/her. We formalize both the definition and security model of our PPCRS-DR, and propose a concrete construction. Furthermore, the security of the proposed PPCRS-DR is formally proven. The implementation shows that it is efficient. The novelty is to implement flexible decryption delegation of CRSs and protect reporters’ privacy.},
  archive      = {J_COMJNL},
  author       = {Han, Jinguang and Susilo, Willy and Chen, Liquan and Lai, Jianchang and Wu, Ge},
  doi          = {10.1093/comjnl/bxad114},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1951-1962},
  shortjournal = {Comput. J.},
  title        = {Privacy-preserving confidential reporting system with designated reporters},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kth min threshold encryption for privacy-preserving data
evaluation. <em>COMJNL</em>, <em>67</em>(5), 1941–1950. (<a
href="https://doi.org/10.1093/comjnl/bxad113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The |$k$| th min threshold is to judge whether the |$k$| th smallest element of an attribute set along with a confidential file is greater than a predefined threshold, which is a fundamental, primitive operation in data evaluation, such as risk evaluation in business investment. However, it will compromise the privacy of the confidential files when proceeding with such a data evaluation because there is often a large amount of sensitive information involved in them, which the organizations/individuals are reluctant to expose due to the risk of losing a competitive advantage. Motivated by the issue how to preserve the privacy of the confidential files during data evaluation, in this research, we first present a new encryption notion called |$k$| th min threshold encryption (KTE) for serving privacy-preserving data evaluation. In this notion, the confidential file will be encrypted under an attribute set for its privacy protection prior to being sent to a receiver, and a decryption key is generated from a threshold |$d$| and a rank |$k$| of element, both selected by the receiver. The decryption will be successful if and only if the |$k$| th smallest element of the attribute set is greater than |$d$|⁠ . We then describe a concrete construction of KTE in the public-key setting. In particular, our construction features optimally short private keys, which only consists of one group element. By virtue of this advantage, it is quite practical because of only two pairing operations for decryption computation.},
  archive      = {J_COMJNL},
  author       = {Chen, Zhenhua and Li, Ting and Xie, Junrui and Li, Ni and Nie, Jingjing},
  doi          = {10.1093/comjnl/bxad113},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1941-1950},
  shortjournal = {Comput. J.},
  title        = {Kth min threshold encryption for privacy-preserving data evaluation},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). API recommendation for mashup creation: A comprehensive
survey. <em>COMJNL</em>, <em>67</em>(5), 1920–1940. (<a
href="https://doi.org/10.1093/comjnl/bxad112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mashups are web applications that expedite software development by reusing existing resources through integrating multiple application programming interfaces (APIs). Recommending the appropriate APIs plays a critical role in assisting developers in building such web applications easily and efficiently. The proliferation of publicly available APIs on the Internet has inspired the community to adopt various models to accomplish the recommendation task. Until present, considerable efforts have been made to recommend the optimal set of APIs, delivering fruitful results and achieving varying recommendation performance. This paper presents a timely review on the topic of API recommendations for mashup creation. Specifically, we investigate and compare not only traditional data mining approaches and recommendation techniques but also more recent approaches based on network representation learning and deep learning techniques. By analyzing the merits and pitfalls of existing approaches, we pinpoint a few promising directions to address the remaining challenges in the current research. This survey provides a timely comprehensive review of the API recommendation research and could be a useful reference for relevant researchers and practitioners.},
  archive      = {J_COMJNL},
  author       = {Alhosaini, Hadeel and Alharbi, Sultan and Wang, Xianzhi and Xu, Guandong},
  doi          = {10.1093/comjnl/bxad112},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1920-1940},
  shortjournal = {Comput. J.},
  title        = {API recommendation for mashup creation: A comprehensive survey},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anonymization of face images with contrastive learning.
<em>COMJNL</em>, <em>67</em>(5), 1910–1919. (<a
href="https://doi.org/10.1093/comjnl/bxad111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photos or videos taken by individuals often carry sensitive details such as facial identities, which has led to an escalating societal interest in privacy protection measures. We suggest an improved face identity transformer that offers password-protected anonymization and de-anonymization of photo-realistic facial images in visual data. Our face identity transformer is designed to (1) erase facial identity information after anonymization, (2) restore the original face when a correct password is provided and (3) generate an incorrect but realistic face when given an incorrect password. The processes of image anonymization and de-anonymization are facilitated through a password scheme, a multi-task learning objective and generative adversarial networks comprising InfoGAN and contrastive learning. In-depth experiments indicate that our methodology can execute anonymization and de-anonymization based on password conditions whilst reducing training time and enhancing image quality compared to existing anonymization procedures. Additionally, it maintains a recognition rate as low as 4.8% for anonymized images without sacrificing the face detection rate of the original method.},
  archive      = {J_COMJNL},
  author       = {Xu, Xintong and Cui, Run and Huang, Chanying and Yan, Kedong},
  doi          = {10.1093/comjnl/bxad111},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1910-1919},
  shortjournal = {Comput. J.},
  title        = {Anonymization of face images with contrastive learning},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). KVFL: Key-value-based persistent fuzzing for IoT web
servers. <em>COMJNL</em>, <em>67</em>(5), 1892–1909. (<a
href="https://doi.org/10.1093/comjnl/bxad110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the number of Internet of Thing (IoT) devices increases, attacks against their vulnerabilities have become a serious threat. The web servers (WSs) in IoT devices provide management services for end-users, which are currently the major attack surface. Several fuzzing solutions for identifying vulnerabilities in IoT devices have been proposed, but there is currently no grey-box fuzzer specifically designed for the unique features of WSs in IoT to effectively detect memory corruption vulnerabilities. We design and implement KVFL, an efficient grey-box fuzzer, to address the issues of low throughput and slow exploration of deep code when fuzzing for IoT WSs. Firstly, KVFL employs a delicate hooking technology that heuristically hijacks and emulates hardware-dependent functions, ensuring WSs can be accurately and efficiently emulated in user-mode. On this basis, KVFL fully utilizes the loop parsing HTTP requests feature of WSs through a redesigned fork-server, to minimize nonessential rebooting losses of the target, thereby significantly improving fuzzing throughput. Secondly, KVFL leverages code coverage feedback to automatically infer a set of valid Keys and derive a Key-Value mutation. This enables the generation of high-quality test cases that can facilitate deeper code exploration of WSs. The evaluation results show that compared to the state-of-the-art IoT grey-box fuzzer FIRM-AFL, KVFL improves the throughput by over 2× and explores 4.5× more edges. Additionally, it identifies all 1-day vulnerabilities with over 7× faster speed than the baseline and detects three previously unknown 0-day vulnerabilities. These all indicate that KVFL is effective and efficient at fuzzing IoT WSs.},
  archive      = {J_COMJNL},
  author       = {Wang, Chiheng and Zhao, Shibin and Peng, Jianshan and Zhu, Junhu},
  doi          = {10.1093/comjnl/bxad110},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1892-1909},
  shortjournal = {Comput. J.},
  title        = {KVFL: Key-value-based persistent fuzzing for IoT web servers},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stealthy backdoor attacks on deep point cloud recognization
networks. <em>COMJNL</em>, <em>67</em>(5), 1879–1891. (<a
href="https://doi.org/10.1093/comjnl/bxad109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks are vulnerable to backdoor attacks. Previous backdoor attacks have mainly focused on images. Unlike images composed of regular pixels, 3D point clouds are composed of irregular three-dimensional XYZ coordinates, which are widely used in areas such as autonomous driving and 3D measurement. As many deep neural networks have been developed for processing 3D point clouds, these networks also face the risk of backdoor attacks. Nevertheless, backdoor attacks on 3D point clouds have rarely been investigated. This paper proposes a stealthy backdoor attack on point clouds in the physical world, aiming to generate trainable non-rigid deformations as backdoor patterns. Instead of directly adding backdoor patterns onto the point clouds, we deform the 3D space of the point clouds to a new space, ensuring that all point clouds have the same backdoor deformation. We use point cloud alignment to overcome the inconsistency of backdoor deformation caused by shifting and scaling in the physical world. We also propose a physical transformation layer to combat the physical transformations. Additionally, we propose mask contrast learning to eliminate pseudo backdoor patterns to make the network’s backdoor property stealthier. Extensive experiments indicate that the proposed method can achieve better attack success rates and stealthiness.},
  archive      = {J_COMJNL},
  author       = {Feng, Le and Qian, Zhenxing and Zhang, Xinpeng and Li, Sheng},
  doi          = {10.1093/comjnl/bxad109},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1879-1891},
  shortjournal = {Comput. J.},
  title        = {Stealthy backdoor attacks on deep point cloud recognization networks},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing auditory brainstem response classification based
on vision transformer. <em>COMJNL</em>, <em>67</em>(5), 1872–1878. (<a
href="https://doi.org/10.1093/comjnl/bxad107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A method for testing the health of ear’s peripheral auditory nerve and its connection to the brainstem is called an auditory brainstem response (ABR). Manual quantification of ABR tests by an audiologist is not only costly but also time-consuming and susceptible to errors. Recently in machine learning have prompted a resurgence of research into ABR classification. This study presents an automated ABR recognition model. The initial step in our design process involves collecting a dataset by extracting ABR test images from sample test reports. Subsequently, we employ an elastic distortion approach to generate new images from the originals, effectively expanding the dataset while preserving the fundamental structure and morphology of the original ABR content. Finally, the Vision Transformer method was exploited to train and develop our model. In the testing phase, the incorporation of both the newly generated and original images yields an impressive accuracy rate of 97.83%. This result is noteworthy when benchmarked against the latest research in the field, underscoring the substantial performance enhancement achieved through the utilization of generated data.},
  archive      = {J_COMJNL},
  author       = {Abubakir Ahmed, Hunar and Majidpour, Jafar and Hussein Ahmed, Mohammed and Kais Jameel, Samer and Majidpour, Amir},
  doi          = {10.1093/comjnl/bxad107},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1872-1878},
  shortjournal = {Comput. J.},
  title        = {Enhancing auditory brainstem response classification based on vision transformer},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Eager term rewriting for the fracterm calculus of common
meadows. <em>COMJNL</em>, <em>67</em>(5), 1866–1871. (<a
href="https://doi.org/10.1093/comjnl/bxad106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eager equality is a novel semantics for equality in the presence of partial operations. We consider term rewriting for eager equality for arithmetic in which division is a partial operator. We use common meadows which are essentially fields that contain an absorptive element |$\bot $|⁠ . The idea is that term rewriting is supposed to be semantics preserving for non- |$\bot $| terms only. We show soundness and adequacy results for eager term rewriting w.r.t. the class of all common meadows. However, we show that an eager term rewrite system which is complete for common meadows of rational numbers is not easy to obtain, if it exists at all.},
  archive      = {J_COMJNL},
  author       = {Bergstra, Jan A and Tucker, John V},
  doi          = {10.1093/comjnl/bxad106},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1866-1871},
  shortjournal = {Comput. J.},
  title        = {Eager term rewriting for the fracterm calculus of common meadows},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An intrusion detection method based on attention mechanism
to improve CNN-BiLSTM model. <em>COMJNL</em>, <em>67</em>(5), 1851–1865.
(<a href="https://doi.org/10.1093/comjnl/bxad105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Security of computer information can be improved with the use of a network intrusion detection system. Since the network environment is becoming more complex, more and more new methods of attacking the network have emerged, making the original intrusion detection methods ineffective. Increased network activity also causes intrusion detection systems to identify errors more frequently. We suggest a new intrusion detection technique in this research that combines a Convolutional Neural Network (CNN) model with a Bi-directional Long Short-term Memory Network (BiLSTM) model for adding attention mechanisms. We distinguish our model from existing methods in three ways. First, we use the NCR-SMOTE algorithm to resample the dataset. Secondly, we use recursive feature elimination method based on extreme random tree to select features. Thirdly, we improve the profitability and accuracy of predictions by adding attention mechanism to CNN-BiLSTM. This experiment uses UNSW-UB15 dataset composed of real traffic, and the accuracy rate of multi-classification is 84.5 |$\%$|⁠ ; the accuracy rate of multi-classification in CSE-IC-IDS2018 dataset reached 98.3 |$\%$|⁠ .},
  archive      = {J_COMJNL},
  author       = {Shou, Dingyu and Li, Chao and Wang, Zhen and Cheng, Song and Hu, Xiaobo and Zhang, Kai and Wen, Mi and Wang, Yong},
  doi          = {10.1093/comjnl/bxad105},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1851-1865},
  shortjournal = {Comput. J.},
  title        = {An intrusion detection method based on attention mechanism to improve CNN-BiLSTM model},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging meta-learning to improve unsupervised domain
adaptation. <em>COMJNL</em>, <em>67</em>(5), 1838–1850. (<a
href="https://doi.org/10.1093/comjnl/bxad104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Domain Adaptation (UDA) techniques in real-world scenarios often encounter limitations due to their reliance on reducing distribution dissimilarity between source and target domains, assuming it leads to effective adaptation. However, they overlook the intricate factors causing domain shifts, including data distribution variations, domain-specific features and nonlinear relationships, thereby hindering robust performance in challenging UDA tasks. The Neuro-Fuzzy Meta-Learning (NF-ML) approach overcomes traditional UDA limitations with its flexible framework that adapts to intricate, nonlinear domain gaps without rigid assumptions. NF-ML enhances domain adaptation by selecting a UDA subset and optimizing their weights via a neuro-fuzzy system, utilizing meta-learning to efficiently adapt models to new domains using previously acquired knowledge. This approach mitigates domain adaptation challenges and bolsters traditional UDA methods’ performance by harnessing the strengths of multiple UDA methods to enhance overall model generalization. The proposed approach shows potential in advancing domain adaptation research by providing a robust and efficient solution for real-world domain shifts. Experiments on three standard image datasets confirm the proposed approach’s superiority over state-of-the-art UDA methods, validating the effectiveness of meta-learning. Remarkably, the Office+Caltech 10, ImageCLEF-DA and combined digit datasets exhibit substantial accuracy gains of 30.9%, 6.8% and 10.9%, respectively, compared with the best-second baseline UDA approach.},
  archive      = {J_COMJNL},
  author       = {Farhadi, Amirfarhad and Sharifi, Arash},
  doi          = {10.1093/comjnl/bxad104},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1838-1850},
  shortjournal = {Comput. J.},
  title        = {Leveraging meta-learning to improve unsupervised domain adaptation},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Keyframe-guided video swin transformer with multi-path
excitation for violence detection. <em>COMJNL</em>, <em>67</em>(5),
1826–1837. (<a href="https://doi.org/10.1093/comjnl/bxad103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Violence detection is a critical task aimed at identifying violent behavior in video by extracting frames and applying classification models. However, the complexity of video data and the suddenness of violent events present significant hurdles in accurately pinpointing instances of violence, making the extraction of frames that indicate violence a challenging endeavor. Furthermore, designing and applying high-performance models for violence detection remains an open problem. Traditional models embed extracted spatial features from sampled frames directly into a temporal sequence, which ignores the spatio-temporal characteristics of video and limits the ability to express continuous changes between adjacent frames. To address the existing challenges, this paper proposes a novel framework called ACTION-VST. First, a keyframe extraction algorithm is developed to select frames that are most likely to represent violent scenes in videos. To transform visual sequences into spatio-temporal feature maps, a multi-path excitation module is proposed to activate spatio-temporal, channel and motion features. Next, an advanced Video Swin Transformer-based network is employed for both global and local spatio-temporal modeling, which enables comprehensive feature extraction and representation of violence. The proposed method was validated on two large-scale datasets, RLVS and RWF-2000, achieving accuracies of over 98 and 93%, respectively, surpassing the state of the art.},
  archive      = {J_COMJNL},
  author       = {Li, Chenghao and Yang, Xinyan and Liang, Gang},
  doi          = {10.1093/comjnl/bxad103},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1826-1837},
  shortjournal = {Comput. J.},
  title        = {Keyframe-guided video swin transformer with multi-path excitation for violence detection},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Policy-based remote user authentication from
multi-biometrics. <em>COMJNL</em>, <em>67</em>(5), 1814–1825. (<a
href="https://doi.org/10.1093/comjnl/bxad102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce the first generic framework of policy-based remote user authentication from multiple biometrics. The proposed framework allows an authorized user to remotely authenticate herself to an authentication server using her multiple biometrics, which enhances both the security and usability of user authentications. The authentication server approves a user’s authentication request if and only if the user’s multiple biometrics satisfies an authentication policy. In particular, the authentication policy can be dynamically updated to satisfy different security and usability requirements in practice. We implement an instantiation of the proposed framework and report its performance under various authentication policies.},
  archive      = {J_COMJNL},
  author       = {Tian, Yangguang and Li, Yingjiu and Deng, Robert H and Yang, Guomin and Li, Nan},
  doi          = {10.1093/comjnl/bxad102},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1814-1825},
  shortjournal = {Comput. J.},
  title        = {Policy-based remote user authentication from multi-biometrics},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gathering over heterogeneous meeting nodes. <em>COMJNL</em>,
<em>67</em>(5), 1794–1813. (<a
href="https://doi.org/10.1093/comjnl/bxad101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider two finite and disjoint sets of homogeneous robots deployed at the nodes of an infinite grid graph. The grid graph also comprises two finite and disjoint sets of prefixed meeting nodes located over the nodes of the grid. The objective of our study is to design a distributed algorithm that gathers all the robots belonging to the first team at one of the meeting nodes belonging to the first type, and all the robots in the second team must gather at one of the meeting nodes belonging to the second type. The robots can distinguish between the two types of meeting nodes. However, a robot cannot identify its team members. This paper assumes the strongest adversarial model, namely the asynchronous scheduler . We have characterized all the initial configurations for which the gathering problem is unsolvable. For the remaining initial configurations, the paper proposes a distributed gathering algorithm. Assuming the robots are capable of global-weak multiplicity detection , the proposed algorithm solves the problem within a finite time period. The algorithm runs in |$\Theta (dn)$| moves and |$O(dn)$| epochs, where |$d$| is the diameter of the minimum enclosing rectangle of all the robots and meeting nodes in the initial configuration, and |$n$| is the total number of robots in the system.},
  archive      = {J_COMJNL},
  author       = {Chakraborty, Abhinav and Bhagat, Subhash and Mukhopadhyaya, Krishnendu},
  doi          = {10.1093/comjnl/bxad101},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1794-1813},
  shortjournal = {Comput. J.},
  title        = {Gathering over heterogeneous meeting nodes},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse backdoor attack against neural networks.
<em>COMJNL</em>, <em>67</em>(5), 1783–1793. (<a
href="https://doi.org/10.1093/comjnl/bxad100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies show that neural networks are vulnerable to backdoor attacks, in which compromised networks behave normally for clean inputs but make mistakes when a pre-defined trigger appears. Although prior studies have designed various invisible triggers to avoid causing visual anomalies, they cannot evade some trigger detectors. In this paper, we consider the stealthiness of backdoor attacks from input space and feature representation space. We propose a novel backdoor attack named sparse backdoor attack, and investigate the minimum required trigger to induce the well-trained networks to make incorrect results. A U-net-based generator is employed to create triggers for each clean image. Considering the stealthiness of the trigger, we restrict the elements of the trigger between −1 and 1. In the aspect of the feature representation domain, we adopt an entanglement cost function to minimize the gap between feature representations of benign and malicious inputs. The inseparability of benign and malicious feature representations contributes to the stealthiness of our attack against various model diagnosis-based defences. We validate the effectiveness and generalization of our method by conducting extensive experiments on multiple datasets and networks.},
  archive      = {J_COMJNL},
  author       = {Zhong, Nan and Qian, Zhenxing and Zhang, Xinpeng},
  doi          = {10.1093/comjnl/bxad100},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1783-1793},
  shortjournal = {Comput. J.},
  title        = {Sparse backdoor attack against neural networks},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An intelligent air monitoring system for pollution
prediction: A predictive healthcare perspective. <em>COMJNL</em>,
<em>67</em>(5), 1763–1782. (<a
href="https://doi.org/10.1093/comjnl/bxad099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extensive potential of Internet of Things (IoT) technology has enabled the widespread real-time perception and analysis of health conditions. Furthermore, the integration of IoT in the healthcare industry has resulted in the development of intelligent applications, including smartphone-based healthcare, wellness-aware recommendations and smart medical systems. Building upon these technological advancements, this research puts forth an enhanced framework designed for the real-time monitoring, detection and prediction of health vulnerabilities arising from air pollution. Specifically, a four-layered model is presented to categorize health-impacting particles associated with air pollution into distinct classes based on probabilistic parameters of Health Adversity (HA). Subsequently, the HA parameters are extracted and temporally analyzed using FogBus, a fog computing platform, to identify vulnerabilities in individual health. To facilitate accurate prediction, an assessment of the Air Impact on Health is conducted using a Differential Evolution-Recurrent Neural Network. Moreover, the temporal analysis of health vulnerability employs the Self-Organized Mapping technique for visualization. The proposed model’s validity is evaluated using a challenging dataset comprising nearly 60 212 data instances obtained from the online University of California, Irvine repository. Performance enhancement is assessed by comparing the proposed model with state-of-the-art decision-making techniques, considering statistical parameters such as temporal effectiveness, coefficient of determination, accuracy, specificity, sensitivity, reliability and stability.},
  archive      = {J_COMJNL},
  author       = {Behal, Veerawali and Singh, Ramandeep},
  doi          = {10.1093/comjnl/bxad099},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1763-1782},
  shortjournal = {Comput. J.},
  title        = {An intelligent air monitoring system for pollution prediction: A predictive healthcare perspective},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SPAW-SMOTE: Space partitioning adaptive weighted synthetic
minority oversampling technique for imbalanced data set learning.
<em>COMJNL</em>, <em>67</em>(5), 1747–1762. (<a
href="https://doi.org/10.1093/comjnl/bxad098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of data imbalance is common in reality, which greatly affects the performance of classifiers. Most of the solutions are to balance the data set by generating new minority class samples, which are faced with the problems of selecting the appropriate area for generating samples, fuzzy classification boundary and uneven distribution of samples. To solve these problems, we propose a novel oversampling algorithm named space partitioning adaptive weighted synthetic minority oversampling technique (SPAW-SMOTE). We first divide the data space into boundary space and non-boundary space based on spatial partitioning techniques. The number of samples to be generated is assigned to different spaces by the designed adaptive weighting algorithm, which is used to solve the problems of uneven distribution of samples and easy to blur the classification boundary. Finally, we also endeavor to develop a new generation algorithm to reduce the probability of overlapping samples generated when synthesizing new samples and to ensure the diversity of new samples. Experimental results on 18 real-world data sets show that the average performance (G-mean, F1-measure and Area Under Curve) of SPAW-SMOTE is significantly better than other existing oversampling techniques.},
  archive      = {J_COMJNL},
  author       = {Zhang, Qiang and He, Junjiang and Li, Tao and Lan, Xiaolong and Fang, Wenbo and Li, Yihong},
  doi          = {10.1093/comjnl/bxad098},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1747-1762},
  shortjournal = {Comput. J.},
  title        = {SPAW-SMOTE: Space partitioning adaptive weighted synthetic minority oversampling technique for imbalanced data set learning},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identifying and ranking influential spreaders in complex
networks by localized decreasing gravity model. <em>COMJNL</em>,
<em>67</em>(5), 1727–1746. (<a
href="https://doi.org/10.1093/comjnl/bxad097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying crucial nodes in complex networks is paid more attention in recent years. Some classical methods, such as degree centrality, betweenness centrality and closeness centrality, have their advantages and disadvantages. Recently, the gravity model is applied to describe the relationship of nodes in a complex network. However, the interaction force in gravity model follows the square law of distance, which is inconsistent with the actual situation. Most people are generally affected by those who are surrounding them, which means that local influence should be emphasized. To address this issue, we propose an indexing method called localized decreasing gravity centrality by maximizing the local influence of a node. In the proposed measure, the mass and radius of gravity model are redefined, which can represent the spreading ability of the node. In addition, a decreasing weight is added to strengthen the local influence of a node. To evaluate the performance of the proposed method, we utilize four different types of networks, including interaction networks, economic networks, collaboration networks and animal social networks. Also, two different infectious disease models, susceptible-infectious-recovered (SIR) and susceptible-exposed-low risk-high risk-recovered (SELHR), are utilized to examine the spreading ability of influential nodes.},
  archive      = {J_COMJNL},
  author       = {Xiang, Nan and Tang, Xiao and Liu, Huiling and Ma, Xiaoxia},
  doi          = {10.1093/comjnl/bxad097},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1727-1746},
  shortjournal = {Comput. J.},
  title        = {Identifying and ranking influential spreaders in complex networks by localized decreasing gravity model},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The orbits of folded crossed cubes. <em>COMJNL</em>,
<em>67</em>(5), 1719–1726. (<a
href="https://doi.org/10.1093/comjnl/bxad096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two vertices |$u$| and |$v$| in a graph |$G=(V,E)$| are in the same orbit if there exists an automorphism |$\phi $| of |$G$| such that |$\phi (u)=v$|⁠ . The orbit number of a graph |$G$|⁠ , denoted by |$Orb(G)$|⁠ , is the smallest number of orbits, which form a partition of |$V(G)$|⁠ , in |$G$|⁠ . All vertex-transitive graphs |$G$| are with |$Orb(G)=1$|⁠ . Since the |$n$| -dimensional hypercube, denoted by |$Q_{n}$|⁠ , is vertex-transitive, it follows that |$Orb(Q_{n})=1$| for |$n\geq 1$|⁠ . Pai, Chang, and Yang proved that the |$n$| -dimensional folded crossed cube, denoted by |$FCQ_{n}$|⁠ , is vertex-transitive if and only if |$n\in \{1,2,4\}$|⁠ , namely |$Orb(FCQ_{1})=Orb(FCQ_{2})=Orb(FCQ_{4})=1$|⁠ . In this paper, we prove that |$Orb(FCQ_{n})=2^{\lceil \frac{n}{2}\rceil -2}$| if |$n\geq 6$| is even and |$Orb(FCQ_{n}) = 2^{\lceil \frac{n}{2}\rceil -1}$| if |$n\geq 3$| is odd.},
  archive      = {J_COMJNL},
  author       = {Liu, Jia-Jie},
  doi          = {10.1093/comjnl/bxad096},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1719-1726},
  shortjournal = {Comput. J.},
  title        = {The orbits of folded crossed cubes},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Similarity regression of functions in different compiled
forms with neural attentions on dual control-flow graphs.
<em>COMJNL</em>, <em>67</em>(5), 1710–1718. (<a
href="https://doi.org/10.1093/comjnl/bxad095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting if two functions in different compiled forms are similar has a wide range of applications in software security. We present a method that leverages both semantic and structural features of functions, learned by a neural-net model on the underlying control-flow graphs (CFGs). In particular, we devise a neural function-similarity regressor (NFSR) with attentions on dual CFGs. We train and evaluate NFSR on a dataset consisting of nearly 4 million functions from over 14 900 binary files. Experiments show that NFSR is superior to the SOTA models of SAFE, Gemini and GMN, especially for binary functions with large CFGs. An ablation study shows that attention on dual CFGs plays a significant role in detecting function similarities.},
  archive      = {J_COMJNL},
  author       = {Zhang, Yun and Liu, Yuling and Cheng, Ge and Wang, Jie},
  doi          = {10.1093/comjnl/bxad095},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1710-1718},
  shortjournal = {Comput. J.},
  title        = {Similarity regression of functions in different compiled forms with neural attentions on dual control-flow graphs},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Underwater wireless sensor network-based delaunay
triangulation (UWSN-DT) algorithm for sonar map fusion. <em>COMJNL</em>,
<em>67</em>(5), 1699–1709. (<a
href="https://doi.org/10.1093/comjnl/bxad094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust and fast image recognition and matching is an important task in the underwater domain. The primary focus of this work is on extracting subsea features with sonar sensor for further Autonomous Underwater Vehicle navigation, such as the robotic localization and landmark mapping applications. With the assistance of high-resolution underwater features in the Side Scan Sonar (SSS) images, an efficient feature detector and descriptor, Speeded Up Robust Feature, is employed to seabed sonar image fusion task. In order to solve the nonlinear intensity difference problem in SSS images, the main novelty of this work is the proposed Underwater Wireless Sensor Network-based Delaunay Triangulation (UWSN-DT) algorithm for improving the performances of sonar map fusion accuracy with low computational complexity, in which the wireless nodes are considered as underwater feature points, since nodes could provide sufficiently useful information for the underwater map fusion, such as the location. In the simulated experiments, it shows that the presented UWSN-DT approach works efficiently and robustly, especially for the subsea environments where there are few distinguishable feature points.},
  archive      = {J_COMJNL},
  author       = {Yuan, Xin and Li, Ning and Gong, Xiaobo and Yu, Changli and Zhou, Xiaoteng and Ortega, José-Fernán Martínez},
  doi          = {10.1093/comjnl/bxad094},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1699-1709},
  shortjournal = {Comput. J.},
  title        = {Underwater wireless sensor network-based delaunay triangulation (UWSN-DT) algorithm for sonar map fusion},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lattice-based homomorphic encryption for privacy-preserving
smart meter data analytics. <em>COMJNL</em>, <em>67</em>(5), 1687–1698.
(<a href="https://doi.org/10.1093/comjnl/bxad093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy-preserving smart meter data collection and analysis are critical for optimizing smart grid environments without compromising privacy. Using homomorphic encryption techniques, smart meters can encrypt collected data to ensure confidentiality, and other untrusted nodes can further compute over the encrypted data without having to recover the underlying plaintext. As an illustrative example, this approach can be useful to compute the monthly electricity consumption without violating consumer privacy by collecting fine-granular data through small increments of time. Toward that end, we propose an architecture for privacy-preserving smart meter data collection, aggregation and analysis based on lattice-based homomorphic encryption. Furthermore, we compare the proposed method with the Paillier and Boneh–Goh–Nissim (BGN) cryptosystems, which are popular alternatives for homomorphic encryption in smart grids. We consider different services with different requirements in terms of multiplicative depth, e.g. billing, variance and nonlinear support vector machine classification. Accordingly, we measure and show the practical overhead of using the proposed homomorphic encryption method in terms of communication traffic (ciphertext size) and latency. Our results show that lattice-based homomorphic encryption is more efficient than Paillier and BGN for both multiplication and addition operations while offering more flexibility in terms of the computation that can be evaluated homomorphically.},
  archive      = {J_COMJNL},
  author       = {Marandi, Ali and Alves, Pedro Geraldo M R and Aranha, Diego F and Jacobsen, Rune Hylsberg},
  doi          = {10.1093/comjnl/bxad093},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1687-1698},
  shortjournal = {Comput. J.},
  title        = {Lattice-based homomorphic encryption for privacy-preserving smart meter data analytics},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coalitional double auction for ridesharing with desired
benefit and QoE constraints. <em>COMJNL</em>, <em>67</em>(5), 1674–1686.
(<a href="https://doi.org/10.1093/comjnl/bxad092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ridesharing is an effective approach to alleviate traffic congestion. In most existing works, drivers and passengers are assigned prices without considering the constraints of desired benefits. This paper investigates ridesharing by formulating a matching and pricing problem to maximize the total payoff of drivers, with the constraints of desired benefit and quality of experience. An efficient algorithm is proposed to solve the formulated problem based on coalitional double auction. Secondary pricing based strategy and sacrificed minimum bid based strategy are proposed to support the algorithm. This paper also proves that the proposed algorithm can achieve a Nash-stable coalition partition in finite steps, and the proposed two strategies guarantee truthfulness, individually rational and budget balance. Extensive simulation results on the real-world dataset of taxi trajectory in Beijing city show that the proposed algorithm outperforms the existing ones, in terms of average total payoff of drivers while meeting the benefits of passengers.},
  archive      = {J_COMJNL},
  author       = {Huang, Jiale and Wu, Jigang and Chen, Long and Wu, Yalan and Li, Yidong},
  doi          = {10.1093/comjnl/bxad092},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1674-1686},
  shortjournal = {Comput. J.},
  title        = {Coalitional double auction for ridesharing with desired benefit and QoE constraints},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cryptanalysis of a type of white-box implementations of the
SM4 block cipher. <em>COMJNL</em>, <em>67</em>(5), 1663–1673. (<a
href="https://doi.org/10.1093/comjnl/bxad091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The SM4 block cipher is a Chinese national standard and an ISO international standard. Since white-box cryptography has many real-life applications nowadays, a few white-box implementations of SM4 has been proposed, among which a type of constructions is dominated, which uses a linear or affine diagonal block encoding to protect the original three 32-bit branches entering a round function and uses its inverse as the input encoding to the S-box layer. In this paper, we analyse the security of this type of constructions against Lepoint et al.’s collision-based attack method. Our experiment under a small fraction of (encodings, round key) combinations shows that the rank of the concerned linear system is much less than the number of the involved unknowns, meaning these white-box SM4 implementations should resist Lepoint et al.’s method, but we leave it as an open problem whether there are such encodings that the rank of the corresponding linear system is slightly less than the number of the involved unknowns, in which scenario Lepoint et al.’s method may be used to recover a round key for the case with linear encodings and to remove most white-box operations until mainly some Boolean masks for the case with affine encodings.},
  archive      = {J_COMJNL},
  author       = {Lu, Jiqiang and Li, Jingyu and Chen, Zexuan and Li, Yanan},
  doi          = {10.1093/comjnl/bxad091},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1663-1673},
  shortjournal = {Comput. J.},
  title        = {Cryptanalysis of a type of white-box implementations of the SM4 block cipher},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enumeration of subtrees of two families of self-similar
networks based on novel two-forest dual transformations.
<em>COMJNL</em>, <em>67</em>(5), 1652–1662. (<a
href="https://doi.org/10.1093/comjnl/bxad090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a structural topological index, the number of subtrees has great significance for the analysis and design of hybrid locally reliable networks. In this paper, with generating function and introducing a novel two-forest dual transformation technique, we solve the subtree enumerating problems of two representatives of the self-similar networks, such as the hierarchical lattice and |$(u,v)$| -flower networks. Moreover, by means of the circle weight transfer technique, two linear time algorithms of computing the subtree generation functions of these two families of networks are also proposed. The subtree density of two special cases for these self-similar networks is briefly discussed as an application.},
  archive      = {J_COMJNL},
  author       = {Sun, Daoqiang and Liu, Hongbo and Yang, Yu and Li, Long and Zhang, Heng and Fahad, Asfand},
  doi          = {10.1093/comjnl/bxad090},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1652-1662},
  shortjournal = {Comput. J.},
  title        = {Enumeration of subtrees of two families of self-similar networks based on novel two-forest dual transformations},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online optimization method of learning process for
meta-learning. <em>COMJNL</em>, <em>67</em>(5), 1645–1651. (<a
href="https://doi.org/10.1093/comjnl/bxad089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-learning is a pivotal and potentially influential machine learning approach to solve challenging problems in reinforcement learning. However, the costly hyper-parameter tuning for training stability of meta-learning is a known shortcoming and currently a hotspot of research. This paper addresses this shortcoming by introducing an online and easily trainable hyper-parameter optimization approach, called Meta Parameters Learning via Meta-Learning (MPML), to combine online hyper-parameter adjustment scheme into meta-learning algorithm, which reduces the need to tune hyper-parameters. Specifically, a basic learning rate for each training task is put forward. Besides, the proposed algorithm dynamically adapts multiple basic learning rate and a shared meta-learning rate through conducting gradient descent alongside the initial optimization steps. In addition, the sensitivity with respect to hyper-parameter choices in the proposed approach are also discussed compared with model-agnostic meta-learning method. The experimental results on reinforcement learning problems demonstrate MPML algorithm is easy to implement and delivers more highly competitive performance than existing meta-learning methods on a diverse set of challenging control tasks.},
  archive      = {J_COMJNL},
  author       = {Xu, Zhixiong and Zhang, Wei and Li, Ailin and Zhao, Feifei and Jing, Yuanyuan and Wan, Zheng and Cao, Lei and Chen, Xiliang},
  doi          = {10.1093/comjnl/bxad089},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1645-1651},
  shortjournal = {Comput. J.},
  title        = {Online optimization method of learning process for meta-learning},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hybrid solution for the cold start problem in
recommendation. <em>COMJNL</em>, <em>67</em>(5), 1637–1644. (<a
href="https://doi.org/10.1093/comjnl/bxad088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems are becoming more and more significant in today’s digital world and in the modern economy. They make a substantial contribution to company operations by offering tailored advice and decreasing overwhelm. Collaborative filtering, being popular in the domain of recommendation, is used to offer recommendations to attract the target audience based on the feedback of people with comparable interests. This method has some limitations, such as a cold-start issue, which makes the system less effective in anticipating unknown objects. We provide a hybrid deep-learning-based strategy centered on a method to enrich user and item profiles to address the cold-start issue in the recommendation process using a collaborative filtering approach. We employ pretrained deep learning models to produce rich user and item feature vectors that aid in the creation of useful suggestions and handling of user and item cold-start issues. The creation of more precise and tailored similarity matrices is made possible by adding metadata to the extracted features of the user and item. The results of the experiment demonstrate that in terms of precision and rate coverage, the proposed method performs better than the baseline techniques.},
  archive      = {J_COMJNL},
  author       = {Jafri, Syed Irteza Hussain and Ghazali, Rozaida and Javid, Irfan and Mazwin Mohmad Hassim, Yana and Hayat Khan, Mubashir},
  doi          = {10.1093/comjnl/bxad088},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1637-1644},
  shortjournal = {Comput. J.},
  title        = {A hybrid solution for the cold start problem in recommendation},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A chinese grammatical error correction model based on
grammatical generalization and parameter sharing. <em>COMJNL</em>,
<em>67</em>(5), 1628–1636. (<a
href="https://doi.org/10.1093/comjnl/bxad087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chinese grammatical error correction (CGEC) is a significant challenge in Chinese natural language processing. Deep-learning-based models tend to have tens of millions or even hundreds of millions of parameters since they model the target task as a sequence-to-sequence problem. This may require a vast quantity of annotated corpora for training and parameter tuning. However, there are currently few open-source annotated corpora for the CGEC task; the existing researches mainly concentrate on using data augmentation technology to alleviate the data-hungry problem. In this paper, rather than expanding training data, we propose a competitive CGEC model from a new insight for reducing model parameters. The model contains three main components: a sequence learning module, a grammatical generalization module and a parameter sharing module. Experimental results on two Chinese benchmarks demonstrate that the proposed model could achieve competitive performance over several baselines. Even if the parameter number of our model is reduced by 1/3, it could reach a comparable |$F_{0.5}$| value of 30.75%. Furthermore, we utilize English datasets to evaluate the generalization and scalability of the proposed model. This could provide a new feasible research direction for CGEC research.},
  archive      = {J_COMJNL},
  author       = {Lin, Nankai and Lin, Xiaotian and Fu, Yingwen and Jiang, Shengyi and Wang, Lianxi},
  doi          = {10.1093/comjnl/bxad087},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1628-1636},
  shortjournal = {Comput. J.},
  title        = {A chinese grammatical error correction model based on grammatical generalization and parameter sharing},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AKGF: Automatic kernel generation for DNN on CPU-FPGA.
<em>COMJNL</em>, <em>67</em>(5), 1619–1627. (<a
href="https://doi.org/10.1093/comjnl/bxad086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While tensor accelerated compilers have proven effective in deploying deep neural networks (DNN) on general-purpose hardware, optimizing for FPGA remains challenging due to the complex DNN architectures and the heterogeneous, semi-open compute units. This paper introduces the Automatic Kernel Generation for DNN on CPU-FPGA (AKGF) framework for efficient deployment of DNN on heterogeneous CPU-FPGA platforms. AKGF generates an intermediate representation (IR) of the DNN using TVM’s Halide IR, annotates the operators of model layers in the IR to compute them on the corresponding hardware cores, and further optimizes the operator code for CPU and FPGA using ARM’s function library and the polyhedral model to enhance model inference speed and power consumption. The experimental tests conducted on a CPU-FPGA board validate the effectiveness of AKGF, demonstrating significant acceleration ratios (up to 6.7x) compared to state-of-the-art accelerators while achieving a 2x power optimization. AKGF effectively leverages the computational capabilities of both CPU and FPGA for high-performance deployment of DNN on CPU-FPGA platforms.},
  archive      = {J_COMJNL},
  author       = {Dong, Dong and Jiang, Hongxu and Diao, Boyu},
  doi          = {10.1093/comjnl/bxad086},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1619-1627},
  shortjournal = {Comput. J.},
  title        = {AKGF: Automatic kernel generation for DNN on CPU-FPGA},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge-aware dual-channel graph neural networks for
denoising recommendation. <em>COMJNL</em>, <em>67</em>(5), 1607–1618.
(<a href="https://doi.org/10.1093/comjnl/bxad085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph (KG) is introduced as side information into recommender systems, which can alleviate the sparsity and cold start problems in collaborative filtering. Existing studies mainly focus on modeling users’ historical behavior data and KG-based propagation. However, they have the limitation of ignoring noise information during recommendation. We consider that noise exists in two parts (i.e. KG and user-item interaction data). In this paper, we propose Knowledge-aware Dual-Channel Graph Neural Networks (KDGNN) to improve the recommendation performance by reducing the noise in the recommendation process. Specifically, (1) for the noise in KG, we design a personalized gating mechanism, namely dual-channel balancing mechanism, to block the propagation of redundant information in KG. (2) For the noise in user-item interaction data, we integrate personalized and knowledge-aware signals to capture user preferences fully and use personalized knowledge-aware attention to denoise user-item interaction data. Compared with existing KG-based methods, we aim to propose a knowledge-aware recommendation method from a new perspective of denoising. We perform performance analysis on three real-world datasets, and experiment results demonstrate that KDGNN achieves strongly competitive performance compared with several compelling state-of-the-art baselines.},
  archive      = {J_COMJNL},
  author       = {Zhang, Hanwen and Wang, Li-e and Sun, Zhigang and Li, Xianxian},
  doi          = {10.1093/comjnl/bxad085},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1607-1618},
  shortjournal = {Comput. J.},
  title        = {Knowledge-aware dual-channel graph neural networks for denoising recommendation},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Thematic editorial, it is hard to imagine a world without
algorithms and data science. <em>COMJNL</em>, <em>67</em>(5), 1605–1606.
(<a href="https://doi.org/10.1093/comjnl/bxae046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  author       = {Kamareddine, Fairouz},
  doi          = {10.1093/comjnl/bxae046},
  journal      = {The Computer Journal},
  month        = {6},
  number       = {5},
  pages        = {1605-1606},
  shortjournal = {Comput. J.},
  title        = {Thematic editorial, it is hard to imagine a world without algorithms and data science},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Correction to: A semantic embedding enhanced topic model
for user-generated textual content modeling in social ecosystems.
<em>COMJNL</em>, <em>67</em>(4), 1604. (<a
href="https://doi.org/10.1093/comjnl/bxad073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  doi          = {10.1093/comjnl/bxad073},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1604},
  shortjournal = {Comput. J.},
  title        = {Correction to: A semantic embedding enhanced topic model for user-generated textual content modeling in social ecosystems},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024e). Correction to: Metaheuristic-enabled artificial neural
network framework for multimodal biometric recognition with local fusion
visual features. <em>COMJNL</em>, <em>67</em>(4), 1603. (<a
href="https://doi.org/10.1093/comjnl/bxad064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  doi          = {10.1093/comjnl/bxad064},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1603},
  shortjournal = {Comput. J.},
  title        = {Correction to: Metaheuristic-enabled artificial neural network framework for multimodal biometric recognition with local fusion visual features},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revisiting the software-efficient stream ciphers RCR-64 and
RCR-32. <em>COMJNL</em>, <em>67</em>(4), 1590–1602. (<a
href="https://doi.org/10.1093/comjnl/bxad084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The synchronous stream ciphers RCR-64 and RCR-32 designed by Sekar, Paul and Preneel are strengthened variants of the ciphers TPy and TPypy (designed by Biham and Seberry), respectively. The RCR ciphers have remained unbroken since they were published in 2007. In this paper, we present arguments that not only support the designers’ security claims but suggest, in general, that the ciphers are secure against several classes of cryptanalytic attacks. We find that the ciphers are best used with 256-bit keys and 384-bit IVs. We also suggest ways to protect software implementations of the RCR ciphers against (cache-)timing and processor flag attacks. Our performance evaluation suggests that the protected implementation of the RCR-64 encrypts long messages at speeds comparable to some of the fastest stream ciphers available today. Consequently, we find that the RCR ciphers may be well suited for PC-based applications in general and streaming audio / video applications in particular. This is the first paper to present a detailed study on the security and performance of the RCR ciphers.},
  archive      = {J_COMJNL},
  author       = {Joseph, Mabin and Sekar, Gautham and Balasubramanian, R},
  doi          = {10.1093/comjnl/bxad084},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1590-1602},
  shortjournal = {Comput. J.},
  title        = {Revisiting the software-efficient stream ciphers RCR-64 and RCR-32},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NDN-RBE: An accountable privacy aware access control
framework for NDN. <em>COMJNL</em>, <em>67</em>(4), 1572–1589. (<a
href="https://doi.org/10.1093/comjnl/bxad083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Named Data Networking (NDN) is an emerging network architecture. An important characteristic of NDN is its in-network cache, which enables Data packets to be available from multiple locations on the Internet. Hence the enforcement of access control mechanisms becomes even more critical in the NDN. This paper proposes a novel access control scheme referred to as Role-Based Encryption for NDN (NDN-RBE), which uses a broadcast encryption mechanism to achieve secure data access control. Our scheme uses the role inheritance property of the traditional Role-Based Access Control (RBAC) model to achieve efficient data access control over hierarchical content. This makes our scheme particularly suitable for large-scale real-world content-centric services like Netflix. Our scheme also supports additional design features such as anonymous signature-based authentication, batch signature verification and two types of privilege revocations. In addition, our formal security analysis demonstrates that our scheme is provably secure against Chosen Plaintext Attacks. Our performance and functionality comparison show that our scheme outperforms other notable existing works in terms of security, functionality, computation, communication and storage overhead. Furthermore, our experimental results show an improvement in content delivery time of the order of 15 percent compared with the other closely related works.},
  archive      = {J_COMJNL},
  author       = {Sultan, Nazatul Haque and Varadharajan, Vijay and Dulal, Saurab and Camtepe, Seyit and Nepal, Surya},
  doi          = {10.1093/comjnl/bxad083},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1572-1589},
  shortjournal = {Comput. J.},
  title        = {NDN-RBE: An accountable privacy aware access control framework for NDN},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reversible data hiding with pattern adaptive prediction.
<em>COMJNL</em>, <em>67</em>(4), 1564–1571. (<a
href="https://doi.org/10.1093/comjnl/bxad082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the area of reversible data hiding (RDH), one of the most popular techniques is prediction-error expansion (PEE), which hides data in the prediction errors with well-preserved image fidelity. The key to a successful PEE-based RDH implementation usually lies in prediction algorithms with high accuracy. Existing PEE-based RDH works often employ one single prediction algorithm, which is usually globally optimized, but with less consideration of the pixel distribution characteristics within local neighborhoods. In this manuscript, the technique of pattern adaptive prediction is proposed for pixel estimation according to the type of local binary pattern (LBP), which is obtained from the pixel’s eight neighborhood. Theoretically speaking, pattern-based predictors can be designed for each and every LBP patterns to create multiple prediction-error histograms (PEHs). However, the process of performance optimization with multiple PEHs requires extremely heavy computing power. To speed up the optimization process, LBP patterns are classified into various groups based on the degree of histogram concentration. Experiments demonstrate that the prediction accuracy is obviously improved and the image fidelity is well preserved.},
  archive      = {J_COMJNL},
  author       = {Yuan, Junying and Zheng, Huicheng and Ni, Jiangqun},
  doi          = {10.1093/comjnl/bxad082},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1564-1571},
  shortjournal = {Comput. J.},
  title        = {Reversible data hiding with pattern adaptive prediction},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An evaluation on the entropy supplying capability of
smartphone sensors. <em>COMJNL</em>, <em>67</em>(4), 1550–1563. (<a
href="https://doi.org/10.1093/comjnl/bxad081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random numbers are very important for the security of computer system. However, generating qualified random numbers is difficult because we cannot always successfully introduce dedicated random number hardware into computer system. Although most operating systems provide random number generation capabilities, the effective entropy supply is still dependent on the hardware platform including memory and clocks etc. However, obtaining hardware events such as clocks requires system privileges, which is not conducive for entropy estimation at the application layer. In contrast, data related to the sensor hardware can be extracted directly at the application layer. These sensor data contain some randomness and may be used as a noise source. In this way, applications can use these sensors to implement their own proprietary random number generators. Before taking these sensors as the noise source, it is necessary to fully evaluate their entropy supply capability. In this paper, 300 Android smartphones and 30 iOS smartphones are selected as samples and their sensor entropy supply capabilities are comprehensively evaluated. Based on the entropy evaluation results, we give some suggestions on how to generate random numbers using these sensor data. We first design a framework for evaluating the entropy supply capability for smartphone sensors, based on the min-entropy estimation method proposed in NIST SP 800-90B. According to this framework, we simulate stationary and mobile working states for each smartphone, and collect sufficient sensor data as the min-entropy estimation dataset. The min-entropy estimation results show that in the stationary working state, each ACCELEROMETER sensor data collection can obtain at least 1.5 bits of entropy in Android, while each GYROSCOPE sensor data collection can obtain at least 20 bits of entropy in iOS. In the mobile working state, each ACCELEROMETER sensor data collection can obtain at least 1.9 bits of entropy, while each GYROSCOPE sensor data acquisition in iOS system can obtain at least 27 bits of entropy. This means that we can still get a stable entropy output from the sensor even when the smartphone is in stationary working state. Statistical analysis of the data using cross correlation methods suggests it is hard for an attacker to guess or predict the random numbers generated by a smartphone through another smartphone put in the similar external environment.},
  archive      = {J_COMJNL},
  author       = {Zhang, Dinghua and Wu, Shihao and Li, Yang and Pan, Quan},
  doi          = {10.1093/comjnl/bxad081},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1550-1563},
  shortjournal = {Comput. J.},
  title        = {An evaluation on the entropy supplying capability of smartphone sensors},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CDNM: Clustering-based data normalization method for
automated vulnerability detection. <em>COMJNL</em>, <em>67</em>(4),
1538–1549. (<a href="https://doi.org/10.1093/comjnl/bxad080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key to deep learning vulnerability detection framework is pre-processing source code and learning vulnerability features. Traditional source code representation techniques take a complete normalization to user-defined symbols but ignore the semantic information associated with vulnerabilities. The current mainstream vulnerability feature learning model is Recurrent Neural Network (RNN), whose time-series structure determines its insufficient remote information acquisition capability. This paper proposes a new vulnerability detection framework to solve the above problems. We propose a new data normalization method in the source code pre-processing phase. The user-defined symbols are clustered using the unsupervised clustering algorithm K-means. The normalized classification is performed according to the clustering results, which preserves the primary semantic information in the source code and ensures the smoothness of the sample data. In the feature extraction stage, we input the source code after performing text representation into Bidirectional Encoder Representations for Transformers (BERT) for feature automation learning, which enhances semantic information extraction and remote information acquisition. Experimental results show that the vulnerability detection precision of this method is 18.3% higher than that of the current mainstream vulnerability detection framework in the real-world data collected by ourselves. Further, our method improves the precision of the state-of-the-art method by 4.2%.},
  archive      = {J_COMJNL},
  author       = {Wu, Tongshuai and Chen, Liwei and Du, Gewangzi and Zhu, Chenguang and Cui, Ningning and Shi, Gang},
  doi          = {10.1093/comjnl/bxad080},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1538-1549},
  shortjournal = {Comput. J.},
  title        = {CDNM: Clustering-based data normalization method for automated vulnerability detection},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A blockchain-based public key infrastructure for IoT-based
healthcare systems. <em>COMJNL</em>, <em>67</em>(4), 1531–1537. (<a
href="https://doi.org/10.1093/comjnl/bxad079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Secure exchange of data among the various stake holders of healthcare systems is of prime importance. As the size of the healthcare networks grew, several variants of Public Key Infrastructures (PKIs) were proposed as a means to achieve reliable authentication, confidentiality, non-repudiation, etc. The most prevalent approach to PKI has been the use of Certificate Authorities (CAs). But, events like the breach of the CA DigiNotar, and the ensuing fake certificates for Google, among other noteworthy high-profile domains, has cast doubts on the reliability of a CA, and therefore on PKIs modelled as CAs too. In this paper, we propose a new approach to a healthcare PKI modelled on a blockchain, incorporating Elliptic Curve Cryptographic methods for secure key generations.},
  archive      = {J_COMJNL},
  author       = {Joseph Antony, Amalan and Singh, Kunwar},
  doi          = {10.1093/comjnl/bxad079},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1531-1537},
  shortjournal = {Comput. J.},
  title        = {A blockchain-based public key infrastructure for IoT-based healthcare systems},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FedEVCP: Federated learning-based anomalies detection for
electric vehicle charging pile. <em>COMJNL</em>, <em>67</em>(4),
1521–1530. (<a href="https://doi.org/10.1093/comjnl/bxad078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle-to-Grid (V2G) is a technology that enables electric vehicles to use smart charging methods to harness low-cost and renewable energy when it is available, and obtain income by feeding energy back into the grid. With the rise of V2G technology, the use of electric vehicles has begun to increase dramatically, which relies on the reliable Electric Vehicle Charging Pile (EVCP). However, most EVCPs are online and networked, introducing many potential network threats, such as Electricity Theft, Identity Theft and False Data Injection etc. Prior work has mostly focused on machine learning, which is not able to effectively capture the relationships and structures in network traffic, making it difficult to deal with the propagation and infection of the novel network attacks. Moreover, most neural network models collect and transfer data from EVCPs to the central server for training, which makes the central server attractive to attackers. It poses a serious threat to user privacy. To address these issues, propose an anomaly detection model that incorporates Federated Learning and Deep Autoencoder, which can increase the amount and diversity of data used to train deep learning models without compromising privacy. The proposed model forms a layer-by-layer unsupervised representation learning algorithm by autoencoder stacking, while batch normalization of hidden layers accelerates the convergence of the model to avoid overfitting and local optima, and introduces an attention mechanism to enhance key features of sequences composed of data vectors to improve the accuracy rate. To prevent the risk of user privacy leakage on the central server, EVCP is allowed to retain local data for model training and send model parameters to the central server for constructing new global models. Experimental results show that the proposed scheme achieves improved detection accuracy with superior performance than other similar models.},
  archive      = {J_COMJNL},
  author       = {Lin, Zhaoliang and Li, Jinguo},
  doi          = {10.1093/comjnl/bxad078},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1521-1530},
  shortjournal = {Comput. J.},
  title        = {FedEVCP: Federated learning-based anomalies detection for electric vehicle charging pile},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exact short products from truncated multipliers.
<em>COMJNL</em>, <em>67</em>(4), 1514–1520. (<a
href="https://doi.org/10.1093/comjnl/bxad077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We sometimes need to compute the most significant digits of the product of small integers with a multiplier requiring much storage, e.g. a large integer (e.g. |$5^{100}$|⁠ ) or an irrational number ( ⁠|$\pi $|⁠ ). We only need to access the most significant digits of the multiplier—as long as the integers are sufficiently small. We provide an efficient algorithm to compute the range of integers given a truncated multiplier and a desired number of digits.},
  archive      = {J_COMJNL},
  author       = {Lemire, Daniel},
  doi          = {10.1093/comjnl/bxad077},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1514-1520},
  shortjournal = {Comput. J.},
  title        = {Exact short products from truncated multipliers},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A large-scale mobile traffic dataset for mobile application
identification. <em>COMJNL</em>, <em>67</em>(4), 1501–1513. (<a
href="https://doi.org/10.1093/comjnl/bxad076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With Internet access shifting from desktop-driven to mobile-driven, application-level mobile traffic identification has become a research hotspot. Although considerable progress has been made in this research field, two obstacles are hindering its further development. Firstly, there is a lack of sharable labeled mobile traffic datasets. Although it is easy to capture mobile traffic, labeling traffic at the application level is non-trivial. Besides, researchers usually hold a conservative attitude toward publishing their datasets for privacy concerns. Secondly, most of the datasets used by existing studies are inadequate to evaluate the proposed methods, since they usually have the problems of inaccurate labels, small scale and simple collection configurations. To tackle these two obstacles, a mobile traffic collection is carried out in this paper. The collected traffic has the advantages of large-scale data size, accurate application-level labels and diverse collection configurations. Then, the collected traffic is anonymized carefully to make it public. Several mobile traffic identification methods are compared based on our anonymized dataset, which proves the applicability of our dataset.},
  archive      = {J_COMJNL},
  author       = {Zhao, Shuang and Chen, Shuhui and Wang, Fei and Wei, Ziling and Zhong, Jincheng and Liang, Jianbing},
  doi          = {10.1093/comjnl/bxad076},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1501-1513},
  shortjournal = {Comput. J.},
  title        = {A large-scale mobile traffic dataset for mobile application identification},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Truncated differential attacks on symmetric primitives with
linear key schedule: WARP and orthros. <em>COMJNL</em>, <em>67</em>(4),
1483–1500. (<a href="https://doi.org/10.1093/comjnl/bxad075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In truncated differential cryptanalysis of symmetric primitives, a generalized framework is to search a distinguisher concerning part of output differences, like truncated differential distribution (TDD) on certain bits (e.g. a nibble) first, and then append several rounds before and after it to recover the secret key. The logarithmic likelihood ratio statistic with respect to the TDD is usually used to distinguish guessed key bits. In this paper, we study how to improve the effect of truncated differential cryptanalysis by considering key schedules of the attacked ciphers. It turns out that for a cipher with a simple key schedule, certain guessed subkey bits may reveal information of the master key, which will help build a stronger TDD distinguisher and reduce the key recovery complexity or attack more rounds. As a result, we explore heuristic techniques to search key-recovery-friendly TDDs and construct automatic search models based on MILP. The refined methods are applied to two recent designs of symmetric primitives, WARP and Orthros, together with peculiarities of their structures as well. For WARP, after making two observations on relations between certain differences with key bits, we propose an algorithm that can find TDDs with low complexities and having potentialities to cover more rounds. Consequently, we launch key recovery attacks on 24 to 27 rounds of WARP. When it comes to Orthros, we present a two-step search algorithm to balance the number of guessed key bits and TDDs, obtaining a key recovery attack on a 7-round variant of it in the weak-key setting. Finally, we perform several verification experiments on round-reduced versions of WARP and Orthros, and the experimental results are consistent with the theoretical distributions and the analysis of generalized key recovery attack framework.},
  archive      = {J_COMJNL},
  author       = {Hou, Shiqi and Wu, Baofeng and Wang, Shichang and Guo, Hao and Lin, Dongdai},
  doi          = {10.1093/comjnl/bxad075},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1483-1500},
  shortjournal = {Comput. J.},
  title        = {Truncated differential attacks on symmetric primitives with linear key schedule: WARP and orthros},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). KEFSAR: A solar-aware routing strategy for rechargeable IoT
based on high-accuracy prediction. <em>COMJNL</em>, <em>67</em>(4),
1467–1482. (<a href="https://doi.org/10.1093/comjnl/bxad074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high energy density of solar energy gives wireless sensor networks advantages in outdoor monitoring applications. However, long-term stable monitoring is challenging due to frequent weather changes, shading by buildings and trees, etc. The existing research usually uses two technologies to solve the above problems: (1) the energy prediction algorithm, and (2) the energy-aware routing strategy. However, in an actual deployment, frequent weather changes can significantly reduce the accuracy of the existing prediction algorithms. When using the algorithms as the support for energy-aware routing, the network lifetime is less than ideal. The existing routing strategies are in need of further improvement. Because of its lack of environmental adaptability, nodes consume energy quickly and have a high mortality rate. Therefore, aiming at the long-term stability of solar wireless sensor networks, this paper proposes a prediction algorithm based on classification and recurrent neural networks, and integrates the shadow judgement method from our previous research to correct the predicted values. Furthermore, we propose a routing optimization model that can flexibly adjust the target according to the solar intensity. The experimental results show that the prediction and routing scheduling algorithm can significantly improve the energy prediction accuracy (30–50%) and prolong the network lifetime (10–42%) in outdoor small sensor scenarios.},
  archive      = {J_COMJNL},
  author       = {Ma, Dongchao and Wang, Dongmei and Huang, Xiaofu and Hu, Yuekun and Ma, Li},
  doi          = {10.1093/comjnl/bxad074},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1467-1482},
  shortjournal = {Comput. J.},
  title        = {KEFSAR: A solar-aware routing strategy for rechargeable IoT based on high-accuracy prediction},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Super structure fault-tolerance assessment of the
generalized hypercube. <em>COMJNL</em>, <em>67</em>(4), 1457–1466. (<a
href="https://doi.org/10.1093/comjnl/bxad072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault-tolerant performance of a network is the prerequisite and guarantee for the normal operation of a network, which is often characterized by connectivity. Let |$H$| denote a connected subgraph of |$G$| and |$H^{*}$| denote the union of the set of all connected subgraphs of |$H$| and the set of the trivial graph. Super |$H$| -connectivity (resp. super |$H^{*}$| -connectivity) satisfies the conditions of both super connectivity and |$H$| -structure connectivity (resp. |$H$| -substructure connectivity). These two kinds of new connectivity provide a new metric to measure the fault-tolerance of the network, that is, the super structure fault-tolerance. The generalized hypercube |$G(m_{r}, m_{r-1},..., m_{1})$| is a universal topology of interconnection networks that contains other commonly used topologies and it has been applied in many data center networks because of its excellent qualities. In this paper, we research the super structure fault-tolerance of |$G(m_{r}, m_{r-1},..., m_{1})$| by studying super |$H$| -connectivity |$\kappa ^{\prime}(G|H)$| and super |$H^{*}$| -connectivity |$\kappa ^{\prime}(G|H^{*})$| for |$H\in \{K_{1,M},\ C_{3},\ C_{4},\ K_{4}\}$|⁠ .},
  archive      = {J_COMJNL},
  author       = {Shu, Chang and Wang, Yan and Fan, Jianxi and Wang, Guijuan},
  doi          = {10.1093/comjnl/bxad072},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1457-1466},
  shortjournal = {Comput. J.},
  title        = {Super structure fault-tolerance assessment of the generalized hypercube},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved related-key rectangle attacks on GIFT.
<em>COMJNL</em>, <em>67</em>(4), 1443–1456. (<a
href="https://doi.org/10.1093/comjnl/bxad071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GIFT is a lightweight cipher proposed by Banik et al . at CHES’17, motivated by the design strategy of PRESENT . GIFT-64[2021] is a variant of GIFT proposed by Sun et al. at EUROCRYPT’22 to achieve better resistance against differential attack while maintaining a similar security level against linear attack. At EUROCRYPT’22, Dong et al . proposed a new rectangle framework considering the key guessing strategies for linear key-schedule ciphers, and established a uniform automatic search model for the whole rectangle attack. In this paper, we extend it to be applicable to bit-oriented ciphers, and construct an automatic search model involved in the distinguisher and key-recovery phase for GIFT . Moreover, we utilize the key relations of the linear key-schedule to the model, and find some new distinguishers both for GIFT-64 and GIFT-64[2021] . To evaluate the probability more accurately, we propose a method to calculate the probability of the 2-round middle part which connects the boomerang distinguisher for GIFT , and apply it with the SAT method to evaluate the probability of the whole distinguishers. As a result, we search out a new 20-round related-key boomerang distinguisher for GIFT-64 , and achieve a 26-round attack with better time complexity than the best previous attack. For GIFT-64[2021] , we find a 20-round boomerang distinguisher and give the first 26-round rectangle attack under related-key scenario.},
  archive      = {J_COMJNL},
  author       = {Yu, Qingyuan and Qin, Lingyue and Dong, Xiaoyang and Jia, Keting},
  doi          = {10.1093/comjnl/bxad071},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1443-1456},
  shortjournal = {Comput. J.},
  title        = {Improved related-key rectangle attacks on GIFT},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Relinearization attack on LPN over large fields.
<em>COMJNL</em>, <em>67</em>(4), 1438–1442. (<a
href="https://doi.org/10.1093/comjnl/bxad070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate algebraic attacks on the Learning Parity with Noise ( ⁠|$\mathsf{LPN}$|⁠ ) problem over large fields in parameter settings relevant to building indistinguishability obfuscation in which the proportion of corrupted equations is inverse-polynomially sparse. Our aim was to obtain a subexponential algorithm using the Macaulay expansion and relinearization. Alas, we did not. Nevertheless, our findings suggest an interesting relation between runtime and the rank of the Macaulay expansion. The runtime of this attack is |$O\big(2^{d \log m}\big)$|⁠ , where |$m$| is the number of initial equations and |$d$| is the degree of the Macaulay expansion. If the resulting system of equations has sufficiently large rank, we show that solving the |$\mathsf{LPN}$| polynomial system requires an |$O(\sqrt{m})$| degree expansion, which would imply a subexponential attack. Under the (more widely believed) assumption that the expanded system is semi-regular, however, we show that an |$O(m)$| degree expansion is required to recover the secret vector. Since |$O(\sqrt{m})$| -degree expansions may not have sufficient rank, we propose a randomized algorithm which introduces carefully chosen equations that hold with high probability to increase the rank and improve the likelihood of a successful attack. We highlight the empirical and theoretical challenges in analyzing this approach. Our code is available at www.tinyurl.com/attacklpn .},
  archive      = {J_COMJNL},
  author       = {Lou, Paul and Sahai, Amit and Sivashankar, Varun},
  doi          = {10.1093/comjnl/bxad070},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1438-1442},
  shortjournal = {Comput. J.},
  title        = {Relinearization attack on LPN over large fields},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hybrid scheme combining duplications and LDPC decoding to
reduce NAND flash. <em>COMJNL</em>, <em>67</em>(4), 1425–1437. (<a
href="https://doi.org/10.1093/comjnl/bxad069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of NAND flash, the storage density has become larger and larger, but the reliability has become lower and lower. To address this problem, stronger error correction codes, low-density parity-check (LDPC) codes, are widely used in flash memory. However, the direct use of LDPC codes for error correction has a great impact on the read and write performance of SSDs. Given this, this paper proposes a new mentality that combines replica and LDPC decoding methods to improve the read performance of solid state drives (SSDs). Considering the characteristics of the quantization level of LDPC soft-decision decoding, this paper proposes the IDSD (LDPC soft-decision decoding and duplication method) and IDHSD (a method combining duplication and LDPC hybrid decoding)schemes. These schemes are mainly for hot data. When reading the data, the IDSD scheme needs to first judge whether it is soft-decision decoding. Then, according to the number of quantization levels recorded, it needs to select whether to use duplication or soft-decision decoding to correct the error data. The IDHSD scheme does not need to judge the soft-decision decoding but directly uses the duplication after the hard-decision decoding fails. Both schemes reduce the average response time to a certain extent and optimize the read performance of the SSD. The experimental results show that compared with the Native Scheme, the average response delay time of the IDSD scheme decreases by 5–30% and the IDHSD scheme slashes by 5–38%. Compared with the Traditional Refresh Scheme, the average response delay time of the IDSD scheme cuts down by 1–30% and the IDHSD scheme lowers by 1–29%.},
  archive      = {J_COMJNL},
  author       = {Zhang, Yaofang and Li, Peixuan and Xie, Ping},
  doi          = {10.1093/comjnl/bxad069},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1425-1437},
  shortjournal = {Comput. J.},
  title        = {A hybrid scheme combining duplications and LDPC decoding to reduce NAND flash},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EBiBa: A post-quantum hash-based signature with small
signature size in the continuous communication of large-scale data.
<em>COMJNL</em>, <em>67</em>(4), 1405–1424. (<a
href="https://doi.org/10.1093/comjnl/bxad068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present eBiBa (enhanced BiBa), a hash-based signature scheme with the smallest possible signature size, while ensuring high feasibility and security in a specific application model. Our scheme is tailored to address the communication requirement of a large-scale public data stream continuously disseminated between two participants while ensuring data source and data integrity authentication. To achieve these goals, firstly, we optimized the classical hash tree mode into a hybrid mode to efficiently perform public key authentication and eliminate the need for an authenticated channel to transmit large amounts of data, unlike the initial BiBa-based broadcast authentication protocol. Secondly, we employed a specific tweakable hash chain function to digest a batch of messages, reducing the required conditions for post-quantum existential unforgeability under adaptive chosen message attack (EUCMA) of eBiBa to a second-pre-image-resistance-like property instead of collision resistance. This results in reduced pre-computation in both key and signature generations. Thirdly, we utilized a forward-secure pseudorandom function to achieve forward-secure of the proposed scheme. Finally, we minimize the signature size through a series of procedures. Firstly, we select BiBa few-time signature as the underlying signature scheme since it is currently the few-time hash-based signature with the smallest signature size that we are aware of; in addition, the hybrid approach we employed can also significantly reduce the signature size compared to using a hash tree solely; for the hash tree structure, we design a specific authentication path in combination with the related communication model to further minimize the signature size; finally, we optimize the authentication approach to achieve the minimum signature size in a single transmission. Our construction minimizes the signature size in the aforementioned model, achieving a compression rate of 0.017 to 0.828 based on distinct values of parameters, as compared to XMSS-256. We also demonstrated that eBiBa can achieve post-quantum forward-secure and EUCMA security.},
  archive      = {J_COMJNL},
  author       = {Li, Lingyun and Lu, Xianhui and Wang, Kunpeng},
  doi          = {10.1093/comjnl/bxad068},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1405-1424},
  shortjournal = {Comput. J.},
  title        = {EBiBa: A post-quantum hash-based signature with small signature size in the continuous communication of large-scale data},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving science that uses code. <em>COMJNL</em>,
<em>67</em>(4), 1381–1404. (<a
href="https://doi.org/10.1093/comjnl/bxad067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As code is now an inextricable part of science it should be supported by competent Software Engineering, analogously to statistical claims being properly supported by competent statistics. If and when code avoids adequate scrutiny, science becomes unreliable and unverifiable because results — text, data, graphs, images, etc — depend on untrustworthy code. Currently, scientists rarely assure the quality of the code they rely on, and rarely make it accessible for scrutiny. Even when available, scientists rarely provide adequate documentation to understand or use it reliably. This paper proposes and justifies ways to improve science using code: 1.   Professional Software Engineers can help, particularly in critical fields such as public health, climate change and energy. 2.   ‘Software Engineering Boards,’ analogous to Ethics or Institutional Review Boards, should be instigated and used. 3.   The Reproducible Analytic Pipeline (RAP) methodology can be generalized to cover code and Software Engineering methodologies,        in a generalization this paper introduces called RAP + . RAP + (or comparable interventions) could be supported and or even required       in journal, conference and funding body policies. The paper’s Supplemental Material provides a summary of Software Engineering best practice relevant to scientific research, including further suggestions for RAP + workflows. ‘Science is what we understand well enough to explain to a computer.’ Donald E. Knuth in |$A=B$| [ 1 ] ‘I have to write to discover what I am doing.’ Flannery O’Connor, quoted in Write for your life [ 2 ] ‘Criticism is the mother of methodology.’ Robert P. Abelson in Statistics as Principled Argument [ 3 ] ‘From its earliest times, science has operated by being open and transparent about methods and evidence, regardless of which technology has been in vogue.’ Editorial in Nature [ 4 ]},
  archive      = {J_COMJNL},
  author       = {Thimbleby, Harold},
  doi          = {10.1093/comjnl/bxad067},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1381-1404},
  shortjournal = {Comput. J.},
  title        = {Improving science that uses code},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A break of barrier to classical differential fault attack on
the nonce-based authenticated encryption algorithm. <em>COMJNL</em>,
<em>67</em>(4), 1370–1380. (<a
href="https://doi.org/10.1093/comjnl/bxad066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It had always been believed that there was an inherent barrier to Differential Fault Attack (DFA) on the nonce-based authenticated encryption algorithm. At CHES 2016, Saha et al. proposed an Internal Differential Fault Attack on a parallelizable counter-mode algorithm. They induce the attack to classical DFA at the expense of one more fault injection in every encryption process. In this paper, we propose the DFA on HYENA, which is a nonce-based authenticated encryption mode for GIFT-128. Our work is the first pure classical DFA on a nonce-based authenticated encryption algorithm with only one fault injected in every decryption process. Firstly, we give the DFA on GIFT-128 with a fault injected into the 39th-round input. Based on this work, we inject a fault in the underlying GIFT-128 of a HYENA decryption process and make this decryption process still generate the correct tag and output plaintext. This makes the necessary conditions of DFA satisfied. Experiments show that at most 56 key bits of HYENA can be recovered with only a few faulty ciphertexts. In addition, our fault injection is easier to achieve than most other work about fault attack, because the injection location is relatively random and the fault type can be arbitrary. It should be noted that the left 72 key bits cannot be recovered in this way.},
  archive      = {J_COMJNL},
  author       = {Liu, Shuai and Ren, Jizhou and Guan, Jie and Hu, Bin and Ma, Sudong and Bai, Hao},
  doi          = {10.1093/comjnl/bxad066},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1370-1380},
  shortjournal = {Comput. J.},
  title        = {A break of barrier to classical differential fault attack on the nonce-based authenticated encryption algorithm},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards accurate smartphone localization using CSI
measurements. <em>COMJNL</em>, <em>67</em>(4), 1361–1369. (<a
href="https://doi.org/10.1093/comjnl/bxad065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In comparison with capturing channel state information (CSI) measurements via a laptop or desktop, using a smartphone to collect CSI measurements incurs the restriction of working with a single access point and significant signal distortions, resulting in limited information for smartphone localization. Therefore, this paper intends to leverage as much available localization information as possible by ( ⁠|$1$|⁠ ) shifting the WiFi frequency from |$2.4$| to |$5$| GHz; ( ⁠|$2$|⁠ ) calibrating the noisy CSI measurements and ( ⁠|$3$|⁠ ) fusing both amplitudes and phases of the CSI measurements, so as to enhance localization accuracy. Specifically, we first filter out distorted CSI measurements based on their distribution characteristics, then apply the advanced uniform manifold approximation and projection method to refine the mapping relations from a high-dimensional fingerprint space to a low-dimensional location space, and design a location fusion algorithm based on the continuous feature scaling model, which is able to distinguish two locations with similar fingerprints. Extensive experimental results show that the localization accuracy of the proposed approach outperforms the state-of-the-art counterparts by at least |$15.5$| and |$18.7\%$| using two off-the-shelf smartphones.},
  archive      = {J_COMJNL},
  author       = {Yang, Runze and Huang, Baoqi and Xu, Zhendong and Jia, Bing and Xu, Gang},
  doi          = {10.1093/comjnl/bxad065},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1361-1369},
  shortjournal = {Comput. J.},
  title        = {Towards accurate smartphone localization using CSI measurements},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial-aware multi-directional autoencoder for
pre-training. <em>COMJNL</em>, <em>67</em>(4), 1346–1360. (<a
href="https://doi.org/10.1093/comjnl/bxad063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Transformers for pre-trained models explore semantic context and spatial relationships for images, which heavily depend on how you select image patches. In this paper, we propose a novel Spatial-aware Multi-directional Patches Multi-cycle Autoencoder (SMPMA) for a pre-trained model that brings the following benefits: (1) Spatial-aware Multi-directional (SM) patches are created with multi-directional spatial locations, transforming a whole image autoencoder problem into a short-span image patches autoencoder problem; (2) SM patches admit a self-cycle autoencoder alignment learning for the first stage and a cross-cycle interaction learning for the second stage, which makes patches align and interact for optimization; (3) SM patches enable to explore local object features and correlation distribution of adjacent pixels by taking arbitrary sampled patches as inputs. Experimental results on four downstream tasks show that our model can achieve state-of-the-art performance over the tasks of image generation, image classification and semantic segmentation.},
  archive      = {J_COMJNL},
  author       = {Yang, Weiwei and Liang, Shangsong and Yin, Jian},
  doi          = {10.1093/comjnl/bxad063},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1346-1360},
  shortjournal = {Comput. J.},
  title        = {Spatial-aware multi-directional autoencoder for pre-training},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A concept forensic methodology for the investigation of IoT
cyberincidents. <em>COMJNL</em>, <em>67</em>(4), 1324–1345. (<a
href="https://doi.org/10.1093/comjnl/bxad062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number of Internet of Things (IoT) forensic investigations has increased considerably over recent years due to the weak nature of the security measures of its devices. In order to ensure the effectiveness and completeness of their examinations, investigators rely on forensic models, frameworks and methodologies. However, given the novelty of the environment, the existing ones are not refined enough, and the conventional counterparts do not satisfy the requirements of the IoT. Consequently, further improvements are needed in order for a more suitable IoT methodology to be designed. After reviewing the proposals from the research community for the development of procedures for performing IoT investigations, this article presents a practical concept methodology for conducting IoT forensic investigations that details step by step the whole examination process from its opening to its closing. In order to test its effectiveness and feasibility, it is submitted to a theoretical, a practical and a hybrid evaluation. Firstly, by comparing its level of detail, practicality and content with the related work. Secondly, by assessing its performance in two practical scenarios that depict real-life forensic investigations and the challenges that they present. And, finally, by studying how the existing models from the research community would have behaved in these cases. After performing these three different evaluations, it can be concluded that the results achieved by the proposed methodology were satisfactory, confirmed the feasibility of the proposal and showed clear benefits compared with the related work in terms of practicality and level of detail.},
  archive      = {J_COMJNL},
  author       = {Castelo Gómez, Juan Manuel and Carrillo-Mondéjar, Javier and Roldán-Gómez, José and Martínez Martínez, José Luis},
  doi          = {10.1093/comjnl/bxad062},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1324-1345},
  shortjournal = {Comput. J.},
  title        = {A concept forensic methodology for the investigation of IoT cyberincidents},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A toolbox for migrating the blockchain-based application
from ethereum to hyperledger fabric. <em>COMJNL</em>, <em>67</em>(4),
1309–1323. (<a href="https://doi.org/10.1093/comjnl/bxad061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The low transaction capacity, high transaction cost and long-term privacy concerns of the current Ethereum platform are notorious. Developers are seeking alternative blockchain platforms to migrate their blockchain-based applications to reduce their applications’ use-cost and improve their applications’ user experience. The Hyperledger Fabric (HLF) platform with resiliency, flexibility, scalability and confidentiality is preferred for developers to migrate their Ethereum blockchain-based applications. However, it is laborious for developers to migrate blockchain-based applications from the Ethereum platform to the HLF platform. In this paper, we first propose a complete and secure migration solution to ease the migration process. The main idea of our solution is to design a toolbox to help developers automatically eliminate the adverse effects that the differences between Ethereum and HLF may bring to the migrated application. Developers with the toolbox can migrate the application with little time and minimal modification. It is theoretically proved that the migrated application with the toolbox is secure. Besides, a prototype of the toolbox is implemented. The extensive experiments demonstrate that the time for the migration process is acceptable, and the toolbox has little impact on the migrated application’s performance.},
  archive      = {J_COMJNL},
  author       = {Zhai, Zhonghao and Shen, Subin and Mao, Yanqin},
  doi          = {10.1093/comjnl/bxad061},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1309-1323},
  shortjournal = {Comput. J.},
  title        = {A toolbox for migrating the blockchain-based application from ethereum to hyperledger fabric},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CSFL: Cooperative security aware federated learning model
using the blockchain. <em>COMJNL</em>, <em>67</em>(4), 1298–1308. (<a
href="https://doi.org/10.1093/comjnl/bxad060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a focus of research in the area of privacy protection since it does not have the privacy issues that arise from data concentration. Although its emergence has attracted widespread attention from academia and industry, existing works on FL still face security challenges. FL can be considered as a cooperative-based task to achieve global model sharing. However, the model raises issues of cooperative security, such as free-riding and poisoning attacks. Therefore, we focus on the behavior of participants with strong cooperative relationships and build a Cooperative Security-aware Federated Learning model using blockchain. In addition, we propose a credit-based economic model including profit and punishment mechanisms to ensure fairness and security among participants. Furthermore, for data privacy, we develop a participation permission strategy to protect the privacy of participants through proxy re-encryption and homomorphic encryption. Finally, the simulation results of the real datasets show that the proposed scheme achieves a good performance in security and accuracy.},
  archive      = {J_COMJNL},
  author       = {Zhang, Jiaomei and Ye, Ayong and Chen, Jianwei and Zhang, Yuexin and Yang, Wenjie},
  doi          = {10.1093/comjnl/bxad060},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1298-1308},
  shortjournal = {Comput. J.},
  title        = {CSFL: Cooperative security aware federated learning model using the blockchain},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid optimal ensemble SVM forest classifier for task
offloading in mobile cloud computing. <em>COMJNL</em>, <em>67</em>(4),
1286–1297. (<a href="https://doi.org/10.1093/comjnl/bxad059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile devices (MDs) are becoming more prevalent and their battery life is optimised by offloading tasks to cloud servers. However, communication costs must be considered when offloading tasks. To make task offloading worthwhile, it is important to measure the energy consumed during communication activities. Thus, a heterogeneous framework is developed to enhance the energy efficiency of smartphones by analysing parameters such as task and non-task offloading, local cloudlets, radio access networks and remote cloud servers. This paper proposes a task offloading framework that uses a novel algorithm, the Hybrid Red Fox Flow Direction-based Ensemble SVM Forest Classifier, to enhance the system parameters and schedule tasks in offloading cloud computing conditions. The multi-objective function aims to improve user satisfaction by maximising resource utilisation and minimising function. The framework was tested in the Cloudsim simulation tool and compared with different techniques, with the results demonstrating its superiority in terms of energy efficiency and system performance. The proposed framework can optimise the energy efficiency of MDs and improve battery life.},
  archive      = {J_COMJNL},
  author       = {Subramaniam, Erana Veerappa Dinesh and Krishnasamy, Valarmathi},
  doi          = {10.1093/comjnl/bxad059},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1286-1297},
  shortjournal = {Comput. J.},
  title        = {Hybrid optimal ensemble SVM forest classifier for task offloading in mobile cloud computing},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A genetic algorithm based scheduling method with carrier
aggregation in 5G networks. <em>COMJNL</em>, <em>67</em>(4), 1279–1285.
(<a href="https://doi.org/10.1093/comjnl/bxad058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, 5G has become an important mechanism to serve a growing amount of users in the network, which provides enhanced mobile broadband (eMBB) and ultra-reliable low latency communications (URLLC) services. Meanwhile, Carrier Aggregation (CA) is a promising 5G technology which lets users aggregate fragmented carrier components (CCs) to enhance transmission efficiency. This paper addresses 5G scheduling problem in the CA-enabled eMBB–URLLC coexistence network using genetic algorithm (GA). The key idea is first treating the serving time slot of each user equipment (UE) as a gene and combining all genes to form a chromosome, then generating a set of chromosomes as the initial population and letting the population evolve to iteratively select the fittest one as the resource scheduling decision. Up to now, this is the first work using GA for solving the scheduling problem in the CA-enabled 5G network. Simulation results show that our proposed method can serve the most UEs ( ⁠|$4\%$|⁠ ) with the highest throughput ( ⁠|$7.1\%$|⁠ ) and lower delay.},
  archive      = {J_COMJNL},
  author       = {Hsu, Ching-Kuo},
  doi          = {10.1093/comjnl/bxad058},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1279-1285},
  shortjournal = {Comput. J.},
  title        = {A genetic algorithm based scheduling method with carrier aggregation in 5G networks},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bandwidth-efficient zero-knowledge proofs for threshold
ECDSA. <em>COMJNL</em>, <em>67</em>(4), 1265–1278. (<a
href="https://doi.org/10.1093/comjnl/bxad057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In most threshold Elliptic Curve Digital Signature Algorithm (ECDSA) signatures using additively homomorphic encryption, the zero-knowledge (ZK) proofs related to the ciphertext or the message space are the bottleneck in terms of bandwidth as well as computation time. In this paper, we propose a compact ZK proof for relations related to the Castagnos–Laguillaumie (CL) encryption, which is 33% shorter and 29% faster than the existing work in PKC 2021. We also give new ZK proofs for relations related to homomorphic operations over the CL ciphertext. These new ZK proofs are useful to construct a bandwidth-efficient universal composable-secure threshold ECDSA without compromising the proactive security and the non-interactivity. In particular, we lowered the communication and computation cost of the key refresh algorithm in the Paillier-based counterpart from |$O(n^3)$| to |$O(n^2)$|⁠ . Considering a 5-signer setting, the bandwidth is better than the Paillier-based counterpart for up to 99, 95 and 35% for key generation, key refreshment and pre-signing, respectively.},
  archive      = {J_COMJNL},
  author       = {Cui, Handong and Chan, Kwan Yin and Yuen, Tsz Hon and Kang, Xin and Chu, Cheng-Kang},
  doi          = {10.1093/comjnl/bxad057},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1265-1278},
  shortjournal = {Comput. J.},
  title        = {Bandwidth-efficient zero-knowledge proofs for threshold ECDSA},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stealth address schemes with fast retrievability based on
subgroup membership assumptions related to factoring. <em>COMJNL</em>,
<em>67</em>(4), 1253–1264. (<a
href="https://doi.org/10.1093/comjnl/bxad056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stealth address is a known technique to ensure the privacy (anonymity) of a recipient participating in a certain transaction in a distributed blockchain scenario. However, most existing stealth address schemes require linear judge time and search time |$\mathcal{O}(n)$|⁠ , where |$n$| is the number of transactions of a certain block, so the only way to claim transactions for a recipient is to traverse the transaction list to find out whether an ever-arrived transaction belongs to him. To overcome this drawback, we proposed the notion of Fast Stealth Address (FSA), a novel approach that simultaneously preserves privacy and improves search efficiency of recipients. We give a generic construction of FSA scheme under subgroup membership assumption related to factoring and instantiate concrete schemes based on specific number-theoretic assumptions. Our framework mainly improves on two aspects: (i) allowing constant recognize time |$\mathcal{O}(1)$| to judge whether a certain block contains recipient’s transactions and (ii) allowing logarithmic search time |$\mathcal{O}(\log{n})$| to find out the precise transactions intended for a recipient. We formalize the security model of an FSA scheme and provide provable security analysis to ensure the security of our constructions. Besides, we implement our schemes to measure their real-world performance on several metrics and give comparison results to stealth address scheme utilized by Monero.},
  archive      = {J_COMJNL},
  author       = {Wang, Xin and Lin, Li and Wang, Yao},
  doi          = {10.1093/comjnl/bxad056},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1253-1264},
  shortjournal = {Comput. J.},
  title        = {Stealth address schemes with fast retrievability based on subgroup membership assumptions related to factoring},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient comparison of independence structures of
log-linear models. <em>COMJNL</em>, <em>67</em>(4), 1226–1252. (<a
href="https://doi.org/10.1093/comjnl/bxad054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Log-linear models are a family of probability distributions which capture relationships between variables. They have been proven useful in a wide variety of fields such as epidemiology, economics and sociology. The interest in using these models is that they are able to capture context-specific independencies, relationships that provide richer structure to the model. Many approaches exist for automatic learning of the independence structure of log-linear models from data. The methods for evaluating these approaches, however, are limited, and are mostly based on indirect measures of the complete density of the probability distribution. Such computation requires additional learning of the numerical parameters of the distribution, which introduces distortions when used for comparing structures. This work addresses this issue by presenting the first measure for the direct and efficient comparison of independence structures of log-linear models. Our method relies only on the independence structure of the models, which is useful when the interest lies in obtaining knowledge from said structure, or when comparing the performance of structure learning algorithms, among other possible uses. We present proof that the measure is a metric, and a method for its computation that is efficient in the number of variables of the domain.},
  archive      = {J_COMJNL},
  author       = {Strappa, Jan and Bromberg, Facundo},
  doi          = {10.1093/comjnl/bxad054},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1226-1252},
  shortjournal = {Comput. J.},
  title        = {Efficient comparison of independence structures of log-linear models},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An ontology as support for specification of non-functional
requirements of AAL systems considering compliance aspects.
<em>COMJNL</em>, <em>67</em>(4), 1211–1225. (<a
href="https://doi.org/10.1093/comjnl/bxad053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ambient assisted living (AAL) is a technological approach that emerged to meet the demands of seniors and people with disabilities. Because they are considered complex and multidisciplinary systems, it is necessary to identify and define which elements need to compose these systems. These challenges and the need for assistance in specifying requirements have led the academic community to explore and establish new approaches for the development of AAL systems. For this, there is the integration of different areas of knowledge, such as ontology, requirements engineering and compliance. Therefore, this work presents a central ontology to support the specification of requirements in AAL systems, where the elements that are part of the type of system are integrated. With the use of this ontology, it was possible to develop a domain ontology for vertical elevation platforms, where a validation was carried out with the industry in relation to the elements that constitute it and, later, a scenario was built to verify the syntactic correction of ontology. With the use of ontology, it will be possible to standardize the understanding of the associated terms and, at the same time, verify the relationships between the elements and help the designer in the requirements specification stage.},
  archive      = {J_COMJNL},
  author       = {Silva, Timóteo G and Alencar, Fernanda},
  doi          = {10.1093/comjnl/bxad053},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {4},
  pages        = {1211-1225},
  shortjournal = {Comput. J.},
  title        = {An ontology as support for specification of non-functional requirements of AAL systems considering compliance aspects},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Correction to: Hitting times of random walks on edge corona
product graphs. <em>COMJNL</em>, <em>67</em>(3), 1210. (<a
href="https://doi.org/10.1093/comjnl/bxad055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  doi          = {10.1093/comjnl/bxad055},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {1210},
  shortjournal = {Comput. J.},
  title        = {Correction to: Hitting times of random walks on edge corona product graphs},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024f). Correction to: Perceptual image hashing based on canny
operator and tensor for copy-move forgery detection. <em>COMJNL</em>,
<em>67</em>(3), 1209. (<a
href="https://doi.org/10.1093/comjnl/bxad032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  doi          = {10.1093/comjnl/bxad032},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {1209},
  shortjournal = {Comput. J.},
  title        = {Correction to: Perceptual image hashing based on canny operator and tensor for copy-move forgery detection},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pancyclic and hamiltonian properties of dragonfly networks.
<em>COMJNL</em>, <em>67</em>(3), 1201–1208. (<a
href="https://doi.org/10.1093/comjnl/bxad052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dragonfly networks have significant advantages in data exchange due to the small network diameter, low cost and modularization. A graph |$G$| is |$vertex$| - |$pancyclic$| if for any vertex |$u\in V(G)$|⁠ , there exist cycles through |$u$| of every length |$\ell $| with |$3\leq \ell \leq |V(G)|$|⁠ . A graph |$G$| is |$Hamiltonian$| - |$connected$| if there exists a Hamiltonian path between any two distinct vertices |$u,v\in V(G)$|⁠ . In this paper, we mainly research the pancyclic and Hamiltonian properties of the dragonfly network |$D(n,h)$|⁠ , and find that it is Hamiltonian with |$n\geq 1,\,\,h\geq 2$|⁠ , pancyclic, vertex-pancyclic and Hamiltonian-connected with |$n\geq 4,\,\,h\geq 2$|⁠ .},
  archive      = {J_COMJNL},
  author       = {Huo, Jin and Yang, Weihua},
  doi          = {10.1093/comjnl/bxad052},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {1201-1208},
  shortjournal = {Comput. J.},
  title        = {Pancyclic and hamiltonian properties of dragonfly networks},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SnorkelPlus: A novel approach for identifying relationships
among biomedical entities within abstracts. <em>COMJNL</em>,
<em>67</em>(3), 1187–1200. (<a
href="https://doi.org/10.1093/comjnl/bxad051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying relationships between biomedical entities from unstructured biomedical text is a challenging task. SnorkelPlus has been proposed to provide the flexibility to extract these biomedical relations without any human effort. Our proposed model, SnorkelPlus, is aimed at finding connections between gene and disease entities. We achieved three objectives: (i) extract only gene and disease articles from NCBI’s, PubMed or PubMed central database, (ii) define reusable label functions and (iii) ensure label function accuracy using generative and discriminative models. We utilized deep learning methods to achieve label training data and achieved an AUROC of 85.60% for the generated gene and disease corpus from PubMed articles. Snorkel achieved an AUPR of 45.73%, which is +2.3% higher than the baseline model. We created a gene–disease relation database using SnorkelPlus from approximately 29 million scientific abstracts without involving annotated training datasets. Furthermore, we demonstrated the generalizability of our proposed application on abstracts of PubMed articles enriched with different gene and disease relations. In the future, we plan to design a graphical database using Neo4j.},
  archive      = {J_COMJNL},
  author       = {Kumar, Ashutosh and Sharaff, Aakanksha},
  doi          = {10.1093/comjnl/bxad051},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {1187-1200},
  shortjournal = {Comput. J.},
  title        = {SnorkelPlus: A novel approach for identifying relationships among biomedical entities within abstracts},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multiple QoS metrics-aware virtual network embedding
algorithm. <em>COMJNL</em>, <em>67</em>(3), 1171–1186. (<a
href="https://doi.org/10.1093/comjnl/bxad050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a key issue of network virtualisation, virtual network embedding (VNE) aims to embed multiple virtual network requests (VNRs) from different applications onto the substrate network effectively. In real networks, about 90% of traffic is generated by different quality of service (QoS) sensitive applications. However, most existing VNE algorithms do not account for the difference. Although several VNE algorithms considered the delay metric of applications, they usually provide strict delay guarantees for all VNRs, leading to a low VNR acceptance ratio. In this paper, we focus on the VNE problem involving multiple QoS metrics and propose a multiple QoS metrics-aware VNE algorithm based on reinforcement learning (RLQ-VNE). We first classify VNRs according to their different requirements for multiple QoS metrics including delay, jitter and packet loss rate, and then introduce reinforcement learning to implement differentiated VNE. Specifically, RLQ-VNE provides strict QoS guarantees for the VNRs with high-level QoS requirements and provides lower QoS guarantees for the VNRs with low-level QoS requirements, thus balancing the QoS guarantee and request acceptance ratio. Simulation results from multiple experimental scenarios show that RLQ-VNE improves the request acceptance ratio and network resource utilisation by sacrificing less QoS.},
  archive      = {J_COMJNL},
  author       = {Lu, Meilian and Li, Meng},
  doi          = {10.1093/comjnl/bxad050},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {1171-1186},
  shortjournal = {Comput. J.},
  title        = {A multiple QoS metrics-aware virtual network embedding algorithm},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Quantum bit commitment without quantum memory.
<em>COMJNL</em>, <em>67</em>(3), 1163–1170. (<a
href="https://doi.org/10.1093/comjnl/bxad049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commitment scheme is a fundamental cryptographic primitive that serve as building blocks for many other two-party protocols. In this paper, we propose a novel quantum bit commitment scheme, which is secure and does not require quantum memory. Our scheme processes the quantum information using coherent states and unambiguous state discrimination (USD) measurements, which can be experimentally realized by linear optics and photon detectors. We ensure the unconditionally hiding and binding property by preventing both Alice and Bob from obtaining complete information about the commitment in committing stage, and eliminate the requirement for quantum memory by performing USD measurements and phase shifts immediately after receiving the coherent states.},
  archive      = {J_COMJNL},
  author       = {Xu, Lidong and Wang, Mingqiang and Qin, Jing},
  doi          = {10.1093/comjnl/bxad049},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {1163-1170},
  shortjournal = {Comput. J.},
  title        = {Quantum bit commitment without quantum memory},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Chronos: An efficient asynchronous byzantine ordered
consensus. <em>COMJNL</em>, <em>67</em>(3), 1153–1162. (<a
href="https://doi.org/10.1093/comjnl/bxad048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Byzantine ordered consensus , introduced by Zhang et al. (OSDI 2020), is a new consensus primitive that additionally guarantees a correctness specification of transaction order, allowing nodes to assign fairly an ordering indicator to the committed transaction. Zhang et al. also presented a concrete Byzantine ordered consensus protocol called Pompē in the partially synchronous network model. However, Pompē cannot prevent an adversary from manipulating message delivery time. In this paper, we present Chronos, the first Byzantine ordered consensus protocol in the asynchronous network model, where an adversary can arbitrarily manipulate message delivery time. To construct Chronos, we propose a variant of asynchronous common subset called signal asynchronous common subset protocol, which guarantees the liveness of Chronos. We implement both Chronos and its baseline HoneyBadgerBFT using Go language and deploy them on 100 Amazon t3.medium instances distributed throughout 10 regions across the world. The experimental results show that Chronos is more efficient than HoneyBadgerBFT for small network, achieving peak throughput of 59 368 tx/s when the batch size is 100 000 and the number of nodes is 4, while the peak of HoneyBadgerBFT is 57 077 tx/s.},
  archive      = {J_COMJNL},
  author       = {Zhang, Zongyang and Zhang, Lingyue and Wang, Zhuo and Li, Yichen and Lu, Rongxing and Yu, Yong},
  doi          = {10.1093/comjnl/bxad048},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {1153-1162},
  shortjournal = {Comput. J.},
  title        = {Chronos: An efficient asynchronous byzantine ordered consensus},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive review of brain tumour detection mechanisms.
<em>COMJNL</em>, <em>67</em>(3), 1126–1152. (<a
href="https://doi.org/10.1093/comjnl/bxad047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brain is regarded as the central part of the human body and has a very complicated structure. The abnormal growth of tissue inside the brain is called a brain tumour. Tumour detection at an early stage is the most difficult task in the discipline of health. In this review article, the authors have deeply analysed and reviewed the brain tumour detection mechanisms which include manual, semi- and fully automated techniques. Today, fully automated mechanisms apply deep learning (DL) methods for tumour detection in brain magnetic resonance images (MRIs). This paper deals with previously published research articles relevant to various brain tumour detection techniques. Review of various types of tumours, MRI modalities, datasets, filters, segmentation methods and DL techniques like long short-term memory, gated recurrent unit network, convolution neural network, auto encoder, deep belief network, recurrent neural network, generative adverse network and deep stacking networks have been included in this paper. It has been observed from the analysis that the use of DL techniques in the detection of brain tumours improves accuracy. Finally, this paper reveals research gaps, limitations of existing methods, challenges in tumour detection and contributions of the proposed article.},
  archive      = {J_COMJNL},
  author       = {Ramtekkar, Praveen Kumar and Pandey, Anjana and Pawar, Mahesh Kumar},
  doi          = {10.1093/comjnl/bxad047},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {1126-1152},
  shortjournal = {Comput. J.},
  title        = {A comprehensive review of brain tumour detection mechanisms},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Breast cancer mass classification using machine learning,
binary-coded genetic algorithms and an ensemble of deep transfer
learning. <em>COMJNL</em>, <em>67</em>(3), 1111–1125. (<a
href="https://doi.org/10.1093/comjnl/bxad046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The diagnosis of breast cancer (BC) as early as possible is crucial for increasing the survival rate. Mammography enables finding the breast tissue changes years before they could develop into cancer symptoms. In this study, machine learning methods for BC mass pathology classification have been investigated using the radiologists’ mass annotations on the screen-film mammograms of the Breast Cancer Digital Repository (BCDR). The performances of precomputed features in the BCDR and discrete wavelet transform followed by Radon transform have been investigated by using four sequential feature selections and three genetic algorithms. Feature fusion from craniocaudal and mediolateral oblique views was shown to increase the performance of the classifier. Mass classification has been implemented by deep transfer learning (DTL) using the weights of ResNet50, NASNetLarge and Xception networks. An ensemble of DTL (EDTL) was shown to have higher classification performance than the DTL models. The proposed EDTL has area under the receiver operating curve (AUC) scores of 0.8843 and 0.9089 for mass classification on the region of interest (ROI) and ROI union datasets, respectively. The proposed EDTL has the highest BC mass classification AUC score on the BCDR to date and may be useful for other datasets.},
  archive      = {J_COMJNL},
  author       = {Tiryaki, Volkan Müjdat and Tutkun, Nedim},
  doi          = {10.1093/comjnl/bxad046},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {1111-1125},
  shortjournal = {Comput. J.},
  title        = {Breast cancer mass classification using machine learning, binary-coded genetic algorithms and an ensemble of deep transfer learning},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic virtual machine scheduling using residual optimum
power-efficiency in the cloud data center. <em>COMJNL</em>,
<em>67</em>(3), 1099–1110. (<a
href="https://doi.org/10.1093/comjnl/bxad045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud data center serves tremendous workload demand due to the ever-increasing usage of internet services. Scheduling of these workloads over the physical servers is the combinatorial problem that resembles to an NP-complete problem. Furthermore, the workload is dynamic and changes at each scheduling interval result in high power consumption and Service Level Agreement (SLA) violation. Virtual Machine (VM) migrations provide the opportunity to balance this dynamic workload. However, it results in additional power consumption and performance loss. Therefore, this paper aims to find the optimal VM scheduling with minimum VM migration to minimize power consumption and ensure the SLA. This paper proposes a Residual Optimum Power Efficiency (ROPE) aware Improved Clonal Selection Algorithm (ICSA) for dynamic VM scheduling. ICSA-ROPE algorithm finds optimal VM schedules at each scheduling interval guided by two optimization functions Total Datacenter Residual Optimum Power-Efficiency (TDCROPE) and VM Migration Cost (VMC). TDCROPE ensures that servers operate at optimum power efficiency as they consume less power and are less prone to SLA violation, while VMC ensures to find optimal VM schedule with minimum VM migrations. The proposed approach is implemented on a CloudSim simulator, and results show that the ICSA-ROPE is 95.54 %,90.34% and 88.49% more significant in terms of performance efficiency than the LrMmt, DthMf and VMS-MCSA.},
  archive      = {J_COMJNL},
  author       = {Ajmera, Kashav and Kumar Tewari, Tribhuwan},
  doi          = {10.1093/comjnl/bxad045},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {1099-1110},
  shortjournal = {Comput. J.},
  title        = {Dynamic virtual machine scheduling using residual optimum power-efficiency in the cloud data center},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A white-box implementation of SM4 with self-equivalence
encoding. <em>COMJNL</em>, <em>67</em>(3), 1087–1098. (<a
href="https://doi.org/10.1093/comjnl/bxad044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {White-box implementation can ensure the security of cryptographic algorithm in white-box attack environment without changing the inputs and outputs of the original algorithm. Most existing white-box implementations construct a series of lookup tables to protect the key. However, with the development of white-box attack techniques, many white-box implementations have been proved to be insecure. In this paper, a new white-box implementation of SM4 is proposed, which is based on an equivalent partial SPN structure of the SM4 algorithm. Our implementation includes three types of table lookup operations and XOR operations. The round keys are obfuscated with the self-equivalences of the S-box and random affine encodings. Security analysis shows that our implementation can resist BGE-type attack, the attack based on affine equivalence algorithm, the structure attack, the collision attack and differential computational analysis. Furthermore, our scheme requires 8.125 MB of memory.},
  archive      = {J_COMJNL},
  author       = {Chen, Jie and Luo, Yinuo and Liu, Jun and Wang, Chao and Zhang, Yueyu and Dong, Xiaoli},
  doi          = {10.1093/comjnl/bxad044},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {1087-1098},
  shortjournal = {Comput. J.},
  title        = {A white-box implementation of SM4 with self-equivalence encoding},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Resilient vector consensus over random dynamic networks
under mobile malicious attacks. <em>COMJNL</em>, <em>67</em>(3),
1076–1086. (<a href="https://doi.org/10.1093/comjnl/bxad043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the problem of resilient vector consensus for a group of dynamic agents against mobile malicious attacks. As real networks often operate under random environment and noises, we approach this problem by considering general random dynamic networks with weighted directed topologies. We propose three types of mobile attack models, which differ in the timing of moving of attackers and the capability of detecting such moving. By employing distributed discrete-time algorithms, the Lyapunov theory and martingale convergence theorem, resilient vector consensus is shown to be reached for all three models when the underlying network satisfies certain stochastic robustness conditions.},
  archive      = {J_COMJNL},
  author       = {Shang, Yilun},
  doi          = {10.1093/comjnl/bxad043},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {1076-1086},
  shortjournal = {Comput. J.},
  title        = {Resilient vector consensus over random dynamic networks under mobile malicious attacks},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analysis and construction of zero-knowledge proofs for the
MinRank problem. <em>COMJNL</em>, <em>67</em>(3), 1060–1075. (<a
href="https://doi.org/10.1093/comjnl/bxad042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The MinRank problem is an NP-complete problem that is prevalent in multivariate cryptography and its goal is to find a non-zero linear combination of given a series of matrices over a ring such that the obtained matrix has a small rank. At Asiacrypt 2001, two Zero-Knowledge Proofs of Knowledge (ZKPoK) for the MinRank problem are proposed, and we call them MRZK and MRZK |$^{\dagger }$|⁠ , respectively. The latter is an improved version of the proof size of the former. However, the efficiency of MRZK |$^{\dagger }$| has been open and not analyzed. While the MRZK protocol is secure, it must be repeated many times due to the soundness error |$2/3$|⁠ , which leads to the large proof size. For 128-bit security, the MRZK protocol is executed at least 219 iterations and the proof size is about 32 KB. In this paper, we first show that the efficiency of MRZK |$^{\dagger }$| is impractical due to unreasonable parameter size. However, when the parameter size is tuned and the efficiency is improved, an imposter can be efficiently constructed. Then, to alleviate the large proof size of MRZK, inspired by the technique designing ZKPoK (Eurocrypt 2020), we propose a sigma protocol with helper to prove the solution to the MinRank problem. Finally, we transform the sigma protocol with helper into a standard ZKPoK (MRZK |$^{\sharp }$|⁠ ) by removing the helper. The MRZK |$^{\sharp }$| protocol can achieve any small soundness error and enjoy the proof size of about 15 KB (53% improvement over MRZK).},
  archive      = {J_COMJNL},
  author       = {Song, Yongcheng and Zhang, Jiang and Huang, Xinyi and Wu, Wei and Chen, Haixia},
  doi          = {10.1093/comjnl/bxad042},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {1060-1075},
  shortjournal = {Comput. J.},
  title        = {Analysis and construction of zero-knowledge proofs for the MinRank problem},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An improved genetic-XGBoost classifier for customer
consumption behavior prediction. <em>COMJNL</em>, <em>67</em>(3),
1041–1059. (<a href="https://doi.org/10.1093/comjnl/bxad041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an increasingly competitive market, predicting the customer’s consumption behavior has a vital role in customer relationship management. In this study, a new classifier for customer consumption behavior prediction is proposed. The proposed methods are as follows: (i) A feature selection method based on least absolute shrinkage and selection operator (Lasso) and Principal Component Analysis (PCA), to achieve efficient feature selection and eliminate correlations between variables. (ii) An improved genetic-eXtreme Gradient Boosting (XGBoost) for customer consumption behavior prediction, to improve the accuracy of prediction. Furthermore, the global search ability and flexibility of the genetic mechanism are used to optimize the XGBoost parameters, which avoids inaccurate parameter settings by manual experience. The adaptive crossover and mutation probabilities are designed to prevent the population from falling into the local extremum. Moreover, the grape-customer consumption behavior dataset is employed to compare the six Lasso-based models from the original, normalized and standardized data sources with the Isometric Mapping, Locally Linear Embedding, Multidimensional Scaling, PCA and Kernel Principal Component Analysis methods. The improved genetic-XGBoost is compared with several well-known parameter optimization algorithms and state-of-the-art classification approaches. Furthermore, experiments are conducted on the University of California Irvine datasets to verify the improved genetic-XGBoost algorithm. All results show that the proposed methods outperform the existing ones. The prediction results provide the decision-making basis for enterprises to formulate better marketing strategies.},
  archive      = {J_COMJNL},
  author       = {Li, Yue and Qi, Jianfang and Jin, Haibin and Tian, Dong and Mu, Weisong and Feng, Jianying},
  doi          = {10.1093/comjnl/bxad041},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {1041-1059},
  shortjournal = {Comput. J.},
  title        = {An improved genetic-XGBoost classifier for customer consumption behavior prediction},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decreasing proof size of BLS scheme. <em>COMJNL</em>,
<em>67</em>(3), 1030–1040. (<a
href="https://doi.org/10.1093/comjnl/bxad040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bootle et al. in CRYPTO 2019 proposed a zero knowledge proof for an |$\mathrm{ISIS}_{m,n,q,\beta }$| instance |$A\vec{s} = \vec{u} \bmod q$| with |$\|\vec{s}\|_{\infty }\leq \beta $| (BLS scheme). It was implemented by transforming the instance into the form |$A^{\prime }\vec{s}^{\prime } =\vec{u}\bmod q$|⁠ , where the coefficients of |$\vec{s}^{\prime}$| are in |$\{0,1,2\}$|⁠ , and proved the latter in an exact way. With the concrete parameters |$m=1024,n=2048,\beta =1,q\approx 2^{32}$|⁠ , their proof is of length 384.03KB. In this paper, we decrease the proof size of BLS scheme by two techniques. The first one takes effect on some special parameters. For these parameters, using the binary basic set instead of the ternary one results in a shorter proof. The second one deals with the repetition of the lower half in BLS scheme. Observing that what the lower half proves is of form |$\mathbf{B}\vec{\mathbf{r}}=\vec{\mathbf{t}}$| with a short vector |$\vec{\mathbf{r}}$| of polynomials, a variant of parallel repetition can be used to shorten the proof size. Combining these two techniques together, the proof size of the above-mentioned instance can be reduced to 220.01KB, only 57.3 |$\%$| of BLS scheme.},
  archive      = {J_COMJNL},
  author       = {Fang, Dong and Huang, Guifang and Wang, Mengfan and Hu, Lei},
  doi          = {10.1093/comjnl/bxad040},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {1030-1040},
  shortjournal = {Comput. J.},
  title        = {Decreasing proof size of BLS scheme},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A publicly verifiable optimistic fair exchange protocol
using decentralized CP-ABE. <em>COMJNL</em>, <em>67</em>(3), 1017–1029.
(<a href="https://doi.org/10.1093/comjnl/bxad039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fair exchange is a challenging problem for two mutually distrusting players. It is widely known that fair exchange is impossible without a trusted third party (TTP). However, relying on a single TTP can cause a single-point failure. An intuitive idea is to adopt multiple TTPs to distribute trust. This paper constructs a two-party optimistic fair exchange (OFE) protocol using decentralized ciphertext-policy attribute-based encryption (CP-ABE), achieving decentralized TTPs. This is achievable because decentralized CP-ABE ciphertext supports a nested access control policy. A nested access control policy fits perfectly in a fair exchange protocol which contains multiple roles (i.e. players and TTPs). Further, we apply non-interactive zero knowledge proofs to prove the well-formedness of ciphertexts, so as to enforce players to follow the protocol specification honestly. Consequently, we construct an OFE protocol in which each player’s operations are publicly verifiable without revealing secret information. Also, we obtain decentralized TTPs with optimism (i.e. the TTPs are involved only when arbitration is required), autonomy (i.e. the TTPs do not need to interact with each other), statelessness (i.e. the TTPs do not need to store data for the exchange protocol) and verifiability (i.e. the TTPs are publicly verifiable). Compared with previous work, our protocol assumes only a public communication channel and each party’s operations are publicly verifiable. Besides, it achieves a favorable |$O(n)$| verification complexity in the normal case, where |$n$| is the number of TTPs. Finally, we present a proof-of-concept implementation to demonstrate the feasibility.},
  archive      = {J_COMJNL},
  author       = {Zhang, Liang and Kan, Haibin and Qiu, Feiyang and Hao, Feng},
  doi          = {10.1093/comjnl/bxad039},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {1017-1029},
  shortjournal = {Comput. J.},
  title        = {A publicly verifiable optimistic fair exchange protocol using decentralized CP-ABE},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 2-layer k-planar graphs density, crossing lemma,
relationships and pathwidth. <em>COMJNL</em>, <em>67</em>(3), 1005–1016.
(<a href="https://doi.org/10.1093/comjnl/bxad038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The |$2$| -layer drawing model is a well-established paradigm to visualize bipartite graphs where vertices of the two parts lie on two horizontal lines and edges lie between these lines. Several beyond-planar graph classes have been studied under this model. Surprisingly, however, the fundamental class of |$k$| -planar graphs has been considered only for |$k=1$| in this context. We provide several contributions that address this gap in the literature. First, we show tight density bounds for the classes of |$2$| -layer |$k$| -planar graphs with |$k\in \{2,3,4,5\}$|⁠ . Based on these results, we provide a Crossing Lemma for |$2$| -layer |$k$| -planar graphs, which then implies a general density bound for |$2$| -layer |$k$| -planar graphs. We prove this bound to be almost optimal with a corresponding lower bound construction. Finally, we study relationships between |$k$| -planarity and |$h$| -quasiplanarity in the |$2$| -layer model and show that |$2$| -layer |$k$| -planar graphs have pathwidth at most |$k+1$| while there are also |$2$| -layer |$k$| -planar graphs with pathwidth at least |$(k+3)/2$|⁠ .},
  archive      = {J_COMJNL},
  author       = {Angelini, Patrizio and Da Lozzo, Giordano and Förster, Henry and Schneck, Thomas},
  doi          = {10.1093/comjnl/bxad038},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {1005-1016},
  shortjournal = {Comput. J.},
  title        = {2-layer k-planar graphs density, crossing lemma, relationships and pathwidth},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How to bind a TPM’s attestation keys with its endorsement
key. <em>COMJNL</em>, <em>67</em>(3), 988–1004. (<a
href="https://doi.org/10.1093/comjnl/bxad037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A trusted platform module is identified by its endorsement key, while it uses an attestation key to provide attestation services, for example, signing a set of platform configuration registers, providing a timestamp or certifying another of its keys. This paper addresses the problem of how a certificate authority binds the endorsement and attestation keys together. This is necessary for the authority to be able to reliably certify the attestation key. This key binding also enables the authority to revoke the attestation key should the endorsement key be compromised. We study all of the existing solutions and show that they either do not solve the problem or cannot be implemented with a real trusted platform module (or both). We propose a new solution which addresses this problem. We develop a security model for our solution and provide a rigorous security proof under this model. We have also implemented the solution using a real trusted platform module, and our implementation results show that this solution is feasible and efficient.},
  archive      = {J_COMJNL},
  author       = {Chen, Liqun and El Kassem, Nada and Newton, Christopher J P},
  doi          = {10.1093/comjnl/bxad037},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {988-1004},
  shortjournal = {Comput. J.},
  title        = {How to bind a TPM’s attestation keys with its endorsement key},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ULDC: Unsupervised learning-based data cleaning for
malicious traffic with high noise. <em>COMJNL</em>, <em>67</em>(3),
976–987. (<a href="https://doi.org/10.1093/comjnl/bxad036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the traffic of novel attacks exceeds current knowledge, realistic traffic labeling methods are prone to mislabeling, which has a significant impact on machine learning-based intrusion detection systems. Data cleaning typically relies on the ability of supervised deep neural networks to learn correct knowledge. Under high noise conditions, noisy labels can affect a supervised network and render it ineffective. To clean traffic datasets under high noise conditions, we propose an unsupervised learning-based data cleaning framework (called ULDC) that does not rely on labels and powerful supervised networks, hence reducing the impact of noisy labels. ULDC evaluates the confidence of observed labels through the distribution and similarity of samples in low dimensions. Moreover, ULDC maximizes the retention of hard samples through adaptive intra-class threshold evaluation, preserving more hard samples for training and improving generalization. In evaluations of ULDC on the CIRA-CIC-DoHBrw-2020 dataset, the percentage of data correction reached more than 75% under high noise, which is better than that of the state-of-the-art methods. ULDC is applicable to traffic data cleaning in both traditional networks and novel networks such as the Internet of Things and mobile networks, and it has been validated on datasets including CIC-IDS-2017 and IoT-23.},
  archive      = {J_COMJNL},
  author       = {Yuan, Qingjun and Zhu, Yuefei and Xiong, Gang and Wang, Yongjuan and Yu, Wentao and Lu, Bin and Gou, Gaopeng},
  doi          = {10.1093/comjnl/bxad036},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {976-987},
  shortjournal = {Comput. J.},
  title        = {ULDC: Unsupervised learning-based data cleaning for malicious traffic with high noise},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wearable sensor-based human activity recognition system
employing bi-LSTM algorithm. <em>COMJNL</em>, <em>67</em>(3), 961–975.
(<a href="https://doi.org/10.1093/comjnl/bxad035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity recognition (HAR) systems employing wearable sensors are a promising area of research for tracking human activity. Recently, wearable devices such as smartwatches and sensors have been developed for activity recognition and monitoring. These systems aim to obtain the subject’s state within his or her environment by exploiting heterogeneous sensors attached to the body. With the development of deep learning, new strategies have emerged to facilitate and solve the HAR problems. In this work, a deep multilayer bidirectional long-short memory (Bi-LSTM) architecture has been implemented to detect human activities. Instead of training a single model as in traditional LSTM methods, two models are presented in the Bi-LSTM scheme, one for learning the input data sequence and the other for learning the reverse sequence. Finally, a new novel postprocessing approach has been proposed based on windowing and voting in the last step to improve the average F1 score. Comprehensive investigations on the three publicly available datasets consisting of a different set of activities were used to evaluate the performance of the proposed framework. The empirical results of this paper on AReM, Mhealth and PAMAP2 datasets attained 95.46, 95.79 and 93.41% average F1 score, respectively. The results also revealed that selecting the window size and implementing the appropriate voting method has a significant effect on improving the average percentage of the F1 score.},
  archive      = {J_COMJNL},
  author       = {Tehrani, Amir and Yadollahzadeh-Tabari, Meisam and Zehtab-Salmasi, Aidin and Enayatifar, Rasul},
  doi          = {10.1093/comjnl/bxad035},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {961-975},
  shortjournal = {Comput. J.},
  title        = {Wearable sensor-based human activity recognition system employing bi-LSTM algorithm},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NODSTAC: Novel outlier detection technique based on spatial,
temporal and attribute correlations on IoT bigdata. <em>COMJNL</em>,
<em>67</em>(3), 947–960. (<a
href="https://doi.org/10.1093/comjnl/bxad034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An outlier in the Internet of Things is an immediate change in data induced by a significant difference in the atmosphere (Event) or sensor malfunction (Error). Outliers in the data cause the decision-maker to make incorrect judgments about data analysis. Hence it is essential to detect outliers in any discipline. The detection of outliers becomes the most crucial task to improve sensor data quality and ensure accuracy, reliability and robustness. In this research, a novel outlier detection technique based on spatial, temporal correlations and attribute correlations is proposed to detect outliers (both Errors and Events). This research uses a correlation measure in the temporal correlation algorithm to determine outliers and the spatial correlation algorithm to classify the outliers, whether the outliers are events or errors. This research uses optimal clusters to improve network lifetime, and malicious nodes were also detected based on spatial–temporal correlations and attribute correlations in these clusters. The experimental results proved that the proposed method in this research outperforms some other models in terms of accuracy against the percentage of outliers infused and detection rate against the false alarm rate.},
  archive      = {J_COMJNL},
  author       = {Brahmam, M Veera and Gopikrishnan, S},
  doi          = {10.1093/comjnl/bxad034},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {947-960},
  shortjournal = {Comput. J.},
  title        = {NODSTAC: Novel outlier detection technique based on spatial, temporal and attribute correlations on IoT bigdata},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient pending interest table content search in NDN
through stable bloom filter. <em>COMJNL</em>, <em>67</em>(3), 941–946.
(<a href="https://doi.org/10.1093/comjnl/bxad033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Named Data Networking (NDN) has gained importance in today’s era due to a paradigm shift in the Internet usage pattern which revolves around the content rather than the respective host addresses. Three important data structures in NDN are Content Store (CS), Pending Interest Table (PIT) and Forwarding Information Base (FIB). The search time of PIT is quite high since its size grows with the addition of new content names, and the Interest packets which are not served by CS are searched in millions of existing entries in the PIT. Hence, lookup time can be improved if, instead of checking all the available entries, initial scanning is done to determine whether the required content name exists in the PIT or not. In this paper, we propose a Stable Bloom Filter (SBF) based PIT called S-PIT, to minimize the PIT search time by identifying the existence of query content through SBF. The various experiments performed show that S-PIT outperforms existing data structures in terms of memory consumption, content insertion time, average search time and false positive rate.},
  archive      = {J_COMJNL},
  author       = {Kaur, Ravneet and Singh, Amrinderpreet and Singh, Aekamjot and Goyal, Amit and Singh, Amritpal and Batra, Shalini},
  doi          = {10.1093/comjnl/bxad033},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {941-946},
  shortjournal = {Comput. J.},
  title        = {An efficient pending interest table content search in NDN through stable bloom filter},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An e-coin based construction for unlinkable priced oblivious
transfer. <em>COMJNL</em>, <em>67</em>(3), 933–940. (<a
href="https://doi.org/10.1093/comjnl/bxad031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A ‘1-out-of-n’ oblivious transfer (OT) protocol involves two participants: a sender, who provides a set of items as input, and a receiver. The protocol guarantees that the receiver gets exactly one of the items of the input set, while the sender is unable to determine which. Priced OT (POT) protocols further allow each item of the input set to be assigned a price in such a way that, after a proper execution of the protocol, the receiver gets the requested item if and only if the corresponding sum of money has been paid. In this paper, we present a construction which takes a ‘1-out-of-n’ OT protocol and transform it into a POT one. Moreover, the resulting system is unlinkable in the sense that the sender is unable to determine whether two executions of the protocol were run by the same receiver or not. When compared to existing unlinkable POT protocols, our construction offers a lower conceptual complexity as it does not involve the use of zero-knowledge proofs.},
  archive      = {J_COMJNL},
  author       = {Borges, Ricard and Sebé, Francesc},
  doi          = {10.1093/comjnl/bxad031},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {933-940},
  shortjournal = {Comput. J.},
  title        = {An e-coin based construction for unlinkable priced oblivious transfer},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A general condition of structural distinguisher.
<em>COMJNL</em>, <em>67</em>(3), 923–932. (<a
href="https://doi.org/10.1093/comjnl/bxad030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a general construction method for structural distinguishers, based on the recently proposed ‘multiple-of- n property’ of a subspace trail. The multiple-of- n property refers to the fact that by appropriate choices of difference for a number of input pairs, it is possible to make sure that the number of times that the difference of the resulting output pairs lie in a particular subspace is always a multiple of n . We weaken the condition for constructing the structural distinguisher from the exact subspace trail to the general subspace trail and this general condition is applied to the SPN cryptographic algorithm, allowing a six-round structural distinguisher to be obtained for the first time. In this way, the best six-round Midori-64 distinguisher is constructed, with its data complexity being identical to Midori’s five-round structural distinguisher. We also applied this new condition to the SKINNY algorithm, extending its structural distinguisher from five to six rounds.},
  archive      = {J_COMJNL},
  author       = {Yang, Yang and Liu, Wenhao and Zeng, Guang},
  doi          = {10.1093/comjnl/bxad030},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {923-932},
  shortjournal = {Comput. J.},
  title        = {A general condition of structural distinguisher},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Balanced off-chain payment channel network routing strategy
based on weight calculation. <em>COMJNL</em>, <em>67</em>(3), 907–922.
(<a href="https://doi.org/10.1093/comjnl/bxad029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Off-chain payment channel network is an effective solution to deal with low throughput and high load on the blockchain. However, the quality of the routing scheme will directly affect the performance of the off-chain network system. Existing routing schemes cannot select an appropriate transaction path according to the actual needs of user nodes. And they may also lead to network congestion, channel imbalance and node centralization. This paper proposes a new balanced routing selection scheme based on weight calculation called BRBW, which builds a weight model by Analytic Hierarchy Process. BRBW comprehensively considers the channel capacity, handling fee and path length to improve transaction success rate, reduce channel congestion and maintain the long-term sustainability of the payment channel. It uses a modified maximum flow algorithm to find paths with sufficient capacity and selects the transaction path for a large payment through linear programming. Finally, we do some experiments and compare with state-of-the-art approaches in the test environment. Simulation results show that under the same network environment, the payment success rate of the BRBW is above 75%, and the channel utilization rate is about 10% higher than other schemes.},
  archive      = {J_COMJNL},
  author       = {Liu, Ya and Wu, Yuanhang and Zhao, Fengyv and Ren, Yanli},
  doi          = {10.1093/comjnl/bxad029},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {907-922},
  shortjournal = {Comput. J.},
  title        = {Balanced off-chain payment channel network routing strategy based on weight calculation},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). New integral distinguishers on permutation of whirlpool.
<em>COMJNL</em>, <em>67</em>(3), 899–906. (<a
href="https://doi.org/10.1093/comjnl/bxad028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whirlpool is a hash function that has been standardized by ISO/IEC. In this paper, we develop a new type of distinguishing property for its underlying permutation |$ W $|⁠ . Division property proposed by Todo at EUROCRYPT 2015 was initially used in the integral cryptanalysis of symmetric-key algorithms. This work for the first time utilizes the MILP method to search for the integral distinguishers of |$ W $| in both the forward and backward directions while concentrating on word-based division property. Under the known-key model, the fact that the permutation used in the hash function does not depend on any secret parameters allows the previous properties to be exploited from the middle, i.e. from an intermediate internal state. Therefore, we apply the inside-out strategy which is the essential step in the zero-sum property to connect the trails in opposite directions. Consequently, we obtain new distinguishers up to full rounds for the |$ W $|⁠ . To further reduce the complexity of the integral distinguishers, we add one round in the middle with the help of subspace trails. Finally, we succeed in extending the length and improving the complexity of the integral distinguishers. To the best of our knowledge, all the results in this paper are competitive with the previous work in both computational cost and memory complexity. It is worth mentioning that the methods presented in this paper are applicable to a broad class of hash functions.},
  archive      = {J_COMJNL},
  author       = {Wang, Bolin and Wu, Wenling and Zhang, Yuhan and Zhang, Li},
  doi          = {10.1093/comjnl/bxad028},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {899-906},
  shortjournal = {Comput. J.},
  title        = {New integral distinguishers on permutation of whirlpool},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Traffic-aware re-grouping for load balance in IEEE 802.11ah
IoT network based on the registered backoff time mechanism.
<em>COMJNL</em>, <em>67</em>(3), 884–898. (<a
href="https://doi.org/10.1093/comjnl/bxad027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IEEE 802.11ah, which is a wireless network protocol designed for Internet of Things (IoT), devised the Restrict Access Window (RAW) mechanism to tackle the collision problem. However, some RAW’s slots may be overloaded; some RAW’s slots may be lightly loaded or have no load because no stations (STA) allocated in these slots need to access channel. This work adopted (i) the Registered Backoff Time (RBT) mechanism such that each STA can register the backoff time for its future channel access in access point (AP) and (ii) the Claiming RAW mechanism such that STAs are allowed to notify AP that they have uplinked data to transmit. In this way, AP can (1) know which STAs have uplinked data to transmit and thus (2) re-schedule those STAs from the overloaded slots to the slots that (i) are underloaded or (ii) no STAs need to access channel based on STAs’ RBTs in advance. As a result, the proposed Registration-based Regrouping for Load-Balance Channel Access (RRG-LBCA) method can achieve load balance among slots to increase the IEEE 802.11ah network’s performance. The performance evaluation results shown that the proposed RRG-LBCA method can eliminate the collision situation and increase the aggregate throughput.},
  archive      = {J_COMJNL},
  author       = {Huang, Chung-Ming and Huang, Shu-Hang},
  doi          = {10.1093/comjnl/bxad027},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {884-898},
  shortjournal = {Comput. J.},
  title        = {Traffic-aware re-grouping for load balance in IEEE 802.11ah IoT network based on the registered backoff time mechanism},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cryptanalysis of reduced-round SipHash. <em>COMJNL</em>,
<em>67</em>(3), 875–883. (<a
href="https://doi.org/10.1093/comjnl/bxad026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SipHash is a family of ARX-based MAC algorithms optimized for short inputs. So far, a lot of implementations and applications for SipHash have been proposed, whereas the cryptanalysis of SipHash still lags behind. In this paper, we study the property of truncated differential in reduced-round SipHash. By exhaustively testing all kinds of 1-bit input differences, we find out the greatest differential biases from corresponding output bits through 3 or 4 SipRounds. Making use of these results, we construct distinguishers for SipHash-2-1 and SipHash-2-2 with practical complexities of |$2^{12}$| and |$2^{36}$|⁠ , respectively. However, one limitation of the latter is that it begins with 1-bit input differences on the most significant message bit, which means it can only work when neglecting the padding rules of SipHash. Furthermore, we reveal the relations between the value of output bias and the difference after the first modular addition step, which is directly determined by corresponding key bits. Based on these relations, we propose a key recovery method for SipHash-2-1 that can obtain a significantly nonuniform distribution of the 128-bit secret key. It is summarized that about |$97\%$| of random keys can be fully recovered under this method within a complexity of |$2^{83}$|⁠ .},
  archive      = {J_COMJNL},
  author       = {He, Le and Yu, Hongbo},
  doi          = {10.1093/comjnl/bxad026},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {875-883},
  shortjournal = {Comput. J.},
  title        = {Cryptanalysis of reduced-round SipHash},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CD-BCM: Cross-domain batch certificates management based on
blockchain. <em>COMJNL</em>, <em>67</em>(3), 864–874. (<a
href="https://doi.org/10.1093/comjnl/bxad025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of information networks, the entities from different network domains interact with each other more and more frequently. Therefore, identity management and authentication are essential in cross-domain setting. The traditional Public Key Infrastructure (PKI) architecture has some problems, including single point of failure, inefficient certificate revocation status management and also lack of privacy protection, which cannot meet the demand of cross-domain identity authentication. Blockchain is suitable for multi-participant collaboration in multi-trust domain scenarios. In this paper, a cross-domain certificate management scheme CD-BCM based on the consortium blockchain is proposed. For the issue of Certificate Authority’s single point of failure, we design a multi-signature algorithm. In addition, we propose a unified structure for batch certificates verification and conversion, which improve the efficiency of erroneous certificate identification. Finally, by comparing with current related schemes, our scheme achieves good functionality and scalability in the scenario of cross-domain certificate management.},
  archive      = {J_COMJNL},
  author       = {Yao, Shixiong and Li, Pei and Chen, Jing and Zeng, Yuexing and Chen, Jiageng and Wang, Donghui},
  doi          = {10.1093/comjnl/bxad025},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {864-874},
  shortjournal = {Comput. J.},
  title        = {CD-BCM: Cross-domain batch certificates management based on blockchain},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automating the deployment of cyber range with OpenStack.
<em>COMJNL</em>, <em>67</em>(3), 851–863. (<a
href="https://doi.org/10.1093/comjnl/bxad024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber Range is an experimental platform based on virtualization technology to construct a controlled simulation environment, providing a real-world simulation environment for cybersecurity personnel to conduct various practical exercises. The problem is that generating a virtual environment satisfying the requirements is labor-intensive and time-consuming. To resolve the above problem, this paper proposes a system to automate the deployment of a cyber range. In our method, the first step is to collect virtual machines (VMs) related to cybersecurity and extract relevant features. Then, machine learning is used to classify VMs to reduce the cost of manual VMs selection. Lastly, leveraging the popular OpenStack cloud platform as the deployment platform enhances the applicability of the cyber range. When it comes time to deploy a virtual environment, the instructor only needs to provide some brief description information of a virtual environment. Then, the system will automatically parse the description file to complete the automated deployment of the virtual environment. This system has been successfully applied to the cyber range of Sichuan University for daily teaching tasks.},
  archive      = {J_COMJNL},
  author       = {Zhou, Shaohong and He, Junjiang and Li, Tao and Lan, Xiaolong and Wang, Yunpeng and Zhao, Hui and Li, Yihong},
  doi          = {10.1093/comjnl/bxad024},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {851-863},
  shortjournal = {Comput. J.},
  title        = {Automating the deployment of cyber range with OpenStack},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extremely lightweight constant-round
membership-authenticated group key establishment for
resource-constrained smart environments toward 5G. <em>COMJNL</em>,
<em>67</em>(3), 840–850. (<a
href="https://doi.org/10.1093/comjnl/bxad023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With rapid development of next-generation mobile networks and communications (5G networks), group-oriented applications in resource-constrained smart environments (RSEs), such as smart homes and smart classrooms, have attracted great attentions. Due to the insecure communications between resource-constrained devices, secure group communications in RSE toward 5G face many challenges. In RSE toward 5G, lightweight communications and low computational overheads are crucial. Besides, the private tokens used to generate the group key are expected to be reused multiple times. However, the conventional frameworks for secure group communications cannot meet these requirements. A practical construction of extremely lightweight constant-round membership authenticated group key establishment framework is proposed in this paper for RSE toward 5G, which not only implements identity authentication among the members and group key establishment but also ensures extremely lightweight computation and communication costs by each group member. In our proposed scheme, the increase in the number of group members will not lead to a linear or logarithmic increase in the communication and calculation costs at the member side. Our framework also resists external and internal attacks and meets all the desirable security features. In this framework, the privacy of tokens can be well protected, so that they can be reused for multiple times. Therefore, our scheme significantly reduces the costs of communication and calculation, and it is more efficient compared with the related schemes in the literature. This proposal is fairly suitable for lightweight membership authentication and group key establishment in RSE toward 5G.},
  archive      = {J_COMJNL},
  author       = {Hsu, Chingfang and Xia, Zhe and Cheng, Tianshu and Harn, Lein},
  doi          = {10.1093/comjnl/bxad023},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {840-850},
  shortjournal = {Comput. J.},
  title        = {Extremely lightweight constant-round membership-authenticated group key establishment for resource-constrained smart environments toward 5G},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pipelined decision trees for online traffic classification
on FPGAs. <em>COMJNL</em>, <em>67</em>(3), 825–839. (<a
href="https://doi.org/10.1093/comjnl/bxad022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision tree (DT)-based machine learning (ML) algorithms are one of the preferred solutions for real-time internet traffic classification in terms of their easy implementation on hardware. However, the rapid increase in today’s newly developed applications and the resulting diversity in internet traffic greatly increases the size of DTs. Therefore, the tree-based hardware classifiers cannot keep up with this growth in terms of resource usage and classification speed. To alleviate the problem, we propose to group application classes by certain rules and create an individual small DT per each group. In this article, a pipelined organization of multiple DT data structures, called pipelined decision trees, is proposed as a scalable solution to tree-based traffic classification. We also propose two distinct algorithms, namely confusion matrix-based class aggregation and leaf count-based class aggregation algorithms, to set group creation rules that allows traffic classification on pipelined smaller DTs in a hierarchical order. We further designed an hardware engine on field programmable gate arrays, which can search those pipelined trees within a single clock cycle by transforming them into bit vectors and implementing multiple range comparisons in parallel. Our architecture with 12 classes can run in 928.88 giga bit per second and achieve 96.04% accuracy.},
  archive      = {J_COMJNL},
  author       = {Erdem, Oğuzhan and Soylu, Tuncay and Carus, Aydın},
  doi          = {10.1093/comjnl/bxad022},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {825-839},
  shortjournal = {Comput. J.},
  title        = {Pipelined decision trees for online traffic classification on FPGAs},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Worst-case analysis of heapsort, exactly. <em>COMJNL</em>,
<em>67</em>(3), 812–824. (<a
href="https://doi.org/10.1093/comjnl/bxad007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  author       = {Suchenek, Marek A},
  doi          = {10.1093/comjnl/bxad007},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {812-824},
  shortjournal = {Comput. J.},
  title        = {Worst-case analysis of heapsort, exactly},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Thematic editorial: The ubiquitous network.
<em>COMJNL</em>, <em>67</em>(3), 809–811. (<a
href="https://doi.org/10.1093/comjnl/bxae032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  author       = {Manolopoulos, Yannis},
  doi          = {10.1093/comjnl/bxae032},
  journal      = {The Computer Journal},
  month        = {4},
  number       = {3},
  pages        = {809-811},
  shortjournal = {Comput. J.},
  title        = {Thematic editorial: The ubiquitous network},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024g). Correction to: Performance evaluation of FPGA-based LSTM
neural networks for pulse signal detection on real-time radar warning
receivers. <em>COMJNL</em>, <em>67</em>(2), 807. (<a
href="https://doi.org/10.1093/comjnl/bxad021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  doi          = {10.1093/comjnl/bxad021},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {807},
  shortjournal = {Comput. J.},
  title        = {Correction to: Performance evaluation of FPGA-based LSTM neural networks for pulse signal detection on real-time radar warning receivers},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Correction to: An attack on a proposed construction of
small-state stream ciphers and proposals for new constructions.
<em>COMJNL</em>, <em>67</em>(2), 806. (<a
href="https://doi.org/10.1093/comjnl/bxad020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  doi          = {10.1093/comjnl/bxad020},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {806},
  shortjournal = {Comput. J.},
  title        = {Correction to: An attack on a proposed construction of small-state stream ciphers and proposals for new constructions},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mobility and security aware real-time task scheduling in
fog-cloud computing for IoT devices: A fuzzy-logic approach.
<em>COMJNL</em>, <em>67</em>(2), 782–805. (<a
href="https://doi.org/10.1093/comjnl/bxad019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to improve the overall task processing time of mobile real-time Internet of Things (IoT) applications in fog-cloud computing, considering the various resource and security requirements along with the time constraints of the task. Fog computing extends the cloud resources to serve the IoT devices at the network edge. In such a scenario, deciding whether the tasks should be processed at the fog layer or submitted to the cloud is critical. Moreover, for real-time applications, the mobility of IoT devices and the limited bandwidth available at the edge devices endanger the low processing time of the task. Besides, the security demands of some IoT applications (i.e. healthcare) require processing the tasks by specific fog or cloud servers to assure confidentiality of information, which may also delay the task processing time. Therefore, we first address the mobility issue by proposing three different algorithms that work on allocating the mobile IoT device to the appropriate edge device (i.e. fog gateway), considering the distance and bandwidth load factors. Then, we offer a novel task scheduling algorithm that uses fuzzy logic to optimize the distribution of tasks between the fog and cloud layers, considering the task security requirements. The algorithm selects the proper processing unit to execute the task in the fog layer by exploiting the task demands (i.e. computation, storage, bandwidth, security) and deadline. Results demonstrate that considering the factors of distance and available bandwidth load while allocating the IoT device to the fog gateway improves the task processing time better than adopting one aspect. Results also show that our proposed scheduling algorithm outperforms other existing algorithms regarding makespan, turnaround time, success ratio and processing time metrics.},
  archive      = {J_COMJNL},
  author       = {Ali, Hala S and Sridevi, R},
  doi          = {10.1093/comjnl/bxad019},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {782-805},
  shortjournal = {Comput. J.},
  title        = {Mobility and security aware real-time task scheduling in fog-cloud computing for IoT devices: A fuzzy-logic approach},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On conditional edge-fault-tolerant strong menger edge
connectivity of folded hypercubes. <em>COMJNL</em>, <em>67</em>(2),
777–781. (<a href="https://doi.org/10.1093/comjnl/bxad018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge connectivity is an important parameter for the reliability of the inter-connection network. A graph |$G$| is strong Menger edge-connected ( ⁠|$SM$| - |$\lambda $| for short) if there exist min |$\{\deg _{G}(u),\deg _{G}(v)\}$| edge-disjoint paths between any pair of vertices |$u$| and |$v$| of |$G$|⁠ . The conditional edge-fault-tolerance strong Menger edge connectivity of |$G$|⁠ , denoted by |$sm_{\lambda }^{r}(G)$|⁠ , is the maximum integer |$m$| such that |$G-F$| remains |$SM$| - |$\lambda $| for any edge set |$F$| with |$|F|\leq m$| and |$\delta (G-F)\geq r$|⁠ , where |$\delta (G-F)\geq r$| is the minimum degree of |$G-F$|⁠ . Most of the previous papers discussed |$sm_{\lambda }^{r}(G)$| in the case of |$r\leq 2$|⁠ . In this paper, we show that |$sm_{\lambda }^{r}(FQ_{n})=2^{r}(n-r+1)-(n+1)$| for |$1\leq r\leq n-2$|⁠ , where |$n\geq 4$|⁠ .},
  archive      = {J_COMJNL},
  author       = {Zhao, Shijie and Li, Pingshan},
  doi          = {10.1093/comjnl/bxad018},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {777-781},
  shortjournal = {Comput. J.},
  title        = {On conditional edge-fault-tolerant strong menger edge connectivity of folded hypercubes},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Time-aware data partition optimization and heterogeneous
task scheduling strategies in spark clusters. <em>COMJNL</em>,
<em>67</em>(2), 762–776. (<a
href="https://doi.org/10.1093/comjnl/bxad017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Spark computing framework provides an efficient solution to address the major requirements of big data processing, but data partitioning and job scheduling in the Spark framework are the two major bottlenecks that limit Spark’s performance. In the Spark Shuffle phase, the data skewing problem caused by unbalanced data partitioning leads to the problem of increased job completion time. In response to the above problems, a balanced partitioning strategy for intermediate data is proposed in this article, which considers the characteristics of intermediate data, establishes a data skewing model and proposes a dynamic partitioning algorithm. In Spark heterogeneous clusters, because of the differences in node performance and task requirements, the default task scheduling algorithm cannot complete scheduling efficiently, which leads to low system task processing efficiency. In order to deal with the above problems, an efficient job scheduling strategy is proposed in this article, which integrates node performance and task requirements, and proposes a task scheduling algorithm using greedy strategy. The experimental results prove that the dynamic partitioning algorithm for intermediate data proposed in this article effectively alleviates the problem that data skew leads to the decrease of system task processing efficiency and shortens the overall task completion time. The efficient job scheduling strategy proposed in this article can efficiently complete the job scheduling tasks under heterogeneous clusters, allocate jobs to nodes in a balanced manner, decrease the overall job completion time and increase the system resource utilization.},
  archive      = {J_COMJNL},
  author       = {Lu, SenXing and Zhao, Mingming and Li, Chunlin and Du, Quanbing and Luo, Youlong},
  doi          = {10.1093/comjnl/bxad017},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {762-776},
  shortjournal = {Comput. J.},
  title        = {Time-aware data partition optimization and heterogeneous task scheduling strategies in spark clusters},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new fuzzy smoothing term model for stereo matching.
<em>COMJNL</em>, <em>67</em>(2), 746–761. (<a
href="https://doi.org/10.1093/comjnl/bxad015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we construct a smoothing term structure, which is an essential part of the energy function in binocular matching. However, the existing energy models are mainly deterministic, which cannot adapt to processing low-quality images, especially when there exists a large proportion of vague areas. In order to perform better in processing these low-quality images, in this paper, we construct the smoothing term based on a fuzzy model, which includes fuzzy segmentation, the fuzzy network between the superpixels and the fuzzy relationship between the pixels. These can be compatible with the uncertainty in the image. In addition, to explain the rationality of the calculation of the degree of correlation between superpixels and further elaborate on the property of these degrees between each superpixel, we propose five corresponding theorems with proofs. After we solve the energy model combined with our proposed smoothing term, we compare our disparity results with the corresponding deterministic model and several state-of-the-art algorithms in the experiment. The results verify the effectiveness of the proposed algorithm.},
  archive      = {J_COMJNL},
  author       = {Hongjin, Zhang and Hui, Wei and Bo, Wang},
  doi          = {10.1093/comjnl/bxad015},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {746-761},
  shortjournal = {Comput. J.},
  title        = {A new fuzzy smoothing term model for stereo matching},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial attacks on network intrusion detection systems
using flow containers. <em>COMJNL</em>, <em>67</em>(2), 728–745. (<a
href="https://doi.org/10.1093/comjnl/bxad014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies adversarial attacks on network intrusion detection systems (IDSs) based on deep or machine learning algorithms. Adversarial attacks on network IDSs must maintain the functional logic of the attack flow. To prevent the produced adversarial examples from violating the attack behavior, most solutions define some limited modification actions. The result limits the production of adversarial examples, and the produced adversarial examples are not guaranteed to find the attack packets. This paper proposes the concept of flow containers to model packets in a flow. Then, we propose a generative adversarial network framework with dual adversarial training to train the generator to produce adversarial flow containers. Flow containers can correlate attack packets and feature vectors of attack flows. We test the evasion rate of the produced adversarial examples using 12 deep and machine learning algorithms. For experiments on the CTU42 data set, the proposed adversarial examples have the highest evasion rates among all 12 classifiers, with the highest evasion rate as high as 1.00. For experiments on the CIC-IDS2017 data set, the proposed adversarial examples have the highest evasion rate among the five classifiers, and the highest evasion rate is also up to 1.00.},
  archive      = {J_COMJNL},
  author       = {Liu, Tzong-Jye},
  doi          = {10.1093/comjnl/bxad014},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {728-745},
  shortjournal = {Comput. J.},
  title        = {Adversarial attacks on network intrusion detection systems using flow containers},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Overfull: Too large aggregate signatures based on lattices.
<em>COMJNL</em>, <em>67</em>(2), 719–727. (<a
href="https://doi.org/10.1093/comjnl/bxad013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Fiat-Shamir with Aborts paradigm of Lyubashevsky has given rise to efficient lattice-based signature schemes. One popular implementation is Dilithium, which has been selected for standardization by the US National Institute of Standards and Technology (NIST). Informally, it can be seen as a lattice analog of the well-known discrete-logarithm-based Schnorr signature. An interesting research question is whether it is possible to combine several unrelated signatures, issued from different signing parties on different messages, into one single aggregated signature. Of course, its size should be significantly smaller than the trivial concatenation of all signatures. Ideally, the aggregation can be done offline by a third party, called public aggregation. Previous works have shown that it is possible to half-aggregate Schnorr signatures, but it was left open if the underlying techniques can be adapted to the lattice setting. In this work, we show that, indeed, we can use similar strategies to obtain a signature scheme allowing for public aggregation whose hardness is proven assuming the intractability of well-studied problems on module lattices. Unfortunately, our scheme produces aggregated signatures that are larger than the trivial solution of concatenating. This is due to peculiarities that seem inherent to lattice-based cryptography. Its motivation is thus mainly pedagogical.},
  archive      = {J_COMJNL},
  author       = {Boudgoust, Katharina and Roux-Langlois, Adeline},
  doi          = {10.1093/comjnl/bxad013},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {719-727},
  shortjournal = {Comput. J.},
  title        = {Overfull: Too large aggregate signatures based on lattices},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The cyclic diagnosability of hypercubes under the PMC model
and the MM* model. <em>COMJNL</em>, <em>67</em>(2), 709–718. (<a
href="https://doi.org/10.1093/comjnl/bxad012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by a multitude of practical applications, many distinct vulnerability parameters of multiprocessor systems have been explored. Traditional connectivity and diagnosability are undoubtedly the most well investigated of these metrics, but often fail to capture the most subtle differences of a multiprocessor system. Subsequently, it is necessary to take into account the minimum degree of components, the size of components or the number of components. However, the structure of the components is ignored in these circumstances. In this work, we propose a novel diagnostic strategy based on cyclic connectivity, namely the cyclic diagnosability. The cyclic diagnosability, denoted by |$ct(G)$|⁠ , is the maximum size of the faulty vertex set |$F$| of |$G$| such that the self-diagnosable system |$G$| can identify all the vertices in |$F$| under the condition that at least two connected components of |$G-F$| contain a cycle. Furthermore, we investigate the cyclic diagnosability of hypercube |$Q_{n}$| under the PMC model and the MM * model, and show that |$ct(Q_{n})=5n-10$| for |$n\geq 7$|⁠ .},
  archive      = {J_COMJNL},
  author       = {Zhang, Hong and Zhou, Shuming and Cheng, Eddie},
  doi          = {10.1093/comjnl/bxad012},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {709-718},
  shortjournal = {Comput. J.},
  title        = {The cyclic diagnosability of hypercubes under the PMC model and the MM* model},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Compressed zero-knowledge proofs for lattice-based
accumulator. <em>COMJNL</em>, <em>67</em>(2), 694–708. (<a
href="https://doi.org/10.1093/comjnl/bxad011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lattice-based cryptographic accumulators, which enable short zero-knowledge arguments of membership, have numerous applications in post-quantum privacy-preserving protocols. However, most efficient quantum-safe zero-knowledge arguments are PCP-based systems and rely on non-falsifiable assumptions. For non-PCP-based constructions using the state-of-the-art techniques on compressing lattice-based zero-knowledge proofs, the concrete size of the resulting proof for accumulators with |$2^{32}$| members is at least 500 KB. In this paper, we propose a compact non-PCP zero-knowledge proof for the lattice-based Merkle-tree, which leads to an efficient post-quantum cryptographic accumulator. The complexity of our construction is logarithmic in |$l\cdot n_{s}$|⁠ , where |$l$| and |$n_{s}$| denote the depth of the underlying Merkle-tree and the size of a node, respectively, and the concrete size is only |$143.7\ $| KB when |$l=32$|⁠ . In particular, we provide an improved lattice-based Bulletproof with efficient knowledge extraction, which allows large challenge space but small soundness slack. Furthermore, the amortized technique can be applied to the Bulletproof without breaking the knowledge soundness due to our improved knowledge extraction. As a direct application, we present a practical lattice-based ring signature, which can achieve logarithmical signing/verifying computational complexity with the number of the ring, while the state-of-the-art constructions (CRYPTO 21) have linear computational complexity.},
  archive      = {J_COMJNL},
  author       = {Si, Shumin and Lin, Xiuhan and Wei, Puwen},
  doi          = {10.1093/comjnl/bxad011},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {694-708},
  shortjournal = {Comput. J.},
  title        = {Compressed zero-knowledge proofs for lattice-based accumulator},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reliability evaluation of multiprocessor system based on the
balanced complete multipartite graphs. <em>COMJNL</em>, <em>67</em>(2),
688–693. (<a href="https://doi.org/10.1093/comjnl/bxad010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliability evaluation of multiprocessor systems is of significant importance in the design and maintenance of multiprocessor systems. Based on edge-connectivity, more refined quantitative indicators for the reliability of multiprocessor systems have been introduced. The extra edge-connectivity and the component edge-connectivity, as two important parameters to evaluate the robustness of multiprocessor systems, are explored extensively. In this paper, we determine the |$h$| -extra edge-connectivity and the |$(g+1)$| -component edge-connectivity of the balanced complete |$t$| -partite graph |$K_{r}^{t}$| for |$t, r\geq 2$|⁠ , where |$1\leq h \leq \lfloor tr/2 \rfloor$| and |$2\leq g \leq tr-1$|⁠ .},
  archive      = {J_COMJNL},
  author       = {Yu, Zhecheng and Xu, Liqiong},
  doi          = {10.1093/comjnl/bxad010},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {688-693},
  shortjournal = {Comput. J.},
  title        = {Reliability evaluation of multiprocessor system based on the balanced complete multipartite graphs},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Explicit upper bound of impossible differentials for
AES-like ciphers: Application to uBlock and midori. <em>COMJNL</em>,
<em>67</em>(2), 674–687. (<a
href="https://doi.org/10.1093/comjnl/bxad009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whether a block cipher can resist impossible differential attack is an important basis to evaluate the security of a block cipher. However, the length of impossible differentials is important for the security evaluation of block ciphers. Most of the previous studies are based on structural cryptanalysis to find the impossible differential, and the structural cryptanalysis covers a lot of specific cryptanalytic vectors which are independent of the nonlinear S-boxes. In this paper, we study the maximum length of the impossible differential of an Advanced Encryption Standard-like cipher in the setting with the details of S-boxes. Inspired by the ‘Divide-and-Conquer’ technique, we propose a new technique called Reduced Block , which combines the details of the S-box. With this tool, the maximum length of impossible differentials can be proven under reasonable assumptions. As applications, we use this tool on uBlock and Midori. Consequently, we prove that for uBlock-128, uBlock-256 and Midori-64, there are no impossible five-round, six-round and seven-round differentials with one active input nibble and one active output nibble, even when considering the details of S-boxes. Furthermore, we reveal some properties of the uBlock S-box and linear layer and demonstrate theoretically that there are no impossible differentials longer than four rounds for uBlock-128 under the assumption that the round keys are independent and uniformly random. This study might provide some insight into the bounds of the length of impossible differentials.},
  archive      = {J_COMJNL},
  author       = {Zhang, Li and Zhang, Yu and Wu, Wenling and Mao, Yongxia and Zheng, Yafei},
  doi          = {10.1093/comjnl/bxad009},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {674-687},
  shortjournal = {Comput. J.},
  title        = {Explicit upper bound of impossible differentials for AES-like ciphers: Application to uBlock and midori},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Providing an approach for early prediction of fall in human
activities based on wearable sensor data and the use of deep learning
algorithms. <em>COMJNL</em>, <em>67</em>(2), 658–673. (<a
href="https://doi.org/10.1093/comjnl/bxad008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Falling is one of the major health concerns, and its early detection is very important. The goal of this study is an early prediction of impending falls using wearable sensors data. The SisFall data set has been used along with two deep learning models (CNN and a combination model named Conv_Lstm). Also, a dynamic sampling method is offered to improve the accuracy of the models by increasing the equilibrium rate between the samples of the majority and minority classes. To fulfill the main idea of this paper, we present a future prediction strategy. Then, by defining a time variable ‘T’, the system replaces and labels the state of the next T s instead of considering the current state only. This leads to predicting falling states at the beginning moments of balance disturbance. The results of the experiments show that the Conv_Lstm model was able to predict the fall in 78% of cases and an average of 340 ms before the accident. Also, for the Sensitivity criterion, a value of 95.18% has been obtained. A post-processing module based on the median filter was implemented, which could increase the accuracy of predictions to 95%.},
  archive      = {J_COMJNL},
  author       = {Keramati Hatkeposhti, Rahman and Yadollahzadeh-Tabari, Meisam and Golsorkhtabariamiri, Mehdi},
  doi          = {10.1093/comjnl/bxad008},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {658-673},
  shortjournal = {Comput. J.},
  title        = {Providing an approach for early prediction of fall in human activities based on wearable sensor data and the use of deep learning algorithms},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revocable public key encryption with equality test without
pairing in cloud storage. <em>COMJNL</em>, <em>67</em>(2), 642–657. (<a
href="https://doi.org/10.1093/comjnl/bxad006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Public key encryption with equality test (PKEET) plays an important role in the cloud storage. It allows a third party to test whether two ciphertexts contain the same message without decryption. Since the third party is not fully trusted, it is sometimes necessary to prevent the third party from testing the ciphertexts all the time. To this end, we propose the notion of revocable public key encryption with equality test (R-PKEET). We give the construction of R-PKEET in the random oracle model, which employs time key to update trapdoor and partial ciphertexts. In details, our scheme enables a user to revoke the third party’s test right by sending a time key to the cloud server, which is responsible for updating partial ciphertexts using the time key. Compared with related works, our scheme achieves both lightweight revocation and lower computational complexity by using Shamir’s secret sharing and Lagrange interpolating polynomial.},
  archive      = {J_COMJNL},
  author       = {Yang, Tian and Ma, Sha and Du, Jiaojiao and Jiang, Chengyu and Huang, Qiong},
  doi          = {10.1093/comjnl/bxad006},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {642-657},
  shortjournal = {Comput. J.},
  title        = {Revocable public key encryption with equality test without pairing in cloud storage},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ransomware detection by distinguishing API call sequences
through LSTM and BERT models. <em>COMJNL</em>, <em>67</em>(2), 632–641.
(<a href="https://doi.org/10.1093/comjnl/bxad005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, ransomware evolved rapidly and the prevention of ransomware has become an important issue. The threat of ransomware is much more sophisticated than before for governments and enterprises; breaches or corruption of sensitive data will cause huge impact on the organization. Early detection is one of the effective method to prevent the ransomware attack. Modern ransomware detection technologies can be divided into two categories: static analysis and dynamic analysis. Dynamic analysis observes the behavior of the running program. Previous research adopted machine learning approach for dynamic analysis and API sequence dataset were used to trained machine learning models for dynamic analysis. In this research, we collected the API calls of the ransomware from reports generated by Cuckoo Sandbox and proposed two detecting models using BERT and LSTM. The result shows that both BERT and LSTM models can successfully predict ransomware with 95% high accuracy. We aimed to compare the performance of two text-based learning model, LSTM and BERT, and analyze the pros and cons. The result shows that API sequence data can be used to train effective ransomware detection models in text-based manner.},
  archive      = {J_COMJNL},
  author       = {Lin, Tu-Liang and Chang, Hong-Yi and Chiang, Yuan-Yao and Lin, Shu-Cheng and Yang, Tsung-Yen and Zhuang, Chun-Jun and Tseng, Wha-Lee and Zhang, Bo-Hao},
  doi          = {10.1093/comjnl/bxad005},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {632-641},
  shortjournal = {Comput. J.},
  title        = {Ransomware detection by distinguishing API call sequences through LSTM and BERT models},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Certificateless broadcast encryption with authorization
suitable for storing personal health records. <em>COMJNL</em>,
<em>67</em>(2), 617–631. (<a
href="https://doi.org/10.1093/comjnl/bxad004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud medical treatment provides real-time data sharing in a cost-effective method, making it more practical to create, collect and manage vast amounts of personal health records (PHR) of patients. However, health information is considered highly sensitive. How to securely store and dynamically process massive patients’ PHR data in a public cloud environment has become one of the most important challenges. Therefore, we introduce a novel solution to the problems of privacy exposure, data security and flexible access of storage modules in medical systems. In this paper, we present a privacy-preserving certificateless broadcast encryption with authorization for the PHR system, which is the best approach to effectively solve the above problems and avoid key escrow. In our work, users (patients) outsource their encrypted data to the cloud server and reallocate data accessing rights of recipients through an authorization set, sharing with a group of authorized receivers (doctors) in a secure and efficient manner. In addition, it is shown to be capable of achieving both plaintext confidentiality and receiver anonymity under the random oracle model. Moreover, the experimental evaluation shows that the proposed scheme enjoys low computational and communication overhead, indicating the feasibility and practicality of the scheme.},
  archive      = {J_COMJNL},
  author       = {Chen, Zhiwei and Deng, Lunzhi and Ruan, Yu and Feng, Shuai and Wang, Tao and Wang, Bo},
  doi          = {10.1093/comjnl/bxad004},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {617-631},
  shortjournal = {Comput. J.},
  title        = {Certificateless broadcast encryption with authorization suitable for storing personal health records},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detecting group shilling attacks in recommender systems
based on user multi-dimensional features and collusive behaviour
analysis. <em>COMJNL</em>, <em>67</em>(2), 604–616. (<a
href="https://doi.org/10.1093/comjnl/bxad003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group shilling attacks are more threatening than individual shilling attacks due to the collusive behaviours among group members, which pose a great challenge to the credibility of recommender systems. Detection of group shilling attacks can reduce the risk caused by such attacks and ensure the credibility of recommendations. The existing methods for detecting group shilling attacks mainly extract features from the rating patterns of users at group level to measure the shilling behaviours of groups. However, they may become ineffective with the change of attack strategy, resulting in a decrease in detection performance. Aiming at this problem, a new solution based on user multi-dimensional features and collusive behaviour analysis is presented for detecting group shilling attacks. First, we employ the information entropy and latent semantic analysis to analyse the user behavioural patterns from dimensions of item, rating, time and interest, and propose a suite of indicators to measure the anomaly behaviours of users. Second, we propose a measure based on the multi-dimensional features of users to capture the collusion of group members from the perspective of their synchronized behaviours and abnormal behaviours, and treat the groups with high collusion as candidate groups. Finally, based on the multi-dimensional features of users, we construct the user behaviour similarity matrix using Gaussian radial basis function (Gaussian-RBF) and adopt the spectral clustering algorithm to spot group shilling attackers in the candidate groups. Experiments show that the detection performance (F1-measure) of the proposed method can achieve 0.965, 0.964, 0.991 and 0.868 on the Netflix, CiaoDVD, Epinions and Amazon datasets, respectively, which is better than that of state-of-the-art methods.},
  archive      = {J_COMJNL},
  author       = {Xu, Yishu and Zhang, Peng and Yu, Hongtao and Zhang, Fuzhi},
  doi          = {10.1093/comjnl/bxad003},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {604-616},
  shortjournal = {Comput. J.},
  title        = {Detecting group shilling attacks in recommender systems based on user multi-dimensional features and collusive behaviour analysis},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Secure and efficient masking of lightweight ciphers in
software and hardware. <em>COMJNL</em>, <em>67</em>(2), 581–603. (<a
href="https://doi.org/10.1093/comjnl/bxad002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Masking is a well used and widely deployed countermeasure against side channel attacks, both in software and hardware. With masking comes at a great cost, search has focused on how to lower a performance penalty or find efficient masking implementation. In particular, our contribution is 2-fold: for software masking, we first find bitsliced implementations of Sbox with Multiplicative Complexity 4 and Multiplicative Depth 2, then adapt the common shares approach introduced by Coron et al. at CHES 2016 to make many cross-products |$a_{i}\cdot b_{j}$| can be reuse for parallel ISW-based 32-bit nonlinear operations. Therefore, we improve the efficiency of 2 |$\times b/4/32$| parallel high-order masking of ISW scheme for RECTANGLE, TANGRAM and KNOT on 32-bit ARM embedded microprocessor, with roughly a 13%-34% speed-up, at cost of |$(1+d) \times 32$| -bit randomness. For hardware masking, 4 bit cubic Sboxes with quadratic decomposition length 2, including RECTANGLE, TANGRAM, KNOT and LWC third-round candidates, can be implemented with a 3-share and 4-share threshold implementation (TI) by decomposing cubic permutations |$S$| as a composition of sub-permutations having lower algebraic degrees. We use two decomposition form: one composition of two quadratic permutations |$G$| and |$F$|⁠ , |$S = F\circ G$|⁠ , is for efficiency; the other composition of some linear permutations |$A_i$| and one quadratic permutation |$G$|⁠ , |$S=A_3 \circ G \circ A_2 \circ G \circ A_1 $|⁠ , is for reducing the area requirements. For |$S = F\circ G$|⁠ , we introduce a new approach of searching through all possible quadratic permutations |$G$| with 2 |$^{25.71}$|⁠ , which is effcient than 2 |$^{26.23}$| in Poschmann et al. at J. Cryptol 2011. For |$S=A_3 \circ G \circ A_2 \circ G \circ A_1 $|⁠ , our approach of finding |$A_i$| with complexity 2 |$^{27.71} $|⁠ , which is effcient than the method introduced by Moradi et al. at ASIACRYPT 2016. In addition, we proposes a new decomposition that |$S=G \circ A_2 \circ G \circ A_1 $|⁠ . We can find the fastest and the smallest hard-ware decomposition implementation of 4-bit permutations for TI with 3 and 4 shares.},
  archive      = {J_COMJNL},
  author       = {Zhao, Xuefeng},
  doi          = {10.1093/comjnl/bxad002},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {581-603},
  shortjournal = {Comput. J.},
  title        = {Secure and efficient masking of lightweight ciphers in software and hardware},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward a novel RESTFUL big data-based urban traffic incident
data web service for connected vehicles. <em>COMJNL</em>,
<em>67</em>(2), 557–580. (<a
href="https://doi.org/10.1093/comjnl/bxad001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Connected vehicles (CVs) are an emerging technology in intelligent transportation systems. Currently, many data-driven intelligent transportation systems (D2ITS) use CV data. Unfortunately, these D2ITS still need serious improvement before they meet higher-level visualization needs. Thus, we aim to develop a new, intelligent data-driven transportation system framework. We focus on visualizing real-time CV data using a big data analytic system in urban areas. In response, we first propose an effective real-time data distribution approach within the Vehicular Ad-hoc NETwork. Second, we develop novel strategies for aggregating, extracting and ingesting data. We provide scalable and fault-tolerant delivery methods without interruption or delay. Finally, we proposed a novel visualization REpresentational State Transfer (REST) web service. We used Simulation of Urban MObility, OMNET++ and Veins to simulate a traffic incident dataset. Then, we tested the Basic Safety Messages in an experimental big data cluster. We used NIFI, Kafka and Cassandra for ingestion, distribution, delivery and storage. The results show accurate performance for packet loss, packet delivery and communication delay. Also, it indicates high throughput and low latency for distributed data delivery systems. Additionally, we obtained the smallest response time for the RESTFUL visualization web service.},
  archive      = {J_COMJNL},
  author       = {Hireche, Samia and Dennai, Abdeslem and Kadri, Boufeldja},
  doi          = {10.1093/comjnl/bxad001},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {557-580},
  shortjournal = {Comput. J.},
  title        = {Toward a novel RESTFUL big data-based urban traffic incident data web service for connected vehicles},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyper-hamiltonian laceability of cartesian products of
cycles and paths. <em>COMJNL</em>, <em>67</em>(2), 548–556. (<a
href="https://doi.org/10.1093/comjnl/bxac196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let |$H$| be a cartesian product graph of even cycles and paths, where the first multiplier is an even cycle of length at least |$4$| and the second multiplier is a path with at least two nodes or an even cycle. Then |$H$| is an equitable bipartite graph, which takes the torus, the column-torus and the even |$k$| -ary |$n$| -cube as its special cases. For any node |$w$| of |$H$| and any two different nodes |$u$| and |$v$| in the partite set of |$H$| not containing |$w$|⁠ , an algorithm was introduced to construct a hamiltonian path connecting |$u$| and |$v$| in |$H-w$|⁠ .},
  archive      = {J_COMJNL},
  author       = {Yang, Yuxing},
  doi          = {10.1093/comjnl/bxac196},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {548-556},
  shortjournal = {Comput. J.},
  title        = {Hyper-hamiltonian laceability of cartesian products of cycles and paths},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved (related-key) differential-based neural
distinguishers for SIMON and SIMECK block ciphers. <em>COMJNL</em>,
<em>67</em>(2), 537–547. (<a
href="https://doi.org/10.1093/comjnl/bxac195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In CRYPTO 2019, Gohr made a pioneering attempt and successfully applied deep learning to the differential cryptanalysis against NSA block cipher Speck 32/64, achieving higher accuracy than the pure differential distinguishers. By its very nature, mining effective features in data plays a crucial role in data-driven deep learning. In this paper, in addition to considering the integrity of the information from the training data of the ciphertext pair, domain knowledge about the structure of differential cryptanalysis is also considered into the training process of deep learning to improve the performance. Meanwhile, taking the performance of the differential-neural distinguisher of Simon 32/64 as an entry point, we investigate the impact of input difference on the performance of the hybrid distinguishers to choose the proper input difference. Eventually, we improve the accuracy of the neural distinguishers of Simon 32/64, Simon 64/128, Simeck 32/64 and Simeck 64/128. We also obtain related-key differential-based neural distinguishers on round-reduced versions of Simon 32/64, Simon 64/128, Simeck 32/64 and Simeck 64/128 for the first time.},
  archive      = {J_COMJNL},
  author       = {Lu, Jinyu and Liu, Guoqiang and Sun, Bing and Li, Chao and Liu, Li},
  doi          = {10.1093/comjnl/bxac195},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {537-547},
  shortjournal = {Comput. J.},
  title        = {Improved (Related-key) differential-based neural distinguishers for SIMON and SIMECK block ciphers},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structure fault tolerance of exchanged hypercube.
<em>COMJNL</em>, <em>67</em>(2), 527–536. (<a
href="https://doi.org/10.1093/comjnl/bxac194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The undirected graph, exchanged hypercube |$EH(s,t)$|⁠ , is a variant of hypercube proposed by Loh et al. It is obtained by removing some links from |$(s+t+1)$| -dimensional hypercube. It retains many excellent properties, so many people have studied its reliability and fault tolerance. In this paper, combining the structure connectivity and substructure connectivity of graphs proposed not long ago, we obtain its |$P_k$| -path, |$C_{2l}$| -cycle and |$K_{1,r}$| -star structure connectivity and substructure connectivity where |$2\le k,r\le s-1\le t-1$| and |$6\le 2l\le s-1\le t-1$|⁠ ; we also establish |$\kappa ^s(EH(s,t);C_4)$| for |$5\le s\le t$| and the upper bound of |$\kappa (EH(s,t);C_4)$| for |$4\le s\le t$|⁠ .},
  archive      = {J_COMJNL},
  author       = {Liu, Heqin and Cheng, Dongqin},
  doi          = {10.1093/comjnl/bxac194},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {527-536},
  shortjournal = {Comput. J.},
  title        = {Structure fault tolerance of exchanged hypercube},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Irreducible fuzzy multiset finite automaton.
<em>COMJNL</em>, <em>67</em>(2), 519–526. (<a
href="https://doi.org/10.1093/comjnl/bxac193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to investigate the algebraic properties of fuzzy multiset finite automaton (FMFA), giving a congruence relation. Using the equivalence classes, a minimal accessible complete FMFA is then presented. In addition, we define the concepts of admissible relation, admissible partition for FMFA and quotient FMFA. Further, we present an algorithm which determines an admissible partition for an FMFA and we suggest the time complexity of the algorithm. In particular, we introduce a connection between the admissible partition and the quotient FMFA and we show that any quotient of a given FMFA and the FMFA itself have the same language. Furthermore, using the quotient FMFA, we obtain an irreducible FMFA with the same language.},
  archive      = {J_COMJNL},
  author       = {Shamsizadeh, Marzieh and Mehdi Zahedi, Mohammad and Abolpour, Khadijeh},
  doi          = {10.1093/comjnl/bxac193},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {519-526},
  shortjournal = {Comput. J.},
  title        = {Irreducible fuzzy multiset finite automaton},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep learning model for energy-aware task scheduling
algorithm based on learning automata for fog computing. <em>COMJNL</em>,
<em>67</em>(2), 508–518. (<a
href="https://doi.org/10.1093/comjnl/bxac192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an artificial intelligence deep learning model for an energy-aware task scheduling algorithm based on learning automata (LA) in the Fog Computing (FC) Applications. FC is a distributed computing model that serves as an intermediate layer between the cloud and Internet of Things (IoT) to improve the quality of service. The IoT is the closest model to the wireless sensor network (WSN). One of its important applications is to create a global approach to health care system infrastructure development that reflects recent advances in WSN. The most influential factor in energy consumption is task scheduling. In this paper, the issue of reducing energy consumption is investigated as an important challenge in the fog environment. Also, an algorithm is presented to solve the task scheduling problem based on LA and measure the makespan (MK) and cost parameters. Then, a new artificial neural network deep model is proposed, based on the presented LA task scheduling fog computing algorithm. The proposed neural model can predict the relation among MK, energy and cost parameters versus VM length for the first time. The proposed model results show that all of the desired parameters can be predicted with high precision.},
  archive      = {J_COMJNL},
  author       = {Ebrahim Pourian, Reza and Fartash, Mehdi and Akbari Torkestani, Javad},
  doi          = {10.1093/comjnl/bxac192},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {508-518},
  shortjournal = {Comput. J.},
  title        = {A deep learning model for energy-aware task scheduling algorithm based on learning automata for fog computing},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Removing watermarks for image processing networks via
referenced subspace attention. <em>COMJNL</em>, <em>67</em>(2), 498–507.
(<a href="https://doi.org/10.1093/comjnl/bxac190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network model extraction attack is the process of retraining a surrogate model based on the outputs of a target model with a given set of inputs. Such attacks are hard to defend for the sake of model owners’ interest. Recently, some work propose model watermarking scheme for image processing networks, which is able to prove the intellectual property of deep models even after the model extraction attack. This scheme makes sure that, once the target model (an image processing network) is watermarked, we can extract the watermark from the output of the surrogate model. In this paper, we propose a new model extraction attack scheme to fight against the latest method. Instead of directly using the output images of a target model, we propose to use their reconstructed versions for model retraining, where an asymmetrical UNet is proposed for image reconstruction. To thoroughly remove the watermarking traces, we propose and incorporate a referenced subspace attention module in the asymmetrical UNet, which removes the watermark by projecting the outputs of the target model into the subspaces of the reference image. Various experiments demonstrate the effectiveness of our attack.},
  archive      = {J_COMJNL},
  author       = {Xue, Yuliang and Zhu, Yuhao and Zhu, Zhiying and Li, Sheng and Qian, Zhenxing and Zhang, Xinpeng},
  doi          = {10.1093/comjnl/bxac190},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {498-507},
  shortjournal = {Comput. J.},
  title        = {Removing watermarks for image processing networks via referenced subspace attention},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hitting times of random walks on edge corona product graphs.
<em>COMJNL</em>, <em>67</em>(2), 485–497. (<a
href="https://doi.org/10.1093/comjnl/bxac189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph products have been extensively applied to model complex networks with striking properties observed in real-world complex systems. In this paper, we study the hitting times for random walks on a class of graphs generated iteratively by edge corona product. We first derive recursive solutions to the eigenvalues and eigenvectors of the normalized adjacency matrix associated with the graphs. Based on these results, we further obtain interesting quantities about hitting times of random walks, providing iterative formulas for two-node hitting time, as well as closed-form expressions for the Kemeny’s constant defined as a weighted average of hitting times over all node pairs, as well as the arithmetic mean of hitting times of all pairs of nodes.},
  archive      = {J_COMJNL},
  author       = {Zhu, Mingzhe and Xu, Wanyue and Li, Wei and Zhang, Zhongzhi and Kan, Haibin},
  doi          = {10.1093/comjnl/bxac189},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {485-497},
  shortjournal = {Comput. J.},
  title        = {Hitting times of random walks on edge corona product graphs},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Secure and compact elliptic curve scalar multiplication with
optimized inversion. <em>COMJNL</em>, <em>67</em>(2), 474–484. (<a
href="https://doi.org/10.1093/comjnl/bxac188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Elliptic curve cryptography (ECC) is a typical public key cryptography technique that can ensure equivalent security with considerably smaller key sizes than Rivest-Shamir-Adleman (RSA). Hence, various implementations based on ECC are recommended for block chain and Internet of Things (IoT) devices. Because elliptic curve scalar multiplication (ECSM) is a fundamental computation in ECC, enhancing the security and efficiency of ECSM is important. ECSM specifies a scalar multiplication algorithm and elliptic curve addition formulae. Elliptic curve addition formulae on affine coordinates are compact from a memory cost perspective but weak against side channel attacks. Elliptic curve complete addition (CA) formulae can achieve secure ECSMs but are inefficient. A newly proposed secure ECSM, which uses a right-to-left (RL) scalar multiplication algorithm and (extended) affine coordinates, takes advantage of elliptic curve addition formulae on affine coordinates. However, it can only scan the input scalars from right to left. We propose new ECSMs, which can scan the input scalars from left to right (LR) based on (extended) affine coordinates. We also prove that our LR ECSMs satisfy secure generality without requiring exceptional computations. We enhance the efficiency of both LR and RL ECSMs with optimized inversion. Our LR ECSM with a memory of 12 field elements reduces that of the Montgomery ladder and Joye’s LR with CA formulae by 36.84% and that of 2-ary RL with (extended) affine coordinates by 14.29%, respectively. Our compact ECSMs are fit for applications on IoT devices and block chain, with a critical memory requirement.},
  archive      = {J_COMJNL},
  author       = {Jin, Yaoan and Miyaji, Atsuko},
  doi          = {10.1093/comjnl/bxac188},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {474-484},
  shortjournal = {Comput. J.},
  title        = {Secure and compact elliptic curve scalar multiplication with optimized inversion},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Finding influencers in complex networks: An effective deep
reinforcement learning approach. <em>COMJNL</em>, <em>67</em>(2),
463–473. (<a href="https://doi.org/10.1093/comjnl/bxac187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maximizing influences in complex networks is a practically important but computationally challenging task for social network analysis, due to its nondeterministic polynomial time (NP)-hard nature. Most current approximation or heuristic methods either require tremendous human design efforts or achieve unsatisfying balances between effectiveness and efficiency. Recent machine learning attempts only focus on speed but lack performance enhancement. In this paper, different from previous attempts, we propose an effective deep reinforcement learning model that achieves superior performances over traditional best influence maximization algorithms. Specifically, we design an end-to-end learning framework that combines graph neural network as the encoder and reinforcement learning as the decoder , named DREIM . Through extensive training on small synthetic graphs, DREIM outperforms the state-of-the-art baseline methods on very large synthetic and real-world networks on solution quality, and we also empirically show its linear scalability with regard to the network size, which demonstrates its superiority in solving this problem.},
  archive      = {J_COMJNL},
  author       = {Liu, Changan and Fan, Changjun and Zhang, Zhongzhi},
  doi          = {10.1093/comjnl/bxac187},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {463-473},
  shortjournal = {Comput. J.},
  title        = {Finding influencers in complex networks: An effective deep reinforcement learning approach},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Perceptual image hashing based on canny operator and tensor
for copy-move forgery detection. <em>COMJNL</em>, <em>67</em>(2),
447–462. (<a href="https://doi.org/10.1093/comjnl/bxac186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Copy-move is a common image forgery operation, which copies and moves a block of an image from one position to another place. Image hashing refers to extracting a unique number sequence from the image by using various image features. In practical application, image hashing is used to replace the image itself, which effectively reduces the cost of image storage and computational complexity. In this paper, we propose a novel image hash extraction scheme: constructing image hashing by combining local feature based on Canny operator and global feature based on tensor. In addition, instead of using the traditional correlation coefficient or Hamming distance, a novel method is proposed to calculate the hash distances. A large number of experiments have proved that our image hashing can achieve a better balance between robustness and discrimination with a shorter hash length. What’s more, we can directly locate the forgery areas from the hashing for copy-move forged images.},
  archive      = {J_COMJNL},
  author       = {Liu, Mengqi and Gao, Hang and Xia, Xiaofan and Gui, Suying and Gao, Tiegang},
  doi          = {10.1093/comjnl/bxac186},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {447-462},
  shortjournal = {Comput. J.},
  title        = {Perceptual image hashing based on canny operator and tensor for copy-move forgery detection},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Query operators for transactional data: Detecting similar
and periodic transactions. <em>COMJNL</em>, <em>67</em>(2), 437–446. (<a
href="https://doi.org/10.1093/comjnl/bxac185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pattern detection for revealing the patterns of users’ behavior is an important analysis-assisting tool toward the understanding and prediction of their attitudes, manners, activities and habits. In this paper, two novel query operators applied to transactional data are introduced to ease the query processing, strengthening query capabilities and revealing valuable patterns for data analysis and mining. The operators are named as PeriodicTransactions and SimilarTransactions, and as their names imply, they measure periodicity and similarity, respectively, in a set of transactions. The operators are formally defined and the corresponding algorithms are also provided. To show the expediency of the operators, the proposed algorithms are implemented and a set of experiments were conducted with real data from the Ethereum blockchain. The results show the feasibility and usefulness of the proposal for identifying these patterns that help to understand user behavior and reveal a rich interaction between senders and recipients, where periodic and similar transactions occur.},
  archive      = {J_COMJNL},
  author       = {Moreno Arboleda, Francisco Javier and Garani, Georgia and Bolivar Zapata, Carlos Daniel},
  doi          = {10.1093/comjnl/bxac185},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {437-446},
  shortjournal = {Comput. J.},
  title        = {Query operators for transactional data: Detecting similar and periodic transactions},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CSEA: A fine-grained framework of climate-season-based
energy-aware in cloud storage systems. <em>COMJNL</em>, <em>67</em>(2),
423–436. (<a href="https://doi.org/10.1093/comjnl/bxac184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous data scale growth increases energy consumption and operating cost that cannot be ignored in cloud storage systems. Previous studies have shown that analyzing the characteristics of I/O access and mining data features is effective for reasonable data distribution in storage systems. The granularity and criterion of classification are the key factors in determining the data distribution. To decrease energy consumption and operating cost, this paper puts forward a fine-grained framework of the climatic-season-based energy-aware in cloud storage system called CSEA. The framework concludes the following three aspects: (i) data feature mining. CSEA discovers potential data features by analyzing data access to provide help with data classification. (ii) K-means clustering algorithm . CSEA uses an unsupervised data classification algorithm in machine learning to divide data into categories based on seasonal characteristics by gathering real I/O access. (iii) data distribution of fine-grained . On the basis of seasonal features, CSEA fuses regional features to further refine the data distribution granularity to save on energy consumption and operating cost. Simulation experiments using extended CloudSimDisk and the constructed mathematical models indicate that CSEA reduces the energy consumption and operating cost compared with the single data classification standard and coarse-grained data distribution.},
  archive      = {J_COMJNL},
  author       = {Yuan, Zhu and Lv, Xueqiang and Xie, Ping and Ge, Haojie and You, Xindong},
  doi          = {10.1093/comjnl/bxac184},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {423-436},
  shortjournal = {Comput. J.},
  title        = {CSEA: A fine-grained framework of climate-season-based energy-aware in cloud storage systems},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A smart contract-based access control framework for smart
healthcare systems. <em>COMJNL</em>, <em>67</em>(2), 407–422. (<a
href="https://doi.org/10.1093/comjnl/bxac183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Security faces huge challenges in Internet of Things (IoT) environments. In particular, conventional access control standards and models tend to be less tailored for IoT due to the constrained nature of smart objects. Usually, a powerful third party is used to handle the access control logic. However, this third party is lacking in transparency and could harm user privacy. Therefore, providing a distributed access control solution, while considering transparency and privacy-preserving awareness in IoT smart systems, is of paramount importance. The described issue can be addressed using the emergent Blockchain technology that provides a promising choice to build a new generation of decentralized and transparent access control solutions. This paper proposes a smart contract-based access control framework for IoT smart healthcare systems, which is based on smart contracts to provide a distributed and trustworthy access control, combined with the GTRBAC model to express fine-grained access control policies while considering temporal authorization constraints. To prove the feasibility and validity of the proposed framework, this paper also provides a detailed technical description and an initial implementation and execution. An experimental evaluation shows that security properties’ analyses on smart contracts achieved the best possible evaluation with no vulnerabilities found, and the cost of access control operations increases linearly as the number of policy constraints increases. Besides, a comparative analysis reveals that the proposed approach can achieve good results with low gas costs and latency.},
  archive      = {J_COMJNL},
  author       = {Abid, Amal and Cheikhrouhou, Saoussen and Kallel, Slim and Tari, Zahir and Jmaiel, Mohamed},
  doi          = {10.1093/comjnl/bxac183},
  journal      = {The Computer Journal},
  month        = {2},
  number       = {2},
  pages        = {407-422},
  shortjournal = {Comput. J.},
  title        = {A smart contract-based access control framework for smart healthcare systems},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-dimensional block trees. <em>COMJNL</em>,
<em>67</em>(1), 391–406. (<a
href="https://doi.org/10.1093/comjnl/bxac182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Block Tree is a data structure for representing repetitive sequences in compressed space, which reaches space comparable with that of Lempel–Ziv compression while retaining fast direct access to any position in the sequence. In this paper, we generalize Block Trees to two dimensions, in order to exploit repetitive patterns in the representation of images, matrices and other kinds of bidimensional data. We demonstrate the practicality of the two-dimensional Block Trees (2D-BTs) in representing the adjacency matrices of Web graphs, and raster images in GIS applications. For this purpose, we integrate our 2D-BT with the |$k^2$| -tree—an efficient structure that exploits clustering and sparseness to compress adjacency matrices—so that it also exploits repetitive patterns. Our experiments show that this structure uses 60–80% of the space of the original |$k^2$| -tree, while being 30% faster to three times slower when accessing cells.},
  archive      = {J_COMJNL},
  author       = {Brisaboa, Nieves R and Gagie, Travis and Gómez-Brandón, Adrián and Navarro, Gonzalo},
  doi          = {10.1093/comjnl/bxac182},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {391-406},
  shortjournal = {Comput. J.},
  title        = {Two-dimensional block trees},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Malware family prediction with an awareness of label
uncertainty. <em>COMJNL</em>, <em>67</em>(1), 376–390. (<a
href="https://doi.org/10.1093/comjnl/bxac181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malware family prediction has been mainly formulated as a multiclass classification to predict one malware family. This approach suffers from label uncertainty, which can mislead malware analysts. To render malware prediction less susceptible to uncertainty, malware family prediction, which entails predicting one or more families, is performed in this study. In this regard, an encoder–decoder malware family prediction model, EnDePMal, with label uncertainty awareness, is proposed. EnDePMal aims to predict all malware families related to samples and preserve their priorities. It comprises a residual neural network-based encoder and a long short-term memory-based decoder with an attention mechanism. The model uses a sequence of malware family names, but not a family name, as a label. Once a visualized malware image is input into EnDePMal, its encoder extracts the important features from the image. Subsequently, its decoder generates family names, where the attention mechanism allows it to focus on relevant features by attending to the encoder’s output. Experimental results show that EnDePMal can predict 77.64% of malware family sequences that preserve their priorities. Moreover, it achieves an accuracy of 93.49% and an F1-score of 0.9282 for malware families with the highest priority, rendering it comparable to the typical multiclass classification model.},
  archive      = {J_COMJNL},
  author       = {Paik, Joon-Young and Jin, Rize},
  doi          = {10.1093/comjnl/bxac181},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {376-390},
  shortjournal = {Comput. J.},
  title        = {Malware family prediction with an awareness of label uncertainty},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new method for identifying influential spreaders in
complex networks. <em>COMJNL</em>, <em>67</em>(1), 362–375. (<a
href="https://doi.org/10.1093/comjnl/bxac180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social networks have an important role in the distribution of ideas. With the rapid development of the social networks, identifying the influential nodes provides a chance to turn the new potential of global information spread into reality. The measurement of the spreading capabilities of nodes is an attractive challenge in social networks analysis. In this paper, a novel method is proposed to identify the influential nodes in complex networks. The proposed method determines the spreading capability of a node based on its local and global positions. The degree centrality is improved by the Shannon entropy to measure the local influence of nodes. The k-shell method is improved by the clustering coefficient to measure the global influence of nodes. To rank the importance of nodes, the entropy weighting method is used to calculate the weight for the local and global influences. The Vlsekriterijumska Optimizacija I Kompromisno Resenje method is used to integrate the local and global influences of a node and obtain its importance. The experiments are conducted on 13 real-world networks to evaluate the performance of the proposed method. The experimental results show that the proposed method is more powerful and accurate to identify influential nodes than other methods.},
  archive      = {J_COMJNL},
  author       = {Qiu, Liqing and Liu, Yuying and Zhang, Jianyi},
  doi          = {10.1093/comjnl/bxac180},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {362-375},
  shortjournal = {Comput. J.},
  title        = {A new method for identifying influential spreaders in complex networks},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Encoding with combination orientation technique for RDH in
dual stego images. <em>COMJNL</em>, <em>67</em>(1), 347–361. (<a
href="https://doi.org/10.1093/comjnl/bxac178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, initially, message intensities are used to create two encoding tables: an index and a code sequence. The table is then updated with |$n$| secret data encoded into it. If the code sequence of the second encoding table matches the code sequence of the preceding or succeeding half of the two encoding tables, the second encoding table is folded. The folding is done when maximum intensity occurs in the utmost succeeding part of the two encoding tables. Finally, using the combination orientation approach, the encoded indices are embedded in the stego images. In the extraction phase, using the two stego images and the encoding with the combination orientation method, the encoded indices are obtained. The decoding is performed to extract the secret message as well as the cover image and there is no occurrence of overflow and underflow problems. It has a high visual quality and a high embedding capacity. The proposed technique achieved a maximum |$\mathrm{PSNR}$| of |$49.49$| dB and a |$648\ 242$| -bit embedding capacity. When compared to traditional techniques, the experimental results show that the proposed method outperforms them.},
  archive      = {J_COMJNL},
  author       = {Shaji, C and Sam, I Shatheesh},
  doi          = {10.1093/comjnl/bxac178},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {347-361},
  shortjournal = {Comput. J.},
  title        = {Encoding with combination orientation technique for RDH in dual stego images},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Designing scenarios for in-organization training using the
CyberCIEGE game. <em>COMJNL</em>, <em>67</em>(1), 338–346. (<a
href="https://doi.org/10.1093/comjnl/bxac177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber security awareness training needs to be personalized to address the policies and requirements of a particular organization, although many of the concepts in this training practice are universal. The main reason for the failure of training programs is that the users do not think about applying security concepts. Therefore, the study aims to design scenarios for in-organization training using CyberCEIGE. In this context, CyberCIEGE is considered a flexible and highly interactive video game that is a security awareness tool. It is responsible for supporting organizational training objectives in an engaging security adventure in the presence of typical users. This game has shown success in utilizing information assurance education. The present study indicates that CyberCIEGE is effective in basic information awareness training programs for security purposes, such as the balance between corporate convenience and individual privacy.},
  archive      = {J_COMJNL},
  author       = {Saeed, Mozamel M},
  doi          = {10.1093/comjnl/bxac177},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {338-346},
  shortjournal = {Comput. J.},
  title        = {Designing scenarios for in-organization training using the CyberCIEGE game},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Key reuse attacks on post-quantum cryptosystems, revisited.
<em>COMJNL</em>, <em>67</em>(1), 323–337. (<a
href="https://doi.org/10.1093/comjnl/bxac176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The National Institute of Standards and Technology (NIST) has been working on standardization of post-quantum cryptography and is approaching the end of round-3 evaluation of algorithms. Key reuse security evaluation is an important part of algorithm evaluation. In order to evaluate the key reuse security of candidate IND-CPA PKEs, at Eurocrypt’19, B |$\breve{\text{a}}$| etu et al . proposed a classical key recovery under plaintext checking attack (KR-PCA) which can recover the reused secret keys by querying an oracle thousands of times. However, the method does not work for cryptosystems which shorten ciphertexts by rounding off the low bits, such as round-3 finalists Kyber and Saber. Subsequently, Dumittan and Vaudenay (ACNS’20) and Qin et al . (ASIACRYPT’21) came up with new effective methods, which require carefully constructed queries. In this paper, we propose an automatic method to recover the reused secret keys of IND-CPA PKEs in Kyber and Saber. Instead of constructing queries carefully, our method uses automated search combined with an optimized bruteforce. The effect and cost of the method depend on the specific parameters. In particular, we can recover the secret keys after thousands of queries in all parameter sets, which is comparable with the current best result.},
  archive      = {J_COMJNL},
  author       = {Wang, Ke and Zhang, Zhenfeng and Jiang, Haodong and Xie, Huiqin and Li, Yanjun and Sun, Ying and Han, Lidong},
  doi          = {10.1093/comjnl/bxac176},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {323-337},
  shortjournal = {Comput. J.},
  title        = {Key reuse attacks on post-quantum cryptosystems, revisited},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). User name-based compression and encryption of images using
chaotic compressive sensing theory. <em>COMJNL</em>, <em>67</em>(1),
304–322. (<a href="https://doi.org/10.1093/comjnl/bxac175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous compression and encryption of images using a novel chaotic map is proposed in this paper. Both compression and encryption of images are carried out by the theory of compressive sensing (CS). A novel chaotic map with a high degree of chaos that is extremely sensitive to its initial parameters is proposed. A measurement matrix for the CS framework is designed based on the proposed map. The compression and recovery of images with different compression/sampling ratios are tested using the designed measurement matrix. Encryption of the compressed data is carried out using the proposed chaotic map and a novel user name-based encryption scheme. The entire encryption/decryption process proposed is completely dependent on the sequence obtained from the proposed chaotic map as well as the authorized user name. Thus, by this process, only authorized people with a valid user name will be able to decrypt the encrypted data and recover the actual underlying image. Simulation results on the proposed scheme with different images show that the average peak signal-to-noise ratio and structural similarity index values of about 32 dB and 0.861 are obtained for a sampling ratio of 0.5. Validations on the proposed map and the encryption process that were carried out using various standard tests prove the efficiency of the system in successfully compressing and encrypting the images. Also, the qualitative evaluation of the proposed compression–encryption process outperforms some of the existing works in the literature.},
  archive      = {J_COMJNL},
  author       = {K, Ashwini},
  doi          = {10.1093/comjnl/bxac175},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {304-322},
  shortjournal = {Comput. J.},
  title        = {User name-based compression and encryption of images using chaotic compressive sensing theory},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantum guess and determine attack on stream ciphers.
<em>COMJNL</em>, <em>67</em>(1), 292–303. (<a
href="https://doi.org/10.1093/comjnl/bxac174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To guarantee the security of symmetric key schemes against quantum adversary, developing quantum cryptanalytic techniques becomes a major worldwide challenge in the post-quantum world. In this paper, we present a general framework of classical guess and determine attack on stream ciphers, and then convert it into quantum guess and determine attack. It shows that, for a given stream cipher with a key size of |$k$| bits and an internal state size of |$n$| bits, if a basic guess and determine attack with a time complexity below |$O ( {{{2}^{{3k}/{2}}}}/{n} )$| is available, there is a quantum guess and determine attack with multiple data that can recover all |$n$| internal state bits of the cipher with complexity below |$O ( {{2^{k / 2}}} )$|⁠ . As applications, we present quantum guess and determine attacks on the SNOW-like stream ciphers. The results show that all of SNOW 1.0 with 128-bit key, SNOW 2.0 with 128-bit key and SOSEMANUK are insecure against quantum guess and determine attack. The resource requirements for implementing a quantum guess and determine attack on SNOW 3G are evaluated as a case study. To the best of our knowledge, this is the first time that the general quantum guess and determine attack is formally proposed and applied to the SNOW-like stream ciphers.},
  archive      = {J_COMJNL},
  author       = {Ding, Lin and Wu, Zheng and Zhang, Guixian and Shi, Tairong},
  doi          = {10.1093/comjnl/bxac174},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {292-303},
  shortjournal = {Comput. J.},
  title        = {Quantum guess and determine attack on stream ciphers},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new approach of evaluating the security against
differential and linear cryptanalysis and its applications to serpent,
NOEKEON and ASCON. <em>COMJNL</em>, <em>67</em>(1), 274–291. (<a
href="https://doi.org/10.1093/comjnl/bxac173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Mixed-Integer Linear Programming (MILP)-based automatic tools have played a significant role in providing security evaluations of symmetric-key primitives. Differential and linear cryptanalysis are the two most important cryptographic techniques. Although some methods have conducted a great effort in exploiting MILP-aided tools in searching for differential (linear) characteristics, traditional methods still suffer from primitives with strong diffusion layers and large sizes, such as NOEKEON. Typically, searching for differential (linear) characteristics of such primitives is difficult, and the corresponding MILP models are too heavy to be solved efficiently. To this end, we propose a simple yet efficient approach to employ MILP to evaluate the security against differential and linear cryptanalysis of such primitives. The core of our approach is to reduce the complex problem to a set of simpler subproblems and obtain the optimal solution of the complex problem by combining all the subproblems. A subproblem is equivalent to searching for all differential (linear) characteristics with a fixed number of active S-boxes in each round. Furthermore, we design an elaborate algorithm consisting of three MILP-aided methods to solve various subproblems and adopt some techniques to improve efficiency further. Applying our new algorithm to three SPN primitives Serpent, NOEKEON and ASCON, we obtain the tightest security bounds against differential and linear cryptanalysis for all three primitives so far and find improved differential and linear characteristics for Serpent and NOEKEON. For Serpent, we improve the upper bound of the maximum probability of 7-round differential characteristics from |$2^{-71}$| to |$2^{-76}$| and find for the first time 7-round differential characteristics. For NOEKEON, our results show that there is no 9-round (10-round) differential (linear) characteristic with a probability (correlation) higher than |$2^{-128}$| ( ⁠|$2^{-64}$|⁠ ), whereas it needs 10 rounds (11 rounds) according to the previous results. In addition, we find an 8-round (9-round) differential (linear) characteristic with a probability (correlation) of |$2^{-127}$| ( ⁠|$2^{-60}$|⁠ ). For ASCON permutation, we provide for the first time an upper bound of the maximum probability (correlation) of 5-round differential (linear) characteristics as |$2^{-70}$| ( ⁠|$2^{-33}$|⁠ ).},
  archive      = {J_COMJNL},
  author       = {Zhou, Chunning and Zhang, Wentao and Cao, Weiwei},
  doi          = {10.1093/comjnl/bxac173},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {274-291},
  shortjournal = {Comput. J.},
  title        = {A new approach of evaluating the security against differential and linear cryptanalysis and its applications to serpent, NOEKEON and ASCON},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SeqMask: Behavior extraction over cyber threat intelligence
via multi-instance learning. <em>COMJNL</em>, <em>67</em>(1), 253–273.
(<a href="https://doi.org/10.1093/comjnl/bxac172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identification and extraction of Tactics, Techniques and Procedures (TTPs) for Cyber Threat Intelligence (CTI) restore the full picture of cyber attacks and guide the analysts to assess the system risk. Existing frameworks can hardly provide uniform and complete processing mechanisms for TTPs information extraction without adequate knowledge background. A multi-instance learning approach named SeqMask is proposed in this paper as a solution. SeqMask extracts behavior keywords from CTI evaluated by the semantic impact, and predicts TTPs labels by conditional probabilities. Still, the framework has two mechanisms to determine the validity of keywords. One using expert experience verification. The other verifies the distortion of the classification effect by blocking existing keywords. In the experiments, SeqMask reached 86.07% and 73.99% in F1 scores for TTPs classifications. For the top 20% of keywords, the expert approval rating is 92.20%, where the average repetition of keywords whose scores between 100% and 90% is 60.02%. Particularly, when the top 65% of the keywords were blocked, the F1 decreased to about 50%; when removing the top 50%, the F1 was under 31%. Further, we also validate the possibility of extracting TTPs from full-size CTI and malware whose F1 are improved by 2.16% and 0.81%.},
  archive      = {J_COMJNL},
  author       = {Ge, Wenhan and Wang, Junfeng},
  doi          = {10.1093/comjnl/bxac172},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {253-273},
  shortjournal = {Comput. J.},
  title        = {SeqMask: Behavior extraction over cyber threat intelligence via multi-instance learning},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hybrid-convolution spatial–temporal recurrent network for
traffic flow prediction. <em>COMJNL</em>, <em>67</em>(1), 236–252. (<a
href="https://doi.org/10.1093/comjnl/bxac171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate traffic flow prediction is valuable for satisfying citizens’ travel needs and alleviating urban traffic pressure. However, it is highly challenging due to the complexity of the urban geospatial structure and the highly nonlinear temporal and spatial dependence on human mobility. Most existing works proposed to rely on strict periods (e.g. daily and weekly) and separate the extraction of temporal and spatial features. Besides, most Recurrent Neural Network (RNN)-based models either fail to capture variations of spatial–temporal features in adjacent timestamps or ignore details of closeness. In this paper, we propose a M ulti-attention based H ybrid-convolution S patial-temporal R ecurrent N etwork (MHSRN) for region-based traffic flow prediction. In MHSRN, we leverage a hybrid-convolution module to capture both shifting features and rich information at the nearest timestamps, and we apply the downsampling procedure to reduce the computation of RNN-based model. Furthermore, we propose to adopt a space-aware multi-attention module to re-perceive global and local spatial–temporal features. We conduct extensive experiments based on three real-world datasets. The results show that the MHSRN outperforms other challenging baselines by approximately 0.2–8.1% in mean absolute error on all datasets. On datasets other than TaxiBJ, the MHSRN reduces the root mean square error by at least 2.8% compared with the RNN-based model.},
  archive      = {J_COMJNL},
  author       = {Zhang, Xu and Wen, Shunjie and Yan, Liang and Feng, Jiangfan and Xia, Ying},
  doi          = {10.1093/comjnl/bxac171},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {236-252},
  shortjournal = {Comput. J.},
  title        = {A hybrid-convolution Spatial–Temporal recurrent network for traffic flow prediction},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Formalizing attack trees to support economic analysis.
<em>COMJNL</em>, <em>67</em>(1), 220–235. (<a
href="https://doi.org/10.1093/comjnl/bxac170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attack trees and attack graphs are both examples of what one might term attack modelling techniques . The primary purpose of such techniques is to help establish and enumerate the ways in which a system could be compromised; as such, they play a key role in the (security) risk analysis process. Given their role and the consequent need to ensure that they are correct, there are good reasons for capturing such artefacts in a formal manner. We describe such a formal approach, which has been motivated by a desire to model attacks from the perspectives of attackers, to support economic analysis. As an illustration, we consider exploitation cost.},
  archive      = {J_COMJNL},
  author       = {Simpson, Andrew and Dellago, Matthias and Woods, Daniel},
  doi          = {10.1093/comjnl/bxac170},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {220-235},
  shortjournal = {Comput. J.},
  title        = {Formalizing attack trees to support economic analysis},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved linear cryptanalysis of block cipher BORON.
<em>COMJNL</em>, <em>67</em>(1), 210–219. (<a
href="https://doi.org/10.1093/comjnl/bxac169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BORON is a lightweight substitution–permutation network cipher proposed in 2017. We reduce the number of guessed key bits by key-bridging technology and first utilize Fast Walsh Transform on BORON to minimize the time complexity. Finally, this paper gives the better key-recovery attack against block cipher BORON than previously proposed by 2 rounds: we realize a 11-round key-recovery attack on BORON-80 and 13-round key-recovery attack on BORON-128. The attacks proposed in this paper are the best attacks against BORON-80/128 to date.},
  archive      = {J_COMJNL},
  author       = {Lv, Yin and Shi, Danping and Hu, Lei and Guo, Zihui and Guo, Yi and Wang, Caibing},
  doi          = {10.1093/comjnl/bxac169},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {210-219},
  shortjournal = {Comput. J.},
  title        = {Improved linear cryptanalysis of block cipher BORON},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differential attack with constants on μ2 block cipher.
<em>COMJNL</em>, <em>67</em>(1), 195–209. (<a
href="https://doi.org/10.1093/comjnl/bxac168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential attack is one of the most important methods in cryptanalysis. When finding a high-probability differential trail, the effect of constant has long been ignored. In this paper, we focus on the effect of constants on the differential attack against |$\mu ^2$|⁠ . |$\mu ^2$| is a newly proposed block cipher based on a Type-II generalized Feistel structure. Its 16-bit F function (denoted as F-box) is an ultra-lightweight permutation equipped with different constants. The designer applied the minimum number of active S-boxes to determine |$\mu ^2$| ’ security margin in the design document. However, the F-boxes use different round constants in different rounds; the constants may lead to incompatibility of differential trails of F-boxes. Therefore, to provide a more precise differential attack on |$\mu ^2$|⁠ , we construct an model based on STP (Simple Theorem Prover) constraint solver to search for the valid differential trails with a more precise probability of |$\mu ^2$| for different starting rounds. Finally, the related-key differential trail covers one more round than the existing methods. Analyzing the effect of constants on the validity and the probability of the differential trail reminds the designers and the attackers to have a more comprehensive analysis of specific ciphers.},
  archive      = {J_COMJNL},
  author       = {Shi, Jiali and Li, Chao and Liu, Guoqiang},
  doi          = {10.1093/comjnl/bxac168},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {195-209},
  shortjournal = {Comput. J.},
  title        = {Differential attack with constants on μ2 block cipher},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non-interactive boolean searchable asymmetric encryption
with bilateral access control. <em>COMJNL</em>, <em>67</em>(1), 179–194.
(<a href="https://doi.org/10.1093/comjnl/bxac166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searchable asymmetric encryption (SAE) enables a client to search over a data owner’s encrypted data. Nevertheless, state-of-the-art SAE schemes allow a data owner to specify access control policy for a client, while they have not considered the threat case of a malicious data owner. To address the problem, this work presents a non-interactive SAE scheme with bilateral access control: (i) allowing data owner and client to both specify policies toward the other party; (ii) allowing client to perform arbitrary boolean queries with sub-linear search complexity. Technically, we extend Cash et al .’s highly scalable SSE into an asymmetric setting and introduce the property of data owner authenticity . By refining identity-based matchmaking encryption, we formalize the syntax and security definition of our SAE with identity-based bilateral access control. Moreover, the security of the proposed SAE can be reduced to discrete logistic assumption and decisional bilinear Diffie–Hellman assumption. As an enhanced extension, we present a non-interactive multi-client SAE scheme with fuzzy identity-based bilateral access control. In addition, we implement the proposed schemes in real cloud platform and evaluate their performance on a real-world dataset. The result confirms that our SAE schemes achieve bilateral access control for both data owner and client with highly acceptable efficiency.},
  archive      = {J_COMJNL},
  author       = {Wang, Xiwen and Zhang, Kai and Li, Jinguo and Wen, Mi and Xu, Shengmin and Ning, Jianting},
  doi          = {10.1093/comjnl/bxac166},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {179-194},
  shortjournal = {Comput. J.},
  title        = {Non-interactive boolean searchable asymmetric encryption with bilateral access control},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An attack on a proposed construction of small-state stream
ciphers and proposals for new constructions. <em>COMJNL</em>,
<em>67</em>(1), 169–178. (<a
href="https://doi.org/10.1093/comjnl/bxac165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small-state stream ciphers (SSCs) idea is based on using key bits not only in the initialization but also continuously in the keystream generation phase. A time-memory-data tradeoff (TMDTO) distinguishing attack was successfully applied against all SSCs in 2017 by Hamann et al . They suggested using not only key bits but also initial value (IV) bits continuously in the keystream generation phase to strengthen SSCs against TMDTO attacks. Then, Hamann and Krause proposed a construction based on using only IV bits continuously in the packet mode. They suggested an instantiation of an SSC and claimed that it is resistant to TMDTO attacks. We point out that accessing IV bits imposes an overhead on cryptosystems that might be unacceptable in some applications. More importantly, we show that the proposed SSC remains vulnerable to TMDTO attacks 1 . To resolve this security threat, the current paper proposes constructions based on storing key or IV bits that are the first to provide full security against TMDTO attacks. Five constructions are proposed for different applications by considering efficiency. Designers can obtain each construction’s minimum volatile state length according to the desirable keystream, key and IV lengths.},
  archive      = {J_COMJNL},
  author       = {Amin-Ghafari, Vahid and Ahmadian Attari, Mahmoud},
  doi          = {10.1093/comjnl/bxac165},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {169-178},
  shortjournal = {Comput. J.},
  title        = {An attack on a proposed construction of small-state stream ciphers and proposals for new constructions},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PNCTS: A prediction and network coding-based transmission
scheme for efficient screen updates delivery in DaaS. <em>COMJNL</em>,
<em>67</em>(1), 153–168. (<a
href="https://doi.org/10.1093/comjnl/bxac164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Desktop as a Service (DaaS) provides users with flexible, customizable and highly secure cloud based virtual desktop access. As the major carrier of execution results in DaaS, screen updates play an important role in users’ quality of experience. In order to bring users the same feelings like manipulating a local device, timeliness and reliability should be balanced. However, a timely but unreliable transmission scheme (i.e. UDP) or a completely reliable transmission scheme (i.e. TCP) is inappropriate for such a transmission scenario, especially under a high-loss network. In this paper, we propose a Prediction and Network Coding based Transmission Scheme (PNCTS) for efficient screen updates delivery in DaaS. As an end-to-end partially reliable transmission scheme, it prioritizes different data obtained by partitioning screen updates and employs network coding and TFRC (TCP Friendly Rate Control) to compensate for data loss and adjust the sending rate of screen updates, respectively. To reduce the overhead of network coding, PNCTS uses a Hidden Markov Model to predict the reliability level of network and makes different encoding strategies for the data with different priorities. Simulation results show that PNCTS can improve display quality and instantaneous goodput effectively while maintaining end-to-end delay and jitter at a relatively low level under the static and time-varying network conditions.},
  archive      = {J_COMJNL},
  author       = {Luo, Qin and Cao, Xin},
  doi          = {10.1093/comjnl/bxac164},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {153-168},
  shortjournal = {Comput. J.},
  title        = {PNCTS: A prediction and network coding-based transmission scheme for efficient screen updates delivery in DaaS},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reliability analysis of the cactus-based networks based on
subsystem. <em>COMJNL</em>, <em>67</em>(1), 142–152. (<a
href="https://doi.org/10.1093/comjnl/bxac163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiprocessor systems play a significant role in big data era. As the probability of presence of processor failures in a multiprocessor system raises with the increase of the system scale, the effect of processor failure is worthy of quantifying. The subsystem reliability of a multiprocessor system is the probability that a fault-free subsystem of certain size still operate with the rise of individual faults. In this work, we employ the probabilistic fault model and the Principle of Inclusion-Exclusion (PIE) to establish the approximation and upper bound on the subsystem reliability of the cactus-based networks through decomposition into |$(n-1)$| -dimensional subsystems by fixing one position-pair. Numerical simulations show that the upper bound derived in this way is close to the approximation of the accurate subsystem reliability.},
  archive      = {J_COMJNL},
  author       = {Liu, Xiaoqing and Zhou, Shuming and Liu, Jiafei and Zhang, Hong},
  doi          = {10.1093/comjnl/bxac163},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {142-152},
  shortjournal = {Comput. J.},
  title        = {Reliability analysis of the cactus-based networks based on subsystem},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Key node identification method integrating information
transmission probability and path diversity in complex network.
<em>COMJNL</em>, <em>67</em>(1), 127–141. (<a
href="https://doi.org/10.1093/comjnl/bxac162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous key node identification approaches assume that the transmission of information on a path always ends positively, which is not necessarily true. In this paper, we propose a new centrality index called Information Rank (IR for short) that associates each path with a score specifying the probability that such path successfully conveys a message. The IR method generates all the shortest paths of any arbitrary length coming out from a node |$u$| and defines the centrality of u as the sum of the scores of all the shortest paths exiting |$u$|⁠ . The IR algorithm is more robust than other centrality indexes based on shortest paths because it uses alternative paths in its computation, and it is computationally efficient because it relies on a Beadth First Search-BFS to generate all shortest paths. We validated the IR algorithm on nine real networks and compared its ability to identify super-spreaders (i.e. nodes capable of spreading an infection in a real network better than others) with five popular centrality indices such as Degree, Betweenness, K-Shell, DynamicRank and PageRank. Experimental results highlight the clear superiority of IR over all considered competitors.},
  archive      = {J_COMJNL},
  author       = {Liu, Xiaoyang and Gao, Luyuan and Fiumara, Giacomo and De Meo, Pasquale},
  doi          = {10.1093/comjnl/bxac162},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {127-141},
  shortjournal = {Comput. J.},
  title        = {Key node identification method integrating information transmission probability and path diversity in complex network},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed partial simulation for graph pattern matching.
<em>COMJNL</em>, <em>67</em>(1), 110–126. (<a
href="https://doi.org/10.1093/comjnl/bxac161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pattern matching in big graphs is important for different modern applications. Recently, this problem was defined in terms of multiple extensions of graph simulation , to reduce complexity and capture more meaningful results. These results were achieved through the relaxation of commonly used constraint in subgraph isomorphism pattern matching. Nevertheless, these graph simulation variant models are still too strict to provide results in many cases, especially when analyzed graphs contain anomalies and incomplete information. To deal with this issue, we introduce a new graph pattern matching (GPM) method, called partial simulation , capable of retrieving matches despite missing parts of the pattern graph, such as vertices and/or edges. Furthermore, considering the number and inequality of the outputs, we define a relevance function to compute a value expressing how each match vertex respects the pattern graph. Similarly, we define partial dual simulation GPM that returns vertices that satisfy a part of the dual simulation constraints and assigns a relevance value to them. Additionally, we provide distributed scalable algorithms to evaluate the proposed partial simulation methods based on the distributed vertex-centric programming paradigm. Finally, our experiments on real-world data graphs demonstrate the effectiveness of the proposed models and the efficiency of their associated algorithms.},
  archive      = {J_COMJNL},
  author       = {Aouar, Aissam and Yahiaoui, Saïd and Sadeg, Lamia and Nouali-Taboudjemat, Nadia and Beghdad Bey, Kadda},
  doi          = {10.1093/comjnl/bxac161},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {110-126},
  shortjournal = {Comput. J.},
  title        = {Distributed partial simulation for graph pattern matching},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Near real-time social distance estimation in london.
<em>COMJNL</em>, <em>67</em>(1), 95–109. (<a
href="https://doi.org/10.1093/comjnl/bxac160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To mitigate the current COVID-19 pandemic, policy makers at the Greater London Authority, the regional governance body of London, UK, are reliant upon prompt, accurate and actionable estimations of lockdown and social distancing policy adherence. Transport for London, the local transportation department, reports they implemented over 700 interventions such as greater signage and expansion of pedestrian zoning at the height of the pandemic’s first wave with our platform providing key data for those decisions. Large well-defined heterogeneous compositions of pedestrian footfall and physical proximity are difficult to acquire, yet necessary to monitor city-wide activity (busyness) and consequently discern actionable policy decisions. To meet this challenge, we leverage our existing large-scale data processing urban air quality machine learning infrastructure to process over 900 camera feeds in near real-time to generate new estimates of social distancing adherence, group detection and camera stability. In this work, we describe our development and deployment of a computer vision and machine learning pipeline. It provides near immediate sampling and contextualization of activity and physical distancing on the streets of London via live traffic camera feeds. We introduce a platform for inspecting, calibrating and improving upon existing methods, describe the active deployment on real-time feeds and provide analysis over an 18 month period.},
  archive      = {J_COMJNL},
  author       = {Walsh, James and Kesa, Oluwafunmilola and Wang, Andrew and Ilas, Mihai and O’Hara, Patrick and Giles, Oscar and Dhir, Neil and Girolami, Mark and Damoulas, Theodoros},
  doi          = {10.1093/comjnl/bxac160},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {95-109},
  shortjournal = {Comput. J.},
  title        = {Near real-time social distance estimation in london},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Communication-aware energy consumption model in
heterogeneous computing systems. <em>COMJNL</em>, <em>67</em>(1), 78–94.
(<a href="https://doi.org/10.1093/comjnl/bxac159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large heterogeneous computing systems are composed of conventional central processing units and graphics processing units (GPUs) where communication plays a crucial role for system performance. This paper presents an energy consumption analytical model in terms of communication perception for the communication–computing pipeline characterization of discrete GPUs systems. We propose a dynamically adaptive energy-efficient task assignment approach, which harnesses particle swarm optimization. Static energy optimization is addressed by optimal task partition granularity. The experimental results demonstrate that the communication-based energy optimization algorithms can be more energy-saving than those without communication consideration. For some application benchmarks, the energy consumption can be saved by up to 31%. This implies the potential that the energy-saving optimization methods can be incorporated in system engineering processes.},
  archive      = {J_COMJNL},
  author       = {Wang, Zhuowei and Wang, Hao and Song, Xiaoyu and Wu, JiaHui},
  doi          = {10.1093/comjnl/bxac159},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {78-94},
  shortjournal = {Comput. J.},
  title        = {Communication-aware energy consumption model in heterogeneous computing systems},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An asynchronous maximum independent set algorithm by myopic
luminous robots on grids. <em>COMJNL</em>, <em>67</em>(1), 57–77. (<a
href="https://doi.org/10.1093/comjnl/bxac158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of constructing a maximum independent set with mobile myopic luminous robots on a grid network whose size is finite but unknown to the robots. In this setting, the robots enter the grid network one by one from a corner of the grid, and they eventually have to be disseminated on the grid nodes so that the occupied positions form a maximum independent set of the network. We assume that robots are asynchronous, anonymous, silent and they execute the same distributed algorithm. In this paper, we propose two algorithms: The first one assumes that the number of light colors of each robot is three and the visible range is two, but uses the additional assumption that a local edge-labeling exists for each node. To remove this assumption, the second one assumes that the number of light colors of each robot is seven, and that the visible range is three. In both algorithms, the number of movements is |$O(n(L+l))$| steps, where |$n$| is the number of nodes and |$L$| and |$l$| are the grid dimensions.},
  archive      = {J_COMJNL},
  author       = {Kamei, Sayaka and Tixeuil, Sébastien},
  doi          = {10.1093/comjnl/bxac158},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {57-77},
  shortjournal = {Comput. J.},
  title        = {An asynchronous maximum independent set algorithm by myopic luminous robots on grids},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Link prediction based on local structure and node
information along local paths. <em>COMJNL</em>, <em>67</em>(1), 45–56.
(<a href="https://doi.org/10.1093/comjnl/bxac157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction aims at predicting the missing links or new links based on known topological or attribute information of networks, which is one of the most significant and challenging tasks in complex network analysis. Recently, many local similarity-based methods have been proposed and they performed well in most cases. However, most of these methods simultaneously ignore the contributions of the local structure information between endpoints and their common neighbors, as well as transmission abilities of different 3-hop paths. To address these issues, in this paper, we propose a novel link prediction method that aims at improving the prediction accuracy of the existing local similarity-based methods by integrating with local structure information and node degree information along 3-hop paths. Extensive experiments have been performed on nine real-world networks and the results demonstrate that our proposed method is superior to the existing state-of-the-art methods.},
  archive      = {J_COMJNL},
  author       = {Li, Tongfeng and Zhang, Ruisheng and Niu, Bojuan and Yao, Yabing and Ma, Jun and Jiang, Jing and Zhao, Zhili},
  doi          = {10.1093/comjnl/bxac157},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {45-56},
  shortjournal = {Comput. J.},
  title        = {Link prediction based on local structure and node information along local paths},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dynamic heterogeneous graph convolution network for
traffic flow prediction. <em>COMJNL</em>, <em>67</em>(1), 31–44. (<a
href="https://doi.org/10.1093/comjnl/bxac156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic prediction has attracted a lot of attention in recent years. However, it is challenging due to the dynamic and heterogeneous spatial–temporal correlations. Most of the existing methods adopt the attention mechanism to capture the dynamic spatial features, but it is difficult for the attention mechanism to learn the real-time change of spatial dependencies, which restrict accurate spatial dependencies learning. Furthermore, most methods are out at elbows when solving heterogeneous spatial–temporal data. To overcome these problems, we propose a novel traffic prediction model called Dynamic spatial–temporal Heterogeneous Graph Convolution Network. Different from the existing methods, we have designed a dynamic localized graph and a corresponding adaptive localized graph convolution network, which are capable of simultaneously capturing dynamic and heterogeneous spatial correlations. We propose a gated adaptive temporal convolution network to capture the temporal heterogeneity of traffic data and enjoy global receptive fields. Finally, a global correlations fusion network is provided to incorporate the global spatial–temporal correlations. Compared with 11 baselines, our proposed model achieves state-of-the-art performance in the accuracy of prediction.},
  archive      = {J_COMJNL},
  author       = {Li, He and Jin, Duo and Li, XueJiao and Qiao, Shaojie},
  doi          = {10.1093/comjnl/bxac156},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {31-44},
  shortjournal = {Comput. J.},
  title        = {A dynamic heterogeneous graph convolution network for traffic flow prediction},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A system for storing anonymous patient healthcare data using
blockchain and its applications. <em>COMJNL</em>, <em>67</em>(1), 18–30.
(<a href="https://doi.org/10.1093/comjnl/bxac155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a system is proposed which uses blockchain technology in healthcare. In this system, patients can access their health records anytime from anywhere. Moreover, the patients’ health records are put into the blockchain anonymously. Whenever a patient visits a healthcare professional, the authorized entity filters patients’ medical report out by eliminating the patients’ sensitive information. Then, the filtered medical data are put into an off-chain database, while the address of the data is put into the blockchain with an assigned pseudo random identity of the patient. Thus, there are multi pseudo random identities for each patient. Unlike previous studies where the patients’ identities/reports were linkable, in the proposed protocol the patients’ identities are not linkable. The proposed system can also be used to show patients’ health status to some entities when a pandemic happens (e.g. COVID-19). During the COVID-19 pandemic, the patients are required to show their series of vaccinations before they travel internationally/nationally or participate in some social events. To travel or join some events, the patient needs to show only a partial medical history to the security guard without leaking any private information. Furthermore, once the anonymous medical data are put into the off-chain database, the data can be used for data mining and machine learning.},
  archive      = {J_COMJNL},
  author       = {Oksuz, Ozgur},
  doi          = {10.1093/comjnl/bxac155},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {18-30},
  shortjournal = {Comput. J.},
  title        = {A system for storing anonymous patient healthcare data using blockchain and its applications},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep hashing and sparse representation of abnormal events
detection. <em>COMJNL</em>, <em>67</em>(1), 3–17. (<a
href="https://doi.org/10.1093/comjnl/bxac152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its widespread application in the field of public security, anomaly detection in crowd scenes has recently become a hot topic. Some deep learning-based methods led to significant accomplishments in this field. Nevertheless, due to the scarcity of data and the misclassification of queries which most of them suffer to some extent from a sudden and infrequent overfitting. Though, we tried to solve the above problems, understand the long video streams and establish an accurate and reliable security system in order to improve its performance in detecting anomalies. We also referred to the hash technique, which has proven to be the most efficient method used when researching about large-scale image recovery. Thus, this article offers a smart video anomaly detection solution. In this paper, we combine the advantages of both deep hashing and deep auto-encoders to show that tracking changes in deep hash components across time and can be used to detect local anomalies. More precisely, we start with a new technique to minimize the mass of input data and information in order to decrease the time of calculation using a new dynamic frame skipping technique. Then, we propose to measure local anomalies by combining semantic with low-level optical flows to balance the performance and perceptibility. The experimental results illustrate that the proposed methods surpass these baselines for the detection and localization of anomalies.},
  archive      = {J_COMJNL},
  author       = {Gnouma, Mariem and Ejbali, Ridha and Zaied, Mourad},
  doi          = {10.1093/comjnl/bxac152},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {3-17},
  shortjournal = {Comput. J.},
  title        = {Deep hashing and sparse representation of abnormal events detection},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Thematic editorial: Mostly artificial intelligence (AI) or
machine learning (ML) now in the engine room, in pursuit of a green
agenda. <em>COMJNL</em>, <em>67</em>(1), 1–2. (<a
href="https://doi.org/10.1093/comjnl/bxad115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  author       = {Angelides, Marios C.},
  doi          = {10.1093/comjnl/bxad115},
  journal      = {The Computer Journal},
  month        = {1},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Comput. J.},
  title        = {Thematic editorial: Mostly artificial intelligence (AI) or machine learning (ML) now in the engine room, in pursuit of a green agenda},
  volume       = {67},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
