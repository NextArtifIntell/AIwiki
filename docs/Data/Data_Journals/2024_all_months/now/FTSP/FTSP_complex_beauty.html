<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>FTSP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ftsp---3">FTSP - 3</h2>
<ul>
<li><details>
<summary>
(2024). Min-max framework for majorization-minimization algorithms
in signal processing applications: An overview. <em>FTSP</em>,
<em>18</em>(4), 310–389. (<a
href="https://doi.org/10.1561/2000000129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This monograph presents a theoretical background and a broad introduction to the Min-Max Framework for Majorization-Minimization (MM4MM), an algorithmic methodology for solving minimization problems by formulating them as min-max problems and then employing majorization-minimization. The monograph lays out the mathematical basis of the approach used to reformulate a minimization problem as a min-max problem. With the prerequisites covered, including multiple illustrations of the formulations for convex and non-convex functions, this work serves as a guide for developing MM4MM-based algorithms for solving non-convex optimization problems in various areas of signal processing. As special cases, we discuss using the majorization-minimization technique to solve min-max problems encountered in signal processing applications and min-max problems formulated using the Lagrangian. Lastly, we present detailed examples of using MM4MM in ten signal processing applications such as phase retrieval, source localization, independent vector analysis, beamforming and optimal sensor placement in wireless sensor networks. The devised MM4MM algorithms are free of hyper-parameters and enjoy the advantages inherited from the use of the majorization-minimization technique such as monotonicity.},
  archive      = {J_FTSP},
  author       = {Astha Saini and Petre Stoica and Prabhu Babu and Aakash Arora},
  doi          = {10.1561/2000000129},
  journal      = {Foundations and Trends® in Signal Processing},
  month        = {11},
  number       = {4},
  pages        = {310-389},
  shortjournal = {Found. Trends Signal Process.},
  title        = {Min-max framework for majorization-minimization algorithms in signal processing applications: An overview},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal deep learning: Encouraging impact on real-world
problems through causality. <em>FTSP</em>, <em>18</em>(3), 200–309. (<a
href="https://doi.org/10.1561/2000000123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causality has the potential to truly transform the way we solve a large number of real-world problems. Yet, so far, its potential largely remains to be unlocked as causality often requires crucial assumptions which cannot be tested in practice. To address this challenge, we propose a new way of thinking about causality-– we call this causal deep learning . Our causal deep learning framework spans three dimensions: (1) a structural dimension, which incorporates partial yet testable causal knowledge rather than assuming either complete or no causal knowledge among the variables of interest; (2) a parametric dimension, which encompasses parametric forms that capture the type of relationships among the variables of interest; and (3) a temporal dimension, which captures exposure times or how the variables of interest interact (possibly causally) over time. Our CDL framework enables us to precisely categorise and compare causal statistical learning methods. We use this categorisation to provide a comprehensive review of the CDL field. More importantly, CDL enables us to make progress on a variety of real-world problems by aiding us to leverage partial causal knowledge (including independencies among variables) and quantitatively characterising causal relationships among variables of interest (possibly over time). Our framework clearly identifies which assumptions are testable and which are not, so the resulting solutions can be judiciously adopted in practice. Our formulation helps us to combine or chain causal representations to solve specific problems without losing track of which assumptions are required to build these solutions, pushing real-world impact in healthcare, economics and business, environmental sciences and education, through causal deep learning.},
  archive      = {J_FTSP},
  author       = {Jeroen Berrevoets and Krzysztof Kacprzyk and Zhaozhi Qian and Mihaela van der Schaar},
  doi          = {10.1561/2000000123},
  journal      = {Foundations and Trends® in Signal Processing},
  month        = {7},
  number       = {3},
  pages        = {200-309},
  shortjournal = {Found. Trends Signal Process.},
  title        = {Causal deep learning: Encouraging impact on real-world problems through causality},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy-based models with applications to speech and language
processing. <em>FTSP</em>, <em>18</em>(1-2), 1–199. (<a
href="https://doi.org/10.1561/2000000117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy-Based Models (EBMs) are an important class of probabilistic models, also known as random fields and undirected graphical models. EBMs are un-normalized and thus radically different from other popular self-normalized probabilistic models such as hidden Markov models (HMMs), autoregressive models, generative adversarial nets (GANs) and variational auto-encoders (VAEs). During these years, EBMs have attracted increasing interest not only from core machine learning but also from application domains such as speech, vision, natural language processing (NLP) and so on, with significant theoretical and algorithmic progress. To the best of our knowledge, there are no review papers about EBMs with applications to speech and language processing. The sequential nature of speech and language also presents special challenges and needs treatment different from processing fix-dimensional data (e.g., images). The purpose of this monograph is to present a systematic introduction to energy-based models, including both algorithmic progress and applications in speech and language processing, which is organized into four main sections. First, we will introduce basics for EBMs, including classic models, recent models parameterized by neural networks, sampling methods, and various learning methods from the classic learning algorithms to the most advanced ones. The next three sections will present how to apply EBMs in three different scenarios, i.e., for modeling marginal, conditional and joint distributions, respectively. 1) EBMs for sequential data with applications in language modeling, where we are mainly concerned with the marginal distribution of a sequence itself; 2) EBMs for modeling conditional distributions of target sequences given observation sequences, with applications in speech recognition, sequence labeling and text generation; 3) EBMs for modeling joint distributions of both sequences of observations and targets, and their applications in semi-supervised learning and calibrated natural language understanding. In addition, we will introduce some open-source toolkits to help the readers to get familiar with the techniques for developing and applying energy-based models.},
  archive      = {J_FTSP},
  author       = {Zhijian Ou},
  doi          = {10.1561/2000000117},
  journal      = {Foundations and Trends® in Signal Processing},
  month        = {3},
  number       = {1-2},
  pages        = {1-199},
  shortjournal = {Found. Trends Signal Process.},
  title        = {Energy-based models with applications to speech and language processing},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
