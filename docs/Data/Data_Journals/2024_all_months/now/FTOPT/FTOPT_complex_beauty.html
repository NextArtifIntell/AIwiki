<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>FTOPT_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ftopt---6">FTOPT - 6</h2>
<ul>
<li><details>
<summary>
(2024). Multi-agent online optimization. <em>FTOPT</em>,
<em>7</em>(2-3), 81–263. (<a
href="https://doi.org/10.1561/2400000037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This monograph provides an overview of distributed online optimization in multi-agent systems. Online optimization approaches planning and decision problems from a robust learning perspective, where one learns through feedback from sequentially arriving costs, resembling a game between a learner (agent) and the environment. Recently, multi-agent systems have become important in diverse areas including smart power grids, communication networks, machine learning, and robotics, where agents work with decentralized data, costs, and decisions to collectively minimize a system-wide cost. In such settings, agents make distributed decisions and collaborate with neighboring agents through a communication network, leading to scalable solutions that often perform as well as centralized methods. The monograph offers a unified introduction, starting with fundamental algorithms for basic problems, and gradually covering state-of-the-art techniques for more complex settings. The interplay between individual agent learning rates, network structure, and communication complexity is highlighted in the overall system performance.},
  archive      = {J_FTOPT},
  author       = {Deming Yuan and Alexandre Proutiere and Guodong Shi},
  doi          = {10.1561/2400000037},
  journal      = {Foundations and Trends® in Optimization},
  month        = {12},
  number       = {2-3},
  pages        = {81-263},
  shortjournal = {Found. Trends Optim.},
  title        = {Multi-agent online optimization},
  volume       = {7},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An invitation to deep reinforcement learning.
<em>FTOPT</em>, <em>7</em>(1), 1–80. (<a
href="https://doi.org/10.1561/2400000049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training a deep neural network to maximize a target objective has become the standard recipe for successful machine learning over the last decade. These networks can be optimized with supervised learning if the target objective is differentiable. However, this is not the case for many interesting problems. Common objectives like intersection over union (IoU), and bilingual evaluation understudy (BLEU) scores or rewards cannot be optimized with supervised learning. A common workaround is to define differentiable surrogate losses, leading to suboptimal solutions with respect to the actual objective. Reinforcement learning (RL) has emerged as a promising alternative for optimizing deep neural networks to maximize non-differentiable objectives in recent years. Examples include aligning large language models via human feedback, code generation, object detection or control problems. This makes RL techniques relevant to the larger machine learning audience. The subject is, however, timeintensive to approach due to the large range of methods, as well as the often highly theoretical presentation. This monograph takes an alternative approach that is different from classic RL textbooks. Rather than focusing on tabular problems, we introduce RL as a generalization of supervised learning, which we first apply to non-differentiable objectives and later to temporal problems. Assuming only basic knowledge of supervised learning, the reader will be able to understand state-of-the-art deep RL algorithms like proximal policy optimization (PPO) after reading this tutorial.},
  archive      = {J_FTOPT},
  author       = {Bernhard Jaeger and Andreas Geiger},
  doi          = {10.1561/2400000049},
  journal      = {Foundations and Trends® in Optimization},
  month        = {12},
  number       = {1},
  pages        = {1-80},
  shortjournal = {Found. Trends Optim.},
  title        = {An invitation to deep reinforcement learning},
  volume       = {7},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Constrained reinforcement learning with average reward
objective: Model-based and model-free algorithms. <em>FTOPT</em>,
<em>6</em>(4), 193–298. (<a
href="https://doi.org/10.1561/2400000038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement Learning (RL) serves as a versatile framework for sequential decision-making, finding applications across diverse domains such as robotics, autonomous driving, recommendation systems, supply chain optimization, biology, mechanics, and finance. The primary objective of these applications is to maximize the average reward. Real-world scenarios often necessitate adherence to specific constraints during the learning process. This monograph focuses on the exploration of various modelbased and model-free approaches for Constrained RL within the context of average reward Markov Decision Processes (MDPs). The investigation commences with an examination of model-based strategies, delving into two foundational methods – optimism in the face of uncertainty and posterior sampling. Subsequently, the discussion transitions to parametrized model-free approaches, where the primal dual policy gradient-based algorithm is explored as a solution for constrained MDPs. The monograph provides regret guarantees and analyzes constraint violation for each of the discussed setups. For the above exploration, we assume the underlying MDP to be ergodic. Further, this monograph extends its discussion to encompass results tailored for weakly communicating MDPs, thereby broadening the scope of its findings and their relevance to a wider range of practical scenarios.},
  archive      = {J_FTOPT},
  author       = {Vaneet Aggarwal and Washim Uddin Mondal and Qinbo Bai},
  doi          = {10.1561/2400000038},
  journal      = {Foundations and Trends® in Optimization},
  month        = {8},
  number       = {4},
  pages        = {193-298},
  shortjournal = {Found. Trends Optim.},
  title        = {Constrained reinforcement learning with average reward objective: Model-based and model-free algorithms},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stochastic optimization methods for policy evaluation in
reinforcement learning. <em>FTOPT</em>, <em>6</em>(3), 145–192. (<a
href="https://doi.org/10.1561/2400000045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This monograph introduces various value-based approaches for solving the policy evaluation problem in the online reinforcement learning (RL) scenario, which aims to learn the value function associated with a specific policy under a single Markov decision process (MDP). Approaches vary depending on whether they are implemented in an on-policy or off-policy manner: In on-policy settings, where the evaluation of the policy is conducted using data generated from the same policy that is being assessed, classical techniques such as TD(0), TD(λ), and their extensions with function approximation or variance reduction are employed in this setting. For off-policy evaluation, where samples are collected under a different behavior policy, this monograph introduces gradient-based two-timescale algorithms like GTD2, TDC, and variance-reduced TDC. These algorithms are designed to minimize the mean-squared projected Bellman error (MSPBE) as the objective function. This monograph also discusses their finite-sample convergence upper bounds and sample complexity.},
  archive      = {J_FTOPT},
  author       = {Yi Zhou and Shaocong Ma},
  doi          = {10.1561/2400000045},
  journal      = {Foundations and Trends® in Optimization},
  month        = {8},
  number       = {3},
  pages        = {145-192},
  shortjournal = {Found. Trends Optim.},
  title        = {Stochastic optimization methods for policy evaluation in reinforcement learning},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Numerical methods for convex multistage stochastic
optimization. <em>FTOPT</em>, <em>6</em>(2), 63–144. (<a
href="https://doi.org/10.1561/2400000044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization problems involving sequential decisions in a stochastic environment were studied in Stochastic Programming (SP), Stochastic Optimal Control (SOC) and Markov Decision Processes (MDP). In this monograph, we mainly concentrate on SP and SOC modeling approaches. In these frameworks, there are natural situations when the considered problems are convex. The classical approach to sequential optimization is based on dynamic programming. It has the problem of the so-called “curse of dimensionality”, in that its computational complexity increases exponentially with respect to the dimension of state variables. Recent progress in solving convex multistage stochastic problems is based on cutting plane approximations of the cost-to-go (value) functions of dynamic programming equations. Cutting plane type algorithms in dynamical settings is one of the main topics of this monograph. We also discuss stochastic approximation type methods applied to multistage stochastic optimization problems. From the computational complexity point of view, these two types of methods seem to be complementary to each other. Cutting plane type methods can handle multistage problems with a large number of stages but a relatively smaller number of state (decision) variables. On the other hand, stochastic approximation type methods can only deal with a small number of stages but a large number of decision variables.},
  archive      = {J_FTOPT},
  author       = {Guanghui Lan and Alexander Shapiro},
  doi          = {10.1561/2400000044},
  journal      = {Foundations and Trends® in Optimization},
  month        = {5},
  number       = {2},
  pages        = {63-144},
  shortjournal = {Found. Trends Optim.},
  title        = {Numerical methods for convex multistage stochastic optimization},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A tutorial on hadamard semidifferentials. <em>FTOPT</em>,
<em>6</em>(1), 1–62. (<a
href="https://doi.org/10.1561/2400000041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Hadamard semidifferential is more general than the Fréchet differential now dominant in undergraduate mathematics education. By slightly changing the definition of the forward directional derivative, the Hadamard semidifferential rescues the chain rule, enforces continuity, and permits differentiation across maxima and minima. It also plays well with convex analysis and naturally extends differentiation to smooth embedded submanifolds, topological vector spaces, and metric spaces of shapes and geometries. The current elementary exposition focuses on the more familiar territory of analysis in Euclidean spaces and applies the semidifferential to some representative problems in optimization and statistics. These include algorithms for proximal gradient descent, steepest descent in matrix completion, and variance components models.},
  archive      = {J_FTOPT},
  author       = {Kenneth Lange},
  doi          = {10.1561/2400000041},
  journal      = {Foundations and Trends® in Optimization},
  month        = {5},
  number       = {1},
  pages        = {1-62},
  shortjournal = {Found. Trends Optim.},
  title        = {A tutorial on hadamard semidifferentials},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
