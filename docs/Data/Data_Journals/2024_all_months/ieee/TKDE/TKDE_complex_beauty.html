<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TKDE_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tkde---666">TKDE - 666</h2>
<ul>
<li><details>
<summary>
(2024). ZBTree: A fast and scalable b<span
class="math inline"><sup>+</sup></span>&lt;mml:math
xmlns:mml=“http://www.w3.org/1998/math/MathML”&gt;&lt;mml:msup&gt;&lt;mml:mrow/&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;-tree
for persistent memory. <em>TKDE</em>, <em>36</em>(12), 9547–9563. (<a
href="https://doi.org/10.1109/TKDE.2024.3421232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present the design and implementation of ZBTree, a hotness-aware B $^+$ -Tree for persistent memory (PMem). ZBTree leverages the PMem+DRAM architecture, which is featured with a volatile operation layer to accelerate data access and an order-preserving persistent layer to achieve fast recovery and low-overhead consistency and persistence guarantees. The operation layer contains inner nodes for indexing and compacted leaf nodes (DLeaves) that hold metadata. Based on leaf node compaction, we present a data lodging method, which supports to load hot data into fast DRAM dynamically, avoiding PMem accesses for subsequent reads of hot data and achieving improved read performance without incurring extra DRAM usage. In addition, we present a lightweight node splitting mechanism with constant persistence overhead that does not vary with node size. Our extensive evaluations show that ZBTree achieves higher throughput by a factor of 1.4x-6.3x compared to state-of-the-art tree indexes under a wide range of workloads. Meanwhile, ZBTree achieves comparable or faster recovery speed compared to existing designs.},
  archive      = {J_TKDE},
  author       = {Wenkui Che and Zhiwen Chen and Daokun Hu and Jianhua Sun and Hao Chen},
  doi          = {10.1109/TKDE.2024.3421232},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9547-9563},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {ZBTree: a fast and scalable b$^+$&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow/&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;-tree for persistent memory},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). When quantum computing meets database: A hybrid sampling
framework for approximate query processing. <em>TKDE</em>,
<em>36</em>(12), 9532–9546. (<a
href="https://doi.org/10.1109/TKDE.2024.3480278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum computing represents a next-generation technology in data processing, promising to transcend the limitations of traditional computation. In this paper, we undertake an early exploration of the potential integration of quantum computing with database query optimization. We introduce a pioneering hybrid classical-quantum algorithm for sampling-based approximate query processing (AQP). The core concept of the algorithm revolves around identifying rare groups, which often follow a long-tail distribution, and applying distinct sampling methodologies to normal and rare groups. By leveraging the quantum capabilities of the diffusion gate and QRAM, the algorithm defines a novel quantum sampling approach that iteratively amplifies the signals of these infrequent groups. The algorithm operates without the need for preprocessing or prior knowledge of workloads or data. It utilizes the power of quadratic acceleration to achieve well-balanced sampling across various data categories. Experimental results demonstrate that in the context of AQP, the new sampling scheme provides higher accuracy at the same sampling cost. Additionally, the benefits of quantum computing become more pronounced as query selectivity increases.},
  archive      = {J_TKDE},
  author       = {Sai Wu and Meng Shi and Dongxiang Zhang and Junbo Zhao and Gongsheng Yuan and Gang Chen},
  doi          = {10.1109/TKDE.2024.3480278},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9532-9546},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {When quantum computing meets database: A hybrid sampling framework for approximate query processing},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). When learned indexes meet persistent memory: The analysis
and the optimization. <em>TKDE</em>, <em>36</em>(12), 9517–9531. (<a
href="https://doi.org/10.1109/TKDE.2023.3342825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emerging persistent memory (PM) is increasingly being leveraged to construct high-performance and persistent indexes. By exploiting data distribution, recent learned indexes open up a new index design paradigm. Some prior studies try to refit the learned index according to the features of PM. However, they neglect to analyze the performance of existing learned index schemes on PM. In this paper, we provide a comprehensive analysis of learned indexes on PM and propose two optimization methods to improve the performance. In particular, we evaluate ALEX, PGM-index, and XIndex after converting them to persistent indexes. With appropriate modifications, some design choices of volatile learned index still show favorable performance on PM under workloads with simple data distribution. But they perform poorly when the data distribution becomes complex. According to the experiment results, we summarize some instructive insights and optimize persistent learned indexes for complex data distributions with two methods: 1) a cost-based insertion pattern selection to minimize PM writes and 2) recoverable internal nodes selective persistence to decrease the overhead of internal lookups. Our evaluations demonstrate the performance of optimized ALEX is 2.09x/1.53x of the original ALEX in insert/search. Meanwhile, it also outperforms the specific-designed persistent learned index.},
  archive      = {J_TKDE},
  author       = {Lixiao Cui and Yijing Luo and Yusen Li and Gang Wang and Xiaoguang Liu},
  doi          = {10.1109/TKDE.2023.3342825},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9517-9531},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {When learned indexes meet persistent memory: The analysis and the optimization},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised graph representation learning beyond aggregated
view. <em>TKDE</em>, <em>36</em>(12), 9504–9516. (<a
href="https://doi.org/10.1109/TKDE.2024.3418576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised graph representation learning aims to condense graph information into dense vector embeddings to support various downstream tasks. To achieve this goal, existing UGRL approaches mainly adopt the message-passing mechanism to simultaneously incorporate graph topology and node attribute with an aggregated view. However, recent research points out that this direct aggregation may lead to issues such as over-smoothing and/or topology distortion, as topology and node attribute of totally different semantics. To address this issue, this paper proposes a novel Graph Dual-view AutoEncoder framework (GDAE) which introduces the node-wise view for an individual node beyond the traditional aggregated view for aggregation of connected nodes. Specifically, the node-wise view captures the unique characteristics of individual node through a decoupling design, i.e., topology encoding by multi-steps random walk while preserving node-wise individual attribute. Meanwhile, the aggregated view aims to better capture the collective commonality among long-range nodes through an enhanced strategy, i.e., topology masking then attribute aggregation. Extensive experiments on 5 synthetic and 11 real-world benchmark datasets demonstrate that GDAE achieves the best results with up to 49.5% and 21.4% relative improvement in node degree prediction and cut-vertex detection tasks and remains top in node classification and link prediction tasks.},
  archive      = {J_TKDE},
  author       = {Jian Zhou and Jiasheng Li and Li Kuang and Ning Gui},
  doi          = {10.1109/TKDE.2024.3418576},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9504-9516},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Unsupervised graph representation learning beyond aggregated view},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uni-modal event-agnostic knowledge distillation for
multimodal fake news detection. <em>TKDE</em>, <em>36</em>(12),
9490–9503. (<a href="https://doi.org/10.1109/TKDE.2024.3477977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid expansion of multimodal content in online social media, automatic detection of multimodal fake news has received much attention. Multimodal joint training commonly used in existing methods is expected to benefit from thoroughly leveraging cross-modal features, yet these methods still suffer from insufficient learning of uni-modal features. Due to the heterogeneity of multimodal networks, optimizing a single objective will inevitably make the models prone to rely on specific modality while leaving other modalities under-optimized. On the other hand, simply expecting each modality to play a significant role in identifying all the rumors is also not appropriate as the multimodal fake news often involves tampering in only one modality. Therefore, how to find the genuine tampering on the per-sample basis becomes the key point to unlock the full power of each modality in a good collaborative manner. To address these issues, we propose a U ni-modal E vent-agnostic K nowledge D istillation framework (UEKD), which aims to transfer knowledge contained in the fine-grained prediction from uni-modal teachers to the multimodal student model through modality-specific distillation. Specifically, we find that the uni-modal teachers simply trained on the whole training set are easy to memorize the event-specific noise information to make a correct but biased prediction, failing to reflect the genuine degree of tampering in each modality. To tackle this problem, we propose to train and validate the teacher models on different domains in training dataset through a cross-validation manner, as the predictions from the out-of-domain teachers can be regarded as event-agnostic knowledge without spurious connections with event-specific information. Finally, to balance the convergence speeds across modalities, we dynamically monitor the involvement of each modality during training, through which we could identify the more under-optimized modalities and re-weight the distillation loss accordingly. Our method could be served as a plug-and-play module for existing multimodal fake news detection backbones. Extensive experiments on three public datasets and four state-of-the-art fake news detection backbones show that our proposed method can improve the performance by a large margin.},
  archive      = {J_TKDE},
  author       = {Guofan Liu and Jinghao Zhang and Qiang Liu and Junfei Wu and Shu Wu and Liang Wang},
  doi          = {10.1109/TKDE.2024.3477977},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9490-9503},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Uni-modal event-agnostic knowledge distillation for multimodal fake news detection},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Type-LDD: A type-driven lite concept drift detector for data
streams. <em>TKDE</em>, <em>36</em>(12), 9476–9489. (<a
href="https://doi.org/10.1109/TKDE.2023.3344602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept drift is a phenomenon that the distribution of data streams changes with time. When this happens, model predictions become less accurate. Hence, concept drift needs to be detected and adapted. Existing drift detection methods are good at determining when drift has occurred, but few retrieve information about how the drift came to be present in the stream, i.e., what type of drift has occurred. Hence, discussing the impact of the type of drift on adaptation is a difficult thing. To fill this gap, we propose a pre-trained framework for training a drift detector called a type-driven lite concept drift detector (Type-LDD) that retrieves information about both when and how a drift has occurred. In our proposed pre-trained framework, the Type-LDD including a drift-type identifier and a drift-point locator was based on a synthetic dataset containing a range of drift types. When repurposing the pre-trained model for detecting new data streams, a knowledge distillation module fine-tunes the proposed Type-LDD to speed up inference and keep detection accuracy. The proposed Type-LDD is validated on both synthetic data and real-world data, and demonstrated that accurately identifying the type of drift that has occurred can improve adaptation accuracy.},
  archive      = {J_TKDE},
  author       = {Hang Yu and Jinpeng Li and Jie Lu and Yiliao Song and Shaorong Xie and Guangquan Zhang},
  doi          = {10.1109/TKDE.2023.3344602},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9476-9489},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Type-LDD: A type-driven lite concept drift detector for data streams},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TripleSurv: Triplet time-adaptive coordinate learning
approach for survival analysis. <em>TKDE</em>, <em>36</em>(12),
9464–9475. (<a href="https://doi.org/10.1109/TKDE.2024.3450910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A core challenge in survival analysis is to model the distribution of time-to-event data, where the event of interest may be a death, failure, or occurrence of a specific event. Previous studies have showed that ranking and maximum likelihood estimation loss functions are widely-used learning approaches for survival analysis. However, ranking loss only focus on the ranking of survival time and does not consider potential effect of samples’ exact survival time values. Furthermore, the maximum likelihood estimation is unbounded and easily subject to outliers (e.g., censored data), which may cause poor performance of modeling. To handle the complexities of learning process and exploit valuable survival time values, we propose a time-adaptive coordinate loss function, TripleSurv, to achieve adaptive adjustments by introducing the differences in the survival time between sample pairs into the ranking, which can encourage the model to quantitatively rank relative risk of pairs, ultimately enhancing the accuracy of predictions. Most importantly, the TripleSurv is proficient in quantifying the relative risk between samples by ranking ordering of pairs, and consider the time interval as a trade-off to calibrate the robustness of model over sample distribution. Our TripleSurv is evaluated on three real-world survival datasets and a public synthetic dataset. The results show that our method outperforms the state-of-the-art methods and exhibits good model performance and robustness on modeling various sophisticated data distributions with different censor rates.},
  archive      = {J_TKDE},
  author       = {Liwen Zhang and Lianzhen Zhong and Fan Yang and Linglong Tang and Di Dong and Hui Hui and Jie Tian},
  doi          = {10.1109/TKDE.2024.3450910},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9464-9475},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {TripleSurv: Triplet time-adaptive coordinate learning approach for survival analysis},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Triple factorization-based SNLF representation with
improved momentum-incorporated AGD: A knowledge transfer approach.
<em>TKDE</em>, <em>36</em>(12), 9448–9463. (<a
href="https://doi.org/10.1109/TKDE.2024.3450469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symmetric, high-dimensional and sparse (SHiDS) networks usually contain rich knowledge regarding various patterns. To adequately extract useful information from SHiDS networks, a novel biased triple factorization-based (TF) symmetric and non-negative latent factor (SNLF) model is put forward by utilizing the transfer learning (TL) method, namely biased TL-incorporated TF-SNLF (BT $^{2}$ -SNLF) model. The proposed BT $^{2}$ -SNLF model mainly includes the following four ideas: 1) the implicit knowledge of the auxiliary matrix in the ternary rating domain is transferred to the target matrix in the numerical rating domain, facilitating the feature extraction; 2) two linear bias vectors are considered into the objective function to discover the knowledge describing the individual entity-oriented effect; 3) an improved momentum-incorporated additive gradient descent algorithm is developed to speed up the model convergence as well as guarantee the non-negativity of target SHiDS networks; and 4) a rigorous proof is provided to show that, under the assumption that the objective function is $L$ -smooth and $\mu$ -convex, when $t\geq t_{0}$ , the algorithm begins to descend and it can find an $\epsilon$ -solution within $O(ln((1+\frac{\mu L}{L(1+\mu )+8\mu })/\epsilon ))$ . Experimental results on six datasets from real applications demonstrate the effectiveness of our proposed T $^{2}$ -SNLF and BT $^{2}$ -SNLF models.},
  archive      = {J_TKDE},
  author       = {Ming Li and Yan Song and Derui Ding and Ran Sun},
  doi          = {10.1109/TKDE.2024.3450469},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9448-9463},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Triple factorization-based SNLF representation with improved momentum-incorporated AGD: A knowledge transfer approach},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Training recommenders over large item corpus with importance
sampling. <em>TKDE</em>, <em>36</em>(12), 9433–9447. (<a
href="https://doi.org/10.1109/TKDE.2023.3344657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By predicting a personalized ranking on a set of items, item recommendation helps users determine the information they need. While optimizing a ranking-focused loss is more in line with the objectives of item recommendation, previous studies have indicated that current sampling-based ranking methods don&#39;t always surpass non-sampling ones. This is because it is either inefficient to sample a pool of representative negatives for better generalization or challenging to gauge their contributions to ranking-focused losses accurately. To this end, we propose a novel weighted ranking loss, which weights each negative with the softmax probability based on model&#39;s predictive score. Our theoretical analysis suggests that optimizing this loss boosts the normalized discounted cumulative gain. Furthermore, it appears that this loss acts as an approximate analytic solution for adversarial training of personalized ranking. To improve optimization efficiency, we approximate the weighted ranking loss with self-normalized importance sampling and show that the loss has good generalization properties. To improve generalization, we further develop efficient cluster-based negative samplers based on clustering over item vectors, to decrease approximation error caused by the divergence between the proposal and the target distribution. Comprehensive evaluations on real-world datasets show that our methods remarkably outperform leading item recommendation algorithms.},
  archive      = {J_TKDE},
  author       = {Defu Lian and Zhenguo Gao and Xia Song and Yucheng Li and Qi Liu and Enhong Chen},
  doi          = {10.1109/TKDE.2023.3344657},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9433-9447},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Training recommenders over large item corpus with importance sampling},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards effective top-n hamming search via bipartite graph
contrastive hashing. <em>TKDE</em>, <em>36</em>(12), 9418–9432. (<a
href="https://doi.org/10.1109/TKDE.2024.3425891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searching on bipartite graphs serves as a fundamental task for various real-world applications, such as recommendation systems, database retrieval, and document querying. Conventional approaches rely on similarity matching in continuous euclidean space of vectorized node embeddings. To handle intensive similarity computation efficiently, hashing techniques for graph-structured data have emerged as a prominent research direction. However, despite the retrieval efficiency in Hamming space, previous studies have encountered catastrophic performance decay . To address this challenge, we investigate the problem of hashing with Graph Convolutional Network for effective Top-N search. Our findings indicate the learning effectiveness of incorporating hashing techniques within the exploration of bipartite graph reception fields, as opposed to simply treating hashing as post-processing to output embeddings. To further enhance the model performance, we advance upon these findings and propose B ipartite G raph C ontrastive H ashing ( BGCH+ ). BGCH+ introduces a novel dual augmentation approach to both intermediate information and hash code outputs in the latent feature spaces, thereby producing more expressive and robust hash codes within a dual self-supervised learning paradigm. Comprehensive empirical analyses on six real-world benchmarks validate the effectiveness of our dual feature contrastive learning in boosting the performance of BGCH+ compared to existing approaches.},
  archive      = {J_TKDE},
  author       = {Yankai Chen and Yixiang Fang and Yifei Zhang and Chenhao Ma and Yang Hong and Irwin King},
  doi          = {10.1109/TKDE.2024.3425891},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9418-9432},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards effective top-N hamming search via bipartite graph contrastive hashing},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards effective long-term wind power forecasting: A deep
conditional generative spatio-temporal approach. <em>TKDE</em>,
<em>36</em>(12), 9403–9417. (<a
href="https://doi.org/10.1109/TKDE.2024.3435859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately forecasting long-term future wind power is critical to achieve safe power grid integration. This problem is quite challenging due to wind power&#39;s high volatility and randomness. In this paper, we propose a novel time series forecasting method, namely Deep Conditional Generative Spatio-Temporal model (DCGST), and its high accuracy is achieved by tackling two critical issues simultaneously: a proper handling of the non-stationarity of multiple wind power time series, and a fine-grained modeling of their complicated yet dynamic spatio-temporal dependencies. Specifically, we first formally define the Spatio-Temporal Concept Drift (STCD) problem of wind power, and then we propose a novel deep conditional generative model to learn probabilistic distributions of future wind power values under STCD. Three different tailored neural networks are designed for distributions parameterization, including a graph-based prior network, an attention-based recognition network, and a stochastic seq2seq-based generation network. They are able to encode the dynamic spatio-temporal dependencies of multiple wind power time series and infer one-to-many mappings for future wind power generation. Compared to existing methods, DCGST can learn better spatio-temporal representations of wind power data and learn better uncertainties of data distribution to generate future values. Comprehensive experiments on real-world datasets including the largest public turbine-level wind power dataset verify the effectiveness, efficiency, generality and scalability of our method.},
  archive      = {J_TKDE},
  author       = {Peiyu Yi and Zhifeng Bao and Feihu Huang and Jince Wang and Jian Peng and Linghao Zhang},
  doi          = {10.1109/TKDE.2024.3435859},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9403-9417},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards effective long-term wind power forecasting: A deep conditional generative spatio-temporal approach},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). ThreatInsight: Innovating early threat detection through
threat-intelligence-driven analysis and attribution. <em>TKDE</em>,
<em>36</em>(12), 9388–9402. (<a
href="https://doi.org/10.1109/TKDE.2024.3474792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity and ongoing evolution of Advanced Persistent Threats (APTs) compromise the efficacy of conventional cybersecurity measures. Firewalls, intrusion detection systems, and antivirus software, which are dependent on static rules and predefined signatures, are increasingly ineffective against these sophisticated threats. Moreover, the use of system audit logs for threat hunting involves a retrospective review of cybersecurity incidents to reconstruct attack paths for attribution, which affects the timeliness and effectiveness of threat detection and response. Even when the attacker is identified, this method does not prevent cyber attacks. To address these challenges, we introduce ThreatInsight, a novel early-stage threat detection solution that minimizes reliance on system audit logs. ThreatInsight detects potential threats by analyzing IPs captured from HoneyPoints. These IPs are processed through threat data mining and threat feature modeling. By employing fact-based and semantic reasoning techniques based on the APT Threat Intelligence Knowledge Graph (APT-TI-KG), ThreatInsight identifies and attributes attackers. The system generates analysis reports detailing the threat knowledge concerning IPs and attributed attackers, equipping analysts with actionable insights and defense strategies. The system architecture includes modules for HoneyPoint IP extraction, Threat Intelligence (TI) data analysis, attacker attribution, and analysis report generation. ThreatInsight facilitates real-time analysis and the identification of potential threats at early stages, thereby enhancing the early detection capabilities of cybersecurity defense systems and improving overall threat detection and proactive defense effectiveness.},
  archive      = {J_TKDE},
  author       = {Ziyu Wang and Yinghai Zhou and Hao Liu and Jing Qiu and Binxing Fang and Zhihong Tian},
  doi          = {10.1109/TKDE.2024.3474792},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9388-9402},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {ThreatInsight: Innovating early threat detection through threat-intelligence-driven analysis and attribution},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). THCN: A hawkes process based temporal causal convolutional
network for extrapolation reasoning in temporal knowledge graphs.
<em>TKDE</em>, <em>36</em>(12), 9374–9387. (<a
href="https://doi.org/10.1109/TKDE.2024.3474051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal Knowledge Graphs (TKGs) serve as indispensable tools for dynamic facts storage and reasoning. However, predicting future facts in TKGs presents a formidable challenge due to the unknowable nature of future facts. Existing temporal reasoning models depend on fact recurrence and periodicity, leading to information degradation over prolonged temporal evolution. In particular, the occurrence of one fact may influence the likelihood of another. To this end, we propose THCN, a novel Temporal Causal Convolutional Network based on Hawkes processes, designed for temporal reasoning under the extrapolation setting. Specifically, THCN harnesses a temporal causal convolutional network with dilated factors to capture historical dependencies among facts spanning diverse time intervals. Then, we construct a conditional intensity function based on Hawkes processes for fitting the likelihood of fact occurrence. Importantly, THCN pioneers a dual-level dynamic modeling mechanism, enabling the simultaneous capture of the collective features of nodes and the individual characteristics of facts. Extensive experiments on six real-world TKG datasets demonstrate our method significantly outperforms the state-of-the-art across all four evaluation metrics, indicating that THCN is more applicable for extrapolation reasoning in TKGs.},
  archive      = {J_TKDE},
  author       = {Tingxuan Chen and Jun Long and Zidong Wang and Shuai Luo and Jincai Huang and Liu Yang},
  doi          = {10.1109/TKDE.2024.3474051},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9374-9387},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {THCN: A hawkes process based temporal causal convolutional network for extrapolation reasoning in temporal knowledge graphs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Task assignment framework for online car-hailing systems
with electric vehicles. <em>TKDE</em>, <em>36</em>(12), 9361–9373. (<a
href="https://doi.org/10.1109/TKDE.2024.3434567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, transportation-as-a-service (TaaS) becomes an increasing trend, and online taxi platforms start to apply electric vehicles to serve passengers. Since the recharging time of an electric vehicle is long and non-negligible, it is necessary to smartly arrange the recharging schedules of electric vehicles in working schedules. In order to maximize the number of served taxi-calling tasks, online taxi platforms assign electric vehicles whose remaining electric power is enough to serve the dynamically arriving taxi-calling tasks and schedule suitable idle vehicles to recharging piles to recharge. We formally define the power-aware electric vehicle assignment (PAEVA) problem to serve as many taxi-calling tasks as possible under the constraints of remaining electric power and deadline. We prove that the PAEVA problem is NP-hard. To solve PAEVA, we design a novel strategy to help arrange the schedules of electric vehicles. Specifically, the strategy requires that, in a time slot and an area gird, the ratio of the number of electric vehicles whose remaining electric power is higher than a threshold $\alpha$ to the number of predicted taxi-calling tasks should be higher than a threshold $\beta$ . We propose two approximation approaches with theoretical guarantees to adaptively determine the values of the two thresholds of the strategy. We evaluate our solutions’ effectiveness and efficiency by comprehensive experiments on real datasets.},
  archive      = {J_TKDE},
  author       = {Wangze Ni and Peng Cheng and Lei Chen and Shiyu Yang},
  doi          = {10.1109/TKDE.2024.3434567},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9361-9373},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Task assignment framework for online car-hailing systems with electric vehicles},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Symmetrical self-representation and data-grouping strategy
for unsupervised feature selection. <em>TKDE</em>, <em>36</em>(12),
9348–9360. (<a href="https://doi.org/10.1109/TKDE.2024.3437364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised feature selection (UFS) is an important technology for dimensionality reduction and has gained great interest in a wide range of fields. Recently, most popular methods are spectral-based which frequently use adaptive graph constraints to promote performance. However, no literature has considered the grouping characteristic of the data features, which is the most basic and important characteristic for arbitrary data. In this paper, based on the spectral analysis method, we first simulate the data feature grouping characteristic. Then, the similarity between data is adaptively reconstructed through the similarity between groups, which can explore the more fine-grained relationship between data than the previous adaptive graph methods. In order to achieve the aforementioned goal, the local similarity matrix and the global similarity matrix are defined, and the weighted KL entropy is used to constrain the relationship between the global similarity matrix and the local similarity matrices. Furthermore, the symmetrical self-representation structure is used to improve the performance of the reconstruction error term in the conventional spectral-based methods. After the model is constructed, a simple but efficient algorithm is proposed to solve the full model. Extensive experiments on 8 benchmark dataset with different types to show the effectiveness of the proposed method.},
  archive      = {J_TKDE},
  author       = {Aihong Yuan and Mengbo You and Yuhan Wang and Xun Li and Xuelong Li},
  doi          = {10.1109/TKDE.2024.3437364},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9348-9360},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Symmetrical self-representation and data-grouping strategy for unsupervised feature selection},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SUHDSA: Secure, useful, and high-performance data stream
anonymization. <em>TKDE</em>, <em>36</em>(12), 9336–9347. (<a
href="https://doi.org/10.1109/TKDE.2024.3476684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses privacy concerns in real-time streaming data, including personal biometric signals and private information from sources such as real-time crime reporting, online sales transactions, and hospital patient-monitoring devices. Anonymization is crucial because it hides sensitive personal data. Achieving anonymity in real-time streaming data involves satisfying the unique demands of real-time scenarios, which is distinct from traditional methods. Specifically, security and minimal information loss must be maintained within a specified timeframe (referred to as the average delay time). The most recent solution in this context is the utility-based approach to data stream anonymization (UBDSA) algorithm developed by Sopaoglu and Abul. This study aims to enhance the performance of UBDSA by introducing a secure, useful, and high-performance data stream anonymization (SUHDSA) algorithm. SUHDSA outperforms UBDSA in terms of runtime and information loss while still ensuring privacy protection and an average delay time. The experimental results, using the same dataset and cluster size as in a previous UBDSA study, demonstrate significant performance improvements with the proposed algorithm. It achieves a minimum runtime of 24.05 s and a maximum runtime of 29.88 s, with information loss rates ranging from 14% to 77%. These results surpass the performance of the previous UBDSA algorithm.},
  archive      = {J_TKDE},
  author       = {Yongwan Joo and Soonseok Kim},
  doi          = {10.1109/TKDE.2024.3476684},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9336-9347},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SUHDSA: Secure, useful, and high-performance data stream anonymization},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SpikeLog: Log-based anomaly detection via potential-assisted
spiking neuron network. <em>TKDE</em>, <em>36</em>(12), 9322–9335. (<a
href="https://doi.org/10.1109/TKDE.2023.3347695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing volume and complexity of log data generated by modern systems have made it challenging to analyze and extract useful insights manually. To address this problem, many machine learning methods have been proposed for log-based anomaly detection. However, most of these methods lack interpretability, and their underlying premises do not always reflect real scenarios. In this paper, we consider a more reasonable premise scenario where a large number of logs are unlabeled, while only a small number of anomalous logs are labeled. Moreover, a small proportion of anomaly contamination may be present. To handle this practical scenario, we propose a novel hybrid potential-assisted framework (SpikeLog) using the membrane potential of spiking neurons. SpikeLog adopts a weakly supervised approach to train an anomaly score model, which effectively utilizes a limited number of labeled anomalies alongside abundant unlabeled logs while ensuring computational efficiency without compromising accuracy. Extensive experiments have demonstrated that SpikeLog outperforms baseline methods in terms of performance, robustness, interpretability, and energy consumption.},
  archive      = {J_TKDE},
  author       = {Jiaxing Qi and Zhongzhi Luan and Shaohan Huang and Carol Fung and Hailong Yang and Depei Qian},
  doi          = {10.1109/TKDE.2023.3347695},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9322-9335},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SpikeLog: Log-based anomaly detection via potential-assisted spiking neuron network},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial-temporal cross-view contrastive pre-training for
check-in sequence representation learning. <em>TKDE</em>,
<em>36</em>(12), 9308–9321. (<a
href="https://doi.org/10.1109/TKDE.2024.3434565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of location-based services (LBS) has yielded massive amounts of data on human mobility. Effectively extracting meaningful representations for user-generated check-in sequences is pivotal for facilitating various downstream services. However, the user-generated check-in data are simultaneously influenced by the surrounding objective circumstances and the user&#39;s subjective intention. Specifically, the temporal uncertainty and spatial diversity exhibited in check-in data make it difficult to capture the macroscopic spatial-temporal patterns of users and to understand the semantics of user mobility activities. Furthermore, the distinct characteristics of the temporal and spatial information in check-in sequences call for an effective fusion method to incorporate these two types of information. In this paper, we propose a novel Spatial-Temporal Cross-view Contrastive Representation (STCCR) framework for check-in sequence representation learning. Specifically, STCCR addresses the above challenges by employing self-supervision from “spatial topic” and “temporal intention” views, facilitating effective fusion of spatial and temporal information at the semantic level. Besides, STCCR leverages contrastive clustering to uncover users’ shared spatial topics from diverse mobility activities, while employing angular momentum contrast to mitigate the impact of temporal uncertainty and noise. We extensively evaluate STCCR on three real-world datasets and demonstrate its superior performance across three downstream tasks.},
  archive      = {J_TKDE},
  author       = {Letian Gong and Huaiyu Wan and Shengnan Guo and Xiucheng Li and Yan Lin and Erwen Zheng and Tianyi Wang and Zeyu Zhou and Youfang Lin},
  doi          = {10.1109/TKDE.2024.3434565},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9308-9321},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Spatial-temporal cross-view contrastive pre-training for check-in sequence representation learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solving the imbalanced problem by metric learning and
oversampling. <em>TKDE</em>, <em>36</em>(12), 9294–9307. (<a
href="https://doi.org/10.1109/TKDE.2024.3419834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced data poses a substantial challenge to conventional classification methods, which often disproportionately favor samples from the majority class. To mitigate this issue, various oversampling techniques have been deployed, but opportunities for optimizing data distributions remain underexplored. By exploiting the ability of metric learning to refine the sample distribution, we propose a novel approach, Imbalance Large Margin Nearest Neighbor (ILMNN). Initially, ILMNN is applied to establish a latent feature space, pulling intra-class samples closer and distancing inter-class samples, thereby amplifying the efficacy of oversampling techniques. Subsequently, we allocate varying weights to samples contingent upon their local distribution and relative class frequency, thereby equalizing contributions from minority and majority class samples. Lastly, we employ Kullback-Leibler (KL) divergence as a safeguard to maintain distributional similarity to the original dataset, mitigating severe intra-class imbalances. Comparative experiments on various class-imbalanced datasets verify that our ILMNN approach yields superior results.},
  archive      = {J_TKDE},
  author       = {Kaixiang Yang and Zhiwen Yu and Wuxing Chen and Zefeng Liang and C. L. Philip Chen},
  doi          = {10.1109/TKDE.2024.3419834},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9294-9307},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Solving the imbalanced problem by metric learning and oversampling},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SLA<span class="math inline"><sup>2</sup></span>&lt;mml:math
xmlns:mml=“http://www.w3.org/1998/math/MathML”&gt;&lt;mml:msup&gt;&lt;mml:mrow/&gt;&lt;mml:mtext&gt;2&lt;/mml:mtext&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;p:
Self-supervised anomaly detection with adversarial perturbation.
<em>TKDE</em>, <em>36</em>(12), 9282–9293. (<a
href="https://doi.org/10.1109/TKDE.2024.3448473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is a foundational yet difficult problem in machine learning. In this work, we propose a new and effective framework, dubbed as SLA 2 P, for unsupervised anomaly detection. Following the extraction of delegate embeddings from raw data, we implement random projections on the features and consider features transformed by disparate projections as being associated with separate pseudo-classes. We then train a neural network for classification on these transformed features to conduct self-supervised learning. Subsequently, we introduce adversarial disturbances to the modified attributes, and we develop anomaly scores built on the classifier&#39;s predictive uncertainties concerning these disrupted features. Our approach is motivated by the fact that as anomalies are relatively rare and decentralized, 1) the training of the pseudo-label classifier concentrates more on acquiring the semantic knowledge of regular data instead of anomalous data; 2) the altered attributes of the normal data exhibit greater resilience to disturbances compared to those of the anomalous data. Therefore, the disrupted modified attributes of anomalies can not be well classified and correspondingly tend to attain lesser anomaly scores. The results of experiments on various benchmark datasets for images, text, and inherently tabular data demonstrate that SLA 2 P achieves state-of-the-art performance consistently.},
  archive      = {J_TKDE},
  author       = {Yizhou Wang and Can Qin and Rongzhe Wei and Yi Xu and Yue Bai and Yun Fu},
  doi          = {10.1109/TKDE.2024.3448473},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9282-9293},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SLA$^{{\text{2}}}$&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow/&gt;&lt;mml:mtext&gt;2&lt;/mml:mtext&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;P: self-supervised anomaly detection with adversarial perturbation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shapley value approximation based on complementary
contribution. <em>TKDE</em>, <em>36</em>(12), 9263–9281. (<a
href="https://doi.org/10.1109/TKDE.2024.3438213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shapley value provides a unique way to fairly assess each player&#39;s contribution in a coalition and has enjoyed many applications. However, the exact computation of Shapley value is #P-hard due to the combinatoric nature of Shapley value. Many existing applications of Shapley value are based on Monte-Carlo approximation, which requires a large number of samples and the assessment of utility on many coalitions to reach high-quality approximation, and thus is still far from being efficient. Can we achieve an efficient approximation of Shapley value by smartly obtaining samples? In this paper, we treat the sampling approach to Shapley value approximation as a stratified sampling problem. Our main technical contributions are a novel stratification design and a sampling method based on Neyman allocation. Moreover, computing the Shapley value in a dynamic setting, where new players may join the game and others may leave it poses an additional challenge due to the considerable cost of recomputing from scratch. To tackle this issue, we propose to capture changes in Shapley value, making our approaches applicable to scenarios with dynamic players. Experimental results on several real data sets and synthetic data sets demonstrate the effectiveness and efficiency of our approaches.},
  archive      = {J_TKDE},
  author       = {Qiheng Sun and Jiayao Zhang and Jinfei Liu and Li Xiong and Jian Pei and Kui Ren},
  doi          = {10.1109/TKDE.2024.3438213},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9263-9281},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Shapley value approximation based on complementary contribution},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sequential trajectory data publishing with adaptive
grid-based weighted differential privacy. <em>TKDE</em>,
<em>36</em>(12), 9249–9262. (<a
href="https://doi.org/10.1109/TKDE.2024.3449433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of wireless communication and localization technologies, the easier collection of trajectory data can bring potential data-driven value. Recently, there has been an increasing interest in how to publish trajectory dataset without revealing personal information. However, since the large-scale and real-world sequential trajectory dataset presents a heterogeneous regional distribution, the existing study ignores the relationship between privacy budget allocation and spatial characteristics, resulting in unreasonable continuity and mapping distortion, and thus lowering the utility of the synthetic dataset. To address this problem, we propose a probability distribution model named Adaptive grid-based Weighted Differential Privacy (AWDP). First, trajectories are adaptively discretized into the multi-resolution grid structures to make trajectories more uniformly distributed and less disturbed by the noise. Second, we allocate different weighted budgets for different grids according to density-based regional characteristics. Third, a spatio-temporal continuity maintenance method is designed to solve unrealistic direction- and density-based continuity deviations of synthetic trajectories. An application system is developed for demonstration purposes which is available online at http://qgailab.com/awdp/ . The extensive experiments on three datasets demonstrate that AWDP performs significantly better than the state-of-the-art model in preserving the density distribution of the original trajectories with differential privacy guarantee and high utility.},
  archive      = {J_TKDE},
  author       = {Guangqiang Xie and Haoran Xu and Jiyuan Xu and Shupeng Zhao and Yang Li and Chang-Dong Wang and Xianbiao Hu and Yonghong Tian},
  doi          = {10.1109/TKDE.2024.3449433},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9249-9262},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Sequential trajectory data publishing with adaptive grid-based weighted differential privacy},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Securing multi-source domain adaptation with global and
domain-wise privacy demands. <em>TKDE</em>, <em>36</em>(12), 9235–9248.
(<a href="https://doi.org/10.1109/TKDE.2024.3459890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Making available a large size of training data for deep learning models and preserving data privacy are two ever-growing concerns in the machine learning community. Multi-source domain adaptation (MDA) leverages the data information from different domains and aggregates them to improve the performance in the target task, while the privacy leakage risk of publishing models under malicious attacker for membership or attribute inference is even more complicated than the one faced by single-source domain adaptation. In this paper, we tackle the problem of effectively protecting data privacy while training and aggregating multi-source information, where each source domain enjoys an independent privacy budget. Specifically, we develop a differentially private MDA (DPMDA) algorithm to provide domain-wise privacy protection with adaptive weighting scheme based on task similarity and task-specific privacy budget. We evaluate our algorithm on three benchmark tasks and show that DPMDA can effectively leverage different private budgets from source domains and consistently outperforms the existing private baselines with a reasonable gap with non-private state-of-the-art.},
  archive      = {J_TKDE},
  author       = {Shuwen Chai and Yutang Xiao and Feng Liu and Jian Zhu and Yuan Zhou},
  doi          = {10.1109/TKDE.2024.3459890},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9235-9248},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Securing multi-source domain adaptation with global and domain-wise privacy demands},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SE factual knowledge in frozen giant code model: A study on
FQN and its retrieval. <em>TKDE</em>, <em>36</em>(12), 9220–9234. (<a
href="https://doi.org/10.1109/TKDE.2024.3436883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Giant pre-trained code models (PCMs) start coming into the developers’ daily practices. Understanding the type and amount of software knowledge in PCMs is essential for integrating PCMs into software engineering (SE) tasks and unlocking their potential. In this work, we conduct the first systematic study on the SE factual knowledge in the state-of-the-art PCM CoPilot, focusing on APIs’ Fully Qualified Names (FQNs), the fundamental knowledge for effective code analysis, search and reuse. Driven by FQNs’ data distribution properties, we design a novel lightweight in-context learning on Copilot for FQN inference, which does not require code compilation as traditional methods or gradient update by recent FQN prompt-tuning. We systematically experiment with five in-context learning design factors to identify the best configuration for practical use. With this best configuration, we investigate the impact of example prompts and FQN data properties on CoPilot&#39;s FQN inference capability. Our results confirm that CoPilot stores diverse FQN knowledge and can be applied for FQN inference due to its high accuracy and non-reliance on code analysis. Additionally, our extended study shows that the in-context learning method can be generalized to retrieve other SE factual knowledge embedded in giant PCMs. Furthermore, we find that the advanced general model GPT-4 also stores substantial SE knowledge. Comparing FQN inference between CoPilot and GPT-4, we observe that as model capabilities improve, the same prompts yield better results. Based on our experience interacting with Copilot, we discuss various opportunities to improve human-CoPilot interaction in the FQN inference task.},
  archive      = {J_TKDE},
  author       = {Qing Huang and Dianshu Liao and Zhenchang Xing and Zhiqiang Yuan and Qinghua Lu and Xiwei Xu and Jiaxing Lu},
  doi          = {10.1109/TKDE.2024.3436883},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9220-9234},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SE factual knowledge in frozen giant code model: A study on FQN and its retrieval},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SaaN 2L-GRL: Two-level graph representation learning
empowered with subgraph-as-a-node. <em>TKDE</em>, <em>36</em>(12),
9205–9219. (<a href="https://doi.org/10.1109/TKDE.2024.3421933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a novel graph representation learning (GRL) model, called Two-Level GRL with Subgraph-as-a-Node (SaaN 2L-GRL in short), that partitions input graphs into smaller subgraphs for effective and scalable GRL in two levels: 1) local GRL and 2) global GRL. To realize the two-level GRL in an efficient manner, we propose an abstracted graph, called Subgraph-as-a-Node Graph (SaaN in short), to effectively maintain the high-level graph topology while significantly reducing the size of the graph. By applying the SaaN graph to both local and global GRL, SaaN 2L-GRL can effectively preserve the overall structure of the entire graph while precisely representing the nodes within each subgraph. Through time complexity analysis, we confirm that SaaN 2L-GRL significantly reduces the learning time of existing GRL models by using the SaaN graph for global GRL, instead of using the original graph, and processing local GRL on subgraphs in parallel. Our extensive experiments show that SaaN 2L-GRL outperforms existing GRL models in both accuracy and efficiency. In addition, we show the effectiveness of SaaN 2L-GRL using diverse kinds of graph partitioning methods, including five community detection algorithms and representative edge- and vertex-cut algorithms.},
  archive      = {J_TKDE},
  author       = {Jeong-Ha Park and Bo-Young Lim and Kisung Lee and Hyuk-Yoon Kwon},
  doi          = {10.1109/TKDE.2024.3421933},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9205-9219},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SaaN 2L-GRL: Two-level graph representation learning empowered with subgraph-as-a-node},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). RTOD: Efficient outlier detection with ray tracing cores.
<em>TKDE</em>, <em>36</em>(12), 9192–9204. (<a
href="https://doi.org/10.1109/TKDE.2024.3453901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier detection in data streams is a critical component in numerous applications, such as network intrusion detection, financial fraud detection, and public health. To detect abnormal behaviors in real-time, these applications generally have stringent requirements for the performance of outlier detection. This paper proposes RTOD, a high-performance outlier detection approach that utilizes RT cores in modern GPUs for acceleration. RTOD transforms distance-based outlier detection in data streams into an efficient ray tracing job. By creating spheres centered at points within a window and casting rays from each point, RTOD identifies the outlier points according to the number of intersections between rays and spheres. Besides, we propose two optimization techniques, namely Grid Filtering and Ray-BVH Inversion, to further accelerate the detection efficiency of RT cores. Experimental results show that RTOD achieves up to 9.9× speedups over existing start-of-the-art outlier detection algorithms.},
  archive      = {J_TKDE},
  author       = {Ziming Wang and Kai Zhang and Yangming Lv and Yinglong Wang and Zhigang Zhao and Zhenying He and Yinan Jing and X. Sean Wang},
  doi          = {10.1109/TKDE.2024.3453901},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9192-9204},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {RTOD: Efficient outlier detection with ray tracing cores},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RollStore: Hybrid onchain-offchain data indexing for
blockchain applications. <em>TKDE</em>, <em>36</em>(12), 9176–9191. (<a
href="https://doi.org/10.1109/TKDE.2024.3436514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interest in building blockchain Decentralized Applications (DApps) has been growing over the past few years. DApps are implemented as smart contracts which are programs that are maintained by a blockchain network. Building DApps, however, faces many challenges—most notably the performance and monetary overhead of writing to blockchain smart contracts. To overcome this challenge, many DApp developers have explored utilizing off-chain resources—nodes outside of the blockchain network—to offload part of the processing and storage. In this paper, we propose RollStore, a data indexing solution for hybrid onchain-offchain DApps. RollStore provides efficiency in terms of reduced cost and latency, as well as security in terms of tolerating Byzantine (i.e., malicious) off-chain nodes. RollStore achieves this by: (1) a three-stage commitment strategy where each stage represents a point in a performance-security trade-off—i.e., the first stage is fast but less secure while the last stage is slower but more secure. (2) utilizing zero-knowledge (zk) proofs to enable the on-chain smart contract to verify off-chain operations with a small cost. (3) Combining Log-Structured Merge (LSM) trees and Merkle Mountain Range (MMR) trees to efficiently enable both access and verification of indexed data. We experimentally evaluate the cost and performance benefits of RollStore while comparing with BlockchainDB and BigChainDB.},
  archive      = {J_TKDE},
  author       = {Qi Lin and Binbin Gu and Faisal Nawab},
  doi          = {10.1109/TKDE.2024.3436514},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9176-9191},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {RollStore: Hybrid onchain-offchain data indexing for blockchain applications},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robustness-reinforced knowledge distillation with
correlation distance and network pruning. <em>TKDE</em>,
<em>36</em>(12), 9163–9175. (<a
href="https://doi.org/10.1109/TKDE.2024.3438074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The improvement in the performance of efficient and lightweight models (i.e., the student model) is achieved through knowledge distillation (KD), which involves transferring knowledge from more complex models (i.e., the teacher model). However, most existing KD techniques rely on Kullback-Leibler (KL) divergence, which has certain limitations. First, if the teacher distribution has high entropy, the KL divergence&#39;s mode-averaging nature hinders the transfer of sufficient target information. Second, when the teacher distribution has low entropy, the KL divergence tends to excessively focus on specific modes, which fails to convey an abundant amount of valuable knowledge to the student. Consequently, when dealing with datasets that contain numerous confounding or challenging samples, student models may struggle to acquire sufficient knowledge, resulting in subpar performance. Furthermore, in previous KD approaches, we observed that data augmentation, a technique aimed at enhancing a model&#39;s generalization, can have an adverse impact. Therefore, we propose a Robustness-Reinforced Knowledge Distillation (R2KD) that leverages correlation distance and network pruning. This approach enables KD to effectively incorporate data augmentation for performance improvement. Extensive experiments on various datasets, including CIFAR-100, FGVR, TinyImagenet, and ImageNet, demonstrate our method&#39;s superiority over current state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Seonghak Kim and Gyeongdo Ham and Yucheol Cho and Daeshik Kim},
  doi          = {10.1109/TKDE.2024.3438074},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9163-9175},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Robustness-reinforced knowledge distillation with correlation distance and network pruning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust multi-view clustering with noisy correspondence.
<em>TKDE</em>, <em>36</em>(12), 9150–9162. (<a
href="https://doi.org/10.1109/TKDE.2024.3423307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep multi-view clustering leverages deep neural networks to achieve promising performance, but almost all existing methods implicitly assume that all views are aligned correctly. This assumption is unrealistic in many real-world scenarios, where noise, occlusion, or sensor differences can inevitably cause misaligned data. Based on this observation, we reveal and study a practical but understudied problem in multi-view clustering (MVC), i.e., noisy correspondence (NC). Considering this problem, we argue that the main challenge is to prevent the model from overfiting NC. To this end, we propose a novel Robust Multi-view Clustering with Noisy Correspondence (RMCNC) method, which alleviates the influence of the misaligned pairs from multi-view data. To be specific, we first compute a united probability with all positive pairs to learn cross-view alignment consistency, thereby alleviating the adverse impact of the individual false positives. To further mitigate the overfitting problem, we propose a noise-tolerance multi-view contrastive loss that avoids overemphasizing noisy data. Moreover, RMCNC is a unified framework, which can deal with both partially view-aligned and NC problems in multi-view clustering. To the best of our knowledge, it could be the first study on NC in multi-view clustering. The experimental results on eight benchmark datasets indicate our RMCNC achieves competitive performance and robustness.},
  archive      = {J_TKDE},
  author       = {Yuan Sun and Yang Qin and Yongxiang Li and Dezhong Peng and Xi Peng and Peng Hu},
  doi          = {10.1109/TKDE.2024.3423307},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9150-9162},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Robust multi-view clustering with noisy correspondence},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rethinking robust multivariate time series anomaly
detection: A hierarchical spatio-temporal variational perspective.
<em>TKDE</em>, <em>36</em>(12), 9136–9149. (<a
href="https://doi.org/10.1109/TKDE.2024.3466291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The robust multivariate time series anomaly detection can facilitate intelligent decisions and timely maintenance in various kinds of monitor systems. However, the robustness is highly restricted by the stochasticity in multivariate time series, which is summarized as temporal stochasticity and spatial stochasticity specifically. In this paper, we explicitly model the temporal stochasticity variables and the latent graph relationship variables into a unified graphical framework, which can achieve better robustness to dynamicity from both the spatial and temporal perspective. First, within the spatial encoder, every connection exists or not is modeled as a binary stochastic variable, and the graph structure can be learnt automatically. Then, the temporal encoder would embed the highly structured time series into latent stochastic variables to capture both complex temporal dependencies and neighbors information. Moreover, we design a history-future combined anomaly score mechanism with both reconstruction decoder and forecasting decoder to improve the anomaly detection performance. By weighting the historical anomaly factor, the future anomaly factor, and the prediction error of current timestamp, the anomaly detection at current timestamp could be more sensitive to anomaly detection. Finally, extensive experiments on three publicly available anomaly detection datasets demonstrate our proposed method can achieve the best performance in terms of recall and F1 compared with state-of-the-arts baselines.},
  archive      = {J_TKDE},
  author       = {Xiao Zhang and Shuqing Xu and Huashan Chen and Zekai Chen and Fuzhen Zhuang and Hui Xiong and Dongxiao Yu},
  doi          = {10.1109/TKDE.2024.3466291},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9136-9149},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Rethinking robust multivariate time series anomaly detection: A hierarchical spatio-temporal variational perspective},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reliable node similarity matrix guided contrastive graph
clustering. <em>TKDE</em>, <em>36</em>(12), 9123–9135. (<a
href="https://doi.org/10.1109/TKDE.2024.3435887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph clustering, which involves the partitioning of nodes within a graph into disjoint clusters, holds significant importance for numerous subsequent applications. Recently, contrastive learning, known for utilizing supervisory information, has demonstrated encouraging results in deep graph clustering. This methodology facilitates the learning of favorable node representations for clustering by attracting positively correlated node pairs and distancing negatively correlated pairs within the representation space. Nevertheless, a significant limitation of existing methods is their inadequacy in thoroughly exploring node-wise similarity. For instance, some hypothesize that the node similarity matrix within the representation space is identical, ignoring the inherent semantic relationships among nodes. Given the fundamental role of instance similarity in clustering, our research investigates contrastive graph clustering from the perspective of the node similarity matrix. We argue that an ideal node similarity matrix within the representation space should accurately reflect the inherent semantic relationships among nodes, ensuring the preservation of semantic similarities in the learned representations. In response to this, we introduce a new framework, Reliable Node Similarity Matrix Guided Contrastive Graph Clustering (NS4GC), which estimates an approximately ideal node similarity matrix within the representation space to guide representation learning. Our method introduces node-neighbor alignment and semantic-aware sparsification, ensuring the node similarity matrix is both accurate and efficiently sparse. Comprehensive experiments conducted on 8 real-world datasets affirm the efficacy of learning the node similarity matrix and the superior performance of NS4GC.},
  archive      = {J_TKDE},
  author       = {Yunhui Liu and Xinyi Gao and Tieke He and Tao Zheng and Jianhua Zhao and Hongzhi Yin},
  doi          = {10.1109/TKDE.2024.3435887},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9123-9135},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Reliable node similarity matrix guided contrastive graph clustering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ReiPool: Reinforced pooling graph neural networks for
graph-level representation learning. <em>TKDE</em>, <em>36</em>(12),
9109–9122. (<a href="https://doi.org/10.1109/TKDE.2024.3466508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph pooling technique as the essential component of graph neural networks has gotten increasing attention recently and it aims to learn graph-level representations for the whole graph. Besides, graph pooling is important in graph classification and graph generation tasks. However, current graph pooling methods mainly coarsen a sequence of small-sized graphs to capture hierarchical structures, potentially resulting in the deterioration of the global structure of the original graph and influencing the quality of graph representations. Furthermore, these methods artificially select the number of graph pooling layers for different graph datasets rather than considering each graph individually. In reality, the structure and size differences among graphs necessitate a specific number of graph pooling layers for each graph. In this work, we propose reinforced pooling graph neural networks via adaptive hybrid graph coarsening networks. Specifically, we design a hybrid graph coarsening strategy to coarsen redundant structures of the original graph while retaining the global structure. In addition, we introduce multi-agent reinforcement learning to adaptively perform the graph coarsening process to extract the most representative coarsened graph for each graph, enhancing the quality of graph-level representations. Finally, we design graph-level contrast to improve the preservation of global information in graph-level representations. Extensive experiments with rich baselines on six benchmark datasets show the effectiveness of ReiPool 1 .},
  archive      = {J_TKDE},
  author       = {Xuexiong Luo and Sheng Zhang and Jia Wu and Hongyang Chen and Hao Peng and Chuan Zhou and Zhao Li and Shan Xue and Jian Yang},
  doi          = {10.1109/TKDE.2024.3466508},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9109-9122},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {ReiPool: Reinforced pooling graph neural networks for graph-level representation learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ProtoUDA: Prototype-based unsupervised adaptation for
cross-domain text recognition. <em>TKDE</em>, <em>36</em>(12),
9096–9108. (<a href="https://doi.org/10.1109/TKDE.2023.3344761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text recognition reads from real scene text or handwritten text, facilitating many real-world applications such as driverless cars, visual Q&amp;A, and image-based machine translation. Although impressive results have been achieved in single-domain text recognition, it still suffers from great challenges in cross-domain due to the domain gaps among the synthetic text, the real scene text, and the handwritten text. Existing standard unsupervised domain adaptation (UDA) methods struggle to solve the text recognition task since they view a domain or a text image (containing a character sequence) as a whole, ignoring the subunits that make up the sequence. In the paper, we present a Prototyped-based Unsupervised Domain Adaptation method for text recognition (ProtoUDA), where the class prototypes are computed from the source domain, target domain, and the mixed (source-target) domain, respectively. Technically, ProtoUDA initially extracts pseudo-labeled character features under word-level supervised information. Further, based on these character features, we propose two parallel and complementary modules to perform class-level and instance-level alignment, which explicitly transfer the knowledge learned in the source domain to the target domain. Among them, class-level alignment is to close the distance between the similar source prototypes and target prototypes. The instance-level alignment is based on contrastive learning, making the character instances of the mixed domain close to the corresponding class mixed prototype while staying away from other class mixed prototypes. To our knowledge, we are the first to adopt contrastive learning in UDA-based text recognition tasks. Extensive experiments on several benchmark datasets show the superiority of our method over state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Xiao-Qian Liu and Xue-Ying Ding and Xin Luo and Xin-Shun Xu},
  doi          = {10.1109/TKDE.2023.3344761},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9096-9108},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {ProtoUDA: Prototype-based unsupervised adaptation for cross-domain text recognition},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prompt tuning on graph-augmented low-resource text
classification. <em>TKDE</em>, <em>36</em>(12), 9080–9095. (<a
href="https://doi.org/10.1109/TKDE.2024.3440068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text classification is a fundamental problem in information retrieval with many real-world applications, such as predicting the topics of online articles and the categories of e-commerce product descriptions. However, low-resource text classification, with no or few labeled samples, presents a serious concern for supervised learning. Meanwhile, many text data are inherently grounded on a network structure, such as a hyperlink/citation network for online articles, and a user-item purchase network for e-commerce products. These graph structures capture rich semantic relationships, which can potentially augment low-resource text classification. In this paper, we propose a novel model called Graph-Grounded Pre-training and Prompting (G2P2) to address low-resource text classification in a two-pronged approach. During pre-training, we propose three graph interaction-based contrastive strategies to jointly pre-train a graph-text model; during downstream classification, we explore handcrafted discrete prompts and continuous prompt tuning for the jointly pre-trained model to achieve zero- and few-shot classification, respectively. Moreover, we explore the possibility of employing continuous prompt tuning for zero-shot inference. Specifically, we aim to generalize continuous prompts to unseen classes while leveraging a set of base classes. To this end, we extend G2P2 into G2P2 $^*$ , hinging on a new architecture of conditional prompt tuning. Extensive experiments on four real-world datasets demonstrate the strength of G2P2 in zero- and few-shot low-resource text classification tasks, and illustrate the advantage of G2P2 $^*$ in dealing with unseen classes.},
  archive      = {J_TKDE},
  author       = {Zhihao Wen and Yuan Fang},
  doi          = {10.1109/TKDE.2024.3440068},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9080-9095},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Prompt tuning on graph-augmented low-resource text classification},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Progressive skeleton learning for effective local-to-global
causal structure learning. <em>TKDE</em>, <em>36</em>(12), 9065–9079.
(<a href="https://doi.org/10.1109/TKDE.2024.3461832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal structure learning (CSL) from observational data is a crucial objective in various machine learning applications. Recent advances in CSL have focused on local-to-global learning, which offers improved efficiency and accuracy. The local-to-global CSL algorithms first learn the local skeleton of each variable in a dataset, then construct the global skeleton by combining these local skeletons, and finally orient edges to infer causality. However, data quality issues such as noise and small samples often result in the presence of problematic asymmetric edges during global skeleton construction, hindering the creation of a high-quality global skeleton. To address this challenge, we propose a novel local-to-global CSL algorithm with a progressive enhancement strategy and make the following novel contributions: 1) To construct an accurate global skeleton, we design a novel strategy to iteratively correct asymmetric edges and progressively improve the accuracy of the global skeleton. 2) Based on the learned accurate global skeleton, we design an integrated global skeleton orientation strategy to infer the correct directions of edges for obtaining an accurate and reliable causal structure. Extensive experiments demonstrate that our method achieves better performance than the existing CSL methods.},
  archive      = {J_TKDE},
  author       = {Xianjie Guo and Kui Yu and Lin Liu and Jiuyong Li and Jiye Liang and Fuyuan Cao and Xindong Wu},
  doi          = {10.1109/TKDE.2024.3461832},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9065-9079},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Progressive skeleton learning for effective local-to-global causal structure learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PrivFusion: Privacy-preserving model fusion via
decentralized federated graph matching. <em>TKDE</em>, <em>36</em>(12),
9051–9064. (<a href="https://doi.org/10.1109/TKDE.2024.3430819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model fusion is becoming a crucial component in the context of model-as-a-service scenarios, enabling the delivery of high-quality model services to local users. However, this approach introduces privacy risks and imposes certain limitations on its applications. Ensuring secure model exchange and knowledge fusion among users becomes a significant challenge in this setting. To tackle this issue, we propose PrivFusion, a novel architecture that preserves privacy while facilitating model fusion under the constraints of local differential privacy. PrivFusion leverages a graph-based structure, enabling the fusion of models from multiple parties without additional training. By employing randomized mechanisms, PrivFusion ensures privacy guarantees throughout the fusion process. To enhance model privacy, our approach incorporates a hybrid local differentially private mechanism and decentralized federated graph matching, effectively protecting both activation values and weights. Additionally, we introduce a perturbation filter adapter to alleviate the impact of randomized noise, thereby recovering the utility of the fused model. Through extensive experiments conducted on diverse image datasets and real-world healthcare applications, we provide empirical evidence showcasing the effectiveness of PrivFusion in maintaining model performance while preserving privacy. Our contributions offer valuable insights and practical solutions for secure and collaborative data analysis within the domain of privacy-preserving model fusion.},
  archive      = {J_TKDE},
  author       = {Qian Chen and Yiqiang Chen and Xinlong Jiang and Teng Zhang and Weiwei Dai and Wuliang Huang and Bingjie Yan and Zhen Yan and Wang Lu and Bo Ye},
  doi          = {10.1109/TKDE.2024.3430819},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9051-9064},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {PrivFusion: Privacy-preserving model fusion via decentralized federated graph matching},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pre-training general trajectory embeddings with maximum
multi-view entropy coding. <em>TKDE</em>, <em>36</em>(12), 9037–9050.
(<a href="https://doi.org/10.1109/TKDE.2023.3347513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatio-temporal trajectories provide valuable information about movement and travel behavior, enabling various downstream tasks that in turn power real-world applications. Learning trajectory embeddings can improve task performance but may incur high computational costs and face limited training data availability. Pre-training learns generic embeddings by means of specially constructed pretext tasks that enable learning from unlabeled data. Existing pre-training methods face (i) difficulties in learning general embeddings due to biases towards certain downstream tasks incurred by the pretext tasks, (ii) limitations in capturing both travel semantics and spatio-temporal correlations, and (iii) the complexity of long, irregularly sampled trajectories. To tackle these challenges, we propose Maximum Multi-view Trajectory Entropy Coding (MMTEC) for learning general and comprehensive trajectory embeddings. We introduce a pretext task that reduces biases in pre-trained trajectory embeddings, yielding embeddings that are useful for a wide variety of downstream tasks. We also propose an attention-based discrete encoder and a NeuralCDE-based continuous encoder that extract and represent travel behavior and continuous spatio-temporal correlations from trajectories in embeddings, respectively. Extensive experiments on two real-world datasets and three downstream tasks offer insight into the design properties of our proposal and indicate that it is capable of outperforming existing trajectory embedding methods.},
  archive      = {J_TKDE},
  author       = {Yan Lin and Huaiyu Wan and Shengnan Guo and Jilin Hu and Christian S. Jensen and Youfang Lin},
  doi          = {10.1109/TKDE.2023.3347513},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9037-9050},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Pre-training general trajectory embeddings with maximum multi-view entropy coding},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PLBR: A semi-supervised document key information extraction
via pseudo-labeling bias rectification. <em>TKDE</em>, <em>36</em>(12),
9025–9036. (<a href="https://doi.org/10.1109/TKDE.2024.3443928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document key information extraction (DKIE) methods often require a large number of labeled samples, imposing substantial annotation costs in practical scenarios. Fortunately, pseudo-labeling based semi-supervised learning (PSSL) algorithms provide an effective paradigm to alleviate the reliance on labeled data by leveraging unlabeled data. However, the main challenges for PSSL in DKIE tasks: 1) context dependency of DKIE results in incorrect pseudo-labels. 2) high intra-class variance and low inter-class variation on DKIE. To this end, this paper proposes a similarity matrix Pseudo-Label Bias Rectification (PLBR) semi-supervised method for DKIE tasks, which improves the quality of pseudo-labels on DKIE benchmarks with rare labels. More specifically, the Similarity Matrix Bias Rectification (SMBR) module is proposed to improve the quality of pseudo-labels, which utilizes the contextual information of DKIE data through the analysis of similarity between labeled and unlabeled data. Moreover, a dual branch adaptive alignment (DBAA) mechanism is designed to adaptively align intra-class variance and alleviate inter-class variation on DKIE benchmarks, which is composed of two adaptive alignment ways. One is the intra-class alignment branch, which is designed to adaptively align intra-class variance. The other one is the inter-class alignment branch, which is developed to adaptively alleviate inter-class variance changes on the representation level. Extensive experiment results on two benchmarks demonstrate that PLBR achieves state-of-the-art performance and its performance surpasses the previous SOTA by $2.11\% \sim 2.53\%$ , $2.09\% \sim 2.49\%$ F1-score on FUNSD and CORD with rare labeled samples, respectively. Code will be open to the public.},
  archive      = {J_TKDE},
  author       = {Pengcheng Guo and Yonghong Song and Boyu Wang and Jiaohao Liu and Qi Zhang},
  doi          = {10.1109/TKDE.2024.3443928},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9025-9036},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {PLBR: A semi-supervised document key information extraction via pseudo-labeling bias rectification},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parallel contraction hierarchies construction on road
networks. <em>TKDE</em>, <em>36</em>(12), 9011–9024. (<a
href="https://doi.org/10.1109/TKDE.2024.3437243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shortest path query on road networks is a fundamental problem to support many location-based services and wide variant applications. Contraction Hierarchies(CH) is widely adopted to accelerate the shortest path query by leveraging shortcuts among vertices. However, the state-of-the-art CH construction method named $\mathsf{VCHCons}$ suffers from inefficiencies due to their strong reliance on pre-determined vertex order. This leads to the generation of a large number of invalid shortcuts and the limit of parallel processing capability. Motivated by it, in this paper, an innovative CH construction algorithm called $\mathsf{ECHCons}$ is devised following an edge-centric paradigm, which addresses the issue of invalid shortcut production by introducing a novel edge-ordering strategy. Furthermore, it optimizes shortcut calculation within a dynamically constructed optimal subgraph, which is significantly smaller than the original network, thus shrinking the traversal space during index construction. To further enhance efficiency and overcome the limitations in parallelism inherent to $\mathsf{VCHCons}$ , our approach leverages batch contraction of edges and introduces a well-defined lower bound technique to unlock more efficient parallel computation resources. Our approach provides both theoretical guarantee and practical advancement in CH construction. Extensive and comprehensive experiments are conducted on real road networks. The experimental results demonstrate the effectiveness and efficiency of our proposed approach.},
  archive      = {J_TKDE},
  author       = {Zi Chen and Xinyu Ji and Long Yuan and Xuemin Lin and Wenjie Zhang and Shan Huang},
  doi          = {10.1109/TKDE.2024.3437243},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {9011-9024},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Parallel contraction hierarchies construction on road networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Optimization techniques for unsupervised complex table
reasoning via self-training framework. <em>TKDE</em>, <em>36</em>(12),
8996–9010. (<a href="https://doi.org/10.1109/TKDE.2024.3439405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structured tabular data is a fundamental data type in numerous fields, and the capacity to reason over tables is crucial for answering questions and validating hypotheses. However, constructing labeled data for complex reasoning tasks is labor-intensive, and the quantity of annotated data remains insufficient to support the intricate demands of real-world applications. To address the insufficient annotation challenge, we present a self-training framework for unsupervised complex tabular reasoning (UCTR-ST) by generating diverse synthetic data with complex logic. Specifically, UCTR-ST incorporates several essential techniques: we aggregate diverse programs and execute them on tables based on a “Program-Management” component, and we bridge the gap between programs and text with a powerful “Program-Transformation” module that generates natural language sentences with complex logic. Furthermore, we optimize the procedure using “Table-Text Manipulator” to handle joint table-text reasoning scenarios. The entire framework utilizes self-training techniques to leverage the unlabeled training data, which results in significant performance improvements when tested on real-world data. Experimental results demonstrate that UCTR-ST achieves above 90% of the supervised model performance on different tasks and domains, reducing the dependence on manual annotation. Additionally, our approach can serve as a data augmentation technique, significantly boosting the performance of supervised models in low-resourced domains.},
  archive      = {J_TKDE},
  author       = {Zhenyu Li and Xiuxing Li and Sunqi Fan and Jianyong Wang},
  doi          = {10.1109/TKDE.2024.3439405},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8996-9010},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Optimization techniques for unsupervised complex table reasoning via self-training framework},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OPF-miner: Order-preserving pattern mining with forgetting
mechanism for time series. <em>TKDE</em>, <em>36</em>(12), 8981–8995.
(<a href="https://doi.org/10.1109/TKDE.2024.3438274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Order-preserving pattern (OPP) mining is a type of sequential pattern mining method in which a group of ranks of time series is used to represent an OPP. This approach can discover frequent trends in time series. Existing OPP mining algorithms consider data points at different time to be equally important; however, newer data usually have a more significant impact, while older data have a weaker impact. We therefore introduce the forgetting mechanism into OPP mining to reduce the importance of older data. This paper explores the mining of OPPs with forgetting mechanism (OPF) and proposes an algorithm called OPF-Miner that can discover frequent OPFs. OPF-Miner performs two tasks, candidate pattern generation and support calculation. In candidate pattern generation, OPF-Miner employs a maximal support priority strategy and a group pattern fusion strategy to avoid redundant pattern fusions. For support calculation, we propose an algorithm called support calculation with forgetting mechanism, which uses prefix and suffix pattern pruning strategies to avoid redundant support calculations. The experiments are conducted on nine datasets and 12 alternative algorithms. The results verify that OPF-Miner is superior to other competitive algorithms. More importantly, OPF-Miner yields good clustering performance for time series, since the forgetting mechanism is employed.},
  archive      = {J_TKDE},
  author       = {Yan Li and Chenyu Ma and Rong Gao and Youxi Wu and Jinyan Li and Wenjian Wang and Xindong Wu},
  doi          = {10.1109/TKDE.2024.3438274},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8981-8995},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {OPF-miner: Order-preserving pattern mining with forgetting mechanism for time series},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Open continual feature selection via granular-ball knowledge
transfer. <em>TKDE</em>, <em>36</em>(12), 8967–8980. (<a
href="https://doi.org/10.1109/TKDE.2024.3428485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel framework for continual feature selection (CFS) in data preprocessing, particularly in the context of an open and dynamic environment where unknown classes may emerge. CFS encounters two primary challenges: the discovery of unknown knowledge and the transfer of known knowledge. To this end, we propose a GBCFS method, which combines the strengths of continual learning (CL) with granular-ball computing (GBC). The GBCFS method focuses on constructing a granular-ball knowledge base to detect unknown classes and facilitate the transfer of previously learned knowledge for further feature selection. GBCFS consists of two stages: initial learning and open learning. The former aims to establish an initial knowledge base through multi-granularity representation using granular balls. The latter utilizes prior granular-ball knowledge to identify unknowns, updates the knowledge base for granular-ball knowledge transfer, reinforces old knowledge, and integrates new knowledge. Subsequently, we devise an optimal feature subset mechanism that incorporates minimal new features into the existing optimal subset, often yielding superior results during each period. Extensive experimental results on public benchmark datasets demonstrate our method&#39;s superiority in terms of both effectiveness and efficiency compared to state-of-the-art feature selection methods.},
  archive      = {J_TKDE},
  author       = {Xuemei Cao and Xin Yang and Shuyin Xia and Guoyin Wang and Tianrui Li},
  doi          = {10.1109/TKDE.2024.3428485},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8967-8980},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Open continual feature selection via granular-ball knowledge transfer},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online learning of temporal association rule on dynamic
multivariate time series data. <em>TKDE</em>, <em>36</em>(12),
8954–8966. (<a href="https://doi.org/10.1109/TKDE.2024.3438259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, rule-based classification on multivariate time series (MTS) data has gained lots of attention, which could improve the interpretability of classification. However, state-of-the-art approaches suffer from three major issues. 1) few existing studies consider temporal relations among features in a rule, which could not adequately express the essential characteristics of MTS data. 2) due to the concept drift and time warping of MTS data, traditional methods could not mine essential characteristics of MTS data. 3) existing online learning algorithms could not effectively update shapelet-based temporal association rules of MTS data due to its temporal relationships among features of different variables. To handle these issues, we propose an online learning method for temporal association rule on dynamically collected MTS data (OTARL). First, a new type of rule named temporal association rule is defined and mined to represent temporal relationships among features in a rule. Second, an online learning mechanism with a probability correlation-based evaluation criterion is proposed to realize the online learning of temporal association rules on dynamically collected MTS data. Finally, an ensemble classification approach based on maximum-likelihood estimation is advanced to further enhance the classification performance. We conduct experiments on ten real-world datasets to verify the effectiveness and efficiency of our approach.},
  archive      = {J_TKDE},
  author       = {Guoliang He and Dawei Jin and Lifang Dai and Xin Xin and Zhiwen Yu and C. L. Philip Chen},
  doi          = {10.1109/TKDE.2024.3438259},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8954-8966},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Online learning of temporal association rule on dynamic multivariate time series data},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online learning and detecting corrupted users for
conversational recommendation systems. <em>TKDE</em>, <em>36</em>(12),
8939–8953. (<a href="https://doi.org/10.1109/TKDE.2024.3448250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational recommendation systems (CRSs) are increasingly prevalent, but they are susceptible to the influence of corrupted user behaviors, such as deceptive click ratings. These behaviors can skew the recommendation process, resulting in suboptimal results. Traditional bandit algorithms, which are typically oriented to single users, do not capitalize on implicit social connections between users, which could otherwise enhance learning efficiency. Furthermore, they cannot identify corrupted users in a real-time, multi-user environment. In this paper, we propose a novel bandit problem, Online Learning and Detecting Corrupted Users (OLDCU), to learn and utilize unknown user relations from disrupted behaviors to speed up learning and detect corrupted users in an online setting. This problem is non-trivial due to the dynamic nature of user behaviors and the difficulty of online detection. To robustly learn and leverage the unknown relations among potentially corrupted users, we propose a novel bandit algorithm RCLUB-WCU, incorporating a conversational mechanism. This algorithm is designed to handle the complexities of disrupted behaviors and to make accurate user relation inferences. To detect corrupted users with bandit feedback, we further devise a novel online detection algorithm, OCCUD, which is based on RCLUB-WCU’s inferred user relations and designed to adapt over time. We prove a sub-linear regret bound for RCLUB-WCU, demonstrating its efficiency. We also analyze the detection accuracy of OCCUD, showing its effectiveness in identifying corrupted users. Through extensive experiments, we validate the performance of our methods. Our results show that RCLUB-WCU and OCCUD outperform previous bandit algorithms and achieve high corrupted user detection accuracy, providing robust and efficient solutions in the field of CRSs.},
  archive      = {J_TKDE},
  author       = {Xiangxiang Dai and Zhiyong Wang and Jize Xie and Tong Yu and John C. S. Lui},
  doi          = {10.1109/TKDE.2024.3448250},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8939-8953},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Online learning and detecting corrupted users for conversational recommendation systems},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online dynamic hybrid broad learning system for real-time
safety assessment of dynamic systems. <em>TKDE</em>, <em>36</em>(12),
8928–8938. (<a href="https://doi.org/10.1109/TKDE.2024.3475028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time safety assessment of dynamic systems is of paramount importance in industrial processes since it provides continuous monitoring and evaluation to prevent potential harm to the environment and individuals. However, there are still several challenges to be resolved due to the requirements of time consumption and the non-stationary nature of real-world environments. In this paper, a novel online dynamic hybrid broad learning system, termed ODH-BLS, is proposed to more fully utilize the co-design advantages of active adaptation and passive adaptation. It makes effective use of limited annotations with the proposed sample value function. Simultaneously, anchor points can be dynamically adjusted to accommodate changes of the underlying distribution, thereby leveraging the value of unlabeled samples. An iterative update rule is also derived to ensure adaptation of the assessment model to real-time data at low computational costs. We also provide theoretical analyses to illustrate its practicality. Several experiments regarding the JiaoLong deep-sea manned submersible are carried out. The results demonstrate that the proposed ODH-BLS method achieves a performance improvement of approximately 8% over the baseline method on the benchmark dataset, showing its effectiveness in solving real-time safety assessment tasks for dynamic systems.},
  archive      = {J_TKDE},
  author       = {Zeyi Liu and Xiao He},
  doi          = {10.1109/TKDE.2024.3475028},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8928-8938},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Online dynamic hybrid broad learning system for real-time safety assessment of dynamic systems},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). One subgraph for all: Efficient reasoning on opening
subgraphs for inductive knowledge graph completion. <em>TKDE</em>,
<em>36</em>(12), 8914–8927. (<a
href="https://doi.org/10.1109/TKDE.2024.3432767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graph Completion (KGC) has garnered massive research interest recently, and most existing methods are designed following a transductive setting where all entities are observed during training. Despite the great progress on the transductive KGC, these methods struggle to conduct reasoning on emerging KGs involving unseen entities. Thus, inductive KGC, which aims to deduce missing links among unseen entities, has become a new trend. Many existing studies transform inductive KGC as a graph classification problem by extracting enclosing subgraphs surrounding each candidate triple. Unfortunately, they still face certain challenges, such as the expensive time consumption caused by the repeat extraction of enclosing subgraphs, and the deficiency of entity-independent feature learning. To address these issues, we propose a global-local anchor representation (GLAR) learning method for inductive KGC. Unlike previous methods that utilize enclosing subgraphs, we extract a shared opening subgraph for all candidates and perform reasoning on it, enabling the model to perform reasoning more efficiently. Moreover, we design some transferable global and local anchors to learn rich entity-independent features for emerging entities. Finally, a global-local graph reasoning model is applied on the opening subgraph to rank all candidates. Extensive experiments show that our GLAR outperforms most existing state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Zhiwen Xie and Yi Zhang and Guangyou Zhou and Jin Liu and Xinhui Tu and Jimmy Xiangji Huang},
  doi          = {10.1109/TKDE.2024.3432767},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8914-8927},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {One subgraph for all: Efficient reasoning on opening subgraphs for inductive knowledge graph completion},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neighbor distribution learning for minority class
augmentation. <em>TKDE</em>, <em>36</em>(12), 8901–8913. (<a
href="https://doi.org/10.1109/TKDE.2024.3447014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have achieved remarkable success in graph-based tasks. However, learning unbiased node representations under class-imbalanced training data remains challenging. Existing solutions may face overfitting due to extensive reuse of those limited labeled data in minority classes. Furthermore, many works address the class-imbalanced issue based on the embeddings generated from the biased GNNs, which make models intrinsically biased towards majority classes. In this paper, we propose a novel data augmentation strategy GraphGLS for semi-supervised class-imbalanced node classification, which aims to select informative unlabeled nodes to augment minority classes with consideration of both global and local information. Specifically, we first design a Global Selection module to learn global information (pseudo-labels) for unlabeled nodes and then select potential ones from them for minority classes. The Local Selection module further conducts filtering over those potential nodes by comparing their neighbor distributions with minority classes. To achieve this, we further design a neighbor distribution auto-encoder to learn a robust node-level neighbor distribution for each node. Then, we define class-level neighbor distribution to capture the overall neighbor characteristics of nodes within the same class. We conduct extensive experiments on multiple datasets, and the results demonstrate the superiority of GraphGLS over state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Mengting Zhou and Zhiguo Gong},
  doi          = {10.1109/TKDE.2024.3447014},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8901-8913},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Neighbor distribution learning for minority class augmentation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-view adaptive fusion network for spatially resolved
transcriptomics data clustering. <em>TKDE</em>, <em>36</em>(12),
8889–8900. (<a href="https://doi.org/10.1109/TKDE.2024.3450333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial transcriptomics technology fully leverages spatial location and gene expression information for spatial clustering tasks. However, existing spatial clustering methods primarily concentrate on utilizing the complementary features between spatial and gene expression information, while overlooking the discriminative features during the integration process. Consequently, the discriminative capability of node representation in the gene expression features is limited. Besides, most existing methods lack a flexible combination mechanism to adaptively integrate spatial and gene expression information. To this end, we propose an end-to-end deep learning method named MAFN for spatially resolved transcriptomics data clustering via a multi-view adaptive fusion network. Specifically, we first adaptively learn inter-view complementary features from spatial and gene expression information. To improve the discriminative capability of gene expression nodes by utilizing spatial information, we employ two GCN encoders to learn intra-view specific features and design a Cross-view Correlation Reduction (CCR) strategy to filter the irrelevant information. Moreover, considering the distinct characteristics of each view, a Cross-view Attention Module (CAM) is utilized to adaptively fuse the multi-view features. Extensive experimental results demonstrate that the proposed MAFN achieves competitive performance in spatial domain identification compared to other state-of-the-art ones.},
  archive      = {J_TKDE},
  author       = {Yanran Zhu and Xiao He and Chang Tang and Xinwang Liu and Yuanyuan Liu and Kunlun He},
  doi          = {10.1109/TKDE.2024.3450333},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8889-8900},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-view adaptive fusion network for spatially resolved transcriptomics data clustering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-task spatial-temporal transformer for multi-variable
meteorological forecasting. <em>TKDE</em>, <em>36</em>(12), 8876–8888.
(<a href="https://doi.org/10.1109/TKDE.2024.3432599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study delves into multi-variable meteorological spatial-temporal prediction, focusing on the simultaneous forecasting of key meteorological parameters such as temperature, wind speed, and atmospheric pressure. The core challenge of this task lies in identifying commonalities across different variables while capturing their unique features and the interactions among them. To address this, we propose a novel multi-task learning framework tailored for multi-variable meteorological forecasting. Our framework integrates a convolutional variable-specific visual representation module and a variable-interactive spatial-temporal inference module. The former extracts distinct variable information independently for each variable, while the latter employs a tri-level attention mechanism across space, time, and variables to uncover both commonalities and interactions among the variables. An adaptive multi-loss optimization strategy and a local information aggregation module are introduced to balance task optimization complexities and enhance representation stability. Comprehensive experiments across various meteorological prediction tasks confirm the effectiveness of our methods, showcasing superior performance over existing approaches.},
  archive      = {J_TKDE},
  author       = {Tian-Bao Li and An-An Liu and Dan Song and Wen-Hui Li and Jing Zhang and Zhi-Qiang Wei and Yu-Ting Su},
  doi          = {10.1109/TKDE.2024.3432599},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8876-8888},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-task spatial-temporal transformer for multi-variable meteorological forecasting},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-source shortest path query with assembly points on
large graphs. <em>TKDE</em>, <em>36</em>(12), 8859–8875. (<a
href="https://doi.org/10.1109/TKDE.2024.3424947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing Multi-source Shortest Path query with Assembly points ( $\mathsf {MSPA}$ ) is a fundamental graph problem. The $\mathsf {MSPA}$ problem locates a set of assembly points to minimize the overall distance for transporting objects from different sources to a destination, where we can assemble objects at assembly points to reduce the total cost. We prove that the $\mathsf {MSPA}$ problem is NP-hard. The intuitive method for computing the optimal set of assembly points and the corresponding set of paths is by Branch-and-Bound. However, the combination of different assembly points is exponential. By analyzing the structure of the path set based on the proposed distance graph, we find that the used paths can be combined into a tree. Hence, by defining the state of subtrees and the state transition equation, we propose a dynamic programming (DP) algorithm by pruning the redundant computation of subtrees. The experiment shows that the DP algorithm can achieve three orders of magnitude speedup in query processing time compared with the optimized Branch-and-Bound algorithm. Moreover, we reduce the transition candidates of the DP algorithm from the entire vertex set to certain neighbors. Extensive experiments are conducted on different types of real-world networks to demonstrate the performance of our DP algorithm.},
  archive      = {J_TKDE},
  author       = {Dian Ouyang and Zhuoran Wang and Fan Zhang and Shiyu Yang and Jianye Yang and Xuemin Lin},
  doi          = {10.1109/TKDE.2024.3424947},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8859-8875},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-source shortest path query with assembly points on large graphs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal graph causal embedding for multimedia-based
recommendation. <em>TKDE</em>, <em>36</em>(12), 8842–8858. (<a
href="https://doi.org/10.1109/TKDE.2024.3424268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimedia-based recommendation (MMRec) models typically rely on observed user-item interactions and the multimodal content of items, such as visual images and textual descriptions, to predict user preferences. Among these, the user&#39;s preference for the displayed multimodal content of items is crucial for interacting with a particular item. We argue that users&#39; preference behaviors (i.e., user-item interactions) for the modality content of items, beyond stemming from their real interest in the modality content, may also be influenced by their conformity to the popularity of items&#39; modality-specific content (e.g., a user might be motivated to interact with a lipstick due to enthusiastic discussions among other users regarding textual reviews of the product). In essence, user-item interactions are jointly triggered by real interest and conformity. However, most existing MMRec models primarily concentrate on modeling users&#39; interest preferences when capturing multimodal user preferences, neglecting the modeling of their conformity preferences, which results in sub-optimal recommendation performance. In this work, we resort to causal theory to propose a novel MMRec model, termed Multimodal Graph Causal Embedding (MGCE), revealing insights into the crucial causal relations of users&#39; modality-specific interest and conformity in interaction behaviors within MMRec scenarios. Inspired by the colliding effect in causal inference and integrating the characteristics of real interest and conformity, we devise multimodal causal embedding learning networks to facilitate the learning of high-quality causal embeddings (multimodal interest and multimodal conformity embeddings) from both the structure-level and feature-level, yielding state-of-the-art performance. Extensive experimental results on three datasets demonstrate the effectiveness of MGCE.},
  archive      = {J_TKDE},
  author       = {Shuaiyang Li and Feng Xue and Kang Liu and Dan Guo and Richang Hong},
  doi          = {10.1109/TKDE.2024.3424268},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8842-8858},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multimodal graph causal embedding for multimedia-based recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-level graph knowledge contrastive learning.
<em>TKDE</em>, <em>36</em>(12), 8829–8841. (<a
href="https://doi.org/10.1109/TKDE.2024.3466530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Contrastive Learning (GCL) stands as a potent framework for unsupervised graph representation learning that has gained traction across numerous graph learning applications. The effectiveness of GCL relies on generating high-quality contrasting samples, enhancing the model’s ability to discern graph semantics. However, the prevailing GCL methods face two key challenges: 1) introducing noise during graph augmentations and 2) requiring additional storage for generated samples, which degrade the model performance. In this paper, we propose novel approaches, GKCL (i.e., Graph Knowledge Contrastive Learning) and DGKCL (i.e., Distilled Graph Knowledge Contrastive Learning), that leverage multi-level graph knowledge to create noise-free contrasting pairs. This framework not only addresses the noise-related challenges but also circumvents excessive storage demands. Furthermore, our method incorporates a knowledge distillation component to optimize the trained embedding tables, reducing the model’s scale while ensuring superior performance, particularly for the scenarios with smaller embedding sizes. Comprehensive experimental evaluations on three public benchmark datasets underscore the merits of our proposed method and elucidate its properties, which primarily reflect the performance of the proposed method equipped with different embedding sizes and how the distillation weight affects the overall performance.},
  archive      = {J_TKDE},
  author       = {Haoran Yang and Yuhao Wang and Xiangyu Zhao and Hongxu Chen and Hongzhi Yin and Qing Li and Guandong Xu},
  doi          = {10.1109/TKDE.2024.3466530},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8829-8841},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-level graph knowledge contrastive learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling dynamic item tendency bias in sequential
recommendation with causal intervention. <em>TKDE</em>, <em>36</em>(12),
8814–8828. (<a href="https://doi.org/10.1109/TKDE.2024.3427719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommendation is a critical but challenging task in capturing users’ potential preferences due to inherent biases in the data. Existing debiasing recommendation methods aim to eliminate biases from historical interaction data collected by recommender systems and have shown promising results. However, there is another significant bias that hinders the improvement of sequential recommendation models: dynamic item tendency bias. This bias arises because a period might have some unique tendencies consisting of items interacted with by users with the same intent, leading to a dynamic tendency distribution that biases the model training towards these tendencies. To address this issue, we propose a causal approach to model dynamic item tendency bias in sequential recommendation. We first extract tendencies on carefully designed item-item graphs through community detection. We then use causal intervention to conduct deconfounded training to capture true user preferences and introduce the beneficial item tendency bias to the inference process through optimal transport techniques. Experimental results on four real-world datasets demonstrate that our proposed method consistently outperforms state-of-the-art debiasing recommendation methods, confirming that our model is effective in reducing dynamic item tendency bias and dealing with tendency drifts.},
  archive      = {J_TKDE},
  author       = {Zihan Liao and Xiaodong Wu and Shuo Shang and Jun Wang and Wei Zhang},
  doi          = {10.1109/TKDE.2024.3427719},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8814-8828},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Modeling dynamic item tendency bias in sequential recommendation with causal intervention},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-agnostic causal embedding learning for
counterfactually group-fair recommendation. <em>TKDE</em>,
<em>36</em>(12), 8801–8813. (<a
href="https://doi.org/10.1109/TKDE.2024.3424906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group-fair recommendation aims at ensuring the equality of recommendation results across user groups categorized by sensitive attributes (e.g., gender, occupation, etc.). Existing group-fair recommendation models traditionally employ original user embeddings for both training and testing, primarily focusing on statistical learning while imposing group fairness constraints under the I.I.D. assumption. However, these models encounter limitations when addressing out-of-distribution (OOD) sensitive attributes. The fundamental issue of unfairness within user embeddings arises from a causal perspective, where each embedding vector comprises an exogenous component devoid of correlations with sensitive attributes and an endogenous component strongly correlated with these attributes. Overlooking the distinction between these two components during model training renders models sensitive to shifts in the distribution of sensitive attributes. This paper introduces the concept of Counterfactual Group Fairness (CGF) along with a corresponding metric to evaluate group fairness in scenarios involving OOD sensitive attributes in recommender systems. Building on this foundation, we propose a model-agnostic causal embedding learning framework named MACE. MACE effectively disentangles user embedding vectors into their exogenous and endogenous parts, thus ensuring group fairness, even in the presence of OOD sensitive attributes in embeddings. Specifically, MACE identifies the exogenous part of each user&#39;s embedding using mutual information minimization, treating it as instrumental variables. Subsequently, under the constraint of CGF, MACE reconstructs the endogenous and exogenous parts using the instrumental variable regression, combines the obtained parts into novel user embeddings using deep neural networks, and uses the combined embeddings for fair recommendation. Experimental results demonstrated that MACE can outperform the state-of-the-art baselines in terms of the metric of CGF while maintaining a comparable recommendation accuracy.},
  archive      = {J_TKDE},
  author       = {Xiao Zhang and Teng Shi and Jun Xu and Zhenhua Dong and Ji-Rong Wen},
  doi          = {10.1109/TKDE.2024.3424906},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8801-8813},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Model-agnostic causal embedding learning for counterfactually group-fair recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MOCOLNet: A momentum contrastive learning network for
multimodal aspect-level sentiment analysis. <em>TKDE</em>,
<em>36</em>(12), 8787–8800. (<a
href="https://doi.org/10.1109/TKDE.2023.3345022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal aspect-level sentiment analysis has attracted increasing attention in recent years. However, existing methods have two unaddressed limitations: (1) due to the lack of labelled pre-training data of dedicated sentiment analysis, the methods with a pre-training manner produce suboptimal prediction results; (2) most existing methods employ a self-attention encoder to fuse multimodal tokens, which not only ignores the alignment relationship between different modal tokens but also makes the model unable to capture the semantic links between images and texts. In this paper, we propose a momentum contrastive learning network (MOCOLNet) to overcome above limitations. First, we merge the pre-training stage with the training stage to design an end-to-end training manner which uses less labelled data dedicated to sentiment analysis to obtain better prediction results. Second, we propose a multimodal contrastive learning method to align the different modal representations before data fusing, and design a cross-modal matching strategy to provide semantic interactive information between texts and images. Moreover, we introduce an auxiliary momentum strategy to increase the robustness of model. We also analyse the effectiveness of the proposed multimodal contrastive learning method using a mutual information theory. Experiments verify that the proposed MOCOLNet is superior to other strong baselines.},
  archive      = {J_TKDE},
  author       = {Jie Mu and Feiping Nie and Wei Wang and Jian Xu and Jing Zhang and Han Liu},
  doi          = {10.1109/TKDE.2023.3345022},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8787-8800},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MOCOLNet: A momentum contrastive learning network for multimodal aspect-level sentiment analysis},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mixed-modality clustering via generative graph structure
matching. <em>TKDE</em>, <em>36</em>(12), 8773–8786. (<a
href="https://doi.org/10.1109/TKDE.2024.3434556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of mixed-modality clustering, which differs from typical multi-modality/view clustering, is to divide samples derived from various modalities into several clusters. This task has to solve two critical semantic gap problems: i) how to generate the missing modalities without the pairwise-modality data; and ii) how to align the representations of heterogeneous modalities. To tackle the above problems, this paper proposes a novel mixed-modality clustering model, which integrates the missing-modality generation and the heterogeneous modality alignment into a unified framework. During the missing-modality generation process, a bidirectional mapping is established between different modalities, enabling generation of preliminary representations for the missing-modality using information from another modality. Then the intra-modality bipartite graphs are constructed to help generate better missing-modality representations by weighted aggregating existing intra-modality neighbors. In this way, a pairwise-modality representation for each sample can be obtained. In the process of heterogeneous modality alignment, each modality is modelled as a graph to capture the global structure among intra-modality samples and is aligned against the heterogeneous modality representations through the adaptive heterogeneous graph matching module. Experimental results on three public datasets show the effectiveness of the proposed model compared to multiple state-of-the-art multi-modality/view clustering methods.},
  archive      = {J_TKDE},
  author       = {Xiaxia He and Boyue Wang and Junbin Gao and Qianqian Wang and Yongli Hu and Baocai Yin},
  doi          = {10.1109/TKDE.2024.3434556},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8773-8786},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Mixed-modality clustering via generative graph structure matching},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mining user consistent and robust preference for unified
cross domain recommendation. <em>TKDE</em>, <em>36</em>(12), 8758–8772.
(<a href="https://doi.org/10.1109/TKDE.2024.3446581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-Domain Recommendation has been popularly studied to resolve data sparsity problem via leveraging knowledge transfer across different domains. In this paper, we focus on the Unified Cross-Domain Recommendation ( Unified CDR ) problem. That is, how to enhance the recommendation performance within and cross domains when users are partially overlapped. It has two main challenges, i.e., 1) how to obtain robust matching solution among the whole users and 2) how to exploit consistent and accurate results across domains. To address these two challenges, we propose MUCRP , a cross-domain recommendation framework for the Unified CDR problem. MUCRP contains three modules, i.e., variational rating reconstruction module, robust variational embedding alignment module, and cycle-consistent preference extraction module. To solve the first challenge, we propose fused Gromov-Wasserstein distribution co-clustering optimal transport to obtain more robust matching solution via considering both semantic and structure information. To tackle the second challenge, we propose embedding-consistent and prediction-consistent losses via dual autoencoder framework to achieve consistent results. Our empirical study on Douban and Amazon datasets demonstrates that MUCRP significantly outperforms the state-of-the-art models.},
  archive      = {J_TKDE},
  author       = {Xiaolin Zheng and Weiming Liu and Chaochao Chen and Jiajie Su and Xinting Liao and Mengling Hu and Yanchao Tan},
  doi          = {10.1109/TKDE.2024.3446581},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8758-8772},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Mining user consistent and robust preference for unified cross domain recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Make heterophilic graphs better fit GNN: A graph rewiring
approach. <em>TKDE</em>, <em>36</em>(12), 8744–8757. (<a
href="https://doi.org/10.1109/TKDE.2024.3441766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have shown superior performance in modeling graph data. Existing studies have shown that a lot of GNNs perform well on homophilic graphs while performing poorly on heterophilic graphs. Recently, researchers have turned their attention to design GNNs for heterophilic graphs by specific model design. Different from existing methods that mitigate heterophily by model design, we propose to study heterophilic graphs from an orthogonal perspective by rewiring the graph to reduce heterophily and make GNNs perform better. Through comprehensive empirical analysis, we verify the potential of graph rewiring methods. Then we propose a method named D eep H eterophily G raph R ewiring (DHGR) to rewire graphs by adding homophilic edges and pruning heterophilic edges. The rewiring operation is implemented by comparing the similarity of neighborhood label/feature distribution of node pairs. Besides, we design a scalable implementation for DHGR to guarantee a high efficiency. DHRG can be easily used as a plug-in module, i.e., a graph pre-processing step, for any GNNs, including both GNNs for homophily and heterophily, to boost their performance on the node classification task. To the best of our knowledge, it is the first work studying graph rewiring for heterophilic graphs. Extensive experiments on 11 public graph datasets demonstrate the superiority of our proposed methods.},
  archive      = {J_TKDE},
  author       = {Wendong Bi and Lun Du and Qiang Fu and Yanlin Wang and Shi Han and Dongmei Zhang},
  doi          = {10.1109/TKDE.2024.3441766},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8744-8757},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Make heterophilic graphs better fit GNN: A graph rewiring approach},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Look into gradients: Learning compact hash codes for
out-of-distribution retrieval. <em>TKDE</em>, <em>36</em>(12),
8730–8743. (<a href="https://doi.org/10.1109/TKDE.2024.3425268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing aims to compress raw data into compact binary descriptors, which has drawn increasing interest for efficient large-scale image retrieval. Current deep hashing often employs evaluation protocols where usually query data and training data are from similar distributions. However, more realistic evaluations should take into account a broad spectrum of distribution shifts with varying degrees. Therefore, we study the problem of out-of-distribution generalization in image retrieval, which seeks to learn a retrieval model from a source domain and generalize to unseen target domains. However, this problem is challenging owing to data scarcity in target domains and the potential overfitting of domain-specific patterns. Here, we propose a novel hashing model named L ooking-int o - g radients (LOG) for image retrieval under out-of-distribution shifts, which comprehensively explores gradients for both data generation and model optimization. Specifically, to overcome data deficiency in target domains, we formalize the worst-case problem to generate challenging virtue samples via adversarial gradient ascend. Besides, to further enhance model generalization capability, we not only identify non-crucial parameters with minor gradients and values and shrink them to zero, but also modify the inconsistent gradients across domains to prevent learning domain-specific patterns. Extensive experiments on various datasets demonstrate that LOG outperforms state-of-the-art methods by up to 8.54%.},
  archive      = {J_TKDE},
  author       = {Haixin Wang and Xinlong Yang and Jinan Sun and Shikun Zhang and Chong Chen and Xian-Sheng Hua and Xiao Luo},
  doi          = {10.1109/TKDE.2024.3425268},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8730-8743},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Look into gradients: Learning compact hash codes for out-of-distribution retrieval},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LogoRA: Local-global representation alignment for robust
time series classification. <em>TKDE</em>, <em>36</em>(12), 8718–8729.
(<a href="https://doi.org/10.1109/TKDE.2024.3459908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) of time series aims to teach models to identify consistent patterns across various temporal scenarios, disregarding domain-specific differences, which can maintain their predictive accuracy and effectively adapt to new domains. However, existing UDA methods struggle to adequately extract and align both global and local features in time series data. To address this issue, we propose the Lo cal- G l o bal R epresentation A lignment framework (LogoRA), which employs a two-branch encoder–comprising a multi-scale convolutional branch and a patching transformer branch. The encoder enables the extraction of both local and global representations from time series. A fusion module is then introduced to integrate these representations, enhancing domain-invariant feature alignment from multi-scale perspectives. To achieve effective alignment, LogoRA employs strategies like invariant feature learning on the source domain, utilizing triplet loss for fine alignment and dynamic time warping-based feature alignment. Additionally, it reduces source-target domain gaps through adversarial training and per-class prototype alignment. Our evaluations on four time-series datasets demonstrate that LogoRA outperforms strong baselines by up to 12.52%, showcasing its superiority in time series UDA tasks.},
  archive      = {J_TKDE},
  author       = {Huanyu Zhang and Yi-Fan Zhang and Zhang Zhang and Qingsong Wen and Liang Wang},
  doi          = {10.1109/TKDE.2024.3459908},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8718-8729},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {LogoRA: Local-global representation alignment for robust time series classification},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight conceptual dictionary learning for text
classification using information compression. <em>TKDE</em>,
<em>36</em>(12), 8711–8717. (<a
href="https://doi.org/10.1109/TKDE.2024.3421255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel supervised dictionary learning framework for text classification, integrating the Lempel-Ziv-Welch (LZW) algorithm for data compression and dictionary construction. This two-phase approach refines dictionaries by optimizing dictionary atoms for discriminative power using mutual information and class distribution. Our method facilitates classifier training, such as SVMs and neural networks. We introduce the information plane area rank (IPAR) to evaluate the information-theoretic performance of our algorithm. Tested on six benchmark text datasets, our model performs nearly as well as top models in limited-vocabulary settings, lagging by only about 2% while using just 10% of the parameters. However, its performance drops in diverse-vocabulary contexts due to the LZW algorithm&#39;s limitations with low-repetition data. This contrast highlights its efficiency and limitations across different dataset types.},
  archive      = {J_TKDE},
  author       = {Li Wan and Tansu Alpcan and Margreta Kuijper and Emanuele Viterbo},
  doi          = {10.1109/TKDE.2024.3421255},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8711-8717},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Lightweight conceptual dictionary learning for text classification using information compression},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LightCTS*: Lightweight correlated time series forecasting
enhanced with model distillation. <em>TKDE</em>, <em>36</em>(12),
8695–8710. (<a href="https://doi.org/10.1109/TKDE.2024.3424451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlated time series (CTS) forecasting is essential in many practical applications, such as traffic management and server load control. Various deep learning based solutions have been proposed to improve forecasting accuracy. However, while models have become increasingly computationally intensive, they struggle to improve accuracy. This study aims instead to enable more lightweight, accurate models suitable for resource-constrained devices. To achieve this goal, we characterize popular CTS forecasting models, yielding two observations for developing lightweight CTS forecasting. On this basis, we propose the LightCTS framework that adopts plain stacking of temporal and spatial operators instead of alternate stacking which is much more computationally expensive. Moreover, LightCTS features light temporal and spatial operators, L-TCN and GL-Former, offering improved computational efficiency without compromising their feature extraction capabilities. LightCTS also encompasses a last-shot compression scheme to reduce redundant temporal features and speed up subsequent computations. Next, we equip LightCTS with two knowledge distillation modules, Tafd and Caad , that result in LightCTS $^\star$ retaining the original benefits of LightCTS , while also being able to adapt to varying levels of ultra-constrained resources. Experimental studies offer detailed insight into these proposals and provide evidence that both LightCTS and LightCTS $^\star$ are capable of nearly state-of-the-art accuracy at substantially reduced computational costs.},
  archive      = {J_TKDE},
  author       = {Zhichen Lai and Dalin Zhang and Huan Li and Christian S. Jensen and Hua Lu and Yan Zhao},
  doi          = {10.1109/TKDE.2024.3424451},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8695-8710},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {LightCTS*: Lightweight correlated time series forecasting enhanced with model distillation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to denoise biomedical knowledge graph for robust
molecular interaction prediction. <em>TKDE</em>, <em>36</em>(12),
8682–8694. (<a href="https://doi.org/10.1109/TKDE.2024.3471508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular interaction prediction plays a crucial role in forecasting unknown interactions between molecules, such as drug-target interaction (DTI) and drug-drug interaction (DDI), which are essential in the field of drug discovery and therapeutics. Although previous prediction methods have yielded promising results by leveraging the rich semantics and topological structure of biomedical knowledge graphs (KGs), they have primarily focused on enhancing predictive performance without addressing the presence of inevitable noise and inconsistent semantics. This limitation has hindered the advancement of KG-based prediction methods. To address this limitation, we propose BioKDN ( Bio medical K nowledge Graph D enoising N etwork) for robust molecular interaction prediction. BioKDN refines the reliable structure of local subgraphs by denoising noisy links in a learnable manner, providing a general module for extracting task-relevant interactions. To enhance the reliability of the refined structure, BioKDN maintains consistent and robust semantics by smoothing relations around the target interaction. By maximizing the mutual information between reliable structure and smoothed relations, BioKDN emphasizes informative semantics to enable precise predictions. Experimental results on real-world datasets show that BioKDN surpasses state-of-the-art models in DTI and DDI prediction tasks, confirming the effectiveness and robustness of BioKDN in denoising unreliable interactions within contaminated KGs.},
  archive      = {J_TKDE},
  author       = {Tengfei Ma and Yujie Chen and Wen Tao and Dashun Zheng and Xuan Lin and Patrick Cheong-Iao Pang and Yiping Liu and Yijun Wang and Longyue Wang and Bosheng Song and Xiangxiang Zeng and Philip S. Yu},
  doi          = {10.1109/TKDE.2024.3471508},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8682-8694},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning to denoise biomedical knowledge graph for robust molecular interaction prediction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning prioritized node-wise message propagation in graph
neural networks. <em>TKDE</em>, <em>36</em>(12), 8670–8681. (<a
href="https://doi.org/10.1109/TKDE.2024.3436909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have recently received significant attention. Learning node-wise message propagation in GNNs aims to set personalized propagation steps for different nodes in the graph. Despite the success, existing methods ignore node priority that can be reflected by node influence and heterophily. In this paper, we propose a versatile framework PriPro, which can be integrated with most existing GNN models and aim to learn prioritized node-wise message propagation in GNNs. Specifically, the framework consists of three components: a backbone GNN model, a propagation controller to determine the optimal propagation steps for nodes, and a weight controller to compute the priority scores for nodes. We design a mutually enhanced mechanism to compute node priority, optimal propagation step and label prediction. We also propose an alternative optimization strategy to learn the parameters in the backbone GNN model and two parametric controllers. We conduct extensive experiments to compare our framework with other 12 state-of-the-art competitors on 10 benchmark datasets. Experimental results show that our framework can lead to superior performance in terms of propagation strategies and node representations.},
  archive      = {J_TKDE},
  author       = {Yao Cheng and Minjie Chen and Caihua Shan and Xiang Li},
  doi          = {10.1109/TKDE.2024.3436909},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8670-8681},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning prioritized node-wise message propagation in graph neural networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Latent structure-aware view recovery for incomplete
multi-view clustering. <em>TKDE</em>, <em>36</em>(12), 8655–8669. (<a
href="https://doi.org/10.1109/TKDE.2024.3445992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multi-view clustering (IMVC) presents a significant challenge due to the need for effectively exploring complementary and consistent information within the context of missing views. One promising strategy to tackle this challenge is to recover missing views by inferring the missing samples. However, such approaches often fail to fully utilize discriminative structural information or adequately address consistency, as it requires such information to be known or learnable in advance, which contradicts the incomplete data setting. In this study, we propose a novel approach called La tent S tructure- A ware view recovery (LaSA) for the IMVC task. Our objective is to recover missing views through discriminative latent representations by leveraging structural information. Specifically, our method offers a unified closed-form formulation that simultaneously performs missing data inference and latent representation learning, using a learned intrinsic graph as structural information. This formulation, incorporating graph structure information, enhances the inference of missing data while facilitating discriminative feature learning. Even when intrinsic graph is initially unknown due to incomplete data, our formulation allows for effective view recovery and intrinsic graph learning through an iterative optimization process. To further enhance performance, we introduce an iterative consistency diffusion process, which effectively leverages the consistency and complementary information across multiple views. Extensive experiments demonstrate the effectiveness of the proposed method compared to state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Cheng Liu and Rui Li and Hangjun Che and Man-Fai Leung and Si Wu and Zhiwen Yu and Hau-San Wong},
  doi          = {10.1109/TKDE.2024.3445992},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8655-8669},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Latent structure-aware view recovery for incomplete multi-view clustering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). L-ASCRA: A linearithmic time approximate spectral clustering
algorithm using topologically-preserved representatives. <em>TKDE</em>,
<em>36</em>(12), 8643–8654. (<a
href="https://doi.org/10.1109/TKDE.2024.3483572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate spectral clustering (ASC) algorithms work on the representative points of the data for discovering intrinsic groups. The existing ASC methods identify fewer representatives as compared to the number of data points to reduce the cubic computational overhead of the spectral clustering technique. However, identifying such representative points without any domain knowledge to capture the shapes and topology of the clusters remains a challenge. This work proposes an ASC method that suitably computes enough well-scattered representatives to efficiently capture the topology of the data, making the ASC faster without the requirement of tuning any external parameters. The proposed ASC algorithm first applies two-level partitioning using both boundary points and centroids-based partitioning to identify quality representatives in less time. In the next step, we calculate the proximity between the neighboring representatives using $k$ -rounds of minimum spanning tree (MST) by considering the distribution of edge weights in each round to find $k$ . The proposed method effectively utilizes the number of representatives in a way that the overall computational time is bounded by $O(N\lg N)$ . The experimental results suggest that the proposed ASC method outperforms the competing ASC methods in terms of both running time and clustering quality.},
  archive      = {J_TKDE},
  author       = {Abdul Atif Khan and Mohammad Maksood Akhter and Rashmi Maheshwari and Sraban Kumar Mohanty},
  doi          = {10.1109/TKDE.2024.3483572},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8643-8654},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {L-ASCRA: A linearithmic time approximate spectral clustering algorithm using topologically-preserved representatives},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large language models on graphs: A comprehensive survey.
<em>TKDE</em>, <em>36</em>(12), 8622–8642. (<a
href="https://doi.org/10.1109/TKDE.2024.3469578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-paired graphs. We then discuss detailed techniques for utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM as Aligner, and compare the advantages and disadvantages of different schools of models. Furthermore, we discuss the real-world applications of such methods and summarize open-source codes and benchmark datasets. Finally, we conclude with potential future research directions in this fast-growing field.},
  archive      = {J_TKDE},
  author       = {Bowen Jin and Gang Liu and Chi Han and Meng Jiang and Heng Ji and Jiawei Han},
  doi          = {10.1109/TKDE.2024.3469578},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8622-8642},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Large language models on graphs: A comprehensive survey},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kairos: Enabling prompt monitoring of information diffusion
over temporal networks. <em>TKDE</em>, <em>36</em>(12), 8607–8621. (<a
href="https://doi.org/10.1109/TKDE.2023.3347621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyses of temporal graphs provide valuable insights into temporal data through the use of two analytical approaches: temporal evolution and temporal information diffusion. The former shows how a network evolves over time; the latter explains how information spreads throughout a network over time. Systems have been mainly proposed to efficiently handle graph snapshots, which are suitable for temporal evolution but inappropriate for temporal information diffusion. For analyses of temporal information diffusion, temporal graph traversal platforms have recently been proposed; however, it is still infeasible to handle infinitely evolving temporal data, especially for monitoring applications. In this paper, we propose an incremental approach and its graph processing engine, Kairos, to enable prompt monitoring of temporal information diffusion. This approach makes it possible to immediately process diffusion results for sources of interest by traversing a part of the whole network, which avoids full traversals influenced by a small change in the network, thus making monitoring applications feasible. The recipes for implementing incremental versions of existing temporal graph traversal algorithms and metrics will make it easier for users to build their ad-hoc programs.},
  archive      = {J_TKDE},
  author       = {Haifa Gaza and Jaewook Byun},
  doi          = {10.1109/TKDE.2023.3347621},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8607-8621},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Kairos: Enabling prompt monitoring of information diffusion over temporal networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Joint optimization of pricing, dispatching and
repositioning in ride-hailing with multiple models interplayed
reinforcement learning. <em>TKDE</em>, <em>36</em>(12), 8593–8606. (<a
href="https://doi.org/10.1109/TKDE.2024.3464563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Popular ride-hailing products, such as DiDi, Uber and Lyft, provide people with transportation convenience. Pricing, order dispatching and vehicle repositioning are three tasks with tight correlation and complex interactions in ride-hailing platforms, significantly impacting each other’s decisions and demand distribution or supply distribution. However, no past work considered combining the three tasks to improve platform efficiency. In this paper, we exploit to optimize pricing, dispatching and repositioning strategies simultaneously. Such a new multi-stage decision-making problem is quite challenging because it involves complex coordination and lacks a unified problem model. To address this problem, we propose a novel J oint optimization framework of P ricing, D ispatching and R epositioning (JPDR) integrating contextual bandit and multi-agent deep reinforcement learning. JPDR consists of two components, including a Soft Actor-Critic (SAC)-based centralized policy for dispatching and repositioning and a pricing strategy learned by a multi-armed contextual bandit algorithm based on the feedback from the former. The two components learn in a mutually guided way to achieve joint optimization because their updates are highly interdependent. Based on real-world data, we implement a realistic environment simulator. Extensive experiments conducted on it show our method outperforms state-of-the-art baselines in terms of both gross merchandise volume and success rate.},
  archive      = {J_TKDE},
  author       = {Zhongyun Zhang and Lei Yang and Jiajun Yao and Chao Ma and Jianguo Wang},
  doi          = {10.1109/TKDE.2024.3464563},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8593-8606},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Joint optimization of pricing, dispatching and repositioning in ride-hailing with multiple models interplayed reinforcement learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Iterative soft prompt-tuning for unsupervised domain
adaptation. <em>TKDE</em>, <em>36</em>(12), 8580–8592. (<a
href="https://doi.org/10.1109/TKDE.2024.3483903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation aims to facilitate learning tasks in unlabeled target domain with knowledge in the related source domain, which has achieved awesome performance with the pre-trained language models (PLMs). Recently, inspired by GPT, the prompt-tuning model has been widely explored in stimulating rich knowledge in PLMs for language understanding. However, existing prompt-tuning methods still directly applied the model that was learned in the source domain into the target domain to minimize the discrepancy between different domains, e.g., the prompts or the template are trained separately to learn embeddings for transferring to the target domain, which is actually the intuition of end-to-end deep-based approach. In this paper, we propose an Iterative Soft Prompt-Tuning method (ItSPT) for better unsupervised domain adaptation. On the one hand, the prompt-tuning model learned in the source domain is converted into an iterative model to find the true label information in the target domain, the domain adaptation method is then regarded as a few-shot learning task. On the other hand, instead of hand-crafted templates, ItSPT adopts soft prompts for both considering the automatic template generation and classification performance. Experiments on both English and Chinese datasets demonstrate that our method surpasses the performance of SOTA methods.},
  archive      = {J_TKDE},
  author       = {Yi Zhu and Shuqin Wang and Jipeng Qiang and Xindong Wu},
  doi          = {10.1109/TKDE.2024.3483903},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8580-8592},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Iterative soft prompt-tuning for unsupervised domain adaptation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Is sharing neighbor generator in federated graph learning
safe? <em>TKDE</em>, <em>36</em>(12), 8568–8579. (<a
href="https://doi.org/10.1109/TKDE.2024.3482448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, as privacy concerns continue to rise, federated graph learning (FGL) which generalizes the classic federated learning to graph data has attracted increasing attention. However, while the focus has been on designing collaborative learning algorithms, the potential risks of privacy leakage through the sharing of necessary graph-related information in FGL, such as node embeddings and neighbor generators, have been largely neglected. In this paper, we verify the potential risks of privacy leakage in FGL, and provide insights about the cautions in FGL algorithm design. Specifically, we propose a novel privacy attack algorithm named Privacy Attack on federated Graph learning (PAG) towards reconstructing participants’ private node attributes and the linkage relationships. The participant performing the PAG attack is able to reconstruct the node attributes of the victim by matching the received gradients of the generator, and then train a link prediction model based on its local sub-graph to inductively infer the linkages connected to these reconstructed nodes. We theoretically and empirically demonstrate that under PAG attack, directly sharing the neighbor generators makes the FGL vulnerable to the data reconstruction attack. Furthermore, an investigation into the key factors that can hinder the success of the PAG attack provides insights into corresponding defense strategies and inspires future research into privacy-preserving FGL.},
  archive      = {J_TKDE},
  author       = {Liuyi Yao and Zhen Wang and Yuexiang Xie and Yaliang Li and Weirui Kuang and Daoyuan Chen and Bolin Ding},
  doi          = {10.1109/TKDE.2024.3482448},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8568-8579},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Is sharing neighbor generator in federated graph learning safe?},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interdependence-adaptive mutual information maximization for
graph contrastive learning. <em>TKDE</em>, <em>36</em>(12), 8556–8567.
(<a href="https://doi.org/10.1109/TKDE.2024.3423409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite remarkable advancements in graph contrastive learning techniques, the identification of interdependent relationships when maximizing cross-view mutual information remains a challenging issue, primarily due to the complexity of graph topology. In this study, we propose to formulate cross-view interdependence from the innovative perspective of information flow. Accordingly, IDEAL, a simple yet effective framework, is proposed for interdependence-adaptive graph contrastive learning. Compared with existing methods, IDEAL concurrently addresses same-node and distinct-node interdependence, circumvents the reliance on additional distribution mining techniques, and is augmentation-aware. Besides, the objective of IDEAL takes advantage of both contrastive and generative learning objectives and is thus capable of learning a uniform embedding distribution while retaining essential semantic information. The effectiveness of IDEAL is validated by extensive empirical evidence. It consistently outperforms state-of-the-art self-supervised methods by considerable margins across seven benchmark datasets with diverse scales and properties and, at the same time, showcases promising training efficiency.},
  archive      = {J_TKDE},
  author       = {Qingqiang Sun and Kai Wang and Wenjie Zhang and Peng Cheng and Xuemin Lin},
  doi          = {10.1109/TKDE.2024.3423409},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8556-8567},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Interdependence-adaptive mutual information maximization for graph contrastive learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Information cascade popularity prediction via probabilistic
diffusion. <em>TKDE</em>, <em>36</em>(12), 8541–8555. (<a
href="https://doi.org/10.1109/TKDE.2024.3465241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information cascade popularity prediction is an important problem in social network content diffusion analysis. Various facets have been investigated (e.g., diffusion structures and patterns, user influence) and, recently, deep learning models based on sequential architecture and graph neural network (GNN) have been leveraged. However, despite the improvements attained in predicting the future popularity, these methodologies fail to capture two essential aspects inherent to information diffusion: (1) the temporal irregularity of cascade event – i.e., users’ re-tweetings at random and non-periodic time instants; and (2) the inherent uncertainty of the information diffusion. To address these challenges, in this work, we present CasDO – a novel framework for information cascade popularity prediction with probabilistic diffusion models and neural ordinary differential equations (ODEs). We devise a temporal ODE network to generalize the discrete state transitions in RNNs to continuous-time dynamics. CasDO introduces a probabilistic diffusion model to consider the uncertainties in information diffusion by injecting noises in the forwarding process and reconstructing cascade embedding in the reversing process. Extensive experiments that we conducted on three large-scale datasets demonstrate the advantages of the CasDO model over baselines.},
  archive      = {J_TKDE},
  author       = {Zhangtao Cheng and Fan Zhou and Xovee Xu and Kunpeng Zhang and Goce Trajcevski and Ting Zhong and Philip S. Yu},
  doi          = {10.1109/TKDE.2024.3465241},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8541-8555},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Information cascade popularity prediction via probabilistic diffusion},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving conversational recommendation system through
personalized preference modeling and knowledge graph. <em>TKDE</em>,
<em>36</em>(12), 8529–8540. (<a
href="https://doi.org/10.1109/TKDE.2024.3421580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational recommendation systems (CRS) can actively discover users’ preferences and perform recommendations during conversations. The majority of works on CRS tend to focus on a single conversation and dig it using knowledge graphs, language models, etc. However, they often overlook the abundant and rich preference information that exists in the user&#39;s historical conversations. Meanwhile, end-to-end generation of recommendation results may lead to a decrease in recommendation quality. In this work, we propose a personalized conversational recommendation system infused with historical interaction information. This framework leverages users’ preferences extracted from their historical conversations and integrates them with the users’ preferences in current conversations. We find that this contributes to higher accuracy in recommendations and fewer recommendation turns. Moreover, we improve the interactive pattern between the recommendation module and the dialogue generation module by utilizing the slot filling method. This enables the results inferred by the recommendation module to be integrated into the conversation naturally and accurately. Our experiments on the benchmark dataset demonstrate that our model significantly outperforms the state-of-the-art methods in the evaluation of recommendations and dialogue generation.},
  archive      = {J_TKDE},
  author       = {Feng Wu and Guoshuai Zhao and Tengjiao Li and Jialie Shen and Xueming Qian},
  doi          = {10.1109/TKDE.2024.3421580},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8529-8540},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Improving conversational recommendation system through personalized preference modeling and knowledge graph},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Imbalanced node classification with synthetic over-sampling.
<em>TKDE</em>, <em>36</em>(12), 8515–8528. (<a
href="https://doi.org/10.1109/TKDE.2024.3443160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, graph neural networks (GNNs) have achieved state-of-the-art performance for node classification. However, most existing GNNs would suffer from the graph imbalance problem. In many real-world scenarios, node classes are imbalanced, with some majority classes making up most parts of the graph. The message propagation mechanism in GNNs would further amplify the dominance of those majority classes, resulting in sub-optimal classification performance. In this work, we seek to address this problem by generating pseudo instances of minority classes to balance the training data, extending previous over-sampling-based techniques. This task is non-trivial, as those techniques are designed with the assumption that instances are independent. Neglection of relation information would complicate this oversampling process. Furthermore, the node classification task typically takes the semi-supervised setting with only a few labeled nodes, providing insufficient supervision for the generation of minority instances. Generated new nodes of low quality would harm the trained classifier. In this work, we address these difficulties by synthesizing new nodes in a constructed embedding space, which encodes both node attributes and topology information. Furthermore, an edge generator is trained simultaneously to model the graph structure and provide relations for new samples. To further improve the data efficiency, we also explore synthesizing mixed “in-between” nodes to utilize nodes from the majority class in this over-sampling process. Experiments on real-world datasets validate the effectiveness of our proposed framework.},
  archive      = {J_TKDE},
  author       = {Tianxiang Zhao and Xiang Zhang and Suhang Wang},
  doi          = {10.1109/TKDE.2024.3443160},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8515-8528},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Imbalanced node classification with synthetic over-sampling},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyperedge graph contrastive learning. <em>TKDE</em>,
<em>36</em>(12), 8502–8514. (<a
href="https://doi.org/10.1109/TKDE.2024.3435861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although various graph contrastive learning (GCL) techniques have been employed to generate augmented views and maximize their mutual information, current solutions only consider the pairwise relationships based on edges, neglecting the high-order information that can help generate more informative augmented views and make better contrast. To fill in this gap, we propose to leverage hyperedge to facilitate GCL, as it connects two or more nodes and can model high-order relationships among multiple nodes. More specifically, hyperedges are constructed based on the original graph. Then, we conduct node-level PageRank based on hyperedges and hyperedge-level PageRank based on nodes to generate augmented views. As to the contrasting stage, different from existing GCL methods that simply treat the corresponding nodes of the anchor in different views as positives and overlook certain nodes strongly associated with the anchor, we build the positives and negatives based on hyperedges, where whether a node is a positive is determined by the number of hyperedges it coexists with the anchor. We compare our hyperedge GCL with state-of-the-art methods on downstream tasks, and the empirical results validate the superiority of our proposal. Further experiments on graph augmentation and graph contrastive loss also demonstrate the effectiveness of the proposed modules.},
  archive      = {J_TKDE},
  author       = {Junfeng Zhang and Weixin Zeng and Jiuyang Tang and Xiang Zhao},
  doi          = {10.1109/TKDE.2024.3435861},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8502-8514},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hyperedge graph contrastive learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyperbolic graph learning for social recommendation.
<em>TKDE</em>, <em>36</em>(12), 8488–8501. (<a
href="https://doi.org/10.1109/TKDE.2023.3343402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social recommendation provides an auxiliary social network structure to enhance recommendation performances. By formulating user-user social network and user-item interaction graph, modern social recommendation architecture is built on learning user and item embeddings into Euclidean space with graph convolution operations. However, the Euclidean space suffers structure distortion when representing the nature power-law distribution of graphs, leading to sub-optimal results for graph based social recommendation. Recently, some studies have explored the alternative of graph embedding learning into hyperbolic space, which can preserve the hierarchy of real-world graphs. However, directly applying current hyperbolic graph embedding models for social recommendation is non-trivial as two challenges: network heterogeneity and social diffusion noise. First, due to the semantic gap existing between social networks and user-item interactions, how to tackle the heterogeneity issue of social recommendation under hyperbolic formulation? Second, explicit modeling of social diffusion easily introduces noise for user preference learning, especially for those active users with amounts of interactions. To tackle the above challenges, in this paper, we propose a Hyperbolic Graph Learning based Social Recommendation (HGSR) model. First, we exploit social structure with hyperbolic social embedding pre-training, which could preserve the hierarchical properties of social networks. Second, we construct the heterogeneous graph based on user-item interactions and social networks, then treat the pre-trained social embeddings as an additional feature input for user preference learning. Such that, we combine explicit heterogeneous graph learning and implicit feature enhancement for the hyperbolic social recommendation, which can well tackle heterogeneity and social noise issues. We conduct empirical studies on four datasets, and extensive experiments demonstrate the effectiveness of our proposed model compared to state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Yonghui Yang and Le Wu and Kun Zhang and Richang Hong and Hailin Zhou and Zhiqiang Zhang and Jun Zhou and Meng Wang},
  doi          = {10.1109/TKDE.2023.3343402},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8488-8501},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hyperbolic graph learning for social recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human-AI interaction: Human behavior routineness shapes AI
performance. <em>TKDE</em>, <em>36</em>(12), 8476–8487. (<a
href="https://doi.org/10.1109/TKDE.2024.3480317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A crucial area of research in Human-AI Interaction focuses on understanding how the integration of AI into social systems influences human behavior, for example, how news-feeding algorithms affect people’s voting decisions. But little attention has been paid to how human behavior shapes AI performance. We fill this research gap by introducing routineness to measure human behavior for the AI system, which assesses the degree of routine in a person’s activity based on their past activities. We apply the proposed routineness metric to two extensive human behavior datasets: the human mobility dataset with over 700 million data samples and the social media dataset with over 3.8 million data samples. Our analysis reveals routineness can effectively detect behavioral changes in human activities. The performance of AI algorithms is profoundly determined by human routineness , which provides valuable guidance for the selection of AI algorithms.},
  archive      = {J_TKDE},
  author       = {Tianao Sun and Kai Zhao and Meng Chen},
  doi          = {10.1109/TKDE.2024.3480317},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8476-8487},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Human-AI interaction: Human behavior routineness shapes AI performance},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HoGRN: Explainable sparse knowledge graph completion via
high-order graph reasoning network. <em>TKDE</em>, <em>36</em>(12),
8462–8475. (<a href="https://doi.org/10.1109/TKDE.2024.3422226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graphs (KGs) are becoming increasingly essential infrastructures in many applications while suffering from incompleteness issues. The KG Completion (KGC) task automatically predicts missing facts based on an incomplete KG. However, existing methods perform unsatisfactorily in real-world scenarios. On the one hand, their performance will dramatically degrade along with the increasing sparsity of KGs. On the other hand, the inference procedure for prediction is an untrustworthy black box. This paper proposes a novel explainable model for sparse KGC, compositing high-order reasoning into a Graph Convolutional Network (GCN), namely HoGRN. It can not only improve the generalization ability to mitigate the information insufficiency issue but also provide interpretability while maintaining the model&#39;s effectiveness and efficiency. Two main components are seamlessly integrated for joint optimization. First, the high-order reasoning component learns high-quality relation representations by capturing endogenous correlation among relations. This can reflect logical rules to justify a broader range of missing facts. Second, the entity updating component leverages a weight-free GCN to efficiently model KG structures with interpretability. For evaluation, we conduct extensive experiments–the results of HoGRN on several sparse KGs present considerable improvements. Further ablation and case studies demonstrate the effectiveness of the main components.},
  archive      = {J_TKDE},
  author       = {Weijian Chen and Yixin Cao and Fuli Feng and Xiangnan He and Yongdong Zhang},
  doi          = {10.1109/TKDE.2024.3422226},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8462-8475},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {HoGRN: Explainable sparse knowledge graph completion via high-order graph reasoning network},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-capacity framework for reversible data hiding using
asymmetric numeral systems. <em>TKDE</em>, <em>36</em>(12), 8447–8461.
(<a href="https://doi.org/10.1109/TKDE.2024.3438943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reversible data hiding (RDH) has been extensively studied in the field of multimedia security. Embedding capacity is an important metric for RDH performance evaluation. However, the embedding capacity of existing methods for independent and identically distributed (i.i.d.) gray-scale signals is still not good enough. In this paper, we propose a high-capacity RDH code construction method that employs asymmetric numeral systems (ANS) coding as the underlying coding framework. Based on the proposed framework, two RDH methods are presented. First, we propose a static RDH method that takes the constant host probability mass function (PMF) as input parameters and offers high embedding performance. Then, we give a dynamic RDH method that can eliminate the need for transmitting the host PMF in advance by designing a reversible dynamic probability calculator. The simulation results on discrete normally distributed signals demonstrate that the performance of the proposed static method is very close to the expected rate-distortion bound, and the proposed dynamic method can achieve satisfactory embedding capacity without prior knowledge of host PMF at the cost of slightly sacrificing steganographic data quality. Moreover, the experimental results on gray-scale images show that the proposed static method provides higher peak signal-to-noise ratio (PSNR) values and larger embedding capacities than some state-of-the-art methods, e.g., the embedding capacity of image Lena is as high as 3.571 bits per pixel.},
  archive      = {J_TKDE},
  author       = {Na Wang and Shuxi Xu and Chuan Qin and Sian-Jheng Lin and Shuo Shao and Yunghsiang S. Han},
  doi          = {10.1109/TKDE.2024.3438943},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8447-8461},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {High-capacity framework for reversible data hiding using asymmetric numeral systems},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical active learning with label proportions on data
regions. <em>TKDE</em>, <em>36</em>(12), 8434–8446. (<a
href="https://doi.org/10.1109/TKDE.2024.3419588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning classification models from real-world data often requires substantial human effort devoted to instance annotation. As the instance-based annotating process can be very time-consuming and costly, we propose a novel active learning framework that builds classification models from human-annotated regions . A region is defined by a set of conjunctive patterns that are formed by value ranges over the input features. A region label is a human assessment of the class proportion in the data population covered by the region. By leveraging learning from label proportions algorithms, regions and their class proportions can be used to train instance-based classification models. However, the key challenge is that in practice, very few regions are defined already. Therefore, to identify regions important for model learning, we design a hierarchical active learning (HAL) framework, which actively builds a hierarchy of regions. Similar to the decision-tree learning process, our approach progressively divides the input data space into smaller sub-regions, solicits labels for the new regions, and retrains the base classification model with all the leaf regions. And we further develop a multi-hierarchy (forest) solution, which builds multiple shallower hierarchies that have more informative, diverse, and simpler regions. We evaluate our HAL framework on numerous impactful classification datasets as well as on a real user study - on the survival analysis of colorectal cancer patients. The results demonstrate that region-based active learning methods can learn high-quality classifiers from very few labeled regions. Hence, our framework is shown very effective in reducing the human annotation effort needed for building classification models.},
  archive      = {J_TKDE},
  author       = {Zhipeng Luo and Qiang Gao and Yazhou He and Hongjun Wang and Milos Hauskrecht and Tianrui Li},
  doi          = {10.1109/TKDE.2024.3419588},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8434-8446},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hierarchical active learning with label proportions on data regions},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous multivariate functional time series modeling:
A state space approach. <em>TKDE</em>, <em>36</em>(12), 8421–8433. (<a
href="https://doi.org/10.1109/TKDE.2024.3472906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional data have been gaining increasing popularity in the field of time series analysis. However, so far modeling heterogeneous multivariate functional time series remains a research gap. To fill it, this paper proposes a time-varying functional state space model (TV-FSSM). It uses functional decomposition to extract features of the functional observations, where the decomposition coefficients are regarded as latent states that evolve according to a tensor autoregressive model. This two-layer structure can on the one hand efficiently extract continuous functional features, and on the other provide a flexible and generalized description of data heterogeneity among different time points. An expectation maximization (EM) framework is developed for parameter estimation, where regularization and constraints are incorporated for better model interoperability. As the sample size grows, an incremental learning version of the EM algorithm is given to efficiently update the model parameters. Some model properties, including model identifiability conditions, convergence issues, time complexities, and bounds of its one-step-ahead prediction errors, are also presented. Extensive experiments on both real and synthetic datasets are performed to evaluate the predictive accuracy and efficiency of the proposed framework.},
  archive      = {J_TKDE},
  author       = {Peiyao Liu and Junpeng Lin and Chen Zhang},
  doi          = {10.1109/TKDE.2024.3472906},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8421-8433},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Heterogeneous multivariate functional time series modeling: A state space approach},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph representation learning based on cognitive spreading
activations. <em>TKDE</em>, <em>36</em>(12), 8408–8420. (<a
href="https://doi.org/10.1109/TKDE.2024.3437781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph representation learning is an emerging area for graph analysis and inference. However, existing approaches for large-scale graphs either sample nodes in sequential walks or manipulate the adjacency matrices of graphs. The former approach can cause sampling bias against less-connected nodes, whereas the latter may suffer from sparsity that exists in many real-world graphs. To learn from structural information in a graph more efficiently and comprehensively, this paper proposes a new graph representation learning approach inspired by the cognitive model of spreading-activation mechanisms in human memory. This approach learns node embeddings by adopting a graph activation model that allows nodes to “activate” their neighbors and spread their own structural information to other nodes through the paths simultaneously. Comprehensive experiments demonstrate that the proposed model performs better than existing methods on several empirical datasets for multiple graph inference tasks. Meanwhile, the spreading-activation-based model is computationally more efficient than existing approaches–the training process converges after only a small number of iterations, and the training time is linear in the number of edges in a graph. The proposed method works for both homogeneous and heterogeneous graphs.},
  archive      = {J_TKDE},
  author       = {Jie Bai and Kang Zhao and Linjing Li and Daniel Zeng and Qiudan Li and Fan Yang and Quannan Zu},
  doi          = {10.1109/TKDE.2024.3437781},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8408-8420},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph representation learning based on cognitive spreading activations},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph diffusion-based representation learning for sequential
recommendation. <em>TKDE</em>, <em>36</em>(12), 8395–8407. (<a
href="https://doi.org/10.1109/TKDE.2024.3477621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommendation is a critical part of the flourishing online applications by suggesting appealing items on users’ next interactions, where global dependencies among items have proven to be indispensable for enhancing the quality of item representations toward a better understanding of user dynamic preferences. Existing methods rely on pre-defined graphs with shallow Graph Neural Networks to capture such necessary dependencies due to the constraint of the over-smoothing problem. However, this graph representation learning paradigm makes them difficult to satisfy the original expectation because of noisy graph structures and the limited ability of shallow architectures for modeling high-order relations. In this paper, we propose a novel Graph Diffusion Representation-enhanced Attention Network for sequential recommendation, which explores the construction of deeper networks by utilizing graph diffusion on adaptive graph structures for generating expressive item representations. Specifically, we design an adaptive graph generation strategy via leveraging similarity learning between item embeddings, automatically optimizing the input graph topology under the guidance of downstream recommendation tasks. Afterward, we propose a novel graph diffusion paradigm with robustness to over-smoothing, which enriches the learned item representations with sufficient global dependencies for attention-based sequential modeling. Moreover, extensive experiments demonstrate the effectiveness of our approach over state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Zhaobo Wang and Yanmin Zhu and Chunyang Wang and Xuhao Zhao and Bo Li and Jiadi Yu and Feilong Tang},
  doi          = {10.1109/TKDE.2024.3477621},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8395-8407},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph diffusion-based representation learning for sequential recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global optimal travel planning for massive travel queries in
road networks. <em>TKDE</em>, <em>36</em>(12), 8377–8394. (<a
href="https://doi.org/10.1109/TKDE.2024.3439409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Travel planning plays an increasingly important role in our society. The travel plans, which consist of the paths each vehicle is suggested to follow and its corresponding departure time, influence the traffic conditions naturally. However, existing travel planning algorithms cannot consider the planning results and their influences simultaneously, so traffic congestion could be created when many vehicles are directed to adopt similar travel plans. In this paper, we propose the Global Optimal Travel Planning (GOTP) problem that aims to minimize traffic congestion by continuously evaluating traffic conditions for a set of planning tasks. Achieving this global optimization goal is non-trivial because travel planning and traffic evaluation are time-consuming and interdependent. To break this dependency, we first propose a GOTP paradigm that interleaves travel planning and traffic evaluation for queries, where the planning consists of departure time planning and travel path planning. To implement the paradigm, we propose the serial model that optimizes travel plans one by one, followed by the batch model that improves processing efficiency, and the iterative model that further optimizes planning quality. Extensive experiments on large real-world networks with synthetic and real workloads validate the effectiveness and efficiency of our methods.},
  archive      = {J_TKDE},
  author       = {Yehong Xu and Lei Li and Mengxuan Zhang and Zizhuo Xu and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2024.3439409},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8377-8394},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Global optimal travel planning for massive travel queries in road networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GeoCo: Geographical correlation enhanced network for POI
recommendation. <em>TKDE</em>, <em>36</em>(12), 8362–8376. (<a
href="https://doi.org/10.1109/TKDE.2024.3425151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User mobility behaviors frequently exhibit a spatial clustering phenomenon, wherein points of interest (POIs) visited by the same user tend to be in close proximity. Consequently, leveraging geographical influences for user preference modeling remains a prevalent approach in POI recommendation tasks. However, existing studies often overlook users’ hidden geographical habits for the following reasons: (1) Geographical features are commonly approximated by manually partitioned regions or fixed distributions, inadequately capturing the nuanced spatial proximity among POIs. (2) POIs with high geographical correlations are not explicitly incorporated as feedback signals during the training process, resulting in a lack of spatial clustering pattern learning within users’ preference representations. This paper introduces GeoCo, a Geo graphical Co rrelation enhanced network for POI recommendation. First, we model POIs’ geographical features using fine-grained hierarchical sequences to capture multilevel spatial relations. Subsequently, we propose a pre-training network that employs the sentence similarity assessment technique to comprehend the semantics of geographical correlations. Second, we introduce a novel multi-objective training process that intuitively learns spatial clustering patterns through user mobility behaviors. Extensive experiments conducted on two location-based social network (LBSN) datasets, Gowalla and Foursquare, demonstrate the superiority of our proposed model over fourteen state-of-the-art baseline models in POI recommendation tasks. Compared with the baselines, GeoCo has achieved a performance improvement of at least 5 $\%$ in Rec@5 and HR@5 on both datasets. Furthermore, we verify the effectiveness of pre-trained location vectors and the multi-objective training process in enhancing the model&#39;s understanding of geographical correlations for user preference construction.},
  archive      = {J_TKDE},
  author       = {Xuan Pan and Xiangrui Cai and Sihan Xu and Ying Zhang and Peng Nie and Xiaojie Yuan},
  doi          = {10.1109/TKDE.2024.3425151},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8362-8376},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GeoCo: Geographical correlation enhanced network for POI recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). General quasi overlap functions and fuzzy neighborhood
systems-based fuzzy rough sets with their applications. <em>TKDE</em>,
<em>36</em>(12), 8349–8361. (<a
href="https://doi.org/10.1109/TKDE.2024.3474728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy rough sets are important mathematical tool for processing data using existing knowledge. Fuzzy rough sets have been widely studied and used into various fields, such as data reduction and image processing, etc. In extensive literature we have studied, general quasi overlap functions and fuzzy neighborhood systems are broader than other all fuzzy operators and knowledge used in existing fuzzy rough sets, respectively. In this article, a novel fuzzy rough sets model (shortly ( I , Q , NS )-fuzzy rough sets) is proposed using fuzzy implications, general quasi overlap functions and fuzzy neighborhood systems, which contains almost all existing fuzzy rough sets. Then, a novel feature selection algorithm (called IQNS-FS algorithm) is proposed and implemented using ( I , Q , NS )-fuzzy rough sets, dependency and specificity measure. The results of 12 datasets indicate that IQNS-FS algorithm performs better than others. Finally, we input the results of IQNS-FS algorithm into single hidden layer neural networks and other classification algorithms, the results illustrate that the IQNS-FS algorithm can be better connected with neural networks than other classification algorithms. The high classification accuracy of single hidden layer neural networks (a very simple structure) further shows that the attributes selected by the IQNS-FS algorithm are important which can express the features of the datasets.},
  archive      = {J_TKDE},
  author       = {Mengyuan Li and Xiaohong Zhang and Jiaoyan Shang and Yingcang Ma},
  doi          = {10.1109/TKDE.2024.3474728},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8349-8361},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {General quasi overlap functions and fuzzy neighborhood systems-based fuzzy rough sets with their applications},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From wide to deep: Dimension lifting network for
parameter-efficient knowledge graph embedding. <em>TKDE</em>,
<em>36</em>(12), 8341–8348. (<a
href="https://doi.org/10.1109/TKDE.2024.3437479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph embedding (KGE) that maps entities and relations into vector representations is essential for downstream applications. Conventional KGE methods require high-dimensional representations to learn the complex structure of knowledge graph, but lead to oversized model parameters. Recent advances reduce parameters by low-dimensional entity representations, while developing techniques (e.g., knowledge distillation or reinvented representation forms) to compensate for reduced dimension. However, such operations introduce complicated computations and model designs that may not benefit large knowledge graphs. To seek a simple strategy to improve the parameter efficiency of conventional KGE models, we take inspiration from that deeper neural networks require exponentially fewer parameters to achieve expressiveness comparable to wider networks for compositional structures. We view all entity representations as a single-layer embedding network, and conventional KGE methods that adopt high-dimensional entity representations equal widening the embedding network to gain expressiveness. To achieve parameter efficiency, we instead propose a deeper embedding network for entity representations, i.e., a narrow entity embedding layer plus a multi-layer dimension lifting network (LiftNet). Experiments on three public datasets show that by integrating LiftNet, four conventional KGE methods with 16-dimensional representations achieve comparable link prediction accuracy as original models that adopt 512-dimensional representations, saving 68.4% to 96.9% parameters.},
  archive      = {J_TKDE},
  author       = {Borui Cai and Yong Xiang and Longxiang Gao and Di Wu and He Zhang and Jiong Jin and Tom Luan},
  doi          = {10.1109/TKDE.2024.3437479},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8341-8348},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {From wide to deep: Dimension lifting network for parameter-efficient knowledge graph embedding},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From a timeline contact graph to close contact tracing and
infection diffusion intervention. <em>TKDE</em>, <em>36</em>(12),
8328–8340. (<a href="https://doi.org/10.1109/TKDE.2024.3423476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel graph structure to address the problems of information spreading in a real-world, frequently updating graph, with two main contributions at hand: accurately tracing infection diffusion according to fine-grained user movements and finding vulnerable vertices under the virus immunization scenario to mitigate infection diffusion. Unlike previous work that primarily predicts the long-term epidemic trend at the census level, this study aims to intervene in the short-term at the individual level. Therefore, two downstream tasks are formulated to illustrate practicalities: E pidemic M itigating in Public A rea problem ( $EMA$ ) and E pidemic Maximized S pread in Public A rea problem ( $ESA$ ), where $EMA$ aims to find intervention strategies, and $ESA$ is an adversarial solution against the intervention strategy to test the robustness. Comprehensive experiments are conducted using two real-world datasets with millions of public transport trips, which demonstrate the effectiveness of our approach and highlight the importance of considering the dynamic nature of close contacts in epidemic modelling.},
  archive      = {J_TKDE},
  author       = {Yipeng Zhang and Zhifeng Bao and Yuchen Li and Baihua Zheng and Xiaoli Wang},
  doi          = {10.1109/TKDE.2024.3423476},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8328-8340},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {From a timeline contact graph to close contact tracing and infection diffusion intervention},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Friedkin-johnsen model for opinion dynamics on signed
graphs. <em>TKDE</em>, <em>36</em>(12), 8313–8327. (<a
href="https://doi.org/10.1109/TKDE.2024.3424974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A signed graph offers richer information than an unsigned graph, since it describes both collaborative and competitive relationships in social networks. In this paper, we study opinion dynamics on a signed graph, based on the Friedkin-Johnsen model. We first interpret the equilibrium opinion in terms of a defined random walk on an augmented signed graph, by representing the equilibrium opinion of every node as a combination of all nodes’ internal opinions, with the coefficient of the internal opinion for each node being the difference of two absorbing probabilities. We then quantify some relevant social phenomena and express them in terms of the $\ell _{2}$ norms of vectors. We also design a nearly-linear time signed Laplacian solver for assessing these quantities, by establishing a connection between the absorbing probability of random walks on a signed graph and that on an associated unsigned graph. We further study the opinion optimization problem by changing the initial opinions of a fixed number of nodes, which can be optimally solved in cubic time. We provide a nearly-linear time algorithm with an error guarantee to approximately solve the problem. Finally, we execute extensive experiments on sixteen real-life signed networks, which show that both of our algorithms are effective and efficient, and are scalable to massive graphs with over 20 million nodes.},
  archive      = {J_TKDE},
  author       = {Xiaotian Zhou and Haoxin Sun and Wanyue Xu and Wei Li and Zhongzhi Zhang},
  doi          = {10.1109/TKDE.2024.3424974},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8313-8327},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Friedkin-johnsen model for opinion dynamics on signed graphs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fractal belief rényi divergence with its applications in
pattern classification. <em>TKDE</em>, <em>36</em>(12), 8297–8312. (<a
href="https://doi.org/10.1109/TKDE.2023.3342907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multisource information fusion is a comprehensive and interdisciplinary subject. Dempster-Shafer (D-S) evidence theory copes with uncertain information effectively. Pattern classification is the core research content of pattern recognition, and multisource information fusion based on D-S evidence theory can be effectively applied to pattern classification problems. However, in D-S evidence theory, highly-conflicting evidence may cause counterintuitive fusion results. Belief divergence theory is one of the theories that are proposed to address problems of highly-conflicting evidence. Although belief divergence can deal with conflict between evidence, none of the existing belief divergence methods has considered how to effectively measure the discrepancy between two pieces of evidence with time evolutionary. In this study, a novel fractal belief Rényi (FBR) divergence is proposed to handle this problem. We assume that it is the first divergence that extends the concept of fractal to Rényi divergence. The advantage is measuring the discrepancy between two pieces of evidence with time evolution, which satisfies several properties and is flexible and practical in various circumstances. Furthermore, a novel algorithm for multisource information fusion based on FBR divergence, namely FBReD-based weighted multisource information fusion, is developed. Ultimately, the proposed multisource information fusion algorithm is applied to a series of experiments for pattern classification based on real datasets, where our proposed algorithm achieved superior performance.},
  archive      = {J_TKDE},
  author       = {Yingcheng Huang and Fuyuan Xiao and Zehong Cao and Chin-Teng Lin},
  doi          = {10.1109/TKDE.2023.3342907},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8297-8312},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fractal belief rényi divergence with its applications in pattern classification},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FPSR+: Toward robust, efficient, and scalable collaborative
filtering with partition-aware item similarity modeling. <em>TKDE</em>,
<em>36</em>(12), 8283–8296. (<a
href="https://doi.org/10.1109/TKDE.2024.3418080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative filtering (CF) has been extensively studied in recommendation, spawning various solutions. While graph convolution networks (GCNs) are effective at representation learning, their efficiency is lacking. Comparatively, item similarity model efficiently establishes direct relationships between items. In spite of this, the modeling problem grows quadratically as the number of items increases. This poses critical scalability issues. In this paper, through an investigation of the latest GCN model, we reveal the feasibility of optimizing the process of similarity modeling using the underlying group structure in the item set. Based on these findings, we propose a novel model which introduces graph partitioning to reduce the scale of similarity modeling problem, dubbed FPSR+. Specifically, we divide similarity modeling of items into sub-problems within each partition, and incorporate global and local prior knowledge to alleviate information loss. Following an analysis of the properties of different items in partitioning, we propose a new hub set selection strategy that improves the robustness of FPSR+ in the small partition case. Extensive experiments on four real-world datasets demonstrate the superior performance of FPSR+ compared with state-of-the-art GCN models and item similarity models, as well as several-fold speedups and reductions in parameter storage.},
  archive      = {J_TKDE},
  author       = {Tianjun Wei and Tommy W. S. Chow and Jianghong Ma},
  doi          = {10.1109/TKDE.2024.3418080},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8283-8296},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {FPSR+: Toward robust, efficient, and scalable collaborative filtering with partition-aware item similarity modeling},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FPrompt-PLM: Flexible-prompt on pretrained language model
for continual few-shot relation extraction. <em>TKDE</em>,
<em>36</em>(12), 8267–8282. (<a
href="https://doi.org/10.1109/TKDE.2024.3419117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relation extraction (RE) aims to identify the relation between two entities within a sentence, which plays a crucial role in information extraction. Traditional supervised setting on RE does not fit the actual scenario, due to the continuous emergence of new relations and the unavailability of massive labeled examples. Continual few-shot relation extraction (CFS-RE) is proposed as a potential solution to the above situation, which requires the model to learn new relations sequentially from a few examples. Apparently, CFS-RE is more challenging than previous RE, as the catastrophic forgetting of old knowledge and few-shot overfitting on a handful of examples. To this end, we propose a novel flexible-prompt framework on pretrained language model named FPrompt-PLM for CFS-RE, which includes flexible-prompt embedding, pretrained-language understanding, and nearest-prototype learning modules. Note that two pools in FPrompt-PLM, i.e., prompt and prototype pools, are continual updated and applied for prediction of all seen relations at current time-step. The former pool records the distinctive prompt embedding in each time period, and the latter records all learned relation prototypes. Besides, three progressive stages are introduced to learn FPrompt-PLM&#39;s parameters and apply this model for CFS-RE testing, which includes meta-training, continual meta-finetuning, and testing stages. And we improve the CFS-RE loss by incorporating multiple distillation losses as well as a novel prototype-diversity loss in these stages to alleviate the catastrophic forgetting and few-shot overfitting problems. Comprehensive experiments on two widely-used datasets show that FPrompt-PLM achieves significant performance improvements over the SOTA baselines.},
  archive      = {J_TKDE},
  author       = {Lingling Zhang and Yifei Li and Qianying Wang and Yun Wang and Hang Yan and Jiaxin Wang and Jun Liu},
  doi          = {10.1109/TKDE.2024.3419117},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8267-8282},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {FPrompt-PLM: Flexible-prompt on pretrained language model for continual few-shot relation extraction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Forecasting turning points in stock price by integrating
chart similarity and multipersistence. <em>TKDE</em>, <em>36</em>(12),
8251–8266. (<a href="https://doi.org/10.1109/TKDE.2024.3444814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting financial data plays a crucial role in financial market. Relying solely on prices or price trends as prediction targets often leads to a vast of invalid transactions. As a result, researchers have increasingly turned their attention to turning points as the prediction target. Surprisingly, existing methods have largely overlooked the role of technical charts, despite turning points being closely related to the technical charts. Recently, several researchers have attempted to utilize chart information via converting price sequences into images for turning point forecasting, but robustness and convergence problems arise. To address these challenges and enhance the turning point predictions, this article introduces a new method known as MPCNet. Specifically, we first transform the price series into a graph structure using chart similarity to robustly extract valuable information from technical charts. Additionally, we introduce the multipersistence topology tool to accurately predict stock turning points and provide convergence guarantee. Experimental results demonstrate the significant superiority of our proposed model over existing methods. Furthermore, based on additional performance evaluations using real stock data, MPCNet consistently achieves the highest average return during the transaction backtesting period. Meanwhile, we provide empirical validation of robustness and theoretical analysis to confirm its convergence, establishing it as a superior tool for financial forecasting.},
  archive      = {J_TKDE},
  author       = {Shangzhe Li and Yingke Liu and Xueyuan Chen and Junran Wu and Ke Xu},
  doi          = {10.1109/TKDE.2024.3444814},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8251-8266},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Forecasting turning points in stock price by integrating chart similarity and multipersistence},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fine-grained semantics enhanced contrastive learning for
graphs. <em>TKDE</em>, <em>36</em>(12), 8238–8250. (<a
href="https://doi.org/10.1109/TKDE.2024.3466990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph contrastive learning defines a contrastive task to pull similar instances close and push dissimilar instances away. It learns discriminative node embeddings without supervised labels, which has aroused increasing attention in the past few years. Nevertheless, existing methods of graph contrastive learning ignore the differences between diverse semantics existed in graphs, which learn coarse-grained node embeddings and lead to sub-optimal performances on downstream tasks. To bridge this gap, we propose a novel F ine-grained S emantics enhanced G raph C ontrastive L earning (FSGCL) in this paper. Concretely, FSGCL first introduces a motif-based graph construction, which employs graph motifs to extract diverse semantics existed in graphs from the perspective of input data. Then, the semantic-level contrastive task is explored to further enhance the utilization of fine-grained semantics from the perspective of model training. Experiments on five real-world datasets demonstrate the superiority of our proposed FSGCL over state-of-the-art methods. To make the results reproducible, we will make our codes public on GitHub after this paper is accepted.},
  archive      = {J_TKDE},
  author       = {Youming Liu and Lin Shu and Chuan Chen and Zibin Zheng},
  doi          = {10.1109/TKDE.2024.3466990},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8238-8250},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fine-grained semantics enhanced contrastive learning for graphs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated learning with heterogeneous client expectations: A
game theory approach. <em>TKDE</em>, <em>36</em>(12), 8220–8237. (<a
href="https://doi.org/10.1109/TKDE.2024.3464488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In federated learning (FL), local models are trained independently by clients, local model parameters are shared with a global aggregator or server, and then the updated model is used to initialize the next round of local training. FL and its variants have become synonymous with privacy-preserving distributed machine learning. However, most FL methods have maximization of model accuracy as their sole objective, and rarely are the clients’ needs and constraints considered. In this paper, we consider that clients have differing performance expectations and resource constraints, and we assume local data quality can be improved at a cost. In this light, we treat FL in the training phase as a game in satisfaction form that seeks to satisfy all clients’ expectations. We propose two novel FL methods, a deep reinforcement learning method and a stochastic method, that embrace this design approach. We also account for the scenario where certain clients can adjust their actions even after being satisfied, by introducing probabilistic parameters in both of our methods. The experimental results demonstrate that our proposed methods converge quickly to a lower cost solution than competing methods. Furthermore, it was found that the probabilistic parameters facilitate the attainment of satisfaction equilibria (SE), addressing scenarios where reaching SEs may be challenging within the confines of traditional games in satisfaction form.},
  archive      = {J_TKDE},
  author       = {Sheng Shen and Chi Liu and Teng Joon Lim},
  doi          = {10.1109/TKDE.2024.3464488},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8220-8237},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Federated learning with heterogeneous client expectations: A game theory approach},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FedEAN: Entity-aware adversarial negative sampling for
federated knowledge graph reasoning. <em>TKDE</em>, <em>36</em>(12),
8206–8219. (<a href="https://doi.org/10.1109/TKDE.2024.3464516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated knowledge graph reasoning (FedKGR) aims to perform reasoning over different clients while protecting data privacy, drawing increasing attention to its high practical value. Previous works primarily focus on data heterogeneity, ignoring challenges from limited data scale and primitive negative sample strategies, i.e., random entity replacement, which yield low-quality negatives and zero loss issues. Meanwhile, generative adversarial networks (GANs) are widely used in different fields to generate high-quality negative samples, but no work has been developed for FedKGR. To this end, we propose a plug-and-play E ntity-aware A dversarial N egative sampling strategy for FedKGR, termed FedEAN. Specifically, we are the first to adopt GANs to generate high-quality negative samples in different clients. It takes the target triplet in each batch as input and outputs high-quality negative samples, which guaranteed by the joint training of the generator and discriminator. Moreover, we design an entity-aware adaptive negative sampling mechanism based on the similarity of entity representations before and after server aggregation, which can persevere the entity global consistency across clients during training. Extensive experiments demonstrate that FedEAN excels with various FedKGR backbones, demonstrating its ability to construct high-quality negative samples and address the zero-loss issue.},
  archive      = {J_TKDE},
  author       = {Lingyuan Meng and Ke Liang and Hao Yu and Yue Liu and Sihang Zhou and Meng Liu and Xinwang Liu},
  doi          = {10.1109/TKDE.2024.3464516},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8206-8219},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {FedEAN: Entity-aware adversarial negative sampling for federated knowledge graph reasoning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature-aware contrastive learning with bidirectional
transformers for sequential recommendation. <em>TKDE</em>,
<em>36</em>(12), 8192–8205. (<a
href="https://doi.org/10.1109/TKDE.2023.3343345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning with Transformer-based sequence encoder has gained predominance for sequential recommendation due to its ability to mitigate the data noise and the data sparsity issue. However, existing contrastive learning approaches for sequential recommendation still suffer from two limitations. First, they mainly center on left-to-right unidirectional Transformers as base encoders, which are suboptimal for sequential recommendation because user behaviors may not be a rigid left-to-right sequence. Second, they devise contrastive learning objectives only from the sequence level, neglecting the rich self-supervision signals from the feature level. To address these limitations, we propose a novel framework called F eature-aware C ontrastive L earning with bidirectional Transformers for sequential Rec ommendation ( FCLRec ) to effectively leverage feature information for sequential recommendation. Specifically, we first augment bidirectional Transformers with a novel feature-aware self-attention module that is able to simultaneously model the complex relationships between sequences and features. Next, we propose a novel feature-aware contrastive learning objective that generates a collection of positive samples via three types of augmentations from three different levels. Finally, we adopt feature prediction as an auxiliary task to strengthen the connections between items and features. Our experimental results on four public benchmark datasets show that FCLRec outperforms the state-of-the-art methods for sequential recommendation.},
  archive      = {J_TKDE},
  author       = {Hanwen Du and Huanhuan Yuan and Pengpeng Zhao and Deqing Wang and Victor S. Sheng and Yanchi Liu and Guanfeng Liu and Lei Zhao},
  doi          = {10.1109/TKDE.2023.3343345},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8192-8205},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Feature-aware contrastive learning with bidirectional transformers for sequential recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Euclidean distance is not your swiss army knife.
<em>TKDE</em>, <em>36</em>(12), 8179–8191. (<a
href="https://doi.org/10.1109/TKDE.2024.3424511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based multi-view learning, which has hitherto been used to discover the intrinsic patterns of graph data giving the credit to its convenience of implementation and effectiveness. Note that even though these approaches have been increasingly adopted in multi-view clustering and have generated promising outcomes, they are still faced with the sub-optimal solution. For one thing, multi-view data can be corrupted in the raw feature space. For the other, most existing approaches normally utilize euclidean distance to obtain the similarity between two samples, which can not be the best option for all types of real-world data and leads to inferior results. Therefore, to overcome the aforementioned issues, we integrate multi-metric learning, graph filtering, and subspace learning into a collaborative learning framework for multi-view clustering. Particularly, we prefer to recover a smooth representation of data by graph filtering, which can reserve the geometric structure of the original multi-view data and discard the corruptions simultaneously. Furthermore, instead of using euclidean distance as a Swiss army knife, multiple metrics are utilized to fully exploit the correlation of data based on the smooth representation, hence finally facilitating the downstream clustering task. Extensive experiments on multi-view clustering tasks validate our theoretical findings of ours and prove the improvement of our method over the SOTA approaches.},
  archive      = {J_TKDE},
  author       = {Yuze Tan and Yixi Liu and Hongjie Wu and Shudong Huang and Zenglin Xu and Ivor W. Tsang and Jiancheng Lv},
  doi          = {10.1109/TKDE.2024.3424511},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8179-8191},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Euclidean distance is not your swiss army knife},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing precision drug recommendations via in-depth
exploration of motif relationships. <em>TKDE</em>, <em>36</em>(12),
8164–8178. (<a href="https://doi.org/10.1109/TKDE.2024.3437775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Making accurate and safe clinical decisions for patients has long been a challenging task. With the proliferation of electronic health records and the rapid advancement of technology, drug recommender systems have emerged as invaluable aids for healthcare professionals, offering precise and secure prescriptions. Among prevailing methods, the exploration of motifs, defined as substructures with specific biological functions, has largely been overlooked. Nevertheless, the substantial impact of the motifs on drug efficacy and patient diseases implies that a more extensive incorporation could potentially improve the recommender systems. In light of this, we introduce DEPOT , an innovative drug recommendation framework developed from a motif-aware perspective. In our approach, we employ chemical decomposition to partition drug molecules into semantic motif-trees and design a structure-aware graph transformer to capture motif collaboration. This innovative practice preserves the topology knowledge and facilitates perception of drug functionality. To delve into the dynamic correlation between motifs and disease progression, we conduct a meticulous investigation from two perspectives: repetition and exploration. This comprehensive analysis allows us to gain valuable insights into the drug turnover, with the former focusing on reusability and the latter on discovering new requirements. We further formulate a historical weighting strategy for drug-drug interaction (DDI) objective, enabling adaptive control over the trade-off between accuracy and safety criteria throughout the training process. Extensive experiments conducted on four data sets validate the effectiveness and robustness of DEPOT .},
  archive      = {J_TKDE},
  author       = {Chuang Zhao and Hongke Zhao and Xiaofang Zhou and Xiaomeng Li},
  doi          = {10.1109/TKDE.2024.3437775},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8164-8178},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Enhancing precision drug recommendations via in-depth exploration of motif relationships},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enabling verifiable and secure range query in multi-user
setting under cloud environments. <em>TKDE</em>, <em>36</em>(12),
8148–8163. (<a href="https://doi.org/10.1109/TKDE.2024.3419930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data outsourcing to the cloud has become increasingly popular for high-speed storage and retrieval. However, privacy and security are pressing concerns that hinder the further development of cloud computing. A common approach is to encrypt data before outsourcing, assuming the cloud is semi-honest. However, in reality, the cloud may be malicious and forge query results unexpectedly. Moreover, most previous schemes are designed for single-user setting, where different users share the same secret key, leading to potential privacy leaks. Therefore, ensuring confidentiality and verifiability in multi-user setting is crucial but has not been well-addressed. In this paper, we formally define the notion of Verifiable and Secure Range Query in Multi-User Setting (VSRQM) and propose a prefix-aware encoding (Pcode) scheme to encode spatial data for query processing. Next, we design a Tree-Aided Verifiable and Secure Index (SATree) on top of the Pcode and symmetric re-encryption scheme. SATree preserves data privacy, provides a mechanism to verify query results’ integrity and achieves sub-linear search time. Additionally, we propose two compression schemes to reduce the space cost of storage and transmission. Finally, we present formal complexity and security analyses and conduct empirical evaluations on real and synthetic datasets to demonstrate our proposed approaches’ practical performance.},
  archive      = {J_TKDE},
  author       = {Ningning Cui and Dong Wang and Huaijie Zhu and Jianxin Li and Jianliang Xu and Xiaochun Yang},
  doi          = {10.1109/TKDE.2024.3419930},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8148-8163},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Enabling verifiable and secure range query in multi-user setting under cloud environments},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Eigenvalue ratio inspired partition learning and fusion for
multiple kernel clustering. <em>TKDE</em>, <em>36</em>(12), 8135–8147.
(<a href="https://doi.org/10.1109/TKDE.2024.3425393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple kernel clustering (MKC) aims to extract and integrate the clustering information from a set of pre-defined kernels for handling data which cannot be linearly separated well. More precisely, existing MKC methods generally devote to learn the complementary information from a set of kernel partitions, whose feature dimensions are commonly fixed as the upper bound $n$ or lower bound $c$ , where $n$ and $c$ represents the number of samples and clusters, respectively. However, the adopting of the lower bound or upper bound generally leads to poor clustering performance caused by the lack or redundancy of clustering information carried by kernel partitions. To tackle this issue, we propose a novel late fusion multiple kernel clustering method, termed as Eigenvalue Ratio Inspired Partition Learning and Fusion for Multiple Kernel Clustering (ERMKC), in this paper. Specifically, we propose an eigenvalue ratio based criterion to guide the kernel partition learning for each single kernel matrix, which ensures more suitable feature dimensions for the learnt kernel partitions. In addition, we also propose a novel late fusion model for fusing the learnt kernel partitions optimally. Furthermore, we conduct extensive experiments on numerous benchmark datasets to evaluate the proposed ERMKC method, whose results verify the effectiveness and advantage of the proposed method compared to the other state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Wenqi Yang and Chang Tang and Xiao Zheng and Xinzhong Zhu and Xinwang Liu},
  doi          = {10.1109/TKDE.2024.3425393},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8135-8147},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Eigenvalue ratio inspired partition learning and fusion for multiple kernel clustering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient unsupervised graph embedding with attributed graph
reduction and dual-level loss. <em>TKDE</em>, <em>36</em>(12),
8120–8134. (<a href="https://doi.org/10.1109/TKDE.2024.3436076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph embedding aims to extract low-dimensional representation vectors, commonly referred to as embeddings, from graph data. The generated embeddings simplify subsequent data analysis and machine learning tasks. Recently, researchers have proposed the use of contrastive learning on graphs to extract node embeddings in an unsupervised manner. Although existing graph contrastive learning methods have significantly advanced this field, there is still potential for further exploration, particularly in optimizing training efficiency and enhancing embedding quality . In this paper, we propose an efficient unsupervised graph embedding method named GEARED. First, the method involves an attributed graph reduction module that converts the raw graph into a reduced graph, greatly improving model training efficiency. Second, GEARED employs a dual-level loss with adaptive scaling factors to ensure the acquisition of high-quality embeddings. Finally, we conduct a partial derivative analysis to elucidate the specific mechanisms through which GEARED is capable of generating high-quality embeddings. Extensive experimental evaluations on 14 benchmark datasets show that GEARED consistently outperforms state-of-the-art methods in terms of training efficiency and classification accuracy. For instance, GEARED achieves a training speedup of over 40 times on both the CS and Physics datasets while maintaining superior classification accuracy.},
  archive      = {J_TKDE},
  author       = {Ziyang Liu and Chaokun Wang and Hao Feng and Ziyang Chen},
  doi          = {10.1109/TKDE.2024.3436076},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8120-8134},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient unsupervised graph embedding with attributed graph reduction and dual-level loss},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient neural network-based estimation of interval
shapley values. <em>TKDE</em>, <em>36</em>(12), 8108–8119. (<a
href="https://doi.org/10.1109/TKDE.2024.3420180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of Shapley Values (SVs) to explain machine learning model predictions is established. Recent research efforts have been devoted to generating efficient Neural Network-based SVs estimates. However, the variability of the generated estimates, which depend on the selected data sampling, model, and training parameters, brings the reliability of such estimates into question. By leveraging the concept of Interval SVs, we propose to incorporate SVs uncertainty directly into the learning process. Specifically, we explain ensemble models composed of multiple predictors, each one generating potentially different outcomes. Unlike all existing approaches, the explainer design is tailored to Interval SVs learning instead of SVs only. We present three new Network-based explainers relying on different ISV paradigms, i.e., a Multi-Task Learning network inspired by the Shapley value&#39;s weighted least squares characterization and two Interval Shapley-Like Value Neural estimators. The experiments thoroughly evaluate the new approaches on ten benchmark datasets, looking for the best compromise between intervals’ accuracy and explainers’ efficiency.},
  archive      = {J_TKDE},
  author       = {Davide Napolitano and Lorenzo Vaiani and Luca Cagliero},
  doi          = {10.1109/TKDE.2024.3420180},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8108-8119},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient neural network-based estimation of interval shapley values},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient heterogeneous graph learning via random
projection. <em>TKDE</em>, <em>36</em>(12), 8093–8107. (<a
href="https://doi.org/10.1109/TKDE.2024.3434956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous Graph Neural Networks (HGNNs) are powerful tools for deep learning on heterogeneous graphs. Typical HGNNs require repetitive message passing during training, limiting efficiency for large-scale real-world graphs. Recent pre-computation-based HGNNs use one-time message passing to transform a heterogeneous graph into regular-shaped tensors, enabling efficient mini-batch training. Existing pre-computation-based HGNNs can be mainly categorized into two styles, which differ in how much information loss is allowed and efficiency. We propose a hybrid pre-computation-based HGNN, named Random Projection Heterogeneous Graph Neural Network (RpHGNN), which combines the benefits of one style&#39;s efficiency with the low information loss of the other style. To achieve efficiency, the main framework of RpHGNN consists of propagate-then-update iterations, where we introduce a Random Projection Squashing step to ensure that complexity increases only linearly. To achieve low information loss, we introduce a Relation-wise Neighbor Collection component with an Even-odd Propagation Scheme, which aims to collect information from neighbors in a finer-grained way. Experimental results indicate that our approach achieves state-of-the-art results on seven small and large benchmark datasets while also being 230% faster compared to the most effective baseline. Surprisingly, our approach not only surpasses pre-processing-based baselines but also outperforms end-to-end methods.},
  archive      = {J_TKDE},
  author       = {Jun Hu and Bryan Hooi and Bingsheng He},
  doi          = {10.1109/TKDE.2024.3434956},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8093-8107},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient heterogeneous graph learning via random projection},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient and private federated trajectory matching.
<em>TKDE</em>, <em>36</em>(12), 8079–8092. (<a
href="https://doi.org/10.1109/TKDE.2024.3424411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Trajectory Matching (FTM) is gaining increasing importance in big trajectory data analytics, supporting diverse applications such as public health, law enforcement, and emergency response. FTM retrieves trajectories that match with a query trajectory from a large-scale trajectory database, while safeguarding the privacy of trajectories in both the query and the database. A naive solution to FTM is to process the query through Secure Multi-party Computation (SMC) across the entire database, which is inherently secure yet inevitably slow due to the massive secure operations. A promising acceleration strategy is to filter irrelevant trajectories from the database based on the query, thus reducing the SMC operations. However, a key challenge is how to publish the query in a way that both preserves privacy and enables efficient trajectory filtering. In this paper, we design ${\sf GIST}$ , a novel framework for efficient Federated Trajectory Matching. ${\sf GIST}$ is grounded in Geo-Indistinguishability, a privacy criterion dedicated to locations. It employs a new privacy mechanism for the query that facilitates efficient trajectory filtering. We theoretically prove the privacy guarantee of the mechanism and the accuracy of the filtering strategy of ${\sf GIST}$ . Extensive evaluations on five real datasets show that ${\sf GIST}$ is significantly faster and incurs up to 2 orders of magnitude lower communication cost than the state-of-the-arts.},
  archive      = {J_TKDE},
  author       = {Yuxiang Wang and Yuxiang Zeng and Shuyuan Li and Yuanyuan Zhang and Zimu Zhou and Yongxin Tong},
  doi          = {10.1109/TKDE.2024.3424411},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8079-8092},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient and private federated trajectory matching},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient and effective augmentation framework with latent
mixup and label-guided contrastive learning for graph classification.
<em>TKDE</em>, <em>36</em>(12), 8066–8078. (<a
href="https://doi.org/10.1109/TKDE.2024.3471659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) with data augmentation obtain promising results among existing solutions for graph classification. Mixup-based augmentation methods for graph classification have already achieved state-of-the-art performance. However, existing mixup-based augmentation methods either operate in the input space and thus face the challenge of balancing efficiency and accuracy, or directly conduct mixup in the latent space without similarity guarantee, thus leading to lacking semantic validity and limited performance. To address these limitations, this paper proposes $\mathcal {G}$ -MixCon, a novel framework leveraging the strengths of Mix up-based augmentation and supervised Con trastive learning (SCL). To the best of our knowledge, this is the first attempt to develop an SCL-based approach for learning graph representations. Specifically, the mixup-based strategy within the latent space named $GDA_{gl}$ and $GDA_{nl}$ are proposed, which efficiently conduct linear interpolation between views of the node or graph level. Furthermore, we design a dual-objective loss function named SupMixCon that can consider both the consistency among graphs and the distances between the original and augmented graph. SupMixCon can guide the training process for SCL in $\mathcal {G}$ -MixCon while achieving a similarity guarantee. Comprehensive experiments are conducted on various real-world datasets, the results show that $\mathcal {G}$ -MixCon demonstrably enhances performance, achieving an average accuracy increment of 6.24%, and significantly increases the robustness of GNNs against noisy labels.},
  archive      = {J_TKDE},
  author       = {Aoting Zeng and Liping Wang and Wenjie Zhang and Xuemin Lin},
  doi          = {10.1109/TKDE.2024.3471659},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8066-8078},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient and effective augmentation framework with latent mixup and label-guided contrastive learning for graph classification},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effective generalized low-rank tensor contextual bandits.
<em>TKDE</em>, <em>36</em>(12), 8051–8065. (<a
href="https://doi.org/10.1109/TKDE.2024.3469782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we aim to build a novel bandits algorithm that is capable of fully harnessing the power of multi-dimensional data and the inherent non-linearity of reward functions to provide high-usable and accountable decision-making services. To this end, we introduce a generalized low-rank tensor contextual bandits model in which an action is formed from three feature vectors, and thus is represented by a tensor. In this formulation, the reward is determined through a generalized linear function applied to the inner product of the action’s feature tensor and a fixed but unknown parameter tensor with low-rank structure. To effectively achieve the trade-off between exploration and exploitation, we introduce an algorithm called “Generalized Low-Rank Tensor Exploration Subspace then Refine” (G-LowTESTR). This algorithm first collects data to explore the intrinsic low-rank tensor subspace information embedded in the scenario, and then converts the original problem into a lower-dimensional generalized linear contextual bandits problem. Rigorous theoretical analysis shows that the regret bound of G-LowTESTR is superior to those in vectorization and matricization cases. We conduct a series of synthetic and real data experiments to further highlight the effectiveness of G-LowTESTR, leveraging its ability to capitalize on the low-rank tensor structure for enhanced learning.},
  archive      = {J_TKDE},
  author       = {Qianxin Yi and Yiyang Yang and Shaojie Tang and Jiapeng Liu and Yao Wang},
  doi          = {10.1109/TKDE.2024.3469782},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8051-8065},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Effective generalized low-rank tensor contextual bandits},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). E2S2: Encoding-enhanced sequence-to-sequence pretraining
for language understanding and generation. <em>TKDE</em>,
<em>36</em>(12), 8037–8050. (<a
href="https://doi.org/10.1109/TKDE.2023.3341917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequence-to-sequence (seq2seq) learning is a popular fashion for large-scale pretraining language models. However, the previous seq2seq pretraining models generally focus on reconstructive objectives on the decoder side and neglect the effect of encoder-side supervision, which we argue may lead to sub-optimal performance. To verify our hypothesis, we first empirically study the functionalities of the encoder and decoder in seq2seq pretrained language models, and find that the encoder takes an important but under-exploitation role than the decoder regarding the downstream performance and neuron activation. Therefore, we propose an encoding-enhanced seq2seq pretraining strategy, namely E2S2 , which improves the seq2seq models via integrating more efficient self-supervised information into the encoders. Specifically, E2S2 adopts two self-supervised objectives on the encoder side from two aspects: 1) locally denoising the corrupted sentence (denoising objective); and 2) globally learning better sentence representations (contrastive objective). With the help of both objectives, the encoder can effectively distinguish the noise tokens and capture high-level (i.e., syntactic and semantic) knowledge, thus strengthening the ability of seq2seq model to accurately achieve the conditional generation. On a large diversity of downstream natural language understanding and generation tasks, E2S2 dominantly improves the performance of its powerful backbone models, e.g., BART and T5. For example, upon BART backbone, we achieve +1.1% averaged gain on the general language understanding evaluation (GLUE) benchmark and +1.75% $F_{0.5}$ score improvement on CoNLL2014 dataset. We also provide in-depth analyses to show the improvement stems from better linguistic representation. We hope that our work will foster future self-supervision research on seq2seq language model pretraining.},
  archive      = {J_TKDE},
  author       = {Qihuang Zhong and Liang Ding and Juhua Liu and Bo Du and Dacheng Tao},
  doi          = {10.1109/TKDE.2023.3341917},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8037-8050},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {E2S2: Encoding-enhanced sequence-to-sequence pretraining for language understanding and generation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamical targeted ensemble learning for streaming data with
concept drift. <em>TKDE</em>, <em>36</em>(12), 8023–8036. (<a
href="https://doi.org/10.1109/TKDE.2024.3460404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept drift is an important characteristic and inevitable difficult problem in streaming data mining. Ensemble learning is commonly used to deal with concept drift. However, most ensemble methods cannot balance the accuracy and diversity of base learners after drift occurs, and cannot adjust adaptively according to the drift type. To solve these problems, this paper proposes a targeted ensemble learning (Targeted EL) method to improve the accuracy and diversity of ensemble learning for streaming data with abrupt and gradual concept drift. First, to improve the accuracy of the base learners, the method adopts different sample weighting strategies for different types of drift to realize bidirectional transfer of new and old distributed samples. Second, the difference matrix is constructed by the prediction results of the base learners on the current samples. According to the drift type, the submatrix with appropriate size and maximum difference sum is extracted adaptively to select appropriate, accuracy and diverse base learners for ensemble. The experimental results show that the proposed method can achieve good generalization performance when dealing with the streaming data with abrupt and gradual concept drift.},
  archive      = {J_TKDE},
  author       = {Husheng Guo and Yang Zhang and Wenjian Wang},
  doi          = {10.1109/TKDE.2024.3460404},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8023-8036},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dynamical targeted ensemble learning for streaming data with concept drift},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DREAM: Domain-agnostic reverse engineering attributes of
black-box model. <em>TKDE</em>, <em>36</em>(12), 8009–8022. (<a
href="https://doi.org/10.1109/TKDE.2024.3460806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models are usually black boxes when deployed on machine learning platforms. Prior works have shown that the attributes (e.g., the number of convolutional layers) of a target black-box model can be exposed through a sequence of queries. There is a crucial limitation: these works assume the training dataset of the target model is known beforehand and leverage this dataset for model attribute attack. However, it is difficult to access the training dataset of the target black-box model in reality. Therefore, whether the attributes of a target black-box model could be still revealed in this case is doubtful. In this paper, we investigate a new problem of black-box reverse engineering, without requiring the availability of the target model’s training dataset. We put forward a general and principled framework DREAM, by casting this problem as out-of-distribution (OOD) generalization. In this way, we can learn a domain-agnostic meta-model to infer the attributes of the target black-box model with unknown training data. This makes our method one of the kinds that can gracefully apply to an arbitrary domain for model attribute reverse engineering with strong generalization ability. Extensive experimental results demonstrate the superiority of our proposed method over the baselines.},
  archive      = {J_TKDE},
  author       = {Rongqing Li and Jiaqi Yu and Changsheng Li and Wenhan Luo and Ye Yuan and Guoren Wang},
  doi          = {10.1109/TKDE.2024.3460806},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {8009-8022},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DREAM: Domain-agnostic reverse engineering attributes of black-box model},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Discovering the representation bottleneck of graph neural
networks. <em>TKDE</em>, <em>36</em>(12), 7998–8008. (<a
href="https://doi.org/10.1109/TKDE.2024.3446584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) rely mainly on the message-passing paradigm to propagate node features and build interactions, and different graph learning problems require different ranges of node interactions. In this work, we explore the capacity of GNNs to capture node interactions under contexts of different complexities. We discover that GNNs usually fail to capture the most informative kinds of interaction styles for diverse graph learning tasks , and thus name this phenomenon as GNNs’ representation bottleneck. As a response, we demonstrate that the inductive bias introduced by existing graph construction mechanisms can result in this representation bottleneck, i.e., preventing GNNs from learning interactions of the most appropriate complexity. To address that limitation, we propose a novel graph rewiring approach based on interaction patterns learned by GNNs to adjust each node&#39;s receptive fields dynamically. Extensive experiments on both real-world and synthetic datasets prove the effectiveness of our algorithm in alleviating the representation bottleneck and its superiority in enhancing the performance of GNNs over state-of-the-art graph rewiring baselines.},
  archive      = {J_TKDE},
  author       = {Fang Wu and Siyuan Li and Stan Z. Li},
  doi          = {10.1109/TKDE.2024.3446584},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7998-8008},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Discovering the representation bottleneck of graph neural networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DINE: Dimensional interpretability of node embeddings.
<em>TKDE</em>, <em>36</em>(12), 7986–7997. (<a
href="https://doi.org/10.1109/TKDE.2024.3425460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph representation learning methods, such as node embeddings, are powerful approaches to map nodes into a latent vector space, allowing their use for various graph learning tasks. Despite their success, these techniques are inherently black-boxes and few studies have focused on investigating local explanations of node embeddings for specific instances. Moreover, explaining the overall behavior of unsupervised embedding models remains an unexplored problem, limiting global interpretability and debugging potentials. We address this gap by developing human-understandable explanations for latent space dimensions in node embeddings. Towards that, we first develop new metrics that measure the global interpretability of embeddings based on the marginal contribution of the latent dimensions to predicting graph structure. We say an embedding dimension is more interpretable if it can faithfully map to an understandable sub-structure in the input graph - like community structure. Having observed that standard node embeddings have low interpretability, we then introduce Dine (Dimension-based Interpretable Node Embedding). This novel approach can retrofit existing node embeddings by making them more interpretable without sacrificing their task performance. We conduct extensive experiments on synthetic and real-world graphs and show that we can simultaneously learn highly interpretable node embeddings with effective performance in link prediction and node classification.},
  archive      = {J_TKDE},
  author       = {Simone Piaggesi and Megha Khosla and André Panisson and Avishek Anand},
  doi          = {10.1109/TKDE.2024.3425460},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7986-7997},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DINE: Dimensional interpretability of node embeddings},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Digger-guider: High-frequency factor extraction for stock
trend prediction. <em>TKDE</em>, <em>36</em>(12), 7973–7985. (<a
href="https://doi.org/10.1109/TKDE.2024.3424475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed increasing attention being paid to AI-based quantitative investment. Compared to traditional low-frequency data (e.g., daily, weekly), high-frequency data (e.g., minute-level) is often underutilized for low-frequency stock trend prediction, leaving the vast potential for improvement. However, valuable and noisy information coexist in high-frequency data. The learning process of high-frequency factor extractors can easily be overwhelmed by noise, leading to overfitting. Moreover, common techniques used to prevent overfitting often result in poor performance on this task since they usually roughly restrict the model’s capacity, making it challenging to model complex trading signals in high-frequency data. When designing high-frequency factor extractors, we face a tough dilemma. A high-capacity model may easily overfit to noise, while a simple but robust model may not capture complex high-frequency patterns. To address these problems, we propose maintaining model capacity while preventing overfitting by constructing two components that balance information and noise through interactions between them. Specifically, we propose a novel learning framework called Digger-Guider to extract informative stock representations from noisy high-frequency data. We develop a high-capacity model called Digger to extract local and detailed features from the high-frequency data, and we design a robust model called Guider to capture global tendency features and help the Digger overcome the noise. The Digger and Guider enhance each other through mutual distillation during training, serving as data-driven regularizations that work well on this task. Extensive experiments on real-world datasets demonstrate that our framework can produce powerful high-frequency stock factors that significantly improve stock trend prediction performance and our understanding of the finance market.},
  archive      = {J_TKDE},
  author       = {Yang Liu and Chang Xu and Min Hou and Weiqing Liu and Jiang Bian and Qi Liu and Tie-Yan Liu},
  doi          = {10.1109/TKDE.2024.3424475},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7973-7985},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Digger-guider: High-frequency factor extraction for stock trend prediction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diffusion-based graph generative methods. <em>TKDE</em>,
<em>36</em>(12), 7954–7972. (<a
href="https://doi.org/10.1109/TKDE.2024.3466301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Being the most cutting-edge generative methods, diffusion methods have shown great advances in wide generation tasks. Among them, graph generation attracts significant research attention for its broad application in real life. In our survey, we systematically and comprehensively review on diffusion-based graph generative methods. We first make a review on three mainstream paradigms of diffusion methods, which are denoising diffusion probabilistic models, score-based genrative models, and stochastic differential equations. Then we further categorize and introduce the latest applications of diffusion models on graphs. In the end, we point out some limitations of current studies and future directions of future explorations.},
  archive      = {J_TKDE},
  author       = {Hongyang Chen and Can Xu and Lingyu Zheng and Qiang Zhang and Xuemin Lin},
  doi          = {10.1109/TKDE.2024.3466301},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7954-7972},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Diffusion-based graph generative methods},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diff-RNTraj: A structure-aware diffusion model for road
network-constrained trajectory generation. <em>TKDE</em>,
<em>36</em>(12), 7940–7953. (<a
href="https://doi.org/10.1109/TKDE.2024.3460051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory data is essential for various applications. However, publicly available trajectory datasets remain limited in scale due to privacy concerns, which hinders the development of trajectory mining and applications. Although some trajectory generation methods have been proposed to expand dataset scale, they generate trajectories in the geographical coordinate system, posing two limitations for practical applications: 1) failing to ensure that the generated trajectories are road-constrained. 2) lacking road-related information. In this paper, we propose a new problem, road network-constrained trajectory (RNTraj) generation, which can directly generate trajectories on the road network with road-related information. Specifically, RNTraj is a hybrid type of data, in which each point is represented by a discrete road segment and a continuous moving rate. To generate RNTraj, we design a diffusion model called Diff-RNTraj, which can effectively handle the hybrid RNTraj using a continuous diffusion framework by incorporating a pre-training strategy to embed hybrid RNTraj into continuous representations. During the sampling stage, a RNTraj decoder is designed to map the continuous representation generated by the diffusion model back to the hybrid RNTraj format. Furthermore, Diff-RNTraj introduces a novel loss function to enhance trajectory’s spatial validity. Extensive experiments conducted on two datasets demonstrate the effectiveness of Diff-RNTraj.},
  archive      = {J_TKDE},
  author       = {Tonglong Wei and Youfang Lin and Shengnan Guo and Yan Lin and Yiheng Huang and Chenyang Xiang and Yuqing Bai and Huaiyu Wan},
  doi          = {10.1109/TKDE.2024.3460051},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7940-7953},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Diff-RNTraj: A structure-aware diffusion model for road network-constrained trajectory generation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep session heterogeneity-aware network for click through
rate prediction. <em>TKDE</em>, <em>36</em>(12), 7927–7939. (<a
href="https://doi.org/10.1109/TKDE.2024.3421594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {CTR (Click-Through Rate) prediction plays an essential role in online advertising systems. Most existing works attempt to capture users’ interests from sessions by assuming that behaviors within a session are homogeneous. However, user interest may change frequently. Thus it is hard to guarantee that behaviors in a session are homogeneous, resulting in users’ interests extracted from sessions being biased. In this paper, we propose a model named Deep Session Heterogeneity-aware Network (DSHN) by learning the relationships of behaviors within sessions and the relevance between the session and target item to alleviate the influence of irrelevant or heterogeneous sessions. We design a heterogeneity-aware mechanism to learn the heterogeneity of items within a session. Then we further design two modules: the Session Heterogeneity Learning module and the Relevance Inference module. The Session Heterogeneity Learning module weighs each session by summarizing the variation of session interest with and without any behavior. The relevance Inference module learns the relevance between the target item and each session in a similar way by learning session interest with and without the target item. Extensive experiments on four datasets demonstrate that our proposed DSHN achieves better results compared to the state-of-the-art.},
  archive      = {J_TKDE},
  author       = {Xin Zhang and Zengmao Wang and Bo Du and Jia Wu and Xiao Zhang and Erli Meng},
  doi          = {10.1109/TKDE.2024.3421594},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7927-7939},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Deep session heterogeneity-aware network for click through rate prediction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep multi-task learning for spatio-temporal incomplete
qualitative event forecasting. <em>TKDE</em>, <em>36</em>(12),
7913–7926. (<a href="https://doi.org/10.1109/TKDE.2024.3460539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting spatiotemporal social events has significant benefits for society to provide the proper amounts and types of resources to manage catastrophes and any accompanying societal risks. Nevertheless, forecasting event subtypes are far more complex than merely extending binary prediction to cover multiple subtypes because of spatial heterogeneity, experiencing a partial set of event subtypes, subtle discrepancy among different event subtypes, nature of the event subtype, spatial correlation of event subtypes. We present D e e p mul t i-task l e arning for spatio-temporal in c omple t e qual i tative e v ent for e casting (DETECTIVE) framework to effectively forecast the subtypes of future events by addressing all these issues. This formulates spatial locations into tasks to handle spatial heterogeneity in event subtypes and learns a joint deep representation of subtypes across tasks. This has the adaptability to be used for different types of problem formulation required by the nature of the events. Furthermore, based on the “first law of geography”, spatially-closed tasks share similar event subtypes or scale patterns so that adjacent tasks can share knowledge effectively. To optimize the non-convex and strongly coupled problem of the proposed model, we also propose algorithms based on the Alternating Direction Method of Multipliers (ADMM). Extensive experiments on real-world datasets demonstrate the model’s usefulness and efficiency.},
  archive      = {J_TKDE},
  author       = {Tanmoy Chowdhury and Yuyang Gao and Liang Zhao},
  doi          = {10.1109/TKDE.2024.3460539},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7913-7926},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Deep multi-task learning for spatio-temporal incomplete qualitative event forecasting},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning approaches for similarity computation: A
survey. <em>TKDE</em>, <em>36</em>(12), 7893–7912. (<a
href="https://doi.org/10.1109/TKDE.2024.3422484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The requirement for appropriate ways to measure the similarity between data objects is a common but vital task in various domains, such as data mining, machine learning and so on. Driven by abundant real-world applications, many well-known similarity (distance) metrics are proposed to measure the pairwise similarity of data pairs, e.g., graph edit distance for graphs and dynamic time warping for time series. However, many similarity metrics suffer from the high time complexity. More specifically, most of the well-known similarity metrics often need quadratic time or even much more time to compute the ground truth similarity and some of them are proven to be NP-hard. With the development of deep learning techniques, there is an emerging research trend on the learning for similarity computation on various data types in the field of database (DB) and data mining, which is quite different with the metric learning studies in the machine learning (ML) literature. Specifically, the studies in the ML focus on the learning for semantic similarity in specific tasks, which is implicitly indicated by the training data, on the data in the feature space. While the studies in the DB literature usually consider the learning for well-defined similarity metrics (e.g., graph edit distance) on the data objects (e.g., graphs), such that it can benefit the similarity computation on data in terms of multiple aspects, such as computation time, metric quality and search heuristic, and the learned representation of data can also be naturally fed to downstream tasks. This survey paper provides a comprehensive review of similarity computation learning on several data types, including set, sequence and graph. Moreover, we first classify the learning-based approaches in terms of their learning target into three categories, i.e., similarity learning, cost matrix learning and search heuristic learning. Then we detail some representative approaches for each category on every data type, and analyze some key features that are utilized by these approaches. Finally, we discuss some challenges and future directions towards the learning for similarity learning on these data types.},
  archive      = {J_TKDE},
  author       = {Peilun Yang and Hanchen Wang and Jianye Yang and Zhengping Qian and Ying Zhang and Xuemin Lin},
  doi          = {10.1109/TKDE.2024.3422484},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7893-7912},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Deep learning approaches for similarity computation: A survey},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Debiased pairwise learning for implicit collaborative
filtering. <em>TKDE</em>, <em>36</em>(12), 7878–7892. (<a
href="https://doi.org/10.1109/TKDE.2024.3479240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning representations from pairwise comparisons has achieved significant success in various fields, including computer vision and information retrieval. In recommendation systems, collaborative filtering algorithms based on pairwise learning are also rooted in this approach. However, a major challenge in collaborative filtering is the lack of labels for negative instances in implicit feedback data, leading to the inclusion of false negatives among randomly selected instances. This issue causes biased optimization objectives and results in biased parameter estimation. In this paper, we propose a novel method to address learning biases arising from implicit feedback data and introduce a modified loss function for pairwise learning, called debiased pairwise loss (DPL). The core idea of DPL is to correct the biased probability estimates caused by false negatives, thereby adjusting the gradients to more closely approximate those of fully supervised data. Implementing DPL requires only a small modification to the existing codebase. Experimental studies on public datasets demonstrate the effectiveness of the proposed method.},
  archive      = {J_TKDE},
  author       = {Bin Liu and Qin Luo and Bang Wang},
  doi          = {10.1109/TKDE.2024.3479240},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7878-7892},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Debiased pairwise learning for implicit collaborative filtering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-regional fraud detection via continual learning with
knowledge transfer. <em>TKDE</em>, <em>36</em>(12), 7865–7877. (<a
href="https://doi.org/10.1109/TKDE.2024.3451161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fraud detection poses a fundamental yet challenging problem to mitigate various risks associated with fraudulent activities. However, existing methods are limited by their reliance on static data within single geographical regions, thereby restricting the trained model’s adaptability across different regions. Practically, when enterprises expand their business into new cities or countries, training a new model from scratch can incur high computational costs and lead to catastrophic forgetting (CF). To address these limitations, we propose cross-regional fraud detection as an incremental learning problem, enabling the development of a unified model capable of adapting across diverse regions without suffering from CF. Subsequently, we introduce Cross-Regional Continual Learning (CCL), a novel paradigm that facilitates knowledge transfer and maintains performance when incrementally training models from previously learned regions to new ones. Specifically, CCL utilizes prototype-based knowledge replay for effective knowledge transfer while implementing a parameter smoothing mechanism to alleviate forgetting. Furthermore, we construct heterogeneous trade graphs (HTGs) and leverage graph-based backbones to enhance knowledge representation and facilitate knowledge transfer by uncovering intricate semantics inherent in cross-regional datasets. Extensive experiments demonstrate the superiority of our proposed method over baseline approaches and its substantial improvement in cross-regional fraud detection performance.},
  archive      = {J_TKDE},
  author       = {Yujie Li and Xin Yang and Qiang Gao and Hao Wang and Junbo Zhang and Tianrui Li},
  doi          = {10.1109/TKDE.2024.3451161},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7865-7877},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cross-regional fraud detection via continual learning with knowledge transfer},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-feature interactive tabular data modeling with
multiplex graph neural networks. <em>TKDE</em>, <em>36</em>(12),
7851–7864. (<a href="https://doi.org/10.1109/TKDE.2024.3440654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rising popularity of tabular data in data science applications has led to a surge of interest in utilizing deep neural networks (DNNs) to address tabular problems. Existing deep neural network methods are not effective in handling two fundamental challenges that are inherent in tabular data: permutation invariance (where the labels remain unchanged regardless of element order) and local dependency (where predictive labels are solely determined by local features). Furthermore, given the inherent heterogeneity among elements in tabular data, effectively capturing heterogeneous feature interactions remains unresolved. In this paper, we propose a novel Multiplex Cross-Feature Interaction Network (MPCFIN) by explicitly and systematically modeling feature relations with interactive graph neural networks. Specifically, MPCFIN first learns the most relevant features associated with individual features, and merges them to form cross-feature embedding. Subsequently, we design a multiplex graph neural network to learn enhanced representation for each sample. Comprehensive experiments on seven datasets demonstrate that MPCFIN exhibits superior performance over deep neural network methods in modeling the tabular data, showcasing consistent interpretability in its cross-feature embedding module for medical diagnosis applications.},
  archive      = {J_TKDE},
  author       = {Mang Ye and Yi Yu and Ziqin Shen and Wei Yu and Qingyan Zeng},
  doi          = {10.1109/TKDE.2024.3440654},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7851-7864},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cross-feature interactive tabular data modeling with multiplex graph neural networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-domain graph level anomaly detection. <em>TKDE</em>,
<em>36</em>(12), 7839–7850. (<a
href="https://doi.org/10.1109/TKDE.2024.3462442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing graph level anomaly detection methods are predominantly unsupervised due to high costs for obtaining labels, yielding sub-optimal detection accuracy when compared to supervised methods. Moreover, they heavily rely on the assumption that the training data exclusively consists of normal graphs. Hence, even the presence of a few anomalous graphs can lead to substantial performance degradation. To alleviate these problems, we propose a cross-domain graph level anomaly detection method , aiming to identify anomalous graphs from a set of unlabeled graphs ( target domain ) by using easily accessible normal graphs from a different but related domain ( source domain ). Our method consists of four components: a feature extractor that preserves semantic and topological information of individual graphs while incorporating the distance between different graphs; an adversarial domain classifier to make graph level representations domain-invariant; a one-class classifier to exploit label information in the source domain; and a class aligner to align classes from both domains based on pseudolabels. Experiments on seven benchmark datasets show that the proposed method largely outperforms state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Zhong Li and Sheng Liang and Jiayang Shi and Matthijs van Leeuwen},
  doi          = {10.1109/TKDE.2024.3462442},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7839-7850},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cross-domain graph level anomaly detection},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conversational recommendation with online learning and
clustering on misspecified users. <em>TKDE</em>, <em>36</em>(12),
7825–7838. (<a href="https://doi.org/10.1109/TKDE.2024.3423442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the domain of conversational recommendation systems (CRSs), the development of recommenders capable of eliciting user preferences through conversation has marked a significant advancement. These systems have been enhanced by incorporating conversational key-terms related to items, which streamline the recommendation process by reducing the extensive exploration that traditional interactive recommenders necessitate. Despite these advancements, CRSs still face significant challenges. The vast number of users and the difficulty in accurately capturing preferences lead to persistent inaccuracies, even when direct user interactions are employed to refine the understanding of user preferences. To tackle these challenges, we propose two innovative bandit algorithms: RCLUMB (Robust Clustering of Misspecified Bandits) and RSCLUMB (Robust Set-based Clustering of Misspecified Bandits). These algorithms employ dynamic graphs and evolving cluster sets, respectively, to represent the changing structure of user preferences, thus leveraging collaborative user preferences to accelerate the learning process. Our algorithms are designed to be resilient against errors in preference modeling and the resulting inaccuracies in clustering. We rigorously analyze the performance of our algorithms and establish regret upper bounds of $O(\epsilon _*T\sqrt{md\log T} + d\sqrt{mT}\log T)$ under milder assumptions than previous works, matching the state-of-the-art results in several degenerate cases. Through extensive experiments on synthetic and real-world datasets, our algorithms demonstrate superior performance over existing algorithms.},
  archive      = {J_TKDE},
  author       = {Xiangxiang Dai and Zhiyong Wang and Jize Xie and Xutong Liu and John C.S. Lui},
  doi          = {10.1109/TKDE.2024.3423442},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7825-7838},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Conversational recommendation with online learning and clustering on misspecified users},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continual learning for smart city: A survey. <em>TKDE</em>,
<em>36</em>(12), 7805–7824. (<a
href="https://doi.org/10.1109/TKDE.2024.3447123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the digitization of modern cities, large data volumes and powerful computational resources facilitate the rapid update of intelligent models deployed in smart cities. Continual learning (CL) is a novel machine learning paradigm that constantly updates models to adapt to changing environments, where the learning tasks, data, and distributions can vary over time. Our survey provides a comprehensive review of continual learning methods that are widely used in smart city development. The content consists of three parts: 1) Methodology-wise. We categorize a large number of basic CL methods and advanced CL frameworks in combination with other learning paradigms including graph learning, spatial-temporal learning, multi-modal learning, and federated learning. 2) Application-wise. We present numerous CL applications covering transportation, environment, public health, safety, networks, and associated datasets related to urban computing. 3) Challenges. We discuss current problems and challenges and envision several promising research directions. We believe this survey can help relevant researchers quickly familiarize themselves with the current state of continual learning research used in smart city development and direct them to future research trends.},
  archive      = {J_TKDE},
  author       = {Li Yang and Zhipeng Luo and Shiming Zhang and Fei Teng and Tianrui Li},
  doi          = {10.1109/TKDE.2024.3447123},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7805-7824},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Continual learning for smart city: A survey},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparison queries generation using mathematical programming
for exploratory data analysis. <em>TKDE</em>, <em>36</em>(12),
7792–7804. (<a href="https://doi.org/10.1109/TKDE.2024.3474828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploratory Data Analysis (EDA) is the interactive process of gaining insights from a dataset. Comparisons are popular insights that can be specified with comparison queries, i.e., specifications of the comparison of subsets of data. In this work, we consider the problem of automatically computing sequences of comparison queries that are coherent, significant and whose overall cost is bounded. Such an automation is usually done by either generating all insights and solving a multi-criteria optimization problem, or using reinforcement learning. In the first case, a large search space has to be explored using exponential algorithms or dedicated heuristics. In the second case, a dataset-specific, time and energy-consuming training, is necessary. We contribute with a novel approach, consisting of decomposing the optimization problem in two: the original problem, that is solved over a smaller search space, and a new problem of generating comparison queries, aiming at generating only queries improving existing solutions of the first problem. This allows to explore only a portion of the search space, without resorting to reinforcement learning. We show that this approach is effective, in that it finds good solutions to the original multi-criteria optimization problem, and efficient, allowing to generate sequences of comparisons in reasonable time.},
  archive      = {J_TKDE},
  author       = {Alexandre Chanson and Nicolas Labroche and Patrick Marcel and Vincent T&#39;Kindt},
  doi          = {10.1109/TKDE.2024.3474828},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7792-7804},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Comparison queries generation using mathematical programming for exploratory data analysis},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cloud-native databases: A survey. <em>TKDE</em>,
<em>36</em>(12), 7772–7791. (<a
href="https://doi.org/10.1109/TKDE.2024.3397508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud databases have been widely accepted and deployed due to their unique advantages, such as high elasticity, high availability, and low cost. Many new techniques, such as compute-storage disaggregation and the log is the database, have been proposed recently to seek for higher elasticity and lower cost. To better harness the power of cloud databases, it is crucial to study and compare the pros and cons of their key techniques. In this paper, we offer a comprehensive survey of cloud-native databases. Particularly, we investigate and summarize the state-of-the-art cloud-native OLTP and OLAP databases, respectively. In the first part, we discuss three types of architectures of cloud-native OLTP database. Then we introduce their key techniques including data placement strategy, storage layer consistency, compute layer consistency, multi-layer recovery, and HTAP optimization. In the second part, we present two kinds of architectures of cloud-native OLAP databases. Then we take a deep dive into their key techniques regarding storage management, query processing, serverless computing, data protection, and machine learning in databases. Finally, we discuss the research challenges and opportunities.},
  archive      = {J_TKDE},
  author       = {Haowen Dong and Chao Zhang and Guoliang Li and Huanchen Zhang},
  doi          = {10.1109/TKDE.2024.3397508},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7772-7791},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cloud-native databases: A survey},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Classification with trust: A supervised approach based on
sequential ellipsoidal partitioning. <em>TKDE</em>, <em>36</em>(12),
7757–7771. (<a href="https://doi.org/10.1109/TKDE.2023.3345658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard metrics of performance of classifiers, such as accuracy and sensitivity, do not reveal the trust or confidence in the predicted labels of data. While other metrics such as the computed probability of a label or the signed distance from a hyperplane can act as a trust measure, these are subjected to heuristic thresholds. This paper presents a convex optimization-based supervised classifier that sequentially partitions a dataset into several ellipsoids, where each ellipsoid contains nearly all points of the same label. By stating classification rules based on this partitioning, Bayes’ formula is then applied to calculate a trust score to a label assigned to a test datapoint determined from these rules. The proposed Sequential Ellipsoidal Partitioning Classifier (SEP-C) exposes dataset irregularities, such as degree of overlap, without requiring a separate exploratory data analysis. The rules of classification, which are free of hyperparameters, are also not affected by class-imbalance, the underlying data distribution, or number of features. SEP-C does not require the use of non-linear kernels when the dataset is not linearly separable. The performance, and comparison with other methods, of SEP-C is demonstrated on the XOR-problem, circle dataset, and other open-source datasets.},
  archive      = {J_TKDE},
  author       = {Ranjani Niranjan and Sachit Rao},
  doi          = {10.1109/TKDE.2023.3345658},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7757-7771},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Classification with trust: A supervised approach based on sequential ellipsoidal partitioning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Change point detection in multi-channel time series via a
time-invariant representation. <em>TKDE</em>, <em>36</em>(12),
7743–7756. (<a href="https://doi.org/10.1109/TKDE.2023.3347356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change Point Detection (CPD) refers to the task of identifying abrupt changes in the characteristics or statistics of time series data. Recent advancements have led to a shift away from traditional model-based CPD approaches, which rely on predefined statistical distributions, toward neural network-based and distribution-free methods using autoencoders. However, many state-of-the-art methods in this category often neglect to explicitly leverage spatial information across multiple channels, making them less effective at detecting changes in cross-channel statistics. In this paper, we introduce an unsupervised, distribution-free CPD method that explicitly incorporates both temporal and spatial (cross-channel) information in multi-channel time series data based on the so-called Time-Invariant Representation (TIRE) autoencoder. Our evaluation, conducted on both simulated and real-life datasets, illustrates the significant advantages of our proposed multi-channel TIRE (MC-TIRE) method, which consistently delivers more accurate CPD results.},
  archive      = {J_TKDE},
  author       = {Zhenxiang Cao and Nick Seeuws and Maarten De Vos and Alexander Bertrand},
  doi          = {10.1109/TKDE.2023.3347356},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7743-7756},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Change point detection in multi-channel time series via a time-invariant representation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal discovery from unknown interventional datasets over
overlapping variable sets. <em>TKDE</em>, <em>36</em>(12), 7725–7742.
(<a href="https://doi.org/10.1109/TKDE.2024.3443997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring causal structures from experimentation is a challenging task in many fields. Most causal structure learning algorithms with unknown interventions are proposed to discover causal relationships over an identical variable set. However, often due to privacy, ethical, financial, and practical concerns, the variable sets observed by multiple sources or domains are not entirely identical. While a few algorithms are proposed to handle the partially overlapping variable sets, they focus on the case of known intervention targets. Therefore, to be close to the real-world environment, we consider discovering causal relationships over overlapping variable sets under the unknown intervention setting and exploring a scenario where a problem is studied across multiple domains. Here, we propose an algorithm for discovering the causal relationships over the integrated set of variables from unknown interventions, mainly handling the entangled inconsistencies caused by the incomplete observation of variables and unknown intervention targets. Specifically, we first distinguish two types of inconsistencies and then deal with respectively them by presenting some lemmas. Finally, we construct a fusion rule to combine learned structures of multiple domains, obtaining the final structures over the integrated set of variables. Theoretical analysis and experimental results on synthetic, benchmark, and real-world datasets have verified the effectiveness of the proposed algorithm.},
  archive      = {J_TKDE},
  author       = {Fuyuan Cao and Yunxia Wang and Kui Yu and Jiye Liang},
  doi          = {10.1109/TKDE.2024.3443997},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7725-7742},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Causal discovery from unknown interventional datasets over overlapping variable sets},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). C2F-explainer: Explaining transformers better through a
coarse-to-fine strategy. <em>TKDE</em>, <em>36</em>(12), 7708–7724. (<a
href="https://doi.org/10.1109/TKDE.2024.3443888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer interpretability research is a hot topic in the area of deep learning. Traditional interpretation methods mostly use the final layer output of the Transformer encoder as masks to generate an explanation map. However, These approaches overlook two crucial aspects. At the coarse-grained level, the mask may contain uncertain information, including unreliable and incomplete object location data; at the fine-grained level, there is information loss on the mask, resulting in spatial noise and detail loss. To address these issues, in this paper, we propose a two-stage coarse-to-fine strategy (C2F-Explainer) for improving Transformer interpretability. Specifically, we first design a sequential three-way mask (S3WM) module to handle the problem of uncertain information at the coarse-grained level. This module uses sequential three-way decisions to process the mask, preventing uncertain information on the mask from impacting the interpretation results, thus obtaining coarse-grained interpretation results with accurate position. Second, to further reduce the impact of information loss at the fine-grained level, we devised an attention fusion (AF) module inspired by the fact that self-attention can capture global semantic information, AF aggregates the attention matrix to generate a cross-layer relation matrix, which is then used to optimize detailed information on the interpretation results and produce fine-grained interpretation results with clear and complete edges. Experimental results show that the proposed C2F-Explainer has good interpretation results on both natural and medical image datasets, and the mIoU is improved by 2.08% on the PASCAL VOC 2012 dataset.},
  archive      = {J_TKDE},
  author       = {Weiping Ding and Xiaotian Cheng and Yu Geng and Jiashuang Huang and Hengrong Ju},
  doi          = {10.1109/TKDE.2024.3443888},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7708-7724},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {C2F-explainer: Explaining transformers better through a coarse-to-fine strategy},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Budget-constrained ego network extraction with maximized
willingness. <em>TKDE</em>, <em>36</em>(12), 7692–7707. (<a
href="https://doi.org/10.1109/TKDE.2024.3446169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many large-scale machine learning approaches and graph algorithms are proposed recently to address a variety of problems in online social networks (OSNs). To evaluate and validate these algorithms and models, the data of ego-centric networks (ego networks) are widely adopted. Therefore, effectively extracting large-scale ego networks from OSNs becomes an important issue, particularly when privacy policies become increasingly strict nowadays. In this paper, we study the problem of extracting ego network data by considering jointly the user willingness, crawling cost, and structure of the network. We formulate a new research problem, named Structure and Willingness Aware Ego Network Extraction (SWAN) and analyze its NP-hardness. We first propose a $(1-\frac{1}{e})$ -approximation algorithm, named Tristar-Optimized Ego Network Identification with Maximum Willingness (TOMW) . In addition to the deterministic approximation algorithm, we also propose to automatically learn an effective heuristic approach with machine learning, to avoid the huge efforts for human to devise a good algorithm. The learning approach is named Willingness-maximized and Structure-aware Ego Network Extraction with Reinforcement Learning (WSRL) , in which we propose a novel constrastive learning strategy, named Contrastive Learning with Performance-boosting Graph Augmentation . We recruited 1,810 real-world participants and conducted an evaluation study to validate our problem formulation and proposed approaches. Moreover, experimental results on real social network datasets show that the proposed approaches outperform the other baselines significantly.},
  archive      = {J_TKDE},
  author       = {Bay-Yuan Hsu and Chia-Hsun Lu and Ming-Yi Chang and Chih-Ying Tseng and Chih-Ya Shen},
  doi          = {10.1109/TKDE.2024.3446169},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7692-7707},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Budget-constrained ego network extraction with maximized willingness},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). BigSet: An efficient set intersection approach.
<em>TKDE</em>, <em>36</em>(12), 7677–7691. (<a
href="https://doi.org/10.1109/TKDE.2024.3432595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Set intersection is a fundamental operation in many applications, such as common neighbor computation in graph-based algorithms, set similarity computation, item recommendation, etc. In the literature, many set intersection methods are proposed. We observe that the state-of-the-art algorithm ${\sf RCode}$ bears several limitations, such as high index time complexity, inefficient for large-sized sets, and not friendly to the generic set intersection. In this paper, we introduce the B ucket S ig nature for Set ( ${\sf BigSet}$ ), an efficient generic set intersection algorithm. ${\sf BigSet}$ consists of two phases, namely the preprocessing phase and the query phase. In the preprocessing phase, ${\sf BigSet}$ partitions the elements of a record into $O(2^{k})$ buckets and uses a bitmap to indicate the status of the buckets where $n$ is the record length and $k$ is the number of bits in the signature. In the query phase, ${\sf BigSet}$ calculates the results using a candidate generating-and-verification framework. Specifically, a set of candidate elements is identified as those falling in the same buckets. Then, for each bucket, ${\sf BigSet}$ collects the common elements using a merge-based method. To improve the performance, we introduce two optimizations, including bucket sharing and size-aware signature construction techniques. We conduct experiments on 10 real graph datasets and 5 real generic set datasets to evaluate the performance of our proposals. The experiment results show that ${\sf BigSet}$ is 20× faster than the leading generic set intersection algorithms. Besides it outperforms the ${\sf RCode}$ with 5× speedup, and while uses up to 8× less memory.},
  archive      = {J_TKDE},
  author       = {Shiding Zhang and Jianye Yang and Wenjie Zhang and Shiyu Yang and Ying Zhang and Xuemin Lin},
  doi          = {10.1109/TKDE.2024.3432595},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7677-7691},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {BigSet: An efficient set intersection approach},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic index tuning: A survey. <em>TKDE</em>,
<em>36</em>(12), 7657–7676. (<a
href="https://doi.org/10.1109/TKDE.2024.3422006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Index tuning plays a crucial role in facilitating the efficiency of data retrieval within database systems, which adjusts index settings to optimize the database performance. Recently, with the growth of data volumes, the complexity of workloads, and the diversification of database applications, various Automatic Index Tuning (AIT) methods have been proposed to address these challenges. In this paper, we provide a comprehensive survey on Automatic Index Tuning. First, we overview the AIT techniques from multiple aspects, including i) problem definition, ii) workflow, iii) framework, iv) index types, v) index interaction, vi) changing factors, vii) automation level, and show the development history. Second, we summarize techniques in the main modules of AIT, including preprocessing , index benefit estimation , and index selection . Preprocessing involves workload compression, index candidate generation, feature representation of workloads and databases, and workload reduction. Index benefit estimation approaches are categorized into empirical methods and machine learning based methods. Index selection involves algorithms of offline AIT and online AIT. Moreover, we summarize the commonly-used datasets in AIT and discuss the applications of index tuning in commercial and opensource database products. Finally, we outline potential future research directions. Our survey aims to enhance both general knowledge and in-depth insights into AIT, and inspire researchers to address the ongoing challenges.},
  archive      = {J_TKDE},
  author       = {Yang Wu and Xuanhe Zhou and Yong Zhang and Guoliang Li},
  doi          = {10.1109/TKDE.2024.3422006},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7657-7676},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Automatic index tuning: A survey},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An balanced, and scalable graph-based multiview clustering
method. <em>TKDE</em>, <em>36</em>(12), 7643–7656. (<a
href="https://doi.org/10.1109/TKDE.2024.3443534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, graph-based multiview clustering methods have become a research hotspot in the clustering field. However, most existing methods lack consideration of cluster balance in their results. In fact, cluster balance is crucial in many real-world scenarios. Additionally, graph-based multiview clustering methods often suffer from high time consumption and cannot handle large-scale datasets. To address these issues, this paper proposes a novel graph-based multiview clustering method. The method is built upon the bipartite graph. Specifically, it employs a label propagation mechanism to update the smaller anchor label matrix rather than the sample label matrix, significantly reducing the computational cost. The introduced balance constraint in the proposed model contributes to achieving balanced clustering results. The entire clustering model combines information from multiple views through graph fusion. The joint graph and view weight parameters in the model are obtained through task-driven self-supervised learning. Moreover, the model can directly obtain clustering results without the need for the two-stage processing typically used in general spectral clustering. Finally, extensive experiments on toy datasets and real-world datasets are conducted to validate the superiority of the proposed method in terms of clustering performance, clustering balance, and time expenditure.},
  archive      = {J_TKDE},
  author       = {Zihua Zhao and Feiping Nie and Rong Wang and Zheng Wang and Xuelong Li},
  doi          = {10.1109/TKDE.2024.3443534},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7643-7656},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {An balanced, and scalable graph-based multiview clustering method},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). AME-LSIFT: Attention-aware multi-label ensemble with label
subset-SpecIfic FeaTures. <em>TKDE</em>, <em>36</em>(12), 7627–7642. (<a
href="https://doi.org/10.1109/TKDE.2024.3447878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label ensemble can achieve superior performance on multi-label learning problems by integrating a number of base classifiers. In existing multi-label ensemble methods, the base classifiers are usually trained with the same original features; it is difficult for each base classifier to capture label-relevant or label subset-relevant information. Meanwhile, the manually designed integrating strategies cannot automatically distinguish the importance of the base classifiers, which also lack flexibility and scalability. In order to resolve these problems, this paper proposes a new multi-label ensemble framework, named Attention-aware Multi-label Ensemble with Label Subset-specIfic FeaTures (AME-LSIFT). It utilizes $c$ -means clustering to produce Label Subset-specIfic FeaTures (LSIFT), constructs a neural network based model for each label subset, and integrates the base models with a dynamic and automatic attention-aware mechanism. Moreover, an objective function that considers both the label subset accuracy and ensemble accuracy is developed for training the proposed AME-LSIFT. Experiments conducted on ten benchmark datasets demonstrate the superior performance of the proposed method compared with state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Xinyin Zhang and Ran Wang and Shuyue Chen and Yuheng Jia and Debby D. Wang},
  doi          = {10.1109/TKDE.2024.3447878},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7627-7642},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {AME-LSIFT: Attention-aware multi-label ensemble with label subset-SpecIfic FeaTures},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial graph neural network for multivariate time
series anomaly detection. <em>TKDE</em>, <em>36</em>(12), 7612–7626. (<a
href="https://doi.org/10.1109/TKDE.2024.3419891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is one of the most significant tasks in multivariate time series analysis, while it remains challenging to model complex patterns for improving detection accuracy and to interpret the root causes of anomalies. However, existing studies either consider only the temporal dependencies, or simply reconstruct the original input for detection, both neglecting the hidden relationships among multivariate. We propose an adversarial graph neural network based anomaly detection model, called SGAT-AE, which consists of a S elf-learning G raph AT tention network (SGAT), an A uto- E ncoder (AE), and an adversarial training component. Specifically, SGAT is a prediction model that discovers the graph dependency relationships among multivariate and acts as a sample generator to confuse AE, while AE reconstructs the samples and acts as a discriminator that distinguishes a real sample from a generated one. A novel adversarial training between SGAT and AE is applied to amplify the errors of anomalies such that the prediction performance of SGAT is improved and the overfitting of AE is avoided. In addition, we aggregate the prediction error, the reconstruction error, and the adversarial error for anomaly detection, and develop a graph based anomaly interpretation method that locates the root causes from both local and global perspectives. Extensive experiments with five real-world data offer evidence that the proposed solution SGAT-AE is capable of achieving better performance when compared with the state-of-the-art proposals.},
  archive      = {J_TKDE},
  author       = {Bolong Zheng and Lingfeng Ming and Kai Zeng and Mengtao Zhou and Xinyong Zhang and Tao Ye and Bin Yang and Xiaofang Zhou and Christian S. Jensen},
  doi          = {10.1109/TKDE.2024.3419891},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7612-7626},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adversarial graph neural network for multivariate time series anomaly detection},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial attack and defense on discrete time dynamic
graphs. <em>TKDE</em>, <em>36</em>(12), 7600–7611. (<a
href="https://doi.org/10.1109/TKDE.2024.3438238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph learning methods have achieved remarkable performance in various domains such as social recommendation, financial fraud detection, and so on. In real applications, the underlying graph is often dynamically evolving and thus, some recent studies focus on integrating the temporal topology information of graphs into the GNN for learning graph embedding. However, the robustness of training GNNs for dynamic graphs has not been discussed so far. The major reason is how to attack dynamic graph embedding still remains largely untouched, let alone how to defend against the attacks. To enable robust training of GNNs for dynamic graphs, in this paper, we investigate the problem of how to generate attacks and defend against attacks for dynamic graph embedding. Attacking dynamic graph embedding is more challenging than attacking static graph embedding as we need to understand the temporal dynamics of graphs as well as its impact on the embedding and the injected perturbations should be distinguished from the natural evolution. In addition, the defense is very challenging as the perturbations may be hidden within the natural evolution. To tackle these technical challenges, in this paper, we first develop a novel gradient-based attack method from an optimization perspective to generate perturbations to fool dynamic graph learning methods, where a key idea is to use gradient dynamics to attack the natural dynamics of the graph. Further, we borrow the idea of the attack method and integrate it with adversarial training to train a more robust dynamic graph learning method to defend against hand-crafted attacks. Finally, extensive experiments on two real-world datasets demonstrate the effectiveness of the proposed attack and defense method, where our defense method not only achieves comparable performance on clean graphs but also significantly increases the defense performance on attacked graphs.},
  archive      = {J_TKDE},
  author       = {Ziwei Zhao and Yu Yang and Zikai Yin and Tong Xu and Xi Zhu and Fake Lin and Xueying Li and Enhong Chen},
  doi          = {10.1109/TKDE.2024.3438238},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7600-7611},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adversarial attack and defense on discrete time dynamic graphs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AdaBoost-stacking based on incremental broad learning
system. <em>TKDE</em>, <em>36</em>(12), 7585–7599. (<a
href="https://doi.org/10.1109/TKDE.2024.3433587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the advantages of fast training speed and competitive performance, Broad Learning System (BLS) has been widely used for classification tasks across various domains. However, the random weight generation mechanism in BLS makes the model unstable, and the performance of BLS may be limited when dealing with some complex datasets. On the other hand, the instability of BLS brings diversity to ensemble learning, and ensemble methods can also reduce the variance and bias of the single BLS. Therefore, we propose an ensemble learning algorithm based on BLS, which includes three modules. To improve the stability and generalization ability of BLS, we utilize BLS as the base classifier in an AdaBoost framework first. Taking advantage of the incremental learning mechanism of BLS, we then propose a selective ensemble method to raise the accuracy and diversity of the BLS ensemble method. In addition, based on the former selective Adaboost framework, we suggest a hierarchical ensemble algorithm, which combines sample and feature dimensions to further improve the fitting ability of the ensemble BLS. Extensive experiments have demonstrated that the proposed method performs better than the original BLS and other state-of-the-art models, proving the effectiveness and versatility of our proposed approaches.},
  archive      = {J_TKDE},
  author       = {Fan Yun and Zhiwen Yu and Kaixiang Yang and C. L. Philip Chen},
  doi          = {10.1109/TKDE.2024.3433587},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7585-7599},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {AdaBoost-stacking based on incremental broad learning system},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Acquiring new knowledge without losing old ones for
effective continual dialogue policy learning. <em>TKDE</em>,
<em>36</em>(12), 7569–7584. (<a
href="https://doi.org/10.1109/TKDE.2023.3344727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dialogue policy learning is the core decision-making module of a task-oriented dialogue system. Its primary objective is to assist users to achieve their goals effectively in as few turns as possible. A practical dialogue-policy agent must be able to expand its knowledge to handle new scenarios efficiently without affecting its performance. Nevertheless, when adapting to new tasks, existing dialogue-policy agents often fail to retain their existing (old) knowledge. To overcome this predicament, we propose a novel continual dialogue-policy model which tackles the issues of “not forgetting the old” and “acquiring the new” from three different aspects: (1) For effective old-task preservation, we introduce the forgetting preventor which uses a behavior cloning technique to force the agent to take actions consistent with the replayed experience to retain the policy trained on historic tasks. (2) For new-task acquisition, we introduce the adaption accelerator which employs an invariant risk minimization mechanism to produce a stable policy predictor to avoid spurious corrections in training data. (3) For reducing the storage cost of the replayed experience, we introduce a replay manager which helps regularly clean up the old data. The effectiveness of the proposed model is evaluated both theoretically and experimentally and demonstrated favorable results.},
  archive      = {J_TKDE},
  author       = {Huimin Wang and Yunyan Zhang and Yifan Yang and Yefeng Zheng and Kam-Fai Wong},
  doi          = {10.1109/TKDE.2023.3344727},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7569-7584},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Acquiring new knowledge without losing old ones for effective continual dialogue policy learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accurate and scalable graph convolutional networks for
recommendation based on subgraph propagation. <em>TKDE</em>,
<em>36</em>(12), 7556–7568. (<a
href="https://doi.org/10.1109/TKDE.2024.3467333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recommendation systems, Graph Convolutional Networks (GCNs) often suffer from significant computational and memory cost when propagating features across the entire user-item graph. While various sampling strategies have been introduced to reduce the cost, the challenge of neighbor explosion persists, primarily due to the iterative nature of neighbor aggregation. This work focuses on exploring subgraph propagation for scalable recommendation by addressing two primary challenges: efficient and effective subgraph construction and subgraph sparsity . To address these challenges, we propose a novel GCN model for recommendation based on Sub graph propagation, called SubGCN. One key component of SubGCN is BiPPR, a technique that fuses both source- and target-based Personalized PageRank (PPR) approximations, to overcome the challenge of efficient and effective subgraph construction . Furthermore, we propose a source-target contrastive learning scheme to mitigate the impact of subgraph sparsity for SubGCN. We conduct extensive experiments on two large and two medium-sized datasets to evaluate the scalability, efficiency, and effectiveness of SubGCN. On medium-sized datasets, compared to full-graph GCNs, SubGCN achieves competitive accuracy while using only 23.79% training time on Gowalla and 16.3% on Yelp2018. On large datasets, where full-graph GCNs ran out of the GPU memory, our proposed SubGCN outperforms widely used sampling strategies in terms of training efficiency and recommendation accuracy.},
  archive      = {J_TKDE},
  author       = {Xueqi Li and Guoqing Xiao and Yuedan Chen and Kenli Li and Gao Cong},
  doi          = {10.1109/TKDE.2024.3467333},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7556-7568},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Accurate and scalable graph convolutional networks for recommendation based on subgraph propagation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on time-series pre-trained models. <em>TKDE</em>,
<em>36</em>(12), 7536–7555. (<a
href="https://doi.org/10.1109/TKDE.2024.3475809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-Series Mining (TSM) is an important research area since it shows great potential in practical applications. Deep learning models that rely on massive labeled data have been utilized for TSM successfully. However, constructing a large-scale well-labeled dataset is difficult due to data annotation costs. Recently, pre-trained models have gradually attracted attention in the time series domain due to their remarkable performance in computer vision and natural language processing. In this survey, we provide a comprehensive review of Time-Series Pre-Trained Models (TS-PTMs), aiming to guide the understanding, applying, and studying TS-PTMs. Specifically, we first briefly introduce the typical deep learning models employed in TSM. Then, we give an overview of TS-PTMs according to the pre-training techniques. The main categories we explore include supervised, unsupervised, and self-supervised TS-PTMs. Further, extensive experiments involving 27 methods, 434 datasets, and 679 transfer learning scenarios are conducted to analyze the advantages and disadvantages of transfer learning strategies, Transformer-based models, and representative TS-PTMs. Finally, we point out some potential directions of TS-PTMs for future work.},
  archive      = {J_TKDE},
  author       = {Qianli Ma and Zhen Liu and Zhenjing Zheng and Ziyang Huang and Siying Zhu and Zhongzhong Yu and James T. Kwok},
  doi          = {10.1109/TKDE.2024.3475809},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7536-7555},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey on time-series pre-trained models},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on service route and time prediction in instant
delivery: Taxonomy, progress, and prospects. <em>TKDE</em>,
<em>36</em>(12), 7516–7535. (<a
href="https://doi.org/10.1109/TKDE.2024.3441309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instant delivery services, such as food delivery and package delivery, have achieved explosive growth in recent years by providing customers with daily-life convenience. An emerging research area within these services is service Route&amp;Time Prediction (RTP), which aims to estimate the future service route as well as the arrival time of a given worker. As one of the most crucial tasks in those service platforms, RTP stands central to enhancing user satisfaction and trimming operational expenditures on these platforms. Despite a plethora of algorithms developed to date, there is no systematic, comprehensive survey to guide researchers in this domain. To fill this gap, our work presents the first comprehensive survey that methodically categorizes recent advances in service route and time prediction. We start by defining the RTP challenge and then delve into the metrics that are often employed. Following that, we scrutinize the existing RTP methodologies, presenting a novel taxonomy of them. We categorize these methods based on three criteria: (i) type of task, subdivided into only-route prediction, only-time prediction, and joint route&amp;time prediction; (ii) model architecture, which encompasses sequence-based and graph-based models; and (iii) learning paradigm, including Supervised Learning (SL) and Deep Reinforcement Learning (DRL). Conclusively, we highlight the limitations of current research and suggest prospective avenues. We believe that the taxonomy, progress, and prospects introduced in this paper can significantly promote the development of this field.},
  archive      = {J_TKDE},
  author       = {Haomin Wen and Youfang Lin and Lixia Wu and Xiaowei Mao and Tianyue Cai and Yunfeng Hou and Shengnan Guo and Yuxuan Liang and Guangyin Jin and Yiji Zhao and Roger Zimmermann and Jieping Ye and Huaiyu Wan},
  doi          = {10.1109/TKDE.2024.3441309},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7516-7535},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey on service route and time prediction in instant delivery: Taxonomy, progress, and prospects},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on privacy in graph neural networks: Attacks,
preservation, and applications. <em>TKDE</em>, <em>36</em>(12),
7497–7515. (<a href="https://doi.org/10.1109/TKDE.2024.3454328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have gained significant attention owing to their ability to handle graph-structured data and the improvement in practical applications. However, many of these models prioritize high utility performance, such as accuracy, with a lack of privacy consideration, which is a major concern in modern society where privacy attacks are rampant. To address this issue, researchers have started to develop privacy-preserving GNNs. Despite this progress, there is a lack of a comprehensive overview of the attacks and the techniques for preserving privacy in the graph domain. In this survey, we aim to address this gap by summarizing the attacks on graph data according to the targeted information, categorizing the privacy preservation techniques in GNNs, and reviewing the datasets and applications that could be used for analyzing/solving privacy issues in GNNs. We also outline potential directions for future research in order to build better privacy-preserving GNNs.},
  archive      = {J_TKDE},
  author       = {Yi Zhang and Yuying Zhao and Zhaoqing Li and Xueqi Cheng and Yu Wang and Olivera Kotevska and Philip S. Yu and Tyler Derr},
  doi          = {10.1109/TKDE.2024.3454328},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7497-7515},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey on privacy in graph neural networks: Attacks, preservation, and applications},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dynamic analysis-powered explanation framework for malware
detection. <em>TKDE</em>, <em>36</em>(12), 7483–7496. (<a
href="https://doi.org/10.1109/TKDE.2024.3436891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been widely adopted in Android malicious software (malware) detection. However, poor explanation in deep learning-based detection models severely undermines user trusts and poses a significant obstacle to their practical promotion in critical security domains. Some studies strive to uncover the rationale behind a model&#39;s decision. Unfortunately, these efforts are often hindered by the limitations of feature extraction methods, such as primarily relying on static analysis to derive separate and approximate behavioral descriptions of applications (apps). As a result, establishing a reliable interpretation for deep learning-based malware detection models remains an open issue. In this work, we propose a novel framework XDeepMal to interpret deep learning-based malware detection models. Specifically, in XDeepMal, we formulate a dynamic analysis tool XTracer + to capture runtime behaviors of apps and automatically generate their continuous behavior trajectories. Then, we propose a novel interpreter to pinpoint certainty behavior fragments that are crucial for deep learning models to make their decisions. This approach regards the identification of the most critical fragments as an optimization problem and leverages heuristic algorithms for implementation. We conduct extensive experiments on a real-world dataset to investigate the effectiveness and reliability of XDeepMal. These experiments cover intuitive case studies (malware family and individual app) and in-depth quantitative analysis. Additionally, we evaluate its coverage and efficiency. Our experimental results demonstrate that XDeepMal is capable of generating convincing interpretations for deep learning (e.g., Transformer) based models within feasible inference time, which greatly benefits security analysts in accurately comprehending why an app is identified as malware by deep learning-based detection models.},
  archive      = {J_TKDE},
  author       = {Huijuan Zhu and Xilong Chen and Liangmin Wang and Zhicheng Xu and Victor S. Sheng},
  doi          = {10.1109/TKDE.2024.3436891},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7483-7496},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A dynamic analysis-powered explanation framework for malware detection},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A derivative topic dissemination model based on
representation learning and topic relevance. <em>TKDE</em>,
<em>36</em>(12), 7468–7482. (<a
href="https://doi.org/10.1109/TKDE.2024.3484496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In social networks, topics often demonstrate a “fission” trend, where new topics arise from existing ones. Effectively predicting collective behavioral patterns during the dissemination of derivative topics is crucial for public opinion management. Addressing the symbiotic, antagonistic nature of “native-derived” topics, a derivative topic propagation model based on representation learning, topic relevance is proposed herein. First, considering the transition in user interest levels, cognitive accumulation at different evolutionary stages of native-derivative topics, a user content representation method, namely DTR2vec, is introduced, based on topic-related feature associations, for learning user content features. Then, evolutionary game theory is introduced by recognizing the symbiotic, antagonistic nature of “native-derived” topics during their propagation. Moreover, implicit relationships between users are explored, user influence is quantified for learning user structural features. Finally, considering the graph convolutional network’s ability to process non-euclidean structured data, the proposed model integrates user content, structural features to predict user forwarding behavior. Experimental results indicate that the proposed model not only effectively predicts the dissemination trends of derivative topics but also more authentically reflects the association, game relationships between native, derivative topics during their dissemination.},
  archive      = {J_TKDE},
  author       = {Qian Li and Yunpeng Xiao and Xinming Zhou and Rong Wang and Sirui Duan and Xiang Yu},
  doi          = {10.1109/TKDE.2024.3484496},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7468-7482},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A derivative topic dissemination model based on representation learning and topic relevance},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A data-driven three-stage adaptive pattern mining approach
for multi-energy loads. <em>TKDE</em>, <em>36</em>(12), 7455–7467. (<a
href="https://doi.org/10.1109/TKDE.2024.3462770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-depth understanding of the multi-energy consumption behavior pattern is the essential to improve the management of multi-energy system (MES). This paper proposes a data-driven three-stage adaptive pattern mining approach for multi-energy loads, which addresses the issues of complex multi-dimensional time-series mining, uncommon daily loads discovery, typical load classification and parameter setting requiring user intervention. In the first stage, the relative state changes over time between different energy loads are excavated based on Autoplait, which realizes time pattern discovery, segmentation and match for multi-dimensional loads. In the second stage, adaptive affinity propagation (AAP) considering trend similarity distance (TSD) is proposed to classify loads into common and uncommon clusters, where uncommon loads are eliminated and daily pattern is obtained by taking average of common loads. In the third stage, AAP with windows dynamic time warping (WDTW) identifies various profiles to obtain typical pattern of daily loads. Specifically, pattern mining provides the key information of multi-energy loads, which is significant to the applications for the demand side, such as load scene compression, load forecasting and demand response analysis. A case study uses MES data from Arizona State University to verify the effectiveness and practicality of the proposed approach.},
  archive      = {J_TKDE},
  author       = {Yixiu Guo and Yong Li and Sisi Zhou and Zhenyu Zhang and Zuyi Li and Mohammad Shahidehpour},
  doi          = {10.1109/TKDE.2024.3462770},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7455-7467},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A data-driven three-stage adaptive pattern mining approach for multi-energy loads},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A bidirectional extraction-then-evaluation framework for
complex relation extraction. <em>TKDE</em>, <em>36</em>(12), 7442–7454.
(<a href="https://doi.org/10.1109/TKDE.2024.3435765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relation extraction is an important task in the field of natural language processing. Previous works mainly focus on adopting pipeline methods or joint methods to model relation extraction in general scenarios. However, these existing methods face challenges when adapting to complex relation extraction scenarios, such as handling overlapped triplets, multiple triplets, and cross-sentence triplets. In this paper, we revisit the advantages and disadvantages of the aforementioned methods in complex relation extraction. Based on the in-depth analysis, we propose a novel two-stage bidirectional extract-then-evaluate framework named BeeRe . In the extraction stage, we first obtain the subject set, relation set, and object set. Then, we design subject- and object-oriented triplet extractors to iteratively recurrent obtain candidate triplets, ensuring high recall. In the evaluation stage, we adopt a relation-oriented triplet filter to determine subject-object pairs based on relations in triplets obtained in the first stage, ensuring high precision. We conduct extensive experiments on three public datasets to show that BeeRe achieves state-of-the-art performance in both complex and general relation extraction scenarios. Even when compared to large language models like closed-source/open-source LLMs, BeeRe still has significant performance gains.},
  archive      = {J_TKDE},
  author       = {Weiyan Zhang and Jiacheng Wang and Chuang Chen and Wanpeng Lu and Wen Du and Haofen Wang and Jingping Liu and Tong Ruan},
  doi          = {10.1109/TKDE.2024.3435765},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7442-7454},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A bidirectional extraction-then-evaluation framework for complex relation extraction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Managing metaverse data tsunami: Actionable insights.
<em>TKDE</em>, <em>36</em>(12), 7423–7441. (<a
href="https://doi.org/10.1109/TKDE.2024.3354960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the metaverse the physical space and the virtual space co-exist, and interact simultaneously. While the physical space is virtually enhanced with information, the virtual space is continuously refreshed with real-time, real-world information. To allow users to process and manipulate information seamlessly between the real and digital spaces, novel technologies must be developed. These include smart interfaces, new augmented realities, and efficient data storage, management, and dissemination techniques. In this paper, we first discuss some promising co-space applications. These applications offer opportunities that neither of the spaces can realize on its own. Then, we further discuss several emerging technologies that empower the construction of metaverse. After that, we discuss comprehensively the data centric challenges. Finally, we discuss and envision what are likely to be required from the database and system perspectives.},
  archive      = {J_TKDE},
  author       = {Bingxue Zhang and Gang Chen and Beng Chin Ooi and Mike Zheng Shou and Kian-Lee Tan and Anthony K. H. Tung and Xiaokui Xiao and James Wei Luen Yip and Meihui Zhang},
  doi          = {10.1109/TKDE.2024.3354960},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7423-7441},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Managing metaverse data tsunami: Actionable insights},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effortless locality on data systems using relational fabric.
<em>TKDE</em>, <em>36</em>(12), 7410–7422. (<a
href="https://doi.org/10.1109/TKDE.2024.3386827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key design decision for data systems is whether they follow the row-store or the column-store paradigm. The former supports transactional workloads, while the latter is better for analytical queries. This decision has a significant impact on the entire data system architecture. The multiple-decade-long journey of these two designs has led to a new family of hybrid transactional/analytical processing (HTAP) architectures. Several efforts have been proposed to reap the benefits of both worlds by proposing systems that maintain multiple copies of data (in different physical layouts) and convert them into the desired layout as required. Due to data duplication, the additional necessary bookkeeping, and the cost of converting data between different layouts, these systems compromise between efficient analytics and data freshness. We depart from existing designs by proposing a radically new approach. We ask the question: “What if we could access any layout and ship only the relevant data through the memory hierarchy by transparently converting rows to (arbitrary groups of) columns?” To achieve this functionality, we capitalize on the reinvigorated trend of hardware specialization (that has been accelerated due to the tapering of Moore&#39;s law) to propose Relational Fabric , a near-data vertical partitioner that allows memory or storage components to perform on-the-fly transparent data transformation. By exposing an intuitive API, Relational Fabric pushes vertical partitioning to the hardware, which profoundly impacts the process of designing and building data systems. (A) There is no need for data duplication and layout conversion, making HTAP systems viable using a single layout. (B) It simplifies the memory and storage manager that needs to maintain and update a single data layout. (C) It reduces unnecessary data movement through the memory hierarchy allowing for better hardware utilization and, ultimately, better performance. In this paper, we present Relational Fabric for both memory and storage. We present our initial results on Relational Fabric for in-memory systems and discuss the challenges of building this hardware and the opportunities it brings for simplicity and innovation in the data system software stack, including physical design, query optimization, query evaluation, and concurrency control.},
  archive      = {J_TKDE},
  author       = {Tarikul Islam Papon and Ju Hyoung Mun and Konstantinos Karatsenidis and Shahin Roozkhosh and Denis Hoornaert and Ahmed Sanaullah and Ulrich Drepper and Renato Mancuso and Manos Athanassoulis},
  doi          = {10.1109/TKDE.2024.3386827},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7410-7422},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Effortless locality on data systems using relational fabric},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient model-relational data management: Challenges and
opportunities. <em>TKDE</em>, <em>36</em>(12), 7399–7409. (<a
href="https://doi.org/10.1109/TKDE.2024.3384276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As modern data pipelines continue to collect, produce, and store various data formats, extracting and combining value from traditional and context-rich sources becomes unsuitable for RDBMS. To tap into the dark data, domain experts analyze and extract insights and integrate them into various data repositories. This can involve out-of-DBMS processing with high manual effort and suboptimal performance. While AI systems based on ML models can automate the analysis, they can further generate context-rich answers. Using multiple data sources and models further exacerbates the problem of consolidating and analyzing the data of interest. We envision an analytical engine co-optimized with components that enable context-rich analysis. First, as all the data from different sources is expensive to clean ahead of time, we propose using online data integration via model-assisted similarity operations. Second, we aim for a holistic pipeline cost- and rule-based optimization across relational and model-based operators. Third, with increasingly heterogeneous hardware and workloads ranging from relational analytics to generative model inference, we envision a system that adapts to the complex query requirements at runtime. Composing ML-driven insights with established approaches aims to expand decades of research and systems-building effort in making complex functionality and performance effortless for the end user.},
  archive      = {J_TKDE},
  author       = {Viktor Sanca and Anastasia Ailamaki},
  doi          = {10.1109/TKDE.2024.3384276},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7399-7409},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient model-relational data management: Challenges and opportunities},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralized and incremental discovery of relaxed
functional dependencies using bitwise similarity. <em>TKDE</em>,
<em>36</em>(12), 7380–7398. (<a
href="https://doi.org/10.1109/TKDE.2024.3403928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decade, there have been numerous extensions to the definition of Functional Dependency ( fd ), culminating in the introduction of Relaxed Functional Dependency ( rfd ), offering more flexible constraints compared to traditional fd s. This increased flexibility makes rfd s well-suited for exploring and profiling data in datasets with lower data quality. However, efficiently identifying rfd s within dynamic data sources presents a significant challenge, as it requires processing an entire dataset from scratch whenever modifications occur. To tackle this problem, incremental discovery algorithms have been defined, but they often suffer when the frequency and the size of batches of updates increase. This article presents a new algorithm, namely D-IndiBits , relying on a new decentralized architecture to balance the workload that drives the incremental discovery process of IndiBits , which is based on bitwise operators for computing attribute similarities. Experiments demonstrate D-IndiBits &#39;s effectiveness compared to fd and rfd discovery algorithms on both static and dynamic real-world data. With batches of modifications of sizes 10 k and 100 k, D-IndiBits is capable of updating the set of rfd s in a few seconds, whereas all other approaches often employ more than 3 hours.},
  archive      = {J_TKDE},
  author       = {Bernardo Breve and Loredana Caruccio and Stefano Cirillo and Vincenzo Deufemia and Giuseppe Polese},
  doi          = {10.1109/TKDE.2024.3403928},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7380-7398},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Decentralized and incremental discovery of relaxed functional dependencies using bitwise similarity},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated data cleaning can hurt fairness in machine
learning-based decision making. <em>TKDE</em>, <em>36</em>(12),
7368–7379. (<a href="https://doi.org/10.1109/TKDE.2024.3365524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we interrogate whether data quality issues track demographic group membership (based on sex, race and age) and whether automated data cleaning — of the kind commonly used in production ML systems — impacts the fairness of predictions made by these systems. To the best of our knowledge, the impact of data cleaning on fairness in downstream tasks has not been investigated in the literature. We first analyse the tuples flagged by common error detection strategies in five research datasets. We find that, while specific data quality issues, such as higher rates of missing values, are associated with membership in historically disadvantaged groups, poor data quality does not generally track demographic group membership. As a follow-up, we conduct a large-scale empirical study on the impact of automated data cleaning on fairness, involving more than 26,000 model evaluations. We observe that, while automated data cleaning is unlikely to worsen accuracy, it is more likely to worsen fairness than to improve it, especially when the cleaning techniques are not carefully chosen. Furthermore, we find that the positive or negative impact of a particular cleaning technique often depends on the choice of fairness metric and group definition (single-attribute or intersectional). We make our code and experimental results publicly available. The analysis we conducted in this paper is difficult, primarily because it requires that we think holistically about disparities in data quality, disparities in the effectiveness of data cleaning methods, and impacts of such disparities on ML model performance for different demographic groups. Such holistic analysis can and should be supported by data engineering tools, and requires substantial data engineering research. Towards this goal, we discuss open research questions, envision the development of fairness-aware data cleaning methods, and their integration into complex pipelines for ML-based decision making.},
  archive      = {J_TKDE},
  author       = {Shubha Guha and Falaah Arif Khan and Julia Stoyanovich and Sebastian Schelter},
  doi          = {10.1109/TKDE.2024.3365524},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7368-7379},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Automated data cleaning can hurt fairness in machine learning-based decision making},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Amalur: The convergence of data integration and machine
learning. <em>TKDE</em>, <em>36</em>(12), 7353–7367. (<a
href="https://doi.org/10.1109/TKDE.2024.3357389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) training data is often scattered across disparate collections of datasets, called data silos . This fragmentation poses a major challenge for data-intensive ML applications: integrating and transforming data residing in different sources demand a lot of manual work and computational resources. With data privacy constraints, data often cannot leave the premises of data silos; hence model training should proceed in a decentralized manner. In this work, we present a vision of bridging traditional data integration (DI) techniques with the requirements of modern machine learning systems. We explore the possibilities of utilizing metadata obtained from data integration processes for improving the effectiveness, efficiency, and privacy of ML models. Towards this direction, we analyze ML training and inference over data silos. Bringing data integration and machine learning together, we highlight new research opportunities from the aspects of systems, representations, factorized learning, and federated learning.},
  archive      = {J_TKDE},
  author       = {Ziyu Li and Wenbo Sun and Danning Zhan and Yan Kang and Lydia Chen and Alessandro Bozzon and Rihan Hai},
  doi          = {10.1109/TKDE.2024.3357389},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7353-7367},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Amalur: The convergence of data integration and machine learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A framework for elastic adaptation of user multiple intents
in sequential recommendation. <em>TKDE</em>, <em>36</em>(12), 7340–7352.
(<a href="https://doi.org/10.1109/TKDE.2024.3354796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, substantial research has been conducted on sequential recommendation, with the objective of forecasting the subsequent item by leveraging a user&#39;s historical sequence of interacted items. Prior studies employ both capsule networks and self-attention techniques to effectively capture diverse underlying intents within a user&#39;s interaction sequence, thereby achieving the most advanced performance in sequential recommendation. However, users could potentially form novel intents from fresh interactions as the lengths of user interaction sequences grow. Consequently, models need to be continually updated or even extended to adeptly encompass these emerging user intents, referred as incremental multi-intent sequential recommendation. In this paper, we propose an effective I ncremental learning framework for user M ulti-intent A daptation in sequential recommendation called IMA, which augments the traditional fine-tuning strategy with the existing-intents retainer, new-intents detector, and projection-based intents trimmer to adaptively expand the model to accommodate user&#39;s new intents and prevent it from forgetting user&#39;s existing intents. Furthermore, we upgrade the IMA into an E lastic M ulti-intent A daptation (EMA) framework which can elastically remove inactive intents and compress user intent vectors under memory space limit. Extensive experiments on real-world datasets verify the effectiveness of the proposed IMA and EMA on incremental multi-intent sequential recommendation, compared with various baselines.},
  archive      = {J_TKDE},
  author       = {Zhikai Wang and Yanyan Shen},
  doi          = {10.1109/TKDE.2024.3354796},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {12},
  number       = {12},
  pages        = {7340-7352},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A framework for elastic adaptation of user multiple intents in sequential recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ze-HFS: Zentropy-based uncertainty measure for heterogeneous
feature selection and knowledge discovery. <em>TKDE</em>,
<em>36</em>(11), 7326–7339. (<a
href="https://doi.org/10.1109/TKDE.2024.3419215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge discovery of heterogeneous data is an active topic in knowledge engineering. Feature selection for heterogeneous data is an important part of effective data analysis. Although there have been many attempts to study the feature selection for heterogeneous data, there are still some challenges, such as the unbalanced problem between the stability and validity of the designed model. Hence, this paper focuses on how to design an effective and robust heterogeneous feature selection method, namely a zentropy-based uncertainty measure for heterogeneous feature selection(Ze-HFS). Different from other entropy-based uncertainty measures, the proposed method does not consider single-level information measures but systematically analyzes and integrates the information between different granular levels, which has an obvious advantage in the study of heterogeneous data knowledge discovery. Specifically, a heterogeneous distance metric is first introduced to construct heterogeneous neighborhood granules and heterogeneous neighborhood rough sets(HNRS). Then, the zentropy-based uncertainty measure is developed by analyzing the granular level structure in the HNRS model. Finally, two significant measures based on the above research are designed for heterogeneous feature selection. Compared with other state-of-the-art methods, the experimental results on 18 public datasets demonstrate the robustness and effectiveness of the proposed method.},
  archive      = {J_TKDE},
  author       = {Kehua Yuan and Duoqian Miao and Witold Pedrycz and Weiping Ding and Hongyun Zhang},
  doi          = {10.1109/TKDE.2024.3419215},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {7326-7339},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Ze-HFS: Zentropy-based uncertainty measure for heterogeneous feature selection and knowledge discovery},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). XKT: Toward explainable knowledge tracing model with
cognitive learning theories for questions of multiple knowledge
concepts. <em>TKDE</em>, <em>36</em>(11), 7308–7325. (<a
href="https://doi.org/10.1109/TKDE.2024.3418098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning ( DL ) based knowledge tracing ( KT ) models have challenges for uninterpretable prediction and parameter representation in educational applications, though they achieved remarkable outcomes in predicting the exercise performance of students. This paper proposes a novel knowledge tracing model of high precision and interpretability (named XKT ) for questions with multiple knowledge concepts based on cognitive learning theories and multidimensional item response theory ( MIRT ). The XKT consists of three differentiable network components: multi-feature embedding, cognition processing network, and MIRT -based neural predictor, which aim to provide an explainable prediction of student exercise performance. Specifically, in XKT , multi-feature embedding learns the rich semantic representation (e.g., knowledge distribution information) to enhance knowledge tracing using a cognition processing network. The cognition processing network performs selective perception, ability memory processing, and long-term knowledge memory processing to ensure the explainable factor representation for the MIRT -based neural predictor. Lastly, the MIRT -based neural predictor employs psychometric parameters to interpret student exercise predictions better. Extensive experiments on four real-world datasets show that XKT outperforms existing KT methods in predicting future learner responses. Moreover, ablation studies further show that XKT offers good interpretability of student performance predictions with multiple knowledge concepts, indicating excellent potential in real-world educational applications.},
  archive      = {J_TKDE},
  author       = {Chang-Qin Huang and Qiong-Hao Huang and Xiaodi Huang and Hua Wang and Ming Li and Kwei-Jay Lin and Yi Chang},
  doi          = {10.1109/TKDE.2024.3418098},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {7308-7325},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {XKT: Toward explainable knowledge tracing model with cognitive learning theories for questions of multiple knowledge concepts},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised graph transformer with augmentation-free
contrastive learning. <em>TKDE</em>, <em>36</em>(11), 7296–7307. (<a
href="https://doi.org/10.1109/TKDE.2024.3386984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers, having the superior ability to capture both adjacent and long-range dependencies, have been applied to the graph representation learning field. Existing methods are permanently established in the supervised setting with several high-quality labels to optimize the graph Transformers effectively. However, such labels are difficult to be obtained in real-world applications, and it remains largely unexplored in unsupervised representation learning that is essential for graph Transformers to be practical. This article first proposes an unsupervised graph Transformer and makes several technical contributions. 1) We first study various typical augmentations on graph contrastive Transformers, and conclude that such augmentations can lead to model degradation due to their domain-agnostic property. On this basis, we propose an Augmentation-free Graph Contrastive Transformer optimized through nearest neighbors to avoid model degradation; 2) Different similarity measures are designed for positive (mutual information) and negative samples (cosine) to improve the contrastive effectiveness; 3) We derive a novel way to precisely maximize mutual information, capturing more discriminative information with an additional entropy maximization. Finally, by performing the augmentation-free graph contrastive learning at different-scale representations, our graph Transformer can learn discriminative representations without supervision. Extensive experiments conducted on various datasets can demonstrate the superiority of our method.},
  archive      = {J_TKDE},
  author       = {Han Zhao and Xu Yang and Kun Wei and Cheng Deng and Dacheng Tao},
  doi          = {10.1109/TKDE.2024.3386984},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {7296-7307},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Unsupervised graph transformer with augmentation-free contrastive learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised domain-agnostic fake news detection using
multi-modal weak signals. <em>TKDE</em>, <em>36</em>(11), 7283–7295. (<a
href="https://doi.org/10.1109/TKDE.2024.3392788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of social media as one of the main platforms for people to access news has enabled the wide dissemination of fake news, having serious impacts on society. Thus, it is really important to identify fake news with high confidence in a timely manner, which is not feasible using manual analysis. This has motivated numerous studies on automating fake news detection. Most of these approaches are supervised, which requires extensive time and labour to build a labelled dataset. Although there have been limited attempts at unsupervised fake news detection, their performance suffers due to not exploiting the knowledge from various modalities related to news records and due to the presence of various latent biases in the existing news datasets (e.g., unrealistic real and fake news distributions). To address these limitations, this work proposes an effective framework for unsupervised fake news detection, which first embeds the knowledge available in four modalities (i.e., source credibility, textual content, propagation speed, and user credibility) in news records and then proposes $(UMD)^{2}$ , a novel noise-robust self-supervised learning technique, to identify the veracity of news records from the multi-modal embeddings. Also, we propose a novel technique to construct news datasets minimizing the latent biases in existing news datasets. Following the proposed approach for dataset construction, we produce a Large-scale Unlabelled News Dataset consisting 419,351 news articles related to COVID-19, acronymed as LUND-COVID . We trained the proposed unsupervised framework using LUND-COVID to exploit the potential of large datasets, and evaluate it using a set of existing labelled datasets. Our results show that the proposed unsupervised framework largely outperforms existing unsupervised baselines for different tasks such as multi-modal fake news detection, fake news early detection and few-shot fake news detection, while yielding notable improvements for unseen domains during training.},
  archive      = {J_TKDE},
  author       = {Amila Silva and Ling Luo and Shanika Karunasekera and Christopher Leckie},
  doi          = {10.1109/TKDE.2024.3392788},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {7283-7295},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Unsupervised domain-agnostic fake news detection using multi-modal weak signals},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unleash the power of inconsistency-based semi-supervised
active learning by dynamic programming of curriculum learning.
<em>TKDE</em>, <em>36</em>(11), 7268–7282. (<a
href="https://doi.org/10.1109/TKDE.2024.3417235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the training procedures of many real-world learning models, gathering and annotating decent amounts of labeled data can be cost-prohibitive. To mitigate this data-hungry problem, active learning (AL) and semi-supervised learning (SSL) are frequently adopted as two effective but often isolated means. Some recent studies explored the potential of combining AL and SSL to better probe the unlabeled data. However, almost all these contemporary SSL-AL works use a simple combination strategy, ignoring SSL and AL&#39;s inherent relation. Further, other methods suffer from high computational costs when dealing with large-scale, high-dimensional datasets. Motivated by the industry practice of labeling data, we first propose an innovative I nconsistency-based virtual a D v E rsarial A ctive L earning (IDEAL) algorithm to further investigate SSL-AL&#39;s potential superiority and achieve mutual enhancement of AL and SSL, i.e., SSL propagates label information to unlabeled samples and provides smoothed embeddings for AL, while AL excludes samples with inconsistent predictions and considerable uncertainty for SSL. We estimate unlabeled samples’ inconsistency by augmentation strategies of different granularities, including fine-grained continuous perturbation exploration and coarse-grained data transformations. Moreover, to solve the problem that the utilization efficiency of unlabeled samples is still insufficient in the process of semi-supervised training, we extend our IDEAL to a curriculum-guided version, namely SPL-IDEAL algorithm. The SPL-IDEAL algorithm can regularize the training process towards better regions in parameter space and denoise the pseudo labels with low confidence, achieving better performance. The extensive experiments, in both text and image benchmark datasets, validate the effectiveness of our proposed IDEAL and SPL-IDEAL algorithms, comparing them against state-of-the-art baselines. Two real-world case studies visualize the practical industrial value of applying and deploying the proposed data sampling algorithms.},
  archive      = {J_TKDE},
  author       = {Jiannan Guo and Yangyang Kang and Xiaolin Li and Wenqiao Zhang and Kun Kuang and Changlong Sun and Siliang Tang and Fei Wu},
  doi          = {10.1109/TKDE.2024.3417235},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {7268-7282},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Unleash the power of inconsistency-based semi-supervised active learning by dynamic programming of curriculum learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). University evaluation through graduate employment
prediction: An influence based graph autoencoder approach.
<em>TKDE</em>, <em>36</em>(11), 7255–7267. (<a
href="https://doi.org/10.1109/TKDE.2024.3402234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is always challenging task for students to select right universities. For students, graduate job placement is the most important component of university quality. However, existing university evaluation methods predominantly depend on either subjective criteria, such as the perceived quality of the learning environment and academic prestige, or on factors like faculty excellence, which may not provide a precise indication of graduate job placement. Indeed, there is still a lack of a data-driven approach to accurately measure university quality based on the employment situation of graduates. Moreover, the inherently unsupervised nature of university evaluation, compounded by the absence of a reasonable ground truth, necessitates the development of a reliable supervised methodology to precisely quantify university quality. Our basic assumption is that highly influential companies would attract graduates from high-ranking universities. To this end, in this paper, we formulate university evaluation problem into the graduate flow prediction problem, and propose an Influence based Graph Autoencoder (IGAE) method to learn the representation of universities based on the employment of their graduates. Specifically, we first build a talent transition graph based on the massive resume information. This graph reveals the flow of talent between institutions. Then, considering the asymmetric and heterogeneous properties of talent flow, an unidirectional aggregation process with a heterogeneous attention mechanism is designed to encode the nodes in the directed graph and preserve the influence terms at the same time. Afterwards, a novel dual self-attention module is exploited to capture the dynamic pattern of institutions to forecast future employment. Furthermore, we design an influence based decoder to predict the existence of talent flows and estimate the frequency of employment, which can be learnt in a joint learning framework. Finally, we conduct extensive experiments on a real-world dataset for performance evaluation. The experimental results clearly validate the effectiveness of our approach compared to the state-of-the-art baselines, and we provide a case study on university influence analysis.},
  archive      = {J_TKDE},
  author       = {Yuyang Ye and Hengshu Zhu and Tianyi Cui and Runlong Yu and Le Zhang and Hui Xiong},
  doi          = {10.1109/TKDE.2024.3402234},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {7255-7267},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {University evaluation through graduate employment prediction: An influence based graph autoencoder approach},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unified ABSA via annotation-decoupled multi-task instruction
tuning. <em>TKDE</em>, <em>36</em>(11), 7242–7254. (<a
href="https://doi.org/10.1109/TKDE.2024.3392836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-Based Sentiment Analysis (ABSA) aims to provide fine-grained aspect-level sentiment information. Different ABSA tasks are designed for different real-world applications. However, application scenarios of ABSA tasks are often diverse, typically requiring training separate systems for each task on the task-specific labeled data and making separate predictions. Second, different tasks often contain shared sentiment elements. Training task-specific models either fail to exploit the shared knowledge among multiple ABSA tasks effectively or neglect the complementarity between tasks. Third, despite the existence of the compound ABSA task such as quadruple extraction and triple extraction, it is not easy to obtain satisfactory performance due to the coupling of multiple elements. To tackle these issues, we present UnifiedABSA , a general-purpose ABSA framework based on multi-task instruction tuning, aiming at “one-model-for-all-tasks”. We also introduce a new annotation-decoupled multi-task learning mechanism that only depends on annotation on the compound task rather than all tasks. This mechanism not only fully utilizes the existing annotations from the compound task, but also alleviates the complicated coupling relationship among multiple elements, making the learning more effective. Extensive experiments show that UnifiedABSA can consistently outperform dedicated models in fully-supervised and low-resource settings for almost all 11 ABSA tasks. We also conduct further experiments to demonstrate the general applicability of our framework.},
  archive      = {J_TKDE},
  author       = {Zengzhi Wang and Rui Xia and Jianfei Yu},
  doi          = {10.1109/TKDE.2024.3392836},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {7242-7254},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Unified ABSA via annotation-decoupled multi-task instruction tuning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TTSR: Tensor-train subspace representation method for visual
domain adaptation. <em>TKDE</em>, <em>36</em>(11), 7229–7241. (<a
href="https://doi.org/10.1109/TKDE.2024.3391019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing methods for visual domain adaptation need to convert high-order tensors into one-order high-dimensional vectors through naive vectorization operations. However, they not only destroy the internal spatial structure within the original high-order tensors, but also result in exponentially increasing model parameters. To address these problems, we propose a novel method for visual domain adaptation by representing tensorial features in tensor-train subspace in this paper. Specifically, we firstly provide a theoretical deduction by constructing a tensor-train subspace and proving its linearity and left-orthogonality. Secondly, to extract common tensorial features between source and target domains, we formulate the visual domain adaptation problem into an optimization problem that models the aforementioned common tensor-train subspace between two domains, as well as their corresponding projections. Thirdly, we design a tensor-train subspace representation algorithm (TTSR) to solve the multiple variables optimization problem by optimizing its sub-problems iteratively, so as to process high-order tensorial features. Finally, we evaluate the performance of our proposed TTSR algorithm by conducting extensive experiments on three popular public datasets. The experimental results demonstrate that the TTSR algorithm can improve the classification accuracy of unlabeled target domain than that of baseline algorithms.},
  archive      = {J_TKDE},
  author       = {Guorui Li and Pengfei Xu and Sancheng Peng and Cong Wang and Yi Cai and Shui Yu},
  doi          = {10.1109/TKDE.2024.3391019},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {7229-7241},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {TTSR: Tensor-train subspace representation method for visual domain adaptation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transductive reward inference on graph. <em>TKDE</em>,
<em>36</em>(11), 7217–7228. (<a
href="https://doi.org/10.1109/TKDE.2024.3398208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we present a transductive inference approach on that reward information propagation graph, which enables the effective estimation of rewards for unlabelled data in offline reinforcement learning. Reward inference is the key to learning effective policies in practical scenarios, while direct environmental interactions are either too costly or unethical and the reward functions are rarely accessible, such as in healthcare and robotics. Our research focuses on developing a reward inference method based on the contextual properties of information propagation on graphs that capitalizes on a constrained number of human reward annotations to infer rewards for unlabelled data. We leverage both the available data and limited reward annotations to construct a reward propagation graph, wherein the edge weights incorporate various influential factors pertaining to the rewards. Subsequently, we employ the constructed graph for transductive reward inference, thereby estimating rewards for unlabelled data. Furthermore, we establish the existence of a fixed point during several iterations of the transductive inference process and demonstrate its at least convergence to a local optimum. Empirical evaluations on locomotion and robotic manipulation tasks validate the effectiveness of our approach. The application of our inferred rewards improves the performance in offline reinforcement learning tasks.},
  archive      = {J_TKDE},
  author       = {Bohao Qu and Xiaofeng Cao and Qing Guo and Yi Chang and Ivor W. Tsang and Chengqi Zhang},
  doi          = {10.1109/TKDE.2024.3398208},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {7217-7228},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Transductive reward inference on graph},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trajectory-aware task coalition assignment in spatial
crowdsourcing. <em>TKDE</em>, <em>36</em>(11), 7201–7216. (<a
href="https://doi.org/10.1109/TKDE.2023.3336642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularity of GPS-equipped smart devices, spatial crowdsourcing (SC) techniques have attracted growing attention in both academia and industry. A fundamental problem in SC is assigning location-based tasks to workers under spatial-temporal constraints. In many real-life applications, workers choose tasks on the basis of their preferred trajectories. However, by existing trajectory-aware task assignment approaches, tasks assigned to a worker may be far apart from each other, resulting in a higher detour cost as the worker needs to deviate from the original trajectory more often than necessary. Motivated by the above observations, we investigate a trajectory-aware task coalition assignment (TCA) problem and prove it to be NP-hard. The goal is to maximize the number of assigned tasks by assigning task coalitions to workers based on their preferred trajectories. For tackling the TCA problem, we develop a batch-based three-stage framework consisting of task grouping, planning, and assignment. First, we design greedy and spanning grouping approaches to generate task coalitions. Second, to gain candidate task coalitions for each worker efficiently, we design task-based and trajectory-based pruning strategies to reduce the search space. Furthermore, a 2-approximate algorithm, termed MST-Euler, is proposed to obtain a route among each worker and task coalition with a minimal detour cost. Third, the MST-Euler Greedy (MEG) algorithm is presented to compute an assignment that results in the maximal number of tasks assigned and a parallel strategy is introduced to boost its efficiency. Extensive experiments on real and synthetic datasets demonstrate the effectiveness and efficiency of the proposed algorithms.},
  archive      = {J_TKDE},
  author       = {Yuan Xie and Fan Wu and Xu Zhou and Wensheng Luo and Yifang Yin and Roger Zimmermann and Keqin Li and Kenli Li},
  doi          = {10.1109/TKDE.2023.3336642},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {7201-7216},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Trajectory-aware task coalition assignment in spatial crowdsourcing},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards rumor detection with multi-granularity evidences: A
dataset and benchmark. <em>TKDE</em>, <em>36</em>(11), 7188–7200. (<a
href="https://doi.org/10.1109/TKDE.2024.3401700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media serves as a real-time collecting and disseminating center of users’ ideas, opinions, and experiences. The deliberate disinformation and rumors propagate rapidly online due to their exaggerated facts, controversial opinions, divisive perspectives, and stunning expressions. Rumor detection approaches typically use social media posts with rumor or non-rumor labels for training and testing without disclosing the rationale behind decision-makings. On one hand, collecting evidence data to verify claims relies on expert efforts. On the other hand, verifying the truthfulness of confusing claims with distracting and lengthy evidences is still challenging. In this paper, we contribute a rumor detection dataset with multi-granularity evidences, denoted as the RD-E dataset, which includes response, fact-check, article, sourcing data and generated evidence by large language models, supporting models to verify the truthfulness of claims on social media. A number of 32,892 claims from 4,525 public individuals and organizations are annotated to 6 kinds of labels, including true, mostly true, half true, mostly false, false, pants on fire, covering a wide range of topics, e.g., politics, economy, society, technology, and health. In the experiments, seven rumor detection models have been investigated and customized on four predefined subtasks for comparisons.},
  archive      = {J_TKDE},
  author       = {Zhenguo Yang and Jiajie Lin and Zhiwei Guo and Yang Li and Xiaoping Li and Qing Li and Wenyin Liu},
  doi          = {10.1109/TKDE.2024.3401700},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {7188-7200},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards rumor detection with multi-granularity evidences: A dataset and benchmark},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards knowledge-aware and deep reinforced cross-domain
recommendation over collaborative knowledge graph. <em>TKDE</em>,
<em>36</em>(11), 7171–7187. (<a
href="https://doi.org/10.1109/TKDE.2024.3391268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain recommendations (CDRs), which can leverage the relatively abundant information from a richer domain to improve the recommendation performance in a sparser domain, have attracted great attention due to their flexible recommendation strategies. Nevertheless, existing CDR approaches still suffer from severe data sparsity and low semantic sampling efficiency issues, and hardly employ existing reinforcement learning models to improve cross-domain recommendation accuracy. To this end, we propose a new Knowledge-aware and Deep Reinforced Cross-Domain Recommendation framework over Collaborative Knowledge Graph (KRCDR). Specifically, we formalize the cross-domain recommendation task as a Markov Decision Process, and propose a knowledge-aware dual state representation approach to enhance state representations within and across domains for target users by leveraging knowledge graph information. Then, to improve the training performance, we propose a Constrained Self-supervised Actor-Critic network (CSAC) model, in which a constrained neighbor pruning strategy is devised to narrow the exploration space and improve the sampling efficiency, and the CSAC is developed to improve the recommendation policy. Additionally, in our proposed CSAC model, a self-supervised output layer within domains is used as an actor network to generate the recommendation policy, and a Q-learning output layer across domains is used as a critic network to feedback reward signals. Finally, based on the KRCDR approach, we design a new algorithm to assist in generating cross-domain recommendation results. Extensive experiments have been conducted on several real-world datasets, which demonstrate the superiority of our proposed approach compared with state-of-the-art baseline methods.},
  archive      = {J_TKDE},
  author       = {Yakun Li and Lei Hou and Juanzi Li},
  doi          = {10.1109/TKDE.2024.3391268},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {7171-7187},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards knowledge-aware and deep reinforced cross-domain recommendation over collaborative knowledge graph},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward lightweight end-to-end semantic learning of real-time
human activity recognition for enabling ambient intelligence.
<em>TKDE</em>, <em>36</em>(11), 7157–7170. (<a
href="https://doi.org/10.1109/TKDE.2024.3386794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building accurate human behavior models is necessary for ambient intelligence. However, human activity recognition (HAR) in continuously monitored physical space meets many challenges to achieve a good performance when using only simple computing resources. In this work, we model HAR as an edge classification problem for a collaborative event graph of context entities in a sequential bipartite graph form. We design a semantic learning framework, called KGAR, to perform HAR by mining, encoding, and exploiting deep semantic knowledge of activities in an end-to-end fashion. KGAR has three components: preprocessor, KGEncoder, and predictor. The preprocessor builds offline a tiny knowledge graph of activities, to model and capture multidimensional semantic relationships between activities and core context entities. KGEncoder encodes the knowledge graph of activities using improved graph neural networks (GNNs) models, to handle confusing context patterns. The predictor may be deployed using some lightweight deep neural network to produce real-time labels. Experimental results show that using KGEncoder in KGAR improves the performance of original deep neural networks by 25% - 439% on five datasets. The time of labeling each sensor event during testing with event streams is less than 0.5 ms. We have also conducted extensive experimental study to show that KGAR outperforms different types of models in more complex activity scenarios. We believe KGAR can be used for real-time HAR in real life with its high prediction performance and a low computing requirement.},
  archive      = {J_TKDE},
  author       = {Surong Yan and Kwei-Jay Lin and Haosen Wang},
  doi          = {10.1109/TKDE.2024.3386794},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {7157-7170},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Toward lightweight end-to-end semantic learning of real-time human activity recognition for enabling ambient intelligence},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The GraphTempo framework for exploring the evolution of a
graph through pattern aggregation. <em>TKDE</em>, <em>36</em>(11),
7143–7156. (<a href="https://doi.org/10.1109/TKDE.2024.3410647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the focus is on the relationships or interactions between entities, graphs offer an intuitive model for many real-world data. Such graphs are usually large and change over time, thus, requiring models and strategies that explore their evolution. We study the evolution of aggregate graphs and introduce the GraphTempo model that allows temporal and graph aggregation not only on node level by grouping individual nodes, but on a pattern level as well, where subgraphs are grouped together. Furthermore, we propose an efficient strategy for exploring the evolution of the graph based on identifying time intervals of significant growth, shrinkage, or stability. Finally, we evaluate the efficiency and effectiveness of the proposed approach using four real graphs.},
  archive      = {J_TKDE},
  author       = {Evangelia Tsoukanara and Georgia Koloniari and Evaggelia Pitoura and Peter Triantafillou},
  doi          = {10.1109/TKDE.2024.3410647},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {7143-7156},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {The GraphTempo framework for exploring the evolution of a graph through pattern aggregation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The capacity and robustness trade-off: Revisiting the
channel independent strategy for multivariate time series forecasting.
<em>TKDE</em>, <em>36</em>(11), 7129–7142. (<a
href="https://doi.org/10.1109/TKDE.2024.3400008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series data comprises various channels of variables. The multivariate forecasting models need to capture the relationship between the channels to accurately predict future values. However, recently, there has been an emergence of methods that employ the Channel Independent (CI) strategy. These methods view multivariate time series data as separate univariate time series and disregard the correlation between channels. Surprisingly, our empirical results have shown that models trained with the CI strategy outperform those trained with the Channel Dependent (CD) strategy, usually by a significant margin. Nevertheless, the reasons behind this phenomenon have not yet been thoroughly explored in the literature. This paper provides comprehensive empirical and theoretical analyses of the characteristics of multivariate time series datasets and the CI/CD strategy. Our results conclude that the CD approach has higher capacity but often lacks robustness to accurately predict distributionally drifted time series. In contrast, the CI approach trades capacity for robust prediction. Practical measures inspired by these analyses are proposed to address the capacity and robustness dilemma, including a modified CD method called Predict Residuals with Regularization (PRReg) that can surpass the CI strategy. We hope our findings can raise awareness among researchers about the characteristics of multivariate time series and inspire the construction of better forecasting models.},
  archive      = {J_TKDE},
  author       = {Lu Han and Han-Jia Ye and De-Chuan Zhan},
  doi          = {10.1109/TKDE.2024.3400008},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {7129-7142},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {The capacity and robustness trade-off: Revisiting the channel independent strategy for multivariate time series forecasting},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Temporal knowledge graph reasoning with dynamic memory
enhancement. <em>TKDE</em>, <em>36</em>(11), 7115–7128. (<a
href="https://doi.org/10.1109/TKDE.2024.3390683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal Knowledge Graph (TKG) reasoning involves predicting future facts based on historical information by learning correlations between entities and relations. Recently, many models have been proposed for the TKG reasoning task. However, most existing models cannot efficiently utilize historical information, which can be summarized in two aspects: 1) Many models only consider the historical information in a fixed time range, resulting in a lack of useful information; 2) some models use all the historical facts, thus some noise or invalid facts are introduced during reasoning. In this regard, we propose a novel TKG reasoning model with dynamic memory enhancement (DyMemR). Inspired by human memory, we introduce memory capacity, memory loss, and repetition stimulation to design a human-like memory pool that could remember potentially useful historical facts. To fully leverage the memory pool, we utilize a two-stage training strategy. The first stage is guided by the memory-based encoding module which learns embeddings from memory-based subgraphs generated through the memory pool. The second stage is the memory-based scoring module that emphasizes the historical facts in the memory pool. Finally, we extensively validate the superiority of DyMemR against various state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Fuwei Zhang and Zhao Zhang and Fuzhen Zhuang and Yu Zhao and Deqing Wang and Hongwei Zheng},
  doi          = {10.1109/TKDE.2024.3390683},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {7115-7128},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Temporal knowledge graph reasoning with dynamic memory enhancement},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal graph multi-aspect embeddings. <em>TKDE</em>,
<em>36</em>(11), 7102–7114. (<a
href="https://doi.org/10.1109/TKDE.2024.3397491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, graph embedding techniques have exhibited great potential for various downstream tasks, which can leverage both topological structures and the temporal dependencies of nodes in their representations, leading to remarkable achievements. However, the multi-role nature of nodes during their temporally interacting is neglected. To tackle this problem, we propose a novel model, Temporal graph Multi-Aspect Embedding (TMAE), to capture the latent multi-aspect characteristics of nodes in temporal graphs, thereby enhancing the quality of graph embeddings. Specifically, we propose to learn the aspect embeddings of nodes and their weights at different timestamps separately for a better adaptation. In contrast to the conventional fixed aspect number assumption, a Hierarchical Dirichlet Process-based approach is employed to dynamically determine the weight of aspects for nodes at different times. Through this framework, we effectively learn the multi-aspect information through Time-reversed Temporal Walks (TTWs). Extensive experiments performed across eight publicly accessible datasets have demonstrated the significant improvements of the proposed TMAE model over state-of-the-art algorithms by taking advantage of the multi-aspect nature.},
  archive      = {J_TKDE},
  author       = {Aimin Sun and Zhiguo Gong},
  doi          = {10.1109/TKDE.2024.3397491},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {7102-7114},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Temporal graph multi-aspect embeddings},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Start from zero: Triple set prediction for automatic
knowledge graph completion. <em>TKDE</em>, <em>36</em>(11), 7087–7101.
(<a href="https://doi.org/10.1109/TKDE.2024.3399832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph (KG) completion aims to find out missing triples in a KG. Some tasks, such as link prediction and instance completion, have been proposed for KG completion. They are triple-level tasks with some elements in a missing triple given to predict the missing element of the triple. However, knowing some elements of the missing triple in advance is not always a realistic setting. In this paper, we propose a novel graph-level automatic KG completion task called Triple Set Prediction (TSP) which assumes none of the elements in the missing triples is given. TSP is to predict a set of missing triples given a set of known triples. To properly and accurately evaluate this new task, we propose 4 evaluation metrics including 3 classification metrics and 1 ranking metric, considering both the partial-open-world and the closed-world assumptions. Furthermore, to tackle the huge candidate triples for prediction, we propose a novel and efficient subgraph-based method GPHT that can predict the triple set fast. To fairly compare the TSP results, we also propose two types of methods RuleTensor-TSP and KGE-TSP applying the existing rule- and embedding-based methods for TSP as baselines. During experiments, we evaluate the proposed methods on two datasets extracted from Wikidata following the relation-similarity partial-open-world assumption proposed by us, and also create a complete family data set to evaluate TSP results following the closed-world assumption. Results prove that the methods can successfully generate a set of missing triples and achieve reasonable scores on the new task, and GPHTperforms better than the baselines with significantly shorter prediction time.},
  archive      = {J_TKDE},
  author       = {Wen Zhang and Yajing Xu and Peng Ye and Zhiwei Huang and Zezhong Xu and Jiaoyan Chen and Jeff Z. Pan and Huajun Chen},
  doi          = {10.1109/TKDE.2024.3399832},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {7087-7101},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Start from zero: Triple set prediction for automatic knowledge graph completion},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatiotemporal-augmented graph neural networks for human
mobility simulation. <em>TKDE</em>, <em>36</em>(11), 7074–7086. (<a
href="https://doi.org/10.1109/TKDE.2024.3409071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human mobility patterns have shown significant applications in policy-decision scenarios and economic behavior researches. The human mobility simulation task aims to generate human mobility trajectories given a small set of trajectory data, which have aroused much concern due to the scarcity and sparsity of human mobility data. Existing methods mostly rely on the static relationships of locations, while largely neglect the dynamic spatiotemporal effects of locations. On the one hand, spatiotemporal correspondences of visit distributions reveal the spatial proximity and the functionality similarity of locations. On the other hand, the varying durations in different locations hinder the iterative generation process of the mobility trajectory. Therefore, we propose a novel framework to model the dynamic spatiotemporal effects of locations, namely S patio T emporal- A ugmented g R aph neural networks (STAR). The STAR framework designs various spatiotemporal graphs to capture the spatiotemporal correspondences and builds a novel dwell branch to simulate the varying durations in locations, which is finally optimized in an adversarial manner. The comprehensive experiments over four real datasets for the human mobility simulation have verified the superiority of STAR to state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Yu Wang and Tongya Zheng and Shunyu Liu and Zunlei Feng and Kaixuan Chen and Yunzhi Hao and Mingli Song},
  doi          = {10.1109/TKDE.2024.3409071},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {7074-7086},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Spatiotemporal-augmented graph neural networks for human mobility simulation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spade+: A generic real-time fraud detection framework on
dynamic graphs. <em>TKDE</em>, <em>36</em>(11), 7058–7073. (<a
href="https://doi.org/10.1109/TKDE.2024.3394155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time fraud detection remains a pressing issue for many financial and e-commerce platforms. $\mathsf {Grab}$ , a prominent technology company in Southeast Asia, addresses this by constructing a transactional graph. This graph aids in pinpointing dense subgraphs, possibly indicative of fraudster networks. Notably, prevalent methods are designed for static graphs, neglecting the evolving nature of transaction graphs. This static approach is ill-suited to the real-time necessities of modern industries. In our earlier work, $\mathsf {Spade}$ , the focus was mainly on edge insertions. However, $\mathsf {Grab}$ &#39;s operational demands necessitated managing outdated transactions. Persistently adding edges without a deletion mechanism might inadvertently lead to densely connected legitimate communities. To resolve this, we present $\mathsf {Spade+}$ , a refined real-time fraud detection system at $\mathsf {Grab}$ . Contrary to $\mathsf {Spade}$ , $\mathsf {Spade+}$ manages both edge additions and removals. Leveraging an incremental approach, $\mathsf {Spade+}$ promptly identifies suspicious communities in large graphs. Moreover, $\mathsf {Spade+}$ efficiently handles batch updates and employs edge packing to diminish latency. A standout feature of $\mathsf {Spade+}$ is its user-friendly APIs, allowing for tailored fraud detection methods. Developers can easily integrate their specific metrics, which $\mathsf {Spade+}$ autonomously refines. Rigorous evaluations validate the prowess of $\mathsf {Spade+}$ ; fraud detection mechanisms powered by $\mathsf {Spade+}$ were up to a million times faster than their static counterparts.},
  archive      = {J_TKDE},
  author       = {Jiaxin Jiang and Yuhang Chen and Bingsheng He and Min Chen and Jia Chen},
  doi          = {10.1109/TKDE.2024.3394155},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {7058-7073},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Spade+: A generic real-time fraud detection framework on dynamic graphs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Smoothing outlier scores is all you need to improve outlier
detectors. <em>TKDE</em>, <em>36</em>(11), 7044–7057. (<a
href="https://doi.org/10.1109/TKDE.2023.3332757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We hypothesize that similar objects should have similar outlier scores . To the best of our knowledge, all existing outlier detectors calculate the outlier score for each object independently regardless of the outlier scores of the other objects. Therefore, they do not guarantee that similar objects have similar outlier scores. To verify our proposed hypothesis, we propose an outlier score post-processing technique for outlier detectors, called neighborhood averaging (NA) for neighborhood smoothing in outlier score space. It pays attention to objects and their neighbors and guarantees them to have more similar outlier scores than their original scores. Given an object and its outlier score from any outlier detector, NA modifies its outlier score by combining it with its $k$ nearest neighbors’ scores. We demonstrate the effectivity of NA by using the well-known $k$ nearest neighbors ( $k$ -NN). Experimental results show that NA improves all 10 tested baseline detectors by 13% on average relative to the original results (from 0.70 to 0.79 AUC) evaluated on nine real-world datasets. Moreover, deep-learning-based detectors and even outlier detectors that are already based on $k$ -NN are also improved. The experiments also show that in some applications, the choice of detector is no more significant when detectors are jointly used with NA. This may pose a challenge to the generally considered idea that the data model is the most important factor.},
  archive      = {J_TKDE},
  author       = {Jiawei Yang and Susanto Rahardja and Pasi Fränti},
  doi          = {10.1109/TKDE.2023.3332757},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {7044-7057},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Smoothing outlier scores is all you need to improve outlier detectors},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SkipNode: On alleviating performance degradation for deep
graph convolutional networks. <em>TKDE</em>, <em>36</em>(11), 7030–7043.
(<a href="https://doi.org/10.1109/TKDE.2024.3374701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolutional Networks (GCNs) suffer from performance degradation when models go deeper. However, earlier works only attributed the performance degeneration to over-smoothing. In this paper, we conduct theoretical and experimental analysis to explore the fundamental causes of performance degradation in deep GCNs: over-smoothing and gradient vanishing have a mutually reinforcing effect that causes the performance to deteriorate more quickly in deep GCNs. On the other hand, existing anti-over-smoothing methods all perform full convolutions up to the model depth. They could not well resist the exponential convergence of over-smoothing due to model depth increasing. In this work, we propose a simple yet effective plug-and-play module, SkipNode , to overcome the performance degradation of deep GCNs. It samples graph nodes in each convolutional layer to skip the convolution operation. In this way, both over-smoothing and gradient vanishing can be effectively suppressed since (1) not all nodes’features propagate through full layers and, (2) the gradient can be directly passed back through “skipped” nodes. We provide both theoretical analysis and empirical evaluation to demonstrate the efficacy of SkipNode and its superiority over SOTA baselines.},
  archive      = {J_TKDE},
  author       = {Weigang Lu and Yibing Zhan and Binbin Lin and Ziyu Guan and Liu Liu and Baosheng Yu and Wei Zhao and Yaming Yang and Dacheng Tao},
  doi          = {10.1109/TKDE.2024.3374701},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {7030-7043},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SkipNode: On alleviating performance degradation for deep graph convolutional networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sketching data distribution by rotation. <em>TKDE</em>,
<em>36</em>(11), 7015–7029. (<a
href="https://doi.org/10.1109/TKDE.2023.3342747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel density estimation is a useful method for estimating the probability distribution of data. It is a challenge to achieve efficient kernel density estimation, especially for large-scale and high-dimension stream data. We propose rotation kernel , a novel kernel function for density estimation. The rotation kernel density can be fast estimated by a data structure named Rotation Kernel Density Sketch (RKDS). RKDS is a time- and memory-efficient method for kernel density estimation, even over data streams and distributed systems. RKDS is applicable for estimating density at specific points and also for representing data distribution. We provide theoretical analysis for rotation kernel and RKDS. Furthermore, we apply RKDS to outlier detection, concept drift detection, and personalized federated learning. Experiments show that our method improves time efficiency by up to $3\times 10^{3}$ times compared with baselines. RKDS also provides comparable detecting precision and better delay on outlier detection and concept drift detection tasks.},
  archive      = {J_TKDE},
  author       = {Runze Lei and Pinghui Wang and Rundong Li and Peng Jia and Junzhou Zhao and Xiaohong Guan},
  doi          = {10.1109/TKDE.2023.3342747},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {7015-7029},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Sketching data distribution by rotation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Similarity transitivity broken-aware multi-modal hashing.
<em>TKDE</em>, <em>36</em>(11), 7003–7014. (<a
href="https://doi.org/10.1109/TKDE.2024.3396492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the low storage cost and fast retrieval speed, multi-modal hashing, which maps the instances with different modal data-views into hash codes, has earned increasing research attention. Most existing supervised multi-modal hashing methods exploit the label information to define the similarities between instance pairs to supervise their training process. However, such methods ignore that the transitivity of their defined similarity has been broken in the multi-label scenarios, i.e., the instance $x$ is similar to the instance $y$ , and the instance $z$ is also similar to the instance $y$ , but $x$ may be not similar to $z$ , which will lead to fluctuations in the model optimization process and damage their retrieval performance. For example, in the first batch with instances $x$ and $y$ but without $z$ , the model will be optimized to make the hash codes of $x$ and $y$ similar to each other; In the second batch with instances $z$ and $y$ but without $x$ , the model will be optimized to make the hash codes of $z$ and $y$ similar to each other; In the third batch with the instances $x$ and $z$ but without $y$ , the model will be optimized to make the hash codes of $z$ and $x$ dissimilar to each other, meanwhile in this process, the hash codes of $z$ and $x$ may be dissimilar to that of $y$ which damage the optimizing results of the first two batches. Therefore, we propose a novel Similarity Transitivity Broken-aware Multi-modal Hashing, called STBMH, to solve this problem by adding a novel regularization loss into the original pair-wise loss. For each instance $x$ in a training batch, the regularization loss will take all instances in the training set into account. Extensive experiments on four widely used datasets show that the proposed method achieves better performance than the state-of-the-art baselines on multi-modal retrieval task.},
  archive      = {J_TKDE},
  author       = {Rong-Cheng Tu and Xian-Ling Mao and Jinyu Liu and Yatai Ji and Wei Wei and Heyan Huang},
  doi          = {10.1109/TKDE.2024.3396492},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {7003-7014},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Similarity transitivity broken-aware multi-modal hashing},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SDAC-DA: Semi-supervised deep attributed clustering using
dual autoencoder. <em>TKDE</em>, <em>36</em>(11), 6989–7002. (<a
href="https://doi.org/10.1109/TKDE.2024.3389049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attributed graph clustering aims to group nodes into disjoint categories using deep learning to represent node embeddings and has shown promising performance across various applications. However, two main challenges hinder further performance improvement. First, reliance on unsupervised methods impedes the learning of low-dimensional, clustering-specific features in the representation layer, thus impacting clustering performance. Second, the predominant use of separate approaches leads to suboptimal learned embeddings that are insufficient for subsequent clustering steps. To address these limitations, we propose a novel method called Semi-supervised Deep Attributed Clustering using Dual Autoencoder (SDAC-DA). This approach enables semi-supervised deep end-to-end clustering in attributed networks, promoting high structural cohesiveness and attribute homogeneity. SDAC-DA transforms the attribute network into a dual-view network, applies a semi-supervised autoencoder layering approach to each view, and integrates dimensionality reduction matrices by considering complementary views. The resulting representation layer contains high clustering-friendly embeddings, which are optimized through a unified end-to-end clustering process for effectively identifying clusters. Extensive experiments on both synthetic and real networks demonstrate the superiority of our proposed method over seven state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Kamal Berahmand and Sondos Bahadori and Maryam Nooraei Abadeh and Yuefeng Li and Yue Xu},
  doi          = {10.1109/TKDE.2024.3389049},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6989-7002},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SDAC-DA: Semi-supervised deep attributed clustering using dual autoencoder},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scope-free global multi-condition-aware industrial missing
data imputation framework via diffusion transformer. <em>TKDE</em>,
<em>36</em>(11), 6977–6988. (<a
href="https://doi.org/10.1109/TKDE.2024.3392897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data is a common phenomenon in the industrial field. The recovery of missing data is crucial to enhance the reliability of subsequent data-driven monitoring and control of industrial processes. Most existing methods are limited by the confined scope of feature extraction, which makes it impossible to rely on global information to impute missing data. In addition, they usually assume that industrial data is a uniform distribution across all working conditions, ignoring the differences in data evolution patterns across different conditions. To address these issues, this paper proposes an innovative scope-free global multi-condition-aware imputation framework based on diffusion transformer (SGMCAI-DiT). First, it extends the diffusion model by introducing conditional probability to capture the condition distribution of the entire data. Then, a noise prediction model is designed based on a novel double-weighted attention mechanism (DW-SA) to broaden the horizons of feature extraction. By discerning the inter-conditional interactions and the intra-conditional local information, the missing data imputation performance can be improved. Finally, the effectiveness and suitability of the proposed SGMCAI-DiT are verified on four real datasets sourced from industrial processes and two public non-industrial datasets. Extensive experimental results demonstrate that the proposed method outperforms several state-of-the-art methods in different missing data scenarios.},
  archive      = {J_TKDE},
  author       = {Diju Liu and Yalin Wang and Chenliang Liu and Xiaofeng Yuan and Kai Wang and Chunhua Yang},
  doi          = {10.1109/TKDE.2024.3392897},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6977-6988},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Scope-free global multi-condition-aware industrial missing data imputation framework via diffusion transformer},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scene-driven multimodal knowledge graph construction for
embodied AI. <em>TKDE</em>, <em>36</em>(11), 6962–6976. (<a
href="https://doi.org/10.1109/TKDE.2024.3399746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embodied AI is one of the most popular studies in artificial intelligence and robotics, which can effectively improve the intelligence of real-world agents (i.e. robots) serving human beings. Scene knowledge is important for an agent to understand the surroundings and make correct decisions in the varied open world. Currently, knowledge base for embodied tasks is missing and most existing work use general knowledge base or pre-trained models to enhance the intelligence of an agent. For conventional knowledge base, it is sparse, insufficient in capacity and cost in data collection. For pre-trained models, they face the uncertainty of knowledge and hard maintenance. To overcome the challenges of scene knowledge, we propose a scene-driven multimodal knowledge graph (Scene-MMKG) construction method combining conventional knowledge engineering and large language models. A unified scene knowledge injection framework is introduced for knowledge representation. To evaluate the advantages of our proposed method, we instantiate Scene-MMKG considering typical indoor robotic functionalities ( Manip ulation and Mob ility), named ManipMob-MMKG . Comparisons in characteristics indicate our instantiated ManipMob-MMKG has broad superiority on data-collection efficiency and knowledge quality. Experimental results on typical embodied tasks show that knowledge-enhanced methods using our instantiated ManipMob-MMKG can improve the performance obviously without re-designing model structures complexly.},
  archive      = {J_TKDE},
  author       = {Yaoxian Song and Penglei Sun and Haoyu Liu and Zhixu Li and Wei Song and Yanghua Xiao and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2024.3399746},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6962-6976},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Scene-driven multimodal knowledge graph construction for embodied AI},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable and sustainable graph-based traffic prediction with
adaptive deep learning. <em>TKDE</em>, <em>36</em>(11), 6949–6961. (<a
href="https://doi.org/10.1109/TKDE.2024.3419036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based deep learning models are becoming prevalent for data-driven traffic prediction in the past years, due to their competence in exploiting the non-euclidean spatial-temporal traffic data. Nonetheless, these models are approaching a limit where drastically increasing model complexity in terms of trainable parameters cannot notably improve the prediction accuracy. Furthermore, the diversity of transportation networks requires traffic predictors to be scalable to various data sizes and quantities, and ever-changing traffic dynamics also call for capacity sustainability. To this end, we propose a novel adaptive deep learning scheme for boosting graph-based traffic predictor performance. The proposed scheme utilizes domain knowledge to decompose the traffic prediction task into sub-tasks, each of which is handled by deep models with low complexity and training difficulty. Further, a stream learning algorithm based on the empirical Fisher information loss is devised to enable predictors to incrementally learn from new data without re-training from scratch. Comprehensive case studies on five real-world traffic datasets indicate outstanding performance improvement of the proposed scheme when equipped to six state-of-the-art predictors. Additionally, the scheme also provides impressive autoregressive long-term predictions and incremental learning efficacy with traffic data streams.},
  archive      = {J_TKDE},
  author       = {James Jianqiao Yu},
  doi          = {10.1109/TKDE.2024.3419036},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6949-6961},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Scalable and sustainable graph-based traffic prediction with adaptive deep learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust tensor subspace learning for incomplete multi-view
clustering. <em>TKDE</em>, <em>36</em>(11), 6934–6948. (<a
href="https://doi.org/10.1109/TKDE.2024.3399707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multi-view clustering has represented a significant role in grouping real images. In this study, a novel robust tensor subspace learning (RTSL) is proposed for incomplete multi-view clustering. Specifically, the missing samples within views are first recovered by matrix factorization. The recovered information is utilized for latent representations learning. And then, the obtained latent representations are organized from all views into a third-order tensor and the intrinsic sample relations are captured with tensor linear representation. Moreover, a low-rank sample coefficient tensor is sought to capture high-order connections among views by imposing the tensor nuclear norm. Compared with traditional learning paradigms in the vector space, the sample relations within each view as well as across views could be preserved with the aid of robust tensor subspace learning. As a result, our model can simultaneously handle the missing samples and exploit the intrinsic correlations, leading to enhanced representation capability and better quality of the recovered data. We design an efficient iterative optimization strategy to solve the proposed method. Experimental results on eight datasets show that our model outperforms other competing approaches.},
  archive      = {J_TKDE},
  author       = {Cheng Liang and Daoyuan Wang and Huaxiang Zhang and Shichao Zhang and Fei Guo},
  doi          = {10.1109/TKDE.2024.3399707},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6934-6948},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Robust tensor subspace learning for incomplete multi-view clustering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Robust knowledge adaptation for dynamic graph neural
networks. <em>TKDE</em>, <em>36</em>(11), 6920–6933. (<a
href="https://doi.org/10.1109/TKDE.2024.3388453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph structured data often possess dynamic characters in nature, such as the addition of links and nodes, in many real-world applications. Recent years have witnessed the increasing attentions paid to dynamic graph neural networks for modelling graph data. However, almost all existing approaches operate under the assumption that, upon the establishment of a new link, the embeddings of the neighboring nodes should undergo updates to learn temporal dynamics. Nevertheless, these approaches face the following limitation: If the node introduced by a new connection contains noisy information, propagating its knowledge to other nodes becomes unreliable and may even lead to the collapse of the model. In this paper, we propose Ada-DyGNN : a robust knowledge Ada ptation framework via reinforcement learning for Dy namic G raph N eural N etworks. In contrast to previous approaches, which update the embeddings of the neighbor nodes immediately after adding a new link, Ada-DyGNN adaptively determines which nodes should be updated. Considering that the decision to update the embedding of one neighbor node can significantly impact other neighbor nodes, we conceptualize the node update selection as a sequence decision problem and employ reinforcement learning to address it effectively. By this means, we can adaptively propagate knowledge to other nodes for learning robust node embedding representations. To the best of our knowledge, our approach constitutes the first attempt to explore robust knowledge adaptation via reinforcement learning specifically tailored for dynamic graph neural networks. Extensive experiments on three benchmark datasets demonstrate that Ada-DyGNN achieves the state-of-the-art performance. In addition, we conduct experiments by introducing different degrees of noise into the dataset, quantitatively and qualitatively illustrating the robustness of Ada-DyGNN.},
  archive      = {J_TKDE},
  author       = {Hanjie Li and Changsheng Li and Kaituo Feng and Ye Yuan and Guoren Wang and Hongyuan Zha},
  doi          = {10.1109/TKDE.2024.3388453},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6920-6933},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Robust knowledge adaptation for dynamic graph neural networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revisiting few-shot learning from a causal perspective.
<em>TKDE</em>, <em>36</em>(11), 6908–6919. (<a
href="https://doi.org/10.1109/TKDE.2024.3397689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning with $N$ -way $K$ -shot scheme is an open challenge in machine learning. Many metric-based approaches have been proposed to tackle this problem, e.g., the Matching Networks and CLIP-Adapter. Despite that these approaches have shown significant progress, the mechanism of why these methods succeed has not been well explored. In this paper, we try to interpret these metric-based few-shot learning methods via causal mechanism. We show that the existing approaches can be viewed as specific forms of front-door adjustment, which can alleviate the effect of spurious correlations and thus learn the causality. This causal interpretation could provide us a new perspective to better understand these existing metric-based methods. Further, based on this causal interpretation, we simply introduce two causal methods for metric-based few-shot learning, which considers not only the relationship between examples but also the diversity of representations. Experimental results demonstrate the superiority of our proposed methods in few-shot classification on various benchmark datasets.},
  archive      = {J_TKDE},
  author       = {Guoliang Lin and Yongheng Xu and Hanjiang Lai and Jian Yin},
  doi          = {10.1109/TKDE.2024.3397689},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6908-6919},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Revisiting few-shot learning from a causal perspective},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recommender systems in the era of large language models
(LLMs). <em>TKDE</em>, <em>36</em>(11), 6889–6907. (<a
href="https://doi.org/10.1109/TKDE.2024.3392335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an indispensable and important component, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have achieved significant advancements in enhancing recommender systems, these DNN-based methods still exhibit some limitations, such as inferior capabilities to effectively capture textual side information about users and items, difficulties in generalization to various recommendation scenarios, and reasoning on their predictions, etc. Meanwhile, the development of Large Language Models (LLMs), such as ChatGPT and GPT-4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization capabilities and reasoning skills. As a result, recent studies have actively attempted to harness the power of LLMs to enhance recommender systems. Given the rapid evolution of this research direction in recommender systems, there is a pressing need for a systematic overview that summarizes existing LLM-empowered recommender systems. Therefore, in this survey, we comprehensively review LLM-empowered recommender systems from various perspectives including pre-training, fine-tuning, and prompting paradigms. More specifically, we first introduce the representative methods to learn user and item representations, leveraging LLMs as feature encoders. Then, we systematically review the emerging advanced techniques of LLMs for enhancing recommender systems from three paradigms, namely pre-training, fine-tuning, and prompting. Finally, we comprehensively discuss the promising future directions in this emerging field.},
  archive      = {J_TKDE},
  author       = {Zihuai Zhao and Wenqi Fan and Jiatong Li and Yunqing Liu and Xiaowei Mei and Yiqi Wang and Zhen Wen and Fei Wang and Xiangyu Zhao and Jiliang Tang and Qing Li},
  doi          = {10.1109/TKDE.2024.3392335},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6889-6907},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Recommender systems in the era of large language models (LLMs)},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Query-oriented data augmentation for session search.
<em>TKDE</em>, <em>36</em>(11), 6877–6888. (<a
href="https://doi.org/10.1109/TKDE.2024.3419131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling contextual information in a search session has drawn more and more attention when understanding complex user intents. Recent methods are all data-driven, i.e., they train different models on large-scale search log data to identify the relevance between search contexts and candidate documents. The common training paradigm is to pair the search context with different candidate documents and train the model to rank the clicked documents higher than the unclicked ones. However, this paradigm neglects the symmetric nature of the relevance between the session context and document, i.e., the clicked documents can also be paired with different search contexts when training. In this work, we propose query-oriented data augmentation to enrich search logs and empower the modeling. We generate supplemental training pairs by altering the most important part of a search context, i.e., the current query, and train our model to rank the generated sequence along with the original sequence. This approach enables models to learn that the relevance of a document may vary as the session context changes, leading to a better understanding of users’ search patterns. We develop several strategies to alter the current query, resulting in new training data with varying degrees of difficulty. Through experimentation on two extensive public search logs, we have successfully demonstrated the effectiveness of our model.},
  archive      = {J_TKDE},
  author       = {Haonan Chen and Zhicheng Dou and Yutao Zhu and Ji-Rong Wen},
  doi          = {10.1109/TKDE.2024.3419131},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6877-6888},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Query-oriented data augmentation for session search},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prototype similarity distillation for
communication-efficient federated unsupervised representation learning.
<em>TKDE</em>, <em>36</em>(11), 6865–6876. (<a
href="https://doi.org/10.1109/TKDE.2024.3386712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated unsupervised representation learning aims at leveraging unlabeled data from multiple parties to learn visual representations without compromising the data privacy and tackle the non-IID challenge by aligning diverse representation spaces. However, model heterogeneity and communication overhead will directly impact the convergence rate and model accuracy of federated unsupervised learning. And it is challenging to learn visual features for downstream tasks under the premise of compatibility with heterogeneous models and reducing communication overhead. To address these issues, we propose a novel communication-efficient federated unsupervised representation learning framework based on prototype similarity distillation (FLPD). In this framework, the global model builds the feature representation space based on the global dataset and steers the optimization of the prototype relations of the client models. In addition to employing discriminative self-supervised learning for model training, each client fine-tunes the local representation space with global prototype similarity via knowledge distillation, which facilitates local models to fit both the local data distribution and the global representation space. In order to maintain the compactness of prototypes within the same category and enhance the separability between prototypes of different categories, a prototype-based consistency constraint is introduced to alleviate the conflict between local and global representation space. Experimental results demonstrate that our framework outperforms other alternative approaches in terms of communication efficiency and accuracy in the federated settings with statistical heterogeneity and model heterogeneity.},
  archive      = {J_TKDE},
  author       = {Chen Zhang and Yu Xie and Tingbin Chen and Wenjie Mao and Bin Yu},
  doi          = {10.1109/TKDE.2024.3386712},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6865-6876},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Prototype similarity distillation for communication-efficient federated unsupervised representation learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PromptCast: A new prompt-based learning paradigm for time
series forecasting. <em>TKDE</em>, <em>36</em>(11), 6851–6864. (<a
href="https://doi.org/10.1109/TKDE.2023.3342137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new perspective on time series forecasting. In existing time series forecasting methods, the models take a sequence of numerical values as input and yield numerical values as output. The existing SOTA models are largely based on the Transformer architecture, modified with multiple encoding mechanisms to incorporate the context and semantics around the historical data. Inspired by the successes of pre-trained language foundation models, we pose a question about whether these models can also be adapted to solve time-series forecasting. Thus, we propose a new forecasting paradigm: prompt-based time series forecasting (PromptCast). In this novel task, the numerical input and output are transformed into prompts and the forecasting task is framed in a sentence-to-sentence manner, making it possible to directly apply language models for forecasting purposes. To support and facilitate the research of this task, we also present a large-scale dataset (PISA) that includes three real-world forecasting scenarios. We evaluate different SOTA numerical-based forecasting methods and language generation models. The benchmark results with various forecasting settings demonstrate the proposed PromptCast with language generation models is a promising research direction. Additionally, in comparison to conventional numerical-based forecasting, PromptCast shows a much better generalization ability under the zero-shot setting.},
  archive      = {J_TKDE},
  author       = {Hao Xue and Flora D. Salim},
  doi          = {10.1109/TKDE.2023.3342137},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6851-6864},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {PromptCast: A new prompt-based learning paradigm for time series forecasting},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy-preserving deep learning based record linkage.
<em>TKDE</em>, <em>36</em>(11), 6839–6850. (<a
href="https://doi.org/10.1109/TKDE.2023.3342757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning -based linkage of records across different databases is becoming increasingly useful in data integration and mining applications to discover new insights from multiple data sources. However, due to privacy and confidentiality concerns, organisations often are unwilling or allowed to share their sensitive data with any external parties, thus making it challenging to build/train deep learning models for record linkage across different organisations’ databases. To overcome this limitation, we propose the first deep learning-based multi-party privacy-preserving record linkage (PPRL) protocol that can be used to link sensitive databases held by multiple different organisations. In our approach, each database owner first trains a local deep learning model, which is then uploaded to a secure environment and securely aggregated to create a global model. The global model is then used by a linkage unit to distinguish unlabelled record pairs as matches and non-matches. We utilise differential privacy to achieve provable privacy protection against re-identification attacks. We evaluate the linkage quality and scalability of our approach using several large real-world databases, showing that it can achieve high linkage quality while providing sufficient privacy protection against existing attacks.},
  archive      = {J_TKDE},
  author       = {Thilina Ranbaduge and Dinusha Vatsalan and Ming Ding},
  doi          = {10.1109/TKDE.2023.3342757},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6839-6850},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Privacy-preserving deep learning based record linkage},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Popularity prediction via modeling temporal dependencies on
dynamic evolution process. <em>TKDE</em>, <em>36</em>(11), 6828–6838.
(<a href="https://doi.org/10.1109/TKDE.2024.3409737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the future popularity of individual information cascades has attracted much attention in various application fields. It is significantly important for online advertising, viral marketing, rumor detection, and social recommendation. Most approaches target modeling forwarding path or learning important information from discrete static graph. These methods either extract complicated hand-crafted features that rely on domain knowledge and have lower generality, or devote to modeling the arriving intensity function of each message and cannot be optimized for the final popularity. Despite some approaches trying to utilize the underlying structural information in discrete snapshots, they neglect to model the temporal information that implicitly underlying abundant diffusion patterns. Meanwhile, they ignore the inherent dependencies among forwarding behaviors of users. In this paper, we propose a novel learning framework for popularity prediction via modeling temporal dependencies on dynamic evolution process, called TEDDY. Our framework not only models the temporal evolution in a separate snapshot via multiple sequences temporal encoder, but also captures the inherent temporal dependencies among different snapshots. We have conducted extensive experiments on two real-world datasets, i.e. Sina Weibo and American Physical Society. Experimental results demonstrate that our proposed TEDDY significantly improves the prediction accuracy and is superior to the state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Peng Bao and Rong Yan and Caipiao Yang},
  doi          = {10.1109/TKDE.2024.3409737},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6828-6838},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Popularity prediction via modeling temporal dependencies on dynamic evolution process},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Persistence-based motif discovery in time series.
<em>TKDE</em>, <em>36</em>(11), 6814–6827. (<a
href="https://doi.org/10.1109/TKDE.2024.3417303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motif Discovery consists of finding repeated patterns and locating their occurrences in a time series without prior knowledge about their shape or location. Most state-of-the-art algorithms rely on three core parameters: the number of motifs to discover, the length of the motifs, and a similarity threshold between motif occurrences. Setting these parameters is difficult in practice and often results from a trial-and-error strategy. In this paper, we propose a new algorithm that discovers motifs of variable length given a single motif length and without requiring a similarity threshold. At its core, the algorithm maps a time series onto a graph, summarizes it with persistent homology - a tool from topological data analysis - and identifies the most relevant motifs from the graph summary. We propose two versions of the algorithm, one requiring the number of motifs to discover and another, adaptive, that infers the number of motifs from the graph summary. Empirical evaluation on 9 labeled datasets, including 6 real-world datasets, shows that both algorithm versions significantly outperform state-of-the-art algorithms.},
  archive      = {J_TKDE},
  author       = {Thibaut Germain and Charles Truong and Laurent Oudre},
  doi          = {10.1109/TKDE.2024.3417303},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6814-6827},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Persistence-based motif discovery in time series},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Out-of-distribution evidence-aware fake news detection via
dual adversarial debiasing. <em>TKDE</em>, <em>36</em>(11), 6801–6813.
(<a href="https://doi.org/10.1109/TKDE.2024.3390431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evidence-aware fake news detection aims to conduct reasoning between news and evidences, which are retrieved based on news content, to find uniformity or inconsistency. However, we find evidence-aware detection models suffer from biases, i.e., spurious correlations between news/evidence contents and true/fake news labels, and are hard to be generalized to Out-Of-Distribution (OOD) situations. To deal with this, we propose a novel Dual Adversarial Learning (DAL) approach. We incorporate news-aspect and evidence-aspect debiasing discriminators, whose targets are both true/fake news labels, in DAL. Then, DAL reversely optimizes news-aspect and evidence-aspect debiasing discriminators to mitigate the impact of news and evidence content biases. At the same time, DAL also optimizes the main fake news predictor, so that the news-evidence interaction module can be learned. This process allows us to teach evidence-aware fake news detection models to better conduct news-evidence reasoning, and minimize the impact of content biases. To be noted, our proposed DAL approach is a plug-and-play module that works well with existing backbones. We conduct comprehensive experiments under two OOD settings, and plug DAL in four evidence-aware fake news detection backbones. Results demonstrate that, DAL significantly and stably outperforms the original backbones and some competitive debiasing methods.},
  archive      = {J_TKDE},
  author       = {Qiang Liu and Junfei Wu and Shu Wu and Liang Wang},
  doi          = {10.1109/TKDE.2024.3390431},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6801-6813},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Out-of-distribution evidence-aware fake news detection via dual adversarial debiasing},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing the number of clusters for billion-scale
quantization-based nearest neighbor search. <em>TKDE</em>,
<em>36</em>(11), 6786–6800. (<a
href="https://doi.org/10.1109/TKDE.2024.3408815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate nearest neighbor search (ANNS) is crucial in various real-world applications, including recommendation systems, data mining, and image retrieval. To date, quantization-based algorithms have emerged as one of the most efficient solutions for ANNS on billion-scale datasets. However, the determination of the optimal number of clusters, a critical factor for peak data performance in quantization-based systems, remains inadequately explored. Previous works often propose numbers of clusters that are not optimal, and the absence of effective methodologies for tuning this parameter leads to suboptimal search performance due to the vast configuration space. In response to this challenge, this paper introduces a novel algorithm that automatically identifies the optimal number of clusters for billion-scale, quantization-based ANNS systems to maximize search efficiency. We propose an analytical model for evaluating retrieval performance, serving as the benchmark for optimizing cluster numbers in quantization-based indexes. Our algorithm applies iterative local adjustments to the ANNS index being constructed, progressively refining the number of clusters. We demonstrate the efficacy of our approach using the popular inverted index structure in quantization-based ANNS systems. Our findings indicate that: (1) By optimizing the number of clusters, the vanilla inverted index exhibits improved retrieval performance on billion-scale datasets when compared to existing state-of-the-art quantization-based methods; and (2) The additional computational overhead introduced by our optimization algorithm is minimal, even when applied to billion-scale datasets.},
  archive      = {J_TKDE},
  author       = {Yujian Fu and Cheng Chen and Xiaohui Chen and Weng-Fai Wong and Bingsheng He},
  doi          = {10.1109/TKDE.2024.3408815},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6786-6800},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Optimizing the number of clusters for billion-scale quantization-based nearest neighbor search},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimization of graph clustering inspired by dynamic belief
systems. <em>TKDE</em>, <em>36</em>(11), 6773–6785. (<a
href="https://doi.org/10.1109/TKDE.2023.3274547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph clustering is essential to understand the nature and behavior of real world such as social network, technical network and transportation network. Different from the existing studies, we propose a new Markov clustering method inspired by belief dynamical system which can be used in general for optimization of different quality measures. By a rigorous theoretical proof, it has been shown that the quality function&#39;s global maximum is a dynamical system&#39;s asymptotically stable fixed point. Under specified conditions, the trajectory of the dynamical converges to the cluster labels of corresponding nodes. Particularly, a general formulation can unite well-known methodologies and the quality functions that correspond to them. The algorithm is fast and its computational complexity is nearly linear with the scale of sparse networks. Finally, we thoroughly evaluate our methodology on a variety of synthetic and real-world networks with various network properties, particularly on the dynamical networks. The results demonstrate that when compared to the current state-of-the-art algorithms, our method performs better on these networks.},
  archive      = {J_TKDE},
  author       = {Huijia Li and Haobin Cao and Yuhao Feng and Xiaoyan Li and Jian Pei},
  doi          = {10.1109/TKDE.2023.3274547},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6773-6785},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Optimization of graph clustering inspired by dynamic belief systems},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ODIN: Object density aware index for c<span
class="math inline"><em>k</em></span>&lt;mml:math
xmlns:mml=“http://www.w3.org/1998/math/MathML”&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:math&gt;NN
queries over moving objects on road networks. <em>TKDE</em>,
<em>36</em>(11), 6758–6772. (<a
href="https://doi.org/10.1109/TKDE.2023.3344662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of processing continuous $k$ nearest neighbor (C $k$ NN) queries over moving objects on road networks, which is an essential operation in a variety of applications. We are particularly concerned with scenarios where the object densities in different parts of the road network evolve over time as the objects move. Existing methods on C $k$ NN query processing are ill-suited for such scenarios as they utilize index structures with fixed granularities and are thus unable to keep up with the evolving object densities. In this paper, we directly address this problem and propose an object density aware index structure called ODIN that is an elastic tree built on a hierarchical partitioning of the road network. It is equipped with the unique capability of dynamically folding/unfolding its nodes, thereby adapting to varying object densities. We further present the ODIN-KNN-Init and ODIN-KNN-Inc algorithms for the initial identification of the $k$ NNs and the incremental update of query result as objects move. Thorough experiments on both real and synthetic datasets confirm the superiority of our proposal over several baseline methods.},
  archive      = {J_TKDE},
  author       = {Ziqiang Yu and Xiaohui Yu and Tao Zhou and Yueting Chen and Yang Liu and Bohan Li},
  doi          = {10.1109/TKDE.2023.3344662},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6758-6772},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {ODIN: object density aware index for c$k$&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:math&gt;NN queries over moving objects on road networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). O(1)-time complexity for fixed sliding-window aggregation
over out-of-order data streams. <em>TKDE</em>, <em>36</em>(11),
6745–6757. (<a href="https://doi.org/10.1109/TKDE.2024.3419566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sliding-window aggregation is one of the core operations in processing and analyzing data streams, but it seriously suffers from the unordered events or elements from data streams. Unordered streams or out-of-order data streams contain events whose order based on their timestamps (called event time) is different from the order based on their arriving times to the system (called ingestion time). Out-of-order data streams typically occur in a distributed environment due to many factors, such as network disruptions and delays. Out-of-order data streams drastically make the processing speed slower and existing works, that can handle out-of-order streams, do not address this problem well and can be further improved. The time complexities of existing approaches are not efficient because they are dependent on $n$ , which is the number of slides in the window. In addition, they ignore the past windows affected by the late-arrival records. In many applications, updating and reporting the results of the past windows affected by the late-arrival records in real time is strongly needed. This paper proposes two solutions: (1) A Maximum-allowed lateness-based IndeXing algorithm with a Constant time complexity (CMiX) for computing the current window, and (2) A Past Window Indexing algorithm (PWiX) for efficient updating the past windows. Experimental results show that CMiX and PWiX can deal with out-of-order data streams significantly better than other existing approaches. CMiX is about 3.21 times faster than the state-of-the-art approach by significantly using less memory. It is important to emphasize that all approaches mentioned in the paper have the following limitations: (1) Aggregation can be both distributive and algebraic, which must be commutative due to the out-of-order of data streams, and (2) The window and slide sizes are assumed to be fixed, and if they are changed, the indices must be reconstructed.},
  archive      = {J_TKDE},
  author       = {Savong Bou and Toshiyuki Amagasa and Hiroyuki Kitagawa},
  doi          = {10.1109/TKDE.2024.3419566},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6745-6757},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {O(1)-time complexity for fixed sliding-window aggregation over out-of-order data streams},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NIERT: Accurate numerical interpolation through unifying
scattered data representations using transformer encoder. <em>TKDE</em>,
<em>36</em>(11), 6731–6744. (<a
href="https://doi.org/10.1109/TKDE.2024.3402444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interpolation for scattered data is a classical problem in numerical analysis, with a long history of theoretical and practical contributions. Recent advances have utilized deep neural networks to construct interpolators, exhibiting excellent and generalizable performance. However, they still fall short in two aspects: 1) inadequate representation learning , resulting from separate embeddings of observed and target points in popular encoder-decoder frameworks and 2) limited generalization power , caused by overlooking prior interpolation knowledge shared across different domains. To overcome these limitations, we present a N umerical I nterpolation approach using E ncoder R epresentation of T ransformers (called NIERT ). On one hand, NIERT utilizes an encoder-only framework rather than the encoder-decoder structure. This way, NIERT can embed observed and target points into a unified encoder representation space, thus effectively exploiting the correlations among them and obtaining more precise representations. On the other hand, we propose to pre-train NIERT on large-scale synthetic mathematical functions to acquire prior interpolation knowledge, and transfer it to multiple interpolation domains with consistent performance gain. On both synthetic and real-world datasets, NIERT outperforms the existing approaches by a large margin, i.e., 4.3 $\sim 14.3\times$ lower MAE on TFRD subsets, and 1.7/1.8/8.7× lower MSE on Mathit/PhysioNet/PTV datasets.},
  archive      = {J_TKDE},
  author       = {Shizhe Ding and Boyang Xia and Milong Ren and Dongbo Bu},
  doi          = {10.1109/TKDE.2024.3402444},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6731-6744},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {NIERT: Accurate numerical interpolation through unifying scattered data representations using transformer encoder},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural networks learn specified information for imbalanced
data classification. <em>TKDE</em>, <em>36</em>(11), 6719–6730. (<a
href="https://doi.org/10.1109/TKDE.2024.3392953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced data problem is a classic topic in artificial intelligence. Neural network approaches to solve this problem mostly rely on resampling or reweighting strategies. However, these methods severely suffer from the learning bias in most cases when the empirical representation of known samples is insufficient. One-class learning can provide an ideal classification property to alleviate this critical issue. However, extending one-class learning to imbalanced data presents problems of hypersphere collapse, ambiguous interclass relations, and compact representations. In this paper, a new one-class learning paradigm is proposed for binary imbalanced data classification. Specifically, a neural network is employed to map known samples to a specified attribute space to solve the problems of hypersphere collapse and ambiguous interclass relations. Then, to alleviate the compact representation problem, a dynamic information potential energy is developed to disperse the mapped majority samples to fill the specified region as much as possible. The proposed method is validated on 34 imbalanced datasets with imbalanced ratios ranging from 16.90 to 100.14. The test results show that the proposed method achieves the best performance on more than half of the test datasets.},
  archive      = {J_TKDE},
  author       = {Zhan ao Huang and Yongsheng Sang and Yanan Sun and Jiancheng Lv},
  doi          = {10.1109/TKDE.2024.3392953},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6719-6730},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Neural networks learn specified information for imbalanced data classification},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Natural language interfaces for tabular data querying and
visualization: A survey. <em>TKDE</em>, <em>36</em>(11), 6699–6718. (<a
href="https://doi.org/10.1109/TKDE.2024.3400824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of natural language processing has revolutionized the way users interact with tabular data, enabling a shift from traditional query languages and manual plotting to more intuitive, language-based interfaces. The rise of large language models (LLMs) such as ChatGPT and its successors has further advanced this field, opening new avenues for natural language processing techniques. This survey presents a comprehensive overview of natural language interfaces for tabular data querying and visualization, which allow users to interact with data using natural language queries. We introduce the fundamental concepts and techniques underlying these interfaces with a particular emphasis on semantic parsing, the key technology facilitating the translation from natural language to SQL queries or data visualization commands. We then delve into the recent advancements in Text-to-SQL and Text-to-Vis problems from the perspectives of datasets, methodologies, metrics, and system designs. This includes a deep dive into the influence of LLMs, highlighting their strengths, limitations, and potential for future improvements. Through this survey, we aim to provide a roadmap for researchers and practitioners interested in developing and applying natural language interfaces for data interaction in the era of large language models.},
  archive      = {J_TKDE},
  author       = {Weixu Zhang and Yifei Wang and Yuanfeng Song and Victor Junqiu Wei and Yuxing Tian and Yiyan Qi and Jonathan H. Chan and Raymond Chi-Wing Wong and Haiqin Yang},
  doi          = {10.1109/TKDE.2024.3400824},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6699-6718},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Natural language interfaces for tabular data querying and visualization: A survey},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nationwide behavior-aware coordinates mining from uncertain
delivery events. <em>TKDE</em>, <em>36</em>(11), 6681–6698. (<a
href="https://doi.org/10.1109/TKDE.2024.3411562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geocoding, associating textual addresses with corresponding GPS coordinates, is vital for many location-based services (e.g., logistics, ridesharing, and social networks). One of the most common Geocoding solutions is using commercial map services such as Google Maps. However, this is typically not practical for some location-based service providers due to real-world challenges like commercial competition and high costs (recurring fees). In this paper, we design a new cost-effective Geocoding framework to automatically infer the geographic coordinates from textual addresses. To achieve this, we take the E-Commerce logistics service as a concrete scenario and design CoMiner , an unsupervised coordinate inference framework based on textual address data, delivery event data, and courier trajectory data. CoMiner includes three main components, (1) A POI-level clustering model, (2) A Delivery Mobility Graph ( DMG ), and (3) A behavior-driven address ranking model. Furthermore, we design CoMiner-W , a coordinates mining algorithm based on WiFi data, to further enhance the effectiveness of CoMiner . We conduct extensive experiments on three large-scale datasets where CoMiner outperforms the state-of-the-art methods by 20.3%. Moreover, we have designed an abnormal delivery event detection system based on CoMiner and deployed it at JD Logistics, which brings a significant reduction in abnormal delivery event rates.},
  archive      = {J_TKDE},
  author       = {Zhiqing Hong and Guang Wang and Wenjun Lyu and Baoshen Guo and Yi Ding and Haotian Wang and Shuai Wang and Yunhuai Liu and Desheng Zhang},
  doi          = {10.1109/TKDE.2024.3411562},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6681-6698},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Nationwide behavior-aware coordinates mining from uncertain delivery events},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-resolution expansion of analysis in time-frequency
domain for time series forecasting. <em>TKDE</em>, <em>36</em>(11),
6667–6680. (<a href="https://doi.org/10.1109/TKDE.2024.3396785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting plays a crucial role in various real-world applications, such as finance, energy, traffic, and healthcare, providing valuable insights for decision-making processes. The aggregation of information windows with different resolutions has proven effective in time series forecasting tasks and provides the model diverse contextual information. As a result, the network can better capture and model the heterogeneity present in the data, thereby improving performance. However, most of the current work focuses on extracting multilevel-resolution information without considering the possibility that important information can be supplemented. Meanwhile, these methods also tend to ignore the effect of resolution on frequency. To address these challenges, we introduce the Time-Frequency Domain Multi-Resolution Expansion Network (TFMRN) for long-series forecasting using multi-resolution time-frequency data. The proposed TFMRN aims to expand the data in both the time and frequency domains, enabling the model to capture finer details that may not be evident in the original data. In addition, we also propose an Information Gating Unit (IGU) to enhance the selection and guidance of rich information from the expanded time-frequency multi-resolution data. Experimental results demonstrate that the proposed method yields better performance compared with the state-of-the-art methods in both univariate and multivariate time forecasting tasks.},
  archive      = {J_TKDE},
  author       = {Kaiwen Yan and Chen Long and Huisi Wu and Zhenkun Wen},
  doi          = {10.1109/TKDE.2024.3396785},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6667-6680},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-resolution expansion of analysis in time-frequency domain for time series forecasting},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-prototypes convex merging based k-means clustering
algorithm. <em>TKDE</em>, <em>36</em>(11), 6653–6666. (<a
href="https://doi.org/10.1109/TKDE.2023.3342209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {K-Means algorithm is a popular clustering method. However, it has two limitations: 1) it gets stuck easily in spurious local minima, and 2) the number of clusters $k$ has to be given a priori. To solve these two issues, a multi-prototypes convex merging based K-Means clustering algorithm (MCKM) is presented. First, based on the structure of the spurious local minima of the K-Means problem, a multi-prototypes sampling (MPS) is designed to select the appropriate number of multi-prototypes for data with arbitrary shapes. Then, a merging technique, called convex merging (CM), merges the multi-prototypes to get a better local minima without $k$ being given a priori. Specifically, CM can obtain the optimal merging and estimate the correct $k$ . By integrating these two techniques with K-Means algorithm, the proposed MCKM is an efficient and explainable clustering algorithm for escaping the undesirable local minima of K-Means problem without given $k$ first. Two theoretical proofs are given to guarantee that the cost of MCKM (MPS+CM) can achieve a constant factor approximation to the optimal cost of the K-Means problem. Experimental results performed on synthetic and real-world data sets have verified the effectiveness of the proposed algorithm.},
  archive      = {J_TKDE},
  author       = {Dong Li and Shuisheng Zhou and Tieyong Zeng and Raymond H. Chan},
  doi          = {10.1109/TKDE.2023.3342209},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6653-6666},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-prototypes convex merging based K-means clustering algorithm},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Multiple kernel clustering with adaptive multi-scale
partition selection. <em>TKDE</em>, <em>36</em>(11), 6641–6652. (<a
href="https://doi.org/10.1109/TKDE.2024.3399738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple kernel clustering (MKC) enhances clustering performance by deriving a consensus partition or graph from a predefined set of kernels. Despite many advanced MKC methods proposed in recent years, the prevalent approaches involve incorporating all kernels by default to capture diverse information within the data. However, learning from all kernels may not be better than one of a few kernels, particularly since some kernels exhibit a higher proportion of noise than semantic content. Additionally, existing MKC methods, whether based on early-fusion or late-fusion approaches, predominantly rely on pairwise relationships among samples or cluster structures, neglecting potential correlations between these two aspects. To this end, we propose a multiple kernel clustering with an adaptive multi-scale partition selection method (MPS), which exploits multiple-dimensional representations and the pairwise cluster structure for clustering. By the proposed kernel selection framework, potentially harmful kernels are dynamically excluded during the kernel fusion process, and then the multi-scale partitions and similarity graphs derived from the retained kernels are utilized to facilitate the improved consensus partition generation. Finally, extensive experiments are conducted to demonstrate the effectiveness of MPS on eight benchmark datasets.},
  archive      = {J_TKDE},
  author       = {Jun Wang and Zhenglai Li and Chang Tang and Suyuan Liu and Xinhang Wan and Xinwang Liu},
  doi          = {10.1109/TKDE.2024.3399738},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6641-6652},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multiple kernel clustering with adaptive multi-scale partition selection},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-modal validation and domain interaction learning for
knowledge-based visual question answering. <em>TKDE</em>,
<em>36</em>(11), 6628–6640. (<a
href="https://doi.org/10.1109/TKDE.2024.3384270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge-based Visual Question Answering (KB-VQA) aims to answer the image-aware question via the external knowledge, which requires an agent to not only understand images but also explicitly retrieve and integrate knowledge facts. Intuitively, to accurately answer the question, we humans can validate the retrieved knowledge based on our memory, and then align the knowledge facts with the image regions to infer answers. However, most existing methods ignore the process of knowledge validation and alignment. In this paper, we propose the Multi-Modal Validation and Domain Interaction Learning method, which consists of two components: 1) Multi-modal validation for knowledge retrieval. We propose the multi-modal validation module (MMV) to evaluate the confidence of each retrieved knowledge fact via images and questions, which preserves knowledge candidates effective for inferring answers. 2) Domain interaction for knowledge integration. We propose the Domain Interaction TRansformer module (DI-TR) to align visual regions with knowledge facts by the interaction learning in the improved transformer. Specifically, the inter-domain and intra-domain masks are injected into each self-attention layer to control the integration scope. The proposed method outperforms several strong baselines on three widely-used knowledge-based datasets: KRVQA, OK-VQA and VQA2.0. Extensive experiments and ablation studies demonstrate the effectiveness of multi-modal knowledge validation and domain interaction learning.},
  archive      = {J_TKDE},
  author       = {Ning Xu and Yifei Gao and An-An Liu and Hongshuo Tian and Yongdong Zhang},
  doi          = {10.1109/TKDE.2024.3384270},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6628-6640},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-modal validation and domain interaction learning for knowledge-based visual question answering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multilingual taxonomic web page categorization through
ensemble knowledge distillation. <em>TKDE</em>, <em>36</em>(11),
6614–6627. (<a href="https://doi.org/10.1109/TKDE.2024.3406368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Web page categorization has been extensively studied in the literature and has been successfully used to improve information retrieval, recommendation, personalization and ad targeting. With the new industry trend of not tracking users’ online behavior without their explicit permission, using contextual targeting to accurately understand web pages in order to display ads that are topically relevant to the pages becomes more important. This is challenging, however, because an ad request only contains the URL of a web page. As a result, there is very limited available text for making accurate classifications. In this paper, we propose a unified multilingual model that can seamlessly classify web pages in 5 high-impact languages using either their full content or just their URLs with limited text. We adopt multiple data sampling techniques to increase coverage for rare categories in our training corpus, and modify the loss using class-based re-weighting to smooth the influence of frequent versus rare categories. We also propose using an ensemble of teacher models for knowledge distillation and explore different ways to create a teacher ensemble. Offline evaluation shows at least 2.6% improvement in mean average precision across 5 languages compared to a URL classification model trained with single-teacher knowledge distillation. The unified model for both full-content and URL-only input further improves the mean average precision of the dedicated URL classification model by 0.6%. We launched the proposed models, which achieve at least 37% better mean average precision than the legacy tree-based models, for contextual targeting in the Yahoo Demand Side Platform, leading to a significant ad delivery and revenue increase.},
  archive      = {J_TKDE},
  author       = {Eric Ye and Xiao Bai and Neil O’Hare and Eliyar Asgarieh and Kapil Thadani and Francisco Perez-Sorrosal and Sujyothi Adiga},
  doi          = {10.1109/TKDE.2024.3406368},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6614-6627},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multilingual taxonomic web page categorization through ensemble knowledge distillation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MulRF: A multi-dimensional range filter for sublinear time
range query processing. <em>TKDE</em>, <em>36</em>(11), 6600–6613. (<a
href="https://doi.org/10.1109/TKDE.2024.3397313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Range query is an important operation on big multi-dimensional data. This paper studies the problem of multi-dimensional range query filtering for speeding up the range query processing by avoiding reading the useless data. To solve the problem, a novel multi-dimensional range filter is proposed to filter the multi-dimensional range queries, while the existing one-dimensional range filters can not provide efficient filtering. Based on the multi-dimensional range filter, an efficient range query processing algorithm is presented. It can directly return the locations of the I/O units that contain the data in the query result without any access to the input dataset. The time complexity of the algorithm is $O(3^{m}h)$ , where $h$ is the number of I/O units partially overlapping with a range query, and $m$ is the dimension number. Since $m$ is usually $o(\sqrt{\log n})$ , it is a sublinear time algorithm if $V=O(n)$ , where $n$ is the size of the input dataset, $V=\prod _{i=1}^{m}d_{i}$ , and $d_{i}$ is the number of distinct values on the $i$ -th dimension of the dataset for $1\leq i\leq m$ . Experimental results show that the multi-dimensional range filter has low false positive rate and good filtering efficiency. The proposed range query processing algorithm achieves at least 3 $\sim$ 7 times improvement compared to the one-dimensional filter based algorithms on different datasets.},
  archive      = {J_TKDE},
  author       = {Shuai Han and Xianmin Liu and Jianzhong Li},
  doi          = {10.1109/TKDE.2024.3397313},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6600-6613},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MulRF: A multi-dimensional range filter for sublinear time range query processing},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). More interpretable graph similarity computation via maximum
common subgraph inference. <em>TKDE</em>, <em>36</em>(11), 6588–6599.
(<a href="https://doi.org/10.1109/TKDE.2024.3387044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph similarity measurement is a fundamental task in various graph-related applications. However, recent learning-based approaches lack interpretability as they directly transform interaction information between two graphs into a hidden vector, making it difficult to understand how the similarity score is derived. To address this issue, we propose an end-to-end paradigm for graph similarity learning called Similarity Computation via Maximum Common Subgraph Inference (INFMCS), which is more interpretable. Our key insight is that the similarity score has a strong correlation with the Maximum Common Subgraph (MCS). We implicitly infer the MCS to obtain the normalized MCS size, with only the similarity score being used as supervision information during training. To capture more global information, we stack vanilla transformer encoder layers with graph convolution layers and propose a novel permutation-invariant node Positional Encoding. Our entire model is simple yet effective. Comprehensive experiments demonstrate that INFMCS consistently outperforms state-of-the-art baselines for graph-graph classification and graph-graph regression tasks. Ablation experiments verify the effectiveness of our proposed computation paradigm and other components. Additionally, visualization and statistical analysis of results demonstrate the interpretability of INFMCS.},
  archive      = {J_TKDE},
  author       = {Zixun Lan and Binjie Hong and Ye Ma and Fei Ma},
  doi          = {10.1109/TKDE.2024.3387044},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6588-6599},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {More interpretable graph similarity computation via maximum common subgraph inference},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Micro-macro spatial-temporal graph-based encoder-decoder for
map-constrained trajectory recovery. <em>TKDE</em>, <em>36</em>(11),
6574–6587. (<a href="https://doi.org/10.1109/TKDE.2024.3396158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recovering intermediate missing GPS points in a sparse trajectory, while adhering to the constraints of the road network, could offer deep insights into users’ moving behaviors in intelligent transportation systems. Although recent studies have demonstrated the advantages of achieving map-constrained trajectory recovery via an end-to-end manner, they still face two significant challenges. First, existing methods are mostly sequence-based models. It is extremely hard for them to comprehensively capture the micro-semantics of individual trajectory, including the information of each GPS point and the movement between two GPS points. Second, existing approaches ignore the impact of the macro-semantics, i.e., the road conditions and the people&#39;s shared travel preferences reflected by a group of trajectories. To address the above challenges, we propose a Micro-Macro Spatial-Temporal Graph-based Encoder-Decoder (MM-STGED). Specifically, we model each trajectory as a graph to efficiently describe the micro-semantics of trajectory and design a novel message-passing mechanism to learn trajectory representations. Additionally, we extract the macro-semantics of trajectories and further incorporate them into a well-designed graph-based decoder to guide trajectory recovery. Extensive experiments conducted on sparse trajectories with three different sampling intervals that are respectively constructed from two real-world trajectory datasets demonstrate the superiority of our proposed model.},
  archive      = {J_TKDE},
  author       = {Tonglong Wei and Youfang Lin and Yan Lin and Shengnan Guo and Lan Zhang and Huaiyu Wan},
  doi          = {10.1109/TKDE.2024.3396158},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6574-6587},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Micro-macro spatial-temporal graph-based encoder-decoder for map-constrained trajectory recovery},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MAP: Model aggregation and personalization in federated
learning with incomplete classes. <em>TKDE</em>, <em>36</em>(11),
6560–6573. (<a href="https://doi.org/10.1109/TKDE.2024.3390041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In some real-world applications, data samples are usually distributed on local devices, where federated learning (FL) techniques are proposed to coordinate decentralized clients without directly sharing users’ private data. FL commonly follows the parameter server architecture and contains multiple personalization and aggregation procedures. The natural data heterogeneity across clients, i.e., Non-I.I.D. data, challenges both the aggregation and personalization goals in FL. In this paper, we focus on a special kind of Non-I.I.D. scene where clients own incomplete classes, i.e., each client can only access a partial set of the whole class set. The server aims to aggregate a complete classification model that could generalize to all classes, while the clients are inclined to improve the performance of distinguishing their observed classes. For better model aggregation, we point out that the standard softmax will encounter several problems caused by missing classes and propose “restricted softmax” as an alternative. For better model personalization, we point out that the hard-won personalized models are not well exploited and propose “inherited private model” to store the personalization experience. Our proposed algorithm named MAP could simultaneously achieve the aggregation and personalization goals in FL. Abundant experimental studies verify the superiorities of our algorithm.},
  archive      = {J_TKDE},
  author       = {Xin-Chun Li and Shaoming Song and Yinchuan Li and Bingshuai Li and Yunfeng Shao and Yang Yang and De-Chuan Zhan},
  doi          = {10.1109/TKDE.2024.3390041},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6560-6573},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MAP: Model aggregation and personalization in federated learning with incomplete classes},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LEVER: Online adaptive sequence learning framework for
high-frequency trading. <em>TKDE</em>, <em>36</em>(11), 6547–6559. (<a
href="https://doi.org/10.1109/TKDE.2023.3336185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the fast development of deep learning techniques in quantitative trading. It still remains unclear how to exploit deep learning techniques to improve high-frequency trading (HFT). Indeed, there are two emerging challenges for the use of deep learning for HFT: (i) how to quantify fast-changing market conditions for tick-level signal prediction; (ii) how to establish a unified trading paradigm for different securities of diverse market conditions and severe signal sparsity. To this end, in this paper, we propose an Online Adaptive Sequence Learning (LEVER) framework, which consists of two distinct components to predict the HFT signals at the tick level for a variety of securities simultaneously. Specifically, we start with a single learner that adopts an encoder-decoder architecture for each security-based HFT signal prediction. In this single learner, an ordered encoder module first captures the variability patterns of the security&#39;s price curve by encoding the input indicator sequence from different time ranges. An unordered decoder module then outlines the pivot points of the price curve as support and resistance levels to quantify the market status. Based on the measured market condition, a prediction module further approximates the impacts of upcoming security data as the potential market momentum to detect the tick-level trading signals. To overcome the computational challenges and signal sparsity posed by online HFT for multiple securities, we develop a competitive active-meta learning paradigm to enhance the signal learners’ learning efficiency for online implementation. Finally, extensive experiments on real-world stock market data demonstrate the effectiveness of our deployed LEVER for improving the performances of the existing industry method by 0.27 in the Sharpe ratio and by 0.09% in a transaction-based return.},
  archive      = {J_TKDE},
  author       = {Zixuan Yuan and Junming Liu and Haoyi Zhou and Denghui Zhang and Hao Liu and Nengjun Zhu and Hui Xiong},
  doi          = {10.1109/TKDE.2023.3336185},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6547-6559},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {LEVER: Online adaptive sequence learning framework for high-frequency trading},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning-based sample tuning for approximate query
processing in interactive data exploration. <em>TKDE</em>,
<em>36</em>(11), 6532–6546. (<a
href="https://doi.org/10.1109/TKDE.2023.3341451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For interactive data exploration, approximate query processing (AQP) is a useful approach that usually uses samples to provide a timely response for queries by trading query accuracy. Existing AQP systems often materialize samples in the memory for reuse to speed up query processing. How to tune the samples according to the workload is one of the key problems in AQP. However, since the data exploration workload is so complex that it cannot be accurately predicted, existing sample tuning approaches cannot adapt to the changing workload very well. To address this problem, this paper proposes a deep reinforcement learning-based sample tuner, RL-STuner . When tuning samples, RL-STuner considers the workload changes from a global perspective and uses a Deep Q-learning Network (DQN) model to select an optimal sample set that has the maximum utility for the current workload. In addition, this paper proposes a set of optimization mechanisms to reduce the sample tuning cost. Experimental results on both real-world and synthetic datasets show that RL-STuner outperforms the existing sample tuning approaches and achieves 1.6×-5.2× improvements on query accuracy with a low tuning cost.},
  archive      = {J_TKDE},
  author       = {Hanbing Zhang and Yinan Jing and Zhenying He and Kai Zhang and X. Sean Wang},
  doi          = {10.1109/TKDE.2023.3341451},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6532-6546},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning-based sample tuning for approximate query processing in interactive data exploration},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning-based attribute-augmented proximity matrix
factorization for attributed network embedding. <em>TKDE</em>,
<em>36</em>(11), 6517–6531. (<a
href="https://doi.org/10.1109/TKDE.2024.3385847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a graph $\mathcal{G}$ with a set of attributes, the attributed network embedding (ANE) aims to learn low-dimensional representations of nodes that preserve both graph topology and node attribute proximity. ANE is shown to be more effective than plain network embedding methods (using only graph topology) on many graph mining tasks. However, existing ANE solutions still provide inferior performance on tasks like node classification and link prediction, as will be shown in our experiments. The key issue is that when combining graph topology and attribute information, most existing solutions take attributes with equal importance, while in real scenarios, different attribute exerts distinct influence over the network due to the heterogeneous nature among attributes. Motivated by this, we present LATAM , a learning-based framework for ANE via trainable proximity matrix factorization. To capture the node-attribute relationships, we first construct the attribute-augmented graph by adding attribute nodes (resp. edges) to the original graph. Then, we define the attribute-augmented random walk and proximity on the attribute-augmented graph, where the weights of different attributes can be learned automatically by our designed loss functions so that more indicative attributes tend to have higher weights, imposing a higher impact on the node connectivity. To achieve this, we incorporate a differentiable SVD to back-propagate gradients of attribute weights in an end-to-end process. To scale our LATAM to large graphs, we further propose sampling techniques to learn attribute weights and an efficient attribute-augmented push algorithm to compute the proximity matrix. Extensive experiments on 8 public attributed networks against 11 existing methods show the effectiveness of our LATAM.},
  archive      = {J_TKDE},
  author       = {Kun Xie and Xiangyu Dong and Yusong Zhang and Xingyi Zhang and Qintian Guo and Sibo Wang},
  doi          = {10.1109/TKDE.2024.3385847},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6517-6531},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning-based attribute-augmented proximity matrix factorization for attributed network embedding},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning from noisy labels via dynamic loss thresholding.
<em>TKDE</em>, <em>36</em>(11), 6503–6516. (<a
href="https://doi.org/10.1109/TKDE.2023.3313604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous researches have proved that deep neural networks (DNNs) can fit almost everything even given data with noisy labels, and result in poor generalization performance. However, recent studies suggest that DNNs tend to gradually memorize the data, moving from correct data to mislabeled data. Inspired by this finding, we propose a novel method named Dynamic Loss Thresholding (DLT) . During the training process, DLT records the loss value of each sample and calculates dynamic loss thresholds. Specifically, DLT compares the loss value of each sample with the current loss threshold. Samples with smaller losses can be considered as clean samples with higher probability and vice versa. Then, DLT discards the potentially corrupted labels and further leverages self-training semi-supervised learning techniques. Experiments on CIFAR-10/100, WebVision and Clothing1M demonstrate substantial improvements over recent state-of-the-art methods. In addition, we investigate two real-world problems. First, we propose a novel approach to estimate the noise rates of datasets based on the loss difference between the early and late training stages of DNNs. Second, we explore the effect of hard samples (which are difficult to be distinguished) on the process of learning from noisy labels.},
  archive      = {J_TKDE},
  author       = {Hao Yang and You-Zhi Jin and Zi-Yin Li and Deng-Bao Wang and Xin Geng and Min-Ling Zhang},
  doi          = {10.1109/TKDE.2023.3313604},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6503-6516},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning from noisy labels via dynamic loss thresholding},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Laplacian convolutional representation for traffic time
series imputation. <em>TKDE</em>, <em>36</em>(11), 6490–6502. (<a
href="https://doi.org/10.1109/TKDE.2024.3419698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatiotemporal traffic data imputation is of great significance in intelligent transportation systems and data-driven decision-making processes. To perform efficient learning and accurate reconstruction from partially observed traffic data, we assert the importance of characterizing both global and local trends in time series. In the literature, substantial works have demonstrated the effectiveness of utilizing the low-rank property of traffic data by matrix/tensor completion models. In this study, we first introduce a Laplacian kernel to temporal regularization for characterizing local trends in traffic time series, which can be formulated as a circular convolution. Then, we develop a low-rank Laplacian convolutional representation (LCR) model by putting the circulant matrix nuclear norm and the Laplacian kernelized temporal regularization together, which is proved to meet a unified framework that has a fast Fourier transform (FFT) solution in log-linear time complexity. Through extensive experiments on several traffic datasets, we demonstrate the superiority of LCR over several baseline models for imputing traffic time series of various time series behaviors (e.g., data noises and strong/weak periodicity) and reconstructing sparse speed fields of vehicular traffic flow. The proposed LCR model is also an efficient solution to large-scale traffic data imputation over the existing imputation models.},
  archive      = {J_TKDE},
  author       = {Xinyu Chen and Zhanhong Cheng and HanQin Cai and Nicolas Saunier and Lijun Sun},
  doi          = {10.1109/TKDE.2024.3419698},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6490-6502},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Laplacian convolutional representation for traffic time series imputation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge-guided causal intervention for weakly-supervised
object localization. <em>TKDE</em>, <em>36</em>(11), 6477–6489. (<a
href="https://doi.org/10.1109/TKDE.2024.3389668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous weakly-supervised object localization (WSOL) methods aim to expand activation map discriminative areas to cover the whole objects, yet neglect two inherent challenges when relying solely on image-level labels. First, the “entangled context” issue arises from object-context co-occurrence (e.g., fish and water), making the model inspection hard to distinguish object boundaries clearly. Second, the “C-L dilemma” issue results from the information decay caused by the pooling layers, which struggle to retain both the semantic information for precise classification and those essential details for accurate localization, leading to a trade-off in performance. In this paper, we propose a knowledge-guided causal intervention method, dubbed KG-CI-CAM, to address these two under-explored issues in one go. More specifically, we tackle the co-occurrence context confounder problem via causal intervention, which explores the causalities among image features, contexts, and categories to eliminate the biased object-context entanglement in the class activation maps. Based on the disentangled object feature, we introduce a multi-source knowledge guidance framework to strike a balance between absorbing classification knowledge and localization knowledge during model training. Extensive experiments conducted on several benchmark datasets demonstrate the effectiveness of KG-CI-CAM in learning distinct object boundaries amidst confounding contexts and mitigating the dilemma between classification and localization performance.},
  archive      = {J_TKDE},
  author       = {Feifei Shao and Yawei Luo and Fei Gao and Yi Yang and Jun Xiao},
  doi          = {10.1109/TKDE.2024.3389668},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6477-6489},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Knowledge-guided causal intervention for weakly-supervised object localization},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interrelated dense pattern detection in multilayer networks.
<em>TKDE</em>, <em>36</em>(11), 6462–6476. (<a
href="https://doi.org/10.1109/TKDE.2024.3398683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a heterogeneous multilayer network with various connections in pharmacology, how can we detect components with intensive interactions and strong dependencies? Can we accurately capture suspicious groups in a multi-lot transaction network under camouflage? These challenges related to dense subgraph detection have been extensively studied in simple graphs (such as bipartite graph, multi-view network) but remain under-explored on complex networks. Existing methods struggle to effectively handle the intricate dependencies , let alone accurately identify the interrelated dense connected patterns within a series of complex heterogeneous networks. In this paper, we propose InDuen , a novel algorithm designed to detect interrelated densest subgraphs in multilayer networks through joint optimization of coupled factorization and local search for an elaborate-designed joint density measure. It is (a) effective for both large synthetic and real networks, (b) resistant to camouflage for anomaly detection, and (c) linearly scalable. Experimental results demonstrate that InDuen outperforms the state-of-the-art baselines in accurately detecting interrelated densest subgraphs under various settings. Furthermore, InDuen uncovers some intriguing patterns in real-world data, i.e., closely cooperated academic groups and interrelated dependent functional components in biology-net. InDuen achieves more than $35 \times$ speedup compared to the SOTA method Destine .},
  archive      = {J_TKDE},
  author       = {Wenjie Feng and Li Wang and Bryan Hooi and See Kiong Ng and Shenghua Liu},
  doi          = {10.1109/TKDE.2024.3398683},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6462-6476},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Interrelated dense pattern detection in multilayer networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving conversational recommender system via contextual
and time-aware modeling with less domain-specific knowledge.
<em>TKDE</em>, <em>36</em>(11), 6447–6461. (<a
href="https://doi.org/10.1109/TKDE.2024.3397321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational Recommender Systems (CRS) has become an emerging research topic seeking to perform recommendations through interactive conversations, which generally consist of generation and recommendation modules. Prior work on CRS tends to incorporate more external and domain-specific knowledge like item reviews to enhance performance. Despite the fact that the collection and annotation of the external domain-specific information needs much human effort and degenerates the generalizability, too much extra knowledge introduces more difficulty to balance among them. Therefore, we propose to fully discover and extract the internal knowledge from the context. We capture both entity-level and contextual-level representations to jointly model user preferences for the recommendation, where a time-aware attention is designed to emphasize the recently appeared items in entity-level representations. We further use the pre-trained BART to initialize the generation module to alleviate the data scarcity and enhance the context modeling. In addition to conducting experiments on a popular dataset (ReDial), we also include a multi-domain dataset (OpenDialKG) to show the effectiveness of our model. Experiments on both datasets show that our model achieves better performance on most evaluation metrics with less external knowledge and generalizes well to other domains. Additional analyses on the recommendation and generation tasks demonstrate the effectiveness of our model in different scenarios.},
  archive      = {J_TKDE},
  author       = {Lingzhi Wang and Shafiq Joty and Wei Gao and Xingshan Zeng and Kam-Fai Wong},
  doi          = {10.1109/TKDE.2024.3397321},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6447-6461},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Improving conversational recommender system via contextual and time-aware modeling with less domain-specific knowledge},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). I/o-efficient multi-criteria shortest paths query processing
on large graphs. <em>TKDE</em>, <em>36</em>(11), 6430–6446. (<a
href="https://doi.org/10.1109/TKDE.2024.3386906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shortest path computation is a basic operation for many graph-based applications and has been extensively studied. However, most existing works only consider the optimal path of a single criterion but ignore real-world situations involving multiple criteria. This paper investigates a new Multi-Criteria Shortest Paths (MCSPs) problem, aiming to compute the shortest paths of all criteria between a vertex pair. It is significant for real-world applications such as GPS navigation and social network analysis. Nevertheless, the rapid growth of graph size or memory-limited devices poses a memory-constraint challenge, making the adaptation of existing methods extremely time-consuming. To solve the memory-constraint MCSPs problem, we propose a general STOP &amp;amp; SHARE scheme to synchronize the search speeds of all criteria for sharing partition accesses. Two algorithms called OHP and MHP , adopting the one-hop strategy and partition exhaustive strategy, respectively, are proposed for implementing our scheme. Moreover, we develop two optimized algorithms, BMHP and BMHPS , to improve query efficiency by combining MHP with the bidirectional technique and a novel in-partition shortcut optimization . We also investigate partition-oriented I/O management . Experimental studies on large real-world graphs demonstrate the effectiveness of the proposed methods over the multi-pass adaptations of the existing methods.},
  archive      = {J_TKDE},
  author       = {Xinjie Zhou and Kai Huang and Lei Li and Mengxuan Zhang and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2024.3386906},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6430-6446},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {I/O-efficient multi-criteria shortest paths query processing on large graphs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). HTAP databases: A survey. <em>TKDE</em>, <em>36</em>(11),
6410–6429. (<a href="https://doi.org/10.1109/TKDE.2024.3389693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since Gartner coined the term, Hybrid Transactional and Analytical Processing (HTAP), numerous HTAP databases have been proposed to combine transactions with analytics in order to enable real-time data analytics for various data-intensive applications. HTAP databases typically process the mixed workloads of transactions and analytical queries in a unified system by leveraging both a row store and a column store. As there are different storage architectures and processing techniques to satisfy various requirements of diverse applications, it is critical to summarize the pros and cons of these key techniques. This paper offers a comprehensive survey of HTAP databases. We mainly classify state-of-the-art HTAP databases according to four storage architectures: (a) Primary Row Store and In-Memory Column Store; (b) Distributed Row Store and Column Store Replica; (c) Primary Row Store and Distributed In-Memory Column Store; and (d) Primary Column Store and Delta Row Store. We then review the key techniques in HTAP databases, including hybrid workload processing, data organization, data synchronization, query optimization, and resource scheduling. We also discuss existing HTAP benchmarks. Finally, we provide the research challenges and opportunities for HTAP techniques.},
  archive      = {J_TKDE},
  author       = {Chao Zhang and Guoliang Li and Jintao Zhang and Xinning Zhang and Jianhua Feng},
  doi          = {10.1109/TKDE.2024.3389693},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6410-6429},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {HTAP databases: A survey},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TKDP: Threefold knowledge-enriched deep prompt tuning for
few-shot named entity recognition. <em>TKDE</em>, <em>36</em>(11),
6397–6409. (<a href="https://doi.org/10.1109/TKDE.2024.3389650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot named entity recognition (NER) exploits limited annotated instances to identify named mentions. Effectively transferring the internal or external resources thus becomes the key to few-shot NER. While the existing prompt tuning methods have shown remarkable few-shot performances, they still fail to make full use of knowledge. In this work, we investigate the integration of rich knowledge to prompt tuning for stronger few-shot NER. We propose incorporating the deep prompt tuning framework with threefold knowledge (namely TKDP ), including the internal 1) context knowledge and the external 2) label knowledge &amp; 3) sememe knowledge . TKDP encodes the three feature sources and incorporates them into soft prompt embeddings, which are further injected into an existing pre-trained language model to facilitate predictions. On five benchmark datasets, the performance of our knowledge-enriched model was boosted by at most 11.53% F1 over the raw deep prompt method, and it significantly outperforms 9 strong-performing baseline systems in 5-/10-/20-shot settings, showing great potential in few-shot NER. Our TKDP framework can be broadly adapted to other few-shot tasks without much effort.},
  archive      = {J_TKDE},
  author       = {Jiang Liu and Hao Fei and Fei Li and Jingye Li and Bobo Li and Liang Zhao and Chong Teng and Donghong Ji},
  doi          = {10.1109/TKDE.2024.3389650},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6397-6409},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {TKDP: Threefold knowledge-enriched deep prompt tuning for few-shot named entity recognition},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hessian aware low-rank perturbation for order-robust
continual learning. <em>TKDE</em>, <em>36</em>(11), 6385–6396. (<a
href="https://doi.org/10.1109/TKDE.2024.3419449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual learning aims to learn a series of tasks sequentially without forgetting the knowledge acquired from the previous ones. In this work, we propose the Hessian Aware Low-Rank Perturbation algorithm for continual learning. By modeling the parameter transitions along the sequential tasks with the weight matrix transformation, we propose to apply the low-rank approximation on the task-adaptive parameters in each layer of the neural networks. Specifically, we theoretically demonstrate the quantitative relationship between the Hessian and the proposed low-rank approximation. The approximation ranks are then globally determined according to the marginal change of the empirical loss estimated by the layer-specific gradient and low-rank approximation error. Furthermore, we control the model capacity by pruning less important parameters to diminish the parameter growth. We conduct extensive experiments on various benchmarks, including a dataset with large-scale tasks, and compare our method against some recent state-of-the-art methods to demonstrate the effectiveness and scalability of our proposed method. Empirical results show that our method performs better on different benchmarks, especially in achieving task order robustness and handling the forgetting issue.},
  archive      = {J_TKDE},
  author       = {Jiaqi Li and Yuanhao Lai and Rui Wang and Changjian Shui and Sabyasachi Sahoo and Charles X. Ling and Shichun Yang and Boyu Wang and Christian Gagné and Fan Zhou},
  doi          = {10.1109/TKDE.2024.3419449},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6385-6396},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hessian aware low-rank perturbation for order-robust continual learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HAQJSK: Hierarchical-aligned quantum jensen-shannon kernels
for graph classification. <em>TKDE</em>, <em>36</em>(11), 6370–6384. (<a
href="https://doi.org/10.1109/TKDE.2024.3389966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose two novel quantum walk kernels, namely the Hierarchical Aligned Quantum Jensen-Shannon Kernels (HAQJSK), between un-attributed graph structures. Different from most classical graph kernels, the proposed HAQJSK kernels can incorporate hierarchical aligned structure information between graphs and transform graphs of random sizes into fixed-size aligned graph structures, i.e., the Hierarchical Transitive Aligned Adjacency Matrix of vertices and the Hierarchical Transitive Aligned Density Matrix of the Continuous-Time Quantum Walks (CTQW). With pairwise graphs to hand, the resulting HAQJSK kernels are defined by computing the Quantum Jensen-Shannon Divergence (QJSD) between their transitive aligned graph structures. We show that the proposed HAQJSK kernels not only reflect richer intrinsic whole graph characteristics in terms of the CTQW, but also address the drawback of neglecting structural correspondence information that arises in most R-convolution graph kernels. Moreover, unlike the previous QJSD based graph kernels associated with the QJSD and the CTQW, the proposed HAQJSK kernels can simultaneously guarantee the properties of permutation invariant and positive definiteness, explaining the theoretical advantages of the HAQJSK kernels. The experiment indicates the effectiveness of the new proposed kernels.},
  archive      = {J_TKDE},
  author       = {Lu Bai and Lixin Cui and Yue Wang and Ming Li and Jing Li and Philip S. Yu and Edwin R. Hancock},
  doi          = {10.1109/TKDE.2024.3389966},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6370-6384},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {HAQJSK: Hierarchical-aligned quantum jensen-shannon kernels for graph classification},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graphusion: Latent diffusion for graph generation.
<em>TKDE</em>, <em>36</em>(11), 6358–6369. (<a
href="https://doi.org/10.1109/TKDE.2024.3389783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph generation is a fundamental task in machine learning with broad impacts on numerous real-world applications such as biomedical discovery and social science. Most recently, generative models, especially diffusion models (DMs), have shown great promise in synthesizing realistic graphs. However, existing DMs methods typically conduct diffusion processes directly in complex graph space (i.e., node feature, adjacency matrix, or both), resulting in high modeling complexity and poor multimodal distribution coverage. In this paper, we propose Graphusion, a novel and unified latent-based graph generative framework to address the problems. Specifically, Graphusion is composed of a variational graph autoencoder mapping raw graphs with high-dimensional discrete space to low-dimensional topology-injected latent space, and latent DMs running there, producing a smoother, faster, and more expressive graph generation procedure. Thanks to the latest space modeling, we further develop principled latent self-guidance to sufficiently cover the whole semantical distribution of the unlabeled graph set. Experiments show that our Graphusion framework can consistently outperform previous graph generation baselines on both generic and molecular graph datasets, demonstrating the generality and extensibility along with further analytical justifications.},
  archive      = {J_TKDE},
  author       = {Ling Yang and Zhilin Huang and Zhilong Zhang and Zhongyi Liu and Shenda Hong and Wentao Zhang and Wenming Yang and Bin Cui and Luxia Zhang},
  doi          = {10.1109/TKDE.2024.3389783},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6358-6369},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graphusion: Latent diffusion for graph generation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Graph structure reshaping against adversarial attacks on
graph neural networks. <em>TKDE</em>, <em>36</em>(11), 6344–6357. (<a
href="https://doi.org/10.1109/TKDE.2024.3403925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have achieved impressive performance in many tasks on graph data. Recent studies show that they are vulnerable to adversarial attacks. Deliberate and unnoticeable perturbations on topology structure could render them near-useless in applications. How to design effective methods to improve the robustness of GNNs is a crucial problem. To solve this problem, some works attempt to design more robust GNN models, while others attempt to remove perturbations from the poisoned graph. Different from the previous works, this paper proposes a general framework termed as GraphReshape to enhance the robustness of GNNs via directly correcting the shifted classification boundary of GNN models in the presence of adversarial attacks. GraphReshape consists of two modules: locating tractive nodes that could correct GNNs and reshaping local structure to improve their representations in the latent space. Extensive experiments on four real-world datasets show that GraphReshape achieves significant performance gain compared with state-of-the-art baselines against different adversarial attacks.},
  archive      = {J_TKDE},
  author       = {Haibo Wang and Chuan Zhou and Xin Chen and Jia Wu and Shirui Pan and Zhao Li and Jilong Wang and Philip S. Yu},
  doi          = {10.1109/TKDE.2024.3403925},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6344-6357},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph structure reshaping against adversarial attacks on graph neural networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph rewiring and preprocessing for graph neural networks
based on effective resistance. <em>TKDE</em>, <em>36</em>(11),
6330–6343. (<a href="https://doi.org/10.1109/TKDE.2024.3397692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) are powerful models for processing graph data and have demonstrated state-of-the-art performance on many downstream tasks. However, existing GNNs can generally suffer from two limitations: over-smoothing and over-squashing, which can significantly undermine their learning ability for large graphs. To overcome these issues simultaneously, by utilizing the concept of effective resistances, we focus on minimizing total constrained resistance while identifying problematic edges using topological redundancy and bottleneck sparsity coefficients. We introduce a novel graph rewiring and preprocessing method guided by effective resistance (GPER), capable of edge addition or removal. Theoretical analysis validates our method&#39;s efficacy in mitigating over-smoothing and over-squashing. In the experiments, we conduct node and graph classifications on the benchmark datasets and can achieve an average improvement of 7.8% and 2.0%, respectively. We also conduct scalability analysis on large graphs with GCN and demonstrate that the proposed preprocess approach can reduce graph size by over 50% while improve the performance.},
  archive      = {J_TKDE},
  author       = {Xu Shen and Pietro Liò and Lintao Yang and Ru Yuan and Yuyang Zhang and Chengbin Peng},
  doi          = {10.1109/TKDE.2024.3397692},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6330-6343},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph rewiring and preprocessing for graph neural networks based on effective resistance},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph out-of-distribution generalization with controllable
data augmentation. <em>TKDE</em>, <em>36</em>(11), 6317–6329. (<a
href="https://doi.org/10.1109/TKDE.2024.3393109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Network (GNN) has demonstrated extraordinary performance in classifying graph properties. However, due to the selection bias of training and testing data (e.g., training on small graphs and testing on large graphs, or training on dense graphs and testing on sparse graphs), distribution deviation is widespread. More importantly, we often observe hybrid structure distribution shift of both scale and density, despite of one-sided biased data partition. The spurious correlations over hybrid distribution deviation degrade the performance of previous GNN methods and show large instability among different datasets. To alleviate this problem, we propose OOD-GMixup to jointly manipulate the training distribution with controllable data augmentation in metric space. Specifically, we first extract the graph rationales to eliminate the spurious correlations due to irrelevant information. Second, we generate virtual samples with perturbation on graph rationale representation domain to obtain potential OOD training samples. Finally, we propose OOD calibration to measure the distribution deviation of virtual samples by leveraging Extreme Value Theory, and further actively control the training distribution by emphasizing the impact of virtual OOD samples. Extensive studies on several real-world datasets on graph classification demonstrate the superiority of our proposed method over state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Bin Lu and Ze Zhao and Xiaoying Gan and Shiyu Liang and Luoyi Fu and Xinbing Wang and Chenghu Zhou},
  doi          = {10.1109/TKDE.2024.3393109},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6317-6329},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph out-of-distribution generalization with controllable data augmentation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph contrastive learning with personalized augmentation.
<em>TKDE</em>, <em>36</em>(11), 6305–6316. (<a
href="https://doi.org/10.1109/TKDE.2024.3388728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph contrastive learning (GCL) has emerged as an effective tool to learn representations for whole graphs in the absence of labels. The key idea is to maximize the agreement between two augmented views of each graph via data augmentation. Existing GCL models mainly focus on applying identical augmentation strategies for all graphs within a given scenario. However, real-world graphs are often not monomorphic but abstractions of diverse natures. Even within the same scenario (e.g., macromolecules and online communities), different graphs might need diverse augmentations to perform effective GCL. Thus, blindly augmenting all graphs without considering their individual characteristics may undermine the performance of GCL arts. However, it is non-trivial to achieve personalized allocation for all graphs since the search space is exponential to the number of graphs. To bridge the gap, we propose the first principled framework, termed as G raph contrastive learning with P ersonalized A ugmentation (GPA). It advances conventional GCL by allowing each graph to choose its own suitable augmentation operations. To cope with the huge search space, we design a tailored augmentation selector by converting the discrete space into a continuous one, which is a plug-and-play module and can be effectively trained with downstream GCL models end to end. Extensive experiments across 10 benchmark datasets demonstrate the superiority of GPA against state-of-the-art competitors. Moreover, by visualizing the learned augmentation distributions across different types of datasets, we show that GPA can effectively identify the most suitable augmentations for each graph based on its characteristics.},
  archive      = {J_TKDE},
  author       = {Xin Zhang and Qiaoyu Tan and Xiao Huang and Bo Li},
  doi          = {10.1109/TKDE.2024.3388728},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6305-6316},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph contrastive learning with personalized augmentation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Granular-ball fuzzy set and its implement in SVM.
<em>TKDE</em>, <em>36</em>(11), 6293–6304. (<a
href="https://doi.org/10.1109/TKDE.2024.3419184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional fuzzy set methods, designed around the finest granularity of inputs-individual points and their membership degrees-often struggle with inefficiencies and label noise. To overcome these challenges, we introduce granular-ball computing into the fuzzy set, creating the new granular-ball fuzzy set framework. This approach uses granular-ball inputs rather than single points, significantly reducing the number of entities and minimizing susceptibility to the noise affecting individual sample points. As a result, our framework enhances both efficiency and robustness compared to traditional methods and is applicable across various domains of fuzzy data processing. Furthermore, we apply this framework to fuzzy support vector machines (FSVMs), developing the Granular-ball Fuzzy Support Vector Machine (GBFSVM). Experimental tests on UCI benchmark datasets show that GBFSVM surpasses traditional models in efficiency and robustness.},
  archive      = {J_TKDE},
  author       = {Shuyin Xia and Xiaoyu Lian and Guoyin Wang and Xinbo Gao and Qinghua Hu and Yabin Shao},
  doi          = {10.1109/TKDE.2024.3419184},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6293-6304},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Granular-ball fuzzy set and its implement in SVM},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global-local feature learning via dynamic spatial-temporal
graph neural network in meteorological prediction. <em>TKDE</em>,
<em>36</em>(11), 6280–6292. (<a
href="https://doi.org/10.1109/TKDE.2024.3397840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The meteorological environment has a profound impact on global health (e.g., air quality), science and technology (e.g., rocket launches), and economic development (e.g., poverty reduction) etc. Meteorological prediction presents numerous challenges to both academia and industry due to its multifaceted nature which encompasses real-time observations and complex modeling. Recent research adopt graph convolutional recurrent network and establish coordinate information to obtain local spatial-temporal pattern. However, the model only utilizes the local spatial-temporal information and fail to fully consider the dynamic meteorological situation. To address the above limitations, we propose a Dynamic Spatial-Temporal Graph Neural Network (DSTGNN) to learn global-local meteorological features. Specifically, we divide the global spatial-temporal information along the timeline to obtain local spatial-temporal information. For the global aspect, we design a random throwedge module during the neighborhood propagation process in graph neural network (GNN) to extract the features and adapt to the dynamic situation. We also establish convolution operation module to learn the features. Next, we perform information fusion on the two modules to capture sufficient features. In addition, we employ graph ordinary differential equation (ODE) network and utilize the coordinate information to obtain the long-term features and coordinate relationships. In the local aspect, we first construct a GNN to conduct graph embedding. Then, we integrate another GNN into a gated recurrent unit (GRU) and also use the coordinate information to explore the features and coordinate relationships. Finally, we combine the global and local features via a global-local features learning layer for meteorological prediction. Experimental results on the four real-world meteorological datasets show that DSTGNN outperforms the baseline models.},
  archive      = {J_TKDE},
  author       = {Yibi Chen and Kenli Li and Chai Kiat Yeo and Keqin Li},
  doi          = {10.1109/TKDE.2024.3397840},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6280-6292},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Global-local feature learning via dynamic spatial-temporal graph neural network in meteorological prediction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global meets local: Dual activation hashing network for
large-scale fine-grained image retrieval. <em>TKDE</em>,
<em>36</em>(11), 6266–6279. (<a
href="https://doi.org/10.1109/TKDE.2024.3393512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Internet era, the exponential growth of fine-grained image databases poses a considerable challenge for efficient information retrieval. Hashing-based approaches gained traction for their computational and storage efficiency, yet fine-grained hashing retrieval presents unique challenges due to small inter-class and large intra-class variations inherent to fine-grained entities. Thus, traditional hashing algorithms falter in discerning these subtle, yet critical, visual differences and fail to generate compact yet semantically rich hash codes. To address this, we introduce a Dual Activation Hashing Network ( DAHNet ) designed to convert high-dimensional image data into optimized binary codes via an innovative feature activation paradigm. The architecture consists of dual branches specifically tailored for global and local semantic activation, thereby establishing direct correspondences between hash codes and distinguishable object parts through a hierarchical activation pipeline. Specifically, our spatial-oriented semantic activation module modulates dominant visual regions while amplifying the activations of subtle yet semantically rich areas in a controlled manner. Building on these activated visual representations, the proposed inter-region semantic enrichment module further enriches them by unearthing semantically complementary cues. Concurrently, DAHNet integrates a channel-oriented semantic activation module that exploits channel-specific correlations to distill contextual cues from spatially-activated visual features, thereby reinforcing robust learning to hash. To maintain the similarity of the original entities, we amalgamate final hash codes from both activation branches, capturing both local textural details and global structural information. Comprehensive evaluations on five fine-grained image retrieval benchmarks demonstrate DAHNet &#39;s superior performance over existing state-of-the-art hashing solutions, especially on 12-bit, improving performance by 4%–15% compared to the current best results on the five benchmarks. Moreover, generalization studies validate the efficacy of our dual-activation framework in the domain of content-based fine-grained image retrieval.},
  archive      = {J_TKDE},
  author       = {Xin Jiang and Hao Tang and Zechao Li},
  doi          = {10.1109/TKDE.2024.3393512},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6266-6279},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Global meets local: Dual activation hashing network for large-scale fine-grained image retrieval},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized measure-biased sampling and priority sampling.
<em>TKDE</em>, <em>36</em>(11), 6251–6265. (<a
href="https://doi.org/10.1109/TKDE.2023.3340673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Query with aggregates is one of the most important classes of ad-hoc queries. Since query response time is critical in many scenarios, small errors are usually tolerable for query processing. In this work, we adopt sampling to provide fast approximate answers to distribution query and subset-sum query. On the one hand, uniform sampler is sub-optimal. On the other hand, both measure-biased sampler and priority sampler need to create a sample for each measure column. It leads to expensive storage cost, when there are dozens or hundreds of measure columns in the table. To address this issue, we generalize both measure-biased sampler and priority sampler, which can compress the samples but still provide fast approximate answers to both distribution query and subset-sum query within a user-specified error bound. Besides, we establish the relationship between measure-biased sampler and priority sampler by constructing a measure-biased sample from a priority sample. We also extend the priority sampler to support multiple types of aggregates for arbitrary subset. In the extensive experimental evaluation, our generalized samplers achieve a remarkable improvement over the original samplers in terms of the error metrics.},
  archive      = {J_TKDE},
  author       = {Zhao Chang and Feifei Li and Yulong Shen},
  doi          = {10.1109/TKDE.2023.3340673},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6251-6265},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Generalized measure-biased sampling and priority sampling},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized graph prompt: Toward a unification of
pre-training and downstream tasks on graphs. <em>TKDE</em>,
<em>36</em>(11), 6237–6250. (<a
href="https://doi.org/10.1109/TKDE.2024.3419109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs can model complex relationships between objects, enabling a myriad of Web applications such as online page/article classification and social recommendation. While graph neural networks (GNNs) have emerged as a powerful tool for graph representation learning, in an end-to-end supervised setting, their performance heavily relies on a large amount of task-specific supervision. To reduce labeling requirement, the “pre-train, fine-tune” and “pre-train, prompt” paradigms have become increasingly common. In particular, prompting is a popular alternative to fine-tuning in natural language processing, which is designed to narrow the gap between pre-training and downstream objectives in a task-specific manner. However, existing study of prompting on graphs is still limited, lacking a universal treatment to appeal to different downstream tasks. In this paper, we propose GraphPrompt , a novel pre-training and prompting framework on graphs. GraphPrompt not only unifies pre-training and downstream tasks into a common task template, but also employs a learnable prompt to assist a downstream task in locating the most relevant knowledge from the pre-trained model in a task-specific manner. In particular, GraphPrompt adopts simple yet effective designs in both pre-training and prompt tuning: During pre-training, a link prediction-based task is used to materialize the task template; during prompt tuning, a learnable prompt vector is applied to the ReadOut layer of the graph encoder. To further enhance GraphPrompt in these two stages, we extend it into GraphPrompt+ with two major enhancements. First, we generalize a few popular graph pre-training tasks beyond simple link prediction to broaden the compatibility with our task template. Second, we propose a more generalized prompt design that incorporates a series of prompt vectors within every layer of the pre-trained graph encoder, in order to capitalize on the hierarchical information across different layers beyond just the readout layer. Finally, we conduct extensive experiments on five public datasets to evaluate and analyze GraphPrompt and GraphPrompt+ .},
  archive      = {J_TKDE},
  author       = {Xingtong Yu and Zhenghao Liu and Yuan Fang and Zemin Liu and Sihong Chen and Xinming Zhang},
  doi          = {10.1109/TKDE.2024.3419109},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6237-6250},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Generalized graph prompt: Toward a unification of pre-training and downstream tasks on graphs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Focusing on relevant responses for multi-modal rumor
detection. <em>TKDE</em>, <em>36</em>(11), 6225–6236. (<a
href="https://doi.org/10.1109/TKDE.2024.3389694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the absence of an official statement about a rumor, people may expose the truth behind such rumor through their responses on social media. Due to the varying relevance of responses in exposing hidden suspicious points within a rumor claim, it is crucial to prioritize those with higher relevance, rather than considering every responding tweets. As for the multi-modal rumor detection, an effective approach for evaluating relevance is aligning responses with the different modalities of the rumor claim in a fine-grained manner. However, owing to the substantial volume of response tweets, it is both costly and redundant to align all responses with the multi-modal claim. In this paper, we propose a novel two-stage model, termed Focal Reasoning Model (FoRM) , to select critical responses for multi-modal rumor detection. More specifically, our FoRM consists of two primary elements: coarse-grained selection and fine-grained reasoning. The coarse-grained selection component employs post-level features of responses to initialize a relevant score for each. Based on these scores, we preserve the responses with higher scores as the candidate ones for subsequent reasoning. Within the fine-grained reasoning component, we develop a relation attention module to investigate fine-grained relationships, specifically token-to-token and token-to-object connections, between the preserved responses and the multi-modal claim, with the goal of discovering valuable clues. Extensive experiments have been conducted on three real-world datasets, and the results demonstrate that our proposed model outperforms all the baselines.},
  archive      = {J_TKDE},
  author       = {Jun Li and Yi Bin and Liang Peng and Yang Yang and Yangyang Li and Hao Jin and Zi Huang},
  doi          = {10.1109/TKDE.2024.3389694},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6225-6236},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Focusing on relevant responses for multi-modal rumor detection},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FedEDB: Building a federated and encrypted data store via
consortium blockchains. <em>TKDE</em>, <em>36</em>(11), 6210–6224. (<a
href="https://doi.org/10.1109/TKDE.2023.3341149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized storage platforms based on consortium blockchains have emerged in the spotlight of research and industry communities because they are flexible, transparent, and eliminated trust in contrast to the traditional centralized data-sharing model. However, due to wide attacking surfaces in a blockchain network, this decentralized data-sharing paradigm is subject to malicious data breaches. Untrusted blockchain nodes can directly obtain sensitive information from the query processing and their local storage. Several studies have been made for solving this dilemma, but they only focus on single-user settings and cannot be directly applied to multi-owners blockchain-based data sharing scenarios. In this paper, we introduce FedEDB, a federated and encrypted data store by using consortium blockchains. Unlike existing solutions that focus on single-user settings, our proposed schemes can efficiently support privacy-preserving and reliable multi-owner queries in the decentralized setting. We start from the practical key aggregation technique to construct the multi-owner search schemes and further refine the underling building blocks to enhance the security. Besides, we integrate the smart contract with our tailored zero-knowledge proof to enforce secure and reliable result verification protocol with fairness. We implement a prototype and thorough security analysis and comprehensive evaluation results confirm the practicability of our design.},
  archive      = {J_TKDE},
  author       = {Yu Guo and Yuxin Xi and Haodi Wang and Mingyue Wang and Cong Wang and Xiaohua Jia},
  doi          = {10.1109/TKDE.2023.3341149},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6210-6224},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {FedEDB: Building a federated and encrypted data store via consortium blockchains},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature selection with discernibility and independence
criteria. <em>TKDE</em>, <em>36</em>(11), 6195–6209. (<a
href="https://doi.org/10.1109/TKDE.2024.3388526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection plays a significant role in data mining and machine learning. It is challenging to determine how many features are necessary to form an optimal feature subset. To address this challenge, an innovative visual 2D feature selection framework is introduced, in which the feature discernibility and independence are defined to evaluate its capability for classification and its relevance to other features, respectively. All features are represented in 2D space with discernibility as $x$ -axis and independence as $y$ -axis. The features located in the upper right corner represent high discernibility and high independence, so comprise the optimal feature subset. This leads to the formation of a family of feature selection algorithms. Three such algorithms are proposed in this paper referred to as FSDIE, FSDIR, and FSDIS (Feature Selection based on the Discernibility and the Independence, respectively, of Exponent, Reciprocal, and anti-Similarity). To speed-up these three algorithms, a clustering based feature preselection first eliminates some unrelated and redundant features. Extensive experiments on UCI datasets, face datasets and gene expression datasets demonstrate that these three 2D feature selection algorithms are superior to the state-of-the-art methods indicating the power of our 2D feature selection framework.},
  archive      = {J_TKDE},
  author       = {Juanying Xie and Mingzhao Wang and Philip W. Grant and Witold Pedrycz},
  doi          = {10.1109/TKDE.2024.3388526},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6195-6209},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Feature selection with discernibility and independence criteria},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Fast long sequence time-series forecasting for edge service
running state based on data drift and non-stationarity. <em>TKDE</em>,
<em>36</em>(11), 6181–6194. (<a
href="https://doi.org/10.1109/TKDE.2024.3390135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The operational state of edge services in remote areas and complex geographical work environments is influenced by scarce hardware resources, unstable communication state and external dynamic environment. This leads to the continuous evolution of reliability data streams of edge services in the form of data drift, producing non-stationary data streams with substantial amounts of noise data, resulting in the difficulty of service operational status prediction and low efficiency. Previous studies mainly utilize stationarity-based methods to attenuate the non-stationarity of original sequence, aiming to enhance predictability using deep learning models. However, stationary series deprived of their inherent non-stationarity struggle to accurately forecast practical emergencies. Furthermore, the size and computational complexity of deep learning models are unsuitable for deployment at the edge. To accurately predict the reliability of edge services in dynamic environments, we propose a lightweight and fast long sequence time-series forecasting method based on data drift and non-stationarity, named FSNet. FSNet introduces a non-stationary information sampling factor to extract external factors influencing data flows and incorporates Moore-Penrose inverse matrix to swiftly update the model weights during runtime. Combining unmanned aerial vehicles as mobile edge servers with edge computing offloading achieves collaborative computation of the model, thereby alleviating the scarcity of resources at the edge, enhancing computational efficiency, and enabling fast and reliable prediction of service operations. Extensive experimental results validate the effectiveness and efficiency of the FSNet.},
  archive      = {J_TKDE},
  author       = {Zhiqiang Zhang and Dandan Zhang and Yun Wang},
  doi          = {10.1109/TKDE.2024.3390135},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6181-6194},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fast long sequence time-series forecasting for edge service running state based on data drift and non-stationarity},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast approximated multiple kernel k-means. <em>TKDE</em>,
<em>36</em>(11), 6171–6180. (<a
href="https://doi.org/10.1109/TKDE.2023.3340743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple Kernel Clustering (MKC) has emerged as a prominent research domain in recent decades due to its capacity to exploit diverse information from multiple views by learning an optimal kernel. Despite the successes achieved by various MKC methods, a significant challenge lies in the computational complexity associated with generating a consensus partition from the optimal kernel matrix, typically of size $n \times n$ , where $n$ represents the number of samples. This computational bottleneck restricts the practical applicability of these methods when confronted with large-scale datasets. Furthermore, certain existing MKC algorithms derive the consensus partition matrix by fusing all base partitions. However, this fusion process may inadvertently overlook critical information embedded in individual base kernels, potentially leading to inferior clustering performance. In light of these challenges, we introduce an innovative and efficient multiple kernel $k$ -means approach, denoted as FAMKKM. Notably, FAMKKM incorporates two approximated partition matrices instead of the original individual partition matric for each base kernel. This strategic substitution significantly reduces computational complexity. Additionally, FAMKKM leverages the original kernel information to guide the fusion of all base partitions, thereby enhancing the quality of the resulting consensus partition matrix. Finally, we substantiate the efficacy and efficiency of the proposed FAMKKM through extensive experiments conducted on six benchmark datasets. Our results demonstrate its superiority over state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Jun Wang and Chang Tang and Xiao Zheng and Xinwang Liu and Wei Zhang and En Zhu and Xinzhong Zhu},
  doi          = {10.1109/TKDE.2023.3340743},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6171-6180},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fast approximated multiple kernel K-means},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast and scalable ridesharing search. <em>TKDE</em>,
<em>36</em>(11), 6159–6170. (<a
href="https://doi.org/10.1109/TKDE.2024.3418433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the next few decades, it is estimated that a quarter of all trips worldwide will be served by shared mobility supported in part by lower carbon footprint compared to private mobility. In particular, on-demand ridesharing is appealing due to its convenience, matching passengers needing rides to vehicles in real time while optimizing the matching. While this matching problem is computationally challenging, the state-of-art greedy search algorithm assigns passengers one at a time to the locally best vehicle and has been shown to perform well in practice. However, in order to scale the algorithm, how to parallelize searches for multiple requests remains challenging due to contention for vehicle tours. Moreover, the request latency may still be too high for on-demand requests. In this paper, we give several techniques to speed up and scale out ridesharing search. To deal with data contention while scaling out greedy search, we introduce a “map-release” and ticketing system that sacrifices read-write consistency to achieve high concurrency, even under high contention, and while avoiding expensive aborts incurred by optimistic approaches. To address high request latency, we give a caching technique to speed up the tour expansion subroutine of greedy search, and we also give a pruning technique to reduce the tour candidates even further compared to existing techniques. Together, these techniques deliver around 7x the throughput and order of magnitude lower latency on a real instance compared to the “embarassingly parallel” parallelized map approach and with better scalability.},
  archive      = {J_TKDE},
  author       = {James Jie Pan and Guoliang Li},
  doi          = {10.1109/TKDE.2024.3418433},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6159-6170},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fast and scalable ridesharing search},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring flat minima for domain generalization with large
learning rates. <em>TKDE</em>, <em>36</em>(11), 6145–6158. (<a
href="https://doi.org/10.1109/TKDE.2024.3392980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain Generalization (DG) aims to generalize to arbitrary unseen domains. A promising approach to improve model generalization in DG is the identification of flat minima. One typical method for this task is SWAD, which involves averaging weights along the training trajectory. However, the success of weight averaging depends on the diversity of weights, which is limited when training with a small learning rate. Instead, we observe that leveraging a large learning rate can simultaneously promote weight diversity and facilitate the identification of flat regions in the loss landscape. However, employing a large learning rate suffers from the convergence problem, which cannot be resolved by simply averaging the training weights. To address this issue, we introduce a training strategy called Lookahead which involves the weight interpolation, instead of average, between fast and slow weights. The fast weight explores the weight space with a large learning rate, which is not converged while the slow weight interpolates with it to ensure the convergence. Besides, weight interpolation also helps identify flat minima by implicitly optimizing the local entropy loss that measures flatness. To further prevent overfitting during training, we propose two variants to regularize the training weight with weighted averaged weight or with accumulated history weight. Taking advantage of this new perspective, our methods achieve state-of-the-art performance on both classification and semantic segmentation domain generalization benchmarks.},
  archive      = {J_TKDE},
  author       = {Jian Zhang and Lei Qi and Yinghuan Shi and Yang Gao},
  doi          = {10.1109/TKDE.2024.3392980},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6145-6158},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Exploring flat minima for domain generalization with large learning rates},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Exploring feature selection with limited labels: A
comprehensive survey of semi-supervised and unsupervised approaches.
<em>TKDE</em>, <em>36</em>(11), 6124–6144. (<a
href="https://doi.org/10.1109/TKDE.2024.3397878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a highly regarded research area in the field of data mining, as it significantly enhances the efficiency and performance of high-dimensional data analysis by eliminating redundant and irrelevant features. Despite the ease of data acquisition, labeling data remains a laborious and expensive task. To leverage the abundance of unlabeled data, researchers have proposed various feature selection methods that operate with limited labels, including semi-supervised feature selection and unsupervised feature selection. However, a comprehensive review encompassing a thorough overview of feature selection algorithms with limited labels is lacking. To bridge this gap, this paper conducts a comprehensive exploration of feature selection methods specifically tailored to limited-label scenarios. These methods are systematically classified into two primary categories: semi-supervised and unsupervised feature selection. Additionally, by introducing a novel taxonomy and discussing future challenges, this survey aims to provide researchers with a comprehensive and in-depth understanding of feature selection in limited-label scenarios. Moreover, it aims to offer valuable insights that can guide further research and development in this domain.},
  archive      = {J_TKDE},
  author       = {Guojie Li and Zhiwen Yu and Kaixiang Yang and Mianfen Lin and C. L. Philip Chen},
  doi          = {10.1109/TKDE.2024.3397878},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6124-6144},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Exploring feature selection with limited labels: A comprehensive survey of semi-supervised and unsupervised approaches},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Exploiting duality in aspect sentiment triplet extraction
with sequential prompting. <em>TKDE</em>, <em>36</em>(11), 6111–6123.
(<a href="https://doi.org/10.1109/TKDE.2024.3391381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect sentiment triplet extraction is an important task in natural language processing. Previous work tends to focus on the interaction between the aspect and opinion, while ignoring the positive impact of sentiment on interaction within the triplet. In this paper, we propose a novel aspect sentiment triplet extraction model based on dual learning with sequential prompting. This model is designed as a bidirectional extraction framework that fully takes sentiment polarity into account in the interaction process of aspect and opinion. Besides, we introduce a dual loss as a regularization term for the extraction model to promote better learning in both directions. We further design a sequential prompting strategy to determine aspect, opinion, and sentiment polarity more accurately, which utilizes the results extracted in the previous step as prior knowledge to guide the prediction of the next target. We conduct experiments on three public datasets and the results show the effectiveness of our method. More importantly, we deploy our method on Fliggy application and the 14-day online A/B testing indicates that Page View Click-Through Rate and Page View Conversion Rate increase by 1.17% and 1.08% when user short reviews are used for tagging items with the help of our method.},
  archive      = {J_TKDE},
  author       = {Jingping Liu and Tao Chen and Hao Guo and Chao Wang and Haiyun Jiang and Yanghua Xiao and Xiang Xu and Baohua Wu},
  doi          = {10.1109/TKDE.2024.3391381},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6111-6123},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Exploiting duality in aspect sentiment triplet extraction with sequential prompting},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Expectation distance-based distributional clustering for
noise-robustness. <em>TKDE</em>, <em>36</em>(11), 6099–6110. (<a
href="https://doi.org/10.1109/TKDE.2024.3386401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a clustering technique that reduces the susceptibility to data noise by learning and clustering the data-distribution and then assigning the data to the cluster of its distribution. In the process, it reduces the impact of noise on clustering results. This method involves introducing a new distance among distributions, namely the expectation distance (denoted, ED), that goes beyond the state-of-art distribution distance of optimal mass transport, also called 2-Wasserstein (denoted, $W_{2}$ ): The latter essentially depends only on the marginal distributions while the former also employs the information about the joint distributions, making it more powerful. Using the ED, the paper extends the classical $K$ -means and $K$ -medoids clustering to those over data-distributions (rather than raw-data) and further introduces $K$ -medoids using $W_{2}$ . The paper also presents the closed-form expressions of the $W_{2}$ and ED distance measures. The implementation results of the proposed ED and the $W_{2}$ distance measures to cluster real-world weather data as well as stock data are also presented, which involves efficiently extracting and using the underlying data distributions—Gaussians for weather data versus lognormals for stock data. The results show striking performance improvement over classical clustering of raw-data, with higher accuracy realized for ED. Also, not only does the distribution-based clustering offer higher accuracy, but it also lowers the computation time due to reduced time-complexity.},
  archive      = {J_TKDE},
  author       = {Rahmat Adesunkanmi and Ratnesh Kumar},
  doi          = {10.1109/TKDE.2024.3386401},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6099-6110},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Expectation distance-based distributional clustering for noise-robustness},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EPS<span class="math inline"><sup>2</sup></span>&lt;mml:math
xmlns:mml=“http://www.w3.org/1998/math/MathML”&gt;&lt;mml:msup&gt;&lt;mml:mrow/&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;:
Privacy preserving set-valued data analysis in the shuffle model.
<em>TKDE</em>, <em>36</em>(11), 6084–6098. (<a
href="https://doi.org/10.1109/TKDE.2023.3341171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collecting and analyzing users’ set-valued data with privacy-preserving is a common scenario in real life. However, the existing solutions in LDP are not efficient enough, where users perturbing their data locally introduces a large amount of noise. The shuffle model, which adds a shuffler in LDP to shuffle all perturbed values, can amplify privacy, then improve utility. Inspired by this, we study the frequency estimation and top- $k$ frequent item estimation of set-valued data in the shuffle model. To solve the challenges of different item quantities of users and further improve the utility, we combine sampling and shuffling together, and propose the Encoding, Padding, Sampling, and Shuffling framework, i.e., EPS $^{2}$ . Based on this framework, we propose three protocols for frequency estimation in different application scenarios, then assemble them into multi-phase protocols for the top- $k$ frequent item estimation. Theoretically, we identify all three protocols gain dual privacy amplification from sampling and shuffling. And by setting the size of users’ set to 1, we can extend this amplified bound to the single-valued frequency estimation scenario, producing a tighter privacy bound than existing works. Finally, we perform experiments on both synthetic and real-world datasets to demonstrate the effectiveness of our protocols.},
  archive      = {J_TKDE},
  author       = {Leixia Wang and Qingqing Ye and Haibo Hu and Xiaofeng Meng},
  doi          = {10.1109/TKDE.2023.3341171},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6084-6098},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {EPS$^{2}$&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow/&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;: privacy preserving set-valued data analysis in the shuffle model},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Empowering molecule discovery for molecule-caption
translation with large language models: A ChatGPT perspective.
<em>TKDE</em>, <em>36</em>(11), 6071–6083. (<a
href="https://doi.org/10.1109/TKDE.2024.3393356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecule discovery plays a crucial role in various scientific fields, advancing the design of tailored materials and drugs, which contributes to the development of society and human well-being. Specifically, molecule-caption translation is an important task for molecule discovery, aligning human understanding with molecular space. However, most of the existing methods heavily rely on domain experts, require excessive computational cost, or suffer from sub-optimal performance. On the other hand, Large Language Models (LLMs), like ChatGPT, have shown remarkable performance in various cross-modal tasks due to their powerful capabilities in natural language understanding, generalization, and in-context learning (ICL), which provides unprecedented opportunities to advance molecule discovery. Despite several previous works trying to apply LLMs in this task, the lack of domain-specific corpus and difficulties in training specialized LLMs still remain challenges. In this work, we propose a novel LLM-based framework ( MolReGPT ) for molecule-caption translation, where an In-Context Few-Shot Molecule Learning paradigm is introduced to empower molecule discovery with LLMs like ChatGPT to perform their in-context learning capability without domain-specific pre-training and fine-tuning. MolReGPT leverages the principle of molecular similarity to retrieve similar molecules and their text descriptions from a local database to enable LLMs to learn the task knowledge from context examples. We evaluate the effectiveness of MolReGPT on molecule-caption translation, including molecule understanding and text-based molecule generation. Experimental results show that compared to fine-tuned models, MolReGPT outperforms MolT5-base and is comparable to MolT5-large without additional training. To the best of our knowledge, MolReGPT is the first work to leverage LLMs via in-context learning in molecule-caption translation for advancing molecule discovery. Our work expands the scope of LLM applications, as well as providing a new paradigm for molecule discovery and design.},
  archive      = {J_TKDE},
  author       = {Jiatong Li and Yunqing Liu and Wenqi Fan and Xiao-Yong Wei and Hui Liu and Jiliang Tang and Qing Li},
  doi          = {10.1109/TKDE.2024.3393356},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6071-6083},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Empowering molecule discovery for molecule-caption translation with large language models: A ChatGPT perspective},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient skyline keyword-based tree retrieval on attributed
graphs. <em>TKDE</em>, <em>36</em>(11), 6056–6070. (<a
href="https://doi.org/10.1109/TKDE.2024.3388988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attributed graphs are graphs, where the vertices have attributes. Such graphs encompass, e.g., social network graph, citation graphs, and knowledge graphs, which have numerous real-world applications. Keyword-based search is a prominent and user-friendly way of querying attributed graphs. One widely used approach to keyword search adopts tree-based query semantics that relies on scoring functions that aggregate distances from a root to keyword-matched vertices. However, it is non-trivial to design scoring functions that capture different users’ keyword preferences. This study defines and solves the skyline KTree retrieval problem that combines keyword querying with skyline functionality on attributed graphs. The result of a skyline KTree query is independent of scoring functions. Hence, no matter which keywords are preferred, users can always find their favorite KTrees in a result. To enable efficient skyline KTree retrieval, we propose algorithm $\mathsf {FilterRefine}$ that first identifies candidate results and then uses them for search space pruning. Computing distances between keywords and vertices is expensive and dominates the computational cost of $\mathsf {FilterRefine}$ . Inspired by subspace skyline query techniques, we convert the skyline KTree retrieval problem into a multi-dimensional subspace skyline problem and propose algorithm $\mathsf {MultiDiSkylineOpt}$ . This algorithm is able to reuse skylines in subspaces and uses bounds on all dimensions to accelerate distance computation. Experimental results on real datasets show that a baseline algorithm cannot report results within a 500 second cut-off time, while the proposed algorithms are able to compute results in reasonable time. In particular, $\mathsf {MultiDiSkylineOpt}$ is able to efficiently retrieve skyline KTrees on large graphs with millions of nodes and hundreds of millions of edges.},
  archive      = {J_TKDE},
  author       = {Dingming Wu and Zhaofen Zhang and Christian S. Jensen and Kezhong Lu},
  doi          = {10.1109/TKDE.2024.3388988},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6056-6070},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient skyline keyword-based tree retrieval on attributed graphs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient key-aggregate cryptosystem with user revocation
for selective group data sharing in cloud storage. <em>TKDE</em>,
<em>36</em>(11), 6042–6055. (<a
href="https://doi.org/10.1109/TKDE.2024.3397721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing has become prevalent due to its extensive storage resources and robust computational capacities. To protect data security and privacy, data owners opt for uploading encrypted data to the cloud. Flexible sharing of these encrypted data in a group of users is a critical functionality in cloud storage. In addition, given that users may exit the group, revocation becomes a crucial requirement in group data-sharing systems. The Key-Aggregate Cryptosystem (KAC) has become a promising mechanism for group data sharing. The decryption rights for any set of ciphertexts can be efficiently delegated by distributing a constant-size aggregate key, while the confidentiality of other ciphertexts outside the set is maintained. However, in previous KAC schemes, revocation remains a challenging task regarding key update, ciphertext re-encryption, and collision resistance. In this paper, we propose a Key-Aggregate Cryptosystem with User Revocation (KAC-UR) scheme to overcome this challenge. The KAC-UR scheme not only achieves flexible data sharing, but also can perform secure and efficient user revocation with properties including collision resistance, revocation without data owner-user communication, and constant ciphertext size. The KAC-UR scheme also enables the cloud server to perform partial decryption, thereby significantly alleviating the computational burden for users. The KAC-UR scheme is chosen plaintext attack secure under the decisional Bilinear Diffie-Hellman Exponent assumption.},
  archive      = {J_TKDE},
  author       = {Jinlu Liu and Jing Qin and Xi Zhang and Huaxiong Wang},
  doi          = {10.1109/TKDE.2024.3397721},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6042-6055},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient key-aggregate cryptosystem with user revocation for selective group data sharing in cloud storage},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effective and efficient multi-view imputation with optimal
transport. <em>TKDE</em>, <em>36</em>(11), 6029–6041. (<a
href="https://doi.org/10.1109/TKDE.2024.3387439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-view data with incomplete information hinder effective data analysis. Existing multi-view imputation methods, which learn the mapping between a complete view and a completely missing view, are not able to deal with the typical multi-view data with missing feature information. In this paper, we propose a unified generative imputation model named ${\sf UGit}$ with optimal transport theory to simultaneously impute the missing features/values of all incomplete views. This imputation is conditional on all the observed values from the multi-view data. ${\sf UGit}$ consists of two modules, i.e., a unified multi-view generator (UMG) and a masking energy discriminator (MED). To effectively and efficiently impute missing features across all views, the generator UMG employs a unified autoencoder in conjunction with the cross-view attention mechanism to learn the data distribution from all observed multi-view data. The discriminator MED leverages a novel masking energy divergence function to make ${\sf UGit}$ differentiable for imputation accuracy enhancement. Extensive experiments on several real-world multi-view data sets demonstrate that, ${\sf UGit}$ speeds up the model training by 4.28x with more than 41% accuracy gain on average, compared to the state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Yangyang Wu and Xiaoye Miao and Zi-ang Nan and Jinshan Zhang and Jianhu He and Jianwei Yin},
  doi          = {10.1109/TKDE.2024.3387439},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6029-6041},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Effective and efficient multi-view imputation with optimal transport},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic graph regularization for multi-stream concept drift
self-adaptation. <em>TKDE</em>, <em>36</em>(11), 6016–6028. (<a
href="https://doi.org/10.1109/TKDE.2024.3401156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept drift is an inevitable problem in non-stationary stream environments, due to evolving data distributions. In practical applications, multi-stream is more complex than single-stream, yet they have received little attention. Addressing concept drift while mining correlations between streams poses a significant challenge. Research focuses on capturing correlations between streams using graph neural networks (GNNs), providing valuable insights. However, these methods fix the correlation graph structure after training, failing to adapt to the new data distributions with dynamic correlations during testing. To bridge this gap, we propose a novel framework named Multi-stream Self-adaptation based on Graph Regularization (MSGR). A new GNN architecture is proposed to capture deep spatio-temporal correlations and learn a correlation graph structure without pre-defined graphs. Each graph node represents a stream and the graph is constructed through Gumbel sampling and an adaptive matrix from stream pairs. Thus we attain a base high-performance GNN for multi-stream multi-step prediction. To adapt to the new data distribution during testing, we design a self-adaptation mechanism by assigning dynamic learning weights for newly arriving samples. Larger learning weights are assigned to relevant samples when drift occurs. The self-adaptation is accomplished by the sub-graph updating and the proposed graph regularization. Error-based drift detection is integrated, and when drift occurs, the weight for sub-graph updating increases by adjusting the regularization coefficient. Thus, MSGR maintains high self-adaptation performance and accurate prediction results consistently regardless of the type and degree of concept drift. Comprehensive testing on real-world and synthetic datasets shows that MSGR achieves state-of-the-art performance.},
  archive      = {J_TKDE},
  author       = {Ming Zhou and Jie Lu and Pengqian Lu and Guangquan Zhang},
  doi          = {10.1109/TKDE.2024.3401156},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6016-6028},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dynamic graph regularization for multi-stream concept drift self-adaptation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual variational graph reconstruction learning for social
recommendation. <em>TKDE</em>, <em>36</em>(11), 6002–6015. (<a
href="https://doi.org/10.1109/TKDE.2024.3386895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a new recommendation pattern combining collaborative filtering and social network, social recommender system strives to introduce auxiliary user relations to alleviate data sparsity problems. Considering the graph structure characteristics of user historical interactions and social network, there have been emerged several innovative works that utilize Graph Neural Network (GNN) for social recommendation to show impressive performance. However, existing works seem to be restricted to exploiting social network as auxiliary information for main recommendation tasks, with little attention on the social network itself at the fine-grained level. From empirical perspective, the effectiveness of directly applying social network to social recommendation via GNNs may be limited since the social information that can be used for training is actually sparser than user interactions, and most of observable social information is not valid. To resolve this problem, we propose a Dual Variational Graph Reconstruction Learning (DVGRL) framework for social recommendation. It treats user interaction graph and social network as equivalent and aims to learn both variational distributions of user preferences from historical interactions and social connections, which are trained simultaneously and used to guide the reconstruction of historical interaction graph and social network. To effectively exploit the social information gleaned from reconstruction learning for enhancing recommendation, we design two inter-domain fusion mechanisms to achieve knowledge transfer from the perspectives of attention features and prior distributions, respectively. Extensive experiments on four real-world datasets validate the effectiveness of DVGRL for social recommendation tasks.},
  archive      = {J_TKDE},
  author       = {Yi Zhang and Yiwen Zhang and Yuchuan Zhao and Shuiguang Deng and Yun Yang},
  doi          = {10.1109/TKDE.2024.3386895},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {6002-6015},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dual variational graph reconstruction learning for social recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Distributed rumor source detection via boosted federated
learning. <em>TKDE</em>, <em>36</em>(11), 5986–6001. (<a
href="https://doi.org/10.1109/TKDE.2024.3390238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to localize the rumor source is an extremely important matter for all sectors of the society. Many researchers have tried to use deep-learning-based graph models to detect rumor sources, but they have neglected how to train their deep-learning-based graph models in the noisy social network environment efficiently . Especially for deep learning models, the performance relies on the data scale. However, even though it is known that a substantial amount of rumor data distributed across multiple edge servers (e.g., cross-platform), due to conflicting business interests, its challenging to coordinate all parties to train a model driven by many samples while avoiding moving data. Federated learning, is an effective technique to bridge this gap. Therefore, this paper proposes a D istributed R umor S ource D etection via B oosted F ederated L earning ( DRSDBFL ). Specifically, this paper proposes an effective rumor source detection method based on a deep-learning-based graph model with a denoising module. To the best of our knowledge, we are the first to attempt the use of a denoising module to reduce the noisy effects of social networks. Then, we propose a novel boosted federated learning mechanism through boosting the high-quality edge worker to improve the training efficiency. Finally, the effectiveness of the proposed method is verified by extensive experiments.},
  archive      = {J_TKDE},
  author       = {Ranran Wang and Yin Zhang and Wenchao Wan and Min Chen and Mohsen Guizani},
  doi          = {10.1109/TKDE.2024.3390238},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5986-6001},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Distributed rumor source detection via boosted federated learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed and joint evidential k-nearest neighbor
classification. <em>TKDE</em>, <em>36</em>(11), 5972–5985. (<a
href="https://doi.org/10.1109/TKDE.2023.3341098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of K -nearest neighbor (K-NN) classification depends significantly on the searched neighborhoods of test samples, namely, the neighborhood size K and the used distance metric. For these two issues, many methods either to acquire the adaptive K or to learn a variant metric have been proposed and yielded appropriate performances. However, most of the existing methods ignore the fact that these two factors can be jointly learned. In this paper, we propose a Joint Evidential K-NN algorithm (JEKNN), which learns the adaptive K of each sample and distance metric jointly based on the feedback of error function. To break the computational bottleneck of handling large datasets, a distributed version of JEKNN (JEKNN $_{\mathrm{{dis}}}$ ) is implemented under Apache Spark, i.e., an optimization algorithm based on distributed gradient descent and data parallelism is proposed to accelerate the training stage. Ablation and comparison experiments on small-scale datasets shows the performance improvement from the joint learning and the state-of-the-art accuracy of JEKNN, respectively. Compared to other KNN-based methods designed for Big Data, experimental results on big datasets demonstrate that JEKNN $_{\mathrm{{dis}}}$ achieves better scaling efficiency without significant loss of accuracy. Besides, the generalization error bound of the proposed algorithm is also analyzed theoretically.},
  archive      = {J_TKDE},
  author       = {Chaoyu Gong and Jim Demmel and Yang You},
  doi          = {10.1109/TKDE.2023.3341098},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5972-5985},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Distributed and joint evidential K-nearest neighbor classification},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discovering and maintaining the best <span
class="math inline"><em>k</em></span>&lt;mml:math
xmlns:mml=“http://www.w3.org/1998/math/MathML”&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:math&gt;
in core decomposition. <em>TKDE</em>, <em>36</em>(11), 5954–5971. (<a
href="https://doi.org/10.1109/TKDE.2024.3389989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mode of $k$ -core and its hierarchical decomposition have been applied in many areas, such as sociology, the world wide web, and biology. Algorithms on related studies often need an input value of parameter $k$ , while there is no existing solution other than manual selection. In this paper, given a graph and a scoring metric, we aim to find the best value of $k$ such that the score of the $k$ -core (or $k$ -core set) is the highest. The problem is challenging because there are various community scoring metrics and the computation is costly on large datasets. With the well-designed vertex ordering, we propose time-and-space-optimal algorithms to compute the best $k$ , which are applicable to most community metrics. As real-world networks are often fast-evolving, we also design a novel framework to maintain the best $k$ -core (set) against graph dynamics. We prove the dynamic algorithms are bounded, i.e., the update cost is decided by the changes of input and output. The proposed algorithms can benefit the solutions to $k$ -core-related problems and their dynamic counterparts. Extensive experiments are conducted on 10 real-world networks with size up to billion-scale, which validates the efficiency of our algorithms and the effectiveness of the resulting $k$ -cores.},
  archive      = {J_TKDE},
  author       = {Deming Chu and Fan Zhang and Wenjie Zhang and Xuemin Lin and Ying Zhang and Yinglong Xia and Chenyi Zhang},
  doi          = {10.1109/TKDE.2024.3389989},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5954-5971},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Discovering and maintaining the best $k$&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:math&gt; in core decomposition},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Determining mean first-passage time for random walks on
stochastic uniform growth tree networks. <em>TKDE</em>, <em>36</em>(11),
5940–5953. (<a href="https://doi.org/10.1109/TKDE.2024.3392786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As known, the commonly-utilized ways to determine mean first-passage time $\overline{\mathcal{F}}$ for random walk on networks are mainly based on Laplacian spectra. However, methods of this type can become prohibitively complicated and even fail to work when the Laplacian matrix of network under consideration is difficult to describe in the first place. In this paper, we propose an effective approach to determining quantity $\overline{\mathcal{F}}$ on some widely-studied tree networks. To this end, we first build up a general formula between Wiener index $\mathcal{W}$ and $\overline{\mathcal{F}}$ on a tree. This enables us to convert issues to answer into calculation of $\mathcal{W}$ on networks in question. As opposed to most of previous work focusing on deterministic growth trees, our goal is to consider stochastic case. Towards this end, we establish a principled framework where randomness is introduced into the process of growing trees. As an immediate consequence, the previously published results upon deterministic cases are thoroughly covered by formulas established in this paper. Additionally, it is also straightforward to obtain Kirchhoff index on our tree networks using the proposed approach. Most importantly, our approach is more manageable than some other methods including spectral technique in situations considered herein.},
  archive      = {J_TKDE},
  author       = {Fei Ma and Ping Wang},
  doi          = {10.1109/TKDE.2024.3392786},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5940-5953},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Determining mean first-passage time for random walks on stochastic uniform growth tree networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep hierarchy-aware proxy hashing with self-paced learning
for cross-modal retrieval. <em>TKDE</em>, <em>36</em>(11), 5926–5939.
(<a href="https://doi.org/10.1109/TKDE.2024.3401050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its low storage cost and high retrieval efficiency, hashing technology is popularly applied in both academia and industry, which provides an interesting solution for cross-modal similarity retrieval. However, most existing supervised cross-modal hashing methods typically view the fixed-level semantic affinity defined by manual labels as supervised signals to guide hash learning, which only represents a small subset of complex semantic relations between multi-modal samples, thus impeding the hash function learning and degrading the obtained hash codes. In the paper, by learning shared hierarchy proxies, a novel deep cross-modal hashing framework, called Deep Hierarchy-aware Proxy Hashing (DHaPH), is proposed to construct the semantic hierarchy in a data-driven manner, thereby capturing the accurate fine-grained semantic relationships and achieving small intra-class scatter and big inter-class scatter. Specifically, by regarding the hierarchical proxies as learnable ancestors, a novel hierarchy-aware proxy loss is designed to model the latent semantic hierarchical structures from different modalities without prior hierarchy knowledge, in which similar samples share the same Lowest Common Ancestor (LCA) and dissimilar points have different LCA. Meanwhile, to adequately capture valuable semantic information from hard pairs, a multi-modal self-paced loss is introduced into cross-modal hashing to reweight multi-modal pairs dynamically, which enables the model to gradually focus on hard pairs while simultaneously learning universal patterns from multi-modal pairs. Extensive experiments on three available benchmark databases demonstrate that our proposed DHaPH framework outperforms the compared baselines with different evaluation metrics.},
  archive      = {J_TKDE},
  author       = {Yadong Huo and Qibing Qin and Wenfeng Zhang and Lei Huang and Jie Nie},
  doi          = {10.1109/TKDE.2024.3401050},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5926-5939},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Deep hierarchy-aware proxy hashing with self-paced learning for cross-modal retrieval},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralized privacy preservation for critical connections
in graphs. <em>TKDE</em>, <em>36</em>(11), 5911–5925. (<a
href="https://doi.org/10.1109/TKDE.2024.3406641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world interconnections among entities can be characterized as graphs. Collecting local graph information with balanced privacy and data utility has garnered notable interest recently. This paper delves into the problem of identifying and protecting critical information of entity connections for individual participants in a graph based on cohesive subgraph searches. This problem has not been addressed in the literature. To address the problem, we propose to extract the critical connections of a queried vertex using a fortress-like cohesive subgraph model known as $p$ -cohesion. A user&#39;s connections within a fortress are obfuscated when being released, to protect critical information about the user. Novel merit and penalty score functions are designed to measure each participant&#39;s critical connections in the minimal $p$ -cohesion, facilitating effective identification of the connections. We further propose to preserve the privacy of a vertex enquired by only protecting its critical connections when responding to queries raised by data collectors. We prove that, under the decentralized differential privacy (DDP) mechanism, one&#39;s response satisfies $(\varepsilon, \delta )$ -DDP when its critical connections are protected while the rest remains unperturbed. The effectiveness of our proposed method is demonstrated through extensive experiments on real-life graph datasets.},
  archive      = {J_TKDE},
  author       = {Conggai Li and Wei Ni and Ming Ding and Youyang Qu and Jianjun Chen and David Smith and Wenjie Zhang and Thierry Rakotoarivelo},
  doi          = {10.1109/TKDE.2024.3406641},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5911-5925},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Decentralized privacy preservation for critical connections in graphs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). D&amp;a: Resource optimization in personalized PageRank
computations using multi-core machines. <em>TKDE</em>, <em>36</em>(11),
5905–5910. (<a href="https://doi.org/10.1109/TKDE.2024.3417264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resource optimization is commonly used in workload management, ensuring efficient and timely task completion utilising available resources. It serves to minimise costs, prompting the development of numerous algorithms tailored to this end. The majority of these techniques focus on scheduling and executing workloads effectively within the provided resource constraints. In this paper, we tackle this problem using another approach. We propose a novel framework D&amp;A to determine the number of cores required in completing a workload under time constraint. We first preprocess a small portion of queries to derive the number of required slots, allowing for the allocation of the remaining workloads into each slot. We introduce a scaling factor in handling the time fluctuation issue caused by random functions. We further establish a lower bound of the number of cores required under this scenario, serving as a baseline for comparison purposes. We examine the framework by computing personalized PageRank values involving intensive computations. Our experimental results show that D&amp;A surpasses the baseline, achieving reductions in the required number of cores ranging from $ 38.89\%$ to $ 73.68\%$ across benchmark datasets comprising millions of vertices and edges.},
  archive      = {J_TKDE},
  author       = {Kai Siong Yow and Chunbo Li},
  doi          = {10.1109/TKDE.2024.3417264},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5905-5910},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {D&amp;A: Resource optimization in personalized PageRank computations using multi-core machines},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CStream: Parallel data stream compression on multicore edge
devices. <em>TKDE</em>, <em>36</em>(11), 5889–5904. (<a
href="https://doi.org/10.1109/TKDE.2024.3386862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the burgeoning realm of Internet of Things (IoT) applications on edge devices, data stream compression has become increasingly pertinent. The integration of added compression overhead and limited hardware resources on these devices calls for a nuanced software-hardware co-design. This paper introduces CStream , a pioneering framework crafted for parallelizing stream compression on multicore edge devices. CStream grapples with the distinct challenges of delivering a high compression ratio, high throughput, low latency, and low energy consumption. Notably, CStream distinguishes itself by accommodating an array of stream compression algorithms, a variety of hardware architectures and configurations, and an innovative set of parallelization strategies, some of which are proposed herein for the first time. Our evaluation showcases the efficacy of a thoughtful co-design involving a lossy compression algorithm, asymmetric multicore processors, and our novel, hardware-conscious parallelization strategies. This approach achieves a $2.8 \times$ compression ratio with only marginal information loss, $4.3 \times$ throughput, 65% latency reduction and 89% energy consumption reduction, compared to designs lacking such strategic integration.},
  archive      = {J_TKDE},
  author       = {Xianzhi Zeng and Shuhao Zhang},
  doi          = {10.1109/TKDE.2024.3386862},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5889-5904},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CStream: Parallel data stream compression on multicore edge devices},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CS-DAHIN: Community search over dynamic attribute
heterogeneous network. <em>TKDE</em>, <em>36</em>(11), 5874–5888. (<a
href="https://doi.org/10.1109/TKDE.2024.3402258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community search (CS) is an important research topic in network analysis, which aims to find a subgraph that satisfies the given conditions. A dynamic attribute heterogeneous information network (DAHIN) is a sequence of attribute heterogeneous information network (AHIN) snapshots, where each snapshot consists of multiple types of vertices as well as edges, and each vertex is associated a set of attribute keywords. CS over DAHIN faces many challenges. In this paper, we study the CS problem over DAHINs, aiming to search for cohesive subgraphs containing query vertex and simultaneously satisfying the connectivity, attribute cohesiveness and interaction stability. To this end, we propose a deep learning model with a three-level attention mechanism and the concept of interaction frequency with respect to multiple semantic relationships to measure the similarity of attributes and the stability of interactions between vertices respectively. In addition, we design three search algorithms to locate the target community by optimizing the degree of interaction stability and attribute similarity between vertices. Extensive experiments, including comparison with existing algorithms, ablation analysis, parameter sensitivity examination, and case studies, are conducted on four real-world datasets to validate the effectiveness and efficiency of the proposed model and search algorithms. The code and model of CS-DAHIN will be open source on GitHub.},
  archive      = {J_TKDE},
  author       = {Yixin Song and Lihua Zhou and Peizhong Yang and Jialong Wang and Lizhen Wang},
  doi          = {10.1109/TKDE.2024.3402258},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5874-5888},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CS-DAHIN: Community search over dynamic attribute heterogeneous network},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-lingual cross-modal retrieval with noise-robust
fine-tuning. <em>TKDE</em>, <em>36</em>(11), 5860–5873. (<a
href="https://doi.org/10.1109/TKDE.2024.3400060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-lingual cross-modal retrieval aims at leveraging human-labeled annotations in a source language to construct cross-modal retrieval models for a new target language, due to the lack of manually-annotated dataset in low-resource languages (target languages). Contrary to the growing developments in the field of monolingual cross-modal retrieval, there has been less research focusing on cross-modal retrieval in the cross-lingual scenario. A straightforward method to obtain target-language labeled data is translating source-language datasets utilizing Machine Translations (MT). However, as MT is not perfect, it tends to introduce noise during translation, rendering textual embeddings corrupted and thereby compromising the retrieval performance. To alleviate this, we propose Noise-Robust Fine-tuning (NRF) which tries to extract clean textual information from a possibly noisy target-language input with the guidance of its source-language counterpart. Besides, contrastive learning involving different modalities are performed to strengthen the noise-robustness of our model. Different from traditional cross-modal retrieval methods which only employ image/video-text paired data for fine-tuning, in NRF, selected parallel data plays a key role in improving the noise-filtering ability of our model. Extensive experiments are conducted on three video-text and image-text retrieval benchmarks across different target languages, and the results demonstrate that our method significantly improves the overall performance without using any image/video-text paired data on target languages.},
  archive      = {J_TKDE},
  author       = {Rui Cai and Jianfeng Dong and Tianxiang Liang and Yonghui Liang and Yabing Wang and Xun Yang and Xun Wang and Meng Wang},
  doi          = {10.1109/TKDE.2024.3400060},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5860-5873},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cross-lingual cross-modal retrieval with noise-robust fine-tuning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Counter-empirical attacking based on adversarial
reinforcement learning for time-relevant scoring system. <em>TKDE</em>,
<em>36</em>(11), 5849–5859. (<a
href="https://doi.org/10.1109/TKDE.2023.3341430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scoring systems are commonly seen for platforms in the era of Big Data. From credit scoring systems in financial services to membership scores in E-commerce shopping platforms, platform managers use such systems to guide users towards the encouraged activity pattern, and manage resources more effectively and efficiently. To establish such scoring systems, several “empirical criteria” are first determined, followed by a dedicated top-down design for each score factor, which usually requires enormous effort to adjust and tune the scoring function in the new application scenario. What&#39;s worse, many fresh projects usually have no ground truth or any experience to evaluate a reasonable scoring system, making the designing even harder. To reduce the effort of manual adjustment of the scoring function in every new scoring system, we innovatively study the scoring system from the preset empirical criteria without any ground truth and propose a novel framework to improve the system from scratch. In this paper, we propose a “counter-empirical attacking” mechanism that can generate “attacking” behavior traces and try to break the empirical rules of the scoring system. Then an adversarial “enhancer” is applied to evaluate the scoring system and find the improvement strategy. By training the adversarial learning problem, a proper scoring function can be learned to be robust to the attacking activity traces that are trying to violate the empirical criteria. Extensive experiments have been conducted on two scoring systems, including a shared computing resource platform and a financial credit system. The experimental results have validated the effectiveness of our proposed framework.},
  archive      = {J_TKDE},
  author       = {Xiangguo Sun and Hong Cheng and Hang Dong and Bo Qiao and Si Qin and Qingwei Lin},
  doi          = {10.1109/TKDE.2023.3341430},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5849-5859},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Counter-empirical attacking based on adversarial reinforcement learning for time-relevant scoring system},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contrastive multi-bit collaborative learning for deep
cross-modal hashing. <em>TKDE</em>, <em>36</em>(11), 5835–5848. (<a
href="https://doi.org/10.1109/TKDE.2024.3419577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep cross-modal hashing, as a promising fast similarity search technique, has attracted broad interest and obtained great success owing to its outstanding representation capability and computational efficiency. Since the inconsistent feature representations and distributions of different modalities (i.e., image and text), prior studies primarily focus on preserving pairwise similarity with global embedding, but fail to further utilize detailed local representations to effectively align such heterogeneous data to jointly bridge the heterogeneous and semantic gaps across modalities. Meanwhile, typical learning networks can learn only one fixed-length hash code rather than multi-length ones, leading to extremely limited flexibility and scalability. To tackle these issues, this paper proposes a novel Contrastive Multi-bit Collaborative Learning (CMCL) network, which hierarchically aligns both global and local features among different modalities and simultaneously generates multi-length hash codes (i.e., 16-, 32-, 64-bits) in one unified transformer-based framework. Specifically, we design a novel cross-modal contrastive alignment module to simultaneously bridge the heterogeneous and semantic gaps across modalities via global and local contrastive learning. Moreover, we propose a multi-bit collaborative optimization module to synchronously produce multi-length hash codes under the explicit guidance of one auxiliary online hash learner with a longer length (i.e., 128-bit). As such, our CMCL framework can jointly alleviate the heterogeneity among modalities from a hierarchical perspective and collaboratively explore the correlations between multi-bit hash codes, thereby yielding multi-length discriminative hash codes in a one-stop learning manner. Comprehensive experiments demonstrate the consistent superiority of our CMCL in multi-bit hash code learning over the state-of-the-art cross-modal hashing baselines.},
  archive      = {J_TKDE},
  author       = {Qingpeng Wu and Zheng Zhang and Yishu Liu and Jingyi Zhang and Liqiang Nie},
  doi          = {10.1109/TKDE.2024.3419577},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5835-5848},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Contrastive multi-bit collaborative learning for deep cross-modal hashing},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contrastive incomplete cross-modal hashing. <em>TKDE</em>,
<em>36</em>(11), 5823–5834. (<a
href="https://doi.org/10.1109/TKDE.2024.3410388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of current deep cross-modal hashing admits a default assumption of the fully-observed cross-modal data. However, such a rigorous common policy is hardly guaranteed for practical large-scale cases, which directly disable the training of prevalent cross-modal retrieval methods with incomplete cross-modal instances and unpaired relations. The main challenges come from the collapsed semantic- and modality-level similarity learning as well as uncertain cross-modal correspondence. In this paper, we propose a Contrastive Incomplete Cross-modal Hashing (CICH) network, which simultaneously determines the cross-modal semantic coordination, unbalanced similarity calibration, and contextual correspondence alignment. Specifically, we design a prototypical semantic similarity coordination module to globally rebuild partially-observed cross-modal similarities under an asymmetric learning scheme. Meanwhile, a semantic-aware contrastive hashing module is established to adaptively perceive and remedy the unbalanced similarities across different modalities with the semantic transition for generating discriminative hash codes. Additionally, a contextual correspondence alignment module is conceived to maximally capture shared knowledge across modalities and eliminate the correspondence uncertainty via a dual contextual information bottleneck formula. To the best of our knowledge, this is the first successful attempt of enabling contrastive learning to incomplete deep cross-modal hashing. Extensive experiments validate the superiority of our CICH against state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Haoyang Luo and Zheng Zhang and Liqiang Nie},
  doi          = {10.1109/TKDE.2024.3410388},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5823-5834},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Contrastive incomplete cross-modal hashing},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ContCommRTD: A distributed content-based
misinformation-aware community detection system for real-time disaster
reporting. <em>TKDE</em>, <em>36</em>(11), 5811–5822. (<a
href="https://doi.org/10.1109/TKDE.2024.3417232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time social media data can provide useful information on evolving hazards. Alongside traditional methods of disaster detection, the integration of social media data can considerably enhance disaster management. In this paper, we investigate the problem of detecting geolocation-content communities on Twitter and propose a novel distributed system that provides in near real-time information on hazard-related events and their evolution. We show that content-based community analysis can lead to better and faster dissemination of hazard-related reports than using only traditional methods, such as satellite or airborne sensing platforms. Our distributed disaster reporting system analyzes the social relationship among worldwide geolocated tweets and applies topic modeling to group tweets by topics. Considering for each tweet the following information: user, timestamp, geolocation, retweets, and replies, we create a publisher-subscriber distribution model for topics. We use content similarity and the proximity of nodes to create a new model for geolocation-content based communities. Users can subscribe to different topics in specific geographical areas or worldwide and receive real-time reports regarding these topics. As misinformation can lead to increased damage if propagated in hazards-related tweets, we propose a new deep learning model to detect fake news. The misinformed tweets are then removed from display. We also show empirically the scalability capabilities of the proposed system.},
  archive      = {J_TKDE},
  author       = {Elena-Simona Apostol and Ciprian-Octavian Truică and Adrian Paschke},
  doi          = {10.1109/TKDE.2024.3417232},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5811-5822},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {ContCommRTD: A distributed content-based misinformation-aware community detection system for real-time disaster reporting},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Confidence-induced granular partial label feature selection
via dependency and similarity. <em>TKDE</em>, <em>36</em>(11),
5797–5810. (<a href="https://doi.org/10.1109/TKDE.2024.3405489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial label learning (PLL) tackles scenarios where the unique ground-truth label of each sample is concealed within a candidate label set. Dimensionality reduction, considering labeling confidence estimation, has become a promising strategy to enhance the generalization performance of PLL models. However, current studies achieve dimensionality reduction, often relying on kNN-based labeling confidence estimation or disregarding potential labeling information. To address this issue, this paper proposes a novel Confidence-induced granular Partial label feature selection method using Dependency and Similarity (CPDS), which consists of two phases: Labeling Confidence Estimation (LCE) and Feature Selection (FS). For LCE, through granular ball computing, the feature space&#39;s similarity and the label space&#39;s correlation between the training data and the granular ball can be fused simultaneously, thereby effectively reconstructing more credible labeling confidence from candidate labels with more diverse semantic representation information. In the FS stage, by leveraging the LC with more diverse information, the proposed PLL neighborhood decision system further effectively combines feature dependency and label similarity to identify a feature subset with more discriminative capabilities, thereby achieving better performance for classification tasks. Among them, feature dependency effectively utilizes the dependency between neighborhoods and equivalence relations, while label similarity fully exploits the similarity between each sample and its neighbors. Extensive experiments show that CPDS significantly outperforms the compared approaches in most cases on nine controlled UCI datasets and five real-world datasets, demonstrating the superiority of the proposed method.},
  archive      = {J_TKDE},
  author       = {Wenbin Qian and Yihui Li and Qianzhi Ye and Shuyin Xia and Jintao Huang and Weiping Ding},
  doi          = {10.1109/TKDE.2024.3405489},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5797-5810},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Confidence-induced granular partial label feature selection via dependency and similarity},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Concept factorization based multiview clustering for
large-scale data. <em>TKDE</em>, <em>36</em>(11), 5784–5796. (<a
href="https://doi.org/10.1109/TKDE.2024.3392209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing large-scale multiview clustering algorithms attempt to capture data distribution in multiple views by selecting view-wise anchor representations beforehand with $k$ -means, or by direct matrix factorization on the original observations. Despite impressive performance, few of them have paid attention to the semantic correlations between anchor bases and cluster centroids, or even the underlying relations between clusters and data samples. In view of this, we propose a C oncept F actorization based M ultiview C lustering for Large-scale Data (CFMC) method with nearly linear complexity. The anchor bases learning, coefficient expression with clear semantic cues and partitioning are integrated together in this unified model. Meanwhile, explicit connections among multiview data, anchor bases and clusters are modeled via coefficient representations with semantic meanings. A four-step alternate minimizing algorithm is designed to handle the optimization problem, which is proved to have linear time complexity w.r.t. the sample size. Extensive experiments conducted on several challenging large-scale datasets confirm the superiority of the method compared with the state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Man-Sheng Chen and Chang-Dong Wang and Dong Huang and Jian-Huang Lai and Philip S. Yu},
  doi          = {10.1109/TKDE.2024.3392209},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5784-5796},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Concept factorization based multiview clustering for large-scale data},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collaborative learning with heterogeneous local models: A
rule-based knowledge fusion approach. <em>TKDE</em>, <em>36</em>(11),
5768–5783. (<a href="https://doi.org/10.1109/TKDE.2023.3341808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) has emerged as a promising collaborative learning paradigm that enables to train machine learning models across decentralized devices, while keeping the training data localized to preserve user privacy. However, the heterogeneity in both decentralized training data and distributed computing resources has posed significant challenges to the design of effective and efficient FL schemes. Most existing solutions either focus on tackling a single type of heterogeneity, or are unable to fully support model heterogeneity with low communication overhead, fast convergence, and good interpretability. In this paper, we present CloREF, a novel rule-based collaborative learning framework that allows devices in FL to use completely different local learning models to cater to both data and resource heterogeneity. In CloREF, each rule is represented as a linear model, which provides good interpretability. Each participating device chooses a local model and trains it using its local data. The decision boundary of each trained local model is then approximated using a set of rules, which effectively bridges the gap arising from model heterogeneity. All participating devices collaborate to select the optimal set of rules as the global model, employing evolutionary optimization to effectively fuse the knowledge acquired from all local models. Experimental results on both synthesized and real-world datasets demonstrate that the rules generated by our proposed method can mimic the behaviors of various learning models with high fidelity ( $&amp;gt; $ 0.95 in most tests), and CloREF gives competitive performance in accuracy, AUC, and communication overhead, compared with both the best-performing model trained centrally and several state-of-the-art model-heterogeneous federated learning schemes.},
  archive      = {J_TKDE},
  author       = {Ying Pang and Haibo Zhang and Jeremiah D. Deng and Lizhi Peng and Fei Teng},
  doi          = {10.1109/TKDE.2023.3341808},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5768-5783},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Collaborative learning with heterogeneous local models: A rule-based knowledge fusion approach},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CIPPO: Contrastive imitation proximal policy optimization
for recommendation based on reinforcement learning. <em>TKDE</em>,
<em>36</em>(11), 5753–5767. (<a
href="https://doi.org/10.1109/TKDE.2024.3402649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems, widely adopted in social networks, personalize user experiences through advanced technologies such as Reinforcement Learning (RL), known for producing high-performance, list-wise recommendations. However, RL-based recommendation methods exhibit biases, specifically: 1) Online bias, which stems from a complex real-world online policy composed of various rules and models rather than a single policy; 2) Training bias, a distributional shift resulting from differences between the target policy and the behavior policy . To address these issues, we introduce a novel framework named Contrastive Imitation Proximal Policy Optimization (CIPPO) for recommendation based on RL. This approach leverages extensively labeled feedback data and incorporates a Masked Imitation Network (MIN) that closely emulates the online policy, thus reducing discrepancies between online and offline environments. Additionally, the clipping function in Proximal Policy Optimization, combined with a specially designed contrastive module, effectively reduces the distributional shift between the behavior and target policies. We conduct offline and online experiments to show the improvements of CIPPO, providing details including ablation tests and parameter analysis to validate the effectiveness and robustness. CIPPO gains 12.79% on ACN and in WeChat Top Stories, a large media platform with over 50 million users.},
  archive      = {J_TKDE},
  author       = {Weilong Chen and Shaoliang Zhang and Ruobing Xie and Feng Xia and Leyu Lin and Xinran Zhang and Yan Wang and Yanru Zhang},
  doi          = {10.1109/TKDE.2024.3402649},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5753-5767},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CIPPO: Contrastive imitation proximal policy optimization for recommendation based on reinforcement learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). <span class="math inline">CheetahTraj</span>&lt;Mml:math
xmlns:mml=“http://www.w3.org/1998/math/MathML”&gt;&lt;mml:mi
mathvariant=“sans-serif”&gt;CheetahTraj&lt;/mml:mi&gt;&lt;/mml:math&gt;:
Efficient visualization for large trajectory dataset with quality
guarantee. <em>TKDE</em>, <em>36</em>(11), 5737–5752. (<a
href="https://doi.org/10.1109/TKDE.2024.3387480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualizing large-scale trajectory dataset is a core subroutine for many applications. However, rendering all trajectories could result in severe visual clutter and incur long visualization delays due to large data volume. Naively sampling the trajectories reduces visualization time but usually harms visual quality, i.e., the generated visualizations may look substantially different from the exact ones without sampling. In this paper, we propose $\mathsf {CheetahTraj}$ , a principled sampling framework that achieves both high visualization quality and low visualization latency. We first define the visual quality function measuring the similarity between two visualizations, based on which we formulate the quality optimal sampling problem ( ${\sf QOSP}$ ). To solve ${\sf QOSP}$ , we design the V isual Q uality G uaranteed S ampling algorithms, which reduce visual clutter while guaranteeing visual quality by considering both trajectory data distribution and human perception properties. We also develop a quad-tree-based index ( $\mathsf {InvQuad}$ ) that allows using trajectory samples computed offline for interactive online visualization. Extensive experiments including case-, user-, and quantitative-studies are conducted on three real-world trajectory datasets, and the results show that $\mathsf {CheetahTraj}$ consistently provides higher visual quality and better efficiency than baseline methods. Compared with visualizing all trajectories, $\mathsf {CheetahTraj}$ reduces the visualization latency by up to 3 orders of magnitude while avoiding visual clutter.},
  archive      = {J_TKDE},
  author       = {Qiaomu Shen and Chaozu Zhang and Xiao Yan and Chuan Yang and Dan Zeng and Wei Zeng and Bo Tang},
  doi          = {10.1109/TKDE.2024.3387480},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5737-5752},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {$\mathsf {CheetahTraj}$&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mml:mi mathvariant=&quot;sans-serif&quot;&gt;CheetahTraj&lt;/mml:mi&gt;&lt;/mml:math&gt;: efficient visualization for large trajectory dataset with quality guarantee},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Calibrated one-class classification for unsupervised time
series anomaly detection. <em>TKDE</em>, <em>36</em>(11), 5723–5736. (<a
href="https://doi.org/10.1109/TKDE.2024.3393996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series anomaly detection is instrumental in maintaining system availability in various domains. Current work in this research line mainly focuses on learning data normality deeply and comprehensively by devising advanced neural network structures and new reconstruction/prediction learning objectives. However, their one-class learning process can be misled by latent anomalies in training data (i.e., anomaly contamination) under the unsupervised paradigm. Their learning process also lacks knowledge about the anomalies. Consequently, they often learn a biased, inaccurate normality boundary. To tackle these problems, this paper proposes calibrated one-class classification for anomaly detection, realizing contamination-tolerant, anomaly-informed learning of data normality via uncertainty modeling-based calibration and native anomaly-based calibration. Specifically, our approach adaptively penalizes uncertain predictions to restrain irregular samples in anomaly contamination during optimization, while simultaneously encouraging confident predictions on regular samples to ensure effective normality learning. This largely alleviates the negative impact of anomaly contamination. Our approach also creates native anomaly examples via perturbation to simulate time series abnormal behaviors. Through discriminating these dummy anomalies, our one-class learning is further calibrated to form a more precise normality boundary. Extensive experiments on ten real-world datasets show that our model achieves substantial improvement over sixteen state-of-the-art contenders.},
  archive      = {J_TKDE},
  author       = {Hongzuo Xu and Yijie Wang and Songlei Jian and Qing Liao and Yongjun Wang and Guansong Pang},
  doi          = {10.1109/TKDE.2024.3393996},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5723-5736},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Calibrated one-class classification for unsupervised time series anomaly detection},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Block-diagonal guided DBSCAN clustering. <em>TKDE</em>,
<em>36</em>(11), 5709–5722. (<a
href="https://doi.org/10.1109/TKDE.2024.3401075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster analysis constitutes a pivotal component of database mining, with DBSCAN being one of the most extensively employed algorithms in this domain. Nevertheless, DBSCAN is encumbered by several limitations, including challenges in processing high-dimensional datasets, a pronounced sensitivity to input parameters, and inconsistencies in generating reliable clustering outcomes. This paper presents a refined version of DBSCAN that utilizes the block-diagonal property of similarity graphs to enhance the clustering process. The core concept involves the construction of a graph that assesses the similarity among high-dimensional data points, capable of transformation into a block-diagonal form via an unknown permutation. This is followed by a cluster-ordering procedure that establishes the requisite permutation, thereby facilitating the straightforward identification of clustering structures through the recognition of diagonal blocks in the permuted graph. The principal obstacle addressed in this study is the construction of a graph that inherently possesses a block-diagonal structure, the permutation of this graph to actualize such a structure, and the autonomous identification of diagonal blocks within the permuted graph. To surmount these challenges, we initially devise a block-diagonal constrained self-representation model to create a similarity graph that exhibits a block-diagonal form post-permutation. A gradient descent-based methodology is proposed to resolve this problem effectively. Concurrently, we engineer a traversal algorithm, inspired by DBSCAN, that discerns clusters of high density within the graph and generates an enhanced cluster ordering. The attainment of a block-diagonal structure is then realized through permutation aligned with the traversal sequence, laying a robust foundation for both automated and interactive cluster analysis. Moreover, a novel split-and-refine algorithm is introduced to autonomously identify all diagonal blocks within the permuted graph, offering theoretical optimality under specific conditions. Extensive evaluations of our method across twelve rigorous real-world benchmark datasets affirm its superiority over contemporary state-of-the-art clustering techniques.},
  archive      = {J_TKDE},
  author       = {Zheng Xing and Weibing Zhao},
  doi          = {10.1109/TKDE.2024.3401075},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5709-5722},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Block-diagonal guided DBSCAN clustering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bi-level graph structure learning for next POI
recommendation. <em>TKDE</em>, <em>36</em>(11), 5695–5708. (<a
href="https://doi.org/10.1109/TKDE.2024.3397683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next point-of-interest (POI) recommendation aims to predict a user&#39;s next destination based on sequential check-in history and a set of POI candidates. Graph neural networks (GNNs) have demonstrated a remarkable capability in this endeavor by exploiting the extensive global collaborative signals present among POIs. However, most of the existing graph-based approaches construct graph structures based on pre-defined heuristics, failing to consider inherent hierarchical structures of POI features such as geographical locations and visiting peaks, or suffering from noisy and incomplete structures in graphs. To address the aforementioned issues, this paper presents a novel Bi -level G raph S tructure L earning ( ${\sf BiGSL}$ ) for next POI recommendation. ${\sf BiGSL}$ first learns a hierarchical graph structure to capture the fine-to-coarse connectivity between POIs and prototypes, and then uses a pairwise learning module to dynamically infer relationships between POI pairs and prototype pairs. Based on the learned bi-level graphs, our model then employs a multi-relational graph network that considers both POI- and prototype-level neighbors, resulting in improved POI representations. Our bi-level structure learning scheme is more robust to data noise and incompleteness, and improves the exploration ability for recommendation by alleviating sparsity issues. Experimental results on three real-world datasets demonstrate the superiority of our model over existing state-of-the-art methods, with a significant improvement in recommendation accuracy and exploration performance.},
  archive      = {J_TKDE},
  author       = {Liang Wang and Shu Wu and Qiang Liu and Yanqiao Zhu and Xiang Tao and Mengdi Zhang and Liang Wang},
  doi          = {10.1109/TKDE.2024.3397683},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5695-5708},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Bi-level graph structure learning for next POI recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bilateral multi-behavior modeling for reciprocal
recommendation in online recruitment. <em>TKDE</em>, <em>36</em>(11),
5681–5694. (<a href="https://doi.org/10.1109/TKDE.2024.3397705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the rapid development of online recruitment platforms, which provide a convenient way for matching job seekers and recruiters. Indeed, this is a reciprocal recommendation problem which needs to consider the preferences of both job seekers and recruiters simultaneously. Existing studies mainly focus on building recommendation models based on text matching or collaborative filtering methods. However, we propose that these methods are limited and insufficient, since the abundant multi-typed bilateral behaviors among users have been largely ignored. Therefore, in this paper, we propose a novel BilAteral Multi-BehaviOr mOdeling (BAMBOO) method for reciprocal recommendation in online recruitment, which can model the multi-typed interactions from expectation perspective and competitiveness perspective . Specifically, for the expectation perspective, we propose to format the historical behaviors of different users as bilateral multi-behavior sequences, and we utilize a transformer-based model to learn the representations of what the users want to obtain. For the competitiveness perspective, we propose to construct a bilateral interaction heterogeneous graph to describe the entire recruitment market, and further utilize a heterogeneous graph transformer-based model to learn the representations of what the users can obtain. Moreover, we utilize contrastive learning methods to enhance these two modules. Furthermore, we propose to decompose the matching probability into the product of two parts, and we train our model based on a multi-task learning strategy. We conduct both offline experiments on real-world datasets and online A/B test, and the experiment results validate the effectiveness of our BAMBOO model compared with several state-of-the-art baseline methods.},
  archive      = {J_TKDE},
  author       = {Zhi Zheng and Xiao Hu and Zhaopeng Qiu and Yuan Cheng and Shanshan Gao and Yang Song and Hengshu Zhu and Hui Xiong},
  doi          = {10.1109/TKDE.2024.3397705},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5681-5694},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Bilateral multi-behavior modeling for reciprocal recommendation in online recruitment},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bidirectional fusion with cross-view graph filter for
multi-view clustering. <em>TKDE</em>, <em>36</em>(11), 5675–5680. (<a
href="https://doi.org/10.1109/TKDE.2024.3413682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing multi-view graph clustering models either seek consistent clustering results from similarity matrices and spectral embeddings respectively or follow direct bidirectional integration of them, which ignores the interaction between them. To make up for this flaw, this paper designs a novel multi-view clustering model that performs B idirectional F usion with C ross-view G raph F ilter (BF-CGF). To be specific, BF-CGF first learns a consistent graph embedding via performing the interaction between multi-view graphs and spectral embeddings with the perspective of the graph spectral domain and then considers seeking a consistent indicator matrix via the graph cut model from the consistent graph embedding and the similarity matrices. To solve the optimization problem of BF-CGF, we propose an efficient iterative algorithm and provide the corresponding convergence and complexity analyses. Extensive experimental results demonstrate that the proposed BF-CGF outperforms state-of-the-art competitors in most benchmark datasets.},
  archive      = {J_TKDE},
  author       = {Xiaojun Yang and Tuoji Zhu and Danyang Wu and Penglei Wang and Yujia Liu and Feiping Nie},
  doi          = {10.1109/TKDE.2024.3413682},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5675-5680},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Bidirectional fusion with cross-view graph filter for multi-view clustering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). BADFL: Backdoor attack defense in federated learning from
local model perspective. <em>TKDE</em>, <em>36</em>(11), 5661–5674. (<a
href="https://doi.org/10.1109/TKDE.2024.3420778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is substantial attention to federated learning with its ability to train a powerful global model collaboratively while protecting data privacy. Despite its many advantages, federated learning is vulnerable to backdoor attacks, where an adversary injects malicious weights into the global model, making the global model&#39;s targeted predictions incorrect. Existing defenses based on identifying and eliminating malicious weights ignore the similarity variation of the local weights during iterations in the malicious model detection and the presence of benign weights in the malicious model during the malicious local weight elimination, resulting in a poor defense and a degradation of global model accuracy. In this paper, we defend against backdoor attacks from the perspective of local models. First, a malicious model detection method based on interpretability techniques is proposed. The method appends a sampling check after clustering to identify malicious models accurately. We further design a malicious local weight elimination method based on local weight contributions. This method preserves the benign weights in the malicious model to maintain their contributions to the global model. Finally, we analyze the security of the proposed method in terms of model closeness and then verify the effectiveness of the proposed method through experiments. In comparison with existing defenses, the results show that BADFL improves the global model accuracy by 23.14% while reducing the attack success rate to 0.04% in the best case.},
  archive      = {J_TKDE},
  author       = {Haiyan Zhang and Xinghua Li and Mengfan Xu and Ximeng Liu and Tong Wu and Jian Weng and Robert H. Deng},
  doi          = {10.1109/TKDE.2024.3420778},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5661-5674},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {BADFL: Backdoor attack defense in federated learning from local model perspective},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AutoSR: Automatic sequential recommendation system design.
<em>TKDE</em>, <em>36</em>(11), 5647–5660. (<a
href="https://doi.org/10.1109/TKDE.2024.3400031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {S equential R ecommendation (SR) System emerged recently as a powerful tool for suggesting users with the next item of interest. Despite their great success, the design of SR systems requires heavy manual work and domain knowledge. In this paper, we present $\mathbf {AutoSR}$ , an effective Auto mated M achine L earning (AutoML) tool that enables automatic design of powerful SR systems based on G raph N eural N etwork (GNN) and R einforcement L earning (RL). In $\mathbf {AutoSR}$ , we summarize the design process of the SR systems and extract effective operations from the existing SR systems to construct our search space. Such an experience-based search space generates diverse SR systems by integrating effective operations of different systems, providing a basic condition for the implementation of AutoML. Besides, we propose a graph-based RL method to efficiently explore the SR search space, where operations have complex and diverse application conditions. Compared with the existing AutoML methods, which ignore potential relations among operations, $\mathbf {AutoSR}$ can greatly avoid invalid SR system design and efficiently discover more powerful SR systems by analyzing the relation graph of various operations. Extensive experimental results show that $\mathbf {AutoSR}$ can gain powerful SR systems, superior to the existing $\mathbf {AutoSR}$ systems used for search space construction. Besides, $\mathbf {AutoSR}$ is more efficient than the existing AutoML algorithms in SR system design, which demonstrate the superiority of $\mathbf {AutoSR}$ .},
  archive      = {J_TKDE},
  author       = {Chunnan Wang and Hongzhi Wang and Junzhe Wang and Guosheng Feng},
  doi          = {10.1109/TKDE.2024.3400031},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5647-5660},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {AutoSR: Automatic sequential recommendation system design},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Attention-aware social graph transformer networks for
stochastic trajectory prediction. <em>TKDE</em>, <em>36</em>(11),
5633–5646. (<a href="https://doi.org/10.1109/TKDE.2024.3390765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory prediction is fundamental to various intelligent technologies, such as autonomous driving and robotics. The motion prediction of pedestrians and vehicles helps emergency braking, reduces collisions, and improves traffic safety. Current trajectory prediction research faces problems of complex social interactions, high dynamics and multi-modality. Especially, it still has limitations in long-time prediction. We propose Attention-aware Social Graph Transformer Networks for multi-modal trajectory prediction. We combine Graph Convolutional Networks and Transformer Networks by generating stable resolution pseudo-images from Spatio-temporal graphs through a designed stacking and interception method. Furthermore, we design the attention-aware module to handle social interaction information in scenarios involving mixed pedestrian-vehicle traffic. Thus, we maintain the advantages of the Graph and Transformer, i.e., the ability to aggregate information over an arbitrary number of neighbors and the ability to perform complex time-dependent data processing. We conduct experiments on datasets involving pedestrian, vehicle, and mixed trajectories, respectively. Our results demonstrate that our model minimizes displacement errors across various metrics and significantly reduces the likelihood of collisions. It is worth noting that our model effectively reduces the final displacement error, illustrating the ability of our model to predict for a long time.},
  archive      = {J_TKDE},
  author       = {Yao Liu and Binghao Li and Xianzhi Wang and Claude Sammut and Lina Yao},
  doi          = {10.1109/TKDE.2024.3390765},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5633-5646},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Attention-aware social graph transformer networks for stochastic trajectory prediction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Application of gradient boosting in the design of fuzzy
rule-based regression models. <em>TKDE</em>, <em>36</em>(11), 5621–5632.
(<a href="https://doi.org/10.1109/TKDE.2024.3392247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study is devoted to the design of gradient boosted fuzzy rule-based models for regression problems. Fuzzy rule-based models are built on the basis of information granules formed in the input and output spaces whose structure involves a family of conditional ‘if-then’ statements. The architecture of fuzzy rule-based models contributes to the realization of a sound tradeoff between modeling accuracy and interpretability and computing overhead. Gradient boosting paradigm has emerged as a powerful learning method realized through sequentially fitting additive base learners to current residuals in the steepest descent way. However, surprisingly, studies on the design and analysis of gradient boosted fuzzy rule-based models are still lacking. In this study, fuzzy rule-based model is regarded as a base learner. Different loss functions and their influence on the performance of the final models are explored. We also thoroughly investigate an impact of the initial quality of the rule-based model (implied by the number of rules) on the process of gradient boosting. The performance of the proposed approach is illustrated by a series of experimental studies concerning synthetic and publicly available datasets.},
  archive      = {J_TKDE},
  author       = {Huimin Zhang and Xingchen Hu and Xiubin Zhu and Xinwang Liu and Witold Pedrycz},
  doi          = {10.1109/TKDE.2024.3392247},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5621-5632},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Application of gradient boosting in the design of fuzzy rule-based regression models},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anomaly detection under contaminated data with
contamination-immune bidirectional GANs. <em>TKDE</em>, <em>36</em>(11),
5605–5620. (<a href="https://doi.org/10.1109/TKDE.2024.3404027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection aims to detect instances that deviate significantly from the majority. Due to the difficulties of collecting a large amount of anomalies in practice, existing methods generally assume the availability of a clean normal dataset and leverage it to detect anomalies by characterizing the normality of normal samples. However, for many application scenarios, collecting a normal dataset that is sufficiently clean is not easy. What is often observed is that a small amount of anomalies are often falsely mixed into the normal dataset, resulting in a contaminated dataset. Obviously, the contamination in the normal dataset could significantly compromise the model&#39;s ability to detect anomalies. To alleviate this issue, two contamination-immune bidirectional generative adversarial networks (BiGAN) are developed, which can learn the probability distribution of normal samples from a contaminated dataset under some mild conditions. Rigorous proofs are provided to guarantee the theoretical correctness of the proposed models. Thanks to the removing of negative influences from the contamination samples, the proposed contamination-immune models can thus be applied to detect anomalies accurately for the scenarios with contaminated datasets. Extensive experimental results show that the proposed method outperforms the current state-of-the-art (SOTA) ones significantly under the scenarios with contaminated training datasets.},
  archive      = {J_TKDE},
  author       = {Qinliang Su and Bowen Tian and Hai Wan and Jian Yin},
  doi          = {10.1109/TKDE.2024.3404027},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5605-5620},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Anomaly detection under contaminated data with contamination-immune bidirectional GANs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial contrastive learning for evidence-aware fake
news detection with graph neural networks. <em>TKDE</em>,
<em>36</em>(11), 5591–5604. (<a
href="https://doi.org/10.1109/TKDE.2023.3341640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalence and perniciousness of fake news have been a critical issue on the Internet, which stimulates the development of automatic fake news detection in turn. In this paper, we focus on the evidence-based fake news detection, where several evidences are utilized to probe the veracity of news (i.e., a claim). Most previous methods first employ sequential models to embed the semantic information and then capture the claim-evidence interaction based on different attention mechanisms. Despite their effectiveness, they still suffer from three weaknesses. First, due to the inherent drawbacks of sequential models, they fail to integrate the relevant information that is scattered far apart in evidences for veracity checking. Second, they underestimate much redundant information contained in evidences that may be useless or even harmful. Third, insufficient data utilization limits the separability and reliability of representations captured by the model, which are sensitive to local evidence. To solve these problems, we propose a unified G raph-based s E mantic structure mining framework with Con TRA stive L earning, namely GETRAL in short. Specifically, different from the existing work that treats claims and evidences as sequences, we first model them as graph-structured data and capture the long-distance semantic dependency among dispersed relevant snippets via neighborhood propagation. After obtaining contextual semantic information, our model reduces information redundancy by performing graph structure learning. Then the fine-grained semantic representations are fed into the downstream claim-evidence interaction module for predictions. Finally, the supervised contrastive learning accompanied with adversarial augmented instances is applied to make full use of data and strengthen the representation learning. Comprehensive experiments have demonstrated the superiority of GETRAL over the state-of-the-arts and validated the efficacy of semantic mining with graph structure and contrastive learning.},
  archive      = {J_TKDE},
  author       = {Junfei Wu and Weizhi Xu and Qiang Liu and Shu Wu and Liang Wang},
  doi          = {10.1109/TKDE.2023.3341640},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5591-5604},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adversarial contrastive learning for evidence-aware fake news detection with graph neural networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AdaRisk: Risk-adaptive deep reinforcement learning for
vulnerable nodes detection. <em>TKDE</em>, <em>36</em>(11), 5576–5590.
(<a href="https://doi.org/10.1109/TKDE.2024.3409869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vulnerable node detection in uncertain graphs is a typical graph mining problem that seeks to identify nodes at a high risk of breakdown under the joint effect from both the self and contagion risk probability. This is an NP-hard problem that is crucial for risk management in many real-world applications such as networked loans and smart grids. Monte Carlo (MC) simulation and its optimized algorithms are commonly used to approximate the breakdown probability, but these methods require a large number of samples to ensure accuracy, which is computationally expensive for large-scale networks. Although recent studies employ Graph Neural Networks (GNNs) to model the contagion process and accelerate the inference, many of these methods suffer from the over-smoothing problem, leading to suboptimal performance under the long-distance risk contagion process. To this end, we propose a novel risk-adaptive deep reinforcement learning-based framework (AdaRisk) for vulnerable nodes detection in uncertain graphs. In particular, we design the Markov Decision Process (MDP) of the vulnerability estimation process in which our agent would approach the risk adaptively based on contagion probability accumulated in prior iterations. To encode state embeddings that incorporate multi-hop contagion information, the agent utilizes a long-distance adaptable policy network to process the input graph and output actions as the vulnerable probability of nodes. We conducted extensive experiments on four benchmark networks and three real-world financial networks to evaluate our proposed framework&#39;s performance. Our results demonstrate that AdaRisk outperforms state-of-the-art baselines in terms of detection performance, and also offers significant running time reductions compared to MC simulation.},
  archive      = {J_TKDE},
  author       = {Fan Li and Zhiyu Xu and Dawei Cheng and Xiaoyang Wang},
  doi          = {10.1109/TKDE.2024.3409869},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5576-5590},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {AdaRisk: Risk-adaptive deep reinforcement learning for vulnerable nodes detection},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Adaptively topological tensor network for multi-view
subspace clustering. <em>TKDE</em>, <em>36</em>(11), 5562–5575. (<a
href="https://doi.org/10.1109/TKDE.2024.3391627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view subspace clustering employs learned self-representation from multiple tensor decompositions to exploit the low-rank information. However, the data structures embedded with self-representation tensors may vary in different multi-view datasets. Therefore, a pre-defined decomposition may not fully exploit low-rank information from various data, resulting in sub-optimal multi-view clustering performance. To alleviate this, we proposed the adaptively topological tensor network (ATTN). ATTN can learn a suitable decomposition structure that can represent the low-rank structure and high-order correlation of the self-representation tensors better in a data-driven way, which can capture the intra-view and inter-view information better. Firstly, instead of connecting the tensor network blindly, ATTN utilizes the correlation between adjacent factors to prune redundant connections from the fully connected tensor network, making the tensor network more expressive. Furthermore, a greedy adaptive rank-increasing strategy is applied to optimize the pruned tensor network structure, which improves the capacity of capturing low-rank structure. We apply ATTN on a multi-view subspace clustering task and utilize the alternating direction method of multipliers(ADMM) method to optimize it. Experiments show that multi-view subspace clustering based on ATTN has better performance on nine multi-view datasets.},
  archive      = {J_TKDE},
  author       = {Yipeng Liu and Jie Chen and Yingcong Lu and Weiting Ou and Zhen Long and Ce Zhu},
  doi          = {10.1109/TKDE.2024.3391627},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5562-5575},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adaptively topological tensor network for multi-view subspace clustering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified framework for mining batch and periodic batch in
data streams. <em>TKDE</em>, <em>36</em>(11), 5544–5561. (<a
href="https://doi.org/10.1109/TKDE.2024.3399024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Batch is an important pattern in data streams, which refers to a group of identical items that arrive closely. We find that some special batches that arrive periodically are of great value. In this paper, we formally define a new pattern, namely periodic batches . A group of periodic batches refers to several batches of the same item, where these batches arrive periodically. Studying periodic batches is important in many applications, such as caches, financial markets, online advertisements, networks, etc. This paper proposes a unified framework, namely the HyperCalm sketch, to detect batch and periodic batch in data streams. HyperCalm sketch takes two phases to detect periodic batches. In phase 1, we propose a time-aware Bloom filter, called HyperBloomFilter ( HyperBF ), to detect batches. In phase 2, we propose an enhanced top- $k$ algorithm, called Calm Space-Saving ( CalmSS ), to report top- $k$ periodic batches. Extensive experiments show HyperCalm outperforms the strawman solutions 4× in term of average relative error and 98.1× in term of speed. All related codes are open-sourced.},
  archive      = {J_TKDE},
  author       = {Zirui Liu and Xiangyuan Wang and Yuhan Wu and Tong Yang and Kaicheng Yang and Hailin Zhang and Yaofeng Tu and Bin Cui},
  doi          = {10.1109/TKDE.2024.3399024},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5544-5561},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A unified framework for mining batch and periodic batch in data streams},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A truthful pricing-based defending strategy against
adversarial attacks in budgeted combinatorial multi-armed bandits.
<em>TKDE</em>, <em>36</em>(11), 5529–5543. (<a
href="https://doi.org/10.1109/TKDE.2023.3335248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study defending strategies against adversarial attacks on Combinatorial Multi-Armed Bandits (CMAB) algorithms. CMAB is an effective sequence decision making tool that has been broadly applied in online real-world applications. We consider a realistic CMAB setting, budgeted CMAB, in which multiple arms associated with pulling costs and unknown rewards are pulled per round, aiming to maximize the cumulative reward under a budget constraint. However, the adversarial attack against budgeted CMAB is rarely studied, posing a very important security issue. Specifically, a suboptimal arm that is not pulled (i.e., attacker) can hijack the budgeted CMAB algorithm&#39;s behavior, forcing itself to be pulled frequently by manipulating other arms’ rewards. Existing strategies cannot prevent such attacks. Motivated by this, we closely study the adversarial attack against a popular budgeted CMAB algorithm, exposing a significant security threat to real-world applications. The attack extends to other algorithms with certain customization. To address this, we incorporate a truthful pricing-based defending strategy that prevents such attacks effectively and ensures arms share pulling costs truthfully. Extensive simulations have illustrated the proposed attack strategy can hijack the algorithm efficiently, while the defending strategy provides attack prevention, individual rationality, and asymptotic truthfulness guarantees.},
  archive      = {J_TKDE},
  author       = {Hengzhi Wang and En Wang and Yongjian Yang and Bo Yang and Jiangchuan Liu},
  doi          = {10.1109/TKDE.2023.3335248},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5529-5543},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A truthful pricing-based defending strategy against adversarial attacks in budgeted combinatorial multi-armed bandits},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on concurrent processing of graph analytical
queries: Systems and algorithms. <em>TKDE</em>, <em>36</em>(11),
5508–5528. (<a href="https://doi.org/10.1109/TKDE.2024.3393936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph analytical queries ( GAQ s) are becoming increasingly important in various domains, including social networks, recommendation systems, and bioinformatics, among others. GAQ s typically require iterative processing of the graph data to compute various metrics and identify patterns or anomalies. Parallel to the burgeoning demand for graph analytics, the need for Concurrent Graph Analytical Queries ( CGAQ s), allowing simultaneous execution of multiple graph queries, is increasing. Within social networks, CGAQ s bolster real-time analytics, concurrently investigate various network properties, such as community detection, path analysis, and influence propagation. In transportation, CGAQ s concurrently optimize multiple routes and manage real-time traffic data, contributing significantly to efficient supply chain strategies and traffic management. The key property of CGAQ s lies in their capacity for shared processing, exploiting the synergies between concurrent queries, which in return opens opportunities for improved system scalability and throughput. In this survey, we present a comprehensive review of system-level and algorithm-level efforts to support CGAQ processing. We introduce a novel survey framework based on three aspects: 1) What are the sharing opportunities exploited? 2) What are the scheduling techniques proposed to maximize sharing? 3) What are the optimizations employed? We also identify important gaps and promising research directions for CGAQ processing.},
  archive      = {J_TKDE},
  author       = {Yuchen Li and Shixuan Sun and Hanhua Xiao and Chang Ye and Shengliang Lu and Bingsheng He},
  doi          = {10.1109/TKDE.2024.3393936},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5508-5528},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey on concurrent processing of graph analytical queries: Systems and algorithms},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A feature importance-based multi-layer CatBoost for student
performance prediction. <em>TKDE</em>, <em>36</em>(11), 5495–5507. (<a
href="https://doi.org/10.1109/TKDE.2024.3393472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Student performance prediction is vital for identifying at-risk students and providing support to help them succeed academically. In this paper, we propose a feature importance-based multi-layer CatBoost approach to predict the students’ grade in the period exam. The idea is to construct a multi-layer structure with increasingly important features layer by layer. Specifically, the feature importance are first calculated and sorted in ascending order. In each layer, features with the least importance are accumulated until reaching a given threshold. Then, these selected features are used to construct the first layer by training the CatBoost. Next, this trained CatBoost is utilized to generate a feature that adds to the feature set with their importance within a threshold. After that, all these feature are used to train the CatBoost in the next layer. This process is repeated until all the features are used. The results show that the proposed model has the best performance. Moreover, the statistical test conducted based on 20-runs of experiments validates the significant superiority of our proposed model over the compared models and demonstrates the efficacy of the multi-layer structure in enhancing the proposed model. This indicates our proposed model can help decision makers in enhancing educational quality.},
  archive      = {J_TKDE},
  author       = {Zongwen Fan and Jin Gou and Shaoyuan Weng},
  doi          = {10.1109/TKDE.2024.3393472},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5495-5507},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A feature importance-based multi-layer CatBoost for student performance prediction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A d-truss-equivalence based index for community search over
large directed graphs. <em>TKDE</em>, <em>36</em>(11), 5482–5494. (<a
href="https://doi.org/10.1109/TKDE.2024.3393864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community Search (CS) aims to enable online and personalized discovery of communities. Recently, attention to the CS problem in directed graphs (di-graph) needs to be improved despite the extensive study conducted on undirected graphs. Nevertheless, the existing studies are plagued by several shortcomings, e.g., Achieving high-performance CS while ensuring the retrieved community is cohesive is challenging. This paper uses the D-truss model to address the limitations of investigating the CS problem in large di-graphs. We aim to implement millisecond-level D-truss CS in di-graphs by building a summarized graph index. To capture the interconnectedness of edges within D-truss communities, we propose an innovative equivalence relation known as D-truss-equivalence, which allows us to divide the edges in a di-graph into a sequence of super nodes (s-nodes). These s-nodes form the D-truss-equivalence-based index, DEBI, an index structure that preserves the truss properties and ensures efficient space utilization. Using DEBI, CS can be performed without time-consuming access to the original graph. The experiments indicate that our method can achieve millisecond-level D-truss community query while ensuring high community quality. In addition, dynamic maintenance of indexes can also be achieved at a lower cost.},
  archive      = {J_TKDE},
  author       = {Wei Ai and Canhao Xie and Tao Meng and Jayi Du and Keqin Li},
  doi          = {10.1109/TKDE.2024.3393864},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5482-5494},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A D-truss-equivalence based index for community search over large directed graphs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A clustering framework for unsupervised and semi-supervised
new intent discovery. <em>TKDE</em>, <em>36</em>(11), 5468–5481. (<a
href="https://doi.org/10.1109/TKDE.2023.3340732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New intent discovery is of great value to natural language processing, allowing for a better understanding of user needs and providing friendly services. However, most existing methods struggle to capture the complicated semantics of discrete text representations when limited or no prior knowledge of labeled data is available. To tackle this problem, we propose a novel clustering framework, USNID, for u nsupervised and s emi-supervised n ew i ntent d iscovery, which has three key technologies. First, it fully utilizes of unsupervised or semi-supervised data to mine shallow semantic similarity relations and provide well-initialized representations for clustering. Second, it designs a centroid-guided clustering mechanism to address the issue of cluster allocation inconsistency and provide high-quality self-supervised targets for representation learning. Third, it captures high-level semantics in unsupervised or semi-supervised data to discover fine-grained intent-wise clusters by optimizing both cluster-level and instance-level objectives. We also propose an effective method for estimating the cluster number in open-world scenarios without knowing the number of new intents beforehand. USNID performs exceptionally well on several benchmark intent datasets, achieving new state-of-the-art results in unsupervised and semi-supervised new intent discovery, and demonstrating robust performance with different cluster numbers.},
  archive      = {J_TKDE},
  author       = {Hanlei Zhang and Hua Xu and Xin Wang and Fei Long and Kai Gao},
  doi          = {10.1109/TKDE.2023.3340732},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5468-5481},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A clustering framework for unsupervised and semi-supervised new intent discovery},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A circumstance-aware neural framework for explainable legal
judgment prediction. <em>TKDE</em>, <em>36</em>(11), 5453–5467. (<a
href="https://doi.org/10.1109/TKDE.2024.3387580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Massive legal documents have promoted the application of legal intelligence. Among them, Legal Judgment Prediction (LJP) has emerged as a critical task, garnering significant attention. LJP aims to predict judgment results for multiple subtasks, including charges, law articles, and terms of penalty. Existing studies primarily focus on utilizing the entire factual description to produce judgment results, overlooking the practical judicial scenario where judges consider various crime circumstances to decide verdicts and sentencing. To this end, in this article, we propose a circumstance-aware LJP framework (i.e., NeurJudge) by exploring the circumstances of crime. Specifically, NeurJudge first separates the factual description into different circumstances with the predicted results of intermediate subtasks and then employs them to yield results of other subtasks. Besides, as confusing verdicts may degrade the performance of LJP, we further develop a variant of NeurJudge (NeurJudge+) that incorporates the semantics of labels (charges and law articles) into facts to yield more expressive and distinguishable fact representations. Finally, to provide explanations for LJP, we extend NeurJudge to an explainable LJP framework E-NeurJudge with a cooperative teacher-student system. The teacher system is NeurJudge which exploits legal particularities well but lacks explanation capability. The student system is a rationalization method that provides explainability but fails to utilize legal particularities. To combine the advantages of the above methods, we use a transferring function to transfer legal particularities from the teacher to the student, making a trade-off between yielding LJP results and rendering them explainable. Extensive experimental results on real-world datasets validate the effectiveness of our proposed frameworks.},
  archive      = {J_TKDE},
  author       = {Linan Yue and Qi Liu and Binbin Jin and Han Wu and Yanqing An},
  doi          = {10.1109/TKDE.2024.3387580},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {11},
  number       = {11},
  pages        = {5453-5467},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A circumstance-aware neural framework for explainable legal judgment prediction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). When transformer meets large graphs: An expressive and
efficient two-view architecture. <em>TKDE</em>, <em>36</em>(10),
5440–5452. (<a href="https://doi.org/10.1109/TKDE.2024.3381125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The successes of applying Transformer to graphs have been witnessed on small graphs (e.g., molecular graphs), yet two barriers prevent its adoption on large graphs (e.g., citation networks). First, despite the benefit of the global receptive field, enormous distant nodes might distract the necessary attention of each target node from its neighborhood. Second, training a Transformer model on large graphs is costly due to the node-to-node attention mechanism&#39;s quadratic computational complexity. To break down these barriers, we propose a two-view architecture Coarformer , wherein a GNN-based module captures fine-grained local information from the original graph, and a Transformer-based module captures coarse yet long-range information on the coarse graph. We further design a cross-view propagation scheme so that these two views can enhance each other. Our graph isomorphism analysis shows the complementary natures of GNN and Transformer, justifying the motivation and design of Coarformer . We conduct extensive experiments on real-world datasets, where Coarformer surpasses any single-view method that solely applies a GNN or Transformer. As an ablation, Coarformer outperforms straightforward combinations of a GNN model and a Transformer-based model, verifying the effectiveness of our coarse global view and the cross-view propagation scheme. Meanwhile, Coarformer consumes the least runtime and GPU memory than those combinations.},
  archive      = {J_TKDE},
  author       = {Weirui Kuang and Zhen Wang and Zhewei Wei and Yaliang Li and Bolin Ding},
  doi          = {10.1109/TKDE.2024.3381125},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5440-5452},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {When transformer meets large graphs: An expressive and efficient two-view architecture},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unravelling token ecosystem of EOSIO blockchain.
<em>TKDE</em>, <em>36</em>(10), 5423–5439. (<a
href="https://doi.org/10.1109/TKDE.2024.3378381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Being the largest Initial Coin Offering project, EOSIO has attracted great interest in cryptocurrency markets. Despite its popularity and prosperity (e.g., 26,311,585,008 token transactions occurred from June 8, 2018 to Aug. 5, 2020), there is almost no work investigating the EOSIO token ecosystem. To fill this gap, we are the first to conduct a systematic investigation of the EOSIO token ecosystem by conducting a comprehensive graph analysis of the entire on-chain EOSIO data (nearly 135 million blocks). We construct token-creator graphs, token-contract creator graphs, token-holder graphs, and token-transfer graphs to characterize token creators, holders, and transfer activities. Through graph analysis, we have obtained many insightful findings and observed some abnormal trading patterns. Moreover, we propose a fake-token detection algorithm to identify tokens generated by fake users or fake transactions and analyze their corresponding manipulation behaviors. Evaluation results also demonstrate the effectiveness of our algorithm.},
  archive      = {J_TKDE},
  author       = {Zigui Jiang and Weilin Zheng and Bo Liu and Hong-Ning Dai and Haoran Xie and Xiapu Luo and Zibin Zheng and Qing Li},
  doi          = {10.1109/TKDE.2024.3378381},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5423-5439},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Unravelling token ecosystem of EOSIO blockchain},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SV-learner: Support-vector contrastive learning for robust
learning with noisy labels. <em>TKDE</em>, <em>36</em>(10), 5409–5422.
(<a href="https://doi.org/10.1109/TKDE.2024.3386829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noisy-label data inevitably gives rise to confusion in various perception applications. In this work, we revisit the theory of support vector machines (SVMs) which mines support vectors to build the maximum-margin hyperplane for robust classification, and propose a robust-to-noise deep learning framework, SV-Learner, including the Support Vector Contrastive Learning (SVCL) and Support Vector-based Noise Screening (SVNS). The SV-Learner mines support vectors to solve the learning problem with noisy labels (LNL) reliably. SVCL adopts support vectors as positive and negative samples, driving robust contrastive learning to enlarge the feature distribution margin for learning convergent feature distributions. SVNS uses support vectors with valid labels to assist in screening noisy ones from confusable samples for reliable clean-noisy sample screening. Finally, Semi-Supervised classification is performed to realize the recognition of noisy samples. Extensive experiments are evaluated on CIFAR-10, CIFAR-100, Clothing1M, and Webvision datasets, and results demonstrate the effectiveness of our proposed approach.},
  archive      = {J_TKDE},
  author       = {Xin Liang and Yanli Ji and Wei-Shi Zheng and Wangmeng Zuo and Xiaofeng Zhu},
  doi          = {10.1109/TKDE.2024.3386829},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5409-5422},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SV-learner: Support-vector contrastive learning for robust learning with noisy labels},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatio-temporal graph neural networks for predictive
learning in urban computing: A survey. <em>TKDE</em>, <em>36</em>(10),
5388–5408. (<a href="https://doi.org/10.1109/TKDE.2023.3333824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With recent advances in sensing technologies, a myriad of spatio-temporal data has been generated and recorded in smart cities. Forecasting the evolution patterns of spatio-temporal data is an important yet demanding aspect of urban computing, which can enhance intelligent management decisions in various fields, including transportation, environment, climate, public safety, healthcare, and others. Traditional statistical and deep learning methods struggle to capture complex correlations in urban spatio-temporal data. To this end, Spatio-Temporal Graph Neural Networks (STGNN) have been proposed, achieving great promise in recent years. STGNNs enable the extraction of complex spatio-temporal dependencies by integrating graph neural networks (GNNs) and various temporal learning methods. In this manuscript, we provide a comprehensive survey on recent progress on STGNN technologies for predictive learning in urban computing. Firstly, we provide a brief introduction to the construction methods of spatio-temporal graph data and the prevalent deep-learning architectures used in STGNNs. We then sort out the primary application domains and specific predictive learning tasks based on existing literature. Afterward, we scrutinize the design of STGNNs and their combination with some advanced technologies in recent years. Finally, we conclude the limitations of existing research and suggest potential directions for future work.},
  archive      = {J_TKDE},
  author       = {Guangyin Jin and Yuxuan Liang and Yuchen Fang and Zezhi Shao and Jincai Huang and Junbo Zhang and Yu Zheng},
  doi          = {10.1109/TKDE.2023.3333824},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5388-5408},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Spatio-temporal graph neural networks for predictive learning in urban computing: A survey},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial-temporal interval aware individual future trajectory
prediction. <em>TKDE</em>, <em>36</em>(10), 5374–5387. (<a
href="https://doi.org/10.1109/TKDE.2023.3332929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present STiSAN $^+$ as an end-to-end mobility trajectory prediction framework. It is a Self-Attention Network (SAN)-based model augmented with two lightweight approaches: Rotary Time Aware Position Encoder (RoTAPE) and multi-head Interval Aware Attention Block (IAAB). These methods allow for the explicit, efficient and effective processing of spatial-temporal intervals among the user&#39;s historical trajectory, without the need for additional parameters or a substantial computational burden. On the one hand, RoTAPE simultaneously encodes day- and hour-level timestamps into the sequence representation via a sinusoidal encoding matrix. Notably, the multi-level temporal differences operate in a mutually independent manner to reflect the periodical pattern, and collectively measure the absolute time interval. On the other hand, IAAB, point-wise injecting the historical spatial-temporal intervals into the attention map, can promote SAN attaching importance to the spatial relations under the constraints of time conditions. Moreover, we equip STiSAN $^+$ with a novel MLP-based module, namely Spatial-Temporal Relation Memory (STR Memory). It endows the interactions inside historical intervals along different directions, and converts them into spatial-temporal relations in future trajectories for accurate predictions. The empirical study on six public LBSN shows that from Next Location Recommendation to Multi-location Future Trajectory Prediction, our STiSAN $^+$ gains average 15.05% and 18.35% improvements against several state-of-the-art sequential models, respectively. We demonstrate the effectiveness of proposed module with an ablation study, and validate the extensibility and interpretability of RoTAPE and IAAB through non-sampled metric evaluation and visualization.},
  archive      = {J_TKDE},
  author       = {Yiheng Jiang and Yongjian Yang and Yuanbo Xu and En Wang},
  doi          = {10.1109/TKDE.2023.3332929},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5374-5387},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Spatial-temporal interval aware individual future trajectory prediction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Result diversification in search and recommendation: A
survey. <em>TKDE</em>, <em>36</em>(10), 5354–5373. (<a
href="https://doi.org/10.1109/TKDE.2024.3382262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diversifying return results is an important research topic in retrieval systems in order to satisfy both the various interests of customers and the equal market exposure of providers. There has been growing attention on diversity-aware research during recent years, accompanied by a proliferation of literature on methods to promote diversity in search and recommendation. However, diversity-aware studies in retrieval systems lack a systematic organization and are rather fragmented. In this survey, we are the first to propose a unified taxonomy for classifying the metrics and approaches of diversification in both search and recommendation, which are two of the most extensively researched fields of retrieval systems. We begin the survey with a brief discussion of why diversity is important in retrieval systems, followed by a summary of the various diversity concerns in search and recommendation, highlighting their relationship and differences. For the survey&#39;s main body, we present a unified taxonomy of diversification metrics and approaches in retrieval systems, from both the search and recommendation perspectives. In the later part of the survey, we discuss the open research questions of diversity-aware research in search and recommendation in an effort to inspire future innovations and encourage the implementation of diversity in real-world systems.},
  archive      = {J_TKDE},
  author       = {Haolun Wu and Yansen Zhang and Chen Ma and Fuyuan Lyu and Bowei He and Bhaskar Mitra and Xue Liu},
  doi          = {10.1109/TKDE.2024.3382262},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5354-5373},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Result diversification in search and recommendation: A survey},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RARE: Robust masked graph autoencoder. <em>TKDE</em>,
<em>36</em>(10), 5340–5353. (<a
href="https://doi.org/10.1109/TKDE.2023.3335222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Masked graph autoencoder (MGAE) has emerged as a promising self-supervised graph pre-training (SGP) paradigm due to its simplicity and effectiveness. However, existing efforts perform the mask-then-reconstruct operation in the raw data space as is done in computer vision (CV) and natural language processing (NLP) areas, while neglecting the important non-Euclidean property of graph data. As a result, the highly unstable local structures largely increase the uncertainty in inferring masked data and decrease the reliability of the exploited self-supervision signals, leading to inferior representations for downstream evaluations. To address this issue, we propose a novel SGP method termed R obust m A sked g R aph auto E ncoder (RARE) to improve the certainty in inferring masked data and the reliability of the self-supervision mechanism by further masking and reconstructing node samples in the high-order latent feature space. Through both theoretical and empirical analyses, we have discovered that performing a joint mask-then-reconstruct strategy in both latent feature and raw data spaces could yield improved stability and performance. To this end, we elaborately design a masked latent feature completion scheme, which predicts latent features of masked nodes under the guidance of high-order sample correlations that are hard to be observed from the raw data perspective. Specifically, we first adopt a latent feature predictor to predict the masked latent features from the visible ones. Next, we encode the raw data of masked samples with a momentum graph encoder and subsequently employ the resulting representations to improve the predicted results through latent feature matching. Extensive experiments on seventeen datasets have demonstrated the effectiveness and robustness of RARE against state-of-the-art (SOTA) competitors across three downstream tasks.},
  archive      = {J_TKDE},
  author       = {Wenxuan Tu and Qing Liao and Sihang Zhou and Xin Peng and Chuan Ma and Zhe Liu and Xinwang Liu and Zhiping Cai and Kunlun He},
  doi          = {10.1109/TKDE.2023.3335222},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5340-5353},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {RARE: Robust masked graph autoencoder},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Prompt-learning for short text classification.
<em>TKDE</em>, <em>36</em>(10), 5328–5339. (<a
href="https://doi.org/10.1109/TKDE.2023.3332787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the short text, the extremely short length, feature sparsity, and high ambiguity pose huge challenges to classification tasks. Recently, as an effective method for tuning Pre-trained Language Models for specific downstream tasks, prompt-learning has attracted a vast amount of attention and research. The main intuition behind the prompt-learning is to insert the template into the input and convert the tasks into equivalent cloze-style tasks. However, most prompt-learning methods only consider the class name and monotonous strategy for knowledge incorporating in cloze-style prediction, which will inevitably incur omissions and bias in short text classification tasks. In this paper, we propose a short text classification method with prompt-learning. Specifically, the top $M$ concepts related to the entity in the short text are retrieved from the open Knowledge Graph like Probase, these concepts are first selected by the distance with class labels, which takes both the short text itself and the class name into consideration during expanding label word space. Then, we conducted four additional strategies for the integration of the expanded concepts, and the union of these concepts are adopted finally in the verbalizer of prompt-learning. Experimental results show that the obvious improvement is obtained compared with other state-of-the-art methods on five well-known datasets.},
  archive      = {J_TKDE},
  author       = {Yi Zhu and Ye Wang and Jipeng Qiang and Xindong Wu},
  doi          = {10.1109/TKDE.2023.3332787},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5328-5339},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Prompt-learning for short text classification},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Preventing inferences through data dependencies on sensitive
data. <em>TKDE</em>, <em>36</em>(10), 5308–5327. (<a
href="https://doi.org/10.1109/TKDE.2023.3336630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simply restricting the computation to non-sensitive part of the data may lead to inferences on sensitive data through data dependencies. Prior work on preventing inference control through data dependencies detect and deny queries which may lead to leakage, or only protect against exact reconstruction of the sensitive data. These solutions result in poor utility, and poor security respectively. In this paper, we present a novel security model called full deniability . Under this stronger security model, any information inferred about sensitive data from non-sensitive data is considered as a leakage. We describe algorithms for efficiently implementing full deniability on a given database instance with a set of data dependencies and sensitive cells. Using experiments on two different datasets, we demonstrate that our approach protects against realistic adversaries while hiding only minimal number of additional non-sensitive cells and scales well with database size and sensitive data.},
  archive      = {J_TKDE},
  author       = {Primal Pappachan and Shufan Zhang and Xi He and Sharad Mehrotra},
  doi          = {10.1109/TKDE.2023.3336630},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5308-5327},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Preventing inferences through data dependencies on sensitive data},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Noised multi-layer networks clustering with graph denoising
and structure learning. <em>TKDE</em>, <em>36</em>(10), 5294–5307. (<a
href="https://doi.org/10.1109/TKDE.2023.3335223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-layer networks treat various types of interactions at each level to model complex systems in nature and society, and clustering of them is of great significance for revealing mechanisms of systems. Vast majority of current algorithms focus on identifying the common communities in clear multi-layer networks, and few attempt has been devoted to the detection of layer-specific communities in noised ones. To address these issues, a joint learning algorithm with G raph D enoising and S tructure L earning (called GDSL ) for the detection of layer-specific communities in noised multi-layer networks is proposed, which simultaneously integrates graph denoising, structure learning, and module detection. To remove noise of networks, GDSL re-constructs affinity graphs for the original ones by preserving community structure. To enhance robustness and discriminative of features, GDSL explores the relations of features among various layers with the Hilbert-Schmidt Independence Criterion and structure learning. Finally, GDSL joins all these procedures with an objective function, and deduces optimization rules. The results show that GDSL not only significantly outperforms baselines but also enhances the robustness of the algorithm, providing an effective model for community detection in noised multi-layer networks.},
  archive      = {J_TKDE},
  author       = {Wenming Wu and Wensheng Zhang and Maoguo Gong and Xiaoke Ma},
  doi          = {10.1109/TKDE.2023.3335223},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5294-5307},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Noised multi-layer networks clustering with graph denoising and structure learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-passage machine reading comprehension through
multi-task learning and dual verification. <em>TKDE</em>,
<em>36</em>(10), 5280–5293. (<a
href="https://doi.org/10.1109/TKDE.2024.3383103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-passage machine reading comprehension (MRC) aims to answer a question by multiple passages. Existing multi-passage MRC approaches have shown that employing passages with and without golden answers (i.e., labeled and unlabeled passages) for model training can improve prediction accuracy. However, when using the unlabeled passages, they either incur the wrong labeling problem or treat the labeled and unlabeled passages equally. In addition, they ignore the original passage information to verify the correctness of the answer. In this paper, we present MLDV-MRC, a novel approach for multi-passage MRC via M ulti-task L earning and D ual V erification. MLDV-MRC adopts the extract-then-select framework, where an extractor is first used to predict answer candidates, then a selector is used to choose the final answer. For the extractor, we adopt multi-task learning with generative adversarial training to train it by using both labeled and unlabeled passages. To train the extractor by backpropagation, we propose a hybrid method which combines boundary-based and content-based extracting methods to produce the answer candidate set and its representation. For the selector, we propose to leverage both the information from answer candidates and original passages to verify the final answer. In particular, we propose a global-local memory-augmented neural network to build the representations of original passages, which fuses the passage-level information and word-level information. The experimental results on three open-domain QA datasets confirm the effectiveness of our approach.},
  archive      = {J_TKDE},
  author       = {Xingyi Li and Xiang Cheng and Min Xia and Qiyu Ren and Zhaofeng He and Sen Su},
  doi          = {10.1109/TKDE.2024.3383103},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5280-5293},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-passage machine reading comprehension through multi-task learning and dual verification},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Multi-class imbalance classification based on data
distribution and adaptive weights. <em>TKDE</em>, <em>36</em>(10),
5265–5279. (<a href="https://doi.org/10.1109/TKDE.2024.3384961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AdaBoost approaches have been used for multi-class imbalance classification with an imbalance ratio measured on class sizes. However, such ratio would assign each training sample of the same class with the same weight, thus failing to reflect the data distribution within a class. We propose to incorporate the density information of training samples into the class imbalance ratio so that samples of the same class could have different weights. As one could use the entire training set to calculate the imbalance and density factors, the weight of a training sample resulting from the two factors remains static throughout the training epochs. However, static weights could not reflect the up-to-date training status of base learners. To deal with this, we propose to design an adaptive weighting mechanism by making use of up-to-date training status to further alleviate the multi-class imbalance issue. Ultimately, we incorporate the class imbalance ratio, the density-based factor, and the adaptive weighting mechanism into a single variable, based on which the adaptive weights of all training samples are computed. Experimental studies are carried out to investigate the effectiveness of the proposed approach and each of the three components in dealing with multi-class imbalance classification problem.},
  archive      = {J_TKDE},
  author       = {Shuxian Li and Liyan Song and Xiaoyu Wu and Zheng Hu and Yiu-ming Cheung and Xin Yao},
  doi          = {10.1109/TKDE.2024.3384961},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5265-5279},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-class imbalance classification based on data distribution and adaptive weights},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MorphDAG: A workload-aware elastic DAG-based blockchain.
<em>TKDE</em>, <em>36</em>(10), 5249–5264. (<a
href="https://doi.org/10.1109/TKDE.2024.3382743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directed Acyclic Graph (DAG)-based blockchain represents a paradigm shift from conventional blockchains, which has the potential to drastically improve throughput performance through concurrent storage and executions. In practice, however, existing DAG-based blockchains fail to deliver such promises, often with limited throughput, high conflicts, and security vulnerabilities under dynamic workloads. The root causes are their unawareness of the workload characteristics of different workload sizes and skewed access patterns. In this article, we propose MorphDAG, the first workload-aware DAG-based blockchain that can significantly enhance throughput without compromising security and achieve elastic scaling under realistic workloads. We derive the theoretically optimal degree of storage concurrency to achieve high throughput while retaining system security as the workload size changes, while enabling fine-grained concurrency adjustment that accommodates a Proof-of-Stake (PoS)-based consensus protocol. We develop a dual-mode transaction processing mechanism that effectively resolves the conflicts brought by skewed access. We implement a prototype of MorphDAG and evaluate under real-world workloads. Extensive evaluations demonstrate that MorphDAG improves end-to-end throughput by up to 2.3× and 2.4× over state-of-the-art DAG-based blockchain systems AdaptChain and OHIE, respectively.},
  archive      = {J_TKDE},
  author       = {Shijie Zhang and Jiang Xiao and Enping Wu and Feng Cheng and Bo Li and Wei Wang and Hai Jin},
  doi          = {10.1109/TKDE.2024.3382743},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5249-5264},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MorphDAG: A workload-aware elastic DAG-based blockchain},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large-scale graph label propagation on GPUs. <em>TKDE</em>,
<em>36</em>(10), 5234–5248. (<a
href="https://doi.org/10.1109/TKDE.2023.3336329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph label propagation ( LP ) is a core component in many downstream applications such as fraud detection, recommendation and image segmentation. In this paper, we propose GLP , a GPU-based framework to enable efficient LP processing on large-scale graphs. By investigating the data processing pipeline in a large e-commerce platform, we have identified two key challenges on integrating GPU-accelerated LP processing to the pipeline: (1) programmability for evolving application logics; (2) demand for real-time performance. Motivated by these challenges, we offer a set of expressive APIs that data engineers can customize and deploy efficient LP algorithms on GPUs with ease. To achieve better performance, we propose novel GPU-centric optimizations by leveraging the community as well as power-law properties of large graphs. Further, we significantly reduce the expensive data transfer cost between CPUs and GPUs by enabling LP processing on compressed graphs. Extensive experiments have confirmed the effectiveness of our proposed approaches over the state-of-the-art GPU methods. Furthermore, our proposed solution supports a real billion-scale graph workload for fraud detection and achieves 13.2x speedup to the current in-house solution running on a high-end multicore machine with compressed graphs.},
  archive      = {J_TKDE},
  author       = {Chang Ye and Yuchen Li and Bingsheng He and Zhao Li and Jianling Sun},
  doi          = {10.1109/TKDE.2023.3336329},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5234-5248},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Large-scale graph label propagation on GPUs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interaction subgraph sequential topology-aware network for
transferable recommendation. <em>TKDE</em>, <em>36</em>(10), 5221–5233.
(<a href="https://doi.org/10.1109/TKDE.2024.3384965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems have primarily been limited to research on a single dataset compared to natural language processing and computer vision, which have seen tremendous growth in transferable tasks. Existing approaches for recommendation systems need to be more scalable to arbitrary tasks, given that previous research efforts on transferable recommendations have only yielded brief explorations and neglected systematic studies of sequential tasks. In this regard, we propose the interaction subgraph sequential topology-aware network (ISTN), which overcomes this limitation, enabling transferable sequence recommendations. ISTN performs subgraph sampling and node labeling of user interactions, captures the topological features of the user interaction sequences with the sequential topology auto-encoder, and employs the sequential preference decoupling module to decouple user interaction sequences for transferable adaptive granularity modeling of user preferences. ISTN requires no fine-tuning, and its knowledge transfer capability from the training dataset to the new dataset delivers accurate, individualized recommendation results. ISTN outperforms state-of-the-art performance in transferable contexts with only minor performance degradation compared to the traditional baseline, as shown in Yelp, MovieLens, and Foursquare experiments.},
  archive      = {J_TKDE},
  author       = {Kang Yang and Ruiyun Yu and Bingyang Guo and Jie Li},
  doi          = {10.1109/TKDE.2024.3384965},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5221-5233},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Interaction subgraph sequential topology-aware network for transferable recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrating visualised automatic temporal relation graph
into multi-task learning for alzheimer’s disease progression prediction.
<em>TKDE</em>, <em>36</em>(10), 5206–5220. (<a
href="https://doi.org/10.1109/TKDE.2024.3385712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer&#39;s disease (AD), the most prevalent dementia, gradually reduces the cognitive abilities of patients while also posing a significant financial burden on the healthcare system. A variety of multi-task learning methods have recently been proposed in order to identify potential MRI-related biomarkers and accurately predict the progression of AD. These methods, however, all use a predefined task relation structure that is rigid and insufficient to adequately capture the intricate temporal relations among tasks. Instead, we propose a novel mechanism for directly and automatically learning the temporal relation and constructing it as an Automatic Temporal relation Graph (AutoTG). We use the sparse group Lasso to select a universal MRI feature set for all tasks and particular sets for various tasks in order to find biomarkers that are useful for predicting the progression of AD. To solve the biconvex and non-smooth objective function, we adopt the alternating optimization and show that the two related sub-optimization problems are amenable to closed-form solution of the proximal operator. To solve the two problems efficiently, the accelerated proximal gradient method is used, which has the fastest convergence rate of any first-order method. We have preprocessed three latest AD datasets, and the experimental results verify our proposed novel multi-task approach outperforms several baseline methods. To demonstrate the high interpretability of our approach, we visualise the automatically learned temporal relation graph and investigate the temporal patterns of the important MRI features.},
  archive      = {J_TKDE},
  author       = {Menghui Zhou and Xulong Wang and Tong Liu and Yun Yang and Po Yang},
  doi          = {10.1109/TKDE.2024.3385712},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5206-5220},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Integrating visualised automatic temporal relation graph into multi-task learning for alzheimer&#39;s disease progression prediction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved contraction-expansion subspace ensemble for
high-dimensional imbalanced data classification. <em>TKDE</em>,
<em>36</em>(10), 5194–5205. (<a
href="https://doi.org/10.1109/TKDE.2024.3384274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced data biases the classifier towards the majority class. Accompanied with high-dimensional characteristics, classification performance is further degraded. Existing researches for skewed data mainly involve resampling, cost-sensitive learning, and classifier ensemble. However, these approaches have some limitations: 1) resampling suffers from noisy and redundant features in high-dimensional skewed data; 2) cost-sensitive learning is hard to construct an optimal cost matrix for sample misclassification; 3) ensemble with random feature subspace easily leads to information loss; 4) ensemble with sample subspace on small-size data easily leads to insufficient description of sample space and suffers from negative impacts of high-dimensional data. This paper proposes an improved contraction-expansion subspace ensemble (ICESE) for high-dimensional imbalanced data classification. First, a contraction-expansion subspace optimization (CESO) is designed to perform subspace selection and transformation, which is beneficial for enhancing the discrimination and diversity of subspace. Then, to strengthen classification capabilities, a CESO-based multilayer optimization structure is developed to construct the improved subspace. Finally, to mitigate the effects of skewed data, ICESE performs a resampling scheme on the improved subspace for constructing a rebalanced subset to base classifier. Experimental results on 24 high-dimensional imbalanced data sets demonstrate that our ICESE outperforms different mainstream ensemble systems in terms of F-score and G-mean.},
  archive      = {J_TKDE},
  author       = {Yuhong Xu and Zhiwen Yu and C. L. Philip Chen},
  doi          = {10.1109/TKDE.2024.3384274},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5194-5205},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Improved contraction-expansion subspace ensemble for high-dimensional imbalanced data classification},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous graph contrastive learning with meta-path
contexts and adaptively weighted negative samples. <em>TKDE</em>,
<em>36</em>(10), 5181–5193. (<a
href="https://doi.org/10.1109/TKDE.2024.3377431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous graph contrastive learning has received wide attention recently. Some existing methods use meta-paths, which are sequences of object types that capture semantic relationships between objects, to construct contrastive views. However, most of them ignore the rich meta-path context information that describes how two objects are connected by meta-paths. Further, they fail to distinguish negative samples, which could adversely affect the model performance. To address the problems, we propose MEOW, which considers both meta-path contexts and weighted negative samples. Specifically, MEOW constructs a coarse view and a fine-grained view for contrast. The former reflects which objects are connected by meta-paths, while the latter uses meta-path contexts and characterizes details on how the objects are connected. Then, we theoretically analyze the InfoNCE loss and recognize its limitations for computing gradients of negative samples. To better distinguish negative samples, we learn hard-valued weights for them based on node clustering and use prototypical contrastive learning to pull close embeddings of nodes in the same cluster. In addition, we propose a variant model AdaMEOW that adaptively learns soft-valued weights of negative samples to further improve node representation. Finally, we conduct extensive experiments to show the superiority of MEOW and AdaMEOW against other state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Jianxiang Yu and Qingqing Ge and Xiang Li and Aoying Zhou},
  doi          = {10.1109/TKDE.2024.3377431},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5181-5193},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Heterogeneous graph contrastive learning with meta-path contexts and adaptively weighted negative samples},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GAN-based temporal association rule mining on multivariate
time series data. <em>TKDE</em>, <em>36</em>(10), 5168–5180. (<a
href="https://doi.org/10.1109/TKDE.2023.3335049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature mining is a challenging work in the field of multivariate time series (MTS) data mining. Traditional methods suffer from three major issues. 1) Learned shapelets may seriously diverge from original subsequences since learning methods do not restrain the learned ones similar to raw sequences, which reduces interpretability. 2) Existing rule mining methods just generate association rules based on feature combination of different variables without considering temporal relations among features, which could not adequately express the essential characteristics of MTS data. 3) Most deep learning methods only mine global and high-level features of MTS data, which affects interpretability. To address these issues, we propose a temporal association rule mining method based on Generative Adversarial Network (GAN) called TAR-GAN. First, a shapelet mining method based on GAN (SGAN) is advanced to discover dataset-level and sample-level shapelets of all variables in MTS data. Second, a Temporal Graph based Rule Mining method (TGRM) is introduced to discover temporal association rules based on the temporal relationships among shapelets of different variables. Meanwhile, a Fast Convolution-based Similarity Measure method s (FCSM) is introduced to measure the similarity between MTS samples and temporal association rules. Furthermore, an adversarial training strategy is introduced to ensure the effectiveness and stability of generated temporal association rules, which could reflect the essential characteristics of MTS data. Extensive experiments on 12 datasets show the effectiveness and efficiency of our method.},
  archive      = {J_TKDE},
  author       = {Guoliang He and Lifang Dai and Zhiwen Yu and C. L. Philip Chen},
  doi          = {10.1109/TKDE.2023.3335049},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5168-5180},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GAN-based temporal association rule mining on multivariate time series data},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FRAMU: Attention-based machine unlearning using federated
reinforcement learning. <em>TKDE</em>, <em>36</em>(10), 5153–5167. (<a
href="https://doi.org/10.1109/TKDE.2024.3382726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Unlearning, a pivotal field addressing data privacy in machine learning, necessitates efficient methods for the removal of private or irrelevant data. In this context, significant challenges arise, particularly in maintaining privacy and ensuring model efficiency when managing outdated, private, and irrelevant data. Such data not only compromises model accuracy but also burdens computational efficiency in both learning and unlearning processes. To mitigate these challenges, we introduce a novel framework: Attention-based Machine Unlearning using Federated Reinforcement Learning (FRAMU). This framework incorporates adaptive learning mechanisms, privacy preservation techniques, and optimization strategies, making it a well-rounded solution for handling various data sources, either single-modality or multi-modality, while maintaining accuracy and privacy. FRAMU&#39;s strengths include its adaptability in fluctuating data landscapes, its ability to unlearn outdated, private, or irrelevant data, and its support for continual model evolution without compromising privacy. Our experiments, conducted on both single-modality and multi-modality datasets, revealed that FRAMU significantly outperformed baseline models. Additional assessments of convergence behavior and optimization strategies further validate the framework&#39;s utility in federated learning applications. Overall, FRAMU advances Machine Unlearning by offering a robust, privacy-preserving solution that optimizes model performance while also addressing key challenges in dynamic data environments.},
  archive      = {J_TKDE},
  author       = {Thanveer Shaik and Xiaohui Tao and Lin Li and Haoran Xie and Taotao Cai and Xiaofeng Zhu and Qing Li},
  doi          = {10.1109/TKDE.2024.3382726},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5153-5167},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {FRAMU: Attention-based machine unlearning using federated reinforcement learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Edge-protected triangle count estimation under relationship
local differential privacy. <em>TKDE</em>, <em>36</em>(10), 5138–5152.
(<a href="https://doi.org/10.1109/TKDE.2024.3381832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Triangle count estimation is a fundamental task in federated graph analysis. Yet, directly collecting local counts from users exposes individuals to severe privacy risks, as the local reports may reveal sensitive social connections. Protecting edge privacy in triangle count estimation is extremely challenging due to the strong data correlation among distinct users and large data sensitivity. Though many efforts have been put into addressing this issue, the existing works fail to provide a stringent privacy guarantee as well as a promising data utility. Motivated by this, we first propose an enhanced privacy notion namely Edge Relationship Local Differential Privacy (Edge-RLDP) that formally considers data correlations and provides a stringent privacy guarantee by hiding multiple edges in the global graph. Based on Edge-RLDP, we further propose a PRI vacy-preserved federated E stimator for T riangle count ( Privet ) with three perturbation algorithms, which enhances the estimation accuracy by designing specialized noise calibration schemes and leveraging a triangle-subsample trick. Theoretically, we prove that Privet achieves $(\varepsilon,\delta)$ -Edge-RLDP. Empirically, we verify that Privet provides promising estimation accuracy in terms of mean relative error.},
  archive      = {J_TKDE},
  author       = {Yuhan Liu and Tianhao Wang and Yixuan Liu and Hong Chen and Cuiping Li},
  doi          = {10.1109/TKDE.2024.3381832},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5138-5152},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Edge-protected triangle count estimation under relationship local differential privacy},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DMM: A deep reinforcement learning based map matching
framework for cellular data. <em>TKDE</em>, <em>36</em>(10), 5120–5137.
(<a href="https://doi.org/10.1109/TKDE.2024.3383881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel map matching framework that adopts deep learning techniques to map a sequence of cell tower locations to a trajectory on a road network. Map matching is an essential pre-processing step for many applications, such as traffic optimization and human mobility analysis. However, most recent approaches are based on hidden Markov models (HMMs) or neural networks that are hard to consider high-order location information or heuristics observed from real driving scenarios. In this paper, we develop a deep reinforcement learning based map matching framework for cellular data, named as DMM, which adopts a recurrent neural network (RNN) coupled with a reinforcement learning scheme to identify the most-likely trajectory of roads given a sequence of cell towers. To transform DMM into a practical system, several challenges are addressed by developing a set of techniques, including spatial-aware representation of input cell tower sequences, an encoder-decoder based RNN network for map matching model with variable-length input and output, and a global heuristics-driven reinforcement learning based scheme for optimizing the parameters of the encoder-decoder map matching model. Extensive experiments on a large-scale anonymized cellular dataset reveal that DMM provides high map matching accuracy and fast inference time.},
  archive      = {J_TKDE},
  author       = {Zhihao Shen and Kang Yang and Xi Zhao and Jianhua Zou and Wan Du and Junjie Wu},
  doi          = {10.1109/TKDE.2024.3383881},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5120-5137},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DMM: A deep reinforcement learning based map matching framework for cellular data},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discovering predictable latent factors for time series
forecasting. <em>TKDE</em>, <em>36</em>(10), 5106–5119. (<a
href="https://doi.org/10.1109/TKDE.2023.3335240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern temporal modeling methods, such as Transformer and its variants, have demonstrated remarkable capabilities in handling sequential data from specific domains like language and vision. Though achieving high performance with large-scale data, they often have redundant or unexplainable structures. When encountering some real-world datasets with limited observable variables that can be affected by many unknown factors, these methods may struggle to identify meaningful patterns and dependencies inherent in data, and thus, the modeling becomes unstable and unpredictable. To tackle this critical issue, in this article, we develop a novel algorithmic framework for inferring latent factors implied by the observed temporal data. The inferred factors are used to form multiple predictable and independent signal components that enable not only the reconstruction of future time series for accurate prediction but also sparse relation reasoning for long-term efficiency. To achieve this, we introduce three characteristics, i.e., predictability, sufficiency, and identifiability, and model these characteristics of latent factors via powerful deep latent dynamics models to infer the predictable signal components. Empirical results on multiple real datasets show the efficiency of our method for different kinds of time series forecasting tasks. Statistical analyses validate the predictability and interpretability of the learned latent factors.},
  archive      = {J_TKDE},
  author       = {Jingyi Hou and Zhen Dong and Jiayu Zhou and Zhijie Liu},
  doi          = {10.1109/TKDE.2023.3335240},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5106-5119},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Discovering predictable latent factors for time series forecasting},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). DForest: A minimal dimensionality-aware indexing for
high-dimensional exact similarity search. <em>TKDE</em>,
<em>36</em>(10), 5092–5105. (<a
href="https://doi.org/10.1109/TKDE.2024.3381111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of similarity search in high-dimensional space is a fundamental problem with numerous applications in computer science, yet it remains challenging due to the curse of dimensionality. This paper introduces DForest, a novel indexing approach designed to address this challenge for both range and kNN queries on high-dimensional data. Unlike previous similarity search approaches that apply a fixed dimensionality reduction to all objects uniformly, our approach determines the minimal dimensionality required for each object within a specified loss threshold and then reduces the dimensionality for each object individually. Furthermore, the query performance is also optimized by deriving the upper and lower bounds of retrieved blocks and computing distances in a low-embedding space preferentially. Theoretical analysis is provided to support our search strategy. Extensive experiments on a variety of datasets verify the superiority of DForest over the state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Lingli Li and Wenjing Sun and Baohua Wu},
  doi          = {10.1109/TKDE.2024.3381111},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5092-5105},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DForest: A minimal dimensionality-aware indexing for high-dimensional exact similarity search},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep multi-modal hashing with semantic enhancement for
multi-label micro-video retrieval. <em>TKDE</em>, <em>36</em>(10),
5080–5091. (<a href="https://doi.org/10.1109/TKDE.2023.3337077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pressing need for low storage and high efficiency has significantly propelled the advancement of deep hashing techniques in the realm of large-scale search and retrieval tasks. As one of the most prevailing forms of user-generated contents, micro-videos usually represent more complicated multi-modal behaviors that are further challenged in multi-label retrieval. Existing multi-modal hashing methods tend to prioritize the complementarity and consistency in multi-modal fusion, while neglecting the completeness problem. In this paper, we propose a deep multi-modal hashing with semantic enhancement (DMHSE) method that effectively integrates complete multi-modal representation learning with discriminative binary coding by means of collaboration between two distinct encoders, FoldCoder and HashCoder. FoldCoder translates latent multi-modal representation learning to a degradation process through mimicking data transmitting. Further, it incorporates a prompt learning paradigm to maximize the utilization of multi-label semantics for guiding representation learning. HashCoder combines pairwise and central constraints to ensure more discriminative hashing results. Pairwise constraint preserves the original local relevance structure, while central constraint tackles the problem of semantic ambiguity in multi-label data by leveraging the global label distribution. Experimental results demonstrate that DMHSE achieves superior performance in multi-label micro-video retrieval tasks.},
  archive      = {J_TKDE},
  author       = {Peiguang Jing and Haoyi Sun and Liqiang Nie and Yun Li and Yuting Su},
  doi          = {10.1109/TKDE.2023.3337077},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5080-5091},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Deep multi-modal hashing with semantic enhancement for multi-label micro-video retrieval},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data provenance via differential auditing. <em>TKDE</em>,
<em>36</em>(10), 5066–5079. (<a
href="https://doi.org/10.1109/TKDE.2023.3334821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rising awareness of data assets, data governance, which is to understand where data comes from, how it is collected, and how it is used, has been assuming ever-growing importance. One critical component of data governance gaining increasing attention is auditing machine learning models to determine if specific data has been used for training. Existing auditing techniques, like shadow auditing methods, have shown feasibility under specific conditions such as having access to label information and knowledge of training protocols. However, these conditions are often not met in most real-world applications. In this paper, we introduce a practical framework for auditing data provenance based on a differential mechanism, i.e., after carefully designed transformation, perturbed input data from the target model&#39;s training set would result in much more drastic changes in the output than those from the model&#39;s non-training set. Our framework is data-dependent and does not require distinguishing training data from non-training data or training additional shadow models with labeled output data. Furthermore, our framework extends beyond point-based data auditing to group-based data auditing, aligning with the needs of real-world applications. Our theoretical analysis of the differential mechanism and the experimental results on real-world data sets verify the proposal&#39;s effectiveness.},
  archive      = {J_TKDE},
  author       = {Xin Mu and Ming Pang and Feida Zhu},
  doi          = {10.1109/TKDE.2023.3334821},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5066-5079},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Data provenance via differential auditing},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contrastive time-series anomaly detection. <em>TKDE</em>,
<em>36</em>(10), 5053–5065. (<a
href="https://doi.org/10.1109/TKDE.2023.3335317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In addition to its success in representation learning, contrastive learning is effective in image anomaly detection. Although contrastive learning depends significantly on data augmentation methods, time-series data augmentation for time-series anomaly detection is not investigated sufficiently. Additionally, although time-series data share a temporal context, the existing contrastive loss contrasts temporally related samples, in which deteriorated anomaly detection performance is observed on time-series data. Herein, we propose contrastive multivariate time-series anomaly detection (CTAD), a multivariate time-series anomaly detection framework that addresses these challenges by incorporating a one-class learning scheme into the contrastive loss based on meticulously designed time-series data augmentations. Specifically, we propose seven types of general time-series data augmentations to be applied variable- and point-wise, and provide guidance on data augmentation methods for contrastive time-series anomaly detection. The superiority of the one-class contrastive loss and the appropriate selection of time-series data augmentation allow CTAD to achieve outstanding performance in multiple datasets, even using a simple long short-term memory network. Furthermore, CTAD is robust to noise as it trains a noise-invariant network. This enables up to 47× faster and 20× more memory-efficient anomaly detection performance compared with existing methods while affording robustness, which are essential considerations in real-world applications.},
  archive      = {J_TKDE},
  author       = {HyunGi Kim and Siwon Kim and Seonwoo Min and Byunghan Lee},
  doi          = {10.1109/TKDE.2023.3335317},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5053-5065},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Contrastive time-series anomaly detection},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bootstrapping on continuous-time dynamic graphs for crowd
flow modeling. <em>TKDE</em>, <em>36</em>(10), 5039–5052. (<a
href="https://doi.org/10.1109/TKDE.2024.3383663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous spatial-temporal learning methods have been proposed for crowd flow modeling, which is an important problem in Intelligent Transportation Systems (ITSs). However, most of the existing methods were designed to use data in one specific form to solve one particular task of crowd flow modeling and the shared patterns among different tasks have been largely ignored. In this paper, we investigate how to learn generic node representations that can simultaneously support various downstream tasks of crowd flow modeling. Along this line, we develop a continuous-time dynamic graph representation learning method based on Boot strapping for C rowd F low modeling ( BootCF ). Our approach follows a training procedure with two phases. In the pre-training phase, the continuous-time dynamic encoder converts edges with timestamps into messages to update the representations of the related traffic nodes. Inspired by the recent progress of contrastive learning, a bootstrapping framework for continuous-time dynamic graphs is designed to calculate pre-training loss and update the model in a self-supervised way, and thus enabling the node representation learning to be task-agnostic. Moreover, a context-aware data augmentation on continuous-time dynamic graphs is proposed to generate the augmented view of input data. Once the general node representations are obtained, the second phase can learn an effective model for any downstream task. Experiments on two real-world datasets show that our approach can achieve significant performance gain on four downstream tasks, which demonstrates that the proposed method has the powerful generalization capability for learning task-agnostic node representations.},
  archive      = {J_TKDE},
  author       = {Yi Xu and Liangzhe Han and Leilei Sun and Bowen Du and Chuanren Liu and Hui Xiong},
  doi          = {10.1109/TKDE.2024.3383663},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5039-5052},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Bootstrapping on continuous-time dynamic graphs for crowd flow modeling},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Black-box adversarial attack on graph neural networks with
node voting mechanism. <em>TKDE</em>, <em>36</em>(10), 5025–5038. (<a
href="https://doi.org/10.1109/TKDE.2024.3380750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have attracted significant research interest in various graph data modeling tasks. To advance trustworthy, reliable, and safe Artificial Intelligence (AI) systems for practical applications, adversarial robustness learning on GNNs has drawn widespread attention among researchers. Numerous attack methods, including white-box attacks, gray-box attacks, and black-box attacks, have been proposed, but black-box attacks are widely considered to be the most challenging and practical in real-world applications. In this paper, we focus on the challenging and realistic black-box attack scenario on GNNs, where the attacker has no information about the structure and parameters of the target model. We first theoretically demonstrate that the loss changes of the GNNs are related to the node voting matrix, which is subject to the graph topology information and is independent to the structures of GNNs. Then, we propose a novel black-box attack strategy for GNNs based on the theoretical results, i.e., node voting influence-based GNNs black-box adversarial attack, named VoteAttack. Specifically, the VoteAttack algorithm iteratively chooses a group of significant nodes based on mutual voting among nodes (the node voting matrix) and considers the voting weights among nodes. Furthermore, the VoteAttack algorithm modifies the attributes of the selected nodes to create a perturbed graph and ultimately utilizes the perturbed graph to attack GNNs. Experimental results on popular GNNs and graph datasets indicate that the proposed attack strategy outperforms baseline strategies.},
  archive      = {J_TKDE},
  author       = {Liangliang Wen and Jiye Liang and Kaixuan Yao and Zhiqiang Wang},
  doi          = {10.1109/TKDE.2024.3380750},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5025-5038},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Black-box adversarial attack on graph neural networks with node voting mechanism},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey for federated learning evaluations: Goals and
measures. <em>TKDE</em>, <em>36</em>(10), 5007–5024. (<a
href="https://doi.org/10.1109/TKDE.2024.3382002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluation is a systematic approach to assessing how well a system achieves its intended purpose. Federated learning (FL) is a novel paradigm for privacy-preserving machine learning that allows multiple parties to collaboratively train models without sharing sensitive data. However, evaluating FL is challenging due to its interdisciplinary nature and diverse goals, such as utility, efficiency, and security. In this survey, we first review the major evaluation goals adopted in the existing studies and then explore the evaluation metrics used for each goal. We also introduce FedEval , an open-source platform that provides a standardized and comprehensive evaluation framework for FL algorithms in terms of their utility, efficiency, and security. Finally, we discuss several challenges and future research directions for FL evaluation.},
  archive      = {J_TKDE},
  author       = {Di Chai and Leye Wang and Liu Yang and Junxue Zhang and Kai Chen and Qiang Yang},
  doi          = {10.1109/TKDE.2024.3382002},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  number       = {10},
  pages        = {5007-5024},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey for federated learning evaluations: Goals and measures},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WebUltron: An ultimate retriever on webpages under the
model-centric paradigm. <em>TKDE</em>, <em>36</em>(9), 4996–5006. (<a
href="https://doi.org/10.1109/TKDE.2023.3332858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document retrieval has been extensively studied within the index-retrieve framework for decades, which has withstood the test of time. However, this approach inherently segregates the indexing and retrieval processes, preventing a cohesive, end-to-end optimization. To bridge this divide, we introduce WebUltron, a revolutionary model-centric indexer for document retrieval. This system embeds the entirety of document knowledge within the model, striving for seamless end-to-end retrieval. Two primary challenges with this indexer are the representation of document identifiers (docids) and the model&#39;s training. Current methods grapple with docids that lack semantic depth and the constraints of limited supervised data, making scaling up to larger datasets challenging. Addressing this, we’ve engineered two novel docid types imbued with richer semantics that also streamline model inference. Further enhancing WebUltron&#39;s capabilities, we’ve developed a three-stage training regimen, leveraging deeper corpus insights and fortifying query-docid relationships. Experiments on two public datasets demonstrate the superiority of WebUltron over advanced baselines for document retrieval.},
  archive      = {J_TKDE},
  author       = {Yujia Zhou and Jing Yao and Ledell Wu and Zhicheng Dou and Ji-Rong Wen},
  doi          = {10.1109/TKDE.2023.3332858},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4996-5006},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {WebUltron: An ultimate retriever on webpages under the model-centric paradigm},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised transfer aided lifelong regression for learning
new tasks without target output. <em>TKDE</em>, <em>36</em>(9),
4981–4995. (<a href="https://doi.org/10.1109/TKDE.2024.3372462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an emerging learning paradigm, lifelong learning solves multiple consecutive tasks based upon previously accumulated knowledge. When facing with a new task, existing lifelong learning approaches need both input and desired output data to construct task models before knowledge transfer can succeed. However, labeling each task requires extensive labors and time, which can be prohibitive for real-world lifelong regression problems. To reduce this burden, we propose to incorporate unsupervised feature into lifelong regression via coupled dictionary learning, enabling to learn new tasks without target output data. Specifically, the input data for each task is encoded as unsupervised feature while both input and output data are used to construct task predictor. The unsupervised feature is linked with task predictor through two dictionaries that are coupled by a joint sparse representation. Because of the learned coupling between the two spaces, the task predictor for the new coming task can be recovered given only the input data. We further incorporate active task selection into this framework, enabling actively choosing tasks to learn in a task-efficient manner. Three case studies are used to evaluate the effectiveness of our method, in comparison with existing lifelong learning approaches. Results show that our method is able to accurately predict new tasks through unsupervised transfer, eliminating the need to label tasks before constructing the predictor.},
  archive      = {J_TKDE},
  author       = {Tong Liu and Xulong Wang and Po Yang and Sheng Chen and Chris J. Harris},
  doi          = {10.1109/TKDE.2024.3372462},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4981-4995},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Unsupervised transfer aided lifelong regression for learning new tasks without target output},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UNM: A universal approach for noisy multi-label learning.
<em>TKDE</em>, <em>36</em>(9), 4968–4980. (<a
href="https://doi.org/10.1109/TKDE.2024.3373500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label image classification relies on a large-scale, well-maintained dataset, which may easily be mislabeled due to various subjective reasons. Existing methods for coping with noise usually focus on improving the model robustness in the case of single-label noise. However, compared with noisy single-label learning, noisy multi-label learning is more practical and challenging. To reduce the negative impact of noisy multi-annotations, we propose a universal approach for noisy multi-label learning (UNM). In UNM, we propose the label-wise embedding network which investigates the semantic alignment between label embeddings and their corresponding output features to learn robust feature representations. Meanwhile, mining the co-occurrence of multi-labels is also added to regularize the noisy network predictions. We cyclically change the fitting status of our label-wise embedding network to distinguish the noisy samples and generate pseudo labels for them. As a result, UNM provides an effective way to exploit the label-wise features and semantic label embeddings in noisy scenarios. To verify the generalizability of our method, we also test our method on Partial Multi-label Learning (PML) and Multi-label Learning with Missing Labels (MLML). Extensive experiments on benchmark datasets including Microsoft COCO, Pascal VOC, and Visual Genome explicitly validate the proposed method.},
  archive      = {J_TKDE},
  author       = {Jia-Yao Chen and Shao-Yuan Li and Sheng-Jun Huang and Songcan Chen and Lei Wang and Ming-Kun Xie},
  doi          = {10.1109/TKDE.2024.3373500},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4968-4980},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {UNM: A universal approach for noisy multi-label learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Text-rich graph neural networks with subjective-objective
semantic modeling. <em>TKDE</em>, <em>36</em>(9), 4956–4967. (<a
href="https://doi.org/10.1109/TKDE.2024.3378914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs), which obtain node embeddings by attribute propagates along graph topology, exhibit significant power in graph-structured data mining. However, graphs in the real world are usually text-rich, where the text can not only be represented as node attributes, but also contains valuable objective semantic structures. Moreover, the graph topology also exhibits complex subjective semantic structures, especially the heterophily where nodes from different classes are prone to build connections, making existing GNNs that work under the assumption of homophily incapable to realize generalization. To tackle aforementioned limitations, we design a new text-rich graph neural network from a unified perspective, namely SO-GNN. It can effectively enhance the expressive power of GNNs by modeling the implicit but informative subjective-objective semantics underlying the text-rich graphs. Specifically, we first introduce a new constrained Markov matrix with well-defined probabilistic diffusion dynamics to guide information propagation, where the neighbors are more appropriate and indicative in providing both local and global subjective semantics. We then construct a flexible heterogeneous text graph to gain a deeper insight into objective semantics, providing indispensable information for learning node embedding. Finally, we unite subjective and objective semantics in an end-to-end manner, so that the model can fully utilize the most relevant information for downstream tasks. Extensive experiments across various text-rich graphs with low-to-high homophily demonstrate the effectiveness and flexibility of the proposed SO-GNN over state-of-the-arts.},
  archive      = {J_TKDE},
  author       = {Yawen Li and Zhizhi Yu and Dongxiao He},
  doi          = {10.1109/TKDE.2024.3378914},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4956-4967},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Text-rich graph neural networks with subjective-objective semantic modeling},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Task allocation in spatial crowdsourcing: An efficient
geographic partition framework. <em>TKDE</em>, <em>36</em>(9),
4943–4955. (<a href="https://doi.org/10.1109/TKDE.2024.3374086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed a revolution in Spatial Crowdsourcing (SC), in which people with mobile connectivity can perform spatio-temporal tasks that involve traveling to specified locations. In this paper, we identify and study in depth a new multi-center-based task allocation problem in the context of SC, where multiple allocation centers exist. In particular, we aim to maximize the total number of the allocated tasks while minimizing the allocated task number difference. To solve the problem, we propose a two-phase framework, called Task Allocation with Geographic Partition, consisting of a geographic partition and a task allocation phase. The first phase divides the whole study area based on the allocation centers by using both a basic Voronoi diagram-based algorithm and an adaptive weighted Voronoi diagram-based algorithm. In the allocation phase, we utilize a Reinforcement Learning method to achieve the task allocation, where a graph neural network with the attention mechanism is used to learn the embeddings of allocation centers, delivery points, and workers. To further improve the efficiency, we propose an early stopping optimization strategy for the adaptive weighted Voronoi diagram-based algorithm in the geographic partition phase and give a distance-constrained graph pruning strategy for the Reinforcement Learning method in the task allocation phase. Extensive experiments give insight into the effectiveness and efficiency of the proposed solutions.},
  archive      = {J_TKDE},
  author       = {Yan Zhao and Xuanlei Chen and Guanyu Ye and Fangda Guo and Kai Zheng and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2024.3374086},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4943-4955},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Task allocation in spatial crowdsourcing: An efficient geographic partition framework},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supervised feature selection via multi-center and local
structure learning. <em>TKDE</em>, <em>36</em>(9), 4930–4942. (<a
href="https://doi.org/10.1109/TKDE.2024.3372657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection has achieved unprecedented success in obtaining sparse discriminative features. However, the existing methods almost use the $\ell _{2,p}$ -norm constraint on transformation matrix to obtain sparse features, which introduces extra parameters and cannot obtain the features directly. In addition, existing algorithms only focused on the global structure and ignored the local structure, leading to poor performance when solving data with non-Gaussian distributions which a single center point cannot describe precisely. Based on above considerations, we propose a supervised feature selection via multi-center and local structure learning. We further introduce trace ratio criterion into our model in favor of improving the discriminant of features selected. In order to address the overlap problem, we use multiple center points to match the distribution of data and construct a $k$ -Nearest Neighbor graph to explore the local structure of the data. In addition, we also propose an efficient method to optimize the transformation matrix with the $\ell _{2,0}$ -norm constraint and can directly obtain the sparse features. We evaluate our method on Toy datasets and several real-world datasets, show improvement over state-of-the-art feature selection methods, and demonstrate the effectiveness of our model in dealing with non-Gaussian distributed data problems.},
  archive      = {J_TKDE},
  author       = {Canyu Zhang and Feiping Nie and Rong Wang and Xuelong Li},
  doi          = {10.1109/TKDE.2024.3372657},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4930-4942},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Supervised feature selection via multi-center and local structure learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Smart contract vulnerability detection based on automated
feature extraction and feature interaction. <em>TKDE</em>,
<em>36</em>(9), 4916–4929. (<a
href="https://doi.org/10.1109/TKDE.2023.3333371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart contract is the core of blockchain operation, and contract vulnerability will cause huge economic losses. Therefore, effective smart contract vulnerability detection is of vital importance and attracts more and more attention. In this paper, we propose a vulnerability detection model (VDM-AEI) based on automatic feature extraction and feature interaction. For the first time, this model converts smart contracts into gray images and uses VGG16 and GRU models to automatically extract vulnerability features and filter effective features, respectively. Then, a contract graph and an expert knowledge feature vector are constructed by using commonly used methods as part of feature construction. Next, AutoInt and DCN networks are used to build a dual feature interaction network to obtain more abundant vulnerability feature information, which extracts high-dimensional nonlinear features from the low and sparse features of the contract graph feature vector and the expert knowledge-defined feature vector. Finally, all output features of GRU, AutoInt and DCN networks are integrated to obtain vulnerability classification results through fully connected neural networks. We conducted extensive experiments on the ESC and VSC datasets for reentrancy vulnerabilities, timestamp dependency vulnerabilities, and infinite loop vulnerabilities. The experimental results prove the effectiveness and accuracy of the VDM-AEI model. Compared with the latest vulnerability detection model CGE, the accuracy rates of the 3 types of vulnerability detection are improved by 10.85%, 6.18%, and 12.34%, respectively. In addition, the predicted F1 scores of VDM-AEI are all greater than 95%, and the recall rate is no less than 94%.},
  archive      = {J_TKDE},
  author       = {Lina Li and Yang Liu and Guodong Sun and Nianfeng Li},
  doi          = {10.1109/TKDE.2023.3333371},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4916-4929},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Smart contract vulnerability detection based on automated feature extraction and feature interaction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SHINE: A scalable heterogeneous inductive graph neural
network for large imbalanced datasets. <em>TKDE</em>, <em>36</em>(9),
4904–4915. (<a href="https://doi.org/10.1109/TKDE.2024.3381240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research interest in machine learning (ML) for graphs has skyrocketed in recent years. However, non-euclidean graph structures inhibit the application of traditional ML algorithms. Consequently, scholars introduced graph learning algorithms tailored to network data, such as graph neural networks (GNNs). Most GNNs are designed for homogeneous and homophilous graphs and are evaluated on small, static, and balanced datasets, deviating from real-world conditions and industry applications. This paper introduces SHINE, a scalable heterogeneous inductive GNN for large imbalanced datasets. SHINE addresses four key challenges: scalability, network heterogeneity, inductive learning on dynamic graphs, and imbalanced node classification. SHINE comprises three core components: 1) a sampler based on nearest-neighbor (NN) search, 2) a heterogeneous GNN (HGNN) layer with a novel relationship aggregator, and 3) aggregator functions tailored to skewed class distributions. The components of SHINE are evaluated on benchmark datasets, while the integrated benefits of SHINE are demonstrated on two fraud detection datasets.},
  archive      = {J_TKDE},
  author       = {Rafaël Van Belle and Jochen De Weerdt},
  doi          = {10.1109/TKDE.2024.3381240},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4904-4915},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SHINE: A scalable heterogeneous inductive graph neural network for large imbalanced datasets},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Sentence bag graph formulation for biomedical distant
supervision relation extraction. <em>TKDE</em>, <em>36</em>(9),
4890–4903. (<a href="https://doi.org/10.1109/TKDE.2024.3377229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel graph-based framework for alleviating key challenges in distantly-supervised relation extraction and demonstrate its effectiveness in the challenging and important domain of biomedical data. Specifically, we propose a graph view of sentence bags referring to an entity pair, which enables message-passing based aggregation of information related to the entity pair over the sentence bag. The proposed framework alleviates the common problem of noisy labeling in distantly supervised relation extraction and also effectively incorporates inter-dependencies between sentences within a bag. Extensive experiments on two large-scale biomedical relation datasets and the widely utilized NYT dataset demonstrate that our proposed framework significantly outperforms the state-of-the-art methods for biomedical distant supervision relation extraction while also providing excellent performance for relation extraction in the general text mining domain.},
  archive      = {J_TKDE},
  author       = {Hao Zhang and Yang Liu and Xiaoyan Liu and Tianming Liang and Gaurav Sharma and Liang Xue and Maozu Guo},
  doi          = {10.1109/TKDE.2024.3377229},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4890-4903},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Sentence bag graph formulation for biomedical distant supervision relation extraction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RNP-miner: Repetitive nonoverlapping sequential pattern
mining. <em>TKDE</em>, <em>36</em>(9), 4874–4889. (<a
href="https://doi.org/10.1109/TKDE.2023.3334300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential pattern mining (SPM) is an important branch of knowledge discovery that aims to mine frequent sub-sequences (patterns) in a sequential database. Various SPM methods have been investigated, and most of them are classical SPM methods, since these methods only consider whether or not a given pattern occurs within a sequence. Classical SPM can only find the common features of sequences, but it ignores the number of occurrences of the pattern in each sequence, i.e., the degree of interest of specific users. To solve this problem, this paper addresses the issue of repetitive nonoverlapping sequential pattern (RNP) mining and proposes the RNP-Miner algorithm. To reduce the number of candidate patterns, RNP-Miner adopts an itemset pattern join strategy. To improve the efficiency of support calculation, RNP-Miner utilizes the candidate support calculation algorithm based on the position dictionary. To validate the performance of RNP-Miner, 10 competitive algorithms and 20 sequence databases were selected. The experimental results verify that RNP-Miner outperforms the other algorithms, and using RNPs can achieve a better clustering performance than raw data and classical frequent patterns.},
  archive      = {J_TKDE},
  author       = {Meng Geng and Youxi Wu and Yan Li and Jing Liu and Philippe Fournier-Viger and Xingquan Zhu and Xindong Wu},
  doi          = {10.1109/TKDE.2023.3334300},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4874-4889},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {RNP-miner: Repetitive nonoverlapping sequential pattern mining},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rewriting queries for hyper-relations. <em>TKDE</em>,
<em>36</em>(9), 4862–4873. (<a
href="https://doi.org/10.1109/TKDE.2024.3374076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyper-relation is a recently proposed formal concept for denormalization of relational databases. Hyper-relations are structures of prejoined relations the schema of which can be represented as a hyper-graph. Instead of entire relations of the tables in the initial normalized schema, hyper-relations prejoin their atomic constituents called base relations, and do that with only those base relations that are really used in the queries being optimized. In this paper, we derive a simple and efficient algorithm for rewriting queries for hyper-relations. The algorithm is based on incremental rewriting procedure that traverses the query expression tree and builds up the transformed query by seamlessly propagating the proposed $\Sigma$ operator, composed of selection and nullification of the tuples in a hyper-relation. The algorithm is applicable to a wide range of multiblock (nested) select-project-(outer) join queries.},
  archive      = {J_TKDE},
  author       = {Dragan Milićev and Živojin Šuštran},
  doi          = {10.1109/TKDE.2024.3374076},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4862-4873},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Rewriting queries for hyper-relations},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revisiting attack-caused structural distribution shift in
graph anomaly detection. <em>TKDE</em>, <em>36</em>(9), 4849–4861. (<a
href="https://doi.org/10.1109/TKDE.2024.3380709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph anomaly detection (GAD) under semi-supervised setting poses a significant challenge due to the distinct structural distribution between anomalous and normal nodes. Specifically, anomalous nodes constitute a minority and exhibit high heterophily and low homophily compared to normal nodes. The distribution of neighbors of the two types of nodes is close, making them difficult to distinguish during aggregation. Furthermore, we discover that apart from various time factors and annotation preferences, graph adversarial attacks can amplify the heterophily difference across training and testing data, namely distribution shift (SDS) in this paper. Current methods for GAD tend to overlook SDS, resulting in poor generalization and limited effectiveness. This work solves the problem from a feature view. We observe that the degree of SDS varies between anomalies and normal nodes. Hence the key lies in (1) resisting high heterophily for anomalies and (2) benefiting the learning of normals from homophily. To this end, we design a Graph Decomposition Network (GDN), which not only teases out the anomaly features that make great contributions to GAD to mitigate the effect of heterophilous neighbors and make them invariant, but also constrain the remaining features for normal nodes to preserve the connectivity of nodes and reinforce the influence of the homophilous neighborhood. To further validate the effectiveness of our method, we illustrate the feature decomposition process in spectral domain, and we also conduct an adversarial attack to incur different heterophily degrees under SDS. Extensive experimental results demonstrate that our framework achieves both accuracy and robustness enhancement.},
  archive      = {J_TKDE},
  author       = {Yuan Gao and Jinghan Li and Xiang Wang and Xiangnan He and Huamin Feng and Yongdong Zhang},
  doi          = {10.1109/TKDE.2024.3380709},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4849-4861},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Revisiting attack-caused structural distribution shift in graph anomaly detection},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). PanDa: Prompt transfer meets knowledge distillation for
efficient model adaptation. <em>TKDE</em>, <em>36</em>(9), 4835–4848.
(<a href="https://doi.org/10.1109/TKDE.2024.3376453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prompt Transfer (PoT) is a recently-proposed approach to improve prompt-tuning, by initializing the target prompt with the existing prompt trained on similar source tasks. However, such a vanilla PoT approach usually achieves sub-optimal performance, as (i) the PoT is sensitive to the similarity of source-target pair and (ii) directly fine-tuning the prompt initialized with source prompt on target task might lead to forgetting of the useful general knowledge learned from source task. To tackle these issues, we propose a new metric to accurately predict the prompt transferability (regarding (i)), and a novel PoT approach (namely PanDa ) that leverages the knowledge distillation technique to alleviate the knowledge forgetting effectively (regarding (ii)). Extensive and systematic experiments on 189 combinations of 21 source and 9 target datasets across 5 scales of PLMs demonstrate that: 1) our proposed metric works well to predict the prompt transferability ; 2) our PanDa consistently outperforms the vanilla PoT approach by 2.3% average score (up to 24.1%) among all tasks and model sizes ; 3) with our PanDa approach, prompt-tuning can achieve competitive and even better performance than model-tuning in various PLM scales scenarios .},
  archive      = {J_TKDE},
  author       = {Qihuang Zhong and Liang Ding and Juhua Liu and Bo Du and Dacheng Tao},
  doi          = {10.1109/TKDE.2024.3376453},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4835-4848},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {PanDa: Prompt transfer meets knowledge distillation for efficient model adaptation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online learning for data streams with incomplete features
and labels. <em>TKDE</em>, <em>36</em>(9), 4820–4834. (<a
href="https://doi.org/10.1109/TKDE.2024.3374357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online learning is critical for handling complex data streams in Big Data-related applications. This study explores a new online learning problem where both the features and labels are incomplete. Such incompleteness poses a critical challenge in determining the latent relationship between incomplete features and labels. Unfortunately, existing online learning methods only consider a few cases of incomplete feature spaces, such as trapezoidal, evolvable, and capricious data streams, limiting their applicability to this problem. To bridge this gap, this study proposes a novel algorithm of O nline L earning for Data Streams with I ncomplete F eatures and L abels (OLIFL). OLIFL imposes no constraints on changing patterns of feature space and does not require all instances to be labeled with two-fold ideas. First, OLIFL explores the informativeness of individual features to update the classifier by dynamically maintaining global feature space and updating the informativeness matrix. Second, it estimates the label confidence of unlabeled instances to control their negative effects by limiting the error upper bound. Extensive experiments on benchmark datasets are conducted in five scenarios: three incomplete feature (trapezoidal, evolvable, and capricious) spaces, and two incomplete labels (only missing labels and missing both features and labels). In addition, we explore the sensitivity of the model to parameters, and its usability and response efficiency in handling concept drifts. The results show that OLIFL significantly outperforms its rivals. Moreover, we use OLIFL to classify a movie review task as real application verification.},
  archive      = {J_TKDE},
  author       = {Dianlong You and Huigui Yan and Jiawei Xiao and Zhen Chen and Di Wu and Limin Shen and Xindong Wu},
  doi          = {10.1109/TKDE.2024.3374357},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4820-4834},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Online learning for data streams with incomplete features and labels},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online feature selection with varying feature spaces.
<em>TKDE</em>, <em>36</em>(9), 4806–4819. (<a
href="https://doi.org/10.1109/TKDE.2024.3377243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection, an essential technique in data mining, is often confined to batch learning or online idealization of data scenarios despite its significance. Existing online feature selection methods have specific assumptions regarding the data stream, such as requiring a fixed feature space with an explicit pattern and complete labeling of samples. Unfortunately, data streams generated in many real scenarios commonly exhibit arbitrarily incomplete feature spaces and scarcity labels, making existing approaches unsuitable for real applications. To fill these gaps, this study proposes a new problem called Online Feature Selection with Varying Features Spaces (OFSVF). OFSVF has a three-fold main idea: 1) it leverages Gaussian Copula to model the incomplete feature correlation in a complete latent space, encoded by continuous variables, 2) it employs a novel tree-ensemble-based approach to select the most informative features on-the-fly, and 3) it develops the underlying geometric structure of instances to establish the relationship between unlabeled and labels. Experimental results are documented to demonstrate the feasibility and effectiveness of our proposed method.},
  archive      = {J_TKDE},
  author       = {Sheng-Da Zhuo and Jin-Jie Qiu and Chang-Dong Wang and Shu-Qiang Huang},
  doi          = {10.1109/TKDE.2024.3377243},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4806-4819},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Online feature selection with varying feature spaces},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mitigating hidden confounding effects for causal
recommendation. <em>TKDE</em>, <em>36</em>(9), 4794–4805. (<a
href="https://doi.org/10.1109/TKDE.2024.3378482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems suffer from confounding biases when there exist confounders affecting both item features and user feedback (e.g., like or not). Existing causal recommendation methods typically assume confounders are fully observed and measured, forgoing the possible existence of hidden confounders in real applications. For instance, product quality is a confounder since it affects both item prices and user ratings, but is hidden for the third-party e-commerce platform due to the difficulty of large-scale quality inspection; ignoring it could result in the bias effect of over-recommending high-price items. This work analyzes and addresses the problem from a causal perspective. The key lies in modeling the causal effect of item features on a user&#39;s feedback. To mitigate hidden confounding effects, it is compulsory but challenging to estimate the causal effect without measuring the confounder. Towards this goal, we propose a Hidden Confounder Removal (HCR) framework that leverages front-door adjustment to decompose the causal effect into two partial effects, according to the mediators between item features and user feedback. The partial effects are independent from the hidden confounder and identifiable. During training, HCR performs multi-task learning to infer the partial effects from historical interactions. We instantiate HCR for two scenarios and conduct experiments on three real-world datasets. Empirical results show that the HCR framework provides more accurate recommendations, especially for less-active users. We will release the code once accepted.},
  archive      = {J_TKDE},
  author       = {Xinyuan Zhu and Yang Zhang and Fuli Feng and Xun Yang and Dingxian Wang and Xiangnan He},
  doi          = {10.1109/TKDE.2024.3378482},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4794-4805},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Mitigating hidden confounding effects for causal recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MatchXML: An efficient text-label matching framework for
extreme multi-label text classification. <em>TKDE</em>, <em>36</em>(9),
4781–4793. (<a href="https://doi.org/10.1109/TKDE.2024.3374750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The eXtreme Multi-label text Classification (XMC) refers to training a classifier that assigns a text sample with relevant labels from an extremely large-scale label set (e.g., millions of labels). We propose MatchXML, an efficient text-label matching framework for XMC. We observe that the label embeddings generated from the sparse Term Frequency-Inverse Document Frequency (TF–IDF) features have several limitations. We thus propose label2vec to effectively train the semantic dense label embeddings by the Skip-gram model. The dense label embeddings are then used to build a Hierarchical Label Tree by clustering. In fine-tuning the pre-trained encoder Transformer, we formulate the multi-label text classification as a text-label matching problem in a bipartite graph. We then extract the dense text representations from the fine-tuned Transformer. Besides the fine-tuned dense text embeddings, we also extract the static dense sentence embeddings from a pre-trained Sentence Transformer. Finally, a linear ranker is trained by utilizing the sparse TF–IDF features, the fine-tuned dense text representations, and static dense sentence features. Experimental results demonstrate that MatchXML achieves the state-of-the-art accuracies on five out of six datasets. As for the training speed, MatchXML outperforms the competing methods on all the six datasets.},
  archive      = {J_TKDE},
  author       = {Hui Ye and Rajshekhar Sunderraman and Shihao Ji},
  doi          = {10.1109/TKDE.2024.3374750},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4781-4793},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MatchXML: An efficient text-label matching framework for extreme multi-label text classification},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Maintaining top-<span class="math inline"><em>t</em></span>
cores in dynamic graphs. <em>TKDE</em>, <em>36</em>(9), 4766–4780. (<a
href="https://doi.org/10.1109/TKDE.2023.3332638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs have been widely used in many applications. One important graph analytics is to explore cohesive subgraphs in a large graph. Among several cohesive subgraphs studied, $k$ -core is one that can be computed in linear time for a static graph. Since graphs are evolving in real applications, in this paper, we study core maintenance which is to reduce the computational cost to compute $k$ -cores for a graph when graphs are updated from time to time dynamically. We identify drawbacks of the existing efficient algorithm, which needs a large search space to find the vertices that need to be updated, and has high overhead to maintain the index built, when a graph is updated. We propose a new order-based approach to maintain an order, called $k$ -order, among vertices, while a graph is updated. Our new algorithm can significantly outperform the state-of-the-art algorithm up to 3 orders of magnitude for the 11 large real graphs tested. In addition, we also study the problem of partial core maintenance, which is to maintain the top- $t$ cores of the graph for a given positive integer $t$ . By instead maintaining only a small subset of cores, further improvement in performance can be obtained.},
  archive      = {J_TKDE},
  author       = {Yikai Zhang and Jeffrey Xu Yu and Ying Zhang and Lu Qin},
  doi          = {10.1109/TKDE.2023.3332638},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4766-4780},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Maintaining top-$t$ cores in dynamic graphs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning with location-based fairness: A
statistically-robust framework and acceleration. <em>TKDE</em>,
<em>36</em>(9), 4750–4765. (<a
href="https://doi.org/10.1109/TKDE.2024.3371460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fairness related to locations (i.e., “where”) is critical for the use of machine learning in a variety of societal domains involving spatial datasets (e.g., agriculture, disaster response, urban planning). Spatial biases incurred by learning, if left unattended, may cause or exacerbate unfair distribution of resources, social division, spatial disparity, etc. The goal of this work is to develop statistically-robust formulations and model-agnostic learning strategies to understand and promote spatial fairness. The problem is challenging as locations are often from continuous spaces with no well-defined categories (e.g., gender), and statistical conclusions from spatial data are fragile to changes in spatial partitionings and scales. Existing studies in fairness-driven learning have generated valuable insights related to non-spatial factors including race, gender, education level, etc., but research to mitigate location-related biases still remains in its infancy, leaving the main challenges unaddressed. To bridge the gap, we first propose a robust space-as-distribution (SPAD) representation of spatial fairness to reduce statistical sensitivity related to partitionings and scales in continuous space. Furthermore, we propose a new SPAD-based stochastic strategy to efficiently optimize over an extensive distribution of fairness criteria, and a bi-level training framework to enforce fairness via adaptive adjustment of priorities among locations. Finally, we extend this framework with a similarity-based training strategy to improve the computational efficiency. Experiments conducted on two real-world problems, crop monitoring in the US and palm oil plantation mapping in Indonesia, show that SPAD can effectively reduce sensitivity in fairness evaluation and the stochastic bi-level training framework can greatly improve the fairness. Controlled experiments also show that similarity-based acceleration can greatly reduce the training time while keeping the prediction performance and fairness results at the same level.},
  archive      = {J_TKDE},
  author       = {Erhu He and Yiqun Xie and Weiye Chen and Sergii Skakun and Han Bao and Rahul Ghosh and Praveen Ravirathinam and Xiaowei Jia},
  doi          = {10.1109/TKDE.2024.3371460},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4750-4765},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning with location-based fairness: A statistically-robust framework and acceleration},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). I-razor: A differentiable neural input razor for feature
selection and dimension search in DNN-based recommender systems.
<em>TKDE</em>, <em>36</em>(9), 4736–4749. (<a
href="https://doi.org/10.1109/TKDE.2023.3332671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Input features play a crucial role in DNN-based recommender systems with thousands of categorical and continuous fields from users, items, contexts, and interactions. Noisy features and inappropriate embedding dimension assignments can deteriorate the performance of recommender systems and introduce unnecessary complexity in model training and online serving. Optimizing the input configuration of DNN models, including feature selection and embedding dimension assignment, has become one of the essential topics in feature engineering. However, in existing industrial practices, feature selection and dimension search are optimized sequentially, i.e., feature selection is performed first, followed by dimension search to determine the optimal dimension size for each selected feature. Such a sequential optimization mechanism increases training costs and risks generating suboptimal input configurations. To address this problem, we propose a differentiable neural i nput razor ( i-Razor ) that enables joint optimization of feature selection and dimension search. Concretely, we introduce an end-to-end differentiable model to learn the relative importance of different embedding regions of each feature. Furthermore, a flexible pruning algorithm is proposed to achieve feature filtering and dimension derivation simultaneously. Extensive experiments on two large-scale public datasets in the Click-Through-Rate (CTR) prediction task demonstrate the efficacy and superiority of i-Razor in balancing model complexity and performance.},
  archive      = {J_TKDE},
  author       = {Yao Yao and Bin Liu and Haoxun He and Dakui Sheng and Ke Wang and Li Xiao and Huanhuan Cao},
  doi          = {10.1109/TKDE.2023.3332671},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4736-4749},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {I-razor: A differentiable neural input razor for feature selection and dimension search in DNN-based recommender systems},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical dynamic graph clustering network.
<em>TKDE</em>, <em>36</em>(9), 4722–4735. (<a
href="https://doi.org/10.1109/TKDE.2023.3333529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Connections between visual components are ubiquitous. Graphs, as a highly flexible data structure, not only allow imposing relational induction bias on data, but can provide a completely distinct learning perspective for regular image data. In this article, we propose a hierarchical dynamic graph clustering network (HDGCN) for visual feature learning. We construct hierarchical graph representations in graph domain in an adaptive, data-adaptive and task-adaptive manner. First, the initial graph is constructed in high-dimensional feature domain of images. To mine the hierarchical geometric features in latent graph space, adaptive clustering network (ClusterNet) is performed to learn discriminative clusters and generates cluster-based coarse graph. Then, graph convolutional networks (GCNs) are used to diffuse, transform and aggregate information among clusters. So, the intra-class and inter-class information is fully explored to increase the discriminativity of graph representations. Next, coarsened graph representations are mapped to grid based on its affinity with linear projection features. To further improve the task adaptation of clusters and hierarchical graph representations, ClusterNet and GCNs are fused in the same framework for end-to-end training and clusters is updated dynamically. We have conducted extensive experiments on classification and segmentation tasks. The experimental results fully validate the robustness of the proposed algorithm.},
  archive      = {J_TKDE},
  author       = {Jie Chen and Licheng Jiao and Xu Liu and Lingling Li and Fang Liu and Puhua Chen and Shuyuan Yang and Biao Hou},
  doi          = {10.1109/TKDE.2023.3333529},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4722-4735},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hierarchical dynamic graph clustering network},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GTRL: An entity group-aware temporal knowledge graph
representation learning method. <em>TKDE</em>, <em>36</em>(9),
4707–4721. (<a href="https://doi.org/10.1109/TKDE.2023.3334165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal Knowledge Graph (TKG) representation learning embeds entities and event types into a continuous low-dimensional vector space by integrating the temporal information, which is essential for downstream tasks, e.g., event prediction and question answering. Existing methods stack multiple graph convolution layers to model the influence of distant entities, leading to the over-smoothing problem. To alleviate the problem, recent studies infuse reinforcement learning to obtain paths that contribute to modeling the influence of distant entities. However, due to the limited number of hops, these studies fail to capture the correlation between entities that are far apart and even unreachable. To this end, we propose GTRL, an entity Group-aware Temporal knowledge graph Representation Learning method. GTRL is the first work that incorporates the entity group modeling to capture the correlation between entities by stacking only a finite number of layers. Specifically, the entity group mapper is proposed to generate entity groups from entities in a learning way. Based on entity groups, the implicit correlation encoder is introduced to capture implicit correlations between any pairwise entity groups. In addition, the hierarchical GCNs are exploited to accomplish the message aggregation and representation updating on the entity group graph and the entity graph. Finally, GRUs are employed to capture the temporal dependency in TKGs. Extensive experiments on six real-world datasets demonstrate that GTRL achieves the state-of-the-art performances on the event prediction task, outperforming the best baseline by an average of 7.35%, 6.09%, 8.31%, and 11.21% in MRR, Hits@1, Hits@3, and Hits@10, respectively.},
  archive      = {J_TKDE},
  author       = {Xing Tang and Ling Chen},
  doi          = {10.1109/TKDE.2023.3334165},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4707-4721},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GTRL: An entity group-aware temporal knowledge graph representation learning method},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph alignment neural network model with graph to sequence
learning. <em>TKDE</em>, <em>36</em>(9), 4693–4706. (<a
href="https://doi.org/10.1109/TKDE.2023.3329380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network alignment aims at detecting the corresponding entities across multiple networks, which is an essential basis for the fusion and analysis of multiple network information. Moreover, embedding-based network alignment has gradually become one of the promising methods. However, existing methods ignore the confusing selection problem caused by the similarity-orientated principle of network embedding and over-dependence on the hypothesis of structural consistency. In this paper, we propose an end-to-end Graph Alignment Neural Network (GANN) model with graph-to-sequence learning. GANN mainly consists of two modules: Graph encoder and Sequence decoder. In graph encoder module, we present a restricted network embedding method, which can not only capture the local structure and attribute information of nodes but also realize the constraint of node embedding and space reconciliation. In sequence decoder module, we propose a graph-to-sequence learning model to address large graphs’ structural consistency hypothesis problem. In this model, an attention-based LSTM mechanism is introduced to infer a node in the source network corresponding to the candidate node sequence in target networks. In this candidate sequence, the correct aligned node is placed at the top. We demonstrate that GANN outperforms the state-of-the-art methods in network alignment tasks on various real-world datasets.},
  archive      = {J_TKDE},
  author       = {Nianwen Ning and Bin Wu and Haoqing Ren and Qiuyue Li},
  doi          = {10.1109/TKDE.2023.3329380},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4693-4706},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph alignment neural network model with graph to sequence learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FELight: Fairness-aware traffic signal control via
sample-efficient reinforcement learning. <em>TKDE</em>, <em>36</em>(9),
4678–4692. (<a href="https://doi.org/10.1109/TKDE.2024.3376745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic congestion is becoming an increasingly prominent problem, and intelligent traffic signal control methods can effectively alleviate it. Recently, there has been a growing trend of applying reinforcement learning to traffic signal control for adaptive signal scheduling. However, most existing methods focus on improving traffic performance while neglecting the issue of scheduling fairness, resulting in long waiting time for some vehicles. Some works attempt to address fairness issues but often sacrifice transport performance. Furthermore, existing methods overlook the challenge of sample efficiency, especially when dealing with diversity-limited traffic data. Therefore, we propose a F airness-aware and sample- E fficient traffic signal control method called FELight. Specifically, we first design a novel fairness metric and integrate it into decision process to penalize cases with high latency by setting a threshold for activating the fairness mechanism. Theoretical comparison with other fairness works proves why and when our fairness could bring advantages. Moreover, counterfactual data augmentation is employed to enrich interaction data, enhancing the sample efficiency of FELight. Self-supervised state representation is introduced to extract informative features from raw states, further improving sample efficiency. Experiments on real traffic datasets demonstrate that FELight provides relatively fairer traffic signal control without compromising performance compared to state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Xinqi Du and Ziyue Li and Cheng Long and Yongheng Xing and Philip S. Yu and Hechang Chen},
  doi          = {10.1109/TKDE.2024.3376745},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4678-4692},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {FELight: Fairness-aware traffic signal control via sample-efficient reinforcement learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature space recovery for efficient incomplete multi-view
clustering. <em>TKDE</em>, <em>36</em>(9), 4664–4677. (<a
href="https://doi.org/10.1109/TKDE.2023.3333522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tensor singular value decomposition (t-SVD) based incomplete multi-view clustering (IMVC) has received wide attention due to its ability to capture high-order correlations. However, t-SVD suffers from rotation sensitivity, failing to fully explore both inter- and intra-view consistencies. Besides, current methods mainly consider inter- or intra-view correlations, ignoring the low-rank information of sample features within views. To address these weaknesses, we first propose a feature space recovery based IMVC (FSR-IMVC) method, where low-rank feature space recovery and low-rank tensor ring based consistency learning are considered into a unified framework. Furthermore, we extend FSR-IMVC by incorporating anchor learning on the latent feature space, resulting in a scalable FSR-IMVC (sFSR-IMVC) approach that is well-suited to large-scale data. In an iterative way, the learned inter- and intra-view correlations will guide the recovery of missing features, while the explored low-rank information from feature spaces will in turn facilitate consistency exploration, eventually achieving outstanding clustering performance. Experimental results show that FSR-IMVC provides a significant improvement over known state-of-the-art algorithms in terms of ACC, NMI and Purity. Compared with FSR-IMVC, sFSR-IMVC performs slightly worse in clustering accuracy, but offers a notable advantage in computational efficiency, particularly for large-scale datasets.},
  archive      = {J_TKDE},
  author       = {Zhen Long and Ce Zhu and Pierre Comon and Yazhou Ren and Yipeng Liu},
  doi          = {10.1109/TKDE.2023.3333522},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4664-4677},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Feature space recovery for efficient incomplete multi-view clustering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ego-network segmentation via (weighted) jaccard median.
<em>TKDE</em>, <em>36</em>(9), 4646–4663. (<a
href="https://doi.org/10.1109/TKDE.2024.3373712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An ego-network is a graph representing the interactions of a node ( ego ) with its neighbors and the interactions among those neighbors. A sequence of ego-networks having the same ego can thus model the evolution of these interactions over time. We introduce the problem of segmenting a sequence of ego-networks into $k$ segments. Each segment is represented by a summary network, and the goal is to minimize the total loss of representing $k$ segments by $k$ summaries. The main challenge is to construct a summary with minimum loss. To address it, we employ the Jaccard Median (JM) problem, for which, however, no effective and efficient algorithms are known. We develop several algorithms for JM: (I) an exact algorithm, based on Mixed Integer Linear Programming; (II) exact and approximation polynomial-time algorithms for minimizing an upper bound of the objective function of JM; and (III) efficient heuristics. We also study a generalization of the segmentation problem, in which there may be multiple edges between a pair of nodes in an ego-network, and develop a series of algorithms (exact algorithms and heuristics) for it, based on a more general problem than JM, called Weighted Jaccard Median ( WJM ). By building upon the above results, we design algorithms for segmenting a sequence of ego-networks. Extensive experiments show that our algorithms produce (near)-optimal solutions to JM or to WJM and that they substantially outperform state-of-the-art methods which can be employed for ego-network segmentation.},
  archive      = {J_TKDE},
  author       = {Haodi Zhong and Grigorios Loukides and Alessio Conte and Solon P. Pissis},
  doi          = {10.1109/TKDE.2024.3373712},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4646-4663},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Ego-network segmentation via (Weighted) jaccard median},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient multiview representation learning with correntropy
and anchor graph. <em>TKDE</em>, <em>36</em>(9), 4632–4645. (<a
href="https://doi.org/10.1109/TKDE.2023.3332682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based multiview clustering methods have attracted much attention because of their ability to mine nonlinear structural information among instances. Although they perform well in many scenarios, they consume a lot of computational resources when dealing with large-scale multiview scenarios. To address this issue, we present a new insight into the anchor graph mechanism and propose a novel Nonnegative Anchor Graph Reconstruction (NAGR) model. NAGR introduces the sparse similarity graph into the symmetric matrix factorization and gets the nonnegative representation that retains the graph structural information. Thereafter, we develop a novel Efficient Multiview nonnegative Representation learning framework with Correntropy and Anchor graph (EMR-CA), which integrates multiview anchor graph reconstruction and consensus nonnegative representation learning into a unified framework. EMR-CA uses multiview anchor graph reconstruction to learn consensus nonnegative representation, where correntropy rather than F-norm is used as the approximation measurement criterion. Specifically, normalized anchor graphs of different views are decomposed into a consensus nonnegative representation and multiple view-specific representations, where the consensus representation retains the neighbor graph information between multiview instances and representative anchors on different views. Finally, the effectiveness of the proposed EMR-CA framework is verified by theoretical analysis and experimental results on large-scale realistic multiview scenarios.},
  archive      = {J_TKDE},
  author       = {Nan Zhang and Xiaoqin Zhang and Shiliang Sun},
  doi          = {10.1109/TKDE.2023.3332682},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4632-4645},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient multiview representation learning with correntropy and anchor graph},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Efficient maximal biclique enumeration on large signed
bipartite graphs. <em>TKDE</em>, <em>36</em>(9), 4618–4631. (<a
href="https://doi.org/10.1109/TKDE.2024.3373654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the problem of maximal biclique enumeration on large signed bipartite graphs. Given a signed bipartite graph $G=(U,V,E,s)$ , a parameter $\theta \in [0.5, 1.0]$ , our goal is to efficiently enumerate all maximal $\theta$ -bicliques in $G$ , where a maximal $\theta$ -biclique $B(L,R)$ is a complete subgraph of $G$ with (1) the proportion of positive neighbors for each vertex in $B$ is at least $\theta$ , and (2) $B$ is not contained in another biclique $B^{\prime }$ , while $B^{\prime }$ also satisfies (1). This problem has many applications, such as biclustering for genes, recommendation of similar groups, collaboration in communities, etc. However, it is computationally challenging due to its #P-completeness. Besides, we prove that even determining the maximality of a $\theta$ -biclique is NP-hard. To the best of our knowledge, there is no efficient and scalable solution to this problem in the literature. In this paper, we first propose a branch-and-bound framework, namely ${\sf MSiBE}$ , which enumerates all maximal $\theta$ -bicliques in a depth-first manner. Then, we develop three effective optimizations to improve the performance of ${\sf MSiBE}$ . (1) The local information of each search space is utilized to enhance the pruning capacity. (2) When expanding the partial biclique, we always focus on the side with fewer candidates first, by which fruitless search branches can be skipped early. (3) We implement ${\sf MSiBE}$ with efficient array reordering techniques and set intersection strategy. To further accelerate the computation, we introduce useful graph reduction techniques. Comprehensive performance studies on 10 real datasets demonstrate that our proposals can significantly outperform the baseline methods by up to 3 orders of magnitude.},
  archive      = {J_TKDE},
  author       = {Jianhua Wang and Jianye Yang and Zhaoquan Gu and Dian Ouyang and Zhihong Tian and Xuemin Lin},
  doi          = {10.1109/TKDE.2024.3373654},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4618-4631},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient maximal biclique enumeration on large signed bipartite graphs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient and privacy-preserving skyline queries over
encrypted data under a blockchain-based audit architecture.
<em>TKDE</em>, <em>36</em>(9), 4603–4617. (<a
href="https://doi.org/10.1109/TKDE.2024.3373602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skyline queries is an advanced data mining algorithm suitable for multi-criteria decision-making scenarios (i.e., medical pre-diagnosis). Privacy-preserving skyline queries schemes are usually constructed by certain methods of cryptography such as additive homomorphic cryptosystem, secret sharing technology, etc. Interestingly, these secure skyline queries schemes require that skyline computations do not reveal any message details, including encrypted inter-tuple domination relations, among which privacy schemes based on homomorphic cryptosystems are the most popular due to their strong security. However, existing secure skyline queries schemes not only suffer from low computational efficiency, but also do not have sufficient security for privacy-key management in the system. To address the above issues, this paper designs an efficient and privacy-preserving skyline queries over encrypted data under a blockchain-based audit architecture. Firstly, we propose a blockchain-based audit architecture that not only provides error auditing functionality but also makes our scheme suitable for (distributed) multi-user scenarios while providing secure key management in the system. Secondly, we implement a series of secure sub-protocols using the CRT-Based Paillier encryption algorithm and construct a privacy sparse matrix elimination protocol to reduce the size of the dataset, leading to a significant reduction in computational cost without compromising privacy. Finally, we put forward our secure skyline queries protocol and prove its security. The performance evaluation shows that our proposed method our proposed method is significantly more efficient (at least 7.4 times faster) compared to current methods.},
  archive      = {J_TKDE},
  author       = {Shuchang Zeng and Chingfang Hsu and Lein Harn and Yining Liu and Yang Liu},
  doi          = {10.1109/TKDE.2024.3373602},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4603-4617},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient and privacy-preserving skyline queries over encrypted data under a blockchain-based audit architecture},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient algorithms for personalized PageRank computation:
A survey. <em>TKDE</em>, <em>36</em>(9), 4582–4602. (<a
href="https://doi.org/10.1109/TKDE.2024.3376000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized PageRank (PPR) is a traditional measure for node proximity on large graphs. For a pair of nodes $\boldsymbol{s}$ and $\boldsymbol{t}$ , the PPR value ${\boldsymbol{\pi }_{s}(t)}$ equals the probability that an $\boldsymbol{\alpha }$ -discounted random walk from $\boldsymbol{s}$ terminates at $\boldsymbol{t}$ and reflects the importance between $\boldsymbol{s}$ and $\boldsymbol{t}$ in a bidirectional way. As a generalization of Google&#39;s celebrated PageRank centrality, PPR has been extensively studied and has found multifaceted applications in many fields, such as network analysis, graph mining, and graph machine learning. Despite numerous studies devoted to PPR over the decades, efficient computation of PPR remains a challenging problem, and there is a dearth of systematic summaries and comparisons of existing algorithms. In this paper, we recap several frequently used techniques for PPR computation and conduct a comprehensive survey of various recent PPR algorithms from an algorithmic perspective. We classify these approaches based on the types of queries they address and review their methodologies and contributions. We also discuss some representative algorithms for computing PPR on dynamic graphs and in parallel or distributed environments.},
  archive      = {J_TKDE},
  author       = {Mingji Yang and Hanzhi Wang and Zhewei Wei and Sibo Wang and Ji-Rong Wen},
  doi          = {10.1109/TKDE.2024.3376000},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4582-4602},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient algorithms for personalized PageRank computation: A survey},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differentially private federated learning on non-iid data:
Convergence analysis and adaptive optimization. <em>TKDE</em>,
<em>36</em>(9), 4567–4581. (<a
href="https://doi.org/10.1109/TKDE.2024.3379001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has attracted increasing attention in recent years due to its data privacy preservation and great applicability to large-scale user scenarios. However, when FL faces numerous clients, it is inevitable to emerge the non-independent and identically distributed (non-iid) data between clients, which brings an enormous challenge for model training and performance analysis like convergence. Besides, due to the non-iid data, the participating clients of FL tend to be extremely heterogeneous so the number of samplings among clients causes a sampling variance problem, which induces a huge variation in convergence. More importantly, although FL can foster privacy security via locally retaining the training data, if local data is secret and sensitive, FL should have more powerful privacy protection to resist the cloud server or third party to infer private information from shared models or intermediate gradients. Facing the non-iid and privacy challenges, we propose a differential privacy (DP) based non-iid FL algorithm called DPNFL to jointly tackle these two issues. Specifically, motivated by the DP and its variants, we are the first to adopt the truncated concentrated differential privacy technique under the FL scenario to more tightly track end-to-end privacy loss, while requiring less noise injection for the same level of DP. To avoid the sampling variance problem, we enable the server to sample the partial clients uniformly without replacement, which also guarantees unbiased sampling. To further improve the algorithm performance, we also propose an adaptive version of DPNFL named AdDPNFL, which adopts the adaptive optimization on the server-side to simultaneously alleviate the impact of non-iid data and DP noise on model utility. Finally, we perform extensive experiments to validate the effectiveness and superiority of our algorithms.},
  archive      = {J_TKDE},
  author       = {Lin Chen and Xiaofeng Ding and Zhifeng Bao and Pan Zhou and Hai Jin},
  doi          = {10.1109/TKDE.2024.3379001},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4567-4581},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Differentially private federated learning on non-iid data: Convergence analysis and adaptive optimization},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DIBA: A re-configurable stream processor. <em>TKDE</em>,
<em>36</em>(9), 4550–4566. (<a
href="https://doi.org/10.1109/TKDE.2024.3381192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stream processing acceleration is driven by the continuously increasing volume and velocity of data generated on the Web and the limitations of storage, computation, and power consumption. Hardware solutions provide better performance and power consumption, but they are hindered by the high research and development costs and the long time to market. In this work, we propose our re-configurable stream processor (Diba), a complete rethinking of a previously proposed customized and flexible query processor that targets real-time stream processing. Diba uses a unidirectional dataflow not dedicated to any specific type of query (operator) on streams, allowing a straightforward placement of processing components on a general data path that facilitates query mapping. In Diba, the concepts of the distribution network and processing components are implemented as two separate entities connected using generic interfaces. This approach allows the adoption of a versatile architecture for a family of queries rather than forcing a rigid chain of processing components to implement such queries. Our experimental evaluations of representative queries from TPC-H yielded processing times of 300, 1220, and 3520 milliseconds for data streams with scale factor sizes of one, four, and ten gigabytes, respectively.},
  archive      = {J_TKDE},
  author       = {Mohammadreza Najafi and Thamir M. Qadah and Mohammad Sadoghi and Hans-Arno Jacobsen},
  doi          = {10.1109/TKDE.2024.3381192},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4550-4566},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DIBA: A re-configurable stream processor},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-aware adaptive compression for stream processing.
<em>TKDE</em>, <em>36</em>(9), 4531–4549. (<a
href="https://doi.org/10.1109/TKDE.2024.3377710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stream processing has been in widespread use, and one of the most common application scenarios is SQL query on streams. By 2021, the global deployment of IoT endpoints reached 12.3 billion, indicating a surge in data generation. However, the escalating demands for high throughput and low latency in stream processing systems have posed significant challenges due to the increasing data volume and evolving user requirements. We present a compression-based stream processing engine, called CompressStreamDB, which enables adaptive fine-grained stream processing directly on compressed streams, to significantly enhance the performance of existing stream processing solutions. CompressStreamDB utilizes nine diverse compression methods tailored for different stream data types and integrates a cost model to automatically select the most efficient compression schemes. CompressStreamDB provides high throughput with low latency in stream SQL processing by identifying and eliminating redundant data among streams. Our evaluation demonstrates that CompressStreamDB improves average performance by 3.84× and reduces average delay by 68.0% compared to the state-of-the-art stream processing solution for uncompressed streams, along with 68.7% space savings. Besides, our edge trials show an average throughput/price ratio of 9.95× and a throughput/power ratio of 7.32× compared to the cloud design.},
  archive      = {J_TKDE},
  author       = {Yu Zhang and Feng Zhang and Hourun Li and Shuhao Zhang and Xiaoguang Guo and Yuxing Chen and Anqun Pan and Xiaoyong Du},
  doi          = {10.1109/TKDE.2024.3377710},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4531-4549},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Data-aware adaptive compression for stream processing},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CHGNN: A semi-supervised contrastive hypergraph learning
network. <em>TKDE</em>, <em>36</em>(9), 4515–4530. (<a
href="https://doi.org/10.1109/TKDE.2024.3380643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypergraphs can model higher-order relationships among data objects that are found in applications such as social networks and bioinformatics. However, recent studies on hypergraph learning that extend graph convolutional networks to hypergraphs cannot learn effectively from features of unlabeled data. To such learning, we propose a contrastive hypergraph neural network, CHGNN, that exploits self-supervised contrastive learning techniques to learn from labeled and unlabeled data. First, CHGNN includes an adaptive hypergraph view generator that adopts an auto-augmentation strategy and learns a perturbed probability distribution of minimal sufficient views. Second, CHGNN encompasses an improved hypergraph encoder that considers hyperedge homogeneity to fuse information effectively. Third, CHGNN is equipped with a joint loss function that combines a similarity loss for the view generator, a node classification loss, and a hyperedge homogeneity loss to inject supervision signals. It also includes basic and cross-validation contrastive losses, associated with an enhanced contrastive loss training process. Experimental results on nine real datasets offer insight into the effectiveness of CHGNN, showing that it outperforms 19 competitors in terms of classification accuracy consistently.},
  archive      = {J_TKDE},
  author       = {Yumeng Song and Yu Gu and Tianyi Li and Jianzhong Qi and Zhenghao Liu and Christian S. Jensen and Ge Yu},
  doi          = {10.1109/TKDE.2024.3380643},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4515-4530},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CHGNN: A semi-supervised contrastive hypergraph learning network},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CCML: Curriculum and contrastive learning enhanced
meta-learner for personalized spatial trajectory prediction.
<em>TKDE</em>, <em>36</em>(9), 4499–4514. (<a
href="https://doi.org/10.1109/TKDE.2024.3376539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial trajectory prediction is a fundamental problem for diverse location-based applications. However, existing methods fall short in learning and generalization, and cannot sufficiently capture users’ spatiotemporal preferences, especially for cold-start users. Moreover, these methods do not explicitly consider the diversity of moving patterns among users and trajectories, i.e., the learning difficulty of different user and trajectory samples, thus hindering the improvement of prediction accuracy. To solve these problems, we propose a novel Curriculum and Contrastive Learning Enhanced Meta-Learner (CCML) that transfers knowledge from users with rich data to cold-start users. Specifically, a Contrastive-based Trajectory Predictor (CTP) is designed as the base model, which utilizes contrastive learning technique on both user-level and trajectory-level, aiming to facilitate a more profound understanding and differentiation of the varied travel behaviors and preferences exhibited by individuals. Meanwhile, CCML also incorporates the curriculum learning and the hard sample mining strategies. It simultaneously considers the learning difficulty of both user and trajectory samples, and presents the learning tasks by an easy-to-hard curriculum. By learning more challenging combinations of user and trajectory samples in each meta-learning iteration, the meta-learner can converge to a better status. Extensive experiments on two real-world datasets demonstrate the superiority of our models.},
  archive      = {J_TKDE},
  author       = {Jing Zhao and Jiajie Xu and Yuan Xu and Junhua Fang and Pingfu Chao and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2024.3376539},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4499-4514},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CCML: Curriculum and contrastive learning enhanced meta-learner for personalized spatial trajectory prediction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BM-FL: A balanced weight strategy for multi-stage federated
learning against multi-client data skewing. <em>TKDE</em>,
<em>36</em>(9), 4486–4498. (<a
href="https://doi.org/10.1109/TKDE.2024.3372708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) combined with Differential Privacy (DP) is widespread in healthcare, finance, and IoT due to its advantages in multi-client data distribution. However, existing FL approaches overlook the differential impact levels among clients and data redundancy issues, resulting in high computational overhead and limited real-time applicability. Additionally, non-independent identical distribution (Non-IID) and imbalanced datasets in multi-clients pose challenges in privacy preservation and model overfitting. Therefore, we propose a balanced weight strategy for multi-stage federated learning against multi-client data skewing, called BM-FL, which involves clients, intermediate trust servers (ITSs), and the central server (CS). First, to protect data privacy, an improved Laplace $\epsilon$ -differential privacy method is employed. Second, a novel generative adversarial network (GAN) called BC-GAN is introduced. It is used to generate realistic fake samples and maintain a balanced proportion of samples across different categories. Then, to make full use of each client&#39;s valuable data, we designe a balanced weight strategy. Moreover, extensive experimental results clearly demonstrate the effectiveness of BM-FL in efficiently handling classification tasks involving Non-IID and imbalanced datasets while maintaining privacy and security. Furthermore, our method attains superior classification accuracy with fewer training epochs compared to relevant classical algorithms.},
  archive      = {J_TKDE},
  author       = {Lixiang Yuan and Mingxing Duan and Guoqing Xiao and Zhuo Tang and Kenli Li},
  doi          = {10.1109/TKDE.2024.3372708},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4486-4498},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {BM-FL: A balanced weight strategy for multi-stage federated learning against multi-client data skewing},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BiTDB: Constructing a built-in TEE secure database for
embedded systems. <em>TKDE</em>, <em>36</em>(9), 4472–4485. (<a
href="https://doi.org/10.1109/TKDE.2024.3380367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose BiTDB, a built-in Trusted Execution Environment (TEE) database for embedded systems, to realize higher system availability while ensuring data confidentiality. With BiTDB, dilemmas that the state-of-the-art research work on secure embedded databases has to face can be significantly reduced and eliminated, including (i) complicated research and realization on searchable encryption algorithms (SEA), (ii) limited support to all database operations, and (iii) almost none of specific design and optimizations toward build-in TEE embedded databases. Through BiTDB, all database operations can process plaintext in TEE instead of retrieving ciphertext by developing complicated SEAs. To enable BiTDB to handle database files in Rich Execution Environment (REE) as local ones, we extend the TEE OS with generic file I/O libraries. Then, we contribute three critical optimizations to significantly reduce redundant memory and file operations between TEE and REE, and BiTDB achieve better system performance and availability in embedded systems. Finally, we have implemented the prototype system based on OP-TEE and SQLite for several typical platforms, including virtualization and hardware environments. The TPC-H test shows BiTDB can achieve 85% (on average) of the original database performance while guaranteeing data confidentiality and integrity.},
  archive      = {J_TKDE},
  author       = {Chengyan Ma and Di Lu and Chaoyue Lv and Ning Xi and Xiaohong Jiang and Yulong Shen and Jianfeng Ma},
  doi          = {10.1109/TKDE.2024.3380367},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4472-4485},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {BiTDB: Constructing a built-in TEE secure database for embedded systems},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attention is not the only choice: Counterfactual reasoning
for path-based explainable recommendation. <em>TKDE</em>,
<em>36</em>(9), 4458–4471. (<a
href="https://doi.org/10.1109/TKDE.2024.3373608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with only pursuing recommendation accuracy, the explainability of a recommendation model has drawn more attention in recent years. Many graph-based recommendations resort to informative paths with the attention mechanism for the explanation. Unfortunately, these attention weights are intentionally designed for model accuracy but not explainability. Recently, some researchers have started to question attention-based explainability because the attention weights are unstable for different reproductions, and they may not always align with human intuition. Inspired by the counterfactual reasoning from causality learning theory, we propose a novel explainable framework targeting path-based recommendations, wherein the explainable weights of paths are learned to replace attention weights. Specifically, we design two counterfactual reasoning algorithms from both path representation and path topological structure perspectives. Moreover, unlike traditional case studies, we also propose a package of explainability evaluation solutions with both qualitative and quantitative methods. We conduct extensive experiments on four real-world datasets, the results of which further demonstrate the effectiveness and reliability of our method.},
  archive      = {J_TKDE},
  author       = {Yicong Li and Xiangguo Sun and Hongxu Chen and Sixiao Zhang and Yu Yang and Guandong Xu},
  doi          = {10.1109/TKDE.2024.3373608},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4458-4471},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Attention is not the only choice: Counterfactual reasoning for path-based explainable recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An experimental study on federated equi-joins.
<em>TKDE</em>, <em>36</em>(9), 4443–4457. (<a
href="https://doi.org/10.1109/TKDE.2024.3375028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data federation has emerged as a novel database system enabling collaborative queries across mutually distrusted data owners. Federated equi-join, a commonly used operation in data federation, combines relations from distinct data owners while preserving their data privacy. Due to the wide applications of this query, many solutions to federated equi-joins have been proposed. However, it is still challenging for practitioners to choose the most appropriate algorithm due to various reasons, including incomplete evaluation protocols (e.g., lack of evaluating multi-way equi-joins), under-explored performance metric (main memory usage), and absence of a standardized comparison. Motivated by this reason, this paper conducts a comprehensive experimental study and builds a new benchmark, called ${\sf FEJ-Bench}$ , for federated equi-joins. The experimental study and the benchmark consist of eight state-of-the-art algorithms and five datasets. Our evaluation reveals the query efficiency ranking, its impact factors, and potential research opportunities. Finally, we open-source ${\sf FEJ-Bench}$ on GitHub, which is the first benchmark for federated equi-joins. Our findings aim to guide researchers and practitioners in deploying federated equi-joins in practice.},
  archive      = {J_TKDE},
  author       = {Shuyuan Li and Yuxiang Zeng and Yuxiang Wang and Yiman Zhong and Zimu Zhou and Yongxin Tong},
  doi          = {10.1109/TKDE.2024.3375028},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4443-4457},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {An experimental study on federated equi-joins},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient verification approach to separation of duty in
attribute-based access control. <em>TKDE</em>, <em>36</em>(9),
4428–4442. (<a href="https://doi.org/10.1109/TKDE.2024.3373562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem considered in this paper is the verification and enforcement of separation of duty (SoD) constraints in attribute based access control (ABAC) systems. We propose an efficient algorithm for checking the satisfiability of SoD constraints. It is based on the idea of partitioning all permissions of SoD constraints into two classes so as to compute the minimal number of users to accomplish each class of permissions, respectively. As a result, several SoD constraints with certain class of permissions can be verified in polynomial time. Experimental results show that our method performs well compared with existing ones. When SoD violations occur, a 0–1 integer programming (IP) based enforcement solution is presented such that SoD violations can be solved once for all and it is provably shown that such solution does not result in the violation of other SoD constraints.},
  archive      = {J_TKDE},
  author       = {Benyuan Yang and Hesuan Hu},
  doi          = {10.1109/TKDE.2024.3373562},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4428-4442},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {An efficient verification approach to separation of duty in attribute-based access control},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AiFed: An adaptive and integrated mechanism for asynchronous
federated data mining. <em>TKDE</em>, <em>36</em>(9), 4411–4427. (<a
href="https://doi.org/10.1109/TKDE.2023.3332770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing concerns on datasecurity and user privacy, a decentralized mechanism is implemented for federated data mining (FDM), which can bridge data silos and collaborate diverse devices in ubiquitous IoT (Internet of Things) systems and services to extract global and shareable knowledge, i.e., encoded in deep neural networks (DDNs). Moreover, compared with FDM in synchronous mode, asynchronous FDM (AFDM) is more suitable to accommodate devices with diversified computing resources and distinguishable working statuses. However, as AFDM is still in its infancy, how to harness heterogeneous resources and biased knowledge of learning participants within the asynchronous context remains to be addressed. Such that, this paper proposes an adaptive and integrated mechanism, named AiFed, in which, a layer-wise optimization of AFDM is implemented based on the integration of two dedicated strategies, i.e., an adaptive local model uploading strategy (ALMU), and an adaptive global model aggregation strategy (AGMA). As shown by the evaluation results, AiFed can outperform five state-of-the-art methods to reduce communication costs by about 61.76% and 56.88%, improve learning accuracy by about 1.66% and 3.05%, and accelerate learning speed by about 22.16% and 37.81% under IID (independent and identically distributed) and Non-IID settings of four standard datasets, respectively.},
  archive      = {J_TKDE},
  author       = {Linlin You and Sheng Liu and Tao Wang and Bingran Zuo and Yi Chang and Chau Yuen},
  doi          = {10.1109/TKDE.2023.3332770},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4411-4427},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {AiFed: An adaptive and integrated mechanism for asynchronous federated data mining},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Agree to disagree: Personalized temporal embedding and
routing for stock forecast. <em>TKDE</em>, <em>36</em>(9), 4398–4410.
(<a href="https://doi.org/10.1109/TKDE.2024.3374373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock forecast is a crucial yet challenging task in modern quantitative trading. Given theoretical and investment merits, recently a variety of deep learning methods have been proposed for automatically simulating stock movements from historical time series. However, these methods typically follow the i.i.d. assumption that actually contradicts the complex trading environment. In reality, individual stocks often exhibit diverse volatility patterns, while macro market scenarios may also change over time, jointly resulting in distribution shifts and weak generalization. To combat these bottlenecks, in this paper we propose a new learning architecture called Personalized Temporal Embedding and Routing ( PTER ) to improve stock forecast by forming a relaxed weight-sharing paradigm. The key of PTER is introducing hypernetworks to guide tailoring target network parameters, such that stock time series are embedded adapting to multi-object multi-scenario data disparities. Specifically, in the encoding stage, PTER first captures hyper-knowledge characterizing the similarity and peculiarity of different stocks and market scenarios. The knowledge space is then projected onto the temporal parameter space, enabling the customization of protruded features from chaotic observation signals. In the inference stage, each sample is dispatched to orthogonal predictor heads to dynamically output expected returns based on market conditions. Through experiments on benchmark datasets spanning over five years on four of the world&#39;s largest exchange markets, we show that PTER improves the cumulative and risk-adjusted revenue performance by a significant margin.},
  archive      = {J_TKDE},
  author       = {Heyuan Wang and Tengjiao Wang and Shun Li and Jiayi Zheng and Weijun Chen and Wei Chen},
  doi          = {10.1109/TKDE.2024.3374373},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4398-4410},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Agree to disagree: Personalized temporal embedding and routing for stock forecast},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified meta-learning framework for fair ranking with
curriculum learning. <em>TKDE</em>, <em>36</em>(9), 4386–4397. (<a
href="https://doi.org/10.1109/TKDE.2024.3377644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent information retrieval systems, it is observed that the datasets used to train machine learning models can be biased, leading to systematic discrimination against certain demographic groups, which means the ranking utility of specific groups is often lower than others in a biased dataset. Training models on these datasets will further decrease the exposure of the minority groups. To address this problem, we propose a Meta Curriculum-based Fair Ranking framework (MCFR) which could alleviate the data bias issue through the weighted loss using gradient-based learning to learn. Specifically, we optimize a meta learner from a sampled dataset ( meta-dataset ), and meanwhile train a ranking model on the whole ( biased ) dataset. The meta-dataset is sampled with a curriculum learning scheduler to guide the meta learner&#39;s training to gradually mitigate the skewness towards biased attributes. The meta learner serves as a weighting function to make the ranking loss focus more on the minority group. We formulate the proposed MCFR as a bilevel optimization problem and solve it using gradients through gradients. Extensive experiments on real-world datasets demonstrate that our approach can be used as a generic framework to work with various ranking losses and fairness metrics.},
  archive      = {J_TKDE},
  author       = {Yuan Wang and Zhiqiang Tao and Yi Fang},
  doi          = {10.1109/TKDE.2024.3377644},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4386-4397},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A unified meta-learning framework for fair ranking with curriculum learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A teacher-free graph knowledge distillation framework with
dual self-distillation. <em>TKDE</em>, <em>36</em>(9), 4375–4385. (<a
href="https://doi.org/10.1109/TKDE.2024.3374773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed great success in handling graph-related tasks with Graph Neural Networks (GNNs). Despite their great academic success, Multi-Layer Perceptrons (MLPs) remain the primary workhorse for practical industrial applications. One reason for such an academic-industry gap is the neighborhood-fetching latency incurred by data dependency in GNNs. To reduce their gaps, Graph Knowledge Distillation (GKD) is proposed, usually based on a standard teacher-student architecture, to distill knowledge from a large teacher GNN into a lightweight student GNN or MLP. However, we found in this paper that neither teachers nor GNNs are necessary for graph knowledge distillation. We propose a Teacher-Free Graph Self-Distillation (TGS) framework that does not require any teacher model or GNNs during both training and inference. More importantly, the proposed TGS framework is purely based on MLPs, where structural information is only implicitly used to guide dual knowledge self-distillation between the target node and its neighborhood. As a result, TGS enjoys the benefits of graph topology awareness in training but is free from data dependency in inference. Extensive experiments have shown that the performance of vanilla MLPs can be greatly improved with dual self-distillation, e.g., TGS improves over vanilla MLPs by 15.54% on average and outperforms state-of-the-art GKD algorithms on six real-world datasets. In terms of inference speed, TGS infers 75×-89× faster than existing GNNs and 16×-25× faster than classical inference acceleration methods.},
  archive      = {J_TKDE},
  author       = {Lirong Wu and Haitao Lin and Zhangyang Gao and Guojiang Zhao and Stan Z. Li},
  doi          = {10.1109/TKDE.2024.3374773},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4375-4385},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A teacher-free graph knowledge distillation framework with dual self-distillation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A novel algorithm for efficiently mining spatial
multi-level co-location patterns. <em>TKDE</em>, <em>36</em>(9),
4361–4374. (<a href="https://doi.org/10.1109/TKDE.2024.3381178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spatial co-location pattern is a collection of spatial features in which instances of features prevalently appear in neighboring spatial regions. Due to the heterogeneity of spatial data distribution, the instances of some patterns appear prevalently in the global region (i.e., Global Prevalent Co-location Patterns, GPCPs), while some patterns are not prevalent globally, and their instances are clustered only in some local regions (i.e., Local Prevalent Co-location Patterns, LPCPs). Multi-level co-location pattern mining aims to mine these two types of patterns simultaneously, but existing methods cannot accurately judge the spatial distribution of patterns in a certain region, leading to unsuitable judgment of both GPCPs and LPCPs. To overcome this problem, this paper firstly proposes the relative distribution coefficient to identify the spatial distribution form of patterns, and provides a more refined way for discovering both GPCPs and LPCPs. Secondly, a novel multi-level co-location pattern mining algorithm is proposed by using the relative distribution coefficient as the interest metrics, and some pruning strategies are suggested to improve the mining efficiency. Finally, extensive experiments are conducted on both real and synthetic datasets to verify the effectiveness and efficiency of the proposed method.},
  archive      = {J_TKDE},
  author       = {Junyi Li and Lizhen Wang and Peizhong Yang and Lihua Zhou},
  doi          = {10.1109/TKDE.2024.3381178},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4361-4374},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A novel algorithm for efficiently mining spatial multi-level co-location patterns},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A light-weight and robust tensor convolutional autoencoder
for anomaly detection. <em>TKDE</em>, <em>36</em>(9), 4346–4360. (<a
href="https://doi.org/10.1109/TKDE.2023.3332784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust PCA is a popular anomaly detection technique and has been widely used in many applications. Although Robust PCA is promising, it is usually designed in a two-order matrix form, which is inferior to the tensor that can capture multilinearity features of data. Moreover, the detection accuracy under Robust PCA further suffers due to its sensitivity to the rank parameter which is hard to set in practice and the limitation of PCA method in capturing the non-linear feature in the data. To address the issues, we propose a Robust Tensor Convolutional Autoencoder (RTCAE) where the autoencoder instead of SVD is exploited to recover the normal data from the corrupted measurement tensor data. However, directly exploiting deep autoencoder may suffer from the problem of high memory consumption and computation overhead due to the large number of parameters used in autoencoder. To make our anomaly detection lightweight, we further design a Light Convolutional Autoencoder (LightCAE) which contains a compressed autoencoder by exploiting tensor factorization to largely compress the parameters while significantly reducing the computation complexity. We conduct extensive experiments on three real data traces to compare the performance of our proposed schemes (RTCAE and lightCAE) with that of seven baseline algorithms. The experiment results demonstrate that our proposed RTCAE achieves the highest anomaly detection accuracy. Moreover, our LightCAE requires over 60 times smaller memory storage than that required in RTCAE while achieving the similar anomaly detection accuracy.},
  archive      = {J_TKDE},
  author       = {Xiaocan Li and Kun Xie and Xin Wang and Gaogang Xie and Kenli Li and Jiannong Cao and Dafang Zhang and Jigang Wen},
  doi          = {10.1109/TKDE.2023.3332784},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {9},
  number       = {9},
  pages        = {4346-4360},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A light-weight and robust tensor convolutional autoencoder for anomaly detection},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Z-laplacian matrix factorization: Network embedding with
interpretable graph signals. <em>TKDE</em>, <em>36</em>(8), 4331–4345.
(<a href="https://doi.org/10.1109/TKDE.2023.3331027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embedding aims to represent nodes with low dimensional vectors while preserving structural information. It has been recently shown that many popular network embedding methods can be transformed into matrix factorization problems. In this paper, we propose the unifying framework “Z-NetMF,” which generalizes random walk samplers to Z-Laplacian graph filters, leading to embedding algorithms with interpretable parameters. In particular, by controlling biases in the time domain, we propose the Z-NetMF-t algorithm, making it possible to scale contributions of random walks of different length. Inspired by node2vec, we design the Z-NetMF-g algorithm, capturing the random walk biases in the graph domain. Moreover, we evaluate the effect of the bias parameters based on node classification and link prediction tasks. The results show that our algorithms, especially the combined model Z-NetMF-gt with biases in both domains, outperform the state-of-art methods while providing interpretable insights at the same time. Finally, we discuss future directions of the Z-NetMF framework.},
  archive      = {J_TKDE},
  author       = {Liangtian Wan and Zhengqiang Fu and Yi Ling and Yuchen Sun and Xiaona Li and Lu Sun and Feng Xia and Xiaoran Yan and Charu C. Aggarwal},
  doi          = {10.1109/TKDE.2023.3331027},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {4331-4345},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Z-laplacian matrix factorization: Network embedding with interpretable graph signals},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using methods from dimensionality reduction for active
learning with low query budget. <em>TKDE</em>, <em>36</em>(8),
4317–4330. (<a href="https://doi.org/10.1109/TKDE.2024.3365189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, it has been challenging to generate enough labeled data for supervised learning models from a large amount of free unlabeled data due to the high cost of the labeling process. Here, the active learning technique provides a solution by annotating a small but highly informative set of unlabeled data. This ensures high generalizability in space and improves classification performance with test data. The task is more challenging when the query budget is small, the data is imbalanced, multiple classes are present, and no predefined knowledge is available. To address these challenges, we present a novel active learner geometrically based on principal component analysis (PCA) and linear discriminant analysis (LDA). The proposed active learner consists of two phases: The PCA-inspired exploration phase, in which regions with high variances are explored, and the LDA-inspired exploitation phase, in which boundary points between classes are selected. The proposed geometric strategy improves the search capabilities of the active learner, allowing it to explore the space of minority classes even with multiple minority classes and a small query budget. Experiments on synthetic and real binary and multi-class imbalanced data show that the proposed algorithm has significant advantages over multiple known active learners.},
  archive      = {J_TKDE},
  author       = {Alaa Tharwat and Wolfram Schenck},
  doi          = {10.1109/TKDE.2024.3365189},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {4317-4330},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Using methods from dimensionality reduction for active learning with low query budget},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Trajectory distribution aware graph convolutional network
for trajectory prediction considering spatio-temporal interactions and
scene information. <em>TKDE</em>, <em>36</em>(8), 4304–4316. (<a
href="https://doi.org/10.1109/TKDE.2023.3329676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian trajectory prediction has been broadly applied in video surveillance and autonomous driving. Most of the current trajectory prediction approaches are committed to improving the prediction accuracy. However, these works remain drawbacks in several aspects, complex interaction modeling among pedestrians, the interactions between pedestrians and environment and the multimodality of pedestrian trajectories. To address the above issues, we propose one new trajectory distribution aware graph convolutional network to improve trajectory prediction performance. First, we propose a novel directed graph and combine multi-head self-attention and graph convolution to capture the spatial interactions. Then, to capture the interactions between pedestrian and environment, we construct a trajectory heatmap, which can reflect the walkable area of the scene and the motion trends of the pedestrian in the scene. Besides, we devise one trajectory distribution-aware module to perceive the distribution information of pedestrian trajectory, aiming at providing rich trajectory information for multi-modal trajectory prediction. Experimental results validate the proposed model can achieve superior trajectory prediction accuracy on the ETH &amp; UCY, SSD, and NBA datasets in terms of both the final displacement error and average displacement error metrics.},
  archive      = {J_TKDE},
  author       = {Ruiping Wang and Zhijian Hu and Xiao Song and Wenxin Li},
  doi          = {10.1109/TKDE.2023.3329676},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {4304-4316},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Trajectory distribution aware graph convolutional network for trajectory prediction considering spatio-temporal interactions and scene information},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward enhanced robustness in unsupervised graph
representation learning: A graph information bottleneck perspective.
<em>TKDE</em>, <em>36</em>(8), 4290–4303. (<a
href="https://doi.org/10.1109/TKDE.2023.3330684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have revealed that GNNs are vulnerable to adversarial attacks. Most existing robust graph learning methods measure model robustness based on label information, rendering them infeasible when label information is not available. A straightforward direction is to employ the widely used Infomax technique from typical Unsupervised Graph Representation Learning (UGRL) to learn robust unsupervised representations. Nonetheless, directly transplanting the Infomax technique from typical UGRL to robust UGRL may involve a biased assumption. In light of the limitation of Infomax, we propose a novel unbiased robust UGRL method called Robust Graph Information Bottleneck (RGIB), which is grounded in the Information Bottleneck (IB) principle. Our RGIB attempts to learn robust node representations against adversarial perturbations by preserving the original information in the benign graph while eliminating the adversarial information in the adversarial graph. There are mainly two challenges to optimizing RGIB: 1) high complexity of adversarial attack to perturb node features and graph structure jointly in the training procedure; 2) mutual information estimation upon adversarially attacked graphs. To tackle these problems, we further propose an efficient adversarial training strategy with only feature perturbations and an effective mutual information estimator with the subgraph-level summary. Moreover, we theoretically establish a connection between our proposed RGIB and the robustness of downstream classifiers, revealing that RGIB can provide a lower bound on the adversarial risk of downstream classifiers. Extensive experiments over several benchmarks and downstream tasks demonstrate the effectiveness and superiority of our proposed method.},
  archive      = {J_TKDE},
  author       = {Jihong Wang and Minnan Luo and Jundong Li and Ziqi Liu and Jun Zhou and Qinghua Zheng},
  doi          = {10.1109/TKDE.2023.3330684},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {4290-4303},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Toward enhanced robustness in unsupervised graph representation learning: A graph information bottleneck perspective},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Supporting your idea reasonably: A knowledge-aware topic
reasoning strategy for citation recommendation. <em>TKDE</em>,
<em>36</em>(8), 4275–4289. (<a
href="https://doi.org/10.1109/TKDE.2024.3365508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of scholarly information, researchers spend much time and effort copiously quoting authoritative works to support their ideas or motivations. We aim to alleviate this situation by proposing a citation recommendation strategy that recalls related papers for a rough idea (a piece of text, i.e., abstract, manuscript). However, the perspective of existing citation recommendations can not be well applied to our task for two defects. First, these methods neglect the reasoning of research topics, which makes the recommendation mechanism not meticulous enough and lacks explainability. For instance, they are not able to mine the hidden citing logic for the candidate paper while recommending. We fill the research gap by constructing structural topics consisting of knowledge concepts from the textual content, where reasoning paths between topics are extracted from an external knowledge graph. Second, the citation network is viewed as a crucial structural context to enhance the recommendation performance, but the new target idea does not have links to the citation network as published papers do. To simulate the prospective topological structure, our model, meanwhile, incorporates a contrastive-learning-based alignment paradigm to encourage the consistency of content embeddings and structure-oriented embeddings. We evaluate our proposed model on three real-world datasets and demonstrate that it significantly improves recommendation accuracy while providing high-quality knowledge-aware reasoning. And an interesting visual example illustrates the reasoning process when our model actually judges samples, which supports the feasibility of our topic-view learning paradigm.},
  archive      = {J_TKDE},
  author       = {Likang Wu and Zhi Li and Hongke Zhao and Zhenya Huang and Yongqiang Han and Junji Jiang and Enhong Chen},
  doi          = {10.1109/TKDE.2024.3365508},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {4275-4289},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Supporting your idea reasonably: A knowledge-aware topic reasoning strategy for citation recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatio-temporal enhanced contrastive and contextual learning
for weather forecasting. <em>TKDE</em>, <em>36</em>(8), 4260–4274. (<a
href="https://doi.org/10.1109/TKDE.2024.3362825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weather forecasting is of great importance for human life and various real-world fields, e.g., traffic prediction, agricultural production, and tourist industry. Existing methods can be roughly divided into two categories: theory-driven (e.g., numerical weather prediction (NWP)) and data-driven methods. Theory-driven methods require a complex simulation of the physical evolution process in the atmosphere model using supercomputers, while most data-driven methods learn the underlying laws from the historical weather records via deep learning models. However, some data-driven methods simply regard all weather variables of monitoring stations as a whole and fail to more granularly exploit complex correlations across different stations, while others prefer to construct large neural networks with massive learnable parameters. To alleviate these defects, we propose a spatio-temporal contrastive self-supervision method and a generative contextual self-supervised technique to capture spatial and temporal dependencies from the station-level and variable-level, respectively. Through these well-designed self-supervised tasks, uncomplicated networks obtain strong capability to capture latent representations for weather changes with time-varying. Thereafter, an effective encoder-decoder based fine-tuning framework is proposed, consisting of three self-supervised encoders. Extensive experiments conducted on four real-world weather condition datasets demonstrate that our method outperforms the state-of-the-art models and also empirically validates the feasibility of each self-supervised task.},
  archive      = {J_TKDE},
  author       = {Yongshun Gong and Tiantian He and Meng Chen and Bin Wang and Liqiang Nie and Yilong Yin},
  doi          = {10.1109/TKDE.2024.3362825},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {4260-4274},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Spatio-temporal enhanced contrastive and contextual learning for weather forecasting},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simple and efficient partial graph adversarial attack: A new
perspective. <em>TKDE</em>, <em>36</em>(8), 4245–4259. (<a
href="https://doi.org/10.1109/TKDE.2024.3364972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the study of graph neural networks becomes more intensive and comprehensive, their robustness and security have received great research interest. The existing global attack methods treat all nodes in the graph as their attack targets. Although existing methods have achieved excellent results, there is still considerable space for improvement. The key problem is that the current approaches rigidly follow the definition of global attacks. They ignore an important issue, i.e., different nodes have different robustness and are not equally resilient to attacks. From a global attacker&#39;s view, we should arrange the attack budget wisely, rather than wasting them on highly robust nodes. To this end, we propose a totally new method named partial graph attack (PGA), which selects the vulnerable nodes as attack targets. First, to select the vulnerable items, we propose a hierarchical target selection policy, which allows attackers to only focus on easy-to-attack nodes. Then, we propose a cost-effective anchor-picking policy to pick the most promising anchors for adding or removing edges, and a more aggressive iterative greedy-based attack method to perform more efficient attacks. Extensive experimental results demonstrate that PGA can achieve significant improvements in both attack effect and attack efficiency compared to existing graph global attack methods.},
  archive      = {J_TKDE},
  author       = {Guanghui Zhu and Mengyu Chen and Chunfeng Yuan and Yihua Huang},
  doi          = {10.1109/TKDE.2024.3364972},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {4245-4259},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Simple and efficient partial graph adversarial attack: A new perspective},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised graph contrastive learning with virtual
adversarial augmentation. <em>TKDE</em>, <em>36</em>(8), 4232–4244. (<a
href="https://doi.org/10.1109/TKDE.2024.3366396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised graph learning aims to improve learning performance by leveraging unlabeled nodes. Typically, it can be approached in two different ways, including predictive representation learning (PRL) where unlabeled data provide clues on input distribution and label-dependent regularization (LDR) which smooths the output distribution with unlabeled nodes to improve generalization. However, most existing PRL approaches suffer from overfitting in an end-to-end setting or cannot encode task-specific information when used as unsupervised pre-training (i.e., two-stage learning). Meanwhile, LDR strategies often introduce redundant and invalid data perturbations that can slow down and mislead the training. To address all these issues, we propose a general framework SemiGraL for semi-supervised learning on graphs, which bridges and facilitates both PRL and LDR in a single shot. By extending a contrastive learning architecture to the semi-supervised setting, we first develop a semi-supervised contrastive representation learning process with virtual adversarial augmentation to map input nodes into a label-preserving representation space while avoiding overfitting. We then introduce a multiview consistency classification process with well-constrained perturbations to achieve adversarially robust classification. Extensive experiments on seven semi-supervised node classification benchmark datasets show that SemiGraL outperforms various baselines while enjoying strong generalization and robustness performance.},
  archive      = {J_TKDE},
  author       = {Yixiang Dong and Minnan Luo and Jundong Li and Ziqi Liu and Qinghua Zheng},
  doi          = {10.1109/TKDE.2024.3366396},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {4232-4244},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Semi-supervised graph contrastive learning with virtual adversarial augmentation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust multi-kernel nearest neighborhood for outlier
detection. <em>TKDE</em>, <em>36</em>(8), 4220–4231. (<a
href="https://doi.org/10.1109/TKDE.2024.3364179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier detection methods based on distance measure have been used in numerous applications due to their effectiveness and interpretability. However, distances among instances heavily depend on the feature space in which they reside. For an outlier, distances from it to the normal instances may be extremely close in one feature space, failing to separate them from each other, while this situation is reversed in another space. Meanwhile, the distance measure is sensitive to a few “marginal instances” (i.e., normal instances located very close to outliers in the feature space) during the estimation of whether a test instance is an outlier or not. In this article, we propose a r obust m ulti- k ernel nearest n eighborhood (RMKN) method for outlier detection. Specifically, in the training phase, we only consider normal instances and transform them into a Polynomial kernel function weighted digraph to capture their geometric relationships in the original feature space. Then, we develop an objective function based on the weighted digraph to find a latent feature space via multi-kernel learning such that distances among normal instances in this latent feature space are as close as possible while preserving their original distributions. In the detecting phase, we design an outlying score based on the two-stage multi-kernel $k$ -nearest nearest neighbors to detect outliers. Extensive experiments with ten datasets show that RMKN is effective and robust.},
  archive      = {J_TKDE},
  author       = {Xinye Wang and Lei Duan and Zhenyang Yu and Chengxin He and Zhifeng Bao},
  doi          = {10.1109/TKDE.2024.3364179},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {4220-4231},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Robust multi-kernel nearest neighborhood for outlier detection},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust and consistent anchor graph learning for multi-view
clustering. <em>TKDE</em>, <em>36</em>(8), 4207–4219. (<a
href="https://doi.org/10.1109/TKDE.2024.3364663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anchor-based multi-view graph clustering has recently gained popularity as an effective approach for clustering data with multiple views. However, existing methods have limitations in terms of handling inconsistent information and noise across views, resulting in an unreliable consensus representation. In addition, post-processing is needed to obtain final results after anchor graph construction, which negatively affects clustering performance. In this article, we propose a Robust and Consistent Anchor Graph Learning method (RCAGL) for multi-view clustering to address these challenges. RCAGL constructs a consistent anchor graph that captures inter-view commonality and filters out view-specific noise by learning a consistent part and a view-specific part simultaneously. A $k$ -connectivity constraint is imposed on the consistent anchor graph, leading to a clear graph structure and direct generation of cluster labels without additional post-processing. Experimental results on several benchmark datasets demonstrate the superiority of RCAGL in terms of clustering accuracy, scalability to large-scale data, and robustness to view-specific noise, outperforming advanced multi-view clustering methods.},
  archive      = {J_TKDE},
  author       = {Suyuan Liu and Qing Liao and Siwei Wang and Xinwang Liu and En Zhu},
  doi          = {10.1109/TKDE.2024.3364663},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {4207-4219},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Robust and consistent anchor graph learning for multi-view clustering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revisiting the effective number theory for imbalanced
learning. <em>TKDE</em>, <em>36</em>(8), 4192–4206. (<a
href="https://doi.org/10.1109/TKDE.2024.3367949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced learning is a traditional yet hot research subarea in machine learning. There are a huge number of imbalanced learning methods proposed in previous literature. This study focuses on one of the most popular imbalanced learning strategies, namely, sample reweighting. The key issue is how to calculate the weights of samples in training. While most studies have relied on intuitive theoretical or heuristic inspirations, few studies have attempted to establish a comprehensive theoretical path for weight calculation. A recent study utilizes the effective number theory for random covering to construct a theoretical weighting framework. In this study, we conduct a deep analysis to theoretically reveal the defects in the existing effective number-based weighting theory. An enhanced effective number theory is established in which data scatter and covering offset among different categories are involved. Subsequently, a new weight calculation manner is proposed based on our new theory, yielding a new loss, namely, NENum loss. In this loss, weights are sample-wise instead of category-wise used in the existing effective number-based weighting. Furthermore, another novel loss that combines weighting and logit perturbation is designed inspired the limitations of the NENum loss. Meta learning is employed to optimize the concrete calculation based on sample-wise training dynamics. We conduct extensive experiments on benchmark imbalanced and standard data corpora. Results validate the reasonableness of our enhanced theory and the effectiveness of the proposed methodology.},
  archive      = {J_TKDE},
  author       = {Ou Wu and Mengyang Li},
  doi          = {10.1109/TKDE.2024.3367949},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {4192-4206},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Revisiting the effective number theory for imbalanced learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Partial label feature selection: An adaptive approach.
<em>TKDE</em>, <em>36</em>(8), 4178–4191. (<a
href="https://doi.org/10.1109/TKDE.2024.3365691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an emerging weakly supervised learning framework, partial label learning aims to induce a multi-class classifier from ambiguous supervision information where each training example is associated with a set of candidate labels, among which only one is the true label. Traditional feature selection methods, either for single label and multiple label problems, are not applicable to partial label learning as the ambiguous information contained in the label space obfuscates the importance of features and misleads the selection process. This makes the selection of a proper feature subset from partial label examples particularly challenging, and therefore has rarely been investigated. In this paper, we propose a novel feature selection algorithm for partial label learning, named PLFS, which considers not only the relationships between features and labels, but also exploits the relationships between instances to select the most informative and important features to enhance the performance of partial label learning. PLFS constructs an adaptive weighted graph to exploit the similarity information among instances, differentiate the label space and weight the feature space, which leads to the selection of a proper feature subset. Extensive experiments over a broad range of benchmark data sets clearly validate the effectiveness of our proposed feature selection approach.},
  archive      = {J_TKDE},
  author       = {Zan Zhang and Jialu Yao and Lin Liu and Jiuyong Li and Lei Li and Xindong Wu},
  doi          = {10.1109/TKDE.2024.3365691},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {4178-4191},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Partial label feature selection: An adaptive approach},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Open-domain semi-supervised learning via glocal cluster
structure exploitation. <em>TKDE</em>, <em>36</em>(8), 4163–4177. (<a
href="https://doi.org/10.1109/TKDE.2024.3368529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning (SSL) aims to reduce the heavy reliance of current deep models on costly manual annotation by leveraging a large amount of unlabeled data in combination with a much smaller set of labeled data. However, most existing SSL methods assume that all labeled and unlabeled data are drawn from the same feature distribution, which can be impractical in real-world applications. In this study, we take the initial step to systematically investigate the open-domain semi-supervised learning setting, where a feature distribution mismatch exists between labeled and unlabeled data. In pursuit of an effective solution for open-domain SSL, we propose a novel framework called GlocalMatch , which aims to exploit both glo bal and lo cal (i.e., glocal) cluster structure of open-domain unlabeled data. The glocal cluster structure is utilized in two complementary ways. First, GlocalMatch optimizes a Glocal Cluster Compacting (GCC) objective, that encourages feature representations of the same class, whether with in the same domain or across different domains, to become closer to each other. Second, GlocalMatch incorporates a Glocal Semantic Aggregation (GSA) strategy to produce more reliable pseudo-labels by aggregating predictions from neighboring clusters. Extensive experiments demonstrate that GlocalMatch outperforms the state-of-the-art SSL methods significantly, achieving superior performance for both in-domain and out-of-domain generalization.},
  archive      = {J_TKDE},
  author       = {Zekun Li and Lei Qi and Yawen Li and Yinghuan Shi and Yang Gao},
  doi          = {10.1109/TKDE.2024.3368529},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {4163-4177},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Open-domain semi-supervised learning via glocal cluster structure exploitation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online learning from evolving feature spaces with deep
variational models. <em>TKDE</em>, <em>36</em>(8), 4144–4162. (<a
href="https://doi.org/10.1109/TKDE.2023.3326365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we explore a novel online learning setting, where the online learners are presented with “doubly-streaming” data. Namely, the data instances constantly streaming in are described by feature spaces that over-time evolve, with new features emerging and old features fading away. The main challenge of this problem lies in the fact that the newly emerging features are described by very few samples, resulting in weak learners that tend to make error predictions. A seemingly plausible idea to overcome the challenge is to establish a relationship between the old and new feature spaces, so that an online learner can leverage the knowledge learned from the old features to better the learning performance on the new features. Unfortunately, this idea does not scale up to high-dimensional feature spaces that entail very complex feature interplay. Specifically. a tradeoff between onlineness , which biases shallow learners, and expressiveness , which requires deep models, is inevitable. Motivated by this, we propose a novel paradigm, named Online Learning Deep models from Data of Double Streams (OLD3S), where a shared latent subspace is discovered to summarize information from the old and new feature spaces, building an intermediate feature mapping relationship. A key trait of OLD3S is to treat the model capacity as a learnable semantics, aiming to yield optimal model depth and parameters jointly in accordance with the complexity and non-linearity of the input data streams in an online fashion. To ablate its efficacy and applicability, two variants of OLD3S are proposed namely, OLD-Linear that learns the relationship by a linear function; and OLD-FD learns that two consecutive feature spaces pre-and-post evolution with fixed deep depth. Besides, instead of re-starting the entire learning process from scratch, OLD3S learns multiple newly emerging feature spaces in a lifelong manner, retaining the knowledge from the learned and vanished feature space to enjoy a jump-start of the new features’ learning process. Both theoretical analysis and empirical studies substantiate the viability and effectiveness of our proposed approach.},
  archive      = {J_TKDE},
  author       = {Heng Lian and Di Wu and Bo-Jian Hou and Jian Wu and Yi He},
  doi          = {10.1109/TKDE.2023.3326365},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {4144-4162},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Online learning from evolving feature spaces with deep variational models},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On efficient shortest path computation on terrain surface: A
direction-oriented approach. <em>TKDE</em>, <em>36</em>(8), 4129–4143.
(<a href="https://doi.org/10.1109/TKDE.2024.3363147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advance of the geo-positioning technology, the terrain surface data has become increasingly popular and has drawn much research attention from both academia and industry. Answering a shortest-path query for a given source and a given destination on a terrain surface is a fundamental problem and has many applications including Geographical Information System and 3D virtual games. We observe that all existing exact algorithms are only aware of the position of the source point and is unaware of the information of the destination point. Motivated by this, in this paper, we propose an efficient algorithm, namely direction-oriented algorithm (DIO Algorithm) , for answering shortest-path queries on a terrain surface. The algorithm properly guides the search along a direction towards the destination instead of blindly searching all possible directions from the source point. To this end, we convert the geodesic shortest path problem to a shortest obstacle-free euclidean path problem in the 2D planar unfolding of the terrain surface. Based on this conversion, we derive for each part of the terrain surface a lower bound on the length of the shortest path from the source to the destination passing through the part with a novel method. The lower bounds provide useful information that can be used to decide the visiting order of the parts on the terrain surface and guides the search of finding the destination quickly. Our experiments verified that our algorithm runs faster than the state-of-the-art by more than one order of magnitude.},
  archive      = {J_TKDE},
  author       = {Victor Junqiu Wei and Raymond Chi-Wing Wong and Cheng Long and David M. Mount and Hanan Samet},
  doi          = {10.1109/TKDE.2024.3363147},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {4129-4143},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {On efficient shortest path computation on terrain surface: A direction-oriented approach},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Network controllability perspectives on graph
representation. <em>TKDE</em>, <em>36</em>(8), 4116–4128. (<a
href="https://doi.org/10.1109/TKDE.2023.3331318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph representations in fixed dimensional feature space are vital in applying learning tools and data mining algorithms to perform graph analytics. Such representations must encode the graph&#39;s topological and structural information at the local and global scales without posing significant computation overhead. This paper employs a unique approach grounded in networked control system theory to obtain expressive graph representations with desired properties. We consider graphs as networked dynamical systems and study their controllability properties to explore the underlying graph structure. The controllability of a networked dynamical system profoundly depends on the underlying network topology, and we exploit this relationship to design novel graph representations using controllability Gramian and related metrics. We discuss the merits of this new approach in terms of the desired properties (for instance, permutation and scale invariance) of the proposed representations. Our evaluation of various benchmark datasets in the graph classification framework demonstrates that the proposed representations either outperform (sometimes by more than 6%), or give similar results to the state-of-the-art embeddings.},
  archive      = {J_TKDE},
  author       = {Anwar Said and Obaid Ullah Ahmad and Waseem Abbas and Mudassir Shabbir and Xenofon Koutsoukos},
  doi          = {10.1109/TKDE.2023.3331318},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {4116-4128},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Network controllability perspectives on graph representation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nearly optimal rates of privacy-preserving sparse
generalized eigenvalue problem. <em>TKDE</em>, <em>36</em>(8),
4101–4115. (<a href="https://doi.org/10.1109/TKDE.2023.3330775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the (sparse) generalized eigenvalue problem (GEP), which arises in a number of modern statistical learning models, such as principal component analysis (PCA), canonical correlation analysis (CCA), Fisher&#39;s discriminant analysis (FDA), and sliced inverse regression (SIR). We provide the first study on GEP in the differential privacy (DP) model under both deterministic and stochastic settings. In the low dimensional case, we provide a $\rho$ -Concentrated DP (CDP) method namely DP-Rayleigh Flow and show if the initial vector is close enough to the optimal vector, its output has an $\ell _{2}$ -norm estimation error of $\tilde{O}(\frac{d}{n}+\frac{d}{n^{2}\rho })$ (under some mild assumptions), where $d$ is the dimension and $n$ is the sample size. Next, we discuss how to find such an initial parameter privately. In the high dimensional sparse case where $d\gg n$ , we propose the DP-Truncated Rayleigh Flow method whose output could achieve an error of $\tilde{O}(\frac{s\log d}{n}+\frac{s\log d}{n^{2}\rho })$ for various statistical models, where $s$ is the sparsity of the underlying parameter. Moreover, we show that these errors in the stochastic setting are optimal up to a factor of $\text{Poly}(\log n)$ by providing the lower bounds of PCA and SIR under the statistical setting and in the CDP model. Finally, to give a separation between $\epsilon$ -DP and $\rho$ -CDP for GEP, we also provide the lower bound $\Omega (\frac{d}{n}+\frac{d^{2}}{n^{2}\epsilon ^{2}})$ and $\Omega (\frac{s\log d}{n}+\frac{s^{2}\log ^{2}d}{n^{2}\epsilon ^{2}})$ of private minimax risk for PCA, under the statistical setting and $\epsilon$ -DP model, in low and high dimensional sparse case respectively. Finally, extensive experiments on both synthetic and real-world data support our previous theoretical analysis.},
  archive      = {J_TKDE},
  author       = {Lijie Hu and Zihang Xiang and Jiabin Liu and Di Wang},
  doi          = {10.1109/TKDE.2023.3330775},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {4101-4115},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Nearly optimal rates of privacy-preserving sparse generalized eigenvalue problem},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-modal traumatic brain injury prognosis via
structure-aware field-wise learning. <em>TKDE</em>, <em>36</em>(8),
4089–4100. (<a href="https://doi.org/10.1109/TKDE.2024.3364385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traumatic brain injury (TBI) remains a growing significant public health problem and prognosis of outcome is difficult due to the multitude of factors that underlie the heterogeneity of TBI. Prognosis aims to forecast the likely development of the disease and significantly affects patient&#39;s recovery and healthcare. Traditionally, TBI prognosis relies on the physician&#39;s insights and their empirical knowledge which makes it infeasible for large-scale implementation. Existing methods utilize a single modality (i.e., either clinical data or Computed Tomography scan images) for TBI prognosis, leaving crucial information from multi-modal data largely underexplored. To address this concern, we explore a Multi-modal Structure-aware Field-wise learning (MSF) method that is capable of mining complex correlations between multi-modal data and TBI outcomes for prognosis on a real-world dataset. Specifically, we develop a High-Level Structure-Aware (HSA) module to capture the structure information of the multilayered clinical data. Experimental results on the publicly available TRACK-TBI dataset demonstrate the viability and effectiveness of our proposed method, by achieving the top-3 accuracy of 96.07% and 98.13% for 3-month and 6-month predictions after injury, respectively.},
  archive      = {J_TKDE},
  author       = {Lu Zhang and Zhibin Li and Shekhar S. Chandra and Fatima Nasrallah},
  doi          = {10.1109/TKDE.2024.3364385},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {4089-4100},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-modal traumatic brain injury prognosis via structure-aware field-wise learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-label classification with high-rank and high-order
label correlations. <em>TKDE</em>, <em>36</em>(8), 4076–4088. (<a
href="https://doi.org/10.1109/TKDE.2023.3330449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploiting label correlations is important to multi-label classification. Previous methods capture the high-order label correlations mainly by transforming the label matrix to a latent label space with low-rank matrix factorization. However, the label matrix is generally a full-rank or approximate full-rank matrix, making the low-rank factorization inappropriate. Besides, in the latent space, the label correlations will become implicit. To this end, we propose a simple yet effective method to depict the high-order label correlations explicitly, and at the same time maintain the high-rank of the label matrix. Moreover, we estimate the label correlations and infer model parameters simultaneously via the local geometric structure of the input to achieve mutual enhancement. Comparative studies over twelve benchmark data sets validate the effectiveness of the proposed algorithm in multi-label classification. The exploited high-order label correlations are consistent with common sense empirically.},
  archive      = {J_TKDE},
  author       = {Chongjie Si and Yuheng Jia and Ran Wang and Min-Ling Zhang and Yanghe Feng and Chongxiao Qu},
  doi          = {10.1109/TKDE.2023.3330449},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {4076-4088},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-label classification with high-rank and high-order label correlations},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Motif-driven contrastive learning of graph representations.
<em>TKDE</em>, <em>36</em>(8), 4063–4075. (<a
href="https://doi.org/10.1109/TKDE.2024.3364059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-training Graph Neural Networks (GNN) via self-supervised contrastive learning has recently drawn lots of attention. However, most existing works focus on node-level contrastive learning, which cannot capture global graph structure. The key challenge to conduct subgraph-level contrastive learning is to sample informative subgraphs that are semantically meaningful. To solve it, we propose to learn graph motifs, which are frequently-occurring subgraph patterns (e.g. functional groups of molecules), for better subgraph sampling. Our framework M ot I f-driven C ontrastive lea R ning O f G raph representations ( MICRO-Graph ) can: 1) use GNNs to extract motifs from large graph datasets; 2) leverage learned motifs to sample informative subgraphs for contrastive learning of GNN. We formulate motif learning as a differentiable clustering problem, and adopt EM-clustering to group similar and significant subgraphs into several motifs. Guided by these learned motifs, a sampler is trained to generate more informative subgraphs, and these subgraphs are used to train GNNs through graph-to-subgraph contrastive learning. By pre-training on the ogbg-molhiv dataset with MICRO-Graph , the pre-trained GNN achieves 2.04 $\%$ ROC-AUC average performance enhancement on various downstream benchmark datasets, which is significantly higher than other state-of-the-art self-supervised learning baselines.},
  archive      = {J_TKDE},
  author       = {Shichang Zhang and Ziniu Hu and Arjun Subramonian and Yizhou Sun},
  doi          = {10.1109/TKDE.2024.3364059},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {4063-4075},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Motif-driven contrastive learning of graph representations},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling spatio-temporal dynamical systems with neural
discrete learning and levels-of-experts. <em>TKDE</em>, <em>36</em>(8),
4050–4062. (<a href="https://doi.org/10.1109/TKDE.2024.3363711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we address the issue of modeling and estimating changes in the state of the spatio-temporal dynamical systems based on a sequence of observations like video frames. Traditional numerical simulation systems depend largely on the initial settings and correctness of the constructed partial differential equations (PDEs). Despite recent efforts yielding significant success in discovering data-driven PDEs with neural networks, the limitations posed by singular scenarios and the absence of local insights prevent them from performing effectively in a broader real-world context. To this end, this article propose the universal expert module – that is, optical flow estimation component, to capture the evolution laws of general physical processes in a data-driven fashion. To enhance local insight, we painstakingly design a finer-grained physical pipeline, since local characteristics may be influenced by various internal contextual information, which may contradict the macroscopic properties of the whole system. Further, we harness currently popular neural discrete learning to unveil the underlying important features in its latent space, this process better injects interpretability, which can help us obtain a powerful prior over these discrete random variables. We conduct extensive experiments and ablations to demonstrate that the proposed framework achieves large performance margins, compared with the existing SOTA baselines.},
  archive      = {J_TKDE},
  author       = {Kun Wang and Hao Wu and Guibin Zhang and Junfeng Fang and Yuxuan Liang and Yuankai Wu and Roger Zimmermann and Yang Wang},
  doi          = {10.1109/TKDE.2024.3363711},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {4050-4062},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Modeling spatio-temporal dynamical systems with neural discrete learning and levels-of-experts},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Metric distribution to vector: Constructing data
representation via broad-scale discrepancies. <em>TKDE</em>,
<em>36</em>(8), 4034–4049. (<a
href="https://doi.org/10.1109/TKDE.2024.3366589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph embedding provides a feasible methodology to conduct pattern classification for graph-structured data by mapping each data into the vectorial space. Various pioneering works are essentially coding method that concentrates on a vectorial representation about the inner properties of a graph in terms of the topological constitution, node attributions, link relations, etc. However, the classification for each targeted data is a qualitative issue based on understanding the overall discrepancies within the dataset scale. From the statistical point of view, these discrepancies manifest a metric distribution over the dataset scale if the distance metric is adopted to measure the pairwise similarity or dissimilarity. Therefore, we present a novel embedding strategy named $\mathbf {MetricDistribution2vec}$ to extract such distribution characteristics into the vectorial representation for each data. We demonstrate the application and effectiveness of our representation method in the supervised prediction tasks on extensive real-world structural graph datasets. The results have gained some unexpected increases compared with a surge of baselines on all the datasets, even if we take the lightweight models as classifiers. Moreover, the proposed method also conducts investigations in Few-Shot classification scenarios, and the results still show attractive discrimination in rare training samples based inference.},
  archive      = {J_TKDE},
  author       = {Xue Liu and Dan Sun and Xiaobo Cao and Hao Ye and Wei Wei},
  doi          = {10.1109/TKDE.2024.3366589},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {4034-4049},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Metric distribution to vector: Constructing data representation via broad-scale discrepancies},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MEL: Efficient multi-task evolutionary learning for
high-dimensional feature selection. <em>TKDE</em>, <em>36</em>(8),
4020–4033. (<a href="https://doi.org/10.1109/TKDE.2024.3366333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a crucial step in data mining to enhance model performance by reducing data dimensionality. However, the increasing dimensionality of collected data exacerbates the challenge known as the “curse of dimensionality”, where computation grows exponentially with the number of dimensions. To tackle this issue, evolutionary computational (EC) approaches have gained popularity due to their simplicity and applicability. Unfortunately, the diverse designs of EC methods result in varying abilities to handle different data, often underutilizing and not sharing information effectively. In this article, we propose a novel approach called PSO-based Multi-task Evolutionary Learning (MEL) that leverages multi-task learning to address these challenges. By incorporating information sharing between different feature selection tasks, MEL achieves enhanced learning ability and efficiency. We evaluate the effectiveness of MEL through extensive experiments on 22 high-dimensional datasets. Comparing against 24 EC approaches, our method exhibits strong competitiveness. In addition, we have open-sourced our code on GitHub.},
  archive      = {J_TKDE},
  author       = {Xubin Wang and Haojiong Shangguan and Fengyi Huang and Shangrui Wu and Weijia Jia},
  doi          = {10.1109/TKDE.2024.3366333},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {4020-4033},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MEL: Efficient multi-task evolutionary learning for high-dimensional feature selection},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MC-SQ and MC-MQ: Ensembles for multi-class quantification.
<em>TKDE</em>, <em>36</em>(8), 4007–4019. (<a
href="https://doi.org/10.1109/TKDE.2024.3372011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantification research proposes methods to estimate the class distribution in an independent sample. Quantification methods find applications in areas that rely on estimated aggregated quantities, such as epidemiology, sentiment analysis, political research, and ecological surveillance. For instance, epidemiologists are often concerned with the dynamics of the number of disease cases across space and time. Thus, while classification predicts individual subjects, quantifiers are the methods that directly estimate the number of cases. Although quantification is a thriving area of research, with numerous approaches proposed in the last decade, most focus has been on binary-class quantifiers. One common approach for multi-class quantification is the one-versus-all (OVA) approach, but empirical evidence suggests its performance is suboptimal. This paper&#39;s first contribution is to elucidate why OVA quantifiers struggle to perform well in multi-class settings due to a distribution shift. To circumvent this problem, our second proposal is two new multi-class quantifiers based on ensemble learning that significantly improve performance for binary and multi-class settings. Our comprehensive experimental setup with 37 state-of-the-art (single and ensemble) quantifiers shows that our ensembles are the best-performing quantifiers and rank first in a recent quantification competition.},
  archive      = {J_TKDE},
  author       = {Zahra Donyavi and Adriane B. S. Serapião and Gustavo Batista},
  doi          = {10.1109/TKDE.2024.3372011},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {4007-4019},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MC-SQ and MC-MQ: Ensembles for multi-class quantification},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Longer pick-up for less pay: Towards discount-based mobility
services. <em>TKDE</em>, <em>36</em>(8), 3992–4006. (<a
href="https://doi.org/10.1109/TKDE.2024.3362893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of mobile Internet technology, on-demand car-hailing services have become essential for people&#39;s daily commuting. Order dispatch is a critical problem in on-demand car-hailing services. However, in most existing works, the service provider is asked to set a unified pick-up distance to prevent long waiting time for requesters. Indeed, different requesters have different tolerance for pick-up distance, and some requesters may accept longer pick-ups if offered discounts for payment. Regarding this fact, we formulate discount-based order dispatch as a coupling of two subproblems, discount determination and order dispatch, aiming to dispatch more orders and thereby more platform profits. We propose customized methods to solve the problems for shared and non-shared mobility services, respectively. We also conduct extensive experiments to evaluate the effectiveness and efficiency of our proposed methods on a real dataset, which shows that our methods can achieve 170% improvements in non-shard services and 43% improvements in ridesharing services on average in terms of attained profit compared to the widely adopted baselines.},
  archive      = {J_TKDE},
  author       = {Wanyi Xie and Zhijia Chen and Chen Jason Zhang and Libin Zheng and Peng Cheng and Jian Yin and Xuemin Lin},
  doi          = {10.1109/TKDE.2024.3362893},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3992-4006},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Longer pick-up for less pay: Towards discount-based mobility services},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learned optimizer for online approximate query processing in
data exploration. <em>TKDE</em>, <em>36</em>(8), 3977–3991. (<a
href="https://doi.org/10.1109/TKDE.2024.3361989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the interactive data exploration, approximate query processing (AQP) can be used to quickly return query results at the cost of accuracy. For online AQP, the sampler can be treated as an operator in the query plan. During the query optimization for AQP, heuristic rules are usually used to guide the sampler push-down. However, due to the complexity and changes of data distribution, the heuristic rule-based optimization methods cannot meet the users’ query accuracy requirements. In this article, we propose a learning-based query optimization method for online AQP. We first introduce the weak equivalence concept and propose a series of push-down rules to guide the sampler push-down during the query optimization. Then, to enable more queries to meet the users’ query accuracy requirements, we propose a deep learning model to further optimize the query plan. By using this model during each push-down process of the sampler, we try to avoid the negative effect of inappropriate sampler push-down on query accuracy, especially when there is an inconsistency between the underlying and intermediate data distribution. Extensive experiments show that the method proposed in this paper can outperform the state-of-the-art online sampling-based AQP method by 1.2×−7.9× in query accuracy.},
  archive      = {J_TKDE},
  author       = {Liyuan Liu and Hanbing Zhang and Yinan Jing and Zhenying He and Kai Zhang and X. Sean Wang},
  doi          = {10.1109/TKDE.2024.3361989},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3977-3991},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learned optimizer for online approximate query processing in data exploration},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learnable model augmentation contrastive learning for
sequential recommendation. <em>TKDE</em>, <em>36</em>(8), 3963–3976. (<a
href="https://doi.org/10.1109/TKDE.2023.3330426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential Recommendation (SR) methods play a crucial role in recommender systems, which aims to capture users’ dynamic interest from their historical interactions. Recently, Contrastive Learning (CL), which has emerged as a successful method for sequential recommendation, utilizes various data augmentations to generate contrastive views to mine supervised signals from data to alleviate data sparsity issues. However, most existing sequential data augmentation methods may destroy semantic sequential interaction characteristics. Meanwhile, they often adopt random operations when generating contrastive views leading to suboptimal performance. To this end, in this paper, we propose a L earnable M odel A ugmentation Contrastive learning for sequential Rec ommendation (LMA4Rec) . Specifically, LMA4Rec first takes the model-based augmentation method to generate constructive views. Then, LMA4Rec uses Learnable Bernoulli Dropout (LBD) to implement learnable model augmentation operations. Next, contrastive learning is used between the contrastive views to extract supervised signals. Furthermore, a novel multi-positive contrastive learning loss alleviates the supervised sparsity issue. Finally, experiments on public datasets show that our LMA4Rec method effectively improved sequential recommendation performance compared with the state-of-the-art baseline methods.},
  archive      = {J_TKDE},
  author       = {Yongjing Hao and Pengpeng Zhao and Xuefeng Xian and Guanfeng Liu and Lei Zhao and Yanchi Liu and Victor S. Sheng and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2023.3330426},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3963-3976},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learnable model augmentation contrastive learning for sequential recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LDPTube: Theoretical utility benchmark and enhancement for
LDP mechanisms in high-dimensional space. <em>TKDE</em>, <em>36</em>(8),
3948–3962. (<a href="https://doi.org/10.1109/TKDE.2024.3369184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While collecting data from a large population, local differential privacy (LDP), which only sends users’ perturbed data to the data collector, becomes a popular solution to preserving each user&#39;s privacy. However, as high-dimensional data collection becomes prevalent for machine learning, LDP suffers from low utility (a.k.a., the dimensionality curse) as its privacy budget in each dimension is severely diluted. In a previous work (Duan et al. 2022), we proposed an analytical framework for benchmarking various LDP mechanisms and a re-calibration protocol for its utility enhancement in high-dimensional space. However, they have several limitations, including difficulty in setting a suitable benchmark parameter (i.e., the probabilistic supremum of deviation), a mismatch of the metric with prevalent experimental metrics, and costly re-benchmarking operation upon population change. In this paper, we propose a toolbox LDPTube to address these issues. It first consists of a non-parametric benchmark in high-dimensional space, which adopts MSE as the metric and avoids re-benchmarking upon population change. Then we adapt this benchmark to personalized LDP, where each user can choose her own privacy budget and privacy region. Last but not the least, we enhance the re-calibration protocol in (Duan et al. 2022) by an adaptive protocol HDR4ME* that opportunistically chooses suitable regularization terms that can maximize utility. We verify the correctness and effectiveness of these new solutions by both theoretical analysis and experimental results.},
  archive      = {J_TKDE},
  author       = {Jiawei Duan and Qingqing Ye and Haibo Hu and Xinyue Sun},
  doi          = {10.1109/TKDE.2024.3369184},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3948-3962},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {LDPTube: Theoretical utility benchmark and enhancement for LDP mechanisms in high-dimensional space},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Inter-intra modality knowledge learning and clustering
noise alleviation for unsupervised visible-infrared person
re-identification. <em>TKDE</em>, <em>36</em>(8), 3934–3947. (<a
href="https://doi.org/10.1109/TKDE.2024.3367304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised visible-infrared person re-identification (USVI-ReID) is a valuable yet under-explored task that addresses the challenge of person retrieval from different modalities without annotations. USVI-ReID is faced with large intra and inter modality discrepancy as well as pseudo label noises. In this paper, we propose a novel learning approach termed Spectrum-Cam Aware and Refined Cross-Pair learning strategy (SCA-RCP), crafted to concurrently tackle these issues. The Dual-Spectrum Augmentation (DSA) method is first presented to mine inter-modality knowledge between the visible and infrared modalities by expanding the diversity of spectra for both modalities. Concurrently, to learn intra-modality knowledge, we divide the clusters into camera-based proxies and introduce the Cross-modal Cam-Aware Proxy learning method (CCAP) to pull together camera-based proxies that correspond to the same identity across modalities. Finally, to mitigate clustering noise, the innovative Refined Cross-Pair learning strategy (RCP) is devised comprising the Intra-modality Label Refinement (ILR) and Bi-directional Cross Pairing (BCP) method. ILR calculates maximum-likely label for each instance by employing the intra-modal cluster memories as a classifier, while BCP establish dependable cross-paired labels in a bidirectional manner. Extensive experiments on the cross-modality datasets demonstrate the superior performance of our model over the state-of-art method.},
  archive      = {J_TKDE},
  author       = {Zhiyong Li and Haojie Liu and Xiantao Peng and Wei Jiang},
  doi          = {10.1109/TKDE.2024.3367304},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3934-3947},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Inter-intra modality knowledge learning and clustering noise alleviation for unsupervised visible-infrared person re-identification},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Instance selection via voronoi neighbors for binary
classification tasks. <em>TKDE</em>, <em>36</em>(8), 3921–3933. (<a
href="https://doi.org/10.1109/TKDE.2023.3328952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large datasets available in many applications have enabled the training of binary classifiers to match or even outperform humans. However, the large volume of data introduces computational burden during the training and calibration of model parameters. Since the optimal decision surface (ODS) of a classification task is often determined by a few nearby instances, a novel PDOC-V method is proposed to identify them. A Bayesian probability model is adopted to describe the ODS. An instance is close to the ODS if its probability of belonging to the positive and negative classes is similar. The probabilities of an instance are estimated by partitioning the input space into cells containing a single instance via the Voronoi diagram and inspecting its Voronoi neighbors. A randomized ray shooting algorithm is adopted to accelerate our algorithm. In many natural datasets, the spatial distribution of instances is often uneven. For such datasets, our method is more robust than existing distance-based instance selection methods. Comprehensive experiments suggest that common classifiers trained on instances selected by PDOC-V can accurately recover the ODS. Moreover, for many natural datasets, common classifiers trained on 10% - 20% of instances can achieve more than 98% of the full set performance.},
  archive      = {J_TKDE},
  author       = {Ying Fu and Kaibo Liu and Wenbin Zhu},
  doi          = {10.1109/TKDE.2023.3328952},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3921-3933},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Instance selection via voronoi neighbors for binary classification tasks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Improving automatic parallel training via balanced memory
workload optimization. <em>TKDE</em>, <em>36</em>(8), 3906–3920. (<a
href="https://doi.org/10.1109/TKDE.2024.3370614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer models have emerged as the leading approach for achieving state-of-the-art performance across various application domains, serving as the foundation for advanced large-scale deep learning (DL) models. However, efficiently training these models across multiple GPUs remains a complex challenge due to the abundance of parallelism options. Existing DL systems either require manual efforts to design distributed training plans or limit parallelism combinations to a constrained search space. In this paper, we present Galvatron-BMW, a novel system framework that integrates multiple prevalent parallelism dimensions and automatically identifies the most efficient hybrid parallelism strategy. To effectively navigate this vast search space, we employ a decision tree approach for decomposition and pruning based on intuitive insights. We further utilize a dynamic programming search algorithm to derive the optimal plan. Moreover, to improve resource utilization and enhance system efficiency, we propose a bi-objective optimization workflow that focuses on workload balance. Our evaluations on different Transformer models demonstrate the capabilities of Galvatron-BMW in automating distributed training under varying GPU memory constraints. Across all tested scenarios, Galvatron-BMW consistently achieves superior system throughput, surpassing previous approaches that rely on limited parallelism strategies.},
  archive      = {J_TKDE},
  author       = {Yujie Wang and Youhe Jiang and Xupeng Miao and Fangcheng Fu and Shenhan Zhu and Xiaonan Nie and Yaofeng Tu and Bin Cui},
  doi          = {10.1109/TKDE.2024.3370614},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3906-3920},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Improving automatic parallel training via balanced memory workload optimization},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HyObscure: Hybrid obscuring for privacy-preserving data
publishing. <em>TKDE</em>, <em>36</em>(8), 3893–3905. (<a
href="https://doi.org/10.1109/TKDE.2023.3331568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Minimizing privacy leakage while ensuring data utility is a critical problem in a privacy-preserving data publishing task, from which data holders can boost platform engagements or enlarge data values. Most prior research concerned only with either privacy-insensitive or exact private data and resorts to a single obscuring method to achieve a privacy-utility tradeoff, which is inadequate for real-life hybrid data especially when facing machine learning-based inference attacks. This work takes a pilot study on privacy-preserving data publishing when both widely adopted generalization and obfuscation operations are employed for privacy-heterogeneous data protection. Specifically, we first propose novel measures for privacy and utility values quantification and formulate the hybrid privacy-preserving data obscuring problem to account for the joint effect of generalization and obfuscation. We then design a novel protection mechanism called HyObscure, which decomposes the original problem into three sub-problems to cross-iteratively optimize the hybrid operations for maximum privacy protection under a certain data utility guarantee. The convergence of the iterative process and the privacy leakage bound of HyObscure are also provided in theory. Extensive experiments demonstrate that HyObscure significantly outperforms a variety of state-of-the-art baseline methods when facing various inference attacks in different scenarios.},
  archive      = {J_TKDE},
  author       = {Xiao Han and Yuncong Yang and Junjie Wu and Hui Xiong},
  doi          = {10.1109/TKDE.2023.3331568},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3893-3905},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {HyObscure: Hybrid obscuring for privacy-preserving data publishing},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). HJE: Joint convolutional representation learning for
knowledge hypergraph completion. <em>TKDE</em>, <em>36</em>(8),
3879–3892. (<a href="https://doi.org/10.1109/TKDE.2024.3365727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge hypergraph representation learning , which projects entities and $n$ -ary relations into a low-dimensional vector space, remains a challenging area to be explored despite the ubiquity of $n$ -ary relational facts in the real world. Current methods are always extensions of those used for knowledge graphs with shallow or deep structures. However, shallow and linear models limit the extraction capacity of the latent knowledge, while deep and non-linear models lead to the overabundance of parameters. In this paper, we propose a novel knowledge hypergraph completion model called HJE, which utilizes the powerful capability of convolutional neural networks for efficient representation learning. Interaction-enhanced 3D convolution and relation-aware 2D convolution are jointly utilized by HJE to extract explicit and implicit global knowledge and semantic information effectively without compromising the translation property of the model. Moreover, HJE constructs a unified learnable embedding matrix to capture entity position information in knowledge tuples. The entity mask mechanism can naturally couple the multilinear scoring approach for $n$ -ary facts to speed up the training convergence of the model. Extensive experimental results on real datasets of knowledge hypergraphs and knowledge graphs demonstrate the superior performance of HJE compared with state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Zhao Li and Chenxu Wang and Xin Wang and Zirui Chen and Jianxin Li},
  doi          = {10.1109/TKDE.2024.3365727},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3879-3892},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {HJE: Joint convolutional representation learning for knowledge hypergraph completion},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). GNNGO3D: Protein function prediction based on 3D structure
and functional hierarchy learning. <em>TKDE</em>, <em>36</em>(8),
3867–3878. (<a href="https://doi.org/10.1109/TKDE.2023.3331005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Protein sequences accumulate in large quantities, and the traditional method of annotating protein function by experiment has been unable to bridge the gap between annotated proteins and unannotated proteins. Machine learning-based protein function prediction is an effective approach to solve this problem. Most of the existing methods only use the protein sequence but ignore the three-dimensional structure which is closely related to the protein function. And the hierarchy of protein functions is not adequately considered. To solve this problem, we propose a graph neural network (GNNGO3D) that combines the three-dimensional structure and functional hierarchy learning. GNNGO3D simultaneously uses three kinds of information: protein sequence, tertiary structure, and hierarchical relationship of protein function to predict protein function. The novelty of GNNGO3D lies in that it integrates the learning of functional level information into the method of predicting protein function by using tertiary structure information, fully learning the relationship between protein functions, and helping to better predict protein function. Experimental results show that our method is superior to existing methods for predicting protein function based on sequence and structure.},
  archive      = {J_TKDE},
  author       = {Liyuan Zhang and Yongquan Jiang and Yan Yang},
  doi          = {10.1109/TKDE.2023.3331005},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3867-3878},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GNNGO3D: Protein function prediction based on 3D structure and functional hierarchy learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fresh data retrieval with speed-adjustable mobile devices in
cyber-physical systems. <em>TKDE</em>, <em>36</em>(8), 3851–3866. (<a
href="https://doi.org/10.1109/TKDE.2023.3348473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile devices have been increasingly deployed in large-scale cyber-physical systems (CPS) to traverse the field and retrieve various data measurements from designated physical entities with stringent performance requirements. This work studies the Availability-constrained real-time Fresh Data Retrieval problem in CPS with a Speed Adjustable mobile device (AFDR-SA). The goal is to maintain the temporal validity of the real-time data with different priorities to be retrieved in the system while meeting the data availability constraints imposed by the communication range between the mobile device and the physical entities. The general case of the AFDR-SA problem is proved to be NP-hard. A dynamic programming (DP)-based optimal algorithm is proposed for a special scenario where the retrieval times of individual data items with the same priority are of the same length. For the general case where data items can have arbitrary retrieval times and different priorities, another different DP-based scheme is proposed, which is proved to be optimal given the retrieval order. A fast heuristic with low complexity is also proposed for the general problem to improve the computational efficiency. The experimental results show that the proposed schemes for the general case outperform the state-of-the-art methods and have close performance compared to the optimal solution while incurring much less computational overhead.},
  archive      = {J_TKDE},
  author       = {Chenchen Fu and Xiaoxing Qiu and Vincent Chau and Zelin Yun and Chun Jason Xue and Weiwei Wu and Junzhou Luo and Song Han},
  doi          = {10.1109/TKDE.2023.3348473},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3851-3866},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fresh data retrieval with speed-adjustable mobile devices in cyber-physical systems},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated continual learning via knowledge fusion: A survey.
<em>TKDE</em>, <em>36</em>(8), 3832–3850. (<a
href="https://doi.org/10.1109/TKDE.2024.3363240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data privacy and silos are nontrivial and greatly challenging in many real-world applications. Federated learning is a decentralized approach to training models across multiple local clients without the exchange of raw data from client devices to global servers. However, existing works focus on a static data environment and ignore continual learning from streaming data with incremental tasks. Federated Continual Learning (FCL) is an emerging paradigm to address model learning in both federated and continual learning environments. The key objective of FCL is to fuse heterogeneous knowledge from different clients and retain knowledge of previous tasks while learning on new ones. In this work, we delineate federated learning and continual learning first and then discuss their integration, i.e., FCL, and particular FCL via knowledge fusion. In summary, our motivations are four-fold: we (1) raise a fundamental problem called “spatial-temporal catastrophic forgetting” and evaluate its impact on the performance using a well-known method called federated averaging (FedAvg), (2) integrate most of the existing FCL methods into two generic frameworks, namely synchronous FCL and asynchronous FCL, (3) categorize a large number of methods according to the mechanism involved in knowledge fusion, and finally (4) showcase an outlook on the future work of FCL.},
  archive      = {J_TKDE},
  author       = {Xin Yang and Hao Yu and Xin Gao and Hao Wang and Junbo Zhang and Tianrui Li},
  doi          = {10.1109/TKDE.2024.3363240},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3832-3850},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Federated continual learning via knowledge fusion: A survey},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FedCORE: Federated learning for cross-organization
recommendation ecosystem. <em>TKDE</em>, <em>36</em>(8), 3817–3831. (<a
href="https://doi.org/10.1109/TKDE.2024.3363505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recommendation system is of vital importance in delivering personalization services, which often brings continuous dual improvement in user experience and organization revenue. However, the data of one single organization may not be enough to build an accurate recommendation model for inactive or new cold-start users. Moreover, due to the recent regulatory restrictions on user privacy and data security, as well as the commercial conflicts, the raw data in different organizations can not be merged to alleviate the scarcity issue in training a model. In order to learn users’ preferences from such cross-silo data of different organizations and then provide recommendations to the cold-start users, we propose a novel federated learning framework, i.e., federated cross-organization recommendation ecosystem (FedCORE). Specifically, we first focus on the ecosystem problem of cross-organization federated recommendation, including cooperation patterns and privacy protection. For the former, we propose a privacy-aware collaborative training and inference algorithm. For the latter, we define four levels of privacy leakage and propose some methods for protecting the privacy. We then conduct extensive experiments on three real-world datasets and two seminal recommendation models to study the impact of cooperation in our proposed ecosystem and the effectiveness of privacy protection.},
  archive      = {J_TKDE},
  author       = {Zhitao Li and Xueyang Wu and Weike Pan and Youlong Ding and Zeheng Wu and Shengqi Tan and Qian Xu and Qiang Yang and Zhong Ming},
  doi          = {10.1109/TKDE.2024.3363505},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3817-3831},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {FedCORE: Federated learning for cross-organization recommendation ecosystem},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient algorithms for rank-regret minimization.
<em>TKDE</em>, <em>36</em>(8), 3801–3816. (<a
href="https://doi.org/10.1109/TKDE.2024.3363009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-criteria decision-making usually requires finding a small representative set from the database. A popular method, the regret minimization set (RMS) query, returns a size $r$ subset $S$ of the full dataset $D$ that minimizes the regret-ratio (the difference between the scores of top-1 in $S$ and top-1 in $D$ , for any utility function). RMS is not shift invariant, causing inconsistency in results. Further, the regret-ratio is often a “made up” number and users may mistake its absolute value. Instead, users do understand the notion of rank. Therefore, in this paper, we consider finding a fixed-size set $S$ to minimize the maximum rank-regret (the rank of top-1 of $S$ in the sorted list of $D$ ) over all possible utility functions, called the rank-regret minimization (RRM) problem, which is shift invariant. In 2D space, we design an exact algorithm 2DRRM for RRM. In HD space, we propose an approximate algorithm HDRRM with theoretical guarantees on rank-regret. It combines the ideas of space discretization and clustering. Extensive experiments verify the efficiency and effectiveness of our algorithms. In particular, HDRRM always has the best output quality in experiments.},
  archive      = {J_TKDE},
  author       = {Xingxing Xiao and Jianzhong Li},
  doi          = {10.1109/TKDE.2024.3363009},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3801-3816},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient algorithms for rank-regret minimization},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Disentangling structured components: Towards adaptive,
interpretable and scalable time series forecasting. <em>TKDE</em>,
<em>36</em>(8), 3783–3800. (<a
href="https://doi.org/10.1109/TKDE.2024.3371931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time-series (MTS) forecasting is a paramount and fundamental problem in many real-world applications. The core issue in MTS forecasting is how to effectively model complex spatial-temporal patterns. In this paper, we develop a adaptive, interpretable and scalable forecasting framework, which seeks to individually model each component of the spatial-temporal patterns. We name this framework SCNN, as an acronym of S tructured C omponent-based N eural N etwork. SCNN works with a pre-defined generative process of MTS, which arithmetically characterizes the latent structure of the spatial-temporal patterns. In line with its reverse process, SCNN decouples MTS data into structured and heterogeneous components and then respectively extrapolates the evolution of these components, the dynamics of which are more traceable and predictable than the original MTS. Extensive experiments are conducted to demonstrate that SCNN can achieve superior performance over state-of-the-art models on three real-world datasets. Additionally, we examine SCNN with different configurations and perform in-depth analyses of the properties of SCNN.},
  archive      = {J_TKDE},
  author       = {Jinliang Deng and Xiusi Chen and Renhe Jiang and Du Yin and Yi Yang and Xuan Song and Ivor W. Tsang},
  doi          = {10.1109/TKDE.2024.3371931},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3783-3800},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Disentangling structured components: Towards adaptive, interpretable and scalable time series forecasting},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dimensionality reduction for partial label learning: A
unified and adaptive approach. <em>TKDE</em>, <em>36</em>(8), 3765–3782.
(<a href="https://doi.org/10.1109/TKDE.2024.3367721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial label learning learns from instances with weak supervision, where each instance is associated with a set of candidate labels, among which only one is valid. Recently, dimensionality reduction has emerged as an effective preprocessing strategy to improve generalization performance. Existing approaches mainly tackle this problem through supervised or unsupervised dimensionality reduction. However, the former requires ground-truth labels, which are concealed in candidate label sets. Consequently, methods in this line may suffer from overfitting due to false positive labels in candidate label set. Conversely, the latter overlooks weakly supervised information in training instances, leading to performance degradation. In this paper, we propose an approach called partial label Dimensionality Reduction via Adaptive Weight ( Draw ) to leverage the strengths of Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA). Specifically, our approach tends to exploit unsupervised and data-driven nature of PCA to capture underlying structure of instances in initial stage. As the ground-truth label is gradually identified, our method increasingly relies on the discriminative ability of LDA to enhance the separation between different classes. Through extensive experiments on diverse partial label datasets, we validate that the proposed dimensionality reduction approach significantly improves classification performance of well-established partial label learning algorithms.},
  archive      = {J_TKDE},
  author       = {Xiang-Ru Yu and Deng-Bao Wang and Min-Ling Zhang},
  doi          = {10.1109/TKDE.2024.3367721},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3765-3782},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dimensionality reduction for partial label learning: A unified and adaptive approach},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differentiable clustering for graph attention.
<em>TKDE</em>, <em>36</em>(8), 3751–3764. (<a
href="https://doi.org/10.1109/TKDE.2024.3363703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph clusters (or communities) represent important graph structural information. In this paper, we present D ifferentiable C lustering for graph AT tention (DCAT). To the best of our knowledge, DCAT is the first solution that incorporates graph clustering into graph attention networks (GAT) to learn cluster-aware attention scores for semi-supervised learning tasks. In DCAT, we propose a novel approach to formulating graph clustering as an auxiliary differentiable objective based on modularity maximization, which can be optimized together with the learning objective of GAT for a semi-supervised task. Specifically, we propose a solution to relaxing modularity maximization from a discrete optimization problem to a differentiable objective with theoretical guarantee so that we can learn cluster-aware attention scores by jointly learning from graph clustering and a semi-supervised learning task. To address the computational challenge, we further propose to reformulate the constraint introduced by the clustering objective into a new form. Our analysis shows that DCAT allocates higher attention scores to nodes within the same cluster, allowing them to have a higher influence in node representation learning, and thus DCAT will generate better node representations for downstream applications. The experimental results on commonly used datasets show that DCAT outperforms popular and state-of-the-art graph neural networks.},
  archive      = {J_TKDE},
  author       = {Haicang Zhou and Tiantian He and Yew-Soon Ong and Gao Cong and Quan Chen},
  doi          = {10.1109/TKDE.2024.3363703},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3751-3764},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Differentiable clustering for graph attention},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DASVDD: Deep autoencoding support vector data descriptor for
anomaly detection. <em>TKDE</em>, <em>36</em>(8), 3739–3750. (<a
href="https://doi.org/10.1109/TKDE.2023.3328882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One-Class anomaly detection aims to detect anomalies from normal samples using a model trained on normal data. With recent advancements in deep learning, researchers have designed efficient one-class anomaly detection methods. Existing works commonly use neural networks to map the data into a more informative representation and then apply an anomaly detection algorithm. In this paper, we propose a method, DASVDD, that jointly learns the parameters of an autoencoder while minimizing the volume of an enclosing hypersphere on its latent representation. We propose a novel anomaly score that combines the autoencoder&#39;s reconstruction error and the distance from the center of the enclosing hypersphere in the latent representation. Minimizing this anomaly score aids us in learning the underlying distribution of the normal class during training. Including the reconstruction error in the anomaly score ensures that DASVDD does not suffer from the hypersphere collapse issue since the DASVDD model does not converge to the trivial solution of mapping all inputs to a constant point in the latent representation. Experimental evaluations on several benchmark datasets show that the proposed method outperforms the commonly used state-of-the-art anomaly detection algorithms while maintaining robust performance across different anomaly classes.},
  archive      = {J_TKDE},
  author       = {Hadi Hojjati and Narges Armanfard},
  doi          = {10.1109/TKDE.2023.3328882},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3739-3750},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DASVDD: Deep autoencoding support vector data descriptor for anomaly detection},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Context-aware commonsense knowledge graph reasoning with
path-guided explanations. <em>TKDE</em>, <em>36</em>(8), 3725–3738. (<a
href="https://doi.org/10.1109/TKDE.2024.3365103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commonsense knowledge graphs (CKGs) store massive commonsense knowledge as triples whose nodes consist of free-form texts. CKG reasoning aims to predict missing nodes in incomplete commonsense triples, which is challenging as it requires more accurate embeddings for reasoning. Compared to conventional knowledge graphs (KGs), CKGs have deficient structural information due to their sparsity and contain nodes indistinguishable due to the conceptual diversity. These issues limit the performance of previous reasoning methods, because they face difficulties obtaining precise CKG representations. To address these issues, we propose a context-aware CKG reasoning framework with path-guided explanations, named CoRPe. First, CoRPe constructs context sentences based on the target commonsense triple using designed templates. The context captures reasoning paths instantiated from the first-order logic. Second, to improve CKG representations, CoRPe injects context semantics and employs a context-augmented tuning strategy on a pre-trained language model (PLM) via a synergistic optimization. Finally, CoRPe embeds structural information using a graph convolutional network (GCN) and associates the textual semantics for joint scoring. Extensive experiments on two CKGs show that CoRPe outperforms state-of-the-art KG and CKG reasoning baselines in terms of embedding and reasoning performance. Furthermore, the interpretability of CoRPe is reflected in the implicit logic during reasoning.},
  archive      = {J_TKDE},
  author       = {Yudai Pan and Jun Liu and Tianzhe Zhao and Lingling Zhang and Qianying Wang},
  doi          = {10.1109/TKDE.2024.3365103},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3725-3738},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Context-aware commonsense knowledge graph reasoning with path-guided explanations},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Compact estimator for streaming triangle counting.
<em>TKDE</em>, <em>36</em>(8), 3712–3724. (<a
href="https://doi.org/10.1109/TKDE.2024.3371228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Streaming triangle counting is a critical issue in graph stream mining, with applications in dense subgraph discovery, web mining, anomaly detection, and more. Recent efforts have focused on estimating triangle counts in graph streams, primarily through sampling methods. However, because of limited memory resources for handling high speed streams, traditional sampling methods suffer from reduced sampling rate and thereby performance loss. In this paper, we propose a new compact data structure called uHLL to process edge streams by considering the tradeoff between estimation accuracy and memory efficiency. Furthermore, different from conventional triangle counting algorithms, we solve the estimation of union set cardinality for edge-local triangle count under both centralized and distributed framework, so as to efficiently estimate the global triangle count by a one-pass streaming algorithm. To the best of our knowledge, this is the first implementation of a distributed framework using a compact data structure for streaming triangle counting. We provide theoretical proof of unbiasedness and derive the variance of the union set and global triangle count. We compare our scheme with 11 algorithms, showing that under the same experimental setting, uHLL and distributed uHLL are at least $ 2.3$ and $ 1.7$ times more accurate than the state-of-the-art, respectively.},
  archive      = {J_TKDE},
  author       = {Jiqing Gu and Chao Song and Haipeng Dai and Li Lu and Ming Liu},
  doi          = {10.1109/TKDE.2024.3371228},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3712-3724},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Compact estimator for streaming triangle counting},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BIC-based mixture model defense against data poisoning
attacks on classifiers: A comprehensive study. <em>TKDE</em>,
<em>36</em>(8), 3697–3711. (<a
href="https://doi.org/10.1109/TKDE.2024.3365548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data Poisoning (DP) is an effective attack that causes trained classifiers to misclassify their inputs. DP attacks significantly degrade a classifier&#39;s accuracy by covertly injecting attack samples into the training set. Broadly applicable to different classifier structures, without strong assumptions about the attacker, an unsupervised Bayesian Information Criterion (BIC)-based mixture model defense against “error generic” DP attacks is herein proposed that: 1) addresses the most challenging embedded DP scenario wherein, if DP is present, the poisoned samples are an a priori unknown subset of the training set, and with no clean validation set available; 2) applies a mixture model both to well-fit potentially multi-modal class distributions and to capture poisoned samples within a small subset of the mixture components; and 3) jointly identifies poisoned components and samples by minimizing the BIC cost defined over the whole training set, with the identified poisoned data removed prior to classifier training. Our experimental results, for various classifier structures and benchmark datasets, demonstrate the effectiveness of our defense under strong DP attacks, as well as its superiority over other DP defenses.},
  archive      = {J_TKDE},
  author       = {Xi Li and David J. Miller and Zhen Xiang and George Kesidis},
  doi          = {10.1109/TKDE.2024.3365548},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3697-3711},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {BIC-based mixture model defense against data poisoning attacks on classifiers: A comprehensive study},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BGAE: Auto-encoding multi-view bipartite graph clustering.
<em>TKDE</em>, <em>36</em>(8), 3682–3696. (<a
href="https://doi.org/10.1109/TKDE.2024.3363217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised multi-view bipartite graph clustering (MVBGC) is a fast-growing research, due to promising scalability in large-scale tasks. Although many variants are proposed by various strategies, a common design is to construct the bipartite graph directly from the input data, i.e., only consider the unidirectional “encoding” process. However, “encoding-decoding” mechanism is a popular design for deep learning, the most representative one is auto-encoder (AE). Enlightened by this, this paper rethinks existing MVBGC paradigms and transfers the “encoding-decoding” design into graph machine learning, and proposes a novel framework termed auto-encoding multi-view bipartite graph clustering (BGAE), which integrates encoding, bipartite graph construction, and decoding modules in a self-supervised learning manner. The encoding module extracts a latent joint representation from the input data, the bipartite graph construction module learns a bipartite graph with connectivity constraint in latent semantic space, and the decoding module recreates the input data via the bipartite graph. Therefore, our novel BGAE combines representation learning, bipartite graph learning, reconstruction learning, and label inference into a unified framework. All the modules are seamlessly integrated and mutually reinforcing for clustering-friendly purposes. Extensive experiments verify the superiority of our novel design and the significance of “decoding” process. To the best of our knowledge, this is the first attempt to explore “encoding-decoding” design in traditional MVBGC.},
  archive      = {J_TKDE},
  author       = {Liang Li and Yuangang Pan and Jie Liu and Yue Liu and Xinwang Liu and Kenli Li and Ivor W. Tsang and Keqin Li},
  doi          = {10.1109/TKDE.2024.3363217},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3682-3696},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {BGAE: Auto-encoding multi-view bipartite graph clustering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Belief rényi divergence of divergence and its application in
time series classification. <em>TKDE</em>, <em>36</em>(8), 3670–3681.
(<a href="https://doi.org/10.1109/TKDE.2024.3369719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series data contains the amount of information to reflect the development process and state of a subject. Especially, the complexity is a valuable factor to illustrate the feature of the time series. However, it is still an open issue to measure the complexity of sophisticated time series due to its uncertainty. In this study, based on the belief R $\acute{e}$ nyi divergence, a novel time series complexity measurement algorithm, called belief R $\acute{e}$ nyi divergence of divergence (BR $\acute{e}$ DOD), is proposed. Specifically, the BR $\acute{e}$ DOD algorithm takes the boundaries of time series value into account. What is more, according to the Dempster-Shafer (D-S) evidence theory, the time series is converted to the basic probability assignments (BPAs) and it measures the divergence of a divergence sequence. Then, the secondary divergence of the time series is figured out to represent the complexity of the time series. In addition, the BR $\acute{e}$ DOD algorithm is applied to sets of cardiac inter-beat interval time series, which shows the superiority of the proposed method over classical machine learning methods and recent well-known works.},
  archive      = {J_TKDE},
  author       = {Lang Zhang and Fuyuan Xiao},
  doi          = {10.1109/TKDE.2024.3369719},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3670-3681},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Belief rényi divergence of divergence and its application in time series classification},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient adaptive multi-kernel learning with safe
screening rule for outlier detection. <em>TKDE</em>, <em>36</em>(8),
3656–3669. (<a href="https://doi.org/10.1109/TKDE.2023.3330708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in multi-kernel-based methods for outlier detection have positioned them as an attractive way to detect instances that are markedly different from the remaining data in a dataset. Currently, most outlier detection approaches based on multi-kernel learning are simply a convex combination of various kernels with handcrafted weights, meaning that these weights may not be suitable. Meanwhile, this combination of weights does not sufficiently consider the intrinsic correlations of instances when fusing different kernels. Thus, a key challenge is how to adaptively learn an appropriate combination of weights for capturing a new feature space in which outliers can be better detected than the original space. Simultaneously, it is still a burning issue to get the optimal combination of weights due to considerable computational cost and memory usage when the feature or instance size is large. In this paper, we propose a novel method for e fficient a daptive m ulti-kernel for o utlier d etection (EAMOD), which automatically learns the optimal weight for each training instance under different kernels using a non-negative function. In addition, we design a safe screening rule (SSR) for EAMOD to improve its training efficiency without any loss of accuracy. To the best of our knowledge, it is the first attempt to develop SSR for multi-kernel-based outlier detection methods. Extensive experiments show that EAMOD is effective and efficient.},
  archive      = {J_TKDE},
  author       = {Xinye Wang and Lei Duan and Chengxin He and Yuanyuan Chen and Xindong Wu},
  doi          = {10.1109/TKDE.2023.3330708},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3656-3669},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {An efficient adaptive multi-kernel learning with safe screening rule for outlier detection},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A survey of multi-dimensional indexes: Past and future
trends. <em>TKDE</em>, <em>36</em>(8), 3635–3655. (<a
href="https://doi.org/10.1109/TKDE.2024.3364183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Index structures are powerful tools for improving query performance and reducing disk access in database systems. Multi-dimensional indexes, in particular, are used to filter records effectively based on multiple attributes. Classical multi-dimensional index structures, such as KD-Tree, Quadtree, and R-Tree, have been widely used in modern databases. However, advancements in hardware and algorithms have led to the emergence of new types of multi-dimensional index structures. In this paper, we begin by reviewing classical multi-dimensional indexes. Next, we explore the approaches that leverage modern hardware features, such as Solid-State Drive, Non-Volatile Memory, Dynamic Random Access Memory, and Graphics Processing Unit, to improve the performance of multi-dimensional indexes in various aspects. Then, we investigate the novel work of multi-dimensional indexes that apply state-of-the-art machine learning techniques. Finally, we discuss the challenges and future research directions for multi-dimensional indexing methods.},
  archive      = {J_TKDE},
  author       = {Mingxin Li and Hancheng Wang and Haipeng Dai and Meng Li and Chengliang Chai and Rong Gu and Feng Chen and Zhiyuan Chen and Shuaituan Li and Qizhi Liu and Guihai Chen},
  doi          = {10.1109/TKDE.2024.3364183},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {8},
  number       = {8},
  pages        = {3635-3655},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey of multi-dimensional indexes: Past and future trends},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vertical federated learning: Concepts, advances, and
challenges. <em>TKDE</em>, <em>36</em>(7), 3615–3634. (<a
href="https://doi.org/10.1109/TKDE.2024.3352628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vertical Federated Learning (VFL) is a federated learning setting where multiple parties with different features about the same set of users jointly train machine learning models without exposing their raw data or model parameters. Motivated by the rapid growth in VFL research and real-world applications, we provide a comprehensive review of the concept and algorithms of VFL, as well as current advances and challenges in various aspects, including effectiveness, efficiency, and privacy. We provide an exhaustive categorization for VFL settings and privacy-preserving protocols and comprehensively analyze the privacy attacks and defense strategies for each protocol. In the end, we propose a unified framework, termed VFLow, which considers the VFL problem under communication, computation, privacy, as well as effectiveness and fairness constraints. Finally, we review the most recent advances in industrial applications, highlighting open challenges and future directions for VFL.},
  archive      = {J_TKDE},
  author       = {Yang Liu and Yan Kang and Tianyuan Zou and Yanhong Pu and Yuanqin He and Xiaozhou Ye and Ye Ouyang and Ya-Qin Zhang and Qiang Yang},
  doi          = {10.1109/TKDE.2024.3352628},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3615-3634},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Vertical federated learning: Concepts, advances, and challenges},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vertex encoding for edge nonexistence determination with
SIMD acceleration. <em>TKDE</em>, <em>36</em>(7), 3600–3614. (<a
href="https://doi.org/10.1109/TKDE.2024.3350919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose to design vertex encoding for determinations of no-result edge queries that should not be executed. Edge query is one of the core operations in mainstream graph databases, which is to retrieve edges connecting two given vertices. Real-world graphs may be too large to be stored in memory and frequently accessing edge data on disk usually incurs much overhead. The average degree of real-world graph tends to be much less than the vertex number, and edges may not exist in most pairs of vertices. Efficiently avoiding no-result edge query executions will certainly improve the performance of graph database. In this article, we propose a new and important problem for determining no-result edge queries: vertex encoding for edge nonexistence determination (VEND, for short). We build a low dimensional vertex encoding for all vertices, and we can efficiently determine most vertex pairs that are connected by no edges just with their corresponding codes. The encoding can be efficiently adjusted when data updates happen. With VEND, we can utilize in-memory efficient operations to filter no-result disk accesses for edge query. We also design SIMD-oriented compression optimizations to further improve performance. Extensive experiments on real-world datasets confirm the effectiveness of our solution.},
  archive      = {J_TKDE},
  author       = {Hangyu Zheng and Youhuan Li and Fang Xiong and Xiaosen Li and Lei Zou and Peifan Shi and Zheng Qin},
  doi          = {10.1109/TKDE.2024.3350919},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3600-3614},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Vertex encoding for edge nonexistence determination with SIMD acceleration},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unifying large language models and knowledge graphs: A
roadmap. <em>TKDE</em>, <em>36</em>(7), 3580–3599. (<a
href="https://doi.org/10.1109/TKDE.2024.3352100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia, and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolve by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and, simultaneously, leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely: 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs , in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.},
  archive      = {J_TKDE},
  author       = {Shirui Pan and Linhao Luo and Yufei Wang and Chen Chen and Jiapu Wang and Xindong Wu},
  doi          = {10.1109/TKDE.2024.3352100},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3580-3599},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Unifying large language models and knowledge graphs: A roadmap},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Towards very deep representation learning for subspace
clustering. <em>TKDE</em>, <em>36</em>(7), 3568–3579. (<a
href="https://doi.org/10.1109/TKDE.2024.3362984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep subspace clustering based on the self-expressive layer has attracted increasing attention in recent years. Due to the self-expressive layer, these methods need to load the whole dataset into one batch for learning the self-expressive coefficients. Such a learning strategy puts a great burden on memory, which severely prevents from the usage of deeper network architectures (e.g., ResNet), and becomes a bottleneck for applying to large-scale data. In this paper, we propose a new deep subspace clustering framework, in order to address the above challenges. In contrast to previous approaches taking the weights of a fully connected layer as the self-expressive coefficients, we attempt to obtain the self-expressive coefficients by learning an energy based network in a mini-batch training manner. By this means, it is no longer necessary to load all data into one batch for learning, thus avoiding the above issue. Considering the powerful representation ability of the recently popular self-supervised learning, we leverage self-supervised representation learning to learn the dictionary for representing data. Finally, we propose a joint framework to learn both the self-expressive coefficients and the dictionary simultaneously. Extensive experiments on three publicly available datasets demonstrate the effectiveness of our method.},
  archive      = {J_TKDE},
  author       = {Yanming Li and Shiye Wang and Changsheng Li and Ye Yuan and Guoren Wang},
  doi          = {10.1109/TKDE.2024.3362984},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3568-3579},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards very deep representation learning for subspace clustering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards hierarchical intent disentanglement for bundle
recommendation. <em>TKDE</em>, <em>36</em>(7), 3556–3567. (<a
href="https://doi.org/10.1109/TKDE.2023.3329175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bundle recommendation aims to recommend a bundle of items for the user to purchase together, for which two scenarios (i.e., Next-bundle recommendation and Within-bundle recommendation) are explored to recommend a specific bundle of items for the user and a specific item to fill the user&#39;s current bundle, respectively. Previous works largely model the user&#39;s preference with a uniform intent, without considering the diversity of intents when adopting the items within the bundle. In the real scenario of bundle recommendation, user intents modeling actually needs to be considered from three hierarchical levels, for that: a user&#39;s intents may be naturally distributed in different bundles (user level), one bundle may contain multiple intents of a user (bundle level), and an item in different bundles may also present different user intents (item level). To this end, we develop a novel model, H ierarchical I ntent D isentangle G raph N etworks (HIDGN) for bundle recommendation. HIDGN is capable of capturing the diversity of the user&#39;s intent precisely and comprehensively from the hierarchical structure with an cross-task intent contrastive learning, which is unified with the supervised next-/within-bundle recommendation sub-tasks as a multi-task framework. Extensive experiments on three benchmark datasets demonstrate that HIDGN outperforms the state-of-the-art methods by 43.0 $\%$ , 13.2 $\%$ , and 73.3 $\%$ , respectively.},
  archive      = {J_TKDE},
  author       = {Ding Zou and Sen Zhao and Wei Wei and Xian-ling Mao and Ruixuan Li and Dangyang Chen and Rui Fang and Yuanyuan Fu},
  doi          = {10.1109/TKDE.2023.3329175},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3556-3567},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards hierarchical intent disentanglement for bundle recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal social graph network hashing for efficient
recommendation. <em>TKDE</em>, <em>36</em>(7), 3541–3555. (<a
href="https://doi.org/10.1109/TKDE.2024.3352255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing-based recommender systems that represent users and items as binary hash codes are recently proposed to significantly improve time and space efficiency. However, the highly developed social media presents two major challenges to hashing-based recommendation algorithms. First, the boundary between information producers and consumers becomes blurred, resulting in the rapid emergence of massive online content. Meanwhile, users’ limited information consumption capacity inevitably causes further interaction sparsity. The inherent high sparsity of data leads to insufficient hash learning. Second, a considerable amount of online content becomes fast-moving consumer goods, such as short videos and news commentary, causing frequent changes in user interests and item popularity. To address the above problems, we propose a Temporal Social Graph Network Hashing (TSGNH) method for efficient recommendation, which generates binary hash codes of users and items through dynamic-adaptive aggregation on a constructed temporal social graph network. Specifically, we build a temporal social graph network to fully capture the social information widely existing in practical recommendation scenarios and propose a dynamic-adaptive aggregation method to capture long-term and short-term characters of users and items. Furthermore, different from the discrete optimization approaches used by existing hashing-based recommendation methods, we devise an end-to-end hashing learning approach that incorporates balanced and de-correlated constraints to learn compact and informative binary hash codes tailored for recommendation scenarios. Extensive experiments on three widely evaluated recommendation datasets demonstrate the superiority of the proposed method.},
  archive      = {J_TKDE},
  author       = {Yang Xu and Lei Zhu and Jingjing Li and Fengling Li and Heng Tao Shen},
  doi          = {10.1109/TKDE.2024.3352255},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3541-3555},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Temporal social graph network hashing for efficient recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TagRec++: Hierarchical label aware attention network for
question categorization. <em>TKDE</em>, <em>36</em>(7), 3529–3540. (<a
href="https://doi.org/10.1109/TKDE.2024.3354504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online learning systems have multiple data repositories in the form of transcripts, books and questions. To enable ease of access, such systems organize the content according to a well defined taxonomy of hierarchical nature (subject - chapter -topic). The task of categorizing inputs to the hierarchical labels is usually cast as a flat multi-class classification problem. Such approaches ignore the semantic relatedness between the terms in the input and the tokens in the hierarchical labels. Alternate approaches also suffer from class imbalance when they only consider leaf level nodes as labels. To tackle the issues, we formulate the task as a dense retrieval problem to retrieve the appropriate hierarchical labels for each content. In this paper, we deal with categorizing questions and learning content. We model the hierarchical labels as a composition of their tokens and use an efficient cross-attention mechanism to fuse the information with the term representations of the content. We also adopt an adaptive in-batch hard negative sampling approach which samples better negatives as the training progresses. We demonstrate that the proposed approach TagRec++ outperforms existing state-of-the-art approaches on question and learning content datasets as measured by Recall@k. In addition, we demonstrate zero-shot capabilities of TagRec++ and preliminary analysis of it&#39;s ability to adapt to label changes.},
  archive      = {J_TKDE},
  author       = {Venktesh V and Mukesh Mohania and Vikram Goyal},
  doi          = {10.1109/TKDE.2024.3354504},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3529-3540},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {TagRec++: Hierarchical label aware attention network for question categorization},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structures aware fine-grained contrastive adversarial
hashing for cross-media retrieval. <em>TKDE</em>, <em>36</em>(7),
3514–3528. (<a href="https://doi.org/10.1109/TKDE.2024.3356258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep cross-media hashing provides an efficient semantic representation learning solution for large-scale cross-media retrieval. The existing methods only consider the inter-media or intra-media semantic association learning, ignore the guiding of semantic structure information, and have weak reasoning ability for implicit fine-grained semantic associations. To tackle this problem, we propose a novel structures aware fine-grained contrastive adversarial hashing method for cross-media retrieval. A novel cross-media contrastive adversarial hash network is constructed for the first time, which integrates the cross-media and intra-media contrastive learning and multi-modal adversarial learning, aiming at maximizing the semantic association between different modalities, and improving the semantic discrimination and consistency of cross-media unified hash representation, thereby the inter-media and intra-media semantic preserving ability can be well enhanced; A fine-grained cross-media semantic feature learning method based on fine-grained semantic reasoning with transformers is proposed, which captures fine-grained salient features of different modalities for semantic association learning, and enhances the reasoning ability of fine-grained implicit semantic association; A semantic label graph convolutional network guided cross-media semantic association learning strategy is proposed, which makes full use of semantic structure information to enhance the learning ability of implicit cross-media semantic associations. Extensive experiments on several large-scale cross-media benchmark datasets demonstrate that the proposed method outperforms the state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Meiyu Liang and Yawen Li and Yang Yu and Xiaowen Cao and Zhe Xue and Ang Li and Kangkang Lu},
  doi          = {10.1109/TKDE.2024.3356258},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3514-3528},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Structures aware fine-grained contrastive adversarial hashing for cross-media retrieval},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Stacked network to realize spectral clustering with
adaptive graph learning. <em>TKDE</em>, <em>36</em>(7), 3501–3513. (<a
href="https://doi.org/10.1109/TKDE.2023.3327043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering with graph learning usually performs eigen-decomposition on the adaptive graph to obtain embedded representation for clustering. In terms of adaptive graph learning, the embedded representation is usually treated as the principal component of the graph to help improve graph structure. However, most adaptive graph learning methods only use a single graph layer. Therefore, the extraction power of embedded representation is restricted to single graph layer and insufficient to explore the intrinsic information. To break through this limitation, this article proposes a stacked network to realize spectral clustering with adaptive graph learning (SCnet-AGL). Specifically, the network allows the development of latent embedded representation underlying the multiple graph layers to reveal the intrinsic information. Meanwhile, we have designed an adaptive graph learning scheme to exploit the latent embedded representation for graph learning. With the advantage of the network, an augmented graph is obtained by incorporating the representation information for graph learning layer by layer. Finally, an efficient algorithm with feedback training scheme is proposed for network training. Experiments on real datasets demonstrate the effectiveness of the proposed network, and show that it is feasible to develop latent embedded representation to improve clustering performance.},
  archive      = {J_TKDE},
  author       = {Junyu Li and Fei Qi and Haoliang Yuan and Cheng Zhong and Hongmin Cai},
  doi          = {10.1109/TKDE.2023.3327043},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3501-3513},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Stacked network to realize spectral clustering with adaptive graph learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SNMCF: A scalable non-negative matrix co-factorization for
student cognitive modeling. <em>TKDE</em>, <em>36</em>(7), 3486–3500.
(<a href="https://doi.org/10.1109/TKDE.2023.3328730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Student cognitive modeling plays an important role in the rapid development of educational data mining research. It aims to discover students’ proficiency in knowledge concepts as well as to predict students’ performance in conducting exercises. Studies in the past few years have been mainly centered around two types of techniques: cognitive diagnosis models and data mining approaches. Cognitive diagnosis models focus on students’ cognitive states and assess their knowledge concept proficiency through handcrafted features. The subjective features may trigger cascading errors in the students’ performance prediction. On the other hand, data mining techniques, e.g., matrix factorization methods, achieve high prediction accuracy by directly modeling the students’ exercising process. It lacks measuring the students’ knowledge concept proficiency. To address the dilemma of the aforementioned methods, in this paper, we propose a scalable non-negative matrix co-factorization (SNMCF) model by jointly modeling the students’ knowledge states and their exercising process. SNMCF can achieve high accuracy in predicting students’ exercise performance while modeling their states of knowledge concepts in a given domain. We conduct extensive experiments on several real-world datasets, including large sparse ones, and demonstrate the effectiveness of our new approach in terms of prediction accuracy, cognitive diagnostic ability, and scalability.},
  archive      = {J_TKDE},
  author       = {Shenbao Yu and Yifeng Zeng and Yinghui Pan and Fan Yang},
  doi          = {10.1109/TKDE.2023.3328730},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3486-3500},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SNMCF: A scalable non-negative matrix co-factorization for student cognitive modeling},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RustGraph: Robust anomaly detection in dynamic graphs by
jointly learning structural-temporal dependency. <em>TKDE</em>,
<em>36</em>(7), 3472–3485. (<a
href="https://doi.org/10.1109/TKDE.2023.3328645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic graph-based data are ubiquitous in the real world, such as social networks, finance systems, and traffic flow. Fast and accurately detecting anomalies in these dynamic graphs is of vital importance. However, despite promising results the current anomaly detection methods have achieved, there are two major limitations when coping with dynamic graphs. The first limitation is that the topological structures and the temporal dynamics have been modeled separately, resulting in less expressive features for detection. The second limitation is that the models have been trained by unreliable noisy labels generated by random negative sampling, rendering it severely vulnerable to subtle perturbations. To overcome the above limitations, we propose RustGraph, a robust anomaly detection framework by jointly learning structural-temporal dependency in dynamic graphs. To this end, we design a variational graph auto-encoder with informative prior that simultaneously encodes both graph structural and temporal information. Then we introduce a fine-grained contrastive learning method to learn better node representations by utilizing the temporal consistency between two snapshots. Furthermore, we formulate the noisy label learning problem for anomaly detection in dynamic graph, and then propose a robust anomaly detector to improve the model performance by leveraging learned graph structure signal. Our extensive experiments on six real-world datasets demonstrate the proposed RustGraph method achieves state-of-the-art performance with an average of 3.64% improvement on AUC-ROC metric compared with all baselines. The codes are available at https://github.com/aubreygjh/RustGraph .},
  archive      = {J_TKDE},
  author       = {Jianhao Guo and Siliang Tang and Juncheng Li and Kaihang Pan and Lingfei Wu},
  doi          = {10.1109/TKDE.2023.3328645},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3472-3485},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {RustGraph: Robust anomaly detection in dynamic graphs by jointly learning structural-temporal dependency},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Row-sparse principal component analysis via coordinate
descent method. <em>TKDE</em>, <em>36</em>(7), 3460–3471. (<a
href="https://doi.org/10.1109/TKDE.2024.3351851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel algorithm to solve the row-sparse principal component analysis problem without relying on any data structure assumption. Sparse principal component analysis was proposed to improve the interpretability of principal component analysis by restricting the number of non-zero elements in each loading vector, but the varying sparsity patterns among different leading vectors may result in trouble on some occasions, such as feature selection. Then row-sparse principal component analysis was proposed, which demands the same sparse pattern among different loading vectors. However, the optimization of row-sparse principal component analysis problems is NP-hard. Although some algorithms were proposed to solve this problem, but they are only applicable to the specific data structure. In this paper, we transform the original row-sparse principal component analysis problem into a new equivalent problem that can be solved by coordinate descent method without relying on any data structure assumption. Then by carefully eliminating redundant structures to avoid repeating computation, we propose a more efficient coordinate descent method to solve this problem. Furthermore, no parameter needs to be tuned in our algorithm. Finally, extensive experiments are conducted on the real world data sets to demonstrate the superiority of our algorithm.},
  archive      = {J_TKDE},
  author       = {Feiping Nie and Qiang Chen and Weizhong Yu and Xuelong Li},
  doi          = {10.1109/TKDE.2024.3351851},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3460-3471},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Row-sparse principal component analysis via coordinate descent method},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Reinforced path reasoning for counterfactual explainable
recommendation. <em>TKDE</em>, <em>36</em>(7), 3443–3459. (<a
href="https://doi.org/10.1109/TKDE.2024.3354077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Counterfactual explanations interpret the recommendation mechanism by exploring how minimal alterations on items or users affect recommendation decisions. Existing counterfactual explainable approaches face huge search space, and their explanations are either action-based (e.g., user click) or aspect-based (i.e., item description). We believe item attribute-based explanations are more intuitive and persuadable for users since they explain by fine-grained demographic features, e.g., brand. Moreover, counterfactual explanations could enhance recommendations by filtering out negative items. In this work, we propose a novel Counterfactual Explainable Recommendation (CERec) to generate item attribute-based counterfactual explanations meanwhile to boost recommendation performance. Our CERec optimizes an explanation policy upon uniformly searching candidate counterfactuals within a reinforcement learning environment. We reduce the huge search space with an adaptive path sampler by using rich context information of a given knowledge graph. We also deploy the explanation policy to a recommendation model to enhance the recommendation. Extensive explainability and recommendation evaluations demonstrate CERec &#39;s ability to provide explanations consistent with user preferences and maintain improved recommendations.},
  archive      = {J_TKDE},
  author       = {Xiangmeng Wang and Qian Li and Dianer Yu and Qing Li and Guandong Xu},
  doi          = {10.1109/TKDE.2024.3354077},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3443-3459},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Reinforced path reasoning for counterfactual explainable recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RDGT: Enhancing group cognitive diagnosis with
relation-guided dual-side graph transformer. <em>TKDE</em>,
<em>36</em>(7), 3429–3442. (<a
href="https://doi.org/10.1109/TKDE.2024.3352640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive diagnosis has been widely recognized as a crucial task in the field of computational education, which is capable of learning the knowledge profiles of students and predicting their future exercise performance. Indeed, considerable research efforts have been made in this direction over the past decades. However, most of the existing studies only focus on individual-level diagnostic modeling, while the group-level cognitive diagnosis still lacks an in-depth exploration, which is more compatible with realistic collaborative learning environments. To this end, in this paper, we propose a R elation-guided D ual-side G raph T ransformer (RDGT) model for achieving effective group-level cognitive diagnosis. Specifically, we first construct the dual-side relation graphs (i.e., student-side and exercise-side) from the group-student-exercise heterogeneous interaction data for explicitly modeling associations between students and exercises, respectively. In particular, the edge weight between two nodes is defined based on the similarity of corresponding student-exercise interactions. Then, we introduce two relation-guided graph transformers to learn the representations of students and exercises by integrating the whole graph information, including both nodes and edge weights. Meanwhile, the inter-group information has been incorporated into the student-side relation graph to further enhance the representations of students. Along this line, we design a cognitive diagnosis module for learning the groups’ proficiency in specific knowledge concepts, which includes an attention-based aggregation strategy to obtain the final group representation and a hybrid loss for optimizing the performance prediction of both group and student. Finally, extensive experiments on 5 real-world datasets clearly demonstrate the effectiveness of our model as well as some interesting findings (e.g., the representative groups and potential collaborations among students).},
  archive      = {J_TKDE},
  author       = {Xiaoshan Yu and Chuan Qin and Dazhong Shen and Haiping Ma and Le Zhang and Xingyi Zhang and Hengshu Zhu and Hui Xiong},
  doi          = {10.1109/TKDE.2024.3352640},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3429-3442},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {RDGT: Enhancing group cognitive diagnosis with relation-guided dual-side graph transformer},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ProjPert: Projection-based perturbation for label protection
in split learning based vertical federated learning. <em>TKDE</em>,
<em>36</em>(7), 3417–3428. (<a
href="https://doi.org/10.1109/TKDE.2024.3349863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the paradigms under which split learning (SL) is used is for the vertical federated learning (VFL) setting, where two or more parties build models over feature-partitioned data. However, to protect the private labels of one party, random noises are needed to perturb the backward derivatives (i.e., gradients w.r.t. forward activations), which incurs the privacy-utility tradeoff. In this work, we introduce ProjPert , a novel algorithm that searches for the optimal “perturbation knobs” for label protection in SL-based VFL. We formulate the problem of perturbation searching as how to minimize the impact on model quality given the desired privacy guarantee. Based on the problem, two solutions are introduced, where the first obtains the optimal perturbation via a simple but effective binary searching scheme, and the second heuristically approximates the optimality within a negligible error bound. Empirical results demonstrate that both our solutions are more effective in protecting the labels and achieve significantly better privacy-utility tradeoffs than state-of-the-art perturbation-based label protection methods. Furthermore, our heuristic solution is very efficient and incurs almost zero extra overhead in the overall running time, improving the usability in real-world applications.},
  archive      = {J_TKDE},
  author       = {Fangcheng Fu and Xuanyu Wang and Jiawei Jiang and Huanran Xue and Bui Cui},
  doi          = {10.1109/TKDE.2024.3349863},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3417-3428},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {ProjPert: Projection-based perturbation for label protection in split learning based vertical federated learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Position matters: Play a sequential game to detect
significant communities. <em>TKDE</em>, <em>36</em>(7), 3402–3416. (<a
href="https://doi.org/10.1109/TKDE.2023.3323567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting significant communities via an algorithmic game-theoretic model has recently shown great promise, which seeks to formulate community detection as a competitive game, enabling us to study the network&#39;s potential structure with a systematic tool. However, fully leveraging its potential to uncover the mechanism behind community formation remains a challenge. Here we propose SCG —a Sequential Community Game model to track and characterize the network&#39;s structural property. Unlike conventional formulations where individual nodes are treated as players, our model considers communities as players who strive to maximize their structural utility by strategically selecting member nodes. By prioritizing significant communities sequentially, SCG enables differentiation between uncovered communities. Importantly, we establish the existence of a strict Nash equilibrium in SCG , suggesting its ability to capture a stable community structure. We run extensive experiments on several synthetic and real-world networks to test SCG &#39;s performance. Results show that SCG can help us well track the network&#39;s structural properties and also give us reliable performance compared to related baselines.},
  archive      = {J_TKDE},
  author       = {Yuyao Wang and Jie Cao and Youquan Wang and Jia Wu and Yangyang Liu},
  doi          = {10.1109/TKDE.2023.3323567},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3402-3416},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Position matters: Play a sequential game to detect significant communities},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Popularity balanced multi-task bundling for mobile crowd
sensing. <em>TKDE</em>, <em>36</em>(7), 3390–3401. (<a
href="https://doi.org/10.1109/TKDE.2023.3348796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Crowd Sensing (MCS) is a data collection technology in which workers finish tasks and get payment. In MCS, some tasks are not preferred workers due to their remote locations or cheap prices, which leads to a huge proportion of unpopular tasks. Although increasing tasks payment is an effective to increase task popularity, however, it may decrease platform utility. In this work, we introduce bundling into MCS to solve this problem. Specially, a Task Bundling Reorganization Mechanism (TBRM) is proposed. In TBRM, unpopular tasks are properly bundled with popular tasks to maximize the minimum of both the number of task completions and expected profit. The TBRM is separated into two phases: the area selection phase and the rule selection phase. First, the randomly generated solution is input into the area selection phase, which selects the portion of the bundle that needs to be reorganized; then, the results of the area selection phase is regarded as input of the rule selection phase, which selects the appropriate task to reorganize; finally, the TBRM repeats this process until convergence. Experimental results demonstrate the effectiveness of the TBRM mechanism.},
  archive      = {J_TKDE},
  author       = {Yan Zhen and Yunfei Wang and Peng He and Yaping Cui and Ruyan Wang and Dapeng Wu},
  doi          = {10.1109/TKDE.2023.3348796},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3390-3401},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Popularity balanced multi-task bundling for mobile crowd sensing},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Personalized prompt for sequential recommendation.
<em>TKDE</em>, <em>36</em>(7), 3376–3389. (<a
href="https://doi.org/10.1109/TKDE.2024.3357498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-training models have shown their power in sequential recommendation. Recently, prompt has been widely explored and verified for tuning after pre-training in NLP, which helps to more effectively and parameter-efficiently extract useful knowledge from pre-training models for downstream tasks, especially in cold-start scenarios. However, it is challenging to bring prompt-tuning from NLP to recommendation, since the tokens of recommendation (i.e., items) are million-level and do not have concrete explainable semantics, and the sequence modeling in recommendation should be personalized. In this work, we first introduce prompt to recommendation models and propose a novel Personalized prompt-based recommendation (PPR) framework for cold-start recommendation. Specifically, we build personalized soft prompt via a prompt generator based on user profiles, and enable a sufficient training on prompts via a new prompt-oriented contrastive learning. PPR is effective, parameter-efficient, and universal in various tasks. In both few-shot and zero-shot recommendation tasks, PPR models achieve significant improvements over baselines in three large-scale datasets. We also verify PPR&#39;s universality in adopting different recommendation models as the backbone. Finally, we explore and confirm the capability of PPR on other tasks such as cross-domain recommendation and user profile prediction, shedding lights on the promising future directions of better using large-scale pre-trained recommendation models.},
  archive      = {J_TKDE},
  author       = {Yiqing Wu and Ruobing Xie and Yongchun Zhu and Fuzhen Zhuang and Xu Zhang and Leyu Lin and Qing He},
  doi          = {10.1109/TKDE.2024.3357498},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3376-3389},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Personalized prompt for sequential recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Personalized elastic embedding learning for on-device
recommendation. <em>TKDE</em>, <em>36</em>(7), 3363–3375. (<a
href="https://doi.org/10.1109/TKDE.2024.3361562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address privacy concerns and reduce network latency, there has been a recent trend of compressing cumbersome recommendation models trained on the cloud and deploying compact recommender models to resource-limited devices for the real-time recommendation. Existing solutions generally overlook device heterogeneity and user heterogeneity. They require devices with the same budget to share the same model and assume the available device resources (e.g., memory) are constant, which is not reflective of reality. Considering device and user heterogeneities as well as dynamic resource constraints, this article proposes a Personalized Elastic Embedding Learning framework (PEEL) for the on-device recommendation, which generates Personalized Elastic Embeddings (PEEs) for devices with various memory budgets in a once-for-all manner, adapting to new or dynamic budgets, and addressing user preference diversity by assigning personalized embeddings for different groups of users. Specifically, it pretrains a global embedding table with collected user-item interaction instances and clusters users into groups. Then, it refines the embedding tables with local interaction instances within each group. PEEs are generated from the group-wise embedding blocks and their weights that indicate the contribution of each embedding block to the local recommendation performance. Given a memory budget, PEEL efficiently generates PEEs by selecting embedding blocks with the largest weights, making it adaptable to dynamic memory budgets on devices. Furthermore, a diversity-driven regularizer is implemented to encourage the expressiveness of embedding blocks, and a controller is utilized to optimize the weights. Extensive experiments are conducted on two public datasets, and the results show that PEEL yields superior performance on devices with heterogeneous and dynamic memory budgets.},
  archive      = {J_TKDE},
  author       = {Ruiqi Zheng and Liang Qu and Tong Chen and Kai Zheng and Yuhui Shi and Hongzhi Yin},
  doi          = {10.1109/TKDE.2024.3361562},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3363-3375},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Personalized elastic embedding learning for on-device recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing long-term efficiency and fairness in ride-hailing
under budget constraint via joint order dispatching and driver
repositioning. <em>TKDE</em>, <em>36</em>(7), 3348–3362. (<a
href="https://doi.org/10.1109/TKDE.2023.3348491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ride-hailing platforms (e.g., Uber and Didi Chuxing) have become increasingly popular in recent years. Efficiency has always been an important metric for such platforms. However, only focusing on efficiency inevitably ignores the fairness of driver incomes, which could impair the sustainability of ride-hailing systems. To optimize such two essential objectives, order dispatching and driver repositioning play an important role, as they impact not only the immediate, but also the future order-serving outcomes of drivers. In practice, the platform offers monetary incentives to drivers for completing the repositioning and has a budget for the repositioning cost. Therefore, in this paper, we aim to exploit joint order dispatching and driver repositioning to optimize both long-term efficiency and fairness in ride-hailing under the budget constraint. To this end, we propose JDRCL, a novel multi-agent reinforcement learning framework, which integrates a group-based action representation that copes with the variable action space, and a primal-dual iterative training algorithm to learn a constraint-satisfying policy that maximizes both the worst and the overall incomes of drivers. Furthermore, we prove the asymptotic convergence rate of our training algorithm. Extensive experiments based on three real-world ride-hailing order datasets show that JDRCL outperforms state-of-the-art baselines on both efficiency and fairness.},
  archive      = {J_TKDE},
  author       = {Jiahui Sun and Haiming Jin and Zhaoxing Yang and Lu Su},
  doi          = {10.1109/TKDE.2023.3348491},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3348-3362},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Optimizing long-term efficiency and fairness in ride-hailing under budget constraint via joint order dispatching and driver repositioning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On efficiently processing MIT queries in trajectory data.
<em>TKDE</em>, <em>36</em>(7), 3329–3347. (<a
href="https://doi.org/10.1109/TKDE.2024.3361948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maximizing Influence (Max-Inf) query is a fundamental operation in spatial data management. Given a set of weighted objects, this query aims to find an optimal location from a candidate set to maximize its influence , which is the total weight of its reverse nearest neighbors. Existing work commonly assumes that every object is in a fixed location. In real life, however, there are a wide variety of drive-in services (e.g., food joints, pharmacies, ATMs, etc.) that are widely accessed by mobile users (i.e., trajectories) instead of the fixed ones. In this paper, we first define the Maximizing Influence query over Trajectories, namely, MIT query, which aims to find an optimal location to maximize the total weight of influenced trajectories. We propose a novel index, QB-tree to hierarchically group trajectories with similar activity regions together for subsequent unified processing, and classify trajectories inside the same node into multiple buckets according to their motion patterns. For each bucket, we construct a rectilinear polygon using the trajectories in it to exclude some irrelevant areas in the minimum boundary rectangle. Moreover, we develop a branch-and-bound approach called BBM to efficiently solve the MIT query. The algorithm adaptively partitions the candidates into disjoint regions and prunes the regions without containing optimal results. Then, by exploiting the QB-tree, the upper and lower bounds are efficiently computed with three-level pruning technique. Practically, we also study a variant of the MIT query, called MDT query. We propose novel pruning bounds in cooperation with QB-tree to answer MDT queries efficiently. Finally, extensive experiments on real and synthetic datasets demonstrate that our index and algorithms have high performance in terms of efficiency, scalability, and genericity.},
  archive      = {J_TKDE},
  author       = {Jian Chen and Hong Gao and Kaiqi Zhang and Jiachi Wang and Yubo Luo and Zhenqing Wu and Jianzhong Li},
  doi          = {10.1109/TKDE.2024.3361948},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3329-3347},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {On efficiently processing MIT queries in trajectory data},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On data distribution leakage in cross-silo federated
learning. <em>TKDE</em>, <em>36</em>(7), 3312–3328. (<a
href="https://doi.org/10.1109/TKDE.2023.3349323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has emerged as a promising privacy-preserving machine learning paradigm, enabling data owners to collaboratively train a joint model by sharing model parameters instead of private training data. However, recent studies reveal the privacy risks in FL by inferring private training data from model parameters. Therefore, differential privacy (DP) is incorporated into FL to safeguard training data. Nevertheless, DP does not provide a strong theoretical guarantee for protecting data distribution, which is also highly sensitive in the cross-silo FL scenarios as it may reflect the business secrets of data owners. In this article, we develop two attack methods to investigate the potential risks of data distribution leakage in differentially private cross-silo FL. We highlight that an honest-but-curious server can successfully infer both the feature and label distributions of each party’s training data without any background knowledge. Specifically, the first attack applies when models are differentiable, while the second attack caters to non-differentiable classification models. Extensive experiments on six benchmark datasets validate the effectiveness of the proposed attacks. The results demonstrate that the state-of-the-art DP-SGD algorithm is still vulnerable to the inference attack on data distribution, emphasizing the necessity of designing more advanced privacy-preserving FL frameworks.},
  archive      = {J_TKDE},
  author       = {Yangfan Jiang and Xinjian Luo and Yuncheng Wu and Xiaochen Zhu and Xiaokui Xiao and Beng Chin Ooi},
  doi          = {10.1109/TKDE.2023.3349323},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3312-3328},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {On data distribution leakage in cross-silo federated learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MPV: Enabling fine-grained query authentication in
hybrid-storage blockchain. <em>TKDE</em>, <em>36</em>(7), 3297–3311. (<a
href="https://doi.org/10.1109/TKDE.2024.3359173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the large-scale data streams produced by distributed terminals, hybrid-storage blockchain (HSB) that combines on-chain and off-chain storages has emerged as a promising solution for secure data storage in decentralized applications. Because all the raw data is outsourced to an untrusted service provider (SP), existing solutions suggest to utilize an on-chain authenticated data structure (ADS) to verify query results retrieved off-chain. However, existing solutions support only coarse-grained authentication making a user abandon all the query results once the validation fails. In this article, we focus on realizing fine-grained authentication for range queries, enabling a user to distinguish authentic data from falsified results. Considering the heavy gas consumption of on-chain storage, we propose two multi-dimensional parity-based verification (MPV) schemes with a trade-off between off-chain and on-chain efficiencies. Our main idea is to design an accumulator-based ADS to summarize well-designed verifiable hypercubes, so that fake results can be quickly located by combining multi-dimensional faces failed validation. Compared with previous solutions, our MPV schemes allow a user to make efficient use of query results by filtering out errors, and thus have higher data utility. The detailed security analysis and extensive experiments demonstrate the security and effectiveness of our MPV schemes, respectively.},
  archive      = {J_TKDE},
  author       = {Qin Liu and Yu Peng and Mingzuo Xu and Hongbo Jiang and Jie Wu and Tian Wang and Tao Peng and Guojun Wang},
  doi          = {10.1109/TKDE.2024.3359173},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3297-3311},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MPV: Enabling fine-grained query authentication in hybrid-storage blockchain},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MGDCF: Distance learning via markov graph diffusion for
neural collaborative filtering. <em>TKDE</em>, <em>36</em>(7),
3281–3296. (<a href="https://doi.org/10.1109/TKDE.2023.3348537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have recently been utilized to build Collaborative Filtering (CF) models to predict user preferences based on historical user-item interactions. However, there is relatively little understanding of how GNN-based CF models relate to some traditional Network Representation Learning (NRL) approaches. In this paper, we show the equivalence between some state-of-the-art GNN-based CF models and a traditional 1-layer NRL model based on context encoding. Based on a Markov process that trades off two types of distances, we present Markov Graph Diffusion Collaborative Filtering (MGDCF) to generalize some state-of-the-art GNN-based CF models. Instead of considering the GNN as a trainable black box that propagates learnable user/item vertex embeddings, we treat GNNs as an untrainable Markov process that can construct constant context features of vertices for a traditional NRL model that encodes context features with a fully-connected layer. Such simplification can help us to better understand how GNNs benefit CF models. Especially, it helps us realize that ranking losses play crucial roles in GNN-based CF tasks. With our proposed simple yet powerful ranking loss InfoBPR, the NRL model can still perform well without the context features constructed by GNNs. We conduct experiments to perform detailed analysis on MGDCF.},
  archive      = {J_TKDE},
  author       = {Jun Hu and Bryan Hooi and Shengsheng Qian and Quan Fang and Changsheng Xu},
  doi          = {10.1109/TKDE.2023.3348537},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3281-3296},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MGDCF: Distance learning via markov graph diffusion for neural collaborative filtering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LSADEN: Local spatial-aware community detection in evolving
geo-social networks. <em>TKDE</em>, <em>36</em>(7), 3265–3280. (<a
href="https://doi.org/10.1109/TKDE.2023.3348975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification of the local community structure in geo-social networks has been gaining increasing attention. The structure of geo-social networks evolves over time with the addition/deletion of edges/nodes and the update of node locations, which has motivated recent studies to mine local communities in dynamic geo-social networks. Mining communities in evolving geo-social networks is essential for understanding the evolution of group behaviors. However, in most previous studies on the community mining in dynamic networks, local spatial-aware communities were not identified in evolving geo-social networks. Therefore, in this study, the problem of determining local spatial-aware communities in evolving geo-social networks is proposed. To address this problem, we propose a parameter-free algorithm, called LSADEN. Specifically, LSADEN involves two main steps: i) selecting candidate nodes, where LSADEN defines the community dominance relation under dynamic environments to obtain candidate nodes that improve the community in terms of the community quality or the smoothness between communities at adjacent time stamps; ii) community expansion, where LSADEN designs the Manhattan distance of communities to add some candidate nodes to the local community. Experimental results on six real-world datasets and one synthetic dataset show that LSADEN performs well both in terms of the quality of communities and the smoothness between communities at adjacent time stamps.},
  archive      = {J_TKDE},
  author       = {Li Ni and Qiuyu Li and Yiwen Zhang and Wenjian Luo and Victor S. Sheng},
  doi          = {10.1109/TKDE.2023.3348975},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3265-3280},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {LSADEN: Local spatial-aware community detection in evolving geo-social networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Link prediction in stagewise graphs. <em>TKDE</em>,
<em>36</em>(7), 3252–3264. (<a
href="https://doi.org/10.1109/TKDE.2024.3351732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A stagewise graph has distinct edge types that represent the different stages. Two nodes can be connected by an edge of the current stage only if an edge of the preceding one is already connecting them. Stagewise graphs can represent many kinds of interactions. For example, a jobseeker-vacancy interaction can be labeled by the subsequent edge types click , apply , job interview , and, finally, hired . Also, biological and medical interactions, such as infection processes or administration of drugs, often occur in stages. In this work, we formalize link prediction problems on such graphs as ‘stagewise link prediction’. Though relevant and rapidly gaining attention, these types of problems are to date highly underexplored. We identify and address arising difficulties, such as competition in stagewise networks. We explore an activation function for stagewise modelling and an evaluation strategy that satisfies the stagewise constraints. We confirm our insights through a set of experiments on both well-chosen simulated data sets and real data related to job recommendation and synthetic biology.},
  archive      = {J_TKDE},
  author       = {Pieter Dewulf and Michiel Stock and Bernard De Baets},
  doi          = {10.1109/TKDE.2024.3351732},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3252-3264},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Link prediction in stagewise graphs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning hierarchical preferences for recommendation with
mixture intention neural stochastic processes. <em>TKDE</em>,
<em>36</em>(7), 3237–3251. (<a
href="https://doi.org/10.1109/TKDE.2023.3348493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User preferences behind users’ decision-making processes are highly diverse and may range from lower-level concepts with more specific intentions and higher-level concepts with more general intentions. In this case, user preferences tend to be expressed hierarchically. However, learning such intentions with different levels from user behaviors is challenging, and remains largely neglected by the existing literature. Meanwhile, user behavior data tends to be sparse because of the limited user response and the vast combinations of users and items, which results in cold-start problems with unclear user intentions. In this paper, we propose a mixture intention neural stochastic process (MINSP), a new view of the stochastic processes family using a general meta-learning mechanism and mixture strategy for robust recommendation with hierarchical preferences modeling. By considering the recommendation process for each user as a stochastic process, MINSP defines distributions over functions and is capable of rapid adaptation to different users. To capture the user&#39;s intention on different levels, an iterative additive algorithm is proposed that minimizes the approximation error by backfitting the residuals of previous approximations. In this case, the induced tree intention hierarchies serve as an aggregated structured representation of the whole preference, summarizing the gist for convenient navigation and better generalization. Furthermore, we theoretically analyze the generalization error bound of the proposed MINSP to guarantee the model performance. Empirical results show that our approach can achieve substantial improvement over the state-of-the-art baselines in terms of recommendation performance, and obtain an interpretable hierarchical intention structure.},
  archive      = {J_TKDE},
  author       = {Huafeng Liu and Liping Jing and Jian Yu and Michael K. Ng},
  doi          = {10.1109/TKDE.2023.3348493},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3237-3251},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning hierarchical preferences for recommendation with mixture intention neural stochastic processes},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning graph ODE for continuous-time sequential
recommendation. <em>TKDE</em>, <em>36</em>(7), 3224–3236. (<a
href="https://doi.org/10.1109/TKDE.2024.3349397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommendation aims at understanding user preference by capturing successive behavior correlations, which are usually represented as the item purchasing sequences based on their past interactions. Existing efforts generally predict the next item via modeling the sequential patterns. Despite effectiveness, there exist two natural deficiencies: (i) user preference is dynamic in nature, and the evolution of collaborative signals is often ignored; and (ii) the observed interactions are often irregularly-sampled, while existing methods model item transitions assuming uniform intervals. Thus, how to effectively model and predict the underlying dynamics for user preference becomes a critical research problem. To tackle the above challenges, in this paper, we focus on continuous-time sequential recommendation and propose a principled graph ordinary differential equation framework named GDERec. Technically, GDERec is characterized by an autoregressive graph ordinary differential equation consisting of two components, which are parameterized by two tailored graph neural networks (GNNs) respectively to capture user preference from the perspective of hybrid dynamical systems. On the one hand, we introduce a novel ordinary differential equation based GNN to implicitly model the temporal evolution of the user-item interaction graph. On the other hand, an attention-based GNN is proposed to explicitly incorporate collaborative attention to interaction signals when the interaction graph evolves over time. The two customized GNNs are trained alternately in an autoregressive manner to track the evolution of the underlying system from irregular observations, and thus learn effective representations of users and items beneficial to the sequential recommendation. Extensive experiments on five benchmark datasets demonstrate the superiority of our model over various state-of-the-art recommendation methods.},
  archive      = {J_TKDE},
  author       = {Yifang Qin and Wei Ju and Hongjun Wu and Xiao Luo and Ming Zhang},
  doi          = {10.1109/TKDE.2024.3349397},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3224-3236},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning graph ODE for continuous-time sequential recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning a structural causal model for intuition reasoning
in conversation. <em>TKDE</em>, <em>36</em>(7), 3210–3223. (<a
href="https://doi.org/10.1109/TKDE.2024.3352575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reasoning, a crucial aspect of NLP research, has not been adequately addressed by prevailing models including Large Language Model. Conversation reasoning, as a critical component of it, remains largely unexplored due to the absence of a well-designed cognitive model. In this article, inspired by intuition theory on conversation cognition, we develop a conversation cognitive model (CCM) that explains how each utterance receives and activates channels of information recursively. Besides, we algebraically transformed CCM into a structural causal model (SCM) under some mild assumptions, rendering it compatible with various causal discovery methods. We further propose a probabilistic implementation of the SCM for utterance-level relation reasoning. By leveraging variational inference, it explores substitutes for implicit causes, addresses the issue of their unobservability, and reconstructs the causal representations of utterances through the evidence lower bounds. Moreover, we constructed synthetic and simulated datasets incorporating implicit causes and complete cause labels, alleviating the current situation where all available datasets are implicit-causes-agnostic. Extensive experiments demonstrate that our proposed method significantly outperforms existing methods on synthetic, simulated, and real-world datasets. Finally, we analyze the performance of CCM under latent confounders and propose theoretical ideas for addressing this currently unresolved issue.},
  archive      = {J_TKDE},
  author       = {Hang Chen and Bingyu Liao and Jing Luo and Wenjing Zhu and Xinyu Yang},
  doi          = {10.1109/TKDE.2024.3352575},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3210-3223},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning a structural causal model for intuition reasoning in conversation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LDPGuard: Defenses against data poisoning attacks to local
differential privacy protocols. <em>TKDE</em>, <em>36</em>(7),
3195–3209. (<a href="https://doi.org/10.1109/TKDE.2024.3358909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The protocols that satisfy Local Differential Privacy (LDP) enable untrusted third parties to collect aggregate information about a population without disclosing each user&#39;s privacy. In particular, each user locally encodes and perturbs his private data before sending it to the data collector, who aggregates and estimates the statistics about the population based on the collected perturbed values from individuals. Owing to their growing importance, LDP protocols have been widely studied and deployed in real-world scenarios (e.g., Chrome and Windows). However, as data poisoning attacks may be injected by attackers who introduce many fake users, the utility of the statistics is heavily poisoned. In this paper, we present a generic and extensible framework called LDPGuard to address the problem. LDPGuard provides effective defenses against data poisoning attacks to LDP protocols for frequency estimation, a basic query of most data analytics tasks. In particular, it first precisely estimates the percentage of fake users and then provides adversarial schemes to defend against particular data poisoning attacks. Experimental study on real-world and synthetic datasets demonstrates the superiority of LDPGuard compared to existing techniques.},
  archive      = {J_TKDE},
  author       = {Kai Huang and Gaoya Ouyang and Qingqing Ye and Haibo Hu and Bolong Zheng and Xi Zhao and Ruiyuan Zhang and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2024.3358909},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3195-3209},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {LDPGuard: Defenses against data poisoning attacks to local differential privacy protocols},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lauca: A workload duplicator for benchmarking transactional
database performance. <em>TKDE</em>, <em>36</em>(7), 3180–3194. (<a
href="https://doi.org/10.1109/TKDE.2024.3360116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating synthetic workloads is essential and critical to the performance evaluation of database systems. When benchmarking database performance for a specific application, the similarity between synthetic workloads and real application workloads determines the credibility of the evaluation results. However, it meets a great challenge to catch workload characteristics for a target online transaction processing (OLTP) application considering the complexity of transaction executions. To address this problem, we propose a work l o a d d u pli ca tor ( Lauca ) that can generate synthetic workloads with highly similar performance metrics compared to a specific application on both centralized and distributed databases. By carefully studying the application-driven workload generation problem, we present Transaction Logic , Data Access Distribution and Partition Access Distribution to characterize runtime workloads and propose novel generation algorithms to guarantee the high fidelity of synthetic workloads. To the best of our knowledge, Lauca is the first application-driven transactional workload generator. We conduct extensive experiments based on TPC-C, SmallBank and YCSB on both centralized and distributed databases. The experimental results show that Lauca consistently generates high-quality synthetic workloads.},
  archive      = {J_TKDE},
  author       = {Siyang Weng and Qingshuai Wang and Luyi Qu and Rong Zhang and Peng Cai and Weining Qian and Aoying Zhou},
  doi          = {10.1109/TKDE.2024.3360116},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3180-3194},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Lauca: A workload duplicator for benchmarking transactional database performance},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Label-free multivariate time series anomaly detection.
<em>TKDE</em>, <em>36</em>(7), 3166–3179. (<a
href="https://doi.org/10.1109/TKDE.2024.3349613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection in multivariate time series has been widely studied in one-class classification (OCC) setting. The training samples in this setting are assumed to be normal. In more practical situations, it is difficult to guarantee that all samples are normal. Meanwhile, preparing a completely clean training dataset is costly and laborious. Such a case may degrade the performance of OCC-based anomaly detection methods which fit the training distribution as the normal distribution. To overcome this limitation, in this paper, we propose MTGFlow, an unsupervised anomaly detection approach for M ultivariate T ime series anomaly detection via dynamic G raph and entity-aware normalizing Flow . MTGFlow first estimates the density of the entire training samples and then identifies anomalous instances based on the density of the test samples within the fitted distribution. This relies on a widely accepted assumption that anomalous instances exhibit more sparse densities than normal ones, with no reliance on the clean training dataset. However, it is intractable to directly estimate the density due to the complex dependencies among entities and their diverse inherent characteristics, not to mention detecting anomalies based on the estimated distribution. In order to address these problems, we utilize the graph structure learning model to learn interdependent and evolving relations among entities, which effectively captures the complex and accurate distribution patterns of multivariate time series. In addition, our approach incorporates the unique characteristics of individual entities by employing an entity-aware normalizing flow. This enables us to represent each entity as a parameterized normal distribution. Furthermore, considering that some entities present similar characteristics, we propose a cluster strategy that capitalizes on the commonalities of entities with similar characteristics, resulting in more precise and detailed density estimation. We refer to this cluster-aware extension as MTGFlow_cluster. Extensive experiments are conducted on six widely used benchmark datasets, in which MTGFlow and MTGFlow_cluster demonstrate their superior detection performance.},
  archive      = {J_TKDE},
  author       = {Qihang Zhou and Shibo He and Haoyu Liu and Jiming Chen and Wenchao Meng},
  doi          = {10.1109/TKDE.2024.3349613},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3166-3179},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Label-free multivariate time series anomaly detection},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid regret minimization: A submodular approach.
<em>TKDE</em>, <em>36</em>(7), 3151–3165. (<a
href="https://doi.org/10.1109/TKDE.2023.3328596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regret minimization queries are important methods to extract representative tuples from databases. They have been extensively investigated in the last decade due to wide applications in multi-criteria decision making. For a given database $D$ and a class $\mathcal {F}$ of utility functions (e.g., all nonnegative linear functions), two typical regret minimization queries considered in existing studies are maximum regret minimization (MRM) and average regret minimization (ARM) queries, whereby a subset of $k$ tuples is selected from $D$ to minimize the maximum or average of regret ratios among all utility functions in $\mathcal {F}$ , respectively. However, due to the different properties of maximum and average regret ratios, the result of one query cannot fulfill the requirement of the other. To the best of our knowledge, there has not yet been any attempt to combine both queries. In this paper, we first introduce the hybrid regret minimization (HRM) query, which simultaneously minimizes the maximum and average regret ratios. We show that finding the optimal result for an HRM query is NP-hard, but it is possible to exploit submodularity for approximate HRM query processing. We propose an efficient asymptotic approximation algorithm based on submodular maximization to process HRM queries and several optimization techniques, such as memoization, lazy evaluation, and stochastic subsampling, to improve query efficiency. Furthermore, we consider extending a multiplicative weights update (MWU) algorithm for multi-objective submodular maximization to provide higher-quality results for HRM queries. Finally, we demonstrate that our proposed algorithms achieve better performance for HRM queries than existing methods specific to MRM and ARM queries through extensive experiments on real-world and synthetic datasets. Meanwhile, our proposed algorithms are efficient and scalable to large datasets.},
  archive      = {J_TKDE},
  author       = {Jiping Zheng and Fanxu Meng and Yanhao Wang and Xiaoyang Wang and Sheng Wang and Yuan Ma and Zhiyang Hao},
  doi          = {10.1109/TKDE.2023.3328596},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3151-3165},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hybrid regret minimization: A submodular approach},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical context representation and self-adaptive
thresholding for multivariate anomaly detection. <em>TKDE</em>,
<em>36</em>(7), 3139–3150. (<a
href="https://doi.org/10.1109/TKDE.2024.3360640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection in multivariate time series is a critical research area, but it is also a challenging one due to its occurrence in various real-world scenarios, such as structural health monitoring and risk management. Traditional approaches for anomaly detection rely on deviating distribution and a static threshold that is set manually. However, static thresholds fail to detect contextual anomalies, leading to a high ratio of false anomalies. Therefore, a self-adaptive thresholding method is required to improve the accuracy of anomaly detection. In this study, we propose HCR-AdaAD, a multivariate anomaly detection framework that combines hierarchical context representation learning with deep learning methods. The core idea is to extract normal time-series patterns by transforming them into images, which can be used to extract spatial features and generate robust representations for normal time series. Next, we adopt Extreme Value Theory (EVT) to set self-adaptive thresholds in streaming time series, which can contribute to the ideal precision for anomaly detection and high interpretability with contextual information. We conducted evaluation experiments on three public datasets, and the results demonstrate the effectiveness and soundness of our proposed model. HCR-AdaAD offers a novel and effective approach to anomaly detection in multivariate time series that outperforms traditional methods, making it a promising solution for real-world applications in various domains.},
  archive      = {J_TKDE},
  author       = {Chunming Lin and Bowen Du and Leilei Sun and Linchao Li},
  doi          = {10.1109/TKDE.2024.3360640},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3139-3150},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hierarchical context representation and self-adaptive thresholding for multivariate anomaly detection},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous graph condensation. <em>TKDE</em>,
<em>36</em>(7), 3126–3138. (<a
href="https://doi.org/10.1109/TKDE.2024.3362863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks greatly facilitate data processing in homogeneous and heterogeneous graphs. However, training GNNs on large-scale graphs poses a significant challenge to computing resources. It is especially prominent on heterogeneous graphs, which contain multiple types of nodes and edges, and heterogeneous GNNs are also several times more complex than the ordinary GNNs. Recently, Graph condensation (GCond) is proposed to address the challenge by condensing large-scale homogeneous graphs into small-scale informative graphs. Its label-based feature initialization and fully-connected design perform well on homogeneous graphs. While in heterogeneous graphs, label information generally only exists in specific types of nodes, making it difficult to be applied directly to heterogeneous graphs. In this article, we propose heterogeneous graph condensation (HGCond). HGCond uses clustering information instead of label information for feature initialization, and constructs a sparse connection scheme accordingly. In addition, we found that the simple parameter exploration strategy in GCond leads to insufficient optimization on heterogeneous graphs. This article proposes an exploration strategy based on orthogonal parameter sequences to address the problem. We experimentally demonstrate that the novel feature initialization and parameter exploration strategy is effective. Experiments show that HGCond significantly outperforms baselines on multiple datasets. On the dataset DBLP, HGCond can condense DBLP to 0.5% of its original scale to obtain DBLP-0.005. GNNs trained on DBLP-0.005 can retain nearly 99% accuracy compared to the GNNs trained on full-scale DBLP.},
  archive      = {J_TKDE},
  author       = {Jian Gao and Jianshe Wu and Jingyi Ding},
  doi          = {10.1109/TKDE.2024.3362863},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3126-3138},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Heterogeneous graph condensation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Half-xor: A fully-dynamic sketch for estimating the number
of distinct values in big tables. <em>TKDE</em>, <em>36</em>(7),
3111–3125. (<a href="https://doi.org/10.1109/TKDE.2024.3359710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Calculating the number of distinct values (i.e., NDV) in a column of a big table is costly yet fundamental to a variety of database applications such as data compression and profiling. To reduce the high time and space cost, a number of sketch methods (e.g., HyperLogLog) have been proposed, which estimate the NDV from a constructed compact data summary of distinct values. However, these methods fail or are costly to manage fully-dynamic scenarios where data is often inserted into and deleted from the table. To solve this issue, we propose a novel sketch method, Half-Xor . Our Half-Xor sketch consists of a compact bit matrix and a small counter array, and it needs to set a few bits and update a counter when handling a data insertion/deletion. Compared with the state-of-the-art mergeable method, our experimental results demonstrate that our method Half-Xor is up to 6.6 times more accurate under the same memory usage and reduces the memory usage by up to 16 times to achieve the same estimation accuracy.},
  archive      = {J_TKDE},
  author       = {Pinghui Wang and Dongdong Xie and Junzhou Zhao and Jinsong Li and Zhicheng Li and Rundong Li and Yang Ren and Jia Di},
  doi          = {10.1109/TKDE.2024.3359710},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3111-3125},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Half-xor: A fully-dynamic sketch for estimating the number of distinct values in big tables},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Give us the facts: Enhancing large language models with
knowledge graphs for fact-aware language modeling. <em>TKDE</em>,
<em>36</em>(7), 3091–3110. (<a
href="https://doi.org/10.1109/TKDE.2024.3360454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, ChatGPT, a representative large language model (LLM), has gained considerable attention. Due to their powerful emergent abilities, recent LLMs are considered as a possible alternative to structured knowledge bases like knowledge graphs (KGs). However, while LLMs are proficient at learning probabilistic language patterns and engaging in conversations with humans, they, like previous smaller pre-trained language models (PLMs), still have difficulty in recalling facts while generating knowledge-grounded contents. To overcome these limitations, researchers have proposed enhancing data-driven PLMs with knowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus improving their performance in generating texts requiring factual knowledge and providing more informed responses to user queries. This paper reviews the studies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced pre-trained language models (KGPLMs) as well as their applications. Inspired by existing studies on KGPLM, this paper proposes enhancing LLMs with KGs by developing knowledge graph-enhanced large language models (KGLLMs). KGLLM provides a solution to enhance LLMs’ factual reasoning ability, opening up new avenues for LLM research.},
  archive      = {J_TKDE},
  author       = {Linyao Yang and Hongyang Chen and Zhao Li and Xiao Ding and Xindong Wu},
  doi          = {10.1109/TKDE.2024.3360454},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3091-3110},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Give us the facts: Enhancing large language models with knowledge graphs for fact-aware language modeling},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Geometric-contextual mutual infomax path aggregation for
relation reasoning on knowledge graph. <em>TKDE</em>, <em>36</em>(7),
3076–3090. (<a href="https://doi.org/10.1109/TKDE.2024.3360258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relation reasoning in K nowledge G raph C ompletion (KGC) aims at predicting missing relations between entities. Recently, effective KGC methods have usually focused on exploring the path pattern between entities, such as reward-based path walking and path context mining, to complete target relations. However, these methods typically suffer from two challenges: 1) They have difficulty in handling the individual representation limitation of candidate paths when there are no paths that directly represent latent relations between entities; 2) They overlook the biases of path context induction, which leads to unreasonable information interfering with the model&#39;s reasoning. To manage these challenges, a G eometric- C ontextual M utual I nfomax (GCMI) path aggregator is proposed for relation reasoning. First, we design an attentive path aggregator with a shared Transformer encoder to capture the contexts from several candidate paths parallelly and integrate these contexts to sufficiently represent the latent relations of each entity pair for reasoning. Then, the GCMI modules are proposed to constrain the local and global biases of path context induction in the Transformer encoder and the path aggregator, respectively, by a straightforward geometric rule. Extensive experiments on 32 real-world relation reasoning tasks demonstrate that our method significantly outperforms 8 state-of-the-art baselines in terms of AP and AUC.},
  archive      = {J_TKDE},
  author       = {Xingrui Zhuo and Gongqing Wu and Zan Zhang and Xindong Wu},
  doi          = {10.1109/TKDE.2024.3360258},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3076-3090},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Geometric-contextual mutual infomax path aggregation for relation reasoning on knowledge graph},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fraud’s bargain attack: Generating adversarial text samples
via word manipulation process. <em>TKDE</em>, <em>36</em>(7), 3062–3075.
(<a href="https://doi.org/10.1109/TKDE.2024.3349708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has revealed that natural language processing (NLP) models are vulnerable to adversarial examples. However, the current techniques for generating such examples rely on deterministic heuristic rules, which fail to produce optimal adversarial examples. In response, this study proposes a new method called the Fraud&#39;s Bargain Attack (FBA), which uses a randomization mechanism to expand the search space and produce high-quality adversarial examples with a higher probability of success. FBA uses the Metropolis-Hasting sampler, a type of Markov Chain Monte Carlo sampler, to improve the selection of adversarial examples from all candidates generated by a customized stochastic process called the Word Manipulation Process (WMP). The WMP method modifies individual words in a contextually-aware manner through insertion, removal, or substitution. Through extensive experiments, this study demonstrates that FBA outperforms other methods in terms of attack success rate, imperceptibility and sentence quality.},
  archive      = {J_TKDE},
  author       = {Mingze Ni and Zhensu Sun and Wei Liu},
  doi          = {10.1109/TKDE.2024.3349708},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3062-3075},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fraud&#39;s bargain attack: Generating adversarial text samples via word manipulation process},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Focused contrastive loss for classification with pre-trained
language models. <em>TKDE</em>, <em>36</em>(7), 3047–3061. (<a
href="https://doi.org/10.1109/TKDE.2023.3327777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning, which learns data representations by contrasting similar and dissimilar instances, has achieved great success in various domains including natural language processing (NLP). Recently, it has been demonstrated that incorporating class labels into contrastive learning, i.e., supervised contrastive learning (SCL), can further enhance the quality of the learned data representations. Although several works have shown empirically that incorporating SCL into classification models leads to better performance, the mechanism of how SCL works for classification is less studied. In this paper, we first investigate how SCL facilitates the classifier learning, where we show that the contrastive region, i.e., the data instances involved in each contrasting operation, has a crucial link to the mechanism of SCL. We reveal that the vanilla SCL is suboptimal since its behavior can be altered by variances in class distributions. Based on this finding, we propose a Fo cused C ontrastive L oss (FoCL) for classification. Compared with SCL, FoCL defines a finer contrastive region, focusing on the data instances surrounding decision boundaries. We conduct extensive experiments on three NLP tasks: text classification, named entity recognition, and relation extraction. Experimental results show consistent and significant improvements of FoCL over strong baselines on various benchmark datasets, especially in few-shot scenarios.},
  archive      = {J_TKDE},
  author       = {Jiayuan He and Yuan Li and Zenan Zhai and Biaoyan Fang and Camilo Thorne and Christian Druckenbrodt and Saber Akhondi and Karin Verspoor},
  doi          = {10.1109/TKDE.2023.3327777},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3047-3061},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Focused contrastive loss for classification with pre-trained language models},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable recommender with geometric information
bottleneck. <em>TKDE</em>, <em>36</em>(7), 3036–3046. (<a
href="https://doi.org/10.1109/TKDE.2024.3350447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainable recommender systems can explain their recommendation decisions, enhancing user trust in the systems. Most explainable recommender systems either rely on human-annotated rationales to train models for explanation generation or leverage the attention mechanism to extract important text spans from reviews as explanations. The extracted rationales are often confined to an individual review and may fail to identify the implicit features beyond the review text. To avoid the expensive human annotation process and to generate explanations beyond individual reviews, we propose to incorporate a geometric prior learnt from user-item interactions into a variational network which infers latent factors from user-item reviews. The latent factors from an individual user-item pair can be used for both recommendation and explanation generation, which naturally inherit the global characteristics encoded in the prior knowledge. Experimental results on three e-commerce datasets show that our model significantly improves the interpretability of a variational recommender using the Wasserstein distance while achieving performance comparable to existing content-based recommender systems in terms of recommendation behaviours.},
  archive      = {J_TKDE},
  author       = {Hanqi Yan and Lin Gui and Menghan Wang and Kun Zhang and Yulan He},
  doi          = {10.1109/TKDE.2024.3350447},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3036-3046},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Explainable recommender with geometric information bottleneck},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Enhancing drug recommendations via heterogeneous graph
representation learning in EHR networks. <em>TKDE</em>, <em>36</em>(7),
3024–3035. (<a href="https://doi.org/10.1109/TKDE.2023.3329025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic health records (EHRs) contain vast medical information like diagnosis, medication, and procedures, enabling personalized drug recommendations and treatment adjustments. However, current drug recommendation methods only model patients’ health conditions from EHR data, neglecting the rich relationships within the data. This paper seeks to utilize a heterogeneous information network (HIN) to represent EHR and develop a graph representation learning method for medication recommendation. However, three critical issues need to be investigated: (1) co-occurrence of diagnosis and drug for the same patient does not imply their relevance; (2) patients’ directly associated information may not be sufficient to reflect their health conditions; and (3) the cold start problem exists when patients have no historical EHRs. To tackle these challenges, we develop a bi-channel heterogeneous local structural encoder to decouple and extract the diverse information in HIN. Additionally, a global information capture and fusion module, aggregating meta-paths to form a global representation, is introduced to fill the information gaps in records. A longitudinal model using rich structural information available in EHR data is proposed for drug recommendations to new patients. Experimental results on real-world EHR data demonstrate significant improvements over existing approaches.},
  archive      = {J_TKDE},
  author       = {Haijun Zhang and Xian Yang and Liang Bai and Jiye Liang},
  doi          = {10.1109/TKDE.2023.3329025},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3024-3035},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Enhancing drug recommendations via heterogeneous graph representation learning in EHR networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient skyline frequent-utility itemset mining algorithm
on massive data. <em>TKDE</em>, <em>36</em>(7), 3009–3023. (<a
href="https://doi.org/10.1109/TKDE.2024.3349454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequent itemset mining (FIM) and high-utility itemset mining (HUIM) are two important branches of itemset mining which is a key technology of knowledge discovery in many applications. Nowadays, there have been extensive algorithms on FIM and HUIM, but few studies consider frequency and utility together, so skyline frequent-utility itemset mining (SFUIM) is proposed to find useful itemsets with both frequency and utility measurements. Nevertheless, SFUIM is more challenging than FIM and HUIM since the search space is large and the calculation cost is expensive without any threshold, especially on large-scale databases. To address it, this paper proposes a novel prefix-based algorithm PSI* to mine skyline frequent-utility itemsets on massive data. PSI* divides the huge database by prefix-based partitioning, so that the calculation of itemsets with a specific prefix-item only involves a partition instead of the database. A multilevel-index based list is presented to compactly maintain the maximal utility under the frequency constraint, and a novel grid-based structure is devised to organize partitions or items by a designed order. Moreover, four efficient pruning strategies are proposed to prune itemsets as early as possible. Substantial experiments show that the PSI* algorithm has better performance than the state-of-the-art algorithms, obviously on large-scale databases.},
  archive      = {J_TKDE},
  author       = {Jingxuan He and Xixian Han and Xiaolong Wan and Jinbao Wang},
  doi          = {10.1109/TKDE.2024.3349454},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {3009-3023},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient skyline frequent-utility itemset mining algorithm on massive data},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient algorithms for group hitting probability queries
on large graphs. <em>TKDE</em>, <em>36</em>(7), 2995–3008. (<a
href="https://doi.org/10.1109/TKDE.2023.3349164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a source node $s$ and a target node $t$ , the hitting probability tells us how likely an $\alpha$ -terminating random walk (which stops with probability $\alpha$ at each step) starting from $s$ can hit $t$ before it stops. This concept originates from the hitting time, a classic concept in random walks. In this paper, we focus on the group hitting probability (GHP) where the target is a set of nodes, measuring the node-to-group structural proximity. For this group version of the hitting probability, we present efficient algorithms for two types of GHP queries: the pairwise query which returns the GHP value of a target set $T$ with respect to (w.r.t.) a source node $s$ , and the top- $k$ query which returns the top- $k$ target sets with the largest GHP value w.r.t. a source node $s$ . We first develop an efficient algorithm named SAMBA for the pairwise query, which is built on a group local push algorithm tailored for GHP, with rigorous analysis for correctness. Next, we show how to speed up SAMBA by combining the group local push algorithm with the Monte Carlo approach, where GHP brings new challenges as it might need to consider every hop of the random walk. We tackle this issue with a new formulation of the GHP and show how to provide approximation guarantees with a detailed theoretical analysis. With SAMBA as the backbone, we develop an iterative algorithm for top- $k$ queries, which adaptively refines the bounds for the candidate target sets, and terminates as soon as it meets the stopping condition, thus saving unnecessary computational costs. We further present an optimization technique to accelerate the top- $k$ query, improving its practical performance. Extensive experiments show that our solutions are orders of magnitude faster than their competitors.},
  archive      = {J_TKDE},
  author       = {Qintian Guo and Dandan Lin and Sibo Wang and Raymond Chi-Wing Wong and Wenqing Lin},
  doi          = {10.1109/TKDE.2023.3349164},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {2995-3008},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient algorithms for group hitting probability queries on large graphs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic quantification with constrained error under unknown
general dataset shift. <em>TKDE</em>, <em>36</em>(7), 2980–2994. (<a
href="https://doi.org/10.1109/TKDE.2023.3349286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantification research has sought to accurately estimate class distributions under dataset shift. While existing methods perform well under assumed conditions of shift, it is not always clear whether such assumptions will hold in a given application. This work extends the analysis and experimental evaluation of our Gain-Some-Lose-Some (GSLS) model for quantification under general dataset shift and incorporates it into a method for dynamically selecting the most appropriate quantification method. Selection by a Kolmogorov-Smirnov test for any shift followed by a newly proposed “Adjusted Kolmogorov-Smirnov” test for non-prior shift is found to best balance quantification and runtime performance. We also present a framework for constraining quantification prediction intervals to user-specified limits by requesting a smaller set of instance class labels from the user than required with confidence-based rejection.},
  archive      = {J_TKDE},
  author       = {Benjamin Denham and Edmund M-K Lai and Roopak Sinha and M. Asif Naeem},
  doi          = {10.1109/TKDE.2023.3349286},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {2980-2994},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dynamic quantification with constrained error under unknown general dataset shift},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic graph embedding via meta-learning. <em>TKDE</em>,
<em>36</em>(7), 2967–2979. (<a
href="https://doi.org/10.1109/TKDE.2023.3329238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs in real-world applications usually evolve constantly presenting dynamic behaviors such as social networks and transportation networks. Hence, dynamic graph embedding has gained much attention recently. In dynamic graphs, both the topology and node attributes could change over time, which pose great challenges for developing effective embedding models. Typically, the evolution process of a dynamic graph can be recorded as a series of snapshots. We observe that the evolution process inherently provides both prior information (previous snapshots) and validation information (the next snapshot). The prior information can be used to fit the evolution process, while the validation information can be used to improve the generalization ability of a graph embedding model. However, existing dynamic graph embedding models only utilize the prior information, but overlook the validation information. To tackle this issue, this paper proposes a novel dynamic graph embedding method via Model-Agnostic Meta-Learning, which utilizes both kinds of information to obtain better graph representation. The extensive experiments on eight real-world datasets demonstrate the superiority of our proposed method over state-of-the-art methods on various graph analysis tasks.},
  archive      = {J_TKDE},
  author       = {Yuren Mao and Yu Hao and Xin Cao and Yixiang Fang and Xuemin Lin and Hua Mao and Zhiqiang Xu},
  doi          = {10.1109/TKDE.2023.3329238},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {2967-2979},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dynamic graph embedding via meta-learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DepMSTAT: Multimodal spatio-temporal attentional transformer
for depression detection. <em>TKDE</em>, <em>36</em>(7), 2956–2966. (<a
href="https://doi.org/10.1109/TKDE.2024.3350071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression is one of the most common mental illnesses, but few of the currently proposed in-depth models based on social media data take into account both temporal and spatial information in the data for the detection of depression. In this paper, we present an efficient, low-covariance multimodal integrated spatio-temporal converter framework called DepMSTAT, which aims to detect depression using acoustic and visual features in social media data. The framework consists of four modules: a data preprocessing module, a token generation module, a Spatial-Temporal Attentional Transformer (STAT) module, and a depression classifier module. To efficiently capture spatial and temporal correlations in multimodal social media depression data, a plug-and-play STAT module is proposed. The module is capable of extracting unimodal spatio-temporal features and fusing unimodal information, playing a key role in the analysis of acoustic and visual features in social media data. Through extensive experiments on a depression database (D-Vlog), the method in this paper shows high accuracy (71.53%) in depression detection, achieving a performance that exceeds most models. This work provides a scaffold for studies based on multimodal data that assists in the detection of depression.},
  archive      = {J_TKDE},
  author       = {Yongfeng Tao and Minqiang Yang and Huiru Li and Yushan Wu and Bin Hu},
  doi          = {10.1109/TKDE.2024.3350071},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {2956-2966},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DepMSTAT: Multimodal spatio-temporal attentional transformer for depression detection},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Denoising item graph with disentangled learning for
recommendation. <em>TKDE</em>, <em>36</em>(7), 2942–2955. (<a
href="https://doi.org/10.1109/TKDE.2024.3361482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the growth of Graph-based Collaborative Filtering (GCF) for high-performance recommendations, but the widely adopted user-item bipartite graphs are subject to deeper layers’ over-smoothing effect and sparse user-item interactions when learning item representations. In this work, we introduce item graph , which regards items as nodes and connecting those that have ever co-occurred in transactions with edges, to preserve higher-order item relations while avoiding the drawbacks of bipartite graphs for item-based recommendation. To cope with the entangled semantics in the edges of an item graph, we first design a denoising scheme via a graph structure learning module with discrete sampling to drop noisy edges with respect to certain latent aspects, where multiple subgraphs can be yielded. We then implement graphical disentangled learning by imposing several structural regularizers that allow for macro conformity and micro divergence among the subgraphs. Finally, we propose a multi-graph fusion module to aggregate users’ preferences in different subgraphs with a user-graph attention mechanism. Extensive experiments on 5 real-world datasets demonstrate the superiority of our method over 16 competitive baseline methods including the recently proposed GCF ones. Particularly, our method shows evident advantages in recommendation under data sparsity conditions.},
  archive      = {J_TKDE},
  author       = {Liang Zhang and Guannan Liu and Xiaohui Liu and Junjie Wu},
  doi          = {10.1109/TKDE.2024.3361482},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {2942-2955},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Denoising item graph with disentangled learning for recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Comfort-aware lane change planning with exit strategy for
autonomous vehicle. <em>TKDE</em>, <em>36</em>(7), 2927–2941. (<a
href="https://doi.org/10.1109/TKDE.2023.3348550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automation in road vehicles is an emerging technology that has developed rapidly over the last decade. There have been many inter-disciplinary challenges posed on existing transportation infrastructure by autonomous vehicles. In this paper, we conduct an algorithmic study on when and how an autonomous vehicle should change its lane, which is a fundamental problem in vehicle automation field and root cause of most ‘phantom’ traffic jams. We propose a prediction-and-decision framework, called Cheetah ( Ch ang e lan e smar t for a utonomous ve h icle), which aims to optimize the lane changing maneuvers of autonomous vehicle while minimizing its impact on surrounding vehicles. In the prediction phase, Cheetah learns the spatio-temporal dynamics from historical trajectories of surrounding vehicles with a deep model (GAS-LED model) and predict their corresponding actions in the near future. A global attention mechanism and state sharing strategy are also incorporated to achieve higher accuracy and better convergence efficiency. Then in the decision phase, Cheetah looks for optimal lane change maneuvers for the autonomous vehicle by taking into account a few factors such as speed, impact on other vehicles and safety issues. A tree-based adaptive beam search algorithm is designed to reduce the search space and improve accuracy. In order to make our framework applicable to more scenarios, we further propose an improved Cheetah (Cheetah + ) framework that makes the autonomous vehicle adapt for exiting a road and meet the requirement for driving comfort. Extensive experiments offer evidence that the proposed framework can advance the state of the art in terms of effectiveness and efficiency.},
  archive      = {J_TKDE},
  author       = {Shuncheng Liu and Xu Chen and Yan Zhao and Han Su and Xiaofang Zhou and Kai Zheng},
  doi          = {10.1109/TKDE.2023.3348550},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {2927-2941},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Comfort-aware lane change planning with exit strategy for autonomous vehicle},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Co-engaged location group search in location-based social
networks. <em>TKDE</em>, <em>36</em>(7), 2910–2926. (<a
href="https://doi.org/10.1109/TKDE.2023.3327405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searching for well-connected user communities in a Location-based Social Network (LBSN) has been extensively investigated. However, very few studies focus on finding a group of locations in an LBSN which are significantly engaged with socially cohesive user groups. In this work, we investigate the problem of C o-engaged L ocation group S earch ( CLS ) from LBSNs where the selected locations are visited frequently by the members of the socially cohesive user groups, and the locations are reachable within a given distance threshold. To the best of our knowledge, this is the first work to search for socially co-engaged location groups in LBSNs. We devise a score function to measure the co-engagement of the location groups by combining social connectivity of the cohesive user groups and check-in density of the users to the selected locations. To solve the CLS problem, we propose a Filter-and-Verify algorithm that effectively filters out ineligible locations, and their corresponding check-in users. Further, we derive a lower bound on the number of check-ins to prune the insignificant locations and develop a novel greedy forward expansion algorithm ( GFA ). To accelerate the computation of CLS , we propose a ranking function and devise an incremental algorithm, GIA , that can filter the unqualified location groups. We establish the effectiveness of our solutions by conducting extensive experiments on three real-world datasets.},
  archive      = {J_TKDE},
  author       = {Nur Al Hasan Haldar and Jianxin Li and Naveed Akhtar and Yan Jia and Ajmal Mian},
  doi          = {10.1109/TKDE.2023.3327405},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {2910-2926},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Co-engaged location group search in location-based social networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayes-enhanced multi-view attention networks for robust POI
recommendation. <em>TKDE</em>, <em>36</em>(7), 2895–2909. (<a
href="https://doi.org/10.1109/TKDE.2023.3329673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {POI recommendation can facilitate various Location-Based Social Network services. Existing methods generally assume the available POI check-ins are the ground-truth depiction of user behaviors. However, in real scenarios, check-in data can be rather unreliable due to both subjective and objective causes including positioning errors and user privacy concerns. The data uncertainty issue may lead to significant negative impacts on POI recommendation, but has not been fully explored. To this end, we investigate a novel problem of robust POI recommendation by considering the uncertainty factors of user check-ins, and propose a Bayes-enhanced Multi-view Attention Network to effectively address it. Specifically, we construct three POI graphs to comprehensively model the dependencies among the POIs from different views, including the personal POI transition graph, the semantic-based and distance-based POI graphs. As the personal graph is usually sparse and sensitive to noise, we design a Bayes-enhanced spatial dependency learning module for data augmentation from the local view. A Bayesian posterior guided graph augmentation approach is adopted to generate a new graph with collaborative signals to increase the data diversity and thus counteract the data uncertainty issue. Next, a multi-view attention-based user preference learning module is proposed. By incorporating the semantic and distance correlations of POIs, the user preference can be effectively refined and finally achieve robust recommendations. We conduct extensive experiments over three datasets. The results show that our proposal significantly outperforms the state-of-the-art methods in POI recommendation when the available check-ins are incomplete and noisy.},
  archive      = {J_TKDE},
  author       = {Jiangnan Xia and Yu Yang and Senzhang Wang and Hongzhi Yin and Jiannong Cao and Philip S. Yu},
  doi          = {10.1109/TKDE.2023.3329673},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {2895-2909},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Bayes-enhanced multi-view attention networks for robust POI recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AStore: Uniformed adaptive learned index and cache for
RDMA-enabled key-value store. <em>TKDE</em>, <em>36</em>(7), 2877–2894.
(<a href="https://doi.org/10.1109/TKDE.2024.3355100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed key-value storage and computation are essential components of cloud services. As the demand for high-performance systems has increased significantly, a new architecture has been motivated to separate computing and storage nodes and connect them using RDMA-enabled networks. Existing RDMA-enabled systems use client-side cached indexes to reduce communication overhead and improve performance. However, such approaches could result in high server CPU contention due to heavy dynamic workloads (i.e., inserts ), and cause a large accuracy gap because of the different indexes between client-side and server-side. These drawbacks limit the performance of RDMA-enabled systems. In this paper, to deal with these issues, we introduce AStore to achieve high performance with low memory footprint. AStore employs a new uniformed architecture, utilizing an adaptive learned index as both the server-side learned index and the client-side cached index, to handle dynamic and static workloads. We propose several optimization techniques to optimize dynamic and static workload procedures and design the leaf node lock mechanism to support high concurrent access. Extensive evaluations on YCSB, LGN, and OSM datasets demonstrate that AStore achieves competitive performance on read-only workloads by up to 75.2%, 107.3% and 57.7%, as well as improving performance on write-read workloads by up to 65.7%, 108.7% and 74.3% than XStore.},
  archive      = {J_TKDE},
  author       = {Pengpeng Qiao and Zhiwei Zhang and Yuntong Li and Ye Yuan and Shuliang Wang and Guoren Wang and Jeffrey Xu Yu},
  doi          = {10.1109/TKDE.2024.3355100},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {2877-2894},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {AStore: Uniformed adaptive learned index and cache for RDMA-enabled key-value store},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An index for set intersection with post-filtering.
<em>TKDE</em>, <em>36</em>(7), 2862–2876. (<a
href="https://doi.org/10.1109/TKDE.2023.3329145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies how to design an index structure on a collection of sets $S_{1}, S_{2},{\ldots }, S_{n}$ to answer the following queries: given distinct set ids $a, b \in [1, n]$ , report $F(S_{a} \cap S_{b})$ where $F(.)$ is a filtering function. We present a solution that can support a great variety of filtering functions — range research, skyline, convex hull, nearest neighbor search, quantile (to name just a few) — with attractive performance guarantees. The guarantees are sensitive to the set collection&#39;s pseudoarboricity , a new notion for quantifying the density of $\lbrace S_{1}, S_{2},{\ldots }, S_{n}\rbrace$ . Our index structures are simple to understand and implement.},
  archive      = {J_TKDE},
  author       = {Ru Wang and Shangqi Lu and Yufei Tao},
  doi          = {10.1109/TKDE.2023.3329145},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {2862-2876},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {An index for set intersection with post-filtering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An effective optimization method for fuzzy <span
class="math inline"><em>k</em></span>k-means with entropy
regularization. <em>TKDE</em>, <em>36</em>(7), 2846–2861. (<a
href="https://doi.org/10.1109/TKDE.2023.3329821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy $k$ -Means with Entropy Regularization method (ERFKM) is an extension to Fuzzy $k$ -Means (FKM) by introducing a maximum entropy term to FKM, whose purpose is trading off fuzziness and compactness. However, ERFKM often converges to a poor local minimum, which affects its performance. In this paper, we propose an effective optimization method to solve this problem, called IRW-ERFKM. First a new equivalent problem for ERFKM is proposed; then we solve it through Iteratively Re-Weighted (IRW) method. Since IRW-ERFKM optimizes the problem with $k\times 1$ instead of $d\times k$ intermediate variables, the space complexity of IRW-ERFKM is greatly reduced. Extensive experiments on clustering performance and objective function value show IRW-ERFKM can get a better local minimum than ERFKM with fewer iterations. Through time complexity analysis, it verifies IRW-ERFKM and ERFKM have the same linear time complexity. Moreover, IRW-ERFKM has advantages on evaluation metrics compared with other methods. What&#39;s more, there are two interesting findings. One is when we use IRW method to solve the equivalent problem of ERFKM with one factor $\mathbf{U}$ , it is equivalent to ERFKM. The other is when the inner loop of IRW-ERFKM is executed only once, IRW-ERFKM and ERFKM are equivalent in this case.},
  archive      = {J_TKDE},
  author       = {Yun Liang and Yijin Chen and Qiong Huang and Haoming Chen and Feiping Nie},
  doi          = {10.1109/TKDE.2023.3329821},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {2846-2861},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {An effective optimization method for fuzzy $k$k-means with entropy regularization},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified and scalable algorithm framework of user-defined
temporal <span class="math inline">(<em>k</em>, 𝒳)</span>(k,x)-core
query. <em>TKDE</em>, <em>36</em>(7), 2831–2845. (<a
href="https://doi.org/10.1109/TKDE.2023.3349310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Querying cohesive subgraphs on temporal graphs (e.g., social network, finance network, etc.) with various conditions has attracted intensive research interests recently. In this paper, we study a novel Temporal $(k,\mathcal {X})$ -Core Query (TXCQ) that extends a fundamental Temporal $k$ -Core Query (TCQ) proposed in our conference paper by optimizing or constraining an arbitrary metric $\mathcal {X}$ of $k$ -core, such as size, engagement, interaction frequency, time span, burstiness, periodicity, etc. Our objective is to address specific TXCQ instances with conditions on different $\mathcal {X}$ in a unified algorithm framework that guarantees scalability. For that, this journal paper proposes a taxonomy of measurement $\mathcal {X}(\cdot )$ and achieve our objective using a two-phase framework while $\mathcal {X}(\cdot )$ is time-insensitive or time-monotonic. Specifically, Phase 1 still leverages the query processing algorithm of TCQ to induce all distinct $k$ -cores during a given time range, and meanwhile locates the “time zones” in which the cores emerge. Then, Phase 2 conducts fast local search and $\mathcal {X}$ evaluation in each time zone with respect to the time insensitivity or monotonicity of $\mathcal {X}(\cdot )$ . By revealing two insightful concepts named tightest time interval and loosest time interval that bound time zones, the redundant core induction and unnecessary $\mathcal {X}$ evaluation in a zone can be reduced dramatically. Our experimental results demonstrate that TXCQ can be addressed as efficiently as TCQ, which achieves the latest state-of-the-art performance, by using a general algorithm framework that leaves $\mathcal {X}(\cdot )$ as a user-defined function.},
  archive      = {J_TKDE},
  author       = {Ming Zhong and Junyong Yang and Yuanyuan Zhu and Tieyun Qian and Mengchi Liu and Jeffrey Xu Yu},
  doi          = {10.1109/TKDE.2023.3349310},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {2831-2845},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A unified and scalable algorithm framework of user-defined temporal $(k,\mathcal {X})$(k,X)-core query},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on generative diffusion models. <em>TKDE</em>,
<em>36</em>(7), 2814–2830. (<a
href="https://doi.org/10.1109/TKDE.2024.3361474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep generative models have unlocked another profound realm of human creativity. By capturing and generalizing patterns within data, we have entered the epoch of all-encompassing Artificial Intelligence for General Creativity (AIGC). Notably, diffusion models, recognized as one of the paramount generative models, materialize human ideation into tangible instances across diverse domains, encompassing imagery, text, speech, biology, and healthcare. To provide advanced and comprehensive insights into diffusion, this survey comprehensively elucidates its developmental trajectory and future directions from three distinct angles: the fundamental formulation of diffusion, algorithmic enhancements, and the manifold applications of diffusion. Each layer is meticulously explored to offer a profound comprehension of its evolution. Structured and summarized approaches are presented here.},
  archive      = {J_TKDE},
  author       = {Hanqun Cao and Cheng Tan and Zhangyang Gao and Yilun Xu and Guangyong Chen and Pheng-Ann Heng and Stan Z. Li},
  doi          = {10.1109/TKDE.2024.3361474},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {2814-2830},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey on generative diffusion models},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A suite of efficient randomized algorithms for streaming
record linkage. <em>TKDE</em>, <em>36</em>(7), 2803–2813. (<a
href="https://doi.org/10.1109/TKDE.2024.3361022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Organizations leverage massive volumes of information and new types of data to generate unprecedented insights and improve their outcomes. Correctly identifying duplicate records that represent the same entity, such as user, customer, patient and so on, a process commonly known as record linkage, can improve service levels, accelerate sales, or elevate healthcare decision support. Towards this direction, blocking methods are used with the aim to group matching records in the same block using a combination of their attributes as blocking keys. This paper introduces a suite of randomized algorithms specifically crafted for streaming record linkage settings. Using a bounded in-memory data structure, in terms of the number of blocks and positions within each block, our algorithms guarantee that the most frequently accessed and the most recently used blocks remain in main memory and, additionally, the records within a block are renewed on a rolling basis. The operation of our algorithms rely on simple random choices, instead of utilizing cumbersome sorting data structures, which ensure that the probability of inactive blocks and older records to remain in main memory decays in order to free space for more promising blocks and fresher records, respectively. We also introduce an algorithm that performs approximate blocking to tackle the problem of misspellings and typos present in the blocking keys. The experimental evaluation showcases that our proposed algorithms scale efficiently to data streams by providing certain accuracy guarantees.},
  archive      = {J_TKDE},
  author       = {Dimitrios Karapiperis and Christos Tjortjis and Vassilios S. Verykios},
  doi          = {10.1109/TKDE.2024.3361022},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {2803-2813},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A suite of efficient randomized algorithms for streaming record linkage},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A neural database for answering aggregate queries on
incomplete relational data. <em>TKDE</em>, <em>36</em>(7), 2790–2802.
(<a href="https://doi.org/10.1109/TKDE.2023.3310914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world datasets are often incomplete due to data collection cost, privacy considerations or as a side effect of data integration/preparation. We focus on answering aggregate queries on such datasets, where data incompleteness causes the answers to be inaccurate. To address this problem, assuming typical relational data, existing work generates synthetic data to complete the database, a challenging task, especially in the presence of bias in observed data. Instead, we propose a paradigm shift by learning to directly estimate query answers, circumventing the difficult data generation step. Our approach, dubbed NeuroComplete, learns to answer queries in three steps. First, NeuroComplete generates a set of queries for which accurate answers can be computed given the incomplete dataset. Next, it embeds queries in a feature space, through which each query is effectively represented with the portion of the database that contributes to the query answer. Finally, it trains a neural network in a supervised learning fashion: both query features (input) and correct answers (labels) are known. The learned model generates accurate answers to new queries at test time, exploiting the generalizability of the learned model in the embedding space. Extensive experimental results on real datasets show up to 4 times for AVG queries and 10 times for COUNT queries error reduction compared with the state-of-the-art.},
  archive      = {J_TKDE},
  author       = {Sepanta Zeighami and Raghav Seshadri and Cyrus Shahabi},
  doi          = {10.1109/TKDE.2023.3310914},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {2790-2802},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A neural database for answering aggregate queries on incomplete relational data},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A generic schema evolution approach for NoSQL and relational
databases. <em>TKDE</em>, <em>36</em>(7), 2774–2789. (<a
href="https://doi.org/10.1109/TKDE.2024.3362273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the same way as with relational systems, schema evolution is a crucial aspect of NoSQL systems. But providing approaches and tools to support NoSQL schema evolution is more challenging than for relational databases. Not only are most NoSQL systems schemaless, but different data models exist without a standard specification for them. Moreover, recent proposals fail to address some key aspects related to the kinds of relationships between entities, the definition of relationship types, and the support of structural variation. In this article, we present a generic schema evolution approach able to support the most popular NoSQL data models (columnar, document, key-value, and graph) and the relational model. The proposal is based on the Orion language that implements a schema change operation taxonomy defined for the U-Schema unified data model that integrates NoSQL and relational abstractions. The consistency of the taxonomy operations is formally evaluated with Alloy, and the Orion semantics is expressed by translating operations into native code to update data and schema. Several database systems are supported, and the engine built for each of them has been validated by testing each individual SCO and refactoring study cases. A study of relative execution time of operations is also shown.},
  archive      = {J_TKDE},
  author       = {Alberto Hernández Chillón and Meike Klettke and Diego Sevilla Ruiz and Jesús García Molina},
  doi          = {10.1109/TKDE.2024.3362273},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {2774-2789},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A generic schema evolution approach for NoSQL and relational databases},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A distributed solution for efficient k shortest paths
computation over dynamic road networks. <em>TKDE</em>, <em>36</em>(7),
2759–2773. (<a href="https://doi.org/10.1109/TKDE.2023.3346377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of identifying the k -shortest paths (KSPs for short) in a dynamic road network is essential to many location-based services. Road networks are dynamic in the sense that the weights of the edges in the corresponding graph constantly change over time, representing evolving traffic conditions. Very often such services have to process numerous KSP queries over large road networks at the same time, thus there is a pressing need to identify distributed solutions for this problem. However, most existing approaches are designed to identify KSPs on a static graph in a sequential manner (i.e., the $(i+1)\text{th}$ shortest path is generated based on the $i\text{th}$ shortest path), restricting their scalability and applicability in a distributed setting. We therefore propose KSP-DG, a distributed algorithm for identifying k -shortest paths in a dynamic graph. It is based on partitioning the entire graph into smaller subgraphs, and reduces the problem of determining KSPs into the computation of partial KSPs in relevant subgraphs, which can execute in parallel on a cluster of servers. A distributed two-level index called DTLP is developed to facilitate the efficient identification of relevant subgraphs. A salient feature of DTLP is that it indexes a set of virtual paths that are insensitive to varying traffic conditions in an efficient and compact fashion, leading to very low maintenance cost in dynamic road networks. This is the first treatment of the problem of processing KSP queries over dynamic road networks. Extensive experiments conducted on real road networks confirm the superiority of our proposal over baseline methods.},
  archive      = {J_TKDE},
  author       = {Ziqiang Yu and Xiaohui Yu and Nick Koudas and Yueting Chen and Yang Liu},
  doi          = {10.1109/TKDE.2023.3346377},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {7},
  number       = {7},
  pages        = {2759-2773},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A distributed solution for efficient k shortest paths computation over dynamic road networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WAKE: A weakly supervised business process anomaly detection
framework via a pre-trained autoencoder. <em>TKDE</em>, <em>36</em>(6),
2745–2758. (<a href="https://doi.org/10.1109/TKDE.2023.3322411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to detect anomalies in business processes is crucial for achieving success in business operations. While unsupervised anomaly detection approaches have gained popularity in recent years due to their label-free nature, in some cases, a limited number of labelled anomalies can be provided and using them can improve the performance of anomaly detection. To address this issue, we propose a novel framework for anomaly detection that uses a pre-trained autoencoder to extract feature representations of traces. An anomaly score generator based on a multi-layer perceptron is utilized to evaluate the extracted features. The entire framework is trained using a joint loss that ensures the generated anomaly scores satisfy a specific distribution without compromising the autoencoder&#39;s ability to reconstruct normal traces. The feature encoder is fine-tuned to provide insights into the cause of anomalies. Additionally, we design a novel technique for calculating anomaly scores to mitigate the effects of varying numbers of potential attribute values. We conduct extensive experiments on both synthetic and real-life logs, and our results demonstrate that our proposed method, WAKE, outperforms state-of-the-art unsupervised deep business process anomaly detection methods by a significant margin. Additionally, it outperforms other weakly supervised anomaly detection methods as well.},
  archive      = {J_TKDE},
  author       = {Wei Guan and Jian Cao and Haiyan Zhao and Yang Gu and Shiyou Qian},
  doi          = {10.1109/TKDE.2023.3322411},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2745-2758},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {WAKE: A weakly supervised business process anomaly detection framework via a pre-trained autoencoder},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variational inference over graph: Knowledge representation
for deep process data analytics. <em>TKDE</em>, <em>36</em>(6),
2730–2744. (<a href="https://doi.org/10.1109/TKDE.2023.3327415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of the industrial Big Data era, accurate estimation of product quality and monitoring of working conditions from historical data have become crucial in the process industry. However, the majority of data-driven approaches predominantly rely on observational data, overlooking the valuable empirical knowledge derived from experience or underlying mechanisms. In order to leverage this knowledge, researchers employ various graph neural network-based methods which introduce connections among process variables for feature extraction. Nevertheless, it is imperative to recognize that process knowledge undergoes changes due to internal or external concept drift. To address this challenge, we propose a novel deep learning module called “variational inference over graph” to effectively harness shifting knowledge. Building upon the self-attention mechanism, we design a probabilistic self-attention mechanism for encoding and reconciling prior knowledge. Instead of directly encoding the prior knowledge through graph neural network edges, we incorporate it as regularization term within the variational inference framework that accounts for knowledge shift. Furthermore, we introduce reparameterization estimator to control the variance resulting from knowledge uncertainty. To showcase the capability of our proposed method, we conduct various experiments on quality prediction task in real industrial processes.},
  archive      = {J_TKDE},
  author       = {Zhichao Chen and Zhihuan Song and Zhiqiang Ge},
  doi          = {10.1109/TKDE.2023.3327415},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2730-2744},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Variational inference over graph: Knowledge representation for deep process data analytics},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variational continuous label distribution learning for
multi-label text classification. <em>TKDE</em>, <em>36</em>(6),
2716–2729. (<a href="https://doi.org/10.1109/TKDE.2023.3323401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label text classification (MLTC) refers to the problem of tagging a given document with the most relevant subset of labels. One of the biggest challenges for MLTC is the existence of class imbalance. Most advanced MLTC models suffer from this issue, which limits the performance of the models. In this paper, we propose a model-agnostic framework named variational continuous label distribution learning (VCLDL) to address this problem. VCLDL theoretically builds a corresponding relationship between the feature space and the label space to mine the information hidden in the observable logical labels. Specifically, VCLDL regards label distribution as a continuous density function in latent space and forms a flexible variational approach to approximate the density function of the labels with the collaboration of the feature space. Combined with VCLDL, MLTC models can pay more attention to the distribution of the whole label set, rather than specific labels with maximum response values, thus the class imbalance problem can be well overcome. Experimental results on multiple benchmark datasets demonstrate that VCLDL can bring significant performance improvements over the existing MLTC models.},
  archive      = {J_TKDE},
  author       = {Xingyu Zhao and Yuexuan An and Ning Xu and Xin Geng},
  doi          = {10.1109/TKDE.2023.3323401},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2716-2729},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Variational continuous label distribution learning for multi-label text classification},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty-guided boundary learning for imbalanced social
event detection. <em>TKDE</em>, <em>36</em>(6), 2701–2715. (<a
href="https://doi.org/10.1109/TKDE.2023.3324510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world social events typically exhibit a severe class-imbalance distribution, which makes the trained detection model encounter a serious generalization challenge. Most studies solve this problem from the frequency perspective and emphasize the representation or classifier learning for tail classes. While in our observation, compared to the rarity of classes, the calibrated uncertainty estimated from well-trained evidential deep learning networks better reflects model performance. To this end, we propose a novel uncertainty-guided class imbalance learning framework—UCL $_{SED}$ , and its variant—UCL-EC $_{SED}$ , for imbalanced social event detection tasks. We aim to improve the overall model performance by enhancing model generalization to those uncertain classes. Considering performance degradation usually comes from misclassifying samples as their confusing neighboring classes, we focus on boundary learning in latent space and classifier learning with high-quality uncertainty estimation. First, we design a novel uncertainty-guided contrastive learning loss, namely UCL and its variant—UCL-EC, to manipulate distinguishable representation distribution for imbalanced data. During training, they force all classes, especially uncertain ones, to adaptively adjust a clear separable boundary in the feature space. Second, to obtain more robust and accurate class uncertainty, we combine the results of multi-view evidential classifiers via the Dempster-Shafer theory under the supervision of an additional calibration method. We conduct experiments on three severely imbalanced social event datasets including Events2012_100, Events2018_100, and CrisisLexT_7. Our model significantly improves social event representation and classification tasks in almost all classes, especially those uncertain ones.},
  archive      = {J_TKDE},
  author       = {Jiaqian Ren and Hao Peng and Lei Jiang and Zhiwei Liu and Jia Wu and Zhengtao Yu and Philip S. Yu},
  doi          = {10.1109/TKDE.2023.3324510},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2701-2715},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Uncertainty-guided boundary learning for imbalanced social event detection},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TiCoSeRec: Augmenting data to uniform sequences by time
intervals for effective recommendation. <em>TKDE</em>, <em>36</em>(6),
2686–2700. (<a href="https://doi.org/10.1109/TKDE.2023.3324312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommendation has now been more widely studied, characterized by its well-consistency with real-world recommendation situations. Most existing works model user preference as the transition pattern from the previous item to the next, ignoring the time interval between these two items. However, we find that the time intervals in different sequences may vary significantly and thus result in the ineffectiveness of user modeling due to the issue of preference drift . Thus we propose an assumption that a sequence with uniformly distributed time intervals (denoted as uniform sequence) is more beneficial for preference learning than that with greatly varying time intervals. We then conduct an empirical study on four real datasets and the results support this assumption. Therefore, we advocate to augment sequence data from the perspective of time intervals, which is not studied in the literature. Specifically, we design five operators (Ti-Crop, Ti-CateReorder, Ti-Mask, Ti-Substitute, Ti-Insert) to transform the original non-uniform sequence to uniform sequence with the consideration of time intervals. Then, we devise a control strategy to execute data augmentation on item sequences in different lengths and a looseness range to ensure the generalization (or diversity) of generated data. Finally, we implement these improvements on a state-of-the-art model CoSeRec and propose Ti me I nterval Aware CoSeRec (TiCoSeRec). Experimental results on four datasets demonstrate that TiCoSeRec achieves significantly better performance than other 11 counterparts recommendation techniques.},
  archive      = {J_TKDE},
  author       = {Yizhou Dang and Enneng Yang and Guibing Guo and Linying Jiang and Xingwei Wang and Xiaoxiao Xu and Qinghui Sun and Hong Liu},
  doi          = {10.1109/TKDE.2023.3324312},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2686-2700},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {TiCoSeRec: Augmenting data to uniform sequences by time intervals for effective recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STWave<span class="math inline"><sup>+</sup></span>+: A
multi-scale efficient spectral graph attention network with long-term
trends for disentangled traffic flow forecasting. <em>TKDE</em>,
<em>36</em>(6), 2671–2685. (<a
href="https://doi.org/10.1109/TKDE.2023.3324501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic forecasting is crucial for public safety and resource optimization, yet is very challenging due to the temporal changes and the dynamic spatial correlations. To capture these intricate dependencies, spatio-temporal networks, such as recurrent neural networks with graph convolution networks, are applied. However, traffic forecasting is still a non-trivial task because of three major challenges: 1) Previous spatio-temporal networks are based on end-to-end training and thus fail to handle the distribution shift in the non-stationary traffic time series. 2) Existing methods always utilize the one-hour input to forecast future traffic and the long-term historical trend knowledge is ignored. 3) The efficient and effective algorithm for modeling multi-scale spatial correlations is still lacking in prior networks. Therefore, in this paper, rather than proposing yet another end-to-end model, we provide a novel disentangle-fusion framework STWave $^{+}$ to mitigate the distribution shift issue. The framework first decouples the complex one-hour traffic data into stable trends and fluctuating events, followed by a dual-channel spatio-temporal network to model trends and events, respectively. Moreover, long-term trends are used as a self-supervised signal in STWave $^{+}$ to teach overall temporal information into one-hour trends through a contrastive loss. Finally, reasonable future traffic can be predicted through the adaptive fusion of one-hour trends and events. Additionally, we incorporate a novel query sampling strategy and multi-scale graph wavelet positional encoding into the full graph attention network to efficiently and effectively model dynamic hierarchical spatial correlations. Extensive experiments on four traffic datasets show the superiority of our approach, i.e., the higher forecasting accuracy with lower computational cost.},
  archive      = {J_TKDE},
  author       = {Yuchen Fang and Yanjun Qin and Haiyong Luo and Fang Zhao and Kai Zheng},
  doi          = {10.1109/TKDE.2023.3324501},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2671-2685},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {STWave$^+$+: A multi-scale efficient spectral graph attention network with long-term trends for disentangled traffic flow forecasting},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). StableGCN: Decoupling and reconciling information
propagation for collaborative filtering. <em>TKDE</em>, <em>36</em>(6),
2659–2670. (<a href="https://doi.org/10.1109/TKDE.2023.3323458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolutional Networks (GCNs) have been widely applied to collaborative filtering, where each layer typically contains neighborhood aggregation and feature transformation. Recent studies have found that feature transformation contributes little to the final recommendation performance. They however eliminated it directly without further exploration, leading to a degradation of model expressive power. In this paper, we show that this problem arises from inconsistent information propagation process, in which the dominance of feature transformation prevents features from being properly smoothed by neighborhood aggregation. To this end, we present StableGCN to decouple and reconcile this contradictory process in an orderly rather than intertwined manner. The coarse-grained node features are first refined by an elaborate extractor, and then smoothed by a specific kind of GCN concerning feature denoising. Consequently, feature transformation and neighborhood aggregation can support each other without sacrificing expressive power. Extensive experiments on six public datasets demonstrate the effectiveness and state-of-the-art performance of StableGCN.},
  archive      = {J_TKDE},
  author       = {Cong Xu and Jun Wang and Wei Zhang},
  doi          = {10.1109/TKDE.2023.3323458},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2659-2670},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {StableGCN: Decoupling and reconciling information propagation for collaborative filtering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatio-temporal memory augmented multi-level attention
network for traffic prediction. <em>TKDE</em>, <em>36</em>(6),
2643–2658. (<a href="https://doi.org/10.1109/TKDE.2023.3322405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic prediction is one of the fundamental spatio-temporal prediction tasks in urban computing, which is of great significance to a wide range of applications, e.g., traffic controlling, vehicle scheduling, etc. Recently, with the expansion of the city and the development of public transportation, long-range and long-term spatio-temporal correlations play a more important role in traffic prediction. However, it is challenging to model long-range spatial dependencies and long-term temporal dependencies simultaneously in two aspects: 1) complex influential factors, including spatial, temporal and external factors. 2) multiple spatio-temporal correlations, including long-range and short-range spatial correlations, as well as long-term and short-term temporal correlations. To solve these issues, we propose a spatio-temporal memory augmented multi-level attention network for fine-grained traffic prediction, entitled ST-MAN. Specifically, we design a spatio-temporal memory network to encode and memorize fine-grained spatial information and representative temporal patterns. Then, we propose a multi-level attention network to explicitly model both short-term local spatio-temporal dependencies and long-term global spatio-temporal dependencies at different spatial scales (i.e., grid and region levels) and temporal scales (i.e., daily and weekly levels). In addition, we design an external component that takes external factors and spatial embeddings as inputs to generate location-aware influence of the external factors much more efficiently. Finally, we design an end-to-end framework optimized with the contrastive objective and supervised objective to boost model performance. Empirical experiments over coarse-grained and fine-grained real-world datasets demonstrate the superiority of the ST-MAN model compared to several state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Yan Liu and Bin Guo and Jingxiang Meng and Daqing Zhang and Zhiwen Yu},
  doi          = {10.1109/TKDE.2023.3322405},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2643-2658},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Spatio-temporal memory augmented multi-level attention network for traffic prediction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simultaneous deep clustering and feature selection via
k-concrete autoencoder. <em>TKDE</em>, <em>36</em>(6), 2629–2642. (<a
href="https://doi.org/10.1109/TKDE.2023.3323580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep learning methods for clustering high-dimensional data perform feature selection and clustering separately, which can result in the exclusion of some important features for clustering. In this paper, we propose a method that performs deep clustering and feature selection simultaneously by inserting a concrete selector layer between the input layer and the first encoder layer of a modified autoencoder. The concrete selector layer performs feature selection, while the modified autoencoder performs clustering in the latent space by incorporating K-means loss and inter-cluster distances. The proposed method, called the K-concrete autoencoder, selects features important for clustering and uses only the selected features to learn K-means-friendly latent representations of the data. Moreover, we propose an extension of the K-concrete autoencoder to provide relative importance of each selected feature. We demonstrate the effectiveness of the proposed method using simulated and real datasets.},
  archive      = {J_TKDE},
  author       = {Woojin Doo and Heeyoung Kim},
  doi          = {10.1109/TKDE.2023.3323580},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2629-2642},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Simultaneous deep clustering and feature selection via K-concrete autoencoder},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Schema-aware hyper-relational knowledge graph embeddings for
link prediction. <em>TKDE</em>, <em>36</em>(6), 2614–2628. (<a
href="https://doi.org/10.1109/TKDE.2023.3323499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graph (KG) embeddings have become a powerful paradigm to resolve link prediction tasks for KG completion. The widely adopted triple-based representation, where each triplet $(h,r,t)$ links two entities $h$ and $t$ through a relation $r$ , oversimplifies the complex nature of the data stored in a KG, in particular for hyper-relational facts, where each fact contains not only a base triplet $(h,r,t)$ , but also the associated key-value pairs $(k,v)$ . Even though a few recent techniques tried to learn from such data by transforming a hyper-relational fact into an n-ary representation (i.e., a set of key-value pairs only without triplets), they result in suboptimal models as they are unaware of the triplet structure, which serves as the fundamental data structure in modern KGs and preserves the essential information for link prediction. Moreover, as the KG schema information has been shown to be useful for resolving link prediction tasks, it is thus essential to incorporate the corresponding hyper-relational schema in KG embeddings. Against this background, we propose sHINGE, a schema-aware hyper-relational KG embedding model, which learns from hyper-relational facts directly (without the transformation to the n-ary representation) and their corresponding hyper-relational schema in a KG. Our extensive evaluation shows the superiority of sHINGE on various link prediction tasks over KGs. In particular, compared to a sizeable collection of 21 baselines, sHINGE consistently outperforms the best-performing triple-based KG embedding method, hyper-relational KG embedding method, and schema-aware KG embedding method by 19.1%, 1.8%, and 12.9%, respectively.},
  archive      = {J_TKDE},
  author       = {Yuhuan Lu and Dingqi Yang and Pengyang Wang and Paolo Rosso and Philippe Cudre-Mauroux},
  doi          = {10.1109/TKDE.2023.3323499},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2614-2628},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Schema-aware hyper-relational knowledge graph embeddings for link prediction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PATNet: Propensity-adjusted temporal network for joint
imputation and prediction using binary EHRs with observation bias.
<em>TKDE</em>, <em>36</em>(6), 2600–2613. (<a
href="https://doi.org/10.1109/TKDE.2023.3321738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive analysis of electronic health records (EHR) is a fundamental task that could provide actionable insights to help clinicians improve the efficiency and quality of care. EHR are commonly recorded in binary format and contain inevitable missing data. The nature of missingness may vary by patients, clinical features, and time, which incurs observation bias. It is essential to account for the binary missingness and observation bias or the predictive performance could be substantially compromised. In this paper, we develop a propensity-adjusted temporal network (PATNet) to conduct data imputation and predictive analysis simultaneously. PATNet contains three subnetworks: 1) an imputation subnetwork that generates the initial imputation based on historical observations, 2) a propensity subnetwork that infers the patient-, feature-, and time-dependent propensity scores, and 3) a prediction subnetwork that produces the missing-informative prediction using the propensity-adjusted imputations and the missing probabilities. To allow the propensity scores to be inferred from data, we use the expectation-maximization (EM) algorithm to learn the imputation and propensity subnetworks and incorporate a low-rank constraint via PARAFAC2 approximation. Extensive evaluation using the MIMIC-III and eICU datasets demonstrates that PATNet outperforms the state-of-the-art methods in terms of binary data imputation, disease progression modeling, and mortality prediction tasks.},
  archive      = {J_TKDE},
  author       = {Kejing Yin and Dong Qian and William K. Cheung},
  doi          = {10.1109/TKDE.2023.3321738},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2600-2613},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {PATNet: Propensity-adjusted temporal network for joint imputation and prediction using binary EHRs with observation bias},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Normalized robust PCA with adaptive reconstruction error
minimization. <em>TKDE</em>, <em>36</em>(6), 2587–2599. (<a
href="https://doi.org/10.1109/TKDE.2023.3325462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal component analysis (PCA) is one of the most versatile techniques for unsupervised dimension reduction, which is implemented as a fundamental preprocessing method in multiple tasks of statistics and machine learning research because of its efficiency. Nevertheless, researchers have concentrated on the identification of outliers that do not conform to the low-dimensional approximation through statistical methods, e.g., outlier rejection, without giving insights on each data point with a dynamic ratio of signal-to-noise components in the high-dimensional regimes. To characterize the dynamic nature of the principal component information, we propose a Normalized Robust PCA with Adaptive Reconstruction Error minimization model, which considers both the adaptive normalization technique and flexible weights learning simultaneously. With this configuration, the principal component information constantly adjusts the degree of sparsity for activated samples. In other words, the signal component&#39;s discrimination and noise information restriction could work cooperatively. Empirical studies on one synthetic dataset and several benchmarks demonstrate the effectiveness of our proposed method over existing outlier rejection methods.},
  archive      = {J_TKDE},
  author       = {Yunlong Gao and Yuzhe Feng and Youwei Xie and Jinyan Pan and Feiping Nie},
  doi          = {10.1109/TKDE.2023.3325462},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2587-2599},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Normalized robust PCA with adaptive reconstruction error minimization},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural abstractive summarization for long text and multiple
tables. <em>TKDE</em>, <em>36</em>(6), 2572–2586. (<a
href="https://doi.org/10.1109/TKDE.2023.3324012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstractive summarization aims to generate a concise summary covering the input document&#39;s salient information. Within a report document, the salient information can be scattered in the textual and non-textual content. However, existing document summarization datasets and methods usually focus on the text and filter out the non-textual content. Missing tabular data can limit produced summaries’ informativeness, especially when summaries require covering quantitative descriptions of critical metrics in tables. Existing datasets and methods cannot meet the requirements of summarizing long text and dozens of tables in each report document. To deal with the scarcity of available datasets, we propose FINDSum, the first large-scale dataset for long text and multi-table summarization. Built on 21,125 annual reports from 3,794 companies, FINDSum has two subsets for summarizing each company&#39;s results of operations and liquidity. Besides, we present four types of summarization methods to jointly consider text and table content when summarizing reports. Additionally, we propose a set of evaluation metrics to assess the usage of numerical information in produced summaries. Our summarization methods significantly outperform advanced baselines, which verifies the necessity of incorporating textual and tabular data when summarizing report documents. We also conduct extensive comparative experiments to identify vital model components and configurations that can improve summarization results.},
  archive      = {J_TKDE},
  author       = {Shuaiqi Liu and Jiannong Cao and Zhongfen Deng and Wenting Zhao and Ruosong Yang and Zhiyuan Wen and Philip S. Yu},
  doi          = {10.1109/TKDE.2023.3324012},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2572-2586},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Neural abstractive summarization for long text and multiple tables},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-view attributed network embedding using manifold
regularization preserving non-negative matrix factorization.
<em>TKDE</em>, <em>36</em>(6), 2563–2571. (<a
href="https://doi.org/10.1109/TKDE.2023.3325461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attributed network has more network information, so more and more attention is paid to the embedding of attributed network. A few existing works have considered the node attributes plays a crucial role in the quality of network embedding. They use the non-negative matrix factorization (NMF) method to mine the network information of network structure and node attributes respectively. Considering the reconstruction error of NMF method, the original network information will be lost when the final network embedding is generated. In this paper, we propose a novel multi-view attributed network embedding model with manifold regularization (Mane). The manifold regularization is added to the model to better reflect the Riemann geometry structure of the network in the feature space to enhance the information. And the problem of missing information of NMF is solved. Our approach uses the NMF to get the non-negative coefficient matrix corresponding to network structure and node attributes. Then cooperative regularization and manifold regularization is added to obtain more information in the final network embedding. The model proposed in this paper has been verified by experiments on several real data sets. The result shows that the model is superior to the state-of-the-art algorithm in node classification task.},
  archive      = {J_TKDE},
  author       = {Weiwei Yuan and Xiang Li and Donghai Guan},
  doi          = {10.1109/TKDE.2023.3325461},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2563-2571},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-view attributed network embedding using manifold regularization preserving non-negative matrix factorization},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multithreading heterogeneous graph aggregation.
<em>TKDE</em>, <em>36</em>(6), 2548–2562. (<a
href="https://doi.org/10.1109/TKDE.2023.3320127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Towards building online analytical services on big heterogeneous graphs, we study the problem of the multithreading graph aggregation. The purpose is to exploit the thread-level parallelism for accelerating the graph aggregation process, which is both data and computation intensive. We identify the sources of parallelization latency caused by multifarious factors, including data distributions and contentions, uneven workload assignments, logical aggregation plan obstructions, etc. To cope with these problems, we investigate a parallelization solution for graph aggregation with a number of threads packaged as threadblocks, categorize the parallelization latency as the thread-level and threadblock-level latency, and propose a series of optimization techniques for alleviating or eliminating the latency on different levels. The solution supports different aggregate functions, scales up to large number of threads, and scales out to big heterogeneous graphs. Experiments on real datasets show that our solution achieves up to 60x acceleration with 256 threads compared to the non-parallelized solution.},
  archive      = {J_TKDE},
  author       = {Kai Zou and Xike Xie and Haoyun Li and X. Sean Wang},
  doi          = {10.1109/TKDE.2023.3320127},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2548-2562},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multithreading heterogeneous graph aggregation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-source domain adaptation enhanced warehouse dwell time
prediction. <em>TKDE</em>, <em>36</em>(6), 2533–2547. (<a
href="https://doi.org/10.1109/TKDE.2023.3324656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Warehouse dwell time ( WDT ) of a truck is a critical metric for evaluating plant-logistics efficiency, including the time of the truck&#39;s queuing outside and loading inside the warehouse. But WDT prediction is challenging as it is affected by diverse factors like loading distinct types and weights of the cargoes, and varying amounts of loading tasks in different time slots. Besides, each trucks’ WDT is transitively influenced by its preceding trucks’ loading time in the queue. In this paper, we propose a multi-block dwell time prediction framework consisting of LSTM model and self-attention mechanism, called SDP . In view of that low performance of SDP brought by sparse loading data of some warehouses, we further design a multi-source adaptation based block-to-block transfer learning module. We present a warehouse similarity measurement based on loading tasks allocated and loading ability of the warehouses, according to which we enhance overall prediction performance by learning from high-performance WDT prediction models of similar warehouses. Experimental results on a large-scale logistics data set demonstrate that our proposal can reduce Mean Absolute Percentage Error (MAPE) by an average of 10.0%, Mean Absolute Error(MAE) by an average of 16.5%, and Root Mean Square Error(RMSE) by an average of 17.0% as compared to the baselines.},
  archive      = {J_TKDE},
  author       = {Wei Zhao and Jiali Mao and Xingyi Lv and Cheqing Jin and Aoying Zhou},
  doi          = {10.1109/TKDE.2023.3324656},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2533-2547},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-source domain adaptation enhanced warehouse dwell time prediction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Locality-aware tail node embeddings on homogeneous and
heterogeneous networks. <em>TKDE</em>, <em>36</em>(6), 2517–2532. (<a
href="https://doi.org/10.1109/TKDE.2023.3313355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the state-of-the-art network embedding approaches often learn high-quality embeddings for high-degree nodes with abundant structural connectivity, the quality of the embeddings for low-degree or tail nodes is often suboptimal due to their limited structural connectivity. While many real-world networks are long-tailed, to date little effort has been devoted to tail node embeddings. In this article, we formulate the goal of learning tail node embeddings as a few-shot regression problem, given the few links on each tail node. In particular, since each node resides in its own local context, we personalize the regression model for each tail node. To reduce overfitting in the personalization, we propose a locality-aware meta-learning framework, called meta-tail2vec , which learns to learn the regression model for the tail nodes at different localities. Moreover, to address the heterogeneity in nodes and edges on heterogeneous information networks (HINs), we further extend the proposed model and formulate meta-tail2vec+ , which is based on a dual-adaptation mechanism to facilitate the locality-aware tail node embeddings on HINs. Finally, we conduct extensive experiments and demonstrate the promising results of both meta-tail2vec and its extension meta-tail2vec+.},
  archive      = {J_TKDE},
  author       = {Zemin Liu and Yuan Fang and Wentao Zhang and Xinming Zhang and Steven C.H. Hoi},
  doi          = {10.1109/TKDE.2023.3313355},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2517-2532},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Locality-aware tail node embeddings on homogeneous and heterogeneous networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning informative representation for fairness-aware
multivariate time-series forecasting: A group-based perspective.
<em>TKDE</em>, <em>36</em>(6), 2504–2516. (<a
href="https://doi.org/10.1109/TKDE.2023.3323956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series (MTS) forecasting penetrates various aspects of our economy and society, whose roles become increasingly recognized. However, often MTS forecasting is unfair, not only degrading their practical benefits but even incurring potential risk. Unfair MTS forecasting may be attributed to disparities relating to advantaged and disadvantaged variables, which has rarely been studied in the MTS forecasting. In this work, we formulate the MTS fairness modeling problem as learning informative representations attending to both advantaged and disadvantaged variables. Accordingly, we propose a novel framework, named FairFor , for fairness-aware MTS forecasting, i.e., fair MTS forecasting . FairFor uses adversarial learning to generate both group-irrelevant and -relevant representations for downstream forecasting. FairFor first adopts recurrent graph convolution to capture spatio-temporal variable correlations and to group variables by leveraging a spectral relaxation of the K-means objective. Then, it utilizes a novel filtering $ \&amp;amp; $ fusion module to filter group-relevant information and generate group-irrelevant representations by orthogonality regularization. The group-irrelevant and -relevant representations form highly informative representations, facilitating to share the knowledge from advantaged variables to disadvantaged variables and guarantee the fairness of forecasting. Extensive experiments on four public datasets demonstrate the FairFor effectiveness for fair forecasting and significant performance improvement.},
  archive      = {J_TKDE},
  author       = {Hui He and Qi Zhang and Shoujin Wang and Kun Yi and Zhendong Niu and Longbing Cao},
  doi          = {10.1109/TKDE.2023.3323956},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2504-2516},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning informative representation for fairness-aware multivariate time-series forecasting: A group-based perspective},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge graph-based behavior denoising and preference
learning for sequential recommendation. <em>TKDE</em>, <em>36</em>(6),
2490–2503. (<a href="https://doi.org/10.1109/TKDE.2023.3325666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommendation seeks to predict users’ next behaviors and recommend related items over time. Existing research has mainly focused on modeling users’ dynamic preferences from their sequential behaviors. However, most of these studies have ignored the negative effects of noise behaviors in the given sequences, which may mislead the recommender. In addition, users’ behavior data is always sparse, which makes it difficult to effectively learn users’ preferences purely from their historical behaviors. Most recently, knowledge graphs (KGs) have been exploited by few researchers for sequential recommendation. However, they always assume all information in KGs or KG paths with limited length are useful for recommendation, which may bring irrelevant information from KGs into the recommender and further mislead the recommender. To address these issues, we propose a novel KG-based behavior denoising and preference learning model named KGDPL for sequential recommendation. We argue that the paths in KGs that reflect semantic relations between entities can not only help to remove noise behaviors and recommend successive items for users, but also provide relevant explanations. Therefore, we first devise a supervised knowledge path selection module to select effective paths between items from KGs for behavior prediction, which aims to filter out irrelevant information from KGs for the given recommendation task. Then, we design a knowledge-enhanced behavior denoising module to mitigate the negative effects of the noise behaviors contained in historical sequences by using the knowledge path information. After that, we propose a knowledge-enhanced preference learning module to better learn users’ personalized and dynamic preferences from their historical behavior sequences and related knowledge information, which can also help tag users and provide explanations for recommendation results. Experimental results on four real-world datasets demonstrate the effectiveness and interpretability of the proposed model KGDPL.},
  archive      = {J_TKDE},
  author       = {Hongzhi Liu and Yao Zhu and Zhonghai Wu},
  doi          = {10.1109/TKDE.2023.3325666},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2490-2503},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Knowledge graph-based behavior denoising and preference learning for sequential recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FocusedCleaner: Sanitizing poisoned graphs for robust
GNN-based node classification. <em>TKDE</em>, <em>36</em>(6), 2476–2489.
(<a href="https://doi.org/10.1109/TKDE.2023.3322129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) are vulnerable to data poisoning attacks, which will generate a poisoned graph as the input to the GNN models. We present FocusedCleaner as a poisoned graph sanitizer to effectively identify the poison injected by attackers. Specifically, FocusedCleaner provides a sanitation framework consisting of two modules: bi-level structural learning and victim node detection. In particular, the structural learning module will reverse the attack process to steadily sanitize the graph while the detection module provides the “focus” – a narrowed and more accurate search region – to structural learning. These two modules will operate in iterations and reinforce each other to sanitize a poisoned graph step by step. As an important application, we show that the adversarial robustness of GNNs trained over the sanitized graph for the node classification task is significantly improved. Extensive experiments demonstrate that FocusedCleaner outperforms the state-of-the-art baselines both on poisoned graph sanitation and improving robustness.},
  archive      = {J_TKDE},
  author       = {Yulin Zhu and Liang Tong and Gaolei Li and Xiapu Luo and Kai Zhou},
  doi          = {10.1109/TKDE.2023.3322129},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2476-2489},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {FocusedCleaner: Sanitizing poisoned graphs for robust GNN-based node classification},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable legal case matching via graph optimal transport.
<em>TKDE</em>, <em>36</em>(6), 2461–2475. (<a
href="https://doi.org/10.1109/TKDE.2023.3321935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Providing human-understandable explanations for the matching predictions is still challenging for current legal case matching methods. One difficulty is that legal cases are semi-structured text documents with complicated case-case and case-law article correlations. To tackle the issue, we propose a novel graph optimal transport (GOT)-based legal case matching model that is able to provide not only the matching predictions but also plausible and faithful explanations for the prediction. The model, called GEIOT-Match, first constructs a heterogeneous graph to explicitly represent the semi-structured nature of legal cases and their associations with the law articles. Therefore, matching two legal cases amounts to identifying the rationales from the paired legal case sub-graphs in the heterogeneous graph and then aligning between them. An inverse optimal transport (IOT) model on graphs is learned to extract rationales from paired legal cases. The extracted rationales and the heterogeneous graph demonstrate the key legal characteristics of legal cases, which can be further used to conduct matching and generate explanations for the matching. Experimental results showed that GEIOT-Match outperformed state-of-the-art baselines in terms of matching prediction, rationale extraction, and natural language explanation generation.},
  archive      = {J_TKDE},
  author       = {Zhongxiang Sun and Weijie Yu and Zihua Si and Jun Xu and Zhenhua Dong and Xu Chen and Hongteng Xu and Ji-Rong Wen},
  doi          = {10.1109/TKDE.2023.3321935},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2461-2475},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Explainable legal case matching via graph optimal transport},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enabling efficient, verifiable, and secure conjunctive
keyword search in hybrid-storage blockchains. <em>TKDE</em>,
<em>36</em>(6), 2445–2460. (<a
href="https://doi.org/10.1109/TKDE.2023.3324128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain has emerged as a prevailing paradigm for decentralized applications due to its reliability and transparency. To scale up retrieval services, a common strategy is to use a hybrid storage model, where on-chain storage is responsible for small metadata and off-chain storage is for outsourced raw data. However, data security and result authenticity are ongoing challenges in this scenario, and little work has been done due to the difficulty of combining result verification and privacy preservation, especially for dynamic updates while supporting forward privacy. In this paper, we formally define the problem of efficient, verifiable, and secure conjunctive keyword search in hybrid-storage blockchains (vsChain) and propose a novel hybrid index that achieves efficient query and verification while supporting dynamic updates with forward privacy guarantee. We also design two optimized schemes to improve query and verification performance by using a partition-based method and an obfuscated counting Bloom filter mechanism. Finally, we provide a theoretical security analysis and empirical evaluations using real and synthetic datasets to demonstrate the feasibility of our proposed schemes.},
  archive      = {J_TKDE},
  author       = {Ningning Cui and Dong Wang and Jianxin Li and Huaijie Zhu and Xiaochun Yang and Jianliang Xu and Jie Cui and Hong Zhong},
  doi          = {10.1109/TKDE.2023.3324128},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2445-2460},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Enabling efficient, verifiable, and secure conjunctive keyword search in hybrid-storage blockchains},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient frequency-based randomization for spatial
trajectories under differential privacy. <em>TKDE</em>, <em>36</em>(6),
2430–2444. (<a href="https://doi.org/10.1109/TKDE.2023.3322471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The uniqueness of trajectory data for user re-identification has received unprecedented attention as the increasing popularity of location-based services boosts the excessive collection of daily trajectories with sufficient spatiotemporal coverage. Consequently, leveraging or releasing personally-sensitive trajectories without proper protection severely threatens individual privacy despite simply removing IDs. Trajectory privacy protection is never a trivial task due to the trade-off between privacy protection, utility preservation, and computational efficiency. Furthermore, recovery attack , one of the most threatening attacks specific to trajectory data, has not been well studied in the current literature. To tackle these challenges, we propose a frequency-based randomization model with a rigorous differential privacy guarantee for privacy-preserving trajectory data publishing. In particular, two randomized mechanisms are introduced for perturbing the local/global frequency distributions of a limited number of significantly essential locations in trajectories by injecting special Laplace noises. To reflect the perturbed distributions on the trajectory level without losing privacy guarantee or data utility, we formulate the trajectory modification tasks as kNN search problems and design two hierarchical indices with powerful pruning strategies and a novel search algorithm to support efficient modification. Extensive experiments on a real-world dataset verify the effectiveness of our approaches in resisting individual re-identification and recovery attacks simultaneously while still preserving desirable data utility. The efficient performance on large-scale data demonstrates the feasibility and scalability in practice.},
  archive      = {J_TKDE},
  author       = {Fengmei Jin and Wen Hua and Lei Li and Boyu Ruan and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2023.3322471},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2430-2444},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient frequency-based randomization for spatial trajectories under differential privacy},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CUBE: Causal intervention-based counterfactual explanation
for prediction models. <em>TKDE</em>, <em>36</em>(6), 2416–2429. (<a
href="https://doi.org/10.1109/TKDE.2023.3322126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent several years have witnessed the rapid explosion of artificial intelligence applied in various domains with the surpassing human-level performance. Despite the success, these models’ underlying mechanisms remain a mystery, as their complicated representations make human understanding impossible. This mystery may cause discrimination and non-robustness in prediction. Making deep learning models more transparent and understandable is gaining popularity, but most of interpretation approaches provide spurious correlations leading to suboptimal, incorrect or even biased interpretations, which could be reduced by causal explanations. Motivated by this, we attempt to study the generation of causal explanations and propose CUBE , a causal intervention-based counterfactual interpretation method. To ensure that the generation process of counterfactual explanation conforms to causality, we model the counterfactual generation process as a causal graph and construct a counterfactual generation model based on the causal intervention; to generate counterfactuals that adhere to the causality, we introduce a causal director to capture the causal relationships in the distribution and guide the generation of counterfactuals; to improve the efficiency of the counterfactual generation when facing a large number of explanation queries, we model it as a sample generation problem and propose an explainable framework based on adversarial generation. The experimental results validate that CUBE outperforms other approaches in terms of both lower time costs and higher explanation quality.},
  archive      = {J_TKDE},
  author       = {Xinyue Shao and Hongzhi Wang and Xiang Chen and Xiao Zhu and Yan Zhang},
  doi          = {10.1109/TKDE.2023.3322126},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2416-2429},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CUBE: Causal intervention-based counterfactual explanation for prediction models},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-domain recommendation via progressive structural
alignment. <em>TKDE</em>, <em>36</em>(6), 2401–2415. (<a
href="https://doi.org/10.1109/TKDE.2023.3324912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain recommendation, as a cutting-edge technology to settle data sparsity and cold start problems, is gaining increasingly popular. Existing research paradigms primarily focus on leveraging the representation of overlapping entities, such as representation aggregation or cross-domain consistency constraints, to facilitate knowledge transfer and enhance the performance of single-domain or dual-domain recommender systems. Even though these approaches bring significant promotions, they still suffer from optimization bottlenecks when faced with sparse overlapping users, which often occurs in reality. Unlocking the full potential of overlapping user information and exploring novel sources of cross-domain knowledge are pivotal in addressing this challenge effectively. On account of this, this paper proposes an innovative cross-domain recommendation framework, namely SEAGULL , to promote dual-target recommendation performance in line with these two perspectives. We bolster the utilization of overlapping user knowledge and extract non-overlapping user interests by refining the message passing mechanism in a unified heterogeneous cross-domain graph and facilitating the transfer of latent structural relationships among users. Specifically, we first construct the interaction of two domains as a unified cross-domain heterogeneous graph and design a novel attention mechanism to incorporate cross-domain collaboration signals between users and items. Second, we perform user structure alignment from global and local levels to extend semantic transfer and information augmentation. Finally, unlike previous work that directly incorporates mixed cross-domain knowledge, we employ a gentle and progressive cross-domain transfer strategy to reduce empirical risk loss. Extensive experiments on five tasks derived from three data sets fully demonstrate the effectiveness of SEAGULL .},
  archive      = {J_TKDE},
  author       = {Chuang Zhao and Hongke Zhao and Xiaomeng Li and Ming He and Jiahui Wang and Jianping Fan},
  doi          = {10.1109/TKDE.2023.3324912},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2401-2415},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cross-domain recommendation via progressive structural alignment},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Counterfactual explainable conversational recommendation.
<em>TKDE</em>, <em>36</em>(6), 2388–2400. (<a
href="https://doi.org/10.1109/TKDE.2023.3322403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational Recommender Systems (CRSs) fundamentally differ from traditional recommender systems by interacting with users in a conversational session to accurately predict their current preferences and provide personalized recommendations. Although current CRSs have achieved favorable recommendation performance, the explainability is still in its infancy stage. Most of the CRSs tend to provide coarse explanations and fail to explore the impact of minimal alterations on the recommendation decisions on items. In this paper, we are the first to incorporate the counterfactual techniques into CRS and propose a C ounterfactual E xplainable C onversational R ecommender (CECR) to enhance the recommendation model from a counterfactual perspective. Counterfactual explanations can offer fine-grained reasons to explain users’ real-time intentions, meanwhile generating counterfactual samples for augmenting the training dataset to enhance recommendation performance. Specifically, CECR adaptively learns users’ preferences based on the conversation context and effectively responds to users’ real-time feedback during multiple rounds of conversation. Furthermore, CECR actively generates counterfactual samples to augment the training set and thus leading to a constant improvement in recommendation performance. Empirical experiments carried out on three benchmark datasets show that our CECR outperforms state-of-the-art CRSs in terms of recommendation performance and explainability.},
  archive      = {J_TKDE},
  author       = {Dianer Yu and Qian Li and Xiangmeng Wang and Qing Li and Guandong Xu},
  doi          = {10.1109/TKDE.2023.3322403},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2388-2400},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Counterfactual explainable conversational recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). COPP-miner: Top-k contrast order-preserving pattern mining
for time series classification. <em>TKDE</em>, <em>36</em>(6),
2372–2387. (<a href="https://doi.org/10.1109/TKDE.2023.3321749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, order-preserving pattern (OPP) mining, a new sequential pattern mining method, has been proposed to mine frequent relative orders in a time series. Although frequent relative orders can be used as features to classify a time series, the mined patterns do not reflect the differences between two classes of time series well. To effectively discover the differences between time series, this paper addresses the top- k contrast OPP (COPP) mining and proposes a COPP-Miner algorithm to discover the top- k contrast patterns as features for time series classification, avoiding the problem of improper parameter setting. COPP-Miner is composed of three parts: extreme point extraction to reduce the length of the original time series, forward mining, and reverse mining to discover COPPs. Forward mining contains three steps: group pattern fusion strategy to generate candidate patterns, the support rate calculation method to efficiently calculate the support of a pattern, and two pruning strategies to further prune candidate patterns. Reverse mining uses one pruning strategy to prune candidate patterns and consists of applying the same process as forward mining. Experimental results validate the efficiency of the proposed algorithm and show that top- k COPPs can be used as features to obtain a better classification performance.},
  archive      = {J_TKDE},
  author       = {Youxi Wu and Yufei Meng and Yan Li and Lei Guo and Xingquan Zhu and Philippe Fournier-Viger and Xindong Wu},
  doi          = {10.1109/TKDE.2023.3321749},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2372-2387},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {COPP-miner: Top-k contrast order-preserving pattern mining for time series classification},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AutoAlign: Fully automatic and effective knowledge graph
alignment enabled by large language models. <em>TKDE</em>,
<em>36</em>(6), 2357–2371. (<a
href="https://doi.org/10.1109/TKDE.2023.3325484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of entity alignment between knowledge graphs (KGs) aims to identify every pair of entities from two different KGs that represent the same entity. Many machine learning-based methods have been proposed for this task. However, to our best knowledge, existing methods all require manually crafted seed alignments, which are expensive to obtain. In this paper, we propose the first fully automatic alignment method named AutoAlign, which does not require any manually crafted seed alignments. Specifically, for predicate embeddings, AutoAlign constructs a predicate-proximity-graph with the help of large language models to automatically capture the similarity between predicates across two KGs. For entity embeddings, AutoAlign first computes the entity embeddings of each KG independently using TransE, and then shifts the two KGs’ entity embeddings into the same vector space by computing the similarity between entities based on their attributes. Thus, both predicate alignment and entity alignment can be done without manually crafted seed alignments. AutoAlign is not only fully automatic, but also highly effective. Experiments using real-world KGs show that AutoAlign improves the performance of entity alignment significantly compared to state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Rui Zhang and Yixin Su and Bayu Distiawan Trisedya and Xiaoyan Zhao and Min Yang and Hong Cheng and Jianzhong Qi},
  doi          = {10.1109/TKDE.2023.3325484},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2357-2371},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {AutoAlign: Fully automatic and effective knowledge graph alignment enabled by large language models},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A trajectory-oriented locality-sensitive hashing method for
user identification. <em>TKDE</em>, <em>36</em>(6), 2343–2356. (<a
href="https://doi.org/10.1109/TKDE.2023.3324427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User identification across social sites, which benefits many applications, has recently been attracting considerable attention. Most existing methods focused more on the effectiveness of user identification, rather than on efficiency. Matching as many cross-site user accounts as possible, which causes very high computation overhead posed by the full-scale pairwise comparisons, remains unsolved, especially when the number of users reaches tens of millions or more. To address this issue, we present a novel l ocality- s ensitive h ashing method for u ser i dentification (UI-LSH), which consists of four components. 1) It involves embedding stay points into vectors, 2) and constructing locality-sensitive hashing families suitable for stay points. 3) It presents a method for projecting stay points into hash buckets that ensures the close stay points are placed in the same bucket with high probability. 4) It constructs the candidate user pairs based on the projection results. The experiments on three ground-truth datasets show that our method reduces the number of user pairs to be compared by as much as 81.87%, 67.68%, and 63.15%, respectively. Overall, UI-LSH holds great promise for significantly improving the efficiency of user identification.},
  archive      = {J_TKDE},
  author       = {Yongjun Li and Xiangyu Li and Wenli Ji},
  doi          = {10.1109/TKDE.2023.3324427},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2343-2356},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A trajectory-oriented locality-sensitive hashing method for user identification},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A robust database watermarking scheme that preserves
statistical characteristics. <em>TKDE</em>, <em>36</em>(6), 2329–2342.
(<a href="https://doi.org/10.1109/TKDE.2023.3324932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Database watermarking can be used for copyright verification and leakage traceability, effectively protecting the security of the database. However, the existing watermarking schemes commonly embed watermarks by modifying the original data, which changes the statistical characteristics and affects the statistical analysis of the database. Therefore, this paper proposes SCPW, a S tatistical C haracteristics P reserving robust database W atermarking framework. First, we perform a theoretical analysis and propose a data modification scheme maintaining the statistical characteristics unchanged. Then, we establish the correspondence between the data and the watermarks that need to be embedded in it by grouping. Finally, the watermark message is embedded into the database through data verification and modification. Specifically, for data that needs to be watermarked, we first verify whether the potential watermark bits extracted from the data are the same as bits that need to be embedded. If they are the same, we regard this original data, usually a floating point number, as a “good number” and do not modify it. Otherwise, we modify the data until it becomes a “good number” using a data modification scheme that preserves the statistical characteristics proposed by the theoretical analysis. In addition, we also use the genetic algorithm to optimize the grouping results and increase the proportion of “good number”, thereby reducing the proportion of data that needs to be modified and further reducing distortion. To our best knowledge, SCPW is the first watermarking scheme that ensures the preservation of statistical characteristics, and the experimental results also prove its excellent ability to preserve statistical characteristics compared to existing schemes. Moreover, experiments also illustrate that our method is robust against a wide range of attacks. When under deletion attack (deletion rate = 90%), the bit error rate of watermark extraction is only 0.8%, which is more than 12% lower than the current best method.},
  archive      = {J_TKDE},
  author       = {Zhiwen Ren and Han Fang and Jie Zhang and Zehua Ma and Ronghao Lin and Weiming Zhang and Nenghai Yu},
  doi          = {10.1109/TKDE.2023.3324932},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2329-2342},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A robust database watermarking scheme that preserves statistical characteristics},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A more context-aware approach for textual adversarial
attacks using probability difference-guided beam search. <em>TKDE</em>,
<em>36</em>(6), 2316–2328. (<a
href="https://doi.org/10.1109/TKDE.2023.3325315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Textual adversarial attacks expose the vulnerabilities of text classifiers and can be used to improve their robustness. Previous context-aware attack models suffer from several limitations. They generally rely on out-of-date substitutes, solely consider the gold label probability, and use the greedy search when generating adversarial examples, often limiting the attack efficiency. To tackle these issues, we propose MC-PDBS , a M ore C ontext-aware textual adversarial attack model using P robability D ifference-guided B eam S earch. MC-PDBS generates substitutes using the newest perturbed text sequences in each attack iteration, enabling the generation of more context-aware adversarial examples. The probability difference is an overall consideration of the probabilities of all class labels, which is more effective than the gold label probability in guiding the selection of attack paths. In addition, the beam search enables MC-PDBS to search attack paths from multiple search channels, thereby avoiding the limited search space problem. Extensive experiments and human evaluation demonstrate that MC-PDBS outperforms previous best models in a series of evaluation metrics, particularly bringing up to a +19.5% attack success rate. Extensive analyses further confirm the effectiveness of MC-PDBS.},
  archive      = {J_TKDE},
  author       = {Huijun Liu and Bin Ji and Jie Yu and Shasha Li and Jun Ma and Zibo Yi and Mengxue Du and Miaomiao Li and Jie Liu and Zeyao Mo},
  doi          = {10.1109/TKDE.2023.3325315},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2316-2328},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A more context-aware approach for textual adversarial attacks using probability difference-guided beam search},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fast learned key-value store for concurrent and
distributed systems. <em>TKDE</em>, <em>36</em>(6), 2301–2315. (<a
href="https://doi.org/10.1109/TKDE.2023.3327009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient key-value (KV) store becomes important for concurrent and distributed systems to deliver high performance. The promising learned indexes leverage deep-learning models to complement existing KV stores and obtain significant performance improvements. However, existing schemes show limited scalability in concurrent systems due to containing high dependency among data. The practical system performance decreases when inserting a large amount of new data due to triggering frequent and inefficient retraining operations. Moreover, existing learned indexes become inefficient in distributed systems, since different machines incur high overheads to guarantee the data consistency when the index structures dynamically change. To address these problems in concurrent and distributed systems, we propose a fine-grained learned index scheme with high scalability, called FineStore, which constructs independent models with a flattened data structure under the trained data array to concurrently process the requests with low overheads. FineStore processes the new requests in-place with the support of non-blocking retraining, hence adapting to the new distributions without blocking the systems. In the distributed systems, different machines efficiently leverage the extended RCU barrier to guarantee the data consistency. We evaluate FineStore via YCSB and real-world datasets, and extensive experimental results demonstrate that FineStore improves the performance respectively by up to 1.8× and 2.5× than state-of-the-art XIndex and Masstree. We have released the open-source codes of FineStore for public use in GitHub.},
  archive      = {J_TKDE},
  author       = {Pengfei Li and Yu Hua and Jingnan Jia and Pengfei Zuo},
  doi          = {10.1109/TKDE.2023.3327009},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {6},
  number       = {6},
  pages        = {2301-2315},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A fast learned key-value store for concurrent and distributed systems},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VeffChain: Enabling freshness authentication of rich queries
over blockchain databases. <em>TKDE</em>, <em>36</em>(5), 2285–2300. (<a
href="https://doi.org/10.1109/TKDE.2023.3316127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the wide adoption of blockchains in data-intensive applications, enabling verifiable queries over a blockchain database is urgently required. Aiming at reducing costs, previous solutions embed a small-sized authenticated data structure (ADS) in each block header, so that a user can verify search results without maintaining a full copy of blockchain databases. However, existing studies focus on exact queries with difficulty to guarantee the freshness of search results. In this article, we propose two frameworks, called $\mathsf{veffChain}$ and $\mathsf{veffChain++}$ , to realize freshness authentication of rich queries over blockchain databases. Specifically, $\mathsf{veffChain}$ concerns about verifiable latest- $K$ exact queries and employs RSA accumulator to generate constant-size ADSs; $\mathsf{veffChain++}$ integrates RSA accumulator into the Trie tree to further authenticate latest- $K$ fuzzy queries. For improved scalability, an adaptive keyword splitting (AKS) solution is proposed to enable ADSs to be incrementally updated. Compared with the state-of-the-art work, our frameworks have the following merits: (1) Freshness Guarantee . The user can efficiently retrieve the freshest data from a blockchain database in a verifiable way. (2) Flexibility . The user can specify different query patterns on demand to retrieve data as accurately as possible. The detailed security analysis and extensive experiments validate the practicality of our frameworks.},
  archive      = {J_TKDE},
  author       = {Qin Liu and Yu Peng and Ziyi Tang and Hongbo Jiang and Jie Wu and Tian Wang and Tao Peng and Guojun Wang},
  doi          = {10.1109/TKDE.2023.3316127},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {2285-2300},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {VeffChain: Enabling freshness authentication of rich queries over blockchain databases},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variational counterfactual prediction under runtime domain
corruption. <em>TKDE</em>, <em>36</em>(5), 2271–2284. (<a
href="https://doi.org/10.1109/TKDE.2023.3321893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To date, various neural methods have been proposed for causal effect estimation based on observational data, where a default assumption is the same distribution and availability of variables at both training and inference (i.e., runtime) stages. However, distribution shift (i.e., domain shift) could happen during runtime, and bigger challenges arise from the impaired accessibility of variables. This is commonly caused by increasing privacy and ethical concerns, which can make arbitrary variables unavailable in the entire runtime data and imputation impractical. We term the co-occurrence of domain shift and inaccessible variables runtime domain corruption , which seriously impairs the generalizability of a trained counterfactual predictor. To counter runtime domain corruption, we subsume counterfactual prediction under the notion of domain adaptation. Specifically, we upper-bound the error w.r.t. the target domain (i.e., runtime covariates) by the sum of source domain error and inter-domain distribution distance. In addition, we build an adversarially unified variational causal effect model, named VEGAN, with a novel two-stage adversarial domain adaptation scheme to reduce the latent distribution disparity between treated and control groups first, and between training and runtime variables afterwards. We demonstrate that VEGAN outperforms other state-of-the-art baselines on individual-level treatment effect estimation in the presence of runtime domain corruption on benchmark datasets.},
  archive      = {J_TKDE},
  author       = {Hechuan Wen and Tong Chen and Li Kheng Chai and Shazia Sadiq and Junbin Gao and Hongzhi Yin},
  doi          = {10.1109/TKDE.2023.3321893},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {2271-2284},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Variational counterfactual prediction under runtime domain corruption},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised detection of behavioural drifts with dynamic
clustering and trajectory analysis. <em>TKDE</em>, <em>36</em>(5),
2257–2270. (<a href="https://doi.org/10.1109/TKDE.2023.3320184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time monitoring of human behaviours, especially in e-Health applications, has been an active area of research in the past decades. On top of IoT-based sensing environments, anomaly detection algorithms have been proposed for the early detection of abnormalities. Gradual change procedures, commonly referred to as drift anomalies, have received much less attention in the literature because they represent a much more challenging scenario than sudden temporary changes (point anomalies). In this article, we propose, for the first time, a fully unsupervised real-time drift detection algorithm named DynAmo, which can identify drift periods as they are happening. DynAmo comprises a dynamic clustering component to capture the overall trends of monitored behaviours and a trajectory generation component, which extracts features from the densest cluster centroids. Finally, we apply an ensemble of divergence tests on sliding reference and detection windows to detect drift periods in the behavioural sequence.},
  archive      = {J_TKDE},
  author       = {Bardh Prenkaj and Paola Velardi},
  doi          = {10.1109/TKDE.2023.3320184},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {2257-2270},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Unsupervised detection of behavioural drifts with dynamic clustering and trajectory analysis},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards a unified understanding of uncertainty
quantification in traffic flow forecasting. <em>TKDE</em>,
<em>36</em>(5), 2239–2256. (<a
href="https://doi.org/10.1109/TKDE.2023.3312261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty is an essential consideration for time series forecasting tasks. In this work, we focus on quantifying the uncertainty of traffic forecasting from a unified perspective. We develop a novel traffic forecasting framework, namely Deep Spatio-Temporal Uncertainty Quantification (DeepSTUQ), which can estimate both aleatoric and epistemic uncertainty. Specifically, we first leverage a spatio-temporal model to model the complex spatio-temporal correlations of traffic data. Subsequently, two independent sub-neural networks maximizing the heterogeneous log-likelihood are developed to estimate aleatoric uncertainty. To estimate epistemic uncertainty, we combine the merits of variational inference and deep ensembling by integrating the Monte Carlo dropout and the Adaptive Weight Averaging re-training methods, respectively. Furthermore, to relax the Gaussianity assumption, mitigate overfitting, and improve horizon-wise uncertainty quantification performance, we define a new calibration method called Multi-horizon Conformal Calibration (MHCC). Finally, we provide a theoretical analysis of the proposed unified approach based on the PAC-Bayes theory. Extensive experiments are conducted on four public datasets, and the empirical results suggest that the proposed method outperforms state-of-the-art methods in terms of both point prediction and uncertainty quantification.},
  archive      = {J_TKDE},
  author       = {Weizhu Qian and Yan Zhao and Dalin Zhang and Bowei Chen and Kai Zheng and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2023.3312261},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {2239-2256},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards a unified understanding of uncertainty quantification in traffic flow forecasting},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TED<span class="math inline"><sup>+</sup></span>+: Towards
discovering top-k edge-diversified patterns in a graph database.
<em>TKDE</em>, <em>36</em>(5), 2224–2238. (<a
href="https://doi.org/10.1109/TKDE.2023.3312566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With an exponentially growing number of graphs from disparate repositories, there is a strong need to analyze a graph database containing an extensive collection of small- or medium-sized data graphs (e.g., chemical compounds). Although subgraph enumeration and subgraph mining have been proposed to bring insights into a graph database by a set of subgraph structures, they often end up with similar or homogenous topologies, which is undesirable in many graph applications. To address this limitation, we propose the Top-k Edge-Diversified Patterns Discovery problem to retrieve a set of subgraphs that cover the maximum number of edges in a database. To efficiently process such query, we present a generic and extensible framework called $\textsc {Ted}^+$ which achieves a guaranteed approximation ratio to the optimal result. Three optimization strategies are further developed to improve the performance, and a lightweight version called TedLite is designed for even larger graph databases. Experimental studies on real-world datasets demonstrate the superiority of $\textsc {Ted}^+$ to traditional techniques.},
  archive      = {J_TKDE},
  author       = {Kai Huang and Yue Cui and Qingqing Ye and Yan Zhao and Xi Zhao and Yao Tian and Kai Zheng and Haibo Hu and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2023.3312566},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {2224-2238},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {TED$^+$+: Towards discovering top-k edge-diversified patterns in a graph database},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tackling long-tailed distribution issue in graph neural
networks via normalization. <em>TKDE</em>, <em>36</em>(5), 2213–2223.
(<a href="https://doi.org/10.1109/TKDE.2023.3315284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have attracted much attention due to their superior learning capability. Despite the successful applications of GNNs in many areas, their performance suffers heavily from the long-tailed node degree distribution. Most prior studies tackle this issue by devising sophisticated model architectures. In this article, we aim to improve the performance of tail nodes (low-degree or hard-to-classify nodes) via a generic and light normalization method. In detail, we propose a novel normalization method for GNNs, termed as ResNorm, which Res hapes a long-tailed distribution into a normal-like distribution via Norm alization. The ResNorm includes two operators. First, the scale operator reshapes the distribution of the node-wise standard deviation (NStd) so as to improve the accuracy of tail nodes. Second, the analysis of the behavior of the standard shift indicates that the standard shift serves as a preconditioner on the weight matrix, increasing the risk of over-smoothing. To address this issue, we design a new shift operator for ResNorm, which simulates the degree-specific parameter strategy in a low-cost manner. Extensive experiments on various node classification benchmark datasets have validated the effectiveness of ResNorm in improving the performance of tail nodes as well as the overall performance.},
  archive      = {J_TKDE},
  author       = {Langzhang Liang and Zenglin Xu and Zixing Song and Irwin King and Yuan Qi and Jieping Ye},
  doi          = {10.1109/TKDE.2023.3315284},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {2213-2223},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Tackling long-tailed distribution issue in graph neural networks via normalization},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatio-temporal trajectory similarity measures: A
comprehensive survey and quantitative study. <em>TKDE</em>,
<em>36</em>(5), 2191–2212. (<a
href="https://doi.org/10.1109/TKDE.2023.3323535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatio-temporal trajectory analytics are useful in diversified applications such as urban planning, infrastructure development, and vehicular networks. Trajectory similarity measure, which aims to evaluate the distance between two trajectories, is a fundamental functionality of trajectory analytics. In this paper, we propose a comprehensive survey that investigates all the most common and representative spatio-temporal trajectory measures. First, we provide an overview of spatio-temporal trajectory measures in terms of three hierarchical perspectives: Non-learning versus Learning, Free Space versus Road Network, and Standalone versus Distributed. Next, we present an evaluation benchmark by designing five real-world transformation scenarios. Based on this benchmark, extensive experiments are conducted to study the effectiveness, robustness, efficiency, and scalability of each measure, which offers guidelines for trajectory measure selection among multiple techniques and applications such as trajectory data mining, deep learning, and distributed processing. Specifically, i) Effectiveness: In terms of trajectory length, DFD and Seg-Frechet are length-sensitive, while OWD and Hausdorff always return same results when varying query trajectory length. In terms of trajectory shape, LCRS and LORS are able to effectively find similar trajectories for query trajectories with different shapes; ii) Robustness: Learning based measures are more robust compared with non-learning based ones. Among non-learning based measures, DFD, Hausdorff, OWD and Seg-Frechet are relatively non-sensitive to noises and different sampling rates; and iii) Efficiency&amp; Scalability: Compared to non-learning based measures, learning based and distributed based measures are more efficient and scalable.},
  archive      = {J_TKDE},
  author       = {Danlei Hu and Lu Chen and Hanxi Fang and Ziquan Fang and Tianyi Li and Yunjun Gao},
  doi          = {10.1109/TKDE.2023.3323535},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {2191-2212},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Spatio-temporal trajectory similarity measures: A comprehensive survey and quantitative study},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rule-guided counterfactual explainable recommendation.
<em>TKDE</em>, <em>36</em>(5), 2179–2190. (<a
href="https://doi.org/10.1109/TKDE.2023.3322227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To empower the trust of current recommender systems, the counterfactual explanation (CE) method is adopted to generate the counterfactual instance for each input and take their changes causing the different outcomes as the explanation. Although promising results have been achieved by existing CE-based methods, we propose to generate the attribute-oriented counterfactual explanation. Different from them, we aim to generate the counterfactual instance by performing the intervention on the attributes, and then build an attribute-oriented counterfactual explainable recommender system. Considering the correlation and categorical values of attributes, how to efficiently generate the reliable counterfactual instances on the attributes challenges us. To alleviate such a problem, we propose to extract the decision rules over the attributes to guide the attribute-oriented counterfactual generation. Specifically, we adopt the gradient boosting decision tree (GBDT) to pre-build the decision rules over the attributes and develop a Rule-guided Counterfactual Explainable Recommendation model ( RCER ) to predict the user-item interaction and generate the counterfactual instances for the user-item pairs. We finally conduct extensive experiments on four publicly datasets, including NYC, LON, Amazon, and Movielens datasets. Experimental results have qualitatively and quantitatively justified the superiority of our model over existing cutting-edge baselines.},
  archive      = {J_TKDE},
  author       = {Yinwei Wei and Xiaoyang Qu and Xiang Wang and Yunshan Ma and Liqiang Nie and Tat-Seng Chua},
  doi          = {10.1109/TKDE.2023.3322227},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {2179-2190},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Rule-guided counterfactual explainable recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revisiting adversarial attacks on graph neural networks for
graph classification. <em>TKDE</em>, <em>36</em>(5), 2166–2178. (<a
href="https://doi.org/10.1109/TKDE.2023.3313059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have achieved tremendous success in the task of graph classification and its diverse downstream real-world applications. Despite the huge success in learning graph representations, current GNN models have demonstrated their vulnerability to potentially existent adversarial examples on graph-structured data. Existing approaches are either limited to structure attacks or restricted to local information, urging for the design of a more general attack framework on graph classification, which faces significant challenges due to the complexity of generating local-node-level adversarial examples using the global-graph-level information. To address this “global-to-local” attack challenge, we present a novel and general framework CAMA to generate adversarial examples via manipulating graph structure and node features. Specifically, we make use of Graph Class Activation Mapping and its variant to produce node-level importance corresponding to the graph classification task. Then through a heuristic design of algorithms, we can perform both feature and structure attacks under unnoticeable perturbation budgets with the help of both node-level and subgraph-level importance. Experiments towards attacking four state-of-the-art graph classification models on six real-world benchmarks verify the flexibility and effectiveness of our framework.},
  archive      = {J_TKDE},
  author       = {Xin Wang and Heng Chang and Beini Xie and Tian Bian and Shiji Zhou and Daixin Wang and Zhiqiang Zhang and Wenwu Zhu},
  doi          = {10.1109/TKDE.2023.3313059},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {2166-2178},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Revisiting adversarial attacks on graph neural networks for graph classification},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reconnecting the estranged relationships: Optimizing the
influence propagation in evolving networks. <em>TKDE</em>,
<em>36</em>(5), 2151–2165. (<a
href="https://doi.org/10.1109/TKDE.2023.3316268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence Maximization (IM), which aims to select a set of users from a social network to maximize the expected number of influenced users, has recently received significant attention for mass communication and commercial marketing. Existing research efforts dedicated to the IM problem depend on a strong assumption: the selected seed users are willing to spread the information after receiving benefits from a company or organization. In reality, however, some seed users may be reluctant to spread the information or need to be paid higher to be motivated. Furthermore, the existing IM works pay little attention to capture users’ influence propagation in the future period. In this paper, we target a new research problem named, Reconnecting Top-$l$l Relationships (RT $l$ R) query, which aims to find $l$ number of previous existing relationships but being estranged later such that reconnecting these relationships will maximize the expected number of influenced users by the given group in a future period. We prove that the RT $l$ R problem is NP-hard. An efficient greedy algorithm is proposed to answer the RT $l$ R queries with the influence estimation technique and the well-chosen link prediction method to predict the near future network structure. We also design a pruning method to reduce unnecessary probing from candidate edges. Further, a carefully designed order-based algorithm is proposed to accelerate the RT $l$ R queries. Finally, we conduct extensive experiments on real-world datasets to demonstrate the effectiveness and efficiency of our proposed methods.},
  archive      = {J_TKDE},
  author       = {Taotao Cai and Qi Lei and Quan Z. Sheng and Ningning Cui and Shuiqiao Yang and Jian Yang and Wei Emma Zhang and Adnan Mahmood},
  doi          = {10.1109/TKDE.2023.3316268},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {2151-2165},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Reconnecting the estranged relationships: Optimizing the influence propagation in evolving networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy-preserving constrained domain generalization via
gradient alignment. <em>TKDE</em>, <em>36</em>(5), 2142–2150. (<a
href="https://doi.org/10.1109/TKDE.2023.3315279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNN) have demonstrated unprecedented success for various applications. However, due to the issue of limited dataset availability and the strict legal and ethical requirements for data privacy protection, the broad applications of DNN (e.g., medical imaging classification) with large-scale training data have been largely hindered, greatly constraining the model generalization capability. In this paper, we aim to tackle this problem by developing the privacy-preserving constrained domain generalization method, aiming to improve the generalization capability under the privacy-preserving condition. In particular, we propose to improve the information aggregation process on the centralized server side with a novel gradient alignment loss, expecting that the trained model can be better generalized to the “unseen” but related data. The rationale and effectiveness of our proposed method can be explained by connecting our proposed method with the Maximum Mean Discrepancy (MMD) which has been widely adopted as the distribution distance measure. Experimental results on three domain generalization benchmark datasets indicate that our method can achieve better cross-domain generalization capability compared to the state-of-the-art federated learning methods.},
  archive      = {J_TKDE},
  author       = {Chris Xing Tian and Haoliang Li and Yufei Wang and Shiqi Wang},
  doi          = {10.1109/TKDE.2023.3315279},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {2142-2150},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Privacy-preserving constrained domain generalization via gradient alignment},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting human mobility via self-supervised
disentanglement learning. <em>TKDE</em>, <em>36</em>(5), 2126–2141. (<a
href="https://doi.org/10.1109/TKDE.2023.3317175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have recently achieved considerable improvements in learning human behavioral patterns and individual preferences from massive spatial-temporal trajectory data. However, most of the existing research concentrates on fusing different semantics underlying sequential trajectories for mobility pattern learning which, in turn, yields a narrow perspective on comprehending human intrinsic motions. In addition, the inherent sparsity and under-explored heterogeneous collaborative items pertaining to human check-ins hinder the potential exploitation of human diverse periodic regularities as well as common interests. Motivated by recent advances in disentanglement learning, we propose a novel disentangled solution called SSDL for tackling the next POI prediction problem. SSDL primarily seeks to disentangle the potential time-invariant and time-varying factors into different latent spaces from massive trajectories, providing an interpretable view to understand the intricate semantics underlying human diverse mobility representations. To address the data sparsity issue, we present two realistic trajectory augmentation approaches to enhance the understanding of both the human intrinsic periodicity/habits and constantly-changing intents. In addition, we devise a POI-centric graph structure to explore heterogeneous collaborative signals underlying historical check-ins. Extensive experiments conducted on four real-world datasets demonstrate that SSDL significantly outperforms the state-of-the-art approaches–for example, it yields up to 8.57% averaged improvement on ACC@1.},
  archive      = {J_TKDE},
  author       = {Qiang Gao and Jinyu Hong and Xovee Xu and Ping Kuang and Fan Zhou and Goce Trajcevski},
  doi          = {10.1109/TKDE.2023.3317175},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {2126-2141},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Predicting human mobility via self-supervised disentanglement learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Penalized flow hypergraph local clustering. <em>TKDE</em>,
<em>36</em>(5), 2110–2125. (<a
href="https://doi.org/10.1109/TKDE.2023.3319019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, hypergraph analysis have attracted increasing attention due to their ability to model complex data correlation, with hypergraph clustering being one of the most important tasks. However, when the scale of hypergraph is large enough, clustering is difficult based on global consistency. Existing flow-based hypergraph local clustering methods have good theoretical cut improvements and runtime guarantees. However, these methods exhibit poor performance when the initial reference node set is small and are prone to causing the output set to shrink into a small subset, resulting in local minima. To address this issue, we propose the Penalized Flow Hypergraph Local Clustering(PFHLC) and provide new conductance guarantees and runtime analyses for our method. First, we use the random walk method to grow the initial seed set, and introduce the random walk information of nodes as penalized flow into the flow-based framework to optimize the output. Second, we propose a generalized objective function containing random walk information, which takes full advantage of the semi-supervised information of the target cluster to protect important nodes. This feature can avoid the local minima of previous flow-based methods. Importantly, our method is strongly-local and can run efficiently on large-scale hypergraphs. We contribute a real-world dataset and the experiments on real-world large-scale datasets show that PFHLC achieves the state-of-the-art significantly.},
  archive      = {J_TKDE},
  author       = {Hao Zhong and Yubo Zhang and Chenggang Yan and Zuxing Xuan and Ting Yu and Ji Zhang and Shihui Ying and Yue Gao},
  doi          = {10.1109/TKDE.2023.3319019},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {2110-2125},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Penalized flow hypergraph local clustering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Partial clustering ensemble. <em>TKDE</em>, <em>36</em>(5),
2096–2109. (<a href="https://doi.org/10.1109/TKDE.2023.3321913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering ensemble often provides robust and stable results without accessing original features of data, and thus has been widely studied. The conventional clustering ensemble methods often take the full multiple base partitions as inputs and provide a consensus clustering result. However, in many real-world applications, full base partitions are hard to obtain because some data may be missing in some base partitions. To tackle this problem, in this paper, we propose a novel partial clustering ensemble method, which takes the partial multiple base partitions as inputs. In this method, we simultaneously fill the missing values in the base partitions and ensemble them by fully considering the consensus and diversity. Moreover, to address the unreliability issue in the partial data scenario, we seamlessly plug it into a self-paced learning framework. The extensive experiments on benchmark data sets demonstrate the effectiveness and efficiency of the proposed method when handling incomplete data.},
  archive      = {J_TKDE},
  author       = {Peng Zhou and Liang Du and Xinwang Liu and Zhaolong Ling and Xia Ji and Xuejun Li and Yi-Dong Shen},
  doi          = {10.1109/TKDE.2023.3321913},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {2096-2109},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Partial clustering ensemble},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Outlier detection using three-way neighborhood
characteristic regions and corresponding fusion measurement.
<em>TKDE</em>, <em>36</em>(5), 2082–2095. (<a
href="https://doi.org/10.1109/TKDE.2023.3312108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outliers carry significant information to reflect an anomaly mechanism, so outlier detection facilitates relevant data mining. In terms of outlier detection, the classical approaches from distances apply to numerical data rather than nominal data, while the recent methods on basic rough sets deal with nominal data rather than numerical data. Aiming at wide outlier detection on numerical, nominal, and hybrid data, this paper investigates three-way neighborhood characteristic regions and corresponding fusion measurement to advance outlier detection. First, neighborhood rough sets are deepened via three-way decision, so they derive three-way neighborhood structures on model boundaries, inner regions, and characteristic regions. Second, the three-way neighborhood characteristic regions motivate the information fusion and weight measurement regarding all features, and thus, a multiple neighborhood outlier factor emerges to establish a new method of outlier detection; furthermore, a relevant outlier detection algorithm (called 3WNCROD, available at https://github.com/BELLoney/3WNCROD ) is designed to comprehensively process numerical, nominal, and mixed data. Finally, the 3WNCROD algorithm is experimentally validated, and it generally outperforms 13 contrast algorithms to perform better for outlier detection.},
  archive      = {J_TKDE},
  author       = {Xianyong Zhang and Zhong Yuan and Duoqian Miao},
  doi          = {10.1109/TKDE.2023.3312108},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {2082-2095},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Outlier detection using three-way neighborhood characteristic regions and corresponding fusion measurement},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neighborhood-enhanced supervised contrastive learning for
collaborative filtering. <em>TKDE</em>, <em>36</em>(5), 2069–2081. (<a
href="https://doi.org/10.1109/TKDE.2023.3317068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While effective in recommendation tasks, collaborative filtering (CF) techniques face the challenge of data sparsity. Researchers have begun leveraging contrastive learning to introduce additional self-supervised signals to address this. However, this approach often unintentionally distances the target user/item from their collaborative neighbors, limiting its efficacy. In response, we propose a solution that treats the collaborative neighbors of the anchor node as positive samples within the final objective loss function. This paper focuses on developing two unique supervised contrastive loss functions that effectively combine supervision signals with contrastive loss. We analyze our proposed loss functions through the gradient lens, demonstrating that different positive samples simultaneously influence updating the anchor node&#39;s embeddings. These samples’ impact depends on their similarities to the anchor node and the negative samples. Using the graph-based collaborative filtering model as our backbone and following the same data augmentation methods as the existing contrastive learning model SGL, we effectively enhance the performance of the recommendation model. Our proposed Neighborhood-Enhanced Supervised Contrastive Loss ( NESCL ) model substitutes the contrastive loss function in SGL with our novel loss function, showing marked performance improvement. On three real-world datasets, Yelp2018, Gowalla, and Amazon-Book, our model surpasses the original SGL by 10.09%, 7.09%, and 35.36% on NDCG@20, respectively.},
  archive      = {J_TKDE},
  author       = {Peijie Sun and Le Wu and Kun Zhang and Xiangzhi Chen and Meng Wang},
  doi          = {10.1109/TKDE.2023.3317068},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {2069-2081},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Neighborhood-enhanced supervised contrastive learning for collaborative filtering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-scale attention flow for probabilistic time series
forecasting. <em>TKDE</em>, <em>36</em>(5), 2056–2068. (<a
href="https://doi.org/10.1109/TKDE.2023.3319672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The probability prediction of multivariate time series is a notoriously challenging but practical task. On the one hand, the challenge is how to effectively capture the cross-series correlations between interacting time series, to achieve accurate distribution modeling. On the other hand, we should consider how to capture the contextual information within time series more accurately to model multivariate temporal dynamics of time series. In this work, we proposed a novel non-autoregressive deep learning model, called Multi-scale Attention Normalizing Flow(MANF), where we combine multi-scale attention with relative position information and the multivariate data distribution is represented by the conditioned normalizing flow. Additionally, compared with autoregressive modeling methods, our model avoids the influence of cumulative error and does not increase the time complexity. Extensive experiments demonstrate that our model achieves state-of-the-art performance on many popular multivariate datasets.},
  archive      = {J_TKDE},
  author       = {Shibo Feng and Chunyan Miao and Ke Xu and Jiaxiang Wu and Pengcheng Wu and Yang Zhang and Peilin Zhao},
  doi          = {10.1109/TKDE.2023.3319672},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {2056-2068},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-scale attention flow for probabilistic time series forecasting},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Investigating the sample weighting mechanism using an
interpretable weighting framework. <em>TKDE</em>, <em>36</em>(5),
2041–2055. (<a href="https://doi.org/10.1109/TKDE.2023.3316168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training deep learning models with unequal sample weights has been shown to enhance model performance in various typical learning scenarios, particularly for imbalanced and noisy-label learning scenarios. A deep understanding of the weighting mechanism facilitates the application of existing weighting strategies and illuminates the design of new weighting strategies for real learning tasks. Scholars have focused on exploring existing weighting methods. However, their studies mainly establish how the weights of samples influence the model training. Little headway is made on the weighting mechanism, i.e., which and how the characteristics of a sample influence its weight. In this study, we adopt a data-driven approach to investigate the weighting mechanism by utilizing an interpretable weighting framework. First, a wide range of sample characteristics is extracted from the classifier network during training. Second, the extracted characteristics are fed into a new neural regression tree (NRT), which is a tree model implemented by a neural network, and its output is the weight of the input sample. Third, the NRT is trained using meta-learning within the whole training process. Once the NRT is learned, the weighting mechanism, including the importance of weighting characteristics, prior modes, and specific weighting rules, can be obtained. We conduct extensive experiments on benchmark noisy and imbalanced data corpora. A package of weighting mechanisms is derived from the learned NRT. Furthermore, our proposed interpretable weighting framework exhibits superior performance in comparison to existing weighting strategies.},
  archive      = {J_TKDE},
  author       = {Xiaoling Zhou and Ou Wu and Mengyang Li},
  doi          = {10.1109/TKDE.2023.3316168},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {2041-2055},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Investigating the sample weighting mechanism using an interpretable weighting framework},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reading broadly to open your mind: Improving open relation
extraction with search documents under self-supervisions. <em>TKDE</em>,
<em>36</em>(5), 2026–2040. (<a
href="https://doi.org/10.1109/TKDE.2023.3317139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open relation extraction is the task of extracting open-domain relation facts from natural language sentences. Existing works either utilize distant-supervised annotations to train a supervised classifier over pre-defined relations, or adopt unsupervised methods with additional dependency on external assumptions. However, these works can only obtain information signals from limited existing knowledge bases or datasets. In this work, we propose a self-supervised framework named Web-SelfORE , which exploits self-supervised signals by requiring a large pretrained language model to extensively read real-world relevant documents from the web, and obtain contextualized relational features by mixing contextualized representations of entities from different documents. We perform adaptive clustering on contextualized relational features and bootstrap the self-supervised signals by improving contextualized features in relation classification. We additionally compare the effectiveness of self-supervisions brought by different document sources, and introduce relevance and redundancy evaluation metrics to obtain higher-quality self-supervisions. Experimental results on four public datasets show the effectiveness and robustness of Web-SelfORE on open-domain relation extraction task when comparing with competitive baselines.},
  archive      = {J_TKDE},
  author       = {Xuming Hu and Zhaochen Hong and Chenwei Zhang and Aiwei Liu and Shiao Meng and Lijie Wen and Irwin King and Philip S. Yu},
  doi          = {10.1109/TKDE.2023.3317139},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {2026-2040},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Reading broadly to open your mind: Improving open relation extraction with search documents under self-supervisions},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). GTCAlign: Global topology consistency-based graph
alignment. <em>TKDE</em>, <em>36</em>(5), 2009–2025. (<a
href="https://doi.org/10.1109/TKDE.2023.3312358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph alignment aims to find correspondent nodes between two graphs. Most existing algorithms assume that correspondent nodes in different graphs have similar local structures. However, this principle may not apply to some real-world application scenarios when two graphs have different densities. Some correspondent node pairs may have very different local structures in these cases. Nevertheless, correspondent nodes are expected to have similar importance, inspiring us to exploit global topology consistency for graph alignment. This paper presents GTCAlign, an unsupervised graph alignment framework based on global topology consistency. An indicating matrix is calculated to show node pairs with consistent global topology based on a comprehensive centrality metric. A graph convolutional network (GCN) encodes local structural and attributive information into low-dimensional node embeddings. Then, node similarities are computed based on the obtained node embeddings under the guidance of the indicating matrix. Moreover, a pair of nodes are more likely to be aligned if most of their neighbors are aligned, motivating us to develop an iterative algorithm to refine the alignment results recursively. We conduct extensive experiments on real-world and synthetic datasets to evaluate the effectiveness of GTCAlign. The experimental results show that GTCAlign outperforms state-of-the-art graph alignment approaches.},
  archive      = {J_TKDE},
  author       = {Chenxu Wang and Peijing Jiang and Xiangliang Zhang and Pinghui Wang and Tao Qin and Xiaohong Guan},
  doi          = {10.1109/TKDE.2023.3312358},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {2009-2025},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GTCAlign: Global topology consistency-based graph alignment},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GIaNt: Protein-ligand binding affinity prediction via
geometry-aware interactive graph neural network. <em>TKDE</em>,
<em>36</em>(5), 1991–2008. (<a
href="https://doi.org/10.1109/TKDE.2023.3314502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drug discovery often relies on the successful prediction of protein-ligand binding affinity. Recent advances have shown great promise in applying graph neural networks (GNNs) for better affinity prediction by learning the representations of protein-ligand complexes. However, existing solutions usually treat protein-ligand complexes as topological graph data, thus the 3D geometry-based biomolecular structural information is not fully utilized. The essential intermolecular interactions with long-range dependencies, including type-wise interactions and molecule-wise interactions, are also neglected in GNN models. To this end, we propose a geometry-aware interactive graph neural network ( GIaNt ) which consists of two components: 3D geometric graph learning network ( 3DG-Net ) and pairwise interactive learning network ( Pi-Net ). Specifically, 3DG-Net iteratively performs the node-edge interaction process to update embeddings of nodes and edges in a unified framework while preserving the 3D geometric factors among atoms, including spatial distance, polar angle and dihedral angle information in 3D space. Moreover, Pi-Net is adopted to incorporate both element type-level and molecule-level interactions. Specially, interactive edges are gathered with a subsequent reconstruction loss to reflect the global type-level interactions. Meanwhile, a pairwise attentive pooling scheme is designed to identify the critical interactive atoms for complex representation learning from a semantic view. An exhaustive experimental study on two benchmarks verifies the superiority of GIaNt .},
  archive      = {J_TKDE},
  author       = {Shuangli Li and Jingbo Zhou and Tong Xu and Liang Huang and Fan Wang and Haoyi Xiong and Weili Huang and Dejing Dou and Hui Xiong},
  doi          = {10.1109/TKDE.2023.3314502},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {1991-2008},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GIaNt: Protein-ligand binding affinity prediction via geometry-aware interactive graph neural network},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring large-scale financial knowledge graph for SMEs
supply chain mining. <em>TKDE</em>, <em>36</em>(5), 1979–1990. (<a
href="https://doi.org/10.1109/TKDE.2023.3317631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While large enterprises are benefiting from their global supply chains in these years, it is not easy for Small and Medium-sized Enterprises (SMEs) to find supply chain partners. Treating it as a supply chain mining problem, some deep learning methods, especially knowledge graph (KG) enhanced ones, can achieve workable performance by utilizing explicit structure information from KG while considering effectiveness. However, such improvement is limited when facing the challenges of scalability, complexity, and noisiness in large-scale KGs. To address these issues, we propose a novel M eta-tag S upported C onnectivity representation L earning framework, also known as MSCL. Specifically, a Meta-tag Collaborative Filtering (MCF) method is proposed to highlight the representative schema from huge number of paths connecting two enterprises in large-scale KG. Furthermore, the DPPs-induced Hierarchical Path Sampling (DHPS), a novel sampling framework, is also developed to capture the latent connectivity pattern in KG more effectively. Moreover, the path-wise knowledge representations and the underlying information inherent in pairwise enterprises are aggregated by a connectivity representation learning (CRL) approach for SMEs supply chain mining. Experimental results from two real-world industries have illustrated that the proposed model can achieve competitive performance compared with other existing baselines.},
  archive      = {J_TKDE},
  author       = {Youru Li and Zhenfeng Zhu and Linxun Chen and Bin Yang and Yaxi Wu and Xiaobo Guo and Bing Han and Yao Zhao},
  doi          = {10.1109/TKDE.2023.3317631},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {1979-1990},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Exploring large-scale financial knowledge graph for SMEs supply chain mining},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EvoS&amp;r: Evolving multiple seeds and radii for varying
density data clustering. <em>TKDE</em>, <em>36</em>(5), 1964–1978. (<a
href="https://doi.org/10.1109/TKDE.2023.3312760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Density clustering has shown advantages over other types of clustering methods for processing arbitrarily shaped datasets. In recent years, extensive research efforts has been made on the improvements of DBSCAN or the algorithms incorporating the concept of density peaks. However, these previous studies remain the problems of being sensitive to the parameter settings, and some of them will stuck in weak results when encountering the situations of varying-density distributions. To overcome these issues, we propose an evolution framework named EvoS&amp;R that evolves multiple seeds and the corresponding radii for varying-density data clustering. Compared with the traditional methods, EvoS&amp;R handles the parameter tuning and multi-density fitting problems in an integrated and straightforward manner. Note that, however, the underlying task in EvoS&amp;R is a mixed-variable optimization problem that is challenging in nature. We specifically design a hybrid encoding differential evolution algorithm with novel encoding, mutation, etc., to solve the optimization problem efficiently. Extensive experiments on density-based datasets shows that our algorithm outperforms the other state-of-the-arts in most cases, which validates the effectiveness of the proposed method.},
  archive      = {J_TKDE},
  author       = {Jun-Xian Chen and Yue-Jiao Gong and Wei-Neng Chen and Jun Zhang},
  doi          = {10.1109/TKDE.2023.3312760},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {1964-1978},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {EvoS&amp;R: Evolving multiple seeds and radii for varying density data clustering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event-based dynamic graph representation learning for patent
application trend prediction. <em>TKDE</em>, <em>36</em>(5), 1951–1963.
(<a href="https://doi.org/10.1109/TKDE.2023.3312333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of what types of patents that companies will apply for in the next period of time can figure out their development strategies and help them discover potential partners or competitors in advance. Although important, this problem has been rarely studied in previous research due to the challenges in modeling companies’ continuously evolving preferences and capturing the semantic correlations of classification codes. To fill this gap, we propose an event-based dynamic graph learning framework for patent application trend prediction. In particular, our method is founded on the memorable representations of both companies and patent classification codes. When a new patent is observed, the representations of the related companies and classification codes are updated according to the historical memories and the currently encoded messages. Moreover, a hierarchical message passing mechanism is provided to capture the semantic proximities of patent classification codes by updating their representations along the hierarchical taxonomy. Finally, the patent application trend is predicted by aggregating the representations of the target company and classification codes from static, dynamic and hierarchical perspectives. Experiments on real-world data demonstrate the effectiveness of our approach under various experimental conditions, and also reveal the abilities of our method in learning semantics of classification codes and tracking technology developing trajectories of companies.},
  archive      = {J_TKDE},
  author       = {Tao Zou and Le Yu and Leilei Sun and Bowen Du and Deqing Wang and Fuzhen Zhuang},
  doi          = {10.1109/TKDE.2023.3312333},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {1951-1963},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Event-based dynamic graph representation learning for patent application trend prediction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DKWS: A distributed system for keyword search on massive
graphs. <em>TKDE</em>, <em>36</em>(5), 1935–1950. (<a
href="https://doi.org/10.1109/TKDE.2023.3313726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the unstructuredness and the lack of schemas of graphs, such as knowledge graphs, social networks, and RDF graphs, keyword search for querying such graphs has been proposed. As graphs have become voluminous, large-scale distributed processing has attracted much interest from the database research community. While there have been several distributed systems, distributed querying techniques for keyword search are still limited. This paper proposes a novel distributed keyword search system called $\mathsf {DKWS}$ . First, we present a monotonic property with keyword search algorithms that guarantees correct parallelization. Second, we present a keyword search algorithm as monotonic backward and forward search phases. Moreover, we propose new tight bounds for pruning nodes being searched. Third, we propose a notify-push paradigm and $\mathsf {PINE}$ programming model of $\mathsf {DKWS}$ . The notify-push paradigm allows asynchronously exchanging the upper bounds of matches across the workers and the coordinator in $\mathsf {DKWS}$ . The $\mathsf {PINE}$ programming model naturally fits keyword search algorithms, as they have distinguished phases, to allow preemptive searches to mitigate staleness in a distributed system. Finally, we investigate the performance and effectiveness of $\mathsf {DKWS}$ through experiments using real-world datasets. We find that $\mathsf {DKWS}$ is up to two orders of magnitude faster than related techniques, and its communication costs are 7.6 times smaller than those of other techniques.},
  archive      = {J_TKDE},
  author       = {Jiaxin Jiang and Byron Choi and Xin Huang and Jianliang Xu and Sourav S Bhowmick},
  doi          = {10.1109/TKDE.2023.3313726},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {1935-1950},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DKWS: A distributed system for keyword search on massive graphs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cryptographic primitives in privacy-preserving machine
learning: A survey. <em>TKDE</em>, <em>36</em>(5), 1919–1934. (<a
href="https://doi.org/10.1109/TKDE.2023.3321803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in machine learning have enabled a broad range of complex applications, such as image recognition, recommendation system and machine translation. Data plays an important role in our increasingly complex and diverse environments, and this also reinforces the importance of data privacy in machine learning-enabled applications. Although there are a number of literature survey articles on machine learning, only a few studies have investigated the cryptographic primitives used in privacy-preserving machine learning (PPML). In other words, there is no, or limited, systematization of knowledge (SoK) that provides a comprehensive introduction to cryptography that have been deployed in PPML. In this paper, we first introduce some basic concepts such as machine learning tasks and processes. Then, we review and systematize the cryptographic primitives used in PPML. We analyze these existing privacy-preserving schemes in their learning process, especially training and inference. Finally, we conclude our survey and provide an outlook on future trends and research directions in the field.},
  archive      = {J_TKDE},
  author       = {Hong Qin and Debiao He and Qi Feng and Muhammad Khurram Khan and Min Luo and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TKDE.2023.3321803},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {1919-1934},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cryptographic primitives in privacy-preserving machine learning: A survey},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Compressed data direct computing for databases.
<em>TKDE</em>, <em>36</em>(5), 1902–1918. (<a
href="https://doi.org/10.1109/TKDE.2023.3316274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directly performing operations on compressed data has been proven to be a big success facing Big Data problems in modern data management systems. These systems have demonstrated significant compression benefits and performance improvement for data analytics applications. However, current systems only focus on data queries, while a complete Big Data system must support both data query and data manipulation. To solve this problem, we develop CompressDB, which is a new storage engine that can support data processing for databases without decompression. CompressDB has the following advantages. First, CompressDB utilizes context-free grammar to compress data, and supports both data query and data manipulation. Second, for adaptability, we integrate CompressDB to file systems so that a wide range of databases can directly use CompressDB without any change. Third, we enable operation pushdown to storage so that we can perform data query and manipulation in storage systems without bringing large data to memory for high efficiency. We validate the efficacy of CompressDB supporting various kinds of database systems, including SQLite, MySQL, LevelDB, MongoDB, ClickHouse, and Neo4j. We evaluate our method using seven real-world datasets with various lengths, structures, and content in both single node and cluster environments. Experiments show that CompressDB achieves 40% throughput improvement and 44% latency reduction, along with 1.75 compression ratio on average.},
  archive      = {J_TKDE},
  author       = {Weitao Wan and Feng Zhang and Chenyang Zhang and Mingde Zhang and Jidong Zhai and Yunpeng Chai and Huanchen Zhang and Wei Lu and Yuxing Chen and Haixiang Li and Anqun Pan and Xiaoyong Du},
  doi          = {10.1109/TKDE.2023.3316274},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {1902-1918},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Compressed data direct computing for databases},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bidirectional attentive multi-view clustering.
<em>TKDE</em>, <em>36</em>(5), 1889–1901. (<a
href="https://doi.org/10.1109/TKDE.2023.3312794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key challenge of multi-view graph-based clustering is to mine consistent clustering structures from multiple graphs. Existing works seek clustering decisions from either multiple spectral embeddings or multiple affinity matrices, ignoring the interactions among them. To address this problem, we propose a Bidirectional Attentive Multi-view Clustering (BAMC) model to explore a consensus space w.r.t. spectral embedding and affinity matrix simultaneously, where they can promote each other to mine richer structural information from multiple graphs. BAMC is composed of a Spectral Embedding Learning (SEL) module, an Affinity Matrix Learning (AML) module, and a Bidirectional Attentive Clustering (BAC) module. SEL seeks consensus spectral embeddings by aligning the distributions of elements sampled from subspaces spanned by multiple spectral embeddings. AML learns a consensus affinity matrix from input affinity matrices. BAC guarantees consistency between the learned consensus spectral embeddings and the affinity matrix. To balance their effects, it also assigns adaptive weights to SEL and AML&#39;s objective functions. To solve the optimization problem involved in BAMC, we propose an efficient algorithm based on the Majority-Minimization framework with an ingenious surrogate problem. Extensive experiments on several synthetic and real-world datasets demonstrate the superb performance of BAMC.},
  archive      = {J_TKDE},
  author       = {Jitao Lu and Feiping Nie and Xia Dong and Rong Wang and Xuelong Li},
  doi          = {10.1109/TKDE.2023.3312794},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {1889-1901},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Bidirectional attentive multi-view clustering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anonymous, secure, traceable, and efficient decentralized
digital forensics. <em>TKDE</em>, <em>36</em>(5), 1874–1888. (<a
href="https://doi.org/10.1109/TKDE.2023.3321712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital forensics is crucial to fight crimes around the world. Decentralized Digital Forensics (DDF) promotes it to another level by channeling the power of blockchain into digital investigations. In this work, we focus on the privacy and security of DDF. Our motivations arise from (1) how to track an anonymous-and-malicious data user who leaks only a part of the previously requested data, (2) how to achieve access control while protecting data from untrusted data centers, and (3) how to enable efficient and secure search on the blockchain. To address these issues, we propose Themis: an anonymous and secure DDF scheme with traceable anonymity, private access control, and efficient search. Our framework is boosted by establishing a Trusted Execution Environment in each authority (blockchain node) for securing the uploading, requesting, and searching. To instantiate the framework, we design a secure and robust watermarking scheme in conjunction with decentralized anonymous authentication, a private and fine-grained access control scheme, and an efficient and secure search scheme based on a dynamically updated data structure. We formally define and prove the privacy and security of Themis. We build a prototype with Ethereum and Intel SGX2 to evaluate its performance, which supports processing data from a considerable number of data providers and investigators.},
  archive      = {J_TKDE},
  author       = {Meng Li and Yanzhe Shen and Guixin Ye and Jialing He and Xin Zheng and Zijian Zhang and Liehuang Zhu and Mauro Conti},
  doi          = {10.1109/TKDE.2023.3321712},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {1874-1888},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Anonymous, secure, traceable, and efficient decentralized digital forensics},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient and distributed framework for real-time
trajectory stream clustering. <em>TKDE</em>, <em>36</em>(5), 1857–1873.
(<a href="https://doi.org/10.1109/TKDE.2023.3312319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive ubiquity of GPS-equipped devices, e.g., mobile phones, vehicles, and vessels, a massive amount of real-time, unbounded, and varying-sampling trajectory streams are being generated continuously. Clustering trajectory streams is useful in real-life applications, such as traffic congestion prediction, crowd flow detection, and moving behavior study. Although several sliding-window based algorithms (that adopt the classic two-phases online-offline processing framework) are proposed for trajectory stream clustering, three challenges exist to meet ever-increasing application demands for effective, efficient, and scalable online clustering: i) How to effectively model unbounded trajectory streams in the online settings for effective clustering? ii) How to achieve truly real-time online processing? iii) How to improve the scalable capability of the clustering algorithm to support large-scale moving trajectory streams? In this paper, we propose an efficient and distributed trajectory stream clustering framework that can: i) model trajectory streams dynamically and effectively in a self-adaptive manner, i.e., $k$ -Segment, which considers both spatial and temporal aspects of trajectory streams, ii) support distributed indexing, processing, and workload balance, and iii) incrementally cluster trajectory streams in an efficient manner. Experiments on a wide range of real-world trajectory datasets show that our framework outperforms state-of-the-art baselines in terms of clustering quality, efficiency, and scalability.},
  archive      = {J_TKDE},
  author       = {Yunjun Gao and Ziquan Fang and Jiachen Xu and Shenghao Gong and Chunhui Shen and Lu Chen},
  doi          = {10.1109/TKDE.2023.3312319},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {1857-1873},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {An efficient and distributed framework for real-time trajectory stream clustering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive clustering based personalized federated learning
framework for next POI recommendation with location noise.
<em>TKDE</em>, <em>36</em>(5), 1843–1856. (<a
href="https://doi.org/10.1109/TKDE.2023.3312511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next point-of-interest (POI) recommendation has been a hot research topic, which enables new paradigms for kinds of location-based services in real-world scenarios. Due to the privacy concerns and rigorous data regulations, federated learning provides a distributed learning framework to collaboratively train the recommendation model without sharing the highly sensitive POI data with others. However, there exist two main challenges, namely location noise , and balance between personalization and knowledge sharing , seriously restrict the development of the federated next POI recommendation. To this end, in this work, we propose an adaptive clustering based personalized federated learning framework for next POI recommendation with location noise, named CPF-POI , to address the above challenges. In detail, within the local client, a location recovery module can efficiently remove noises under the given assumption from the noisy POI data in which the recovery error bound can be theoretically proved. Then, within the parameter server, an adaptive clustering scheme is proposed to capture the internal relatedness among all clients to augment positive knowledge sharing. In order to make a balance between personalization and knowledge sharing under personalized federated learning framework, we design an alternative optimization process between clustering similar clients and minimizing local personalized loss functions. Finally, extensive experiments are conducted on two diverse real-world datasets to show the advantages of CPF-POI over state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Ziming Ye and Xiao Zhang and Xu Chen and Hui Xiong and Dongxiao Yu},
  doi          = {10.1109/TKDE.2023.3312511},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {5},
  number       = {5},
  pages        = {1843-1856},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adaptive clustering based personalized federated learning framework for next POI recommendation with location noise},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards practical oblivious join processing. <em>TKDE</em>,
<em>36</em>(4), 1829–1842. (<a
href="https://doi.org/10.1109/TKDE.2023.3310038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud computing, remote accesses over the cloud data inevitably bring the issue of trust. Despite strong encryption schemes, adversaries can still learn sensitive information from encrypted data by observing data access patterns. Oblivious RAMs (ORAMs) are proposed to protect against access pattern attacks. However, directly deploying ORAM constructions in an encrypted database brings large computational overhead. In this work, we focus on oblivious joins over a cloud database. Existing studies in the literature are restricted to either primary-foreign key joins or binary equi-joins. Our major contribution is to support general band joins and multiway equi-joins. For oblivious join without ORAMs, we extend the existing binary equi-join algorithm to support general band joins obliviously. For oblivious join with ORAMs, we integrate $B$ -tree indices into ORAMs for each input table and retrieve blocks through the indices in join processing. The key point is to avoid retrieving tuples that make no contribution to the final join result and bound the number of accesses to each $B$ -tree index. The effectiveness and efficiency of our algorithms are demonstrated through extensive evaluations over real-world datasets. Our method shows orders of magnitude speedup for oblivious multiway equi-joins in comparison with baseline algorithms.},
  archive      = {J_TKDE},
  author       = {Zhao Chang and Dong Xie and Sheng Wang and Feifei Li and Yulong Shen},
  doi          = {10.1109/TKDE.2023.3310038},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1829-1842},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards practical oblivious join processing},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards flexible and adaptive neural process for cold-start
recommendation. <em>TKDE</em>, <em>36</em>(4), 1815–1828. (<a
href="https://doi.org/10.1109/TKDE.2023.3304839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems have been widely adopted in various online personal e-commerce applications for improving user experience. A long-standing challenge in recommender systems is how to provide accurate recommendation to users in cold-start situations where only a few user-item interactions can be observed. Recently, meta learning methods provide a promising solution, and most of them follow a way of parameter initialization where predictions can be fast adapted via multiple gradient descent steps. While these meta-learning recommenders promote model performance, how to derive a fundamental paradigm that enables both flexible approximations of complex user interaction distributions and effective task adaptations of global knowledge still remains a critical yet under-explored problem. To this end, we present the Flow-based Adaptive Neural Process (FANP), a new probabilistic meta-learning model where estimating the preference of each user is governed by an underlying stochastic process. Following an encoder-decoder generative framework, FANP is an effective few-shot function estimator that directly maps limited user interactions to a predictive distribution without complicated gradient updates. Through introducing a conditional normalization flow-based encoder, FANP can get rid of the model bias on latent variables and thereby derive more flexible variational distributions. Meanwhile, we propose a task-adaptive mechanism capturing the relevance of different tasks for improving adaptation ability of global knowledge. The learned task-specific and task-relevant representations are simultaneously exploited to generate the decoder parameters via a novel modulation-augmented hypernetwork. FANP is evaluated on both scenario-specific and user-specific cold-start recommendations on various real-world datasets. Extensive experimental results and detailed model analyses demonstrate that our model yields superior performance compared with multiple state-of-the-art meta-learning recommenders.},
  archive      = {J_TKDE},
  author       = {Xixun Lin and Chuan Zhou and Jia Wu and Lixin Zou and Shirui Pan and Yanan Cao and Bin Wang and Shuaiqiang Wang and Dawei Yin},
  doi          = {10.1109/TKDE.2023.3304839},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1815-1828},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards flexible and adaptive neural process for cold-start recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Task assignment with efficient federated preference learning
in spatial crowdsourcing. <em>TKDE</em>, <em>36</em>(4), 1800–1814. (<a
href="https://doi.org/10.1109/TKDE.2023.3311816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial Crowdsourcing (SC) is finding widespread application in today&#39;s online world. As we have transitioned from desktop crowdsourcing applications (e.g., Wikipedia) to SC applications (e.g., Uber), there is a sense that SC systems must not only provide effective task assignment but also need to ensure privacy. To achieve these often-conflicting objectives, we propose a framework, Task Assignment with Federated Preference Learning, that performs task assignment based on worker preferences while keeping the data decentralized and private in each platform center (e.g., each delivery center of an SC company). The framework includes a federated preference learning phase and a task assignment phase. Specifically, in the first phase, we build a local preference model for each platform center based on historical data. We provide means of horizontal federated learning that makes it possible to collaboratively train these local preference models under the orchestration of a central server. Specifically, we provide a practical method that accelerates federated preference learning based on stochastic controlled averaging and achieves low communication costs while considering data heterogeneity among clients. The task assignment phase aims to achieve effective and efficient task assignment by considering workers’ preferences. Extensive evaluations on real data offer insight into the effectiveness and efficiency of the paper&#39;s proposals.},
  archive      = {J_TKDE},
  author       = {Hao Miao and Xiaolong Zhong and Jiaxin Liu and Yan Zhao and Xiangyu Zhao and Weizhu Qian and Kai Zheng and Christian S. Jensen},
  doi          = {10.1109/TKDE.2023.3311816},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1800-1814},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Task assignment with efficient federated preference learning in spatial crowdsourcing},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synthesizing human trajectories based on variational point
processes. <em>TKDE</em>, <em>36</em>(4), 1785–1799. (<a
href="https://doi.org/10.1109/TKDE.2023.3312209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthesized human trajectories are instrumental for a large number of applications. However, existing trajectory synthesizing models are limited in either modeling variable-length trajectories with continuous temporal distribution or incorporating multi-dimensional context information. In this paper, we propose a novel probabilistic model based on the variational temporal point process to synthesize human trajectories. This model combines the classical temporal point process with the novel neural variational inference framework, leading to its strong ability to model human trajectories with continuous temporal distribution, variable length, and multi-dimensional context information. Extensive experimental results on two real-world trajectory datasets show that our proposed model can synthesize trajectories most similar to real-world human trajectories compared with four representative baseline algorithms in terms of a number of usability metrics, demonstrating its effectiveness.},
  archive      = {J_TKDE},
  author       = {Huandong Wang and Qizhong Zhang and Yuchen Wu and Depeng Jin and Xing Wang and Lin Zhu and Li Yu},
  doi          = {10.1109/TKDE.2023.3312209},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1785-1799},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Synthesizing human trajectories based on variational point processes},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stage-aware hierarchical attentive relational network for
diagnosis prediction. <em>TKDE</em>, <em>36</em>(4), 1773–1784. (<a
href="https://doi.org/10.1109/TKDE.2023.3310478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Electronic Health Records (EHR) have become valuable for enhancing medical decision making, as well as online disease detection and monitoring. Meanwhile, deep learning-based methods have achieved great success in health risk prediction and diagnosis prediction based on EHR. Nevertheless, deep learning-based models usually require high volumes of data due to the vast amount of parameters. In addition, a considerable proportion of medical codes appear rarely in the EHR data which poses huge difficulties for clinical applications. Hence, some works propose to adopt medical ontologies to enhance the prediction performance and provide interpretable prediction results. However, these medical ontologies are often small-scale and coarse-grained, most of diagnoses and medical concepts are not included, lacking many diagnoses and medical concepts, let alone various relationships between these concepts. To overcome this limitation, we propose to incorporate existing large-scale medical knowledge graphs (KGs) into diagnosis prediction and devise a Stage-aware H ierarchical A ttentive R elational Network, named HAR . Specifically, for each visit, a personalized sub-KG is extracted from the existing medical KG, on which HAR conducts relation-specific message passing and hierarchical message aggregation to refine representations of nodes that correspond to medical codes in visits. HAR takes the specific stage of a patient&#39;s disease progression into consideration, which participates in the computation of relation-level and node-level attention. Extensive experiments on two public datasets demonstrate the effectiveness of HAR in improving both the visit-level precision and code-level accuracy of the diagnosis prediction task.},
  archive      = {J_TKDE},
  author       = {Liping Wang and Qiang Liu and Mengqi Zhang and Yaxuan Hu and Shu Wu and Liang Wang},
  doi          = {10.1109/TKDE.2023.3310478},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1773-1784},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Stage-aware hierarchical attentive relational network for diagnosis prediction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Out-of-distribution generalization with causal feature
separation. <em>TKDE</em>, <em>36</em>(4), 1758–1772. (<a
href="https://doi.org/10.1109/TKDE.2023.3312255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by empirical risk minimization, machine learning algorithm tends to exploit subtle statistical correlations existing in the training environment for prediction, while the spurious correlations are unstable across environments, leading to poor generalization performance. Accordingly, the problem of the Out-of-distribution (OOD) generalization aims to exploit an invariant/stable relationship between features and outcomes that generalizes well on all possible environments. To address the spurious correlation induced by the selection bias, in this article, we propose a novel Clique-based Causal Feature Separation (CCFS) algorithm by explicitly incorporating the causal structure to identify causal features of outcome for OOD generalization. Specifically, the proposed CCFS algorithm identifies the largest clique in the learned causal skeleton. Theoretically, we guarantee that either the largest clique or the rest of the causal skeleton is exactly the set of all causal features of the outcome. Finally, we separate the causal features from the non-causal ones with a sample-reweighting decorrelator for OOD prediction. Extensive experiments validate the effectiveness of the proposed CCFS method on both causal feature identification and OOD generalization tasks.},
  archive      = {J_TKDE},
  author       = {Haotian Wang and Kun Kuang and Long Lan and Zige Wang and Wanrong Huang and Fei Wu and Wenjing Yang},
  doi          = {10.1109/TKDE.2023.3312255},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1758-1772},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Out-of-distribution generalization with causal feature separation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-task decouple learning with hierarchical attentive
point process. <em>TKDE</em>, <em>36</em>(4), 1741–1757. (<a
href="https://doi.org/10.1109/TKDE.2023.3305628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential data mining is ubiquitous in various scenarios. Modeling event sequence and predicting event occurrence is of vital importance in sequential data mining, and Temporal Point Processes (TPP) are widely used in this area. Conventional TPP use objective functions as sum of classification loss for event type and regression loss for occurrence time, leading to practical limitations that conventional TPP is unable to predict the occurrence of each type of event and distinguish the dependency within and between different event types. To tackle these defects, we propose a Multi-task Decouple Learning (MTDL) framework to model TPP from a novel perspective of Multi-task Learning (MTL), i.e., predicting the next-step occurrence time for all event types using a weighted multi-task regression loss. We experiment with three state-of-the-arts, showing that the proposed MTDL framework can improve the performance of original TPP models. Moreover, we develop a Hierarchical Attentive Point Process (HAPP) to further exploit the potential of the proposed MTDL framework, using a hierarchical attention mechanism to capture the inner-sequence time dependency within the same type of events and the inter-sequence dependency between different types of events. Experiments on real-world business dataset and public datasets show the efficacy of the proposed method.},
  archive      = {J_TKDE},
  author       = {Weichang Wu and Xiaolu Zhang and Shiwan Zhao and Chilin Fu and Jun Zhou},
  doi          = {10.1109/TKDE.2023.3305628},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1741-1757},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-task decouple learning with hierarchical attentive point process},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-label clinical time-series generation via conditional
GAN. <em>TKDE</em>, <em>36</em>(4), 1728–1740. (<a
href="https://doi.org/10.1109/TKDE.2023.3310909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning has been successfully adopted in a wide range of applications related to electronic health records (EHRs) such as representation learning and clinical event prediction. However, due to privacy constraints, limited access to EHR becomes a bottleneck for deep learning research. To mitigate these concerns, generative adversarial networks (GANs) have been successfully used for generating EHR data. However, there are still challenges in high-quality EHR generation, including generating time-series EHR data and imbalanced uncommon diseases. In this work, we propose a M ulti-label T ime-series GAN (MTGAN) to generate EHR and simultaneously improve the quality of uncommon disease generation. The generator of MTGAN uses a gated recurrent unit (GRU) with a smooth conditional matrix to generate sequences and uncommon diseases. The critic gives scores using Wasserstein distance to recognize real samples from synthetic samples by considering both data and temporal features. We also propose a training strategy to calculate temporal features for real data and stabilize GAN training. Furthermore, we design multiple statistical metrics and prediction tasks to evaluate the generated data. Experimental results demonstrate the quality of the synthetic data and the effectiveness of MTGAN in generating realistic sequential EHR data, especially for uncommon diseases.},
  archive      = {J_TKDE},
  author       = {Chang Lu and Chandan K. Reddy and Ping Wang and Dong Nie and Yue Ning},
  doi          = {10.1109/TKDE.2023.3310909},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1728-1740},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-label clinical time-series generation via conditional GAN},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-hop knowledge graph reasoning in few-shot scenarios.
<em>TKDE</em>, <em>36</em>(4), 1713–1727. (<a
href="https://doi.org/10.1109/TKDE.2023.3304665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL)-based multi-hop reasoning has become an interpretable way for knowledge graph reasoning owing to its persuasive explanations for the predicted results, but the reasoning performance of these methods drops significantly over few-shot relations (only contain few triplets). To address this problem, recent studies introduce meta-learning into RL-based reasoning methods. However, the performance of these studies is still limited due to the following points: (1) the overall reasoning accuracy is impaired due to the low reasoning accuracies over some hard relations; (2) the reasoning process becomes laborious and ineffective owing to the existence of noisy data; (3) the generalizability is negatively affected due to the lack of knowledge-sharing. To tackle these challenges, we propose a novel model HMLS consisting of two modules HHML ( H ierarchical H ardness-aware M eta-reinforcement L earning) and HHS ( H ierarchical H ardness-aware S ampling). Specifically, HHML contains the following two components: (1) a hardness-aware RL conducts multi-hop reasoning by training hardness-aware batches and reducing noise; (2) a knowledge-sharing meta-learning adapts to few-shot relations by exploiting common features in the hierarchical relation structure. The other module HHS generates hardness-aware batches from relation and relation-cluster levels. The experimental results demonstrate that this work notably outperforms the state-of-the-art approaches in few-shot scenarios.},
  archive      = {J_TKDE},
  author       = {Shangfei Zheng and Wei Chen and Weiqing Wang and Pengpeng Zhao and Hongzhi Yin and Lei Zhao},
  doi          = {10.1109/TKDE.2023.3304665},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1713-1727},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-hop knowledge graph reasoning in few-shot scenarios},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to understand the vague graph for stock prediction
with momentum spillovers. <em>TKDE</em>, <em>36</em>(4), 1698–1712. (<a
href="https://doi.org/10.1109/TKDE.2023.3310592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of deep graph learning, our study uniquely addresses the under-explored area of vague graph learning. While the effectiveness of deep graph learning is recognized across various disciplines, the nuances of vague graph learning — whether its inherent vagueness should be incorporated or disregarded and its influence on deep graph learning efficiency — remain largely unexamined. We fill this gap by introducing a novel decoupled graph learning framework. This is achieved by proposing a matrix-based or a tensor-based fusion module to estimate unobservable node attributes, a hybrid attention mechanism to bridge nodes with both explicit and implicit relationships, and a message-passing mechanism for feature-sensitive transporting. The design principle of decoupling allows it to accommodate ambiguities in any or all of these aspects of node representation, linking, and message passing. Furthermore, we leverage an extensive stock dataset spanning 64 years across the entire U.S. market to assess our framework. This real-world data not only adds a practical dimension to our study but also highlights the effectiveness of vague graph learning. Remarkably, our framework demonstrates superiority over state-of-the-art algorithms, marking performance enhancements of at least 6.73%, 7.25%, and 11.39% in terms of Rank IC, $R^{2}$ , and Rank ICIR, respectively.},
  archive      = {J_TKDE},
  author       = {Rong Xing and Rui Cheng and Jiwen Huang and Qing Li and Jingmei Zhao},
  doi          = {10.1109/TKDE.2023.3310592},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1698-1712},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning to understand the vague graph for stock prediction with momentum spillovers},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kullback-leibler divergence-based out-of-distribution
detection with flow-based generative models. <em>TKDE</em>,
<em>36</em>(4), 1683–1697. (<a
href="https://doi.org/10.1109/TKDE.2023.3309853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has revealed that deep generative models including flow-based models and Variational Autoencoders may assign higher likelihoods to out-of-distribution (OOD) data than in-distribution (ID) data. However, we cannot sample OOD data from the model. This counterintuitive phenomenon has not been satisfactorily explained and brings obstacles to OOD detection with flow-based models. In this article, we prove theorems to investigate the Kullback-Leibler divergence in flow-based model and give two explanations for the above phenomenon. Based on our theoretical analysis, we propose a new method KLODS to leverage KL divergence and local pixel dependence of representations to perform anomaly detection. Experimental results on prevalent benchmarks demonstrate the effectiveness and robustness of our method. For group anomaly detection, our method achieves 98.1% AUROC on average with a small batch size of 5. On the contrary, the baseline typicality test-based method only achieves 64.6% AUROC on average due to its failure on challenging problems. Our method also outperforms the state-of-the-art method by 9.1% AUROC. For point-wise anomaly detection, our method achieves 90.7% AUROC on average and outperforms the baseline by 5.2% AUROC. Besides, our method has the least notable failures and is the most robust one.},
  archive      = {J_TKDE},
  author       = {Yufeng Zhang and Jialu Pan and Wanwei Liu and Zhenbang Chen and Kenli Li and Ji Wang and Zhiming Liu and Hongmei Wei},
  doi          = {10.1109/TKDE.2023.3309853},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1683-1697},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Kullback-leibler divergence-based out-of-distribution detection with flow-based generative models},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrating entity attributes for error-aware knowledge
graph embedding. <em>TKDE</em>, <em>36</em>(4), 1667–1682. (<a
href="https://doi.org/10.1109/TKDE.2023.3310149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs (KGs) can structurally organize large-scale information in the form of triples and significantly support many real-world applications. While most KG embedding algorithms hold the assumption that all triples are correct, considerable errors were inevitably injected during the construction process. It is urgent to develop effective error-aware KG embedding, since errors in KGs would lead to significant performance degradation in downstream applications. To this end, we propose a novel framework named Attributed Error-aware Knowledge Embedding (AEKE). It leverages the semantics contained in entity attributes to guide the KG embedding model learning against the impact of erroneous triples. We design two triple-level hypergraphs to model the topological structures of the KG and its attributes, respectively. The confidence score of each triple is jointly calculated based on self-contradictory within the triple, consistency between local and global structures, and homogeneity between structures and attributes. We leverage confidence scores to adaptively update the weighted aggregation in the multi-view graph learning framework and margin loss in KG embedding, such that potential errors will contribute little to KG learning. Experiments on three real-world KGs demonstrate that AEKE outperforms state-of-the-art KG embedding and error detection algorithms.},
  archive      = {J_TKDE},
  author       = {Qinggang Zhang and Junnan Dong and Qiaoyu Tan and Xiao Huang},
  doi          = {10.1109/TKDE.2023.3310149},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1667-1682},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Integrating entity attributes for error-aware knowledge graph embedding},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incremental maximal clique enumeration for hybrid edge
changes in large dynamic graphs. <em>TKDE</em>, <em>36</em>(4),
1650–1666. (<a href="https://doi.org/10.1109/TKDE.2023.3311398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incremental maximal clique enumeration (IMCE), which maintains maximal cliques in dynamic graphs, is a fundamental problem in graph analysis. A maximal clique has a solid descriptive power of dense structures in graphs. Real-world graph data is often large and dynamic. Studies on IMCE face significant challenges in the efficiency of incremental batch computation and hybrid edge changes. Moreover, with growing graph sizes, new requirements occur on indexing global maximal cliques and obtaining maximal cliques under specific vertex scope constraints. This work presents a new data structure SOMEi to maintain intermediate maximal cliques during construction. SOMEi serves as a space-efficient index to retrieve scope-constrained maximal cliques on the fly. Based on SOMEi, we design a procedure-oriented IMCE algorithm to deal with hybrid edge changes within a unified algorithm framework. In particular, the algorithm is able to process a large batch of edge changes and significantly improve the average processing time of a single edge change through an efficient pruning strategy. Experimental results on real and synthetic graph data demonstrate that the proposed algorithm outperforms all the baselines and achieves good efficiency through pruning.},
  archive      = {J_TKDE},
  author       = {Ting Yu and Ting Jiang and Mohamed Jaward Bah and Chen Zhao and Hao Huang and Mengchi Liu and Shuigeng Zhou and Zhao Li and Ji Zhang},
  doi          = {10.1109/TKDE.2023.3311398},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1650-1666},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Incremental maximal clique enumeration for hybrid edge changes in large dynamic graphs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HSMH: A hierarchical sequence multi-hop reasoning model with
reinforcement learning. <em>TKDE</em>, <em>36</em>(4), 1638–1649. (<a
href="https://doi.org/10.1109/TKDE.2023.3303617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The incompleteness of knowledge graphs (KGs) negatively impacts the performance of KGs in downstream applications (e.g., recommendation systems and information retrieval). This phenomenon has brought an increasing rise in research related to knowledge graph reasoning. Recently, emerged reinforcement learning-based multi-hop reasoning methods can infer missing information through multi-hop reasoning according to the existing information in KGs, which has better reasoning performance and interpretability. However, these methods always use relation-entity pairs that have been pre-cropped as the action space of agents for path reasoning, which leads to two problems: 1) insufficient learning and reasoning ability of reasoning models and 2) the hard convergence of the training process of agents. To address these problems, we propose a H ierarchical S equence M ulti H op (HSMH) reasoning framework, which consists of the interactive search reasoning model, local-global knowledge fusion mechanism, and action optimization mechanism. We use interactive search reasoning models to select relations and entities independently, thus fully mining the semantic information of relations and entities and improving the learning and reasoning ability of reasoning models. In the HSMH framework, we design the local-global knowledge fusion mechanism to acquire the local knowledge of entities and neighboring relations and the global knowledge about KG structure, which can improve the learning ability of reasoning models. Meanwhile, we design the action optimization mechanism to combine the filtered action space and the additional action space for efficient path reasoning of agents. Experimental results on benchmark datasets show that our proposed HSMH framework comprehensively outperforms the state-of-the-art multi-hop reasoning model.},
  archive      = {J_TKDE},
  author       = {Dan Wang and Bo Li and Bin Song and Chen Chen and F. Richard Yu},
  doi          = {10.1109/TKDE.2023.3303617},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1638-1649},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {HSMH: A hierarchical sequence multi-hop reasoning model with reinforcement learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical aggregations for high-dimensional multiplex
graph embedding. <em>TKDE</em>, <em>36</em>(4), 1624–1637. (<a
href="https://doi.org/10.1109/TKDE.2023.3305809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the problem of multiplex graph embedding, that is, graphs in which nodes interact through multiple types of relations (dimensions). In recent years, several methods have been developed to address this problem. However, the need for more effective and specialized approaches grows with the production of graph data with diverse characteristics. In particular, real-world multiplex graphs may exhibit a high number of dimensions, making it difficult to construct a single consensus representation. Furthermore, important information can be hidden in complex latent structures scattered in multiple dimensions. To address these issues, we propose HMGE, a novel embedding method based on hierarchical aggregation for high-dimensional multiplex graphs. Hierarchical aggregation consists in learning a hierarchical combination of the graph dimensions and refining the embeddings at each hierarchy level. Non-linear combinations are computed from previous ones, thus uncovering complex information and latent structures hidden in the multiplex graph dimensions. Moreover, we leverage mutual information maximization between local patches and global summaries to train the model without supervision. This allows to captures globally relevant information present in diverse locations of the graph. Detailed experiments on synthetic and real-world data illustrate the suitability of our approach on downstream supervised tasks, including link prediction and node classification.},
  archive      = {J_TKDE},
  author       = {Kamel Abdous and Nairouz Mrabah and Mohamed Bouguessa},
  doi          = {10.1109/TKDE.2023.3305809},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1624-1637},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hierarchical aggregations for high-dimensional multiplex graph embedding},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GREASE: Graph imbalance reduction by adding sets of edges.
<em>TKDE</em>, <em>36</em>(4), 1611–1623. (<a
href="https://doi.org/10.1109/TKDE.2023.3304478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world data can often be represented as a heterogeneous network relating nodes of different types. E.g., a job market can be represented as a job seeker-skill-vacancy network. It can be relevant to consider the imbalance between nodes of different types, in terms of whether they are similarly connected in the network. For example, it is desirable that job seekers and vacancies are mixed well. If they are not, then there is imbalance. We propose to quantify the imbalance between two sets of nodes in a network as the Earth Mover&#39;s Distance between the sets. Given this quantification, we introduce GREASE (Graph imbalance REduction by Adding Sets of Edges), a method that selects a fixed number of unconnected node-pairs, which—if links were added between them—aims to maximally reduce the imbalance. In the job market network, GREASE can be used to select skills that job seekers do not yet have, but could strive to acquire, to reduce the imbalance between job seekers and vacancies. GREASE may also be used in other applications, such as reducing controversy between opposing sides on a polarizing topic. We evaluated GREASE on several datasets and find that GREASE outperforms baselines in reducing network imbalance.},
  archive      = {J_TKDE},
  author       = {Yoosof Mashayekhi and Bo Kang and Jefrey Lijffijt and Tijl De Bie},
  doi          = {10.1109/TKDE.2023.3304478},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1611-1623},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GREASE: Graph imbalance reduction by adding sets of edges},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph adversarial immunization for certifiable robustness.
<em>TKDE</em>, <em>36</em>(4), 1597–1610. (<a
href="https://doi.org/10.1109/TKDE.2023.3311105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite achieving great success, graph neural networks (GNNs) are vulnerable to adversarial attacks. Existing defenses focus on developing adversarial training or model modification. In this paper, we propose and formulate graph adversarial immunization , i.e., vaccinating part of graph structure to improve certifiable robustness of graph against any admissible adversarial attack. We first propose edge-level immunization to vaccinate node pairs. Unfortunately, such edge-level immunization cannot defend against emerging node injection attacks, since it only immunizes existing node pairs. To this end, we further propose node-level immunization. To avoid computationally intensive combinatorial optimization associated with adversarial immunization, we develop AdvImmune-Edge and AdvImmune-Node algorithms to effectively obtain the immune node pairs or nodes. Extensive experiments demonstrate the superiority of AdvImmune methods. In particular, AdvImmune-Node remarkably improves the ratio of robust nodes by 79 $\%$ , 294 $\%$ , and 100 $\%$ , after immunizing only 5 $\%$ of nodes. Furthermore, AdvImmune methods show excellent defensive performance against various attacks, outperforming state-of-the-art defenses. To the best of our knowledge, this is the first attempt to improve certifiable robustness from graph data perspective without losing performance on clean graphs, providing new insights into graph adversarial learning.},
  archive      = {J_TKDE},
  author       = {Shuchang Tao and Qi Cao and Huawei Shen and Yunfan Wu and Liang Hou and Xueqi Cheng},
  doi          = {10.1109/TKDE.2023.3311105},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1597-1610},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph adversarial immunization for certifiable robustness},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fraction-score: A generalized support measure for weighted
and maximal co-location pattern mining. <em>TKDE</em>, <em>36</em>(4),
1582–1596. (<a href="https://doi.org/10.1109/TKDE.2023.3304365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-location patterns, which capture the phenomenon that objects with certain labels are often located in close geographic proximity, are defined based on a support measure which quantifies the prevalence of a pattern candidate in the form of a label set. Existing support measures share the idea of counting the number of instances of a given label set $C$ as its support, where an instance of $C$ is an object set whose objects collectively carry all labels in $C$ and are located close to one another. However, they suffer from various weaknesses, e.g., fail to capture all possible instances, or overlook the cases when multiple instances overlap. In this paper, we propose a new measure called Fraction-Score which counts instances fractionally if they overlap. Fraction-Score captures all possible instances, and handles the cases where instances overlap appropriately (so that the supports defined are more meaningful and anti-monotonic). We develop efficient algorithms to solve the co-location pattern mining problem defined with Fraction-Score. Furthermore, to obtain representative patterns, we develop an efficient algorithm for mining the maximal co-location patterns, which are those patterns without proper superset patterns. We conduct extensive experiments using real and synthetic datasets, which verified the superiority of our proposals.},
  archive      = {J_TKDE},
  author       = {Harry Kai-Ho Chan and Cheng Long and Da Yan and Raymond Chi-Wing Wong and Hua Lu},
  doi          = {10.1109/TKDE.2023.3304365},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1582-1596},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fraction-score: A generalized support measure for weighted and maximal co-location pattern mining},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FlGan: GAN-based unbiased federated learning under non-IID
settings. <em>TKDE</em>, <em>36</em>(4), 1566–1581. (<a
href="https://doi.org/10.1109/TKDE.2023.3309858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) suffers from low convergence and significant accuracy loss due to local biases caused by non-Independent and Identically Distributed (non-IID) data. To enhance the non-IID FL performance, a straightforward idea is to leverage the Generative Adversarial Network (GAN) to mitigate local biases using synthesized samples. Unfortunately, existing GAN-based solutions have inherent limitations, which do not support non-IID data and even compromise user privacy. To tackle the above issues, we propose a GAN-based unbiased FL scheme, called FlGan , to mitigate local biases using synthesized samples generated by GAN while preserving user-level privacy in the FL setting. Specifically, FlGan first presents a federated GAN algorithm using the divide-and-conquer strategy that eliminates the problem of model collapse in non-IID settings. To guarantee user-level privacy, FlGan then exploits Fully Homomorphic Encryption (FHE) to design the privacy-preserving GAN augmentation method for the unbiased FL. Extensive experiments show that FlGan achieves unbiased FL with $10\%-60\%$ accuracy improvement compared with two state-of-the-art FL baselines (i.e., FedAvg and FedSGD) trained under different non-IID settings. The FHE-based privacy guarantees only cost about 0.53% of the total overhead in FlGan .},
  archive      = {J_TKDE},
  author       = {Zhuoran Ma and Yang Liu and Yinbin Miao and Guowen Xu and Ximeng Liu and Jianfeng Ma and Robert H. Deng},
  doi          = {10.1109/TKDE.2023.3309858},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1566-1581},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {FlGan: GAN-based unbiased federated learning under non-IID settings},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible tensor learning for multi-view clustering with
markov chain. <em>TKDE</em>, <em>36</em>(4), 1552–1565. (<a
href="https://doi.org/10.1109/TKDE.2023.3305624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering has gained great progress recently, which employs the representations from different views for improving the final performance. In this paper, we focus on the problem of multi-view clustering based on the Markov chain by considering low-rank constraints. Since most existing methods fail to simultaneously characterize the relations among different entries in a tensor from the global perspective and describe local structures of similarity matrices of a tensor, we propose a novel Flexible Tensor Learning for Multi-view Clustering with the Markov chain (FTLMCM) to solve this problem. We also construct transition probability matrices based on the Markov chain to fully utilize the connection between the Markov chain and spectral clustering. Specifically, the low-rank constraints of the tensor, the frontal slices and the lateral slices of the tensor are imposed on the objective function of the proposed method to achieve these goals. Besides, these three constraints can be optimized jointly to achieve mutual refinement. FTLMCM also uses the tensor rotation to better explore the relationships among different views. We formulate FTLMCM as a problem of low-rank tensor recovery and solve it with the augmented Lagrangian multiplier. Experiments on six different benchmark data sets under six metrics demonstrate that the proposed method is able to achieve better clustering performance.},
  archive      = {J_TKDE},
  author       = {Yalan Qin and Zhenjun Tang and Hanzhou Wu and Guorui Feng},
  doi          = {10.1109/TKDE.2023.3305624},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1552-1565},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Flexible tensor learning for multi-view clustering with markov chain},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FairSample: Training fair and accurate graph convolutional
neural networks efficiently. <em>TKDE</em>, <em>36</em>(4), 1537–1551.
(<a href="https://doi.org/10.1109/TKDE.2023.3306378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fairness in Graph Convolutional Neural Networks (GCNs) becomes a more and more important concern as GCNs are adopted in many crucial applications. Societal biases against sensitive groups may exist in many real world graphs. GCNs trained on those graphs may be vulnerable to being affected by such biases. In this paper, we adopt the well-known fairness notion of demographic parity and tackle the challenge of training fair and accurate GCNs efficiently. We present an in-depth analysis on how graph structure bias, node attribute bias, and model parameters may affect the demographic parity of GCNs. Our insights lead to FairSample, a framework that jointly mitigates the three types of biases. We employ two intuitive strategies to rectify graph structures. First, we inject edges across nodes that are in different sensitive groups but similar in node features. Second, to enhance model fairness and retain model quality, we develop a learnable neighbor sampling policy using reinforcement learning. To address the bias in node features and model parameters, FairSample is complemented by a regularization objective to optimize fairness.},
  archive      = {J_TKDE},
  author       = {Zicun Cong and Baoxu Shi and Shan Li and Jaewon Yang and Qi He and Jian Pei},
  doi          = {10.1109/TKDE.2023.3306378},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1537-1551},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {FairSample: Training fair and accurate graph convolutional neural networks efficiently},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient <span
class="math inline"><em>k</em></span>k-clique counting on large graphs:
The power of color-based sampling approaches. <em>TKDE</em>,
<em>36</em>(4), 1518–1536. (<a
href="https://doi.org/10.1109/TKDE.2023.3314643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {$K$ -clique counting is a fundamental problem in network analysis which has attracted much attention in recent years. Computing the count of $k$ -cliques in a graph for a large $k$ (e.g., $k=8$ ) is often intractable as the number of $k$ -cliques increases exponentially w.r.t. (with respect to) $k$ . Existing exact $k$ -clique counting algorithms are often hard to handle large dense graphs, while sampling-based solutions either require a huge number of samples or consume very high storage space to achieve a satisfactory accuracy. To overcome these limitations, we propose a new framework to estimate the number of $k$ -cliques which integrates both the exact $k$ -clique counting technique and three novel color-based sampling techniques. The key insight of our framework is that we only apply the exact algorithm to compute the $k$ -clique counts in the sparse regions of a graph, and use the proposed color-based sampling approaches to estimate the number of $k$ -cliques in the dense regions of the graph. Specifically, we develop three novel dynamic programming based $k$ -color set sampling techniques to efficiently estimate the $k$ -clique counts, where a $k$ -color set contains $k$ nodes with $k$ different colors. Since a $k$ -color set is often a good approximation of a $k$ -clique in the dense regions of a graph, our sampling-based solutions are extremely efficient and accurate. Moreover, the proposed sampling techniques are space efficient which use near-linear space w.r.t. graph size. We conduct extensive experiments to evaluate our algorithms using 8 real-life graphs. The results show that our best algorithm is at least one order of magnitude faster than the state-of-the-art sampling-based solutions (with the same relative error 0.1%) and can be up to three orders of magnitude faster than the state-of-the-art exact algorithm on large graphs.},
  archive      = {J_TKDE},
  author       = {Xiaowei Ye and Rong-Hua Li and Qiangqiang Dai and Hongzhi Chen and Guoren Wang},
  doi          = {10.1109/TKDE.2023.3314643},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1518-1536},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient $k$k-clique counting on large graphs: The power of color-based sampling approaches},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). DIOR: Learning to hash with label noise via dual partition
and contrastive learning. <em>TKDE</em>, <em>36</em>(4), 1502–1517. (<a
href="https://doi.org/10.1109/TKDE.2023.3312109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the excellent computing efficiency, learning to hash has acquired broad popularity for Big Data retrieval. Although supervised hashing methods have achieved promising performance recently, they presume that all training samples are appropriately annotated. Unfortunately, label noise is ubiquitous owing to erroneous annotations in real-world applications, which could seriously deteriorate the retrieval performance due to imprecise supervised guidance and severe memorization of noisy data. Here we propose a comprehensive method DIOR to handle the difficulties of learning to hash with label noise. DIOR performs partitions from two complementary levels, namely sample level and parameter level. On the one hand, DIOR divides the dataset into a labeled set with clean samples and an unlabeled set with noisy samples using an ensemble of perturbed views. Then we train the network in a contrastive semi-supervised manner by reconstructing label embeddings for both reliable supervision of clean data and sufficient exploration of noisy data. On the other hand, inspired by recent pruning techniques, DIOR divides the parameters in the hashing network into crucial parameters and non-crucial parameters, and then optimizes them separately to reduce the overfitting of noisy data. Extensive experiments on four popular benchmark datasets demonstrate the effectiveness of DIOR.},
  archive      = {J_TKDE},
  author       = {Haixin Wang and Huiyu Jiang and Jinan Sun and Shikun Zhang and Chong Chen and Xian-Sheng Hua and Xiao Luo},
  doi          = {10.1109/TKDE.2023.3312109},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1502-1517},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DIOR: Learning to hash with label noise via dual partition and contrastive learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deciphering clusters with a deterministic measure of
clustering tendency. <em>TKDE</em>, <em>36</em>(4), 1489–1501. (<a
href="https://doi.org/10.1109/TKDE.2023.3306024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering, a key aspect of exploratory data analysis, plays a crucial role in various fields such as information retrieval. Yet, the sheer volume and variety of available clustering algorithms hinder their application to specific tasks, especially given their propensity to enforce partitions, even when no clear clusters exist, often leading to fruitless efforts and erroneous conclusions. This issue highlights the importance of accurately assessing clustering tendencies prior to clustering. However, existing methods either rely on subjective visual assessment, which hinders automation of downstream tasks, or on correlations between subsets of target datasets and random distributions, limiting their practical use. Therefore, we introduce the Proximal Homogeneity Index (PHI) , a novel and deterministic statistic that reliably assesses the clustering tendencies of datasets by analyzing their internal structures via knowledge graphs. Leveraging PHI and the boundaries between clusters, we establish the Partitioning Sensitivity Index (PSI) , a new statistic designed for cluster quality assessment and optimal clustering identification. Comparative studies using twelve synthetic and real-world datasets demonstrate PHI and PSI&#39;s superiority over existing metrics for clustering tendency assessment and cluster validation. Furthermore, we demonstrate the scalability of PHI to large and high-dimensional datasets, and PSI&#39;s broad effectiveness across diverse cluster analysis tasks.},
  archive      = {J_TKDE},
  author       = {Alec F. Diallo and Paul Patras},
  doi          = {10.1109/TKDE.2023.3306024},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1489-1501},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Deciphering clusters with a deterministic measure of clustering tendency},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continuous-time user preference modelling for temporal sets
prediction. <em>TKDE</em>, <em>36</em>(4), 1475–1488. (<a
href="https://doi.org/10.1109/TKDE.2023.3309982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a sequence of sets, where each set has a timestamp and contains an arbitrary number of elements, temporal sets prediction aims to predict the elements in the subsequent set. Previous studies for temporal sets prediction mainly focus on the modelling of elements and implicitly represent each user&#39;s preference based on his/her interacted elements. However, user preferences are often continuously evolving and the evolutionary trend cannot be fully captured with the indirect learning paradigm of user preferences. To this end, we propose a continuous-time user preference modelling framework for temporal sets prediction, which explicitly models the evolving preference of each user by maintaining a memory bank to store the states of all the users and elements. Specifically, we first construct a universal sequence by arranging all the user-set interactions in a non-descending temporal order, and then chronologically learn from each user-set interaction. For each interaction, we continuously update the memories of the related user and elements based on their currently encoded messages and past memories. Moreover, we present a personalized user behavior learning module to discover user-specific characteristics based on each user&#39;s historical sequence, which aggregates the previously interacted elements from dual perspectives according to the user and elements. Finally, we develop a set-batch algorithm to improve the model efficiency, which can create time-consistent batches in advance and achieve 3.5× and 3.0× speedups in the training and evaluation process on average. Experiments on four real-world datasets demonstrate the superiority of our approach over state-of-the-arts under both transductive and inductive settings. The good interpretability of our method is also shown.},
  archive      = {J_TKDE},
  author       = {Le Yu and Zihang Liu and Leilei Sun and Bowen Du and Chuanren Liu and Weifeng Lv},
  doi          = {10.1109/TKDE.2023.3309982},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1475-1488},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Continuous-time user preference modelling for temporal sets prediction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Clustering ensemble via diffusion on adaptive multiplex.
<em>TKDE</em>, <em>36</em>(4), 1463–1474. (<a
href="https://doi.org/10.1109/TKDE.2023.3311409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing clustering ensemble methods often directly integrate multiple weak base results to obtain a consensus one which can improve the clustering performance. However, since the base results are weak and the clustering ensemble can improve the performance, why not refine the weak base results via the clustering ensemble, and then boost the clustering ensemble with the refined base results? To fulfill this idea, in this article, we propose a novel clustering ensemble method with an adaptive multiplex. We first use the multiplex to represent the multiple weak base results. Then, we learn an updated representation by diffusing the representation on the multiplex with a manifold ranking model. Since the multiplex characterizes the structure information of all base results, the learned representation can ensemble such structure information during diffusion. Next, the multiplex is refined by such representation, which is a process of refining base results via ensemble. We iteratively learn the representation (i.e., do ensemble) and update the multiplex (i.e., do refinement), which can make the ensemble and refinement be boosted by each other. At last, the final consensus result is obtained from the refined multiplex. The extensive experiments demonstrate the effectiveness and superiority of the proposed framework.},
  archive      = {J_TKDE},
  author       = {Peng Zhou and Boao Hu and Dengcheng Yan and Liang Du},
  doi          = {10.1109/TKDE.2023.3311409},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1463-1474},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Clustering ensemble via diffusion on adaptive multiplex},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beyond co-occurrence: Multi-modal session-based
recommendation. <em>TKDE</em>, <em>36</em>(4), 1450–1462. (<a
href="https://doi.org/10.1109/TKDE.2023.3309995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Session-based recommendation is devoted to characterizing preferences of anonymous users based on short sessions. Existing methods mostly focus on mining limited item co-occurrence patterns exposed by item ID within sessions, while ignoring what attracts users to engage with certain items is rich multi-modal information displayed on pages. Generally, the multi-modal information can be classified into two categories: descriptive information (e.g., item images and description text) and numerical information (e.g., price). In this paper, we aim to improve session-based recommendation by modeling the above multi-modal information holistically. There are mainly three issues to reveal user intent from multi-modal information: (1) How to extract relevant semantics from heterogeneous descriptive information with different noise? (2) How to fuse these heterogeneous descriptive information to comprehensively infer user interests? (3) How to handle probabilistic influence of numerical information on user behaviors? To solve above issues, we propose a novel multi-modal session-based recommendation (MMSBR) that models both descriptive and numerical information under a unified framework. Specifically, a pseudo-modality contrastive learning is devised to enhance the representation learning of descriptive information. Afterwards, a hierarchical pivot transformer is presented to fuse heterogeneous descriptive information. Moreover, we represent numerical information with Gaussian distribution and design a Wasserstein self-attention to handle the probabilistic influence mode. Extensive experiments on three real-world datasets demonstrate the effectiveness of the proposed MMSBR. Further analysis also proves that our MMSBR can alleviate the cold-start problem in SBR effectively.},
  archive      = {J_TKDE},
  author       = {Xiaokun Zhang and Bo Xu and Fenglong Ma and Chenliang Li and Liang Yang and Hongfei Lin},
  doi          = {10.1109/TKDE.2023.3309995},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1450-1462},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Beyond co-occurrence: Multi-modal session-based recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on neural data-to-text generation. <em>TKDE</em>,
<em>36</em>(4), 1431–1449. (<a
href="https://doi.org/10.1109/TKDE.2023.3304385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-to-text Generation (D2T) aims to generate textual natural language statements that can fluently and precisely describe the structured data such as graphs, tables, and meaning representations (MRs) in the form of key-value pairs. It is a typical and crucial task in natural language generation (NLG). Early D2T systems generated texts with the cost of human engineering in designing domain specific rules and templates, and achieved acceptable performance in coherence, fluency, and fidelity. In recent years, the data-driven D2T systems based on deep learning have reached state-of-the-art (SOTA) performance in more challenging datasets. In this paper, we provide a comprehensive review on existing neural data-to-text generation approaches. We first introduce available D2T resources, including systematically categorized D2T datasets and mainstream evaluation metrics. Next, we survey existing works based on the taxonomy along two axes: neural end-to-end D2T and neural modular D2T. We also discuss the potential applications and the adverse impacts. Finally, we present readers with the challenges faced by neural D2T and outline some potential future directions in this area.},
  archive      = {J_TKDE},
  author       = {Yupian Lin and Tong Ruan and Jingping Liu and Haofen Wang},
  doi          = {10.1109/TKDE.2023.3304385},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1431-1449},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey on neural data-to-text generation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of knowledge enhanced pre-trained language models.
<em>TKDE</em>, <em>36</em>(4), 1413–1430. (<a
href="https://doi.org/10.1109/TKDE.2023.3310002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-trained Language Models (PLMs) which are trained on large text corpus via self-supervised learning method, have yielded promising performance on various tasks in Natural Language Processing (NLP). However, though PLMs with huge parameters can effectively possess rich knowledge learned from massive training text and benefit downstream tasks at the fine-tuning stage, they still have some limitations such as poor reasoning ability due to the lack of external knowledge. Research has been dedicated to incorporating knowledge into PLMs to tackle these issues. In this paper, we present a comprehensive review of Knowledge Enhanced Pre-trained Language Models (KE-PLMs) to provide a clear insight into this thriving field. We introduce appropriate taxonomies respectively for Natural Language Understanding (NLU) and Natural Language Generation (NLG) to highlight these two main tasks of NLP. For NLU, we divide the types of knowledge into four categories: linguistic knowledge, text knowledge, knowledge graph (KG), and rule knowledge. The KE-PLMs for NLG are categorized into KG-based and retrieval-based methods. Finally, we point out some promising future directions of KE-PLMs.},
  archive      = {J_TKDE},
  author       = {Linmei Hu and Zeyi Liu and Ziwang Zhao and Lei Hou and Liqiang Nie and Juanzi Li},
  doi          = {10.1109/TKDE.2023.3310002},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1413-1430},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey of knowledge enhanced pre-trained language models},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hierarchical block distance model for ultra
low-dimensional graph representations. <em>TKDE</em>, <em>36</em>(4),
1399–1412. (<a href="https://doi.org/10.1109/TKDE.2023.3304344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Representation Learning (GRL) has become central for characterizing structures of complex networks and performing tasks such as link prediction, node classification, network reconstruction, and community detection. Whereas numerous generative GRL models have been proposed, many approaches have prohibitive computational requirements hampering large-scale network analysis, fewer are able to explicitly account for structure emerging at multiple scales, and only a few explicitly respect important network properties such as homophily and transitivity. This paper proposes a novel scalable graph representation learning method named the Hierarchical Block Distance Model (HBDM). The HBDM imposes a multiscale block structure akin to stochastic block modeling (SBM) and accounts for homophily and transitivity by accurately approximating the latent distance model (LDM) throughout the inferred hierarchy. The HBDM naturally accommodates unipartite, directed, and bipartite networks whereas the hierarchy is designed to ensure linearithmic time and space complexity enabling the analysis of very large-scale networks. We evaluate the performance of the HBDM on massive networks consisting of millions of nodes. Importantly, we find that the proposed HBDM framework significantly outperforms recent scalable approaches in all considered downstream tasks. Surprisingly, we observe superior performance even imposing ultra-low two-dimensional embeddings facilitating accurate direct and hierarchical-aware network visualization and interpretation.},
  archive      = {J_TKDE},
  author       = {Nikolaos Nakis and Abdulkadir Çelikkanat and Sune Lehmann and Morten Mørup},
  doi          = {10.1109/TKDE.2023.3304344},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1399-1412},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A hierarchical block distance model for ultra low-dimensional graph representations},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A fast algorithm for moderating critical nodes via edge
removal. <em>TKDE</em>, <em>36</em>(4), 1385–1398. (<a
href="https://doi.org/10.1109/TKDE.2023.3309987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Critical nodes in networks are extremely vulnerable to malicious attacks to trigger negative cascading events such as the spread of misinformation and diseases. Therefore, effective moderation of critical nodes is very vital for mitigating the potential damages caused by such malicious diffusions. The current moderation methods are computationally expensive. Furthermore, they disregard the fundamental metric of information centrality, which measures the dissemination power of nodes. We investigate the problem of removing $k$ edges from a network to minimize the information centrality of a target node $v$ while preserving the network&#39;s connectivity. We prove that this problem is computationally challenging: it is NP-complete and its objective function is not supermodular. However, we propose three approximation greedy algorithms using novel techniques such as random walk-based Schur complement approximation and fast sum estimation. One of our algorithms runs in nearly linear time in the number of edges. To complement our theoretical analysis, we conduct a comprehensive set of experiments on synthetic and real networks with over one million nodes. Across various settings, the experimental results illustrate the effectiveness and efficiency of our proposed algorithms.},
  archive      = {J_TKDE},
  author       = {Changan Liu and Xiaotian Zhou and Ahad N. Zehmakan and Zhongzhi Zhang},
  doi          = {10.1109/TKDE.2023.3309987},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {4},
  number       = {4},
  pages        = {1385-1398},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A fast algorithm for moderating critical nodes via edge removal},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). XMQAs: Constructing complex-modified question-answering
dataset for robust question understanding. <em>TKDE</em>,
<em>36</em>(3), 1371–1384. (<a
href="https://doi.org/10.1109/TKDE.2023.3303916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Question understanding is an important issue to the success of a Knowledge-based Question Answering (KBQA) system.However, the existing study does not pay enough attention to this issue given that the questions in the existing KBQA datasets are usually expressed in simple and straightforward way. This is not in line with the actual linguistic conventions, which often use a lot of modifiers. To facilitate the study on evaluating and enhancing the question understanding ability of the KBQA systems, this paper proposes to construct a complex-modified question-answering (XMQAs) dataset based on existing KBQA datasets. With the help of knowledge bases and dictionaries, three kinds of modifiers are defined and applied to original simple-expressed questions. These modifiers could make the expression of these questions complex without changing their semantics. Based on XMQAs, we then propose a novel question understanding algorithm upon existing KBQA models, which greatly improves the robustness of their question understanding abilities. We conduct extensive experiments on XMQAs and two widely acknowledged KBQA datasets. The empirical results demonstrate that our proposed algorithm can improve the performance of KBQA models on not only the complex-modified questions, but also simple-expressed questions.},
  archive      = {J_TKDE},
  author       = {Yuyan Chen and Yanghua Xiao and Zhixu Li and Bang Liu},
  doi          = {10.1109/TKDE.2023.3303916},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1371-1384},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {XMQAs: Constructing complex-modified question-answering dataset for robust question understanding},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unified dense subgraph detection: Fast spectral theory based
algorithms. <em>TKDE</em>, <em>36</em>(3), 1356–1370. (<a
href="https://doi.org/10.1109/TKDE.2023.3272574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How can we effectively detect fake reviews or fraudulent links on a website? How can we spot communities that suddenly appear based on users’ interactions? And how can we efficiently find the minimum cut in a large graph? All of these are related to the finding of dense subgraphs, a significant primitive problem in graph analysis with extensive applications across various domains. In this paper, we focus on formulating the problem of the densest subgraph detection and theoretically compare and contrast several correlated problems. Moreover, we propose a unified framework, GenDS , for the densest subgraph detection, provide some theoretical analysis based on the network flow and spectral graph theory, and devise simple and computationally efficient algorithms, SpecGDS and GepGDS , to solve it by leveraging the spectral properties and greedy search. We conduct thorough experiments on 40 real-world networks with up to 1.47 billion edges from various domains. We demonstrate that our SpecGDS yields up to $58.6 \ \times$ speedup and achieves better or approximately equal-quality solutions for the densest subgraph detection compared to the baselines. GepGDS also reveals some properties of generalized eigenvalue problems for the GenDS . Also, our methods scale linearly with the graph size and are proven effective in applications such as finding collaborations that appear suddenly in an extensive, time-evolving co-authorship network.},
  archive      = {J_TKDE},
  author       = {Wenjie Feng and Shenghua Liu and Danai Koutra and Xueqi Cheng},
  doi          = {10.1109/TKDE.2023.3272574},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1356-1370},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Unified dense subgraph detection: Fast spectral theory based algorithms},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-layer space-oriented partitioning for non-point data.
<em>TKDE</em>, <em>36</em>(3), 1341–1355. (<a
href="https://doi.org/10.1109/TKDE.2023.3297975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-point spatial objects (e.g., polygons, linestrings, etc.) are ubiquitous. We study the problem of indexing non-point objects in memory for range queries and spatial intersection joins. We propose a secondary partitioning technique for space-oriented partitioning indices (e.g., grids), which improves their performance significantly, by avoiding the generation and elimination of duplicate results. Our approach is easy to implement and can be used by any space-partitioning index to significantly reduce the cost of range queries and intersection joins. In addition, the secondary partitions can be processed independently, which makes our method appropriate for distributed and parallel indexing. Experiments on real datasets confirm the advantage of our approach against alternative duplicate elimination techniques and data-oriented state-of-the-art spatial indices. We also show that our partitioning technique, paired with optimized partition-to-partition join algorithms, typically reduces the cost of spatial joins by around 50%.},
  archive      = {J_TKDE},
  author       = {Dimitrios Tsitsigkos and Panagiotis Bouros and Konstantinos Lampropoulos and Nikos Mamoulis and Manolis Terrovitis},
  doi          = {10.1109/TKDE.2023.3297975},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1341-1355},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Two-layer space-oriented partitioning for non-point data},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Topic modeling on document networks with dirichlet optimal
transport barycenter. <em>TKDE</em>, <em>36</em>(3), 1328–1340. (<a
href="https://doi.org/10.1109/TKDE.2023.3303465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text documents are often interconnected in a network structure, e.g., academic papers via citations, Web pages via hyperlinks. On the one hand, though Graph Neural Networks (GNNs) have shown promising ability to derive effective embeddings for such networked documents, they do not assume a latent topic structure and result in uninterpretable embeddings. On the other hand, topic models can infer semantically interpretable topic distributions for documents by associating each topic with a group of understandable key words. However, most topic models mainly focus on plain text within documents and fail to leverage network structure across documents. Network connectivity reveals topic similarity between linked documents, and modeling it could uncover meaningful semantics. Motivated by above two challenges, in this paper, we propose a GNN-based neural topic model that both captures network connectivity and derives semantically interpretable topic distributions for networked documents. For network modeling, we build the model based on the theory of Optimal Transport Barycenter, which captures network structure by allowing the topic distribution of a document to generate the content of its linked neighbors. For semantic interpretability, we extend optimal transport by incorporating semantically related words in the embedding space. Since Dirichlet prior in Latent Dirichlet Allocation successfully improves topic quality, we also analyze Dirichlet as an optimal transport prior distribution to improve topic interpretability. We design rejection sampling to simulate Dirichlet distribution. Extensive experiments on document classification, clustering, link prediction, and topic analysis verify the effectiveness of our model.},
  archive      = {J_TKDE},
  author       = {Delvin Ce Zhang and Hady W. Lauw},
  doi          = {10.1109/TKDE.2023.3303465},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1328-1340},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Topic modeling on document networks with dirichlet optimal transport barycenter},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Super resolution graph with conditional normalizing flows
for temporal link prediction. <em>TKDE</em>, <em>36</em>(3), 1311–1327.
(<a href="https://doi.org/10.1109/TKDE.2023.3295367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal link prediction on dynamic graphs has attracted considerable attention. Most methods focus on the graph at each timestamp and extract features for prediction. As graphs are directly compressed into feature matrices, the important latent information at each timestamp has not been well revealed. Eventually, the acquisition of dynamic evolution-related patterns is rendered inadequately. In this paper, inspired by the process of Super-Resolution (SR), a novel deep generative model SRG (Super Resolution Graph) is proposed. We innovatively introduce the concepts of the Low-Resolution (LR) graph, which is a single adjacent matrix at a timestamp, and the High-Resolution (HR) graph, which includes the link status of surrounding snapshots. Specifically, two major aspects are considered regarding the construction of the HR graph. For edges, we endeavor to obtain an extensive information transmission description that affects the current link status. For nodes, similar to the SR process, the neighbor relationship among nodes is maintained. In this form, we could predict the link status from a new perspective: Under the supervision of the graph moving average strategy, the conditional normalizing flow effectively realizes the transformation between LR and HR graphs. Extensive experiments on six real-world datasets from different applications demonstrate the effectiveness of our proposal.},
  archive      = {J_TKDE},
  author       = {Yanting Yin and Yajing Wu and Xuebing Yang and Wensheng Zhang and Xiaojie Yuan},
  doi          = {10.1109/TKDE.2023.3295367},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1311-1327},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Super resolution graph with conditional normalizing flows for temporal link prediction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sample efficient offline-to-online reinforcement learning.
<em>TKDE</em>, <em>36</em>(3), 1299–1310. (<a
href="https://doi.org/10.1109/TKDE.2023.3302804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offline reinforcement learning (RL) makes it possible to train the agents entirely from a previously collected dataset. However, constrained by the quality of the offline dataset, offline RL agents typically have limited performance and cannot be directly deployed. Thus, it is desirable to further finetune the pretrained offline RL agents via online interactions with the environment. Existing offline-to-online RL algorithms suffer from the low sample efficiency issue, due to two inherent challenges, i.e., exploration limitation and distribution shift. To this end, we propose a sample-efficient offline-to-online RL algorithm via Optimistic Exploration and Meta Adaptation (OEMA). Specifically, we first propose an optimistic exploration strategy according to the principle of optimism in the face of uncertainty. This allows agents to sufficiently explore the environment in a stable manner. Moreover, we propose a meta learning based adaptation method, which can reduce the distribution shift and accelerate the offline-to-online adaptation process. We empirically demonstrate that OEMA improves the sample efficiency on D4RL benchmark. Besides, we provide in-depth analyses to verify the effectiveness of both optimistic exploration and meta adaptation.},
  archive      = {J_TKDE},
  author       = {Siyuan Guo and Lixin Zou and Hechang Chen and Bohao Qu and Haotian Chi and Philip S. Yu and Yi Chang},
  doi          = {10.1109/TKDE.2023.3302804},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1299-1310},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Sample efficient offline-to-online reinforcement learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RFDG: Reinforcement federated domain generalization.
<em>TKDE</em>, <em>36</em>(3), 1285–1298. (<a
href="https://doi.org/10.1109/TKDE.2023.3301036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the training process of federated learning models, the domain information of the target test data on the server can differ greatly from the training data of each client, leading to a decrease in the performance of the federated model. Additionally, due to privacy protection during federated training, clients cannot see the target domain test data, and the distribution information of the target data cannot be used. This poses a new challenge for federated learning. Domain generalization techniques are often used in centralized frameworks to resolve such problems. In recent years, the domain generalization method based on feature decorrelation has enabled models to learn knowledge with a stronger generalization ability in unseen target domain data. However, existing methods require data centralization in the feature decorrelation process, which conflicts with data privacy protection in federated learning. To address these issues, we propose Reinforcement Federated Domain Generalization (RFDG), which incorporates domain generalization in federated learning via reinforcement learning. RFDG can improve the generalization ability of the federated model of unseen target domain test data. We design a reinforcement federated feature decorrelation policy that uses reinforcement learning technology to transform the sample reweight work into a parameterized sample reweight policy that can be shared among federated learning clients. We develop reinforcement federated experience replay techniques to supplement the feature information loss of local data due to the mini-batch mechanism during the policy learning process. When the policy is shared by each client, those features can be decorrelated from a global perspective, allowing the model to focus on capturing the fundamental association between features and labels to learn domain-invariant knowledge. We verified the effectiveness of our method through extensive experiments using four publicly available datasets.},
  archive      = {J_TKDE},
  author       = {Zeli Guan and Yawen Li and Zhenhui Pan and Yuxin Liu and Zhe Xue},
  doi          = {10.1109/TKDE.2023.3301036},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1285-1298},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {RFDG: Reinforcement federated domain generalization},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Refining codes for locality sensitive hashing.
<em>TKDE</em>, <em>36</em>(3), 1274–1284. (<a
href="https://doi.org/10.1109/TKDE.2023.3297195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to hash is of particular interest in information retrieval for large-scale data due to its high efficiency and effectiveness. Most studies in hashing concentrate on constructing new hashing models, but rarely touch the correlation and redundancy between hash bits derived. In this article, we first introduce a general schema of hash bit reduction to derive compact and informative binary codes for hashing techniques. Further, we take locality sensitive hashing, one of the most widely-used hashing methods, as an example and propose a novel and two-stage binary code refinement method under the reduction schema. Specifically, the proposed method includes two stages, i.e., bit evaluation and bit refinement. The former stage aims to initially extract a small portion of informative hash bits in terms of their importance and quality evaluated by bit balance and similarity preservation. Then, the representation capabilities of the reduced hash bits are strengthened further by refining their binary values. The purpose of refinement is to lessen the correlations and redundancies between the reduced bits, making themselves more discriminative. The experimental results on three widely-used data collections confirm the effectiveness of the proposed bit reduction method and its superiority over the state-of-the-art hashing methods, as well as a bit selection method.},
  archive      = {J_TKDE},
  author       = {Huawen Liu and Wenhua Zhou and Zongda Wu and Shichao Zhang and Gang Li and Xuelong Li},
  doi          = {10.1109/TKDE.2023.3297195},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1274-1284},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Refining codes for locality sensitive hashing},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Query-aware explainable product search with reinforcement
knowledge graph reasoning. <em>TKDE</em>, <em>36</em>(3), 1260–1273. (<a
href="https://doi.org/10.1109/TKDE.2023.3297331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Product search is one of the most effective tools for people to browse and purchase products on e-commerce platforms. Recent advances have mainly focused on ranking products by their likelihood to be purchased through retrieval models. However, they overlook the problem that users may not understand why certain products are retrieved for them. The lack of appropriate explanations can lead to an unsatisfactory user experience and further decrease user trust in the platforms. To address this problem, we propose a Query-aware Explainable Product Search with Reinforcement Knowledge Reasoning, namely QEPS , which uses search behaviors related to the current query to reinforce explanations. Specifically, with the aim of retrieving suitable products with explanations, QEPS takes full advantage of the user-product knowledge graph (KG) and develops a reinforcement learning approach, characterized by the demonstration-guided policy network and query-aware rewards, to perform explicit multi-step reasoning on the KG. The reasoning paths between users and products are automatically derived from the current query-related search behavior, which can provide valuable signals as to why the retrieved products are more likely to satisfy the user&#39;s search intent. Empirical experiments on four datasets show that our model achieves remarkable performance and is able to generate reasonable explanations for the search results.},
  archive      = {J_TKDE},
  author       = {Qiannan Zhu and Haobo Zhang and Qing He and Zhicheng Dou},
  doi          = {10.1109/TKDE.2023.3297331},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1260-1273},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Query-aware explainable product search with reinforcement knowledge graph reasoning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parallel and external-memory construction of minimal perfect
hash functions with PTHash. <em>TKDE</em>, <em>36</em>(3), 1249–1259.
(<a href="https://doi.org/10.1109/TKDE.2023.3303341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A function $f : U \to \lbrace 0,\ldots,n-1\rbrace$ is a minimal perfect hash function for a set $S \subseteq U$ of size $n$ , if $f$ bijectively maps $S$ into the first $n$ natural numbers. These functions are important for many practical applications in computing, such as search engines, computer networks, and databases. Several algorithms have been proposed to build minimal perfect hash functions that: scale well to large sets, retain fast evaluation time, and take very little space, e.g., 2 – 3 bits/key. PTHash is one such algorithm, achieving very fast evaluation in compressed space, typically many times faster than other techniques. In this work, we propose a new construction algorithm for PTHash enabling: (1) multi-threading , to either build functions more quickly or more space-efficiently, and (2) external-memory processing , to scale to inputs much larger than the available internal memory. Only few other algorithms in the literature share these features, despite of their practical impact. We conduct an extensive experimental assessment on large real-world string collections and show that, with respect to other techniques, PTHash is competitive in construction time and space consumption, but retains 2 – 6× better lookup time.},
  archive      = {J_TKDE},
  author       = {Giulio Ermanno Pibiri and Roberto Trani},
  doi          = {10.1109/TKDE.2023.3303341},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1249-1259},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Parallel and external-memory construction of minimal perfect hash functions with PTHash},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Outliers robust unsupervised feature selection for
structured sparse subspace. <em>TKDE</em>, <em>36</em>(3), 1234–1248.
(<a href="https://doi.org/10.1109/TKDE.2023.3297226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is one of the important topics of machine learning, and it has a wide range of applications in data preprocessing. At present, feature selection based on $\ell _{2,1}$ -norm regularization is a relatively mature method, but it is not enough to maximize the sparsity and parameter-tuning leads to increased costs. Later scholars found that the $\ell _{2,0}$ -norm constraint is more conductive to feature selection, but it is difficult to solve and lacks convergence guarantees. To address these problems, we creatively propose a novel Outliers Robust Unsupervised Feature Selection for structured sparse subspace (ORUFS), which utilizes $\ell _{2,0}$ -norm constraint to learn a structured sparse subspace and avoid tuning the regularization parameter. Moreover, by adding binary weights, outliers are directly eliminated and the robustness of model is improved. More importantly, a Re-Weighted (RW) algorithm is exploited to solve our $\ell _{p}$ -norm problem. For the NP-hard problem of $\ell _{2,0}$ -norm constraint, we develop an effective iterative optimization algorithm with strict convergence guarantees and closed-form solution. Subsequently, we provide theoretical analysis about convergence and computational complexity. Experimental results on real-world datasets illustrate that our method is superior to the state-of-the-art methods in clustering and anomaly detection tasks.},
  archive      = {J_TKDE},
  author       = {Sisi Wang and Feiping Nie and Zheng Wang and Rong Wang and Xuelong Li},
  doi          = {10.1109/TKDE.2023.3297226},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1234-1248},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Outliers robust unsupervised feature selection for structured sparse subspace},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Multivariate cognitive response framework for student
performance prediction on MOOC. <em>TKDE</em>, <em>36</em>(3),
1221–1233. (<a href="https://doi.org/10.1109/TKDE.2023.3302848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on student&#39;s cognitive structure, the cognitive diagnostic models (CDMs) can reveal the potential relationships among the student&#39;s knowledge level, test item features and the corresponding item scores, and then predict each student&#39;s future performance. However, due to the simplistic prior information and deficient cognitive mechanism, most of the existing CDMs have limited prediction performance. To address the issues, we propose the multivariate cognitive response framework (MvCRF). We first collect student&#39;s learning activity logs to calculate the corresponding effort trait. Considering both student&#39;s ability trait and effort trait, MvCRF then introduces the compensation mechanism to calculate student&#39;s knowledge level. In addition, we introduce not only the slip and guessing parameters in prediction but also the skill weakness parameter related with the student&#39;s knowledge level and the importance of each skill on solving specific item. Experimental results on both simulation study and real-data application on MOOC demonstrate that MvCRF achieves better prediction performance, robustness and interpretability than the baseline CDMs.},
  archive      = {J_TKDE},
  author       = {Lianhong Wang and Xiaoyao Li and Zhihui Luo and Zinan Hu and Qing Yan},
  doi          = {10.1109/TKDE.2023.3302848},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1221-1233},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multivariate cognitive response framework for student performance prediction on MOOC},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-free subsampling method based on uniform designs.
<em>TKDE</em>, <em>36</em>(3), 1210–1220. (<a
href="https://doi.org/10.1109/TKDE.2023.3297167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subsampling or subdata selection is a useful approach in large-scale statistical learning. Most existing studies focus on model-based subsampling methods which significantly depend on the model assumption. In this article, we consider the model-free subsampling strategy for generating subdata from the original full data. In order to measure the goodness of representation of a subdata with respect to the original data, we propose a criterion, generalized empirical $F$ -discrepancy (GEFD), and study its theoretical properties in connection with the classical generalized $\ell _{2}$ -discrepancy in the theory of uniform designs. These properties allow us to develop a kind of low-GEFD data-driven subsampling method based on the existing uniform designs. By simulation examples and a real case study, we show that the proposed subsampling method is superior to the random sampling method. Moreover, our method keeps robust under diverse model specifications while other popular model-based subsampling methods are under-performing. In practice, such a model-free property is more appealing than the model-based subsampling methods, where the latter may have poor performance when the model is misspecified, as demonstrated in our simulation studies. In addition, our method is orders of magnitude faster than other model-free subsampling methods, which makes it more applicable for subsampling of Big Data.},
  archive      = {J_TKDE},
  author       = {Mei Zhang and Yongdao Zhou and Zheng Zhou and Aijun Zhang},
  doi          = {10.1109/TKDE.2023.3297167},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1210-1220},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Model-free subsampling method based on uniform designs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Let the information fly: Reconstructing social network after
a node deleted. <em>TKDE</em>, <em>36</em>(3), 1198–1209. (<a
href="https://doi.org/10.1109/TKDE.2023.3303488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite considerable research into information diffusion, most models focus on static networks. Networks in the real world change over time and information is lost when a node disappears. We sought to resolve the problem of information loss by defining information and information loss in general terms. Experiments demonstrate the efficacy of the proposed scheme in resolving problems of information loss in real-world networks using only a few edges.},
  archive      = {J_TKDE},
  author       = {Shiou-Chi Li and Jen-Wei Huang},
  doi          = {10.1109/TKDE.2023.3303488},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1198-1209},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Let the information fly: Reconstructing social network after a node deleted},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning idempotent representation for subspace clustering.
<em>TKDE</em>, <em>36</em>(3), 1183–1197. (<a
href="https://doi.org/10.1109/TKDE.2023.3303343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The critical point for the success of spectral-type subspace clustering algorithms is to seek reconstruction coefficient matrices that can faithfully reveal the subspace structures of data sets. An ideal reconstruction coefficient matrix should have two properties: 1) it is block-diagonal with each block indicating a subspace; 2) each block is fully connected. We find that a normalized membership matrix naturally satisfies the above two conditions. Therefore, in this paper, we devise an idempotent representation (IDR) algorithm to pursue reconstruction coefficient matrices approximating normalized membership matrices. IDR designs a new idempotent constraint. And by combining the doubly stochastic constraints, the coefficient matrices which are close to normalized membership matrices could be directly achieved. We present an optimization algorithm for solving IDR problem and analyze its computation burden as well as convergence. The comparisons between IDR and related algorithms show the superiority of IDR. Plentiful experiments conducted on both synthetic and real-world datasets prove that IDR is an effective subspace clustering algorithm.},
  archive      = {J_TKDE},
  author       = {Lai Wei and Shiteng Liu and Rigui Zhou and Changming Zhu and Jin Liu},
  doi          = {10.1109/TKDE.2023.3303343},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1183-1197},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning idempotent representation for subspace clustering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge-aware collaborative filtering with pre-trained
language model for personalized review-based rating prediction.
<em>TKDE</em>, <em>36</em>(3), 1170–1182. (<a
href="https://doi.org/10.1109/TKDE.2023.3301884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized review-based rating prediction aims at leveraging existing reviews to model user interests and item characteristics for rating prediction. Most of the existing studies mainly encounter two issues. First, the rich knowledge contained in the fine-grained aspects of each review and the knowledge graph is rarely considered to complement the pure text for better modeling user-item interactions. Second, the power of pre-trained language models is not carefully studied for personalized review-based rating prediction. To address these issues, we propose an approach named Knowledge-aware Collaborative Filtering with Pre-trained Language Model (KCF-PLM). For the first issue, to utilize rich knowledge, KCF-PLM develops a transformer network to model the interactions of the extracted aspects w.r.t. a user-item pair. For the second issue, to better represent users and items, KCF-PLM takes all the historical reviews of a user or an item as input to pre-trained language models. Moreover, KCF-PLM integrates the transformer network and the pre-trained language models through representation propagation on the knowledge graph and user-item guided attention of the aspect representations. Thus KCF-PLM combines review text, aspect, knowledge graph, and pre-trained language models together for review-based rating prediction. We conduct comprehensive experiments on several public datasets, demonstrating the effectiveness of KCF-PLM.},
  archive      = {J_TKDE},
  author       = {Quanxiu Wang and Xinlei Cao and Jianyong Wang and Wei Zhang},
  doi          = {10.1109/TKDE.2023.3301884},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1170-1182},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Knowledge-aware collaborative filtering with pre-trained language model for personalized review-based rating prediction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Iterative graph self-distillation. <em>TKDE</em>,
<em>36</em>(3), 1161–1169. (<a
href="https://doi.org/10.1109/TKDE.2023.3303885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there has been increasing interest in the challenge of how to discriminatively vectorize graphs. To address this, we propose a method called Iterative Graph Self-Distillation (IGSD) which learns graph-level representation in an unsupervised manner through instance discrimination using a self-supervised contrastive learning approach. IGSD involves a teacher-student distillation process that uses graph diffusion augmentations and constructs the teacher model using an exponential moving average of the student model. The intuition behind IGSD is to predict the teacher network representation of the graph pairs under different augmented views. As a natural extension, we also apply IGSD to semi-supervised scenarios by jointly regularizing the network with both supervised and self-supervised contrastive loss. Finally, we show that fine-tuning the IGSD-trained models with self-training can further improve graph representation learning. Empirically, we achieve significant and consistent performance gain on various graph datasets in both unsupervised and semi-supervised settings, which well validates the superiority of IGSD.},
  archive      = {J_TKDE},
  author       = {Hanlin Zhang and Shuai Lin and Weiyang Liu and Pan Zhou and Jian Tang and Xiaodan Liang and Eric P. Xing},
  doi          = {10.1109/TKDE.2023.3303885},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1161-1169},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Iterative graph self-distillation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identifying large structural balanced cliques in signed
graphs. <em>TKDE</em>, <em>36</em>(3), 1145–1160. (<a
href="https://doi.org/10.1109/TKDE.2023.3295803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Signed graphs have been used to capture the polarity of relationships through positive/negative edge signs. In this paper, we consider balanced cliques — a clique is balanced if its vertex set $C$ can be partitioned into $C_{L}$ and $C_{R}$ such that all negative edges are between $C_{L}$ and $C_{R}$ — and study the problems of maximum balanced clique computation and large balanced clique enumeration. Our main idea is a novel graph reduction that transforms a balanced clique problem over a signed graph $G$ to problems over small subgraphs of $G$ . Specifically, for each vertex $u$ in $G$ , we extract the subgraph $G_{u}$ of $G$ induced by $V_{L} \cup V_{R}$ ; $V_{L}$ is $u$ and $u$ &#39;s positive neighbors while $V_{R}$ is $u$ &#39;s negative neighbors. Then, we remove from $G_{u}$ all positive edges between $V_{L}$ and $V_{R}$ and all negative edges between vertices of the same set; denote the resulting graph of discarding edge signs as $g_{u}$ . We show that all balanced cliques containing $u$ in $G$ can be found by processing $g_{u}$ . Due to the small size and no edge signs, large cliques containing $u$ in $g_{u}$ can be efficiently identified. Experimental results on real signed graphs demonstrated the advantages of our techniques.},
  archive      = {J_TKDE},
  author       = {Kai Yao and Lijun Chang and Lu Qin},
  doi          = {10.1109/TKDE.2023.3295803},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1145-1160},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Identifying large structural balanced cliques in signed graphs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generating counterfactual instances for explainable
class-imbalance learning. <em>TKDE</em>, <em>36</em>(3), 1130–1144. (<a
href="https://doi.org/10.1109/TKDE.2023.3302847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing class imbalance learning paradigms focus on lifting the importance of minority instance, aiming to improve the model in terms of certain evaluation metrics (e.g., AUC and $F_{1}$ -measure). One drawback of these methods is that they lack enough transparency, hence, cannot be fully trusted in vital domains. To this end, this paper deal with the class imbalance learning task with counterfactual instances. Given an instance and a classifier, a counterfactual is a fake instance which, while having smallest distance to the original instance, is classified as a different class by the classifier. Therefore, the most important features for a classifier can be identified by inspecting the difference between an instance and its counterfactual. To utilize counterfactuals, a novel Explainable Generative Adversarial Network (EXGAN) is proposed. EXGAN has a unique “two generators versus multiple discriminators” architecture where the generators are used to generate effective counterfactuals and discriminators are trained for the class imbalance learning task. In addition to the architecture, an innovative ensemble loss function ensuring each discriminator complementing each other is designed to overcome the class imbalance issue. Extensive experiments prove that the counterfactuals generated by EXGAN can be used to produce effective local explanation and provide significant better class imbalance learning ability than existing competitors.},
  archive      = {J_TKDE},
  author       = {Zhi Chen and Jiang Duan and Li Kang and Hongyan Xu and Rui Chen and Guoping Qiu},
  doi          = {10.1109/TKDE.2023.3302847},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1130-1144},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Generating counterfactual instances for explainable class-imbalance learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast and slow thinking: A two-step schema-aware approach for
instance completion in knowledge graphs. <em>TKDE</em>, <em>36</em>(3),
1113–1129. (<a href="https://doi.org/10.1109/TKDE.2023.3304137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern Knowledge Graphs (KG) often suffer from an incompleteness issue (i.e., missing facts). By representing a fact as a triplet $(h,r,t)$ linking two entities $h$ and $t$ via a relation $r$ , existing KG completion approaches mostly consider a link prediction task to solve this problem, i.e., given two elements of a triplet predicting the missing one, such as $(h,r,?)$ . However, this task implicitly has a strong yet impractical assumption on the two given elements in a triplet, which have to be correlated, resulting otherwise in meaningless predictions, such as ( Marie Curie , headquarters location , ?). Against this background, this paper studies an instance completion task suggesting $r$ - $t$ pairs for a given $h$ , i.e., $(h,?,?)$ . Inspired by the human psychological principle “fast-and-slow thinking”, we propose a two-step schema-aware approach RETA++ to efficiently solve our instance completion problem. It consists of two components: a fast RETA-Filter efficiently filtering candidate $r$ - $t$ pairs schematically matching the given $h$ , and a deliberate RETA-Grader leveraging a KG embedding model scoring each candidate $r$ - $t$ pair considering the plausibility of both the input triplet and its corresponding schema. RETA++ systematically integrates them by training RETA-Grader on the reduced solution space output by RETA-Filter via a customized negative sampling process, so as to fully benefit from the efficiency of RETA-Filter in solution space reduction and the deliberation of RETA-Grader in scoring candidate triplets. We evaluate our approach against a sizable collection of state-of-the-art techniques on three real-world KG datasets. Results show that RETA-Filter can efficiently reduce the solution space for the instance completion task, outperforming best baseline techniques by 10.61%–84.75% on the reduced solution space size, while also being 1.7×–29.6x faster than these techniques. Moreover, RETA-Grader trained on the reduced solution space also significantly outperforms the best state-of-the-art techniques on the instance completion task by 31.90%–105.02%.},
  archive      = {J_TKDE},
  author       = {Dingqi Yang and Bingqing Qu and Paolo Rosso and Philippe Cudre-Mauroux},
  doi          = {10.1109/TKDE.2023.3304137},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1113-1129},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fast and slow thinking: A two-step schema-aware approach for instance completion in knowledge graphs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Enhancing locally adaptive smoothing of graph neural
networks via laplacian node disagreement. <em>TKDE</em>, <em>36</em>(3),
1099–1112. (<a href="https://doi.org/10.1109/TKDE.2023.3303212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) are designed to perform inference on data described by graph-structured node features and topology information. From the perspective of graph signal denoising, the typical message passing schemes of GNNs act as a globally uniform smoothing that minimizes disagreements between embeddings of connected nodes. However, the level of smoothing over different regions of the graph should be different, especially for those inter-class regions. This deviation limits the expressiveness of GNNs, and then renders them fragile to over-smoothing, long-range dependencies, and non-homophily settings. In this paper, we find that the node disagreements of initial graph features can present more trustworthy constraints on node embeddings, thereby enhancing the locally adaptive smoothing of GNNs. To spread the inherent disagreements of nodes, we propose the Laplacian node disagreement to jointly measure the initial features and output embeddings. With such a measurement, we then present a new graph signal denoising objective deriving a more effective message passing scheme and further incorporate it into the GNN architecture, named Laplacian node disagreement-based GNN (LND-GNN). Learning from its output node representations, we integrate an auxiliary disagreement constraint into the overall classification loss. Experiments demonstrate the expressive ability of LND-GNN in the downstream semi-supervised node classification task.},
  archive      = {J_TKDE},
  author       = {Yu Wang and Liang Hu and Xiaofeng Cao and Yi Chang and Ivor W. Tsang},
  doi          = {10.1109/TKDE.2023.3303212},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1099-1112},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Enhancing locally adaptive smoothing of graph neural networks via laplacian node disagreement},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient classification by removing bayesian confusing
samples. <em>TKDE</em>, <em>36</em>(3), 1084–1098. (<a
href="https://doi.org/10.1109/TKDE.2023.3303425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving the generalization performance of classifiers from data pre-processing perspective has recently received considerable attention in the machine learning community. Although many methods have been proposed in the past decades, most of them lack theoretical foundations and cannot guarantee better generalization performance of classifiers on processed datasets. To overcome this flaw, in this paper, we propose a method, which is supported by Bayesian decision theory and percolation theory, to improve generalization performance by removing Bayesian confusing samples (abbr. BCS). Specifically, for a training set, we define the samples that misclassified by the Bayesian optimal classifier as BCS and prove that a classifier trained on the training set after removing BCS can obtain better generalization performance. To find out BCS, we indicate that BCS can be identified according to the size of global homogeneous cluster, a set of samples with the same labels, based on percolation theory. Based on these analysis, we propose a method to construct global homogeneous clusters and remove BCS from the training set. Extensive experiments show that the proposed method is effective for a number of classical and state-of-the-art classifiers.},
  archive      = {J_TKDE},
  author       = {Fuyuan Cao and Qingqiang Chen and Ying Xing and Jiye Liang},
  doi          = {10.1109/TKDE.2023.3303425},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1084-1098},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient classification by removing bayesian confusing samples},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient balanced signed biclique search in signed
bipartite graphs. <em>TKDE</em>, <em>36</em>(3), 1069–1083. (<a
href="https://doi.org/10.1109/TKDE.2023.3296721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding bicliques is a fundamental problem in bipartite graph analysis, and can find numerous applications. However, previous studies only focus on unsigned bipartite graphs. Signed information, such as friend and enemy, naturally exists in real-world networks. It is critical to leverage signed information to better characterize biclique. To fill this gap, we propose a novel biclique model, named balanced signed biclique, by leveraging the property of balance theory. Specifically, given a signed bipartite graph $G$ and two positive integers $\tau _{U}, \tau _{V}$ , a subgraph $S=(U_{S},V_{S},E_{S})$ of $G$ is a balanced signed biclique if $i)$ $S$ is a biclique without any unstable motif, i.e., unbalanced butterfly, and $ii)$ $|U_{S}| \geq \tau _{U}$ and $|V_{S}| \geq \tau _{V}$ . In this paper, we propose and investigate two important problems, i.e., maximal balanced signed biclique enumeration and maximum balanced signed biclique identification. Due to the unique features of signed bipartite graphs, the previous works cannot be applied to our problems directly. For the enumeration task, to construct a reasonable baseline, we extend the existing biclique enumeration framework for unsigned bipartite graphs and integrate the developed balanced bipartite graph property. To scale for large networks, optimized strategies are proposed to overcome the three limitations in the baseline method. For the identification task, we first propose a baseline method by leveraging the proposed enumeration framework. Moreover, employing novel optimizations, an anchor balanced bipartite graph based search framework is introduced to accelerate the search. Finally, extensive experiments are conducted on 8 real-world datasets to demonstrate the efficiency and effectiveness of the proposed techniques and model.},
  archive      = {J_TKDE},
  author       = {Renjie Sun and Yanping Wu and Xiaoyang Wang and Chen Chen and Wenjie Zhang and Xuemin Lin},
  doi          = {10.1109/TKDE.2023.3296721},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1069-1083},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient balanced signed biclique search in signed bipartite graphs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic assortment selection under inventory and limited
switches constraints. <em>TKDE</em>, <em>36</em>(3), 1056–1068. (<a
href="https://doi.org/10.1109/TKDE.2023.3301649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing the assortment of products to display to customers is key to increasing revenue for both offline and online retailers. To trade-off between exploring customers’ preferences and exploiting customers’ choices learned from data, in this article, by adopting the Multi-Nomial Logit (MNL) choice model to capture customers’ choices over products, we study the problem of optimizing assortments over a planning horizon $T$ for maximizing the profit of the retailer. To make the problem setting more practical, we consider both the inventory constraint and the limited switches constraint, where the retailer is forced to stop the sales when the resources are depleted and is forbidden to switch the assortment shown to customers too many times. Such a setting suits the case when an online retailer wants to optimize the assortment selection for a population of customers dynamically. We develop an efficient UCB-like algorithm to optimize the assortments while learning customers’ choices from data. We prove that our algorithm can achieve a sub-linear regret bound $\tilde{O}(T^{\max \lbrace 2/3-\alpha /3,1/2\rbrace })$ if $O(T^\alpha)$ switches are allowed. Extensive numerical experiments show that our algorithm outperforms baselines, and the gap between our algorithm&#39;s performance and the theoretical upper bound is small.},
  archive      = {J_TKDE},
  author       = {Hongbin Zhang and Qixin Zhang and Feng Wu and Yu Yang},
  doi          = {10.1109/TKDE.2023.3301649},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1056-1068},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dynamic assortment selection under inventory and limited switches constraints},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain adaptation and summary distillation for unsupervised
query focused summarization. <em>TKDE</em>, <em>36</em>(3), 1044–1055.
(<a href="https://doi.org/10.1109/TKDE.2023.3296441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text summarizing is the task of reducing a document&#39;s length while maintaining its essential information. In the age of information explosion, how to obtain the content that users needed from a large volume of information becomes particularly significant. Under such circumstances, query-focused abstractive summarization ( qfs ) becomes more dominant since it is able to focus on user needs while delivering fluent, concise, succinct paraphrased summaries. However, unlike generic summarization, which has achieved remarkable progress driven by a substantial amount of parallel data, the qfs struggles due to a deficiency of parallel corpus. Therefore, in this paper, we leverage a typical large generic summarization dataset to facilitate the pressing demands on unsupervised qfs . The large-scale query-free benchmark is automatically transformed into a query-focused dataset (Query-CNNDM) while preserving its informative summaries. We propose a simple yet effective unsupervised method, called D omain A daptation and S ummary D istillation method ( DASD ). In the model, to achieve the domain adaptation for unsupervised qfs , we design a query-aware gap sentence generation (q-GSG) strategy to equip the model with the capability of learning target textual knowledge and obtaining a good initialization at the target domain. As instance-specific regularization, we train a teacher model with the Query-CNNDM to generate pseudo-labels for summary distillation. Experimental results indicate that our DASD model achieves state-of-the-art performance on two benchmark datasets, Debatepedia and Wikiref, in a zero-shot setting and shows good generalization to the abstractive few-shot qfs .},
  archive      = {J_TKDE},
  author       = {Jiancheng Du and Yang Gao},
  doi          = {10.1109/TKDE.2023.3296441},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1044-1055},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Domain adaptation and summary distillation for unsupervised query focused summarization},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distance information improves heterogeneous graph neural
networks. <em>TKDE</em>, <em>36</em>(3), 1030–1043. (<a
href="https://doi.org/10.1109/TKDE.2023.3300879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous graph neural network (HGNN) has shown superior performance and attracted considerable research interest. However, HGNN inherits the limitation of expressive power from GNN via learning $individual$ node embeddings based on their structural neighbors, largely ignoring the potential correlations between nodes and leading to sub-optimal performance. How to establish correlations among multiple node embeddings and improve the expressive power of HGNN is still an open problem. To solve the above problem, we propose a simple and effective technique called heterogeneous distance encoding (HDE) to fundamentally improve the expressive power of HGNN. Specifically, we define heterogeneous shortest path distance to describe the relative distance between nodes, and then jointly encode such distances for multiple nodes of interest to establish their correlation. By simply injecting the encoded correlation into the neighbor aggregating process, we can learn more expressive heterogeneous graph representations for downstream tasks. More importantly, the proposed HDE relies only on the graph structure and ensures the inductive ability of HGNN. We also propose an efficient HDE algorithm that can significantly reduce the computational overhead. Significant improvements on both transductive and inductive tasks over four real-world graphs demonstrate the effectiveness of HDE in improving the expressive power of HGNN.},
  archive      = {J_TKDE},
  author       = {Chuan Shi and Houye Ji and Zhiyuan Lu and Ye Tang and Pan Li and Cheng Yang},
  doi          = {10.1109/TKDE.2023.3300879},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1030-1043},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Distance information improves heterogeneous graph neural networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Denoising variational graph of graphs auto-encoder for
predicting structured entity interactions. <em>TKDE</em>,
<em>36</em>(3), 1016–1029. (<a
href="https://doi.org/10.1109/TKDE.2023.3298490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interactions between structured entities play important roles in a wide range of applications such as chemistry, material science, biology, and medical science. Recently, graph-based methods have been exploited to effectively predict the interactions among structured entities. However, these methods usually only focus on structural information of the entities and are incapable of fully utilizing the interaction information between the entities. In this paper, we propose a Denoising Variational Graph of Graphs Auto-encoder (DVGGA) which follows the graph of graphs framework to capture both structural information in structured entities and interaction information among structured entities. With denoising criterion, DVGGA is able to capture the information from the useful structures of the local graph and address the overfitting issue caused by redundant substructures. Extensive experiments conducted on real-world datasets show that DVGGA outperforms the state-of-the-art structured entity interaction prediction methods.},
  archive      = {J_TKDE},
  author       = {Han Chen and Hanchen Wang and Hongmei Chen and Ying Zhang and Wenjie Zhang and Xuemin Lin},
  doi          = {10.1109/TKDE.2023.3298490},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1016-1029},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Denoising variational graph of graphs auto-encoder for predicting structured entity interactions},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DB-LSH 2.0: Locality-sensitive hashing with query-based
dynamic bucketing. <em>TKDE</em>, <em>36</em>(3), 1000–1015. (<a
href="https://doi.org/10.1109/TKDE.2023.3295831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Locality-sensitive hashing (LSH) is a promising family of methods for the high-dimensional approximate nearest neighbor (ANN) search problem due to its sub-linear query time and strong theoretical guarantee. Existing LSH methods either suffer from large index sizes and hash boundary problems, or incur a linear cost for high-quality candidate identification. This dilemma is addressed in a novel method called DB-LSH proposed in this paper. It organizes the projected spaces with multi-dimensional indexes instead of fixed-width hash buckets, which significantly reduces space costs. High-quality candidates can be generated efficiently by dynamically constructing query-based hypercubic buckets with the required widths through index-based window queries. A novel incremental search strategy called DBI-LSH is also developed to further boost the query performance, which incrementally accesses the next best point for higher accuracy and efficiency. Considering the intermediate query information of each query, DBA-LSH is designed to adaptively tune termination conditions without scarifying the success probability. Our theoretical analysis proves that DB-LSH has a smaller query cost than the existing work while DBA-LSH and DBI-LSH have lower expected query costs than DB-LSH. An extensive range of experiments on real-world data show the superiority of our approaches over the state-of-the-art methods in both efficiency and accuracy.},
  archive      = {J_TKDE},
  author       = {Yao Tian and Xi Zhao and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2023.3295831},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {1000-1015},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DB-LSH 2.0: Locality-sensitive hashing with query-based dynamic bucketing},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Comprehensive privacy analysis on federated recommender
system against attribute inference attacks. <em>TKDE</em>,
<em>36</em>(3), 987–999. (<a
href="https://doi.org/10.1109/TKDE.2023.3295601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, recommender systems are crucially important for the delivery of personalized services that satisfy users’ preferences. With personalized recommendation services, users can enjoy a variety of recommendations such as movies, books, ads, restaurants, and more. Despite the great benefits, personalized recommendations typically require the collection of personal data for user modelling and analysis, which can make users susceptible to attribute inference attacks. Specifically, the vulnerability of existing centralized recommenders under attribute inference attacks leaves malicious attackers a backdoor to infer users’ private attributes, as the systems remember information of their training data (i.e., interaction data and side information). An emerging practice is to implement recommender systems in the federated setting, which enables all user devices to collaboratively learn a shared global recommender while keeping all the training data on device. However, the privacy issues in federated recommender systems have been rarely explored. In this paper, we first design a novel attribute inference attacker to perform a comprehensive privacy analysis of the GCN-based federated recommender models. The experimental results show that the vulnerability of each model component against attribute inference attack is varied, highlighting the need for new defense approaches. Therefore, we propose a novel adaptive privacy-preserving approach to protect users’ sensitive data in the presence of attribute inference attacks and meanwhile maximize the recommendation accuracy. Extensive experimental results on two real-world datasets validate the superior performance of our model on both recommendation effectiveness and resistance to inference attacks.},
  archive      = {J_TKDE},
  author       = {Shijie Zhang and Wei Yuan and Hongzhi Yin},
  doi          = {10.1109/TKDE.2023.3295601},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {987-999},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Comprehensive privacy analysis on federated recommender system against attribute inference attacks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collaborative graph neural networks for attributed network
embedding. <em>TKDE</em>, <em>36</em>(3), 972–986. (<a
href="https://doi.org/10.1109/TKDE.2023.3298002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have shown prominent performance on attributed network embedding. However, existing efforts mainly focus on exploiting network structures, while the exploitation of node attributes is rather limited as they only serve as node features at the initial layer. This simple strategy impedes the potential of node attributes in augmenting node connections, leading to limited receptive field for inactive nodes with few or even no neighbors. Furthermore, the training objectives (i.e., reconstructing network structures) of most GNNs also do not include node attributes, although studies have shown that reconstructing node attributes is beneficial. Thus, it is encouraging to deeply involve node attributes in the key components of GNNs, including graph convolution operations and training objectives. However, this is a nontrivial task since an appropriate way of integration is required to maintain the merits of GNNs. To bridge the gap, in this paper, we propose COllaborative graph Neural Networks–CONN, a tailored GNN architecture for attribute network embedding. It improves model capacity by 1) selectively diffusing messages from neighboring nodes and involved attribute categories, and 2) jointly reconstructing node-to-node and node-to-attribute-category interactions via cross-correlation. Experiments on real-world networks demonstrate that CONN excels state-of-the-art embedding algorithms with a great margin.},
  archive      = {J_TKDE},
  author       = {Qiaoyu Tan and Xin Zhang and Xiao Huang and Hao Chen and Jundong Li and Xia Hu},
  doi          = {10.1109/TKDE.2023.3298002},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {972-986},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Collaborative graph neural networks for attributed network embedding},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Boosting subspace co-clustering via bilateral graph
convolution. <em>TKDE</em>, <em>36</em>(3), 960–971. (<a
href="https://doi.org/10.1109/TKDE.2023.3300814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspace clustering seeks to cluster high-dimensional data lying in a union of low-dimensional subspaces. It has achieved state-of-the-art results in image clustering, but text clustering of document-term matrices, has proved more impervious to advances with this approach, even though text data satisfies the assumptions of subspace clustering. We hypothesize that this is because such matrices are generally sparser and higher-dimensional than images. This, combined with the complexity of subspace clustering, which is generally cubic in the number of inputs, makes its use impractical in the context of text. Here we address these issues with a view to leveraging subspace clustering for networked (or not) text data. We first extend the concept of subspace clustering to co-clustering, which is suitable to deal with document-term matrices because of the interplay engendered between the document and word representations. We then address the sparsity problem through bilateral graph convolution, which promotes the grouping effect that has been credited for the effectiveness of some subspace clustering models. The proposed formulation results in an algorithm that is computationally/spatially efficient. Experiments using real-world datasets demonstrate the superior performance, in terms of document clustering, word clustering, and computational efficiency, of our proposed approach over the baselines and comparable methods.},
  archive      = {J_TKDE},
  author       = {Chakib Fettal and Lazhar Labiod and Mohamed Nadif},
  doi          = {10.1109/TKDE.2023.3300814},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {960-971},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Boosting subspace co-clustering via bilateral graph convolution},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on arabic named entity recognition: Past, recent
advances, and future trends. <em>TKDE</em>, <em>36</em>(3), 943–959. (<a
href="https://doi.org/10.1109/TKDE.2023.3303136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As more and more Arabic texts emerged on the Internet, extracting important information from these Arabic texts is especially useful. As a fundamental technology, Named entity recognition (NER) serves as the core component in information extraction technology, while also playing a critical role in many other Natural Language Processing (NLP) systems, such as question answering and knowledge graph building. In this paper, we provide a comprehensive review of the development of Arabic NER, especially the recent advances in deep learning and pre-trained language model. Specifically, we first introduce the background of Arabic NER, including the characteristics of Arabic and existing resources for Arabic NER. Then, we systematically review the development of Arabic NER methods. Traditional Arabic NER systems focus on feature engineering and designing domain-specific rules. In recent years, deep learning methods achieve significant progress by representing texts via continuous vector representations. With the growth of pre-trained language model, Arabic NER yields better performance. Finally, we conclude the method gap between Arabic NER and NER methods from other languages, which helps outline future directions for Arabic NER.},
  archive      = {J_TKDE},
  author       = {Xiaoye Qu and Yingjie Gu and Qingrong Xia and Zechang Li and Zhefeng Wang and Baoxing Huai},
  doi          = {10.1109/TKDE.2023.3303136},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {943-959},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey on arabic named entity recognition: Past, recent advances, and future trends},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). <span
class="math inline"><em>D</em><sup>2</sup><em>M</em><em>T</em><em>S</em></span>:
Enabling dependable data collection with multiple crowdsourcers trust
sharing in mobile crowdsensing. <em>TKDE</em>, <em>36</em>(3), 927–942.
(<a href="https://doi.org/10.1109/TKDE.2023.3294503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When enjoying mobile crowdsensing (MCS), it is vital to evaluate the trustworthiness of mobile users (MUs) without disclosing their sensitive information. However, the existing schemes ignore this requirement in the multiple crowdsourcers (CSs) scenario. The lack of a credible sharing about MUs’ trustworthiness results in an inaccurate trust evaluation, disabling allocating tasks to reliable MUs. To address it, based on the analysis of the desired properties, we propose a scheme enabling d ependable d ata collection with m ultiple crowdsourcers t rust s haring ( $D^{2}MTS$ ). Specifically, we design the MU anonymous management. Two kinds of MU generated pseudonym systems without relationships are presented to mark each MU in trust evaluation and task execution, respectively. Through the devised pseudonym changes on these pseudonyms and the common token distribution algorithm, $D^{2}MTS$ realizes privacy-preserving trust sharing. Moreover, to guarantee credible sharing, based on the hash chain, $D^{2}MTS$ records MUs’ trustworthiness with the unforgeable signature on the blockchain established by multiple CSs which do not trust each other naturally. Extensive experiments show that compared with the other works, $D^{2}MTS$ &#39;s detection ratio of vicious MUs and the percentage of reliable MUs among the selected ones can increase by 208.61% and 28.27%. Both computational and communication delays are limited.},
  archive      = {J_TKDE},
  author       = {Bin Luo and Xinghua Li and Ximeng Liu and Jingjing Guo and Yanbing Ren and Siqi Ma and Jianfeng Ma},
  doi          = {10.1109/TKDE.2023.3294503},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  number       = {3},
  pages        = {927-942},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {$D^{2}MTS$: Enabling dependable data collection with multiple crowdsourcers trust sharing in mobile crowdsensing},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). XSimGCL: Towards extremely simple graph contrastive learning
for recommendation. <em>TKDE</em>, <em>36</em>(2), 913–926. (<a
href="https://doi.org/10.1109/TKDE.2023.3288135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning (CL) has recently been demonstrated critical in improving recommendation performance. The underlying principle of CL-based recommendation models is to ensure the consistency between representations derived from different graph augmentations of the user-item bipartite graph. This self-supervised approach allows for the extraction of general features from raw data, thereby mitigating the issue of data sparsity. Despite the effectiveness of this paradigm, the factors contributing to its performance gains have yet to be fully understood. This paper provides novel insights into the impact of CL on recommendation. Our findings indicate that CL enables the model to learn more evenly distributed user and item representations, which alleviates the prevalent popularity bias and promoting long-tail items. Our analysis also suggests that the graph augmentations, previously considered essential, are relatively unreliable and of limited significance in CL-based recommendation. Based on these findings, we put forward an e X tremely Sim ple G raph C ontrastive L earning method ( XSimGCL ) for recommendation, which discards the ineffective graph augmentations and instead employs a simple yet effective noise-based embedding augmentation to generate views for CL. A comprehensive experimental study on four large and highly sparse benchmark datasets demonstrates that, though the proposed method is extremely simple, it can smoothly adjust the uniformity of learned representations and outperforms its graph augmentation-based counterparts by a large margin in both recommendation accuracy and training efficiency.},
  archive      = {J_TKDE},
  author       = {Junliang Yu and Xin Xia and Tong Chen and Lizhen Cui and Nguyen Quoc Viet Hung and Hongzhi Yin},
  doi          = {10.1109/TKDE.2023.3288135},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {913-926},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {XSimGCL: Towards extremely simple graph contrastive learning for recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-step strategy for domain adaptation retrieval.
<em>TKDE</em>, <em>36</em>(2), 897–912. (<a
href="https://doi.org/10.1109/TKDE.2023.3289882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional hash-based retrieval method rely on the assumption that the query and database are of the identical domain. However, cross-domain problem often occurs in real-world applications, leading to the unsatisfactory performance of existing hashing methods. Recently, some researchers have put forward domain adaptation retrieval (DAR) under the perspective of domain adaptation (DA) and achieved promising results. But the following limitations still exist: 1) a single function is used to handle two challenges, i.e., domain adaptation and hashing, which is not flexible to explore enough underlying information for simultaneously accomplishing these challenges well; 2) non-dominant features in the sample are ignored; 3) the dissimilarity structure of dissimilar samples is not taken into account. To address the above problems, we propose a novel framework named two-step strategy (TSS) for domain adaptation retrieval, which advocates dividing DAR into two steps: DA step and hashing step. A DA function and a hash function are learned to handle the above two challenges, respectively, making the process more reasonable. Additionally, a discriminant semantic fusion loss is proposed to improve the discriminative ability among classes. Unlike other works that focus on discovering dominant features, we exploit the neglected non-dominant features and assign them attention with sinusoidal semantic embedding, actively creating a clear separation between classes. At last, we present an adaptive similarity preserving loss to preserve the similarity structure of the original data in all intra-domain and inter-domain hash codes. Extensive experiments on various datasets demonstrate that the proposed TSS achieves state-of-the-art performance.},
  archive      = {J_TKDE},
  author       = {Yonghao Chen and Xiaozhao Fang and Yuanyuan Liu and Wei Zheng and Peipei Kang and Na Han and Shengli Xie},
  doi          = {10.1109/TKDE.2023.3289882},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {897-912},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Two-step strategy for domain adaptation retrieval},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TRustworthy uncertainty propagation for sequential
time-series analysis in RNNs. <em>TKDE</em>, <em>36</em>(2), 882–896.
(<a href="https://doi.org/10.1109/TKDE.2023.3288628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The massive time-series production through the Internet of Things and digital healthcare requires novel data modeling and prediction. Recurrent neural networks (RNNs) are extensively used for analyzing time-series data. However, these models are unable to assess prediction uncertainty, which is particularly critical in heterogeneous and noisy environments. Bayesian inference allows reasoning about predictive uncertainty by estimating the posterior distribution of the parameters. The challenge remains in propagating the high-dimensional distribution through the sequential, non-linear layers of RNNs, resulting in mode collapse leading to erroneous uncertainty estimation and exacerbating the gradient explosion problem. This paper proposes a TRustworthy Uncertainty propagation for Sequential Time-series analysis (TRUST) in RNNs by introducing a Gaussian prior over network parameters and estimating the first two moments of the Gaussian variational distribution using the evidence lower bound. We propagate the variational moments through the sequential, non-linear layers of RNNs using the first-order Taylor approximation. The propagated covariance of the predictive distribution captures uncertainty in the output decision. The extensive experiments using ECG5000 and PeMS-SF classification and weather and power consumption prediction tasks demonstrate 1) significant robustness of TRUST-RNNs against noise and adversarial attacks and 2) self-assessment through the uncertainty that increases significantly with increasing noise.},
  archive      = {J_TKDE},
  author       = {Dimah Dera and Sabeen Ahmed and Nidhal Carla Bouaynaya and Ghulam Rasool},
  doi          = {10.1109/TKDE.2023.3288628},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {882-896},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {TRustworthy uncertainty propagation for sequential time-series analysis in RNNs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards effective and robust graph contrastive learning with
graph autoencoding. <em>TKDE</em>, <em>36</em>(2), 868–881. (<a
href="https://doi.org/10.1109/TKDE.2023.3288280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph contrastive learning (GCL) has become the de-facto approach to conducting self-supervised learning on graphs for its superior performance. However, non-semantic graph augmentation methods prevent it from achieving better performance, and it suffers from vulnerability to graph attacks. To deal with these problems, we propose AEGCL to leverage graph AutoEncoder in Graph Contrastive Learning which directly targets graph property reconstruction to boost GCL effectiveness and robustness. Specifically, AEGCL has two distinctive characteristics, (1) a novel adaptive augmentation strategy based on motif centrality is proposed, which leverages semantic significant higher-order graph property; (2) the original attributed graph is decoupled into feature graph and topology graph to extract their dedicated information, and a simple AttnFuse is proposed to combine the two augmented graphs and the two decoupled graphs. Graph autoencoder can thus be applied to the topology domain and raw attribute domain. Empirically, extensive experiments on benchmark graph datasets show that AEGCL outperforms existing baseline methods in terms of classification accuracy and robustness.},
  archive      = {J_TKDE},
  author       = {Wen-Zhi Li and Chang-Dong Wang and Jian-Huang Lai and Philip S. Yu},
  doi          = {10.1109/TKDE.2023.3288280},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {868-881},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards effective and robust graph contrastive learning with graph autoencoding},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The influence of digital technologies on knowledge
management in engineering: A systematic literature review.
<em>TKDE</em>, <em>36</em>(2), 854–867. (<a
href="https://doi.org/10.1109/TKDE.2023.3285952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital technologies are gaining widespread acceptance in engineering and offer opportunities for collating and curating knowledge during and beyond the life cycle of engineering products. Knowledge is central to strategy and operations in most engineering organizations and digital technologies have been employed in attempts to improve current knowledge management practices. A systematic literature review was undertaken to address the question: how do digital technologies influence knowledge management in the engineering sector? Twenty-seven primary studies were identified from 3097 papers on these topics within the engineering literature published between 2010 and 2022. Four knowledge management processes supported by digital technologies were recognized: knowledge creation, storage and retrieval, sharing and application. In supporting knowledge management, digital technologies were found to have been acting in five roles: repositories, transactive memory systems, communication spaces, boundary objects and non-human actors. However, the ability of digital technologies to perform these roles simultaneously had not been considered and similarly knowledge management had not been addressed as a holistic process. Hence, it was concluded that a holistic approach to knowledge management combined with the deployment of digital technologies in multiple roles simultaneously would likely yield significant competitive advantage and organizational value for organizations in the engineering sector.},
  archive      = {J_TKDE},
  author       = {Yuxin Yao and Eann A. Patterson and Richard J. Taylor},
  doi          = {10.1109/TKDE.2023.3285952},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {854-867},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {The influence of digital technologies on knowledge management in engineering: A systematic literature review},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Social-enhanced explainable recommendation with knowledge
graph. <em>TKDE</em>, <em>36</em>(2), 840–853. (<a
href="https://doi.org/10.1109/TKDE.2023.3292504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems are of crucial importance due to their wide applications. Knowledge graph (KG) enabled recommendation schemes have attracted great attention due to their superior performance and interpretability. However, the rich social information is not exploited for those systems, which limits the recommendation performance (Wang et al., 2023). In this paper, a novel explainable recommendation scheme is proposed by exploiting our designed social enhanced knowledge graph attention network (SKGAN). The hidden relations among users and items are learned and used for recommendation with the collaborative KG (CKG) and the user social graph (USG). Moreover, the high-order semantic information in both CKG and USG are obtained by using the graph convolution networks (GCNs) and the node level attention algorithm. Furthermore, a graph level user-specific attention algorithm is proposed to capture the user personalized preference between CKG and USG. Extensive experiment results demonstrate that normalized discounted cumulative gain (NDCG), precision, recall and hits ratio (HR) achieved with our proposed recommendation system are the best among those obtained with the state-of-the-art benchmark recommendation systems.},
  archive      = {J_TKDE},
  author       = {Chunyu Liu and Wei Wu and Siyu Wu and Lu Yuan and Rui Ding and Fuhui Zhou and Qihui Wu},
  doi          = {10.1109/TKDE.2023.3292504},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {840-853},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Social-enhanced explainable recommendation with knowledge graph},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised local community detection. <em>TKDE</em>,
<em>36</em>(2), 823–839. (<a
href="https://doi.org/10.1109/TKDE.2023.3290095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to the lack of a universal definition of communities, some semi-supervised community detection approaches learn the concept of community structures from known communities, and then dig out communities using learned concepts of communities. In some cases, users are only interested in the community containing a given node. However, communities detected by these semi-supervised approaches may not contain a given node. Besides, these methods traverse the entire network to detect many communities and cost more resources than a local algorithm. Therefore, it is necessary and meaningful to find the local community that contains a given node with prior information on the local network around the given node. We call this a Semi-supervised Local Community Detection (SLCD) problem. In this paper, prior information refers to certain known communities. To address the SLCD problem, we propose the Semi-supervised Local community detection with the Structural Similarity algorithm, called SLSS, which uses some known communities instead of all known communities. The idea of SLSS is to use the structural similarity between the known communities and the detected community, calculated by the graph kernel, to guide the expansion of the community. Experimental results show that SLSS outperforms other algorithms on six real-world datasets.},
  archive      = {J_TKDE},
  author       = {Li Ni and Junnan Ge and Yiwen Zhang and Wenjian Luo and Victor S. Sheng},
  doi          = {10.1109/TKDE.2023.3290095},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {823-839},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Semi-supervised local community detection},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reliability-driven local community search in dynamic
networks. <em>TKDE</em>, <em>36</em>(2), 809–822. (<a
href="https://doi.org/10.1109/TKDE.2023.3290295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community search over large dynamic graph has become an important research problem in modern complex networks, such as the online social network, collaboration network and biological networks. Network data in the time-varied environment has motivated several recent studies to identify the evolution of the communities. However, these studies mostly match communities of different snapshot or utilize the aggregation of the disjoint structural information and ignores the cohesion continuity. To fill this research gap, in this work, we propose a novel $(\theta,k)$ -core reliable community (CRC) and define the reliable community search problem which jointly considers member engagement, connection strength and cohesion continuity of the community in the dynamic network. We propose an online search algorithm based on eligible edge filtering and we further construct the Weighted Core Forest-Index (WCF-index) and develop efficient index-based querying algorithm with strong pruning properties. We also propose top- $l$ reliable community search problem that couples query based distance to reduce the free rider effect in local community search and support flexible multiple query vertices. Extensive experiments are conducted to show the efficiency and effectiveness of the proposed algorithms.},
  archive      = {J_TKDE},
  author       = {Yifu Tang and Jianxin Li and Nur Al Hasan Haldar and Ziyu Guan and Jiajie Xu and Chengfei Liu},
  doi          = {10.1109/TKDE.2023.3290295},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {809-822},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Reliability-driven local community search in dynamic networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probabilistic modeling of assimilate-contrast effects in
online rating systems. <em>TKDE</em>, <em>36</em>(2), 795–808. (<a
href="https://doi.org/10.1109/TKDE.2023.3292352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online rating system serves as an indispensable building block for many web applications. Previous studies showed that due to assimilate-contrast effects, historical ratings could significantly distort users’ ratings, leading to low accuracy of product quality estimation and recommendation. To understand assimilate-contrast effects, an “accurate” model is still missing as previous models do not capture important factors like rating recency, selection bias, etc. Furthermore, an analytical framework to characterize product estimation accuracy under assimilate-contrast effects is also missing. This paper aims to fill in this gap. We propose a probabilistic model to quantify the aforementioned important factors on assimilate-contrast effects. We apply stochastic approximation theory to show that when the rating bias satisfies mild contraction conditions, the aggregate rating converges under aggregate opinion heterogeneity. We also apply non-stationary Markov chain theory to show that when the strength of assimilate-contrast satisfies mild stable conditions, the aggregate rating converges under rating recency. We also derive an equation to characterize the converged aggregate ratings. These conditions reveal important insights on how the aforementioned factors influence the convergence and guide the online rating system operator to design appropriate rating aggregation rules and rating displaying strategies. We apply it to rating prediction tasks and product recommendation tasks. Experiment results on four public datasets show that our model can improve the rating prediction and recommendation accuracy over previous models significantly, under various metrics like RMSE, NDCG, etc. We also demonstrate the flexibility of our model by showing that it can be applied to enhance other rating behavior models.},
  archive      = {J_TKDE},
  author       = {Hong Xie and Mingze Zhong and Xiaoyu Shi and Xiaoying Zhang and Jiang Zhong and Mingsheng Shang},
  doi          = {10.1109/TKDE.2023.3292352},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {795-808},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Probabilistic modeling of assimilate-contrast effects in online rating systems},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). PEAK: Privacy-enhanced incentive mechanism for distributed
-anonymity in LBS. <em>TKDE</em>, <em>36</em>(2), 781–794. (<a
href="https://doi.org/10.1109/TKDE.2023.3295451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To motivate users’ assistance for protecting others’ location privacy by distributed K -anonymity in Location-Based Service (LBS), many incentive mechanisms have been proposed, where users obtain monetary compensation for their assistance. However, most existing distributed K -anonymity incentive mechanisms rely on trusted third parties and ignore users’ malicious strategies, which destroys LBS&#39;s distributed structure as well as leads to users’ privacy leakage and incentive ineffectiveness. To solve the above problems, we propose a P rivacy- E nhanced incentive mech A nism for distributed K -anonymity (PEAK). With determining the monetary transaction relationship and location transmission between users, PEAK enables the anonymous cloaking region construction without the trusted server. Meanwhile, PEAK devises role identification mechanism and accountability mechanism to restrain and punish malicious users, which protects users’ location privacy and implements effective motivation on users’ assistance. Theoretical analysis based on the game theory shows that PEAK constrains users’ malicious strategies while satisfying individual rationality, computational efficiency, and satisfaction ratio. Extensive experiments based on the real-world dataset demonstrate that PEAK improves security and feasibility, especially reaching the success rate of anonymous cloaking region construction to more than 90 $\%$ and decreasing the malicious users’ utilities significantly.},
  archive      = {J_TKDE},
  author       = {Man Zhang and Xinghua Li and Yinbin Miao and Bin Luo and Yanbing Ren and Siqi Ma},
  doi          = {10.1109/TKDE.2023.3295451},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {781-794},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {PEAK: Privacy-enhanced incentive mechanism for distributed -anonymity in LBS},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On atomicity and confidentiality across blockchains under
failures. <em>TKDE</em>, <em>36</em>(2), 766–780. (<a
href="https://doi.org/10.1109/TKDE.2023.3255842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed applications that utilize heterogeneous blockchain systems have the potential to be widely deployed. In such applications, users from different blockchains can transact with one another through cross-chain transactions . There are two essential features of particular relevance for those applications during cross-chain transactions: the atomicity in that either all or none of the blockchains involved confirm a cross-chain transaction, the confidentiality in that a blockchain involved in a cross-chain transaction is only accessible for designated users. Existing cross-chain proposals have largely relied on permissioned blockchains to ensure confidentiality. However, we found that failures could occur when reading or writing information during transaction confirmations across permissioned blockchains, namely read/write (r/w) failures, which can lead to the violation of atomicity. In this paper, we propose a novel mechanism, Unity , to ensure both atomicity and confidentiality of cross-chain transactions under r/w failures by leveraging permissioned blockchains. When failures occur in reading or writing data, Unity classifies the data into two categories based on its status - whether data is the latest version or not, and presents different solutions for atomicity. Specifically, when data is not the latest, we design a four-phase-commit protocol ( $\text{4pc}$ ), in which consensus on confirming or aborting a cross-chain transaction can be achieved. If data is the latest when r/w failures occur, we propose a smart contract based solution ( $\text{SSC}$ ). We examine the effectiveness of Unity theoretically and through experiments. With a failure probability of 0.7, Unity achieves 98% more atomic cross-chain transactions when compared with the state-of-the-art cross-chain platform, Hyperservice.},
  archive      = {J_TKDE},
  author       = {Yuechen Tao and Bo Li and Baochun Li},
  doi          = {10.1109/TKDE.2023.3255842},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {766-780},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {On atomicity and confidentiality across blockchains under failures},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NEAR: Non-supervised explainability architecture for
accurate review-based collaborative filtering. <em>TKDE</em>,
<em>36</em>(2), 750–765. (<a
href="https://doi.org/10.1109/TKDE.2022.3226189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a critical issue in explainable recommender systems that compounds the challenges of explainability yet is rarely tackled: the lack of ground-truth explanation texts for training. It is unrealistic to expect every user-item pair in a dataset to have a corresponding target explanation. Hence, we pioneer the first non-supervised explainability architecture for review-based collaborative filtering (called NEAR) as our novel contribution to the theory of explanation construction in recommender systems. While maintaining excellent recommendation performance, our approach reformulates explainability as a non-supervised (i.e., unsupervised and self-supervised) explanation generation task. We formally define two explanation types, both of which NEAR can produce. An invariant explanation, fixed for all users, is based on the unsupervised extractive summary of an item&#39;s reviews via embedding clustering. Meanwhile, a variant explanation, personalized for a specific user, is a sentence-level text generated by our customized Transformer conditioned on every user-item-rating tuple and artificial ground-truth (self-supervised label) from one of the invariant explanation&#39;s sentences. Our empirical evaluation illustrates that NEAR&#39;s rating prediction accuracy is better than the other state-of-the-art baselines. Moreover, experiments and assessments show that NEAR-generated variant explanations are more personalized and distinct than those from other Transformer-based models, and our invariant explanations are preferred over those from other contemporary models in real life.},
  archive      = {J_TKDE},
  author       = {Reinald Adrian Pugoy and Hung-Yu Kao},
  doi          = {10.1109/TKDE.2022.3226189},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {750-765},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {NEAR: Non-supervised explainability architecture for accurate review-based collaborative filtering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-view fuzzy representation learning with rules based
model. <em>TKDE</em>, <em>36</em>(2), 736–749. (<a
href="https://doi.org/10.1109/TKDE.2023.3295874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised multi-view representation learning has been studied extensively for mining multi-view data. However, some critical challenges remain. On the one hand, the existing methods cannot explore multi-view data comprehensively since they usually learn a common representation between views and ignore the specific information within each view. On the other hand, to mine the nonlinear relationship between the data, kernel or neural network methods are commonly used for multi-view representation learning but they lack interpretability. To this end, this paper proposes a new multi-view fuzzy representation learning method based on the interpretable Takagi-Sugeno-Kang (TSK) fuzzy system (MVRL_FS). The method realizes multi-view representation learning from two aspects. First, multi-view data are transformed into a high-dimensional fuzzy feature space, while the common information between views and specific information of each view are explored simultaneously. Second, a new regularization method based on ${L}_{2,1}$ -norm regression is proposed to mine the consistency information between views, while the geometric structure of the data is preserved through the Laplacian graph. Extensive experiments on many benchmark multi-view datasets are conducted to validate the superiority of the proposed method.},
  archive      = {J_TKDE},
  author       = {Wei Zhang and Zhaohong Deng and Te Zhang and Kup-Sze Choi and Shitong Wang},
  doi          = {10.1109/TKDE.2023.3295874},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {736-749},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-view fuzzy representation learning with rules based model},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-modal knowledge graph construction and application: A
survey. <em>TKDE</em>, <em>36</em>(2), 715–735. (<a
href="https://doi.org/10.1109/TKDE.2022.3224228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the resurgence of knowledge engineering which is featured by the fast growth of knowledge graphs. However, most of existing knowledge graphs are represented with pure symbols, which hurts the machine&#39;s capability to understand the real world. The multi-modalization of knowledge graphs is an inevitable key step towards the realization of human-level machine intelligence. The results of this endeavor are Multi-modal Knowledge Graphs (MMKGs). In this survey on MMKGs constructed by texts and images, we first give definitions of MMKGs, followed with the preliminaries on multi-modal tasks and techniques. We then systematically review the challenges, progresses and opportunities on the construction and application of MMKGs respectively, with detailed analyses of the strengths and weaknesses of different solutions. We finalize this survey with open research problems relevant to MMKGs.},
  archive      = {J_TKDE},
  author       = {Xiangru Zhu and Zhixu Li and Xiaodan Wang and Xueyao Jiang and Penglei Sun and Xuwu Wang and Yanghua Xiao and Nicholas Jing Yuan},
  doi          = {10.1109/TKDE.2022.3224228},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {715-735},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-modal knowledge graph construction and application: A survey},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interpretable stochastic block influence model: Measuring
social influence among homophilous communities. <em>TKDE</em>,
<em>36</em>(2), 708–714. (<a
href="https://doi.org/10.1109/TKDE.2023.3289848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision-making on networks can be explained by both homophily and social influences. While homophily drives the formation of communities with similar characteristics, social influences occur both within and between communities. Social influences can be reasoned through role theory, which indicates that the influences among individuals depending on their roles and the behavior of interest. To operationalize these social science theories, we empirically identify the homophilous communities and use the community structures to capture such “roles”, affecting particular decision-making processes. We propose a generative model named the Stochastic Block Influence Model and jointly analyze both network formation and behavioral influences within and between different empirically-identified communities. To evaluate the performance and demonstrate the interpretability of our method, we study the adoption decisions for a microfinance product in Indian villages. We show that although individuals tend to form links within communities, there are strongly positive and negative social influences between communities, supporting the weak ties theory. Moreover, communities with shared characteristics are associated with positive influences. In contrast, communities that do not overlap are associated with negative influences. Our framework facilitates the quantification of the influences underlying decision communities and is thus a helpful tool for driving information diffusion, viral marketing, and technology adoption.},
  archive      = {J_TKDE},
  author       = {Yan Leng and Tara Sowrirajan and Yujia Zhai and Alex Pentland},
  doi          = {10.1109/TKDE.2023.3289848},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {708-714},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Interpretable stochastic block influence model: Measuring social influence among homophilous communities},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrated personalized and diversified search based on
search logs. <em>TKDE</em>, <em>36</em>(2), 694–707. (<a
href="https://doi.org/10.1109/TKDE.2023.3291006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized search and search result diversification are two possible solutions to cope with the query ambiguity problem in search engines. In most existing studies, they have been investigated separately, but intuitively, they address the problem from two complementary perspectives and should be combined. Some recent work tried to combine them by restricting result diversification to the subtopics corresponding to the user’s personal profile. However, diversification can be required even when the subtopics are outside the user’s profile. In this paper, we propose a more general approach to integrate them based on users’ implicit feedback in query logs. The proposed approach PER+DIV aggregates a document’s novelty score and personal relevance score dynamically according to how much the query falls into the user’s interests. To train the model based on user clicks in the logs, we consider user click as a result of both personal relevance and result diversity and a new method is proposed to isolate and model these two factors. To evaluate the model, we design several diversified and personalized metrics in addition to the traditional click-based metrics. Experimental results on a large-scale query log dataset show that the proposed integrated method significantly outperforms the existing personalization and diversification approaches.},
  archive      = {J_TKDE},
  author       = {Jiongnan Liu and Zhicheng Dou and Jian-Yun Nie and Ji-Rong Wen},
  doi          = {10.1109/TKDE.2023.3291006},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {694-707},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Integrated personalized and diversified search based on search logs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Individual and structural graph information bottlenecks for
out-of-distribution generalization. <em>TKDE</em>, <em>36</em>(2),
682–693. (<a href="https://doi.org/10.1109/TKDE.2023.3290792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Out-of-distribution (OOD) graph generalization are critical for many real-world applications. Existing methods neglect to discard spurious or noisy features of inputs, which are irrelevant to the label. Besides, they mainly conduct instance-level class-invariant graph learning and fail to utilize the structural class relationships between graph instances. In this work, we endeavor to address these issues in a unified framework, dubbed I ndividual and S tructural G raph I nformation B ottlenecks ( IS-GIB ). To remove class spurious feature caused by distribution shifts, we propose Individual Graph Information Bottleneck (I-GIB) which discards irrelevant information by minimizing the mutual information between the input graph and its embeddings. To leverage the structural intra- and inter-domain correlations, we propose Structural Graph Information Bottleneck (S-GIB). Specifically for a batch of graphs with multiple domains, S-GIB first computes the pair-wise input-input, embedding-embedding, and label-label correlations. Then it minimizes the mutual information between input graph and embedding pairs while maximizing the mutual information between embedding and label pairs. The critical insight of S-GIB is to simultaneously discard spurious features and learn invariant features from a high-order perspective by maintaining class relationships under multiple distributional shifts. Notably, we unify the proposed I-GIB and S-GIB to form our complementary framework IS-GIB. Extensive experiments conducted on both node- and graph-level tasks consistently demonstrate the superior generalization ability of IS-GIB.},
  archive      = {J_TKDE},
  author       = {Ling Yang and Jiayi Zheng and Heyuan Wang and Zhongyi Liu and Zhilin Huang and Shenda Hong and Wentao Zhang and Bin Cui},
  doi          = {10.1109/TKDE.2023.3290792},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {682-693},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Individual and structural graph information bottlenecks for out-of-distribution generalization},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GS-RS: A generative approach for alleviating cold start and
filter bubbles in recommender systems. <em>TKDE</em>, <em>36</em>(2),
668–681. (<a href="https://doi.org/10.1109/TKDE.2023.3290140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender Systems (RSs) typically face the cold-start problem and the filter-bubble problem when users suffer the familiar, repeated, and even predictable recommendations, making them bored and unsatisfied. The key to solving these issues is learning users’ fine-grained preferences and recommending appealing and unexplored items deviating from users’ historical items. However, existing models consider cold-start or filter bubble problems separately and ignore that they can reinforce mutually and damage the models’ performance accuracy. To this end, we devise a novel serendipity-oriented recommender system ( G enerative S elf-constrained S erendipitous R ecommender S ystem, GS$^{2}$2-RS ) that generates users’ fine-grained preferences to enhance the recommendation performance. Specifically, GS $^{2}$ -RS extracts users’ interest and satisfaction preferences and generates virtual but convincible neighbors’ preferences from themselves with a twin Conditional Generative Adversarial Nets (not from real neighbors). Then we introduce the serendipity item, which is low-interest but high-satisfaction among candidate items. We use the serendipity item to improve the diversity of recommended items, which relieves the filter-bubble problem. Along with this line, a gated mechanism is applied to their fine-grained preferences (interests, satisfactions) to obtain their serendipity items. Finally, these serendipity items are inversely injected into the original user-item rating matrix and build a relatively dense matrix as the input for backbone RS models. Note that GS $^{2}$ -RS tackles cold-start and filter-bubble problems in a unified framework without any additional side information and enriches the interpretability of recommendation models. We comprehensively validate GS $^{2}$ -RS for solving cold-start and filter bubble problems on four real-world benchmark datasets. Extensive experiments illustrate GS $^{2}$ -RS&#39;s superiority in accuracy, serendipity, and interpretability over state-of-the-art models. Also, we can plug our model into existing recommender systems as a preprocessing procedure to enhance their performance.},
  archive      = {J_TKDE},
  author       = {Yuanbo Xu and En Wang and Yongjian Yang and Hui Xiong},
  doi          = {10.1109/TKDE.2023.3290140},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {668-681},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GS-RS: A generative approach for alleviating cold start and filter bubbles in recommender systems},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GPU-based efficient parallel heuristic algorithm for
high-utility itemset mining in large transaction datasets.
<em>TKDE</em>, <em>36</em>(2), 652–667. (<a
href="https://doi.org/10.1109/TKDE.2023.3290371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heuristic algorithms have been developed to find approximate solutions for high-utility itemset mining (HUIM) problems that compensate for the performance bottlenecks of exact algorithms. However, heuristic algorithms still face the problem of long runtime and insufficient mining quality, especially for large transaction datasets with thousands to tens of thousands of items and up to millions of transactions. To solve these problems, a novel GPU-based efficient parallel heuristic algorithm for HUIM (PHA-HUIM) is proposed in this paper. The iterative process of PHA-HUIM consists of three main steps: the search strategy, fitness evaluation, and ring topology communication. The search strategy and ring topology communication are designed to run in constant time on GPU. The parallelism of fitness evolution helps to substantially accelerate the algorithm. A new data structure with a sort-mapping strategy is proposed to enhance the search ability and reduce memory usage. To improve the mining quality, a multi-start strategy with an unbalanced allocation strategy is employed in the search process. Ring topology communication is adopted to maintain population diversity. A load balancing strategy is introduced to reduce the thread divergence to improve the parallel efficiency. The experimental results on nine large datasets show that PHA-HUIM outperforms state-of-the-art HUIM algorithms in terms of speedup performance, runtime, and mining quality.},
  archive      = {J_TKDE},
  author       = {Wei Fang and Haipeng Jiang and Hengyang Lu and Jun Sun and Xiaojun Wu and Jerry Chun-Wei Lin},
  doi          = {10.1109/TKDE.2023.3290371},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {652-667},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GPU-based efficient parallel heuristic algorithm for high-utility itemset mining in large transaction datasets},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GNN cleaner: Label cleaner for graph structured data.
<em>TKDE</em>, <em>36</em>(2), 640–651. (<a
href="https://doi.org/10.1109/TKDE.2023.3288002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Network (GNN) has emerged as a predominant tool for graph data analysis. Despite their proliferation, the low-quality labels of many real-world graphs will undermine their performance dramatically. Existing studies on learning neural networks with noisy labels mainly focus on independent data and thus cannot fully exploit the structural information of graph data. Currently, there are few studies of robustness to noisy labels for graph-structured data even if this problem is commonly seen in real-world settings. To remedy this deficiency, we propose GNN Cleaner which utilizes structural information of graph data to combat noisy labels. More specifically, a pseudo label is computed from the neighboring labels for each node in the training set via a modified version of label propagation. Additionally, a novel method is developed to learn to correct the labels adaptively and dynamically. Extensive experiments show that GNN Cleaner can train GNNs robustly and correct both the synthetic and real-world noisy labels even if the noise is severe. Moreover, GNN Cleaner is model-agnostic and can be combined with various GNNs to improve their robustness against label noise.},
  archive      = {J_TKDE},
  author       = {Jun Xia and Haitao Lin and Yongjie Xu and Cheng Tan and Lirong Wu and Siyuan Li and Stan Z. Li},
  doi          = {10.1109/TKDE.2023.3288002},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {640-651},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GNN cleaner: Label cleaner for graph structured data},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From minimum change to maximum density: On determining
near-optimal s-repair. <em>TKDE</em>, <em>36</em>(2), 627–639. (<a
href="https://doi.org/10.1109/TKDE.2023.3294401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dirty data are commonly observed in real applications, making cleaning them a key step in data preparation. The widely adopted idea of cleaning dirty data is based on detecting conflicts w.r.t. integrity constraints. Typical S-repair methods remove a minimal set of tuples (to avoid excessive removal and information loss) such that integrity constraints are no longer violated in remaining tuples. Unfortunately, multiple candidates of minimal removal sets may exist and are difficult to determine which one is indeed proper. We intuitively notice that a clean tuple often has more close neighbors (i.e., higher density) than dirty tuples. Hence, in this paper, we study the problem of finding the optimal S-repair under integrity constraints with the highest density, among various minimal removal sets. Our major contributions include (1) the np -hardness analysis on solving the problem, (2) a heuristic algorithm for efficiently tackling the problem and returning the optimal solution in certain cases, (3) an approximation performance bounded method with the same optimal solution guarantee. Experiments on real datasets collected from industry with real-world errors demonstrate the superiority of our work in cleaning dirty tuples.},
  archive      = {J_TKDE},
  author       = {Yu Sun and Shaoxu Song and Xiaojie Yuan},
  doi          = {10.1109/TKDE.2023.3294401},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {627-639},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {From minimum change to maximum density: On determining near-optimal S-repair},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extracting top- frequent and diversified patterns in
knowledge graphs. <em>TKDE</em>, <em>36</em>(2), 608–626. (<a
href="https://doi.org/10.1109/TKDE.2022.3233594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A knowledge graph contains many real-world facts that can be used to support various analytical tasks, e.g., exceptional fact discovery and the check of claims. In this work, we attempt to extract top- $k$ frequent and diversified patterns from knowledge graph by well capturing user interest. Specifically, we first formalize the core-based top- $k$ frequent pattern discovery problem, which finds the top- $k$ frequent patterns that are extended from a core pattern specified by user query and have the highest frequency. In addition, to diversify the top- $k$ frequent patterns, we define a distance function to measure the dissimilarity between two patterns, and return top- $k$ patterns in which the pairwise diversity of any two resultant patterns exceeds a given threshold. As the search space of candidate patterns is exponential w.r.t. the number of nodes and edges in the knowledge graph, discovering frequent and diversified patterns is computationally challenging. To achieve high efficiency, we propose a suite of techniques, including (1) We devise a meta-index to avoid generating invalid candidate patterns; (2) We propose an upper bound of the frequency score (i.e., $\mathsf {MNI}$ ) of the candidate pattern, which is used to prune unqualified candidates earlier and prioritize the enumeration order of patterns; (3) We design an advanced join-based approach to compute the $\mathsf {MNI}$ of candidate patterns efficiently; and (4) We develop a lower bound for distance function and incrementally compute the pairwise diversity among the patterns. Using real-world knowledge graphs, we experimentally verify the efficiency and effectiveness of our proposed techniques. We also demonstrate the utility of the extracted patterns by case studies.},
  archive      = {J_TKDE},
  author       = {Jian Zeng and Leong Hou U and Xiao Yan and Yan Li and Mingji Han and Bo Tang},
  doi          = {10.1109/TKDE.2022.3233594},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {608-626},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Extracting top- frequent and diversified patterns in knowledge graphs},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evidence reasoning and curriculum learning for
document-level relation extraction. <em>TKDE</em>, <em>36</em>(2),
594–607. (<a href="https://doi.org/10.1109/TKDE.2023.3292974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document-level Relation Extraction (RE) is a promising task aiming at identifying relations of multiple entity pairs in a document. Compared with the sentence-level counterpart, it has raised two significant challenges: a) In most cases, a relational fact can be adequately expressed via a small subset of sentences from the document, namely evidence. But the traditional method cannot model such strong semantic correlations between evidence sentences that collaborate to describe a specific relation; b) The data of this task is extremely long-tail in terms of too many NA instances and imbalanced relational types. Such data can mislead the tail prediction bias to the head categories in the RE model. In this paper, we present a novel E vidence reasoning and C urriculum learning method for D oc RE (DRE-EC) to address these challenges. Particularly, we first formulate evidence extraction as a sequential decision problem through a crafted reinforcement learning mechanism with an efficient path searching strategy to reduce the action space. Providing the evidence for each entity pair as a customized-filtered document in advance helps infer the relations better. To address the long-tail issue, we further develop a hybrid curriculum learning method at the NA-level (NC) and relation-level (RC) with our customized difficulty measure score. In NC, the NA samples are scheduled in an easy-to-hard scheme and gradually added, resulting in the data distribution from ideal and balanced to real and unbalanced. In RC, the scheme is switched into hard-to-easy to enhance the hard and tail samples. In addition, we propose a new Equalization adaptive Focal Loss(EFLoss) that can adjust to the changing data distribution and focus more on the tail categories. We conduct various experiments on two document-level RE benchmarks and achieve a remarkable improvement over previous competitive baselines. Furthermore, we provide detailed analyses of the advantages and effectiveness of our method.},
  archive      = {J_TKDE},
  author       = {Tianyu Xu and Jianfeng Qu and Wen Hua and Zhixu Li and Jiajie Xu and An Liu and Lei Zhao and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2023.3292974},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {594-607},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Evidence reasoning and curriculum learning for document-level relation extraction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ensemble clustering with attentional representation.
<em>TKDE</em>, <em>36</em>(2), 581–593. (<a
href="https://doi.org/10.1109/TKDE.2023.3292573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble clustering has emerged as a powerful framework for analyzing heterogeneous and complex data. Despite the abundance of existing schemes, co-association matrix-based methods remain the mainstream approach. However, focusing solely on pairwise correlations falls short of fully capturing the intricate cluster relationships. Moreover, despite its potential, ensemble clustering has yet to effectively leverage the powerful representation capabilities of neural networks. To address these limitations, we propose a deep ensemble clustering method called Ensemble Clustering with Attentional Representation (ECAR). Our method considers the results of base partitions as groups with related information to explore higher-order fusion information. ECAR captures the importance of each sample’s association with its related group by employing an attentional network, and encodes this information into a low-dimensional representation. The attentional network is trained by jointly optimizing the clustering loss from soft assignments learned from the embeddings and the reconstruction loss from the weighted graph generated from ensemble clustering. During training, the weights of base partitions are adaptively refined to promote diversity and consistency while reducing the impact of low-quality and redundant base partitions. Extensive experimental results on real-world datasets demonstrate the substantial improvement of our method over existing baseline ensemble clustering methods and deep clustering methods.},
  archive      = {J_TKDE},
  author       = {Zhezheng Hao and Zhoumin Lu and Guoxu Li and Feiping Nie and Rong Wang and Xuelong Li},
  doi          = {10.1109/TKDE.2023.3292573},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {581-593},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Ensemble clustering with attentional representation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhance knowledge graph embedding by mixup. <em>TKDE</em>,
<em>36</em>(2), 569–580. (<a
href="https://doi.org/10.1109/TKDE.2023.3292379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs have important applications for many computational tasks, such as personalized recommendations, information search, and natural language processing. Knowledge graph embedding, which is to learn representations of nodes and relations, is very critical to facilitate these applications and thus has been studied extensively in the literature. Most existing knowledge graphs (e.g., Freebase) have a data scarcity issue, i.e., the number of observed triplets is much less than that of all possible pairs of nodes. While the data augmentation technique has been widely applied to addressing data scarcity in other domains (e.g., image data), there are few prior studies exploring it for knowledge graph embedding probably because the discrete data structure of knowledge graphs prohibits the employment of most data augmentation methods. To fill this research gap, this paper introduces a novel data augmentation framework, namely knowledge graph mixup (KG Mixup), to enhance knowledge graph embedding. Based on the proposed framework, we develop two specific methods: vanilla mixup and influence mixup. Both approaches generate virtual mixup triplets and incorporate them into the learning process through a new mixup loss function. While vanilla mixup generates virtual triplets based on a uniform distribution, the influence mixup approach employs the influence function to guide the generation of mixup samples. Experiments with multiple datasets have shown that both approaches significantly outperform knowledge graph embedding models trained by the ordinary training framework.},
  archive      = {J_TKDE},
  author       = {Tianyang Xie and Yong Ge},
  doi          = {10.1109/TKDE.2023.3292379},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {569-580},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Enhance knowledge graph embedding by mixup},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Elastic multi-view subspace clustering with pairwise and
high-order correlations. <em>TKDE</em>, <em>36</em>(2), 556–568. (<a
href="https://doi.org/10.1109/TKDE.2023.3293498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering has become an important research topic in machine learning and computer vision communities, which aims at achieving a consensus partition of data points across different views. However, the existing multi-view clustering methods fail to simultaneously consider the pairwise and high-order correlations among different views in the process of obtaining the final results. In this article, we propose the Elastic multi-view Subspace Clustering with pairwise and high-order Correlations (ESCC) to solve this problem. ESCC simultaneously explores the pairwise and high-order correlations among different views, resulting in a more comprehensive shared representation. ESCC formulates these two kinds of correlations into a unified objective framework, which are able to be jointly optimized to refine each other. As an instantiation, we construct an example of ESCC (e-ESCC) in this work. To be specific, e-ESCC uses the multi-layer neural networks to study the pairwise correlation from multiple views with the guidance of the latent representation. It is also able to help obtain the nonlinear subspaces of the multi-view data. e-ESCC collects multi-view similarity matrices into a tensor and utilizes the low-rank tensor norm to exploit the high-order correlation among different views. The augmented Lagrangian multiplier is adopted to solve the formulated problem of e-ESCC. Experiments on eight data sets validate the superiority of our method over 15 state-of-the-art multi-view clustering methods under six metrics.},
  archive      = {J_TKDE},
  author       = {Yalan Qin and Nan Pu and Hanzhou Wu},
  doi          = {10.1109/TKDE.2023.3293498},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {556-568},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Elastic multi-view subspace clustering with pairwise and high-order correlations},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual contrastive learning for efficient static feature
representation in sequential recommendations. <em>TKDE</em>,
<em>36</em>(2), 544–555. (<a
href="https://doi.org/10.1109/TKDE.2023.3289469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Static user and item features constitute important information to be taken into account in the recommendation process. However, as these features are usually sparse and of large-vocabulary, existing deep learning-based methods typically construct large tables of high-dimensional feature embeddings, which is inefficient in terms of memory storage and is computationally problematic. On the other hand, while product quantization-based methods have been proposed to compress latent embeddings, they usually come at the cost of compromising recommendation performance due to the restrictive expressive power, as feature correlations and user-item interactions are not properly captured in the compression process. To address these issues, we propose a novel Dual Contrastive Learning method to generate low-dimensional discrete static feature representations that significantly reduce memory storage and computational complexity, while simultaneously producing superior recommendation performance. Extensive offline experiments on three large-scale industrial datasets demonstrate that our proposed model significantly outperforms the selected baselines. In addition, we conducted an online A/B test at Alibaba and show that the proposed model significantly improves the average video streaming time, while reducing the size of the feature embedding table by 90% over the currently deployed system.},
  archive      = {J_TKDE},
  author       = {Pan Li and Maofei Que and Alexander Tuzhilin},
  doi          = {10.1109/TKDE.2023.3289469},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {544-555},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dual contrastive learning for efficient static feature representation in sequential recommendations},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Dual attention graph convolutional network for relation
extraction. <em>TKDE</em>, <em>36</em>(2), 530–543. (<a
href="https://doi.org/10.1109/TKDE.2023.3289879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dependency-based models are widely used to extract semantic relations in text. Most existing dependency-based models establish stacked structures to merge contextual and dependency information, which encode the contextual information first and then encode the dependency information. However, this unidirectional information flow weakens the representation of words in the sentence, which further restricts the performance of existing models. To establish bidirectional information flow, a dual attention graph convolutional network (DAGCN) with a parallel structure is proposed. Most importantly, DAGCN can build multi-turn interactions between contextual and dependency information to imitate the multi-turn looking-back actions of human beings. In addition, multi-layer adjacency matrix-aware multi-head attention (AMAtt), including context-to-dependency attention and dependency-to-context attention, is carefully designed as a merge mechanism in the parallel structure to preserve the structural information of sentences and dependency trees during interactions. Furthermore, DAGCN is evaluated on the popular PubMed dataset, TACRED dataset and SemEval 2010 Task 8 dataset to demonstrate its validity. Experimental results show that our model outperforms the existing dependency-based models.},
  archive      = {J_TKDE},
  author       = {Donghao Zhang and Zhenyu Liu and Weiqiang Jia and Fei Wu and Hui Liu and Jianrong Tan},
  doi          = {10.1109/TKDE.2023.3289879},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {530-543},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dual attention graph convolutional network for relation extraction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DropConn: Dropout connection based random GNNs for molecular
property prediction. <em>TKDE</em>, <em>36</em>(2), 518–529. (<a
href="https://doi.org/10.1109/TKDE.2023.3290032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, molecular data mining has attracted a lot of attention owing to its great application potential in material and drug discovery. However, this mining task faces a challenge posed by the scarcity of labeled molecular graphs. To overcome this challenge, we introduce a novel data augmentation and a semi-supervised confidence-aware consistency regularization training framework for molecular property prediction. The core of our framework is a data augmentation strategy on molecular graphs, named DropConn (Dropout Connection). DropConn generates pseudo molecular graphs by softening the hard connections of chemical bonds (as edges), where the soft weights are calculated from edge features so that the adaptive interactions between different atoms can be incorporated. Besides, to enhance the model&#39;s generalization ability, a consistency regularization training strategy is proposed to take full advantage of massive unlabeled data. Furthermore, DropConn can serve as a plugin that can be seamlessly added to many existing models. Extensive experiments under both non-pre-training setting and fine-tuning setting demonstrate that DropConn can obtain superior performance (up to 8.22%) over state-of-the-art methods on molecular property prediction tasks.},
  archive      = {J_TKDE},
  author       = {Dan Zhang and Wenzheng Feng and Yuandong Wang and Zhongang Qi and Ying Shan and Jie Tang},
  doi          = {10.1109/TKDE.2023.3290032},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {518-529},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DropConn: Dropout connection based random GNNs for molecular property prediction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discovering dynamic patterns from spatiotemporal data with
time-varying low-rank autoregression. <em>TKDE</em>, <em>36</em>(2),
504–517. (<a href="https://doi.org/10.1109/TKDE.2023.3294440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of discovering interpretable dynamic patterns from spatiotemporal data is studied in this paper. For that purpose, we develop a time-varying reduced-rank vector autoregression (VAR) model whose coefficient matrices are parameterized by low-rank tensor factorization. Benefiting from the tensor factorization structure, the proposed model can simultaneously achieve model compression and pattern discovery. In particular, the proposed model allows one to characterize nonstationarity and time-varying system behaviors underlying spatiotemporal data. To evaluate the proposed model, extensive experiments are conducted on various spatiotemporal datasets representing different nonlinear dynamical systems, including fluid dynamics, sea surface temperature, USA surface temperature, and NYC taxi trips. Experimental results demonstrate the effectiveness of the proposed model for analyzing spatiotemporal data and characterizing spatial/temporal patterns. In the spatial context, the spatial patterns can be automatically extracted and intuitively characterized by the spatial modes. In the temporal context, the complex time-varying system behaviors can be revealed by the temporal modes in the proposed model. Thus, our model lays an insightful foundation for understanding complex spatiotemporal data in real-world dynamical systems.},
  archive      = {J_TKDE},
  author       = {Xinyu Chen and Chengyuan Zhang and Xiaoxu Chen and Nicolas Saunier and Lijun Sun},
  doi          = {10.1109/TKDE.2023.3294440},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {504-517},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Discovering dynamic patterns from spatiotemporal data with time-varying low-rank autoregression},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differentiable and scalable generative adversarial models
for data imputation. <em>TKDE</em>, <em>36</em>(2), 490–503. (<a
href="https://doi.org/10.1109/TKDE.2023.3293129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data imputation has been extensively explored to solve the missing data problem. The dramatically increasing volume of incomplete data makes the imputation models computationally infeasible in many real-life applications. In this paper, we propose an effective scalable imputation system named ${\sf SCIS}$ to significantly speed up the training of the differentiable generative adversarial imputation models under accuracy-guarantees for large-scale incomplete data. ${\sf SCIS}$ consists of two modules, differentiable imputation modeling (DIM) and sample size estimation (SSE). DIM leverages a new masking Sinkhorn divergence function to make an arbitrary generative adversarial imputation model differentiable, while for such a differentiable imputation model, SSE can estimate an appropriate sample size to ensure the user-specified imputation accuracy of the final model. Moreover, ${\sf SCIS}$ can also accelerate the autoencoder based imputation models. Extensive experiments upon several real-life large-scale datasets demonstrate that, our proposed system can accelerate the generative adversarial model training by 6.23x. Using around 1.27% samples, ${\sf SCIS}$ yields competitive accuracy with the state-of-the-art imputation methods in much shorter computation time.},
  archive      = {J_TKDE},
  author       = {Yangyang Wu and Jun Wang and Xiaoye Miao and Wenjia Wang and Jianwei Yin},
  doi          = {10.1109/TKDE.2023.3293129},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {490-503},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Differentiable and scalable generative adversarial models for data imputation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collaborative knowledge graph fusion by exploiting the open
corpus. <em>TKDE</em>, <em>36</em>(2), 475–489. (<a
href="https://doi.org/10.1109/TKDE.2023.3289949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To ease the process of building Knowledge Graphs (KGs) from scratch, a cost-effective method is required to enrich a KG using the triples extracted from a corpus. However, it is challenging to enrich a KG with newly extracted triples since they contain noisy information. This paper proposes to refine a KG by leveraging information extracted from a corpus. In particular, we first formulate the task of building KGs as two coupled sub-tasks, namely join event extraction and knowledge graph fusion. We then propose a collaborative knowledge graph fusion framework, which is composed of an explorer and a supervisor, to allow the involved two sub-tasks to mutually assist each other in an alternative manner. More concretely, an explorer extracts triples from a corpus supervised by both the ground-truth annotation and the KG provided by the supervisor. Furthermore, a supervisor then evaluates the extracted triples and enriches the KG with those that are highly ranked. To implement this evaluation, we further propose a translated relation alignment scoring mechanism to align and translate the extracted triples to the KG. Experimental results verify that this collaboration can improve both the performance of our sub-tasks, and contribute to high-quality enriched knowledge graphs.},
  archive      = {J_TKDE},
  author       = {Yue Wang and Yao Wan and Lu Bai and Lixin Cui and Zhuo Xu and Ming Li and Philip S. Yu and Edwin R. Hancock},
  doi          = {10.1109/TKDE.2023.3289949},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {475-489},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Collaborative knowledge graph fusion by exploiting the open corpus},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Causal distillation for alleviating performance
heterogeneity in recommender systems. <em>TKDE</em>, <em>36</em>(2),
459–474. (<a href="https://doi.org/10.1109/TKDE.2023.3290545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation performance usually exhibits a long-tail distribution over users — a small portion of head users enjoy much more accurate recommendation services than the others. We reveal two sources of this performance heterogeneity problem: the uneven distribution of historical interactions (a natural source); and the biased training of recommender models (a model source). As addressing this problem cannot sacrifice the overall performance, a wise choice is to eliminate the model bias while maintaining the natural heterogeneity. The key to debiased training lies in eliminating the effect of confounders that influence both the user&#39;s historical behaviors and the next behavior. The emerging causal recommendation methods achieve this by modeling the causal effect between user behaviors, however potentially neglect unobserved confounders (e.g., friend suggestions) that are hard to measure in practice. To address unobserved confounders, we resort to the front-door adjustment (FDA) in causal theory and propose a causal multi-teacher distillation framework (CausalD). FDA requires proper mediators in order to estimate the causal effects of historical behaviors on the next behavior. To achieve this, we equip CausalD with multiple heterogeneous recommendation models to model the mediator distribution. Then, the causal effect estimated by FDA is the expectation of recommendation prediction over the mediator distribution and the prior distribution of historical behaviors, which is technically achieved by multi-teacher ensemble. To pursue efficient inference, CausalD further distills multiple teachers into one student model to directly infer the causal effect for making recommendations. We instantiate CausalD on two representative models, DeepFM and DIN, and conduct extensive experiments on three real-world datasets, which validate the superiority of CausalD over state-of-the-art methods. Through in-depth analysis, we find that CausalD largely improves the performance of tail users, reduces the performance heterogeneity, and enhances the overall performance.},
  archive      = {J_TKDE},
  author       = {Shengyu Zhang and Ziqi Jiang and Jiangchao Yao and Fuli Feng and Kun Kuang and Zhou Zhao and Shuo Li and Hongxia Yang and Tat-seng Chua and Fei Wu},
  doi          = {10.1109/TKDE.2023.3290545},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  number       = {2},
  pages        = {459-474},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Causal distillation for alleviating performance heterogeneity in recommender systems},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). WL-align: Weisfeiler-lehman relabeling for aligning users
across networks via regularized representation learning. <em>TKDE</em>,
<em>36</em>(1), 445–458. (<a
href="https://doi.org/10.1109/TKDE.2023.3277843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aligning users across networks using graph representation learning has been found effective where the alignment is accomplished in a low-dimensional embedding space. Yet, highly precise alignment remains challenging, especially for nodes with long-range connectivity to labeled anchors. To alleviate this limitation, we propose WL-Align which employs a regularized representation learning framework to learn distinctive node representations. It extends the Weisfeiler-Lehman Isormorphism Test and learns the alignment in alternating phases of “across-network Weisfeiler-Lehman relabeling” and “proximity-preserving representation learning”. The across-network Weisfeiler-Lehman relabeling is achieved through iterating the anchor-based label propagation and a similarity-based hashing to exploit the known anchors’ connectivity to different nodes in an efficient and robust manner. The representation learning module preserves the second-order proximity within individual networks and is regularized by the across-network Weisfeiler-Lehman hash labels. Extensive experiments on real-world and synthetic datasets have demonstrated that our proposed WL-Align outperforms the state-of-the-art methods, achieving significant performance improvements in the “exact matching” scenario.},
  archive      = {J_TKDE},
  author       = {Li Liu and Penggang Chen and Xin Li and William K. Cheung and Youmin Zhang and Qun Liu and Guoyin Wang},
  doi          = {10.1109/TKDE.2023.3277843},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {445-458},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {WL-align: Weisfeiler-lehman relabeling for aligning users across networks via regularized representation learning},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-stage asymmetric similarity preserving hashing for
cross-modal retrieval. <em>TKDE</em>, <em>36</em>(1), 429–444. (<a
href="https://doi.org/10.1109/TKDE.2023.3283984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing-based techniques present appealing solutions for cross-modal retrieval due to its low storage requirements and excellent query efficiency. The majority of cross-modal hashing methods typically adopt equal-length encoding scheme to represent multimodal data and achieve cross-modal similarity search. However, such scheme can be regarded as a relatively strict limitation, because it sacrifices the flexible representation of multimodal data in reality and cannot always guarantee the optimal retrieval performance. To address the challenge, this paper focuses on encoding heterogeneous data with varying hash lengths. To achieve this purpose, we propose a flexible cross-modal hashing approach, named Two-stage Asymmetric Similarity Preserving Hashing, TASPH for short, which can be applied to both unequal-length and equal-length retrieval scenarios. Specifically, in the first stage, TASPH designs a novel discrete asymmetric strategy to learn the modality-specific hash codes with varying lengths, enabling a flexible representation of heterogeneous data. Simultaneously, TASPH utilizes two semantic transformation matrices to establish the semantic correlations between varying hash codes. Different from most of the existing approaches that employ relaxation solutions, TASPH satisfies the discrete constraints without any relaxation. In the second stage, the learned semantic transformation matrices are employed to alleviate cross-modal heterogeneity, which guarantees that TASPH can learn more powerful hash functions to improve the discriminative ability of hash codes. Abundant experiments conducted on three benchmark datasets demonstrate encouraging results compared with the state-of-the-art approaches under different retrieval scenarios.},
  archive      = {J_TKDE},
  author       = {Junfan Huang and Peipei Kang and Na Han and Yonghao Chen and Xiaozhao Fang and Hongbo Gao and Guoxu Zhou},
  doi          = {10.1109/TKDE.2023.3283984},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {429-444},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Two-stage asymmetric similarity preserving hashing for cross-modal retrieval},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards semi-supervised universal graph classification.
<em>TKDE</em>, <em>36</em>(1), 416–428. (<a
href="https://doi.org/10.1109/TKDE.2023.3280859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks have pushed state-of-the-arts in graph classifications recently. Typically, these methods are studied within the context of supervised end-to-end training, which necessities copious task-specific labels. However, in real-world circumstances, labeled data could be limited, and there could be a massive corpus of unlabeled data, even from unknown classes as a complementary. Towards this end, we study the problem of semi-supervised universal graph classification, which not only identifies graph samples which do not belong to known classes, but also classifies the remaining samples into their respective classes. This problem is challenging due to a severe lack of labels and potential class shifts. In this paper, we propose a novel graph neural network framework named UGNN, which makes the best of unlabeled data from the subgraph perspective. To tackle class shifts, we estimate the certainty of unlabeled graphs using multiple subgraphs, which facilities the discovery of unlabeled data from unknown categories. Moreover, we construct semantic prototypes in the embedding space for both known and unknown categories and utilize posterior prototype assignments inferred from the Sinkhorn-Knopp algorithm to learn from abundant unlabeled graphs across different subgraph views. Extensive experiments on six datasets verify the effectiveness of UGNN in different settings.},
  archive      = {J_TKDE},
  author       = {Xiao Luo and Yusheng Zhao and Yifang Qin and Wei Ju and Ming Zhang},
  doi          = {10.1109/TKDE.2023.3280859},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {416-428},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards semi-supervised universal graph classification},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Static and streaming discovery of maximal linear
representation between time series. <em>TKDE</em>, <em>36</em>(1),
401–415. (<a href="https://doi.org/10.1109/TKDE.2023.3287273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, many applications, like the Internet of Things and Industrial Internet, collect data points from sensors continuously to form long time series. Finding the correlation between time series is a fundamental task for many time series mining problems. However, it is meaningless to directly measure the global correlation between two long time series due to concept shift or noise data. To tackle this challenge, in this paper, we formulate the novel problem of finding maximal significant linear representation. The major idea is that, given two time series and a quality constraint, we want to find the longest gapped time interval on which a time series can be linearly represented by the other within the quality constraint requirement. We develop both exact and approximate algorithms (with approximation quality guarantees), which exploit a novel representation of the linear correlation between time series on subsequences, and transform the problem into a geometric search. Moreover, we propose an online approach to find this correlation in each sliding window incrementally for the streaming data. We present a systematic empirical study to verify the efficiency and effectiveness of our approaches.},
  archive      = {J_TKDE},
  author       = {Zeyu Wang and Zhenying He and Peng Wang and Yang Wang and Wei Wang},
  doi          = {10.1109/TKDE.2023.3287273},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {401-415},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Static and streaming discovery of maximal linear representation between time series},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatiotemporal propagation learning for network-wide flight
delay prediction. <em>TKDE</em>, <em>36</em>(1), 386–400. (<a
href="https://doi.org/10.1109/TKDE.2023.3286690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and interpretable delay predictions are vital for decision-making in the aviation industry. However, effectively incorporating spatiotemporal dependencies and external factors related to delay propagation remains a challenge. To address this challenge, we propose the SpatioTemporal Propagation Network (STPN), a novel space-time separable graph convolutional network that models delay propagation by considering both spatial and temporal factors. STPN uses a multi-graph convolution model that considers both geographic proximity and airline schedules from a spatial perspective, while employing a multi-head self-attention mechanism that can be learned end-to-end and explicitly accounts for various types of temporal dependencies in delay time series from a temporal perspective. Experiments on two real-world delay datasets show that STPN outperforms state-of-the-art methods for multi-step ahead arrival and departure delay prediction in large-scale airport networks. Additionally, the counterfactuals generated by STPN provide evidence of its ability to learn explainable delay propagation patterns. Comprehensive experiments also demonstrate that STPN sets a robust benchmark for general spatiotemporal forecasting. The code for STPN is available at https://github.com/Kaimaoge/STPN .},
  archive      = {J_TKDE},
  author       = {Yuankai Wu and Hongyu Yang and Yi Lin and Hong Liu},
  doi          = {10.1109/TKDE.2023.3286690},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {386-400},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Spatiotemporal propagation learning for network-wide flight delay prediction},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatio-temporal joint graph convolutional networks for
traffic forecasting. <em>TKDE</em>, <em>36</em>(1), 372–385. (<a
href="https://doi.org/10.1109/TKDE.2023.3284156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shifted their focus towards formulating traffic forecasting as a spatio-temporal graph modeling problem. Typically, they constructed a static spatial graph at each time step and then connected each node with itself between adjacent time steps to create a spatio-temporal graph. However, this approach failed to explicitly reflect the correlations between different nodes at different time steps, thus limiting the learning capability of graph neural networks. Additionally, those models overlooked the dynamic spatio-temporal correlations among nodes by using the same adjacency matrix across different time steps. To address these limitations, we propose a novel approach called Spatio-Temporal Joint Graph Convolutional Networks (STJGCN) for accurate traffic forecasting on road networks over multiple future time steps. Specifically, our method encompasses the construction of both pre-defined and adaptive spatio-temporal joint graphs (STJGs) between any two time steps, which represent comprehensive and dynamic spatio-temporal correlations. We further introduce dilated causal spatio-temporal joint graph convolution layers on the STJG to capture spatio-temporal dependencies from distinct perspectives with multiple ranges. To aggregate information from different ranges, we propose a multi-range attention mechanism. Finally, we evaluate our approach on five public traffic datasets and experimental results demonstrate that STJGCN is not only computationally efficient but also outperforms 11 state-of-the-art baseline methods.},
  archive      = {J_TKDE},
  author       = {Chuanpan Zheng and Xiaoliang Fan and Shirui Pan and Haibing Jin and Zhaopeng Peng and Zonghan Wu and Cheng Wang and Philip S. Yu},
  doi          = {10.1109/TKDE.2023.3284156},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {372-385},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Spatio-temporal joint graph convolutional networks for traffic forecasting},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Size-constrained community search on large networks: An
effective and efficient solution. <em>TKDE</em>, <em>36</em>(1),
356–371. (<a href="https://doi.org/10.1109/TKDE.2023.3280483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a fundamental graph problem, community search is applied in various areas, e.g., social networks, the world wide web, and biology. A common requirement from real applications is to return a community with a bounded size while most existing solutions do not constrain community size. Recent studies on size-constrained community search still have some critical issues, e.g., the existence of a better cohesiveness objective, some queries returning empty results, and inefficiency on partial queries. Thus, in this paper, we study the size-constrained truss community search (STCS). Given a graph $G$ , a query vertex $q$ , and size constraint $[l,h]$ , the STCS problem aims to find a subgraph containing $q$ with the largest min-support among all connected subgraphs having at least $l$ and at most $h$ vertices. We prove the STCS problem is NP-hard and APX-hard unless P = NP. An effective heuristic is proposed to quickly find a high-quality initial result. Then, a branch and bound algorithm is introduced to find the exact result, with novel optimizations, e.g., budget-cost-based bounding and branching strategies. Extensive experiments verify that the community quality returned by our algorithm is better and our algorithm is faster by up to 5 orders of magnitude, compared with the state-of-the-art.},
  archive      = {J_TKDE},
  author       = {Fan Zhang and Haicheng Guo and Dian Ouyang and Shiyu Yang and Xuemin Lin and Zhihong Tian},
  doi          = {10.1109/TKDE.2023.3280483},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {356-371},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Size-constrained community search on large networks: An effective and efficient solution},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised learning for recommender systems: A survey.
<em>TKDE</em>, <em>36</em>(1), 335–355. (<a
href="https://doi.org/10.1109/TKDE.2023.3282907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, neural architecture-based recommender systems have achieved tremendous success, but they still fall short of expectation when dealing with highly sparse data. Self-supervised learning (SSL), as an emerging technique for learning from unlabeled data, has attracted considerable attention as a potential solution to this issue. This survey paper presents a systematic and timely review of research efforts on self-supervised recommendation (SSR). Specifically, we propose an exclusive definition of SSR, on top of which we develop a comprehensive taxonomy to divide existing SSR methods into four categories: contrastive, generative, predictive, and hybrid. For each category, we elucidate its concept and formulation, the involved methods, as well as its pros and cons. Furthermore, to facilitate empirical comparison, we release an open-source library SELFRec ( https://github.com/Coder-Yu/SELFRec ), which incorporates a wide range of SSR models and benchmark datasets. Through rigorous experiments using this library, we derive and report some significant findings regarding the selection of self-supervised signals for enhancing recommendation. Finally, we shed light on the limitations in the current research and outline the future research directions.},
  archive      = {J_TKDE},
  author       = {Junliang Yu and Hongzhi Yin and Xin Xia and Tong Chen and Jundong Li and Zi Huang},
  doi          = {10.1109/TKDE.2023.3282907},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {335-355},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Self-supervised learning for recommender systems: A survey},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SAFER-STUDENT for safe deep semi-supervised learning with
unseen-class unlabeled data. <em>TKDE</em>, <em>36</em>(1), 318–334. (<a
href="https://doi.org/10.1109/TKDE.2023.3279139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep semi-supervised learning (SSL) methods aim to utilize abundant unlabeled data to improve the seen-class classification. However, in the open-world scenario, collected unlabeled data tend to contain unseen-class data, which would degrade the generalization to seen-class classification. Formally, we define the problem as safe deep semi-supervised learning with unseen-class unlabeled data. One intuitive solution is removing these unseen-class instances after detecting them during the SSL process. Nevertheless, the performance of unseen-class identification is limited by the lack of suitable score function, the uncalibrated model, and the small number of labeled data. To this end, we propose a safe SSL method called SAFE R -STUDENT from the teacher-student view. First, to enhance the ability of teacher model to identify seen and unseen classes, we propose a general scoring framework called D iscrepancy with R aw (DR). Second, based on unseen-class data mined by teacher model from unlabeled data, we calibrate student model by newly proposed U nseen-class E nergy-bounded C alibration (UEC) loss. Third, based on seen-class data mined by teacher model from unlabeled data, we propose W eighted C onfirmation B ias E limination (WCBE) loss to boost seen-class classification of student model. Extensive studies show that SAFE R -STUDENT remarkably outperforms the state-of-the-art, verifying the effectiveness of our method in the under-explored problem.},
  archive      = {J_TKDE},
  author       = {Rundong He and Zhongyi Han and Xiankai Lu and Yilong Yin},
  doi          = {10.1109/TKDE.2023.3279139},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {318-334},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SAFER-STUDENT for safe deep semi-supervised learning with unseen-class unlabeled data},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantum bandit with amplitude amplification exploration in
an adversarial environment. <em>TKDE</em>, <em>36</em>(1), 311–317. (<a
href="https://doi.org/10.1109/TKDE.2023.3279207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid proliferation of learning systems in an arbitrarily changing environment mandates the need to manage tensions between exploration and exploitation. This work proposes a quantum-inspired bandit learning approach for the learning-and-adapting-based offloading problem where a client observes and learns the costs of each task offloaded to the candidate resource providers, e.g., fog nodes. In this approach, a new action update strategy and novel probabilistic action selection are adopted, provoked by the amplitude amplification and collapse postulate in quantum computation theory. We devise a locally linear mapping between a quantum-mechanical phase in a quantum domain, e.g., Grover-type search algorithm, and a distilled probability-magnitude in a value-based decision-making domain, e.g., adversarial multi-armed bandit algorithm. The proposed algorithm is generalized, via the devised mapping, for better learning weight adjustments on favorable/unfavorable actions, and its effectiveness is verified via simulation.},
  archive      = {J_TKDE},
  author       = {Byungjin Cho and Yu Xiao and Pan Hui and Daoyi Dong},
  doi          = {10.1109/TKDE.2023.3279207},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {311-317},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Quantum bandit with amplitude amplification exploration in an adversarial environment},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PUTS: Privacy-preserving and utility-enhancing framework for
trajectory synthesization. <em>TKDE</em>, <em>36</em>(1), 296–310. (<a
href="https://doi.org/10.1109/TKDE.2023.3288154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle trajectory data is essential for traffic management and location-based services. However, publishing real-life trajectory data has been challenging because vehicle trajectories contain users’ sensitive information. Differential privacy addresses such problems by publishing a synthetic version of the input dataset, but existing works always assume the real-world data is absolutely accurate. This assumption no longer holds in trajectory data because it typically contains errors due to inaccurate positioning services, which leads to poor performance of data synthesized by such trajectories. Even worse, existing works may generate unrealistic trajectories due to their coarse data synthesis methods, resulting in low practical utility or even inability to handle complex tasks. In this paper, we propose a P rivacy-preserving and U tility-enhancing framework for T rajectory S ynthesization ( PUTS ). Our framework mitigates the impact of data errors in trajectories on differential privacy mechanisms, by exploiting map-matching techniques and real-world road network structure. In PUTS , a two-layer approach from path to trajectory synthesis is proposed to not only guarantee the reality of synthetic trajectories, but also scale up PUTS in real-world applications. Extensive experiments on real-world datasets show that PUTS significantly outperforms existing methods in terms of utility in a range of real-world applications.},
  archive      = {J_TKDE},
  author       = {Xinyue Sun and Qingqing Ye and Haibo Hu and Jiawei Duan and Qiao Xue and Tianyu Wo and Jie Xu},
  doi          = {10.1109/TKDE.2023.3288154},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {296-310},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {PUTS: Privacy-preserving and utility-enhancing framework for trajectory synthesization},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural similarity search on supergraph containment.
<em>TKDE</em>, <em>36</em>(1), 281–295. (<a
href="https://doi.org/10.1109/TKDE.2023.3279920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supergraph search is a fundamental graph query processing problem. Supergraph search aims to find all data graphs contained in a given query graph based on the subgraph isomorphism. Existing algorithms construct the indices and adopt the filtering-and-verification framework which is usually computationally expensive and can cause redundant computations. Recently, various learning-based methods have been proposed for a good trade-off between accuracy and efficiency for query processing tasks. However, to the best of our knowledge, there is no learning-based method proposed for the supergraph search task. In this paper, we propose the first learning-based method for similarity search on supergraph containment, named Neural Supergraph similarity Search ( NSS ). NSS first learns the representations for query and data graphs and then efficiently conducts the supergraph search on the representation space whose complexity is linear to the number of data graphs. The carefully designed Wasserstein discriminator and reconstruction network enable NSS to better capture the interrelation, structural and label information between and within the query and data graphs. Experiments demonstrate that the NSS is up to 6 orders of magnitude faster than the state-of-the-art exact supergraph search algorithm in terms of query processing and more accurate compared to the other learning-based solutions.},
  archive      = {J_TKDE},
  author       = {Hanchen Wang and Jianke Yu and Xiaoyang Wang and Chen Chen and Wenjie Zhang and Xuemin Lin},
  doi          = {10.1109/TKDE.2023.3279920},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {281-295},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Neural similarity search on supergraph containment},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural moderation of ASMR erotica content in social
networks. <em>TKDE</em>, <em>36</em>(1), 275–280. (<a
href="https://doi.org/10.1109/TKDE.2023.3283501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularity of video/audio streaming applications in recent years, the wide spread of Autonomous Sensory Meridian Response (ASMR) erotica content is becoming a serious issue in social networks. Due to the subtle nature of ASMR erotica and its relative rareness in real scenario, detecting ASMR erotica contents is a challenging task. In this article, we propose a novel neural framework for ASMR erotica content moderation. The proposed framework consists of a pipeline of novel strategies to tackle challenges unique in ASMR Erotica Contents such as data scarcity and imbalanced data. Based on large-scale industrial data, the proposed framework demonstrates high moderation accuracy in quantitative analysis and significantly outperforming the existing counterparts.},
  archive      = {J_TKDE},
  author       = {Yixin Chen and Di Jiang and Conghui Tan and Yuanfeng Song and Chen Zhang and Lei Chen},
  doi          = {10.1109/TKDE.2023.3283501},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {275-280},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Neural moderation of ASMR erotica content in social networks},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-scale self-supervised graph contrastive learning with
injective node augmentation. <em>TKDE</em>, <em>36</em>(1), 261–274. (<a
href="https://doi.org/10.1109/TKDE.2023.3278463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Contrastive Learning (GCL) with Graph Neural Networks (GNN) has emerged as a promising method for learning latent node representations in a self-supervised manner. Most of existing GCL methods employ random sampling for graph view augmentation and maximize the agreement of the node representations between the views. However, the random augmentation manner, which is likely to produce very similar graph view samplings, may easily result in incomplete nodal contextual information, thus weakening the discrimination of node representations. To this end, this paper proposes a novel trainable scheme from the perspective of node augmentation, which is theoretically proved to be injective and utilizes the subgraphs consisting of each node with its neighbors to enhance the distinguishability of nodal view. Notably, our proposed scheme tries to enrich node representations via a multi-scale contrastive training that integrates three different levels of training granularity, i.e., subgraph level, graph- and node-level contextual information. In particular, the subgraph-level objective between augmented and original node views is constructed to enhance the discrimination of node representations while graph- and node-level objectives with global and local information from the original graph are developed to improve the generalization ability of representations. Experiment results demonstrate that our framework outperforms existing state-of-the-art baselines and even surpasses several supervised counterparts on four real-world datasets for node classification.},
  archive      = {J_TKDE},
  author       = {Haonan Zhang and Yuyang Ren and Luoyi Fu and Xinbing Wang and Guihai Chen and Chenghu Zhou},
  doi          = {10.1109/TKDE.2023.3278463},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {261-274},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-scale self-supervised graph contrastive learning with injective node augmentation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-modal hashing for efficient multimedia retrieval: A
survey. <em>TKDE</em>, <em>36</em>(1), 239–260. (<a
href="https://doi.org/10.1109/TKDE.2023.3282921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of multimedia contents, multimedia retrieval is facing unprecedented challenges on both storage cost and retrieval speed. Hashing technique can project the high-dimensional data into compact binary hash codes. With it, the most time-consuming semantic similarity computation during the multimedia retrieval process can be significantly accelerated with fast Hamming distance computation, and meanwhile the storage cost can be reduced greatly by the binary embedding. In the light of this, multi-modal hashing has recently received considerable attention to support large-scale multimedia retrieval. Different from uni-modal hashing, the multi-modal hashing focuses on modeling the multi-modal semantics and further preserving them into binary hash codes with hash learning. In this paper, we first systematically review the existing learning to hash methods for efficient multimedia retrieval, categorizing them according to the multimedia retrieval tasks, the specific multi-modal semantic modeling techniques, and hash learning strategies. Thereafter, we present the performance comparison results. We ultimately discuss the challenges and potential research directions that may require further investigation in multi-modal hash learning.},
  archive      = {J_TKDE},
  author       = {Lei Zhu and Chaoqun Zheng and Weili Guan and Jingjing Li and Yang Yang and Heng Tao Shen},
  doi          = {10.1109/TKDE.2023.3282921},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {239-260},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-modal hashing for efficient multimedia retrieval: A survey},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge graph contrastive learning based on
relation-symmetrical structure. <em>TKDE</em>, <em>36</em>(1), 226–238.
(<a href="https://doi.org/10.1109/TKDE.2023.3282989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph embedding (KGE) aims at learning powerful representations to benefit various artificial intelligence applications. Meanwhile, contrastive learning has been widely leveraged in graph learning as an effective mechanism to enhance the discriminative capacity of the learned representations. However, the complex structures of KG make it hard to construct appropriate contrastive pairs. Only a few attempts have integrated contrastive learning strategies with KGE. But, most of them rely on language models ( e.g., Bert) for contrastive pair construction instead of fully mining information underlying the graph structure, hindering expressive ability. Surprisingly, we find that the entities within a relational symmetrical structure are usually similar and correlated. To this end, we propose a knowledge graph contrastive learning framework based on relation-symmetrical structure, KGE-SymCL, which mines symmetrical structure information in KGs to enhance the discriminative ability of KGE models. Concretely, a plug-and-play approach is proposed by taking entities in the relation-symmetrical positions as positive pairs. Besides, a self-supervised alignment loss is designed to pull together positive pairs. Experimental results on link prediction and entity classification datasets demonstrate that our KGE-SymCL can be easily adopted to various KGE models for performance improvements. Moreover, extensive experiments show that our model could outperform other state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Ke Liang and Yue Liu and Sihang Zhou and Wenxuan Tu and Yi Wen and Xihong Yang and Xiangjun Dong and Xinwang Liu},
  doi          = {10.1109/TKDE.2023.3282989},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {226-238},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Knowledge graph contrastive learning based on relation-symmetrical structure},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human cognition-based consistency inference networks for
multi-modal fake news detection. <em>TKDE</em>, <em>36</em>(1), 211–225.
(<a href="https://doi.org/10.1109/TKDE.2023.3280555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing models for multi-modal fake news detection focus mainly on capturing common similar semantics between different modalities to improve detection performance. However, they ignore the extraction of inconsistent features between these modalities. The intuitive cognition way people identify a piece of fake news is generally to discover if there are inconsistent semantics among news content itself and its comments, which could be abstracted as “comparing news image-text consistency - finding valuable comments - reasoning in-/consistency between news and comments”. Inspired by the cognitive process, we propose Human Cognition-based Consistency Inference Networks (HCCIN) to comprehensively explore consistent and inconsistent semantics for multi-modal fake news detection. Specifically, we first design cross-modal alignment layer to learn consistent semantics between textual and visual information within the multi-modal news, and then the comment clue discovery layer is devoted to ascertaining the most-concerned semantics by audiences between comments. Finally, we develop collaborative inference layer to drive news consistent semantics and the most-concerned semantics to reason and discover consistent and inconsistent information between them. Experiments on three public datasets, including Weibo, Twitter, and PHEME, reveal the superiority of our HCCIN.},
  archive      = {J_TKDE},
  author       = {Lianwei Wu and Pusheng Liu and Yongqiang Zhao and Peng Wang and Yangning Zhang},
  doi          = {10.1109/TKDE.2023.3280555},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {211-225},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Human cognition-based consistency inference networks for multi-modal fake news detection},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical alignment with polar contrastive learning for
next-basket recommendation. <em>TKDE</em>, <em>36</em>(1), 199–210. (<a
href="https://doi.org/10.1109/TKDE.2023.3282914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next-basket recommendation methods focus on the inference of the next basket by considering the corresponding basket sequence. Although many methods have been developed for the task, they usually suffer from data sparsity. The number of interactions between entities is relatively small compared to their huge bases, so it is crucial to mine as much hidden information as possible from the limited historical interactions for prediction. However, the existing methods mainly just treat the next-basket recommendation task as a single-view sequential prediction problem, which leads to the inadequate mining of the information hidden in multiple views, and the mining of other patterns in the historical interactions is neglected, thus making it difficult to learn high-quality representations and limiting the recommendation effect. To alleviate the above issues, we propose a novel method named HapCL for next-basket recommendation, which mines information from multiple views and patterns with the help of polar contrastive learning. A hierarchical module is designed to mine multiple patterns of historical interactions from different views at two levels. In order to mine self-supervised signals, we design a polar contrastive learning module with a novel graph-based augmentation approach. Experiments on three real-world datasets validate the effectiveness of HapCL.},
  archive      = {J_TKDE},
  author       = {Ting-Ting Su and Chang-Dong Wang and Wu-Dong Xi and Jian-Huang Lai and Philip S. Yu},
  doi          = {10.1109/TKDE.2023.3282914},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {199-210},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hierarchical alignment with polar contrastive learning for next-basket recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GraphMM: Graph-based vehicular map matching by leveraging
trajectory and road correlations. <em>TKDE</em>, <em>36</em>(1),
184–198. (<a href="https://doi.org/10.1109/TKDE.2023.3287739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Map matching of sparse vehicle trajectories is a fundamental problem in location-based services, such as traffic flow analysis and vehicle routing. Existing literature mainly relies on sequence-to-sequence (Seq2Seq) models to capture the intra-trajectory correlation of an input trajectory and to sequentially predict the matched road segments. Due to the limited expressive capability of sequential models, these methods fall short of extracting inter-trajectory and trajectory-road correlations as well as correlation between road segments . We present GraphMM , a graph-based approach that explicitly utilizes all aforementioned correlations. Our model exploits the graph nature of map matching and incorporates graph neural networks and conditional models to leverage both road and trajectory graph topology, while manages to align road segments and trajectories in latent space. We formally analyze the expressive power of our model in capturing various correlations and propose efficient algorithms for model training and inference. In particular, our optimization techniques dramatically reduce the computational complexity, making our model feasible on datasets with thousands of road segments. Extensive experiments show that our model significantly enhances prediction accuracy, while improving training and inference efficiency by up to an order of magnitude over both the industrial implementation of the hidden Markov model and state-of-the-art Seq2Seq-based methods.},
  archive      = {J_TKDE},
  author       = {Yu Liu and Qian Ge and Wei Luo and Qiang Huang and Lei Zou and Haixu Wang and Xin Li and Chang Liu},
  doi          = {10.1109/TKDE.2023.3287739},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {184-198},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GraphMM: Graph-based vehicular map matching by leveraging trajectory and road correlations},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FuFaction: Fuzzy factual inconsistency correction on
crowdsourced documents with hybrid-mask at the hidden-state level.
<em>TKDE</em>, <em>36</em>(1), 167–183. (<a
href="https://doi.org/10.1109/TKDE.2023.3286097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, crowdsourced documents like Wikipedia pages and comments on products are all over the Internet. However, documents generated by crowdsourcing participants may contain inconsistent facts, implicit semantics and fabricated contents, thus threatening the trustworthiness of information content security available in Internet. To address this problem, we propose FuFaction, enabled by an enhanced observation mechanism based on the notion of hybrid-mask consisting of a hard-mask and a soft-mask, to eliminate factual inconsistencies on crowdsourced documents at the hidden-state level (or in a fuzzy way), according to the given evidence retrieved from an external open domain. Specifically, instead of focusing on a specific category of factual inconsistency, FuFaction captures anomalous hidden-states between a crowdsourced document and evidence obtained via a reverse-attention mechanism, where a hard-mask controls the attending direction as bidirectional and unidirectional for better understanding on semantics. Then, a soft-mask is generated with the help of the hard-masked reverse-attention to revise or mask anomalous hidden-states on the crowdsourced document. Afterwards, the masked hidden-states are further refined by a cross reverse-attention and factual consistency reinforcement strategy, based on which a new crowdsourced document with higher factual consistency is generated via neural text generation. According to our experimental results, FuFaction can effectively deal with the fuzzy factual inconsistencies on crowdsourced documents, achieving the overall best performance in terms of factual consistency metrics with a little higher (yet still competitive) editing cost on literal vocabulary, so as to reflect factually consistent semantics supported by the given evidence.},
  archive      = {J_TKDE},
  author       = {Huan Rong and Gongchi Chen and Tinghuai Ma and Victor S. Sheng and Elisa Bertino},
  doi          = {10.1109/TKDE.2023.3286097},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {167-183},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {FuFaction: Fuzzy factual inconsistency correction on crowdsourced documents with hybrid-mask at the hidden-state level},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploiting group-level behavior pattern for session-based
recommendation. <em>TKDE</em>, <em>36</em>(1), 152–166. (<a
href="https://doi.org/10.1109/TKDE.2023.3280310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Session-based recommendation (SBR) is a challenging task, which aims to predict users’ future interests based on anonymous behavior sequences. Existing methods leverage powerful representation learning approaches to encode sessions into a low-dimensional space. However, despite such achievements, the existing studies focus on the instance-level session learning, while neglecting the group-level users’ preferences (e.g., the common preferences of group users in repeat consumption). To this end, we propose a novel R epeat-aware N eural M echanism for S ession-based R ecommendation (RNMSR). In RNMSR, we propose to learn the user preference from two levels: (i) instance-level , which employs GNNs on a similarity-based item-pairwise session graph to capture the users’ preference in instance-level. (ii) group-level , which converts sessions into group-level behavior patterns to model the group-level users’ preferences. In RNMSR, we combine instance-level and group-level user preference to model the repeat consumption of users, i.e., whether users take repeated consumption and which items are preferred by users. Extensive experiments are conducted on three real-world datasets, i.e., Diginetica, Yoochoose, and Nowplaying, demonstrating that the proposed method consistently achieves state-of-the-art performance in all the tests.},
  archive      = {J_TKDE},
  author       = {Ziyang Wang and Wei Wei and Shanshan Feng and Xian-Ling Mao and Minghui Qiu and Dangyang Chen and Rui Fang},
  doi          = {10.1109/TKDE.2023.3280310},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {152-166},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Exploiting group-level behavior pattern for session-based recommendation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Embedding graph convolutional networks in recurrent neural
networks for predictive monitoring. <em>TKDE</em>, <em>36</em>(1),
137–151. (<a href="https://doi.org/10.1109/TKDE.2023.3286017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive monitoring of business processes is a subfield of process mining that aims to predict, among other things, the characteristics of the next event or the sequence of the next events. Although multiple approaches based on deep learning have been proposed, mainly recurrent neural networks and convolutional neural networks, none of them really exploit the structural information available in process models. This paper proposes an approach that simultaneously learns spatio-temporal information from both the event log and the process model by combining recurrent neural networks with graph convolutional networks. Thus, common patterns from process models, such as loops or parallels, can be learned while avoiding overwriting information during the encoding phase. An experimental evaluation of real-life event logs shows that our approach is more consistent and outperforms the current state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Efrén Rama-Maneiro and Juan C. Vidal and Manuel Lama},
  doi          = {10.1109/TKDE.2023.3286017},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {137-151},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Embedding graph convolutional networks in recurrent neural networks for predictive monitoring},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient privacy-preserving spatial data query in cloud
computing. <em>TKDE</em>, <em>36</em>(1), 122–136. (<a
href="https://doi.org/10.1109/TKDE.2023.3283020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of geographic location technology and the explosive growth of data, a large amount of spatial data is outsourced to the cloud server for reducing the local high storage and computing burdens, but at the same time causes security issues. Thus, extensive privacy-preserving spatial data query schemes have been proposed. Most of the existing schemes use Asymmetric Scalar-Product-Preserving Encryption (ASPE) to encrypt data, but ASPE has proven to be insecure against known plaintext attack. And the existing schemes require users to provide more information about query range and thus generate a large amount of ciphertexts, which causes high storage and computational burdens. To solve these issues, based on enhanced ASPE designed in our conference version, we first propose a basic Privacy-preserving Spatial Data Query (PSDQ) scheme by using a new unified index structure, which only requires users to provide less information about query range. Then, we propose an enhanced PSDQ scheme (PSDQ $^+$ ) by using Geohash-based $R$ -tree structure (called $GR$ -tree) and efficient pruning strategy, which greatly reduces the query time. Formal security analysis proves that our schemes achieve Indistinguishability under Chosen Plaintext Attack (IND-CPA), and extensive experiments demonstrate that our schemes are efficient in practice.},
  archive      = {J_TKDE},
  author       = {Yinbin Miao and Yutao Yang and Xinghua Li and Linfeng Wei and Zhiquan Liu and Robert H. Deng},
  doi          = {10.1109/TKDE.2023.3283020},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {122-136},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient privacy-preserving spatial data query in cloud computing},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient discovery of functional dependencies on massive
data. <em>TKDE</em>, <em>36</em>(1), 107–121. (<a
href="https://doi.org/10.1109/TKDE.2023.3288209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional dependencies are the most common constraints in the design theory for relational databases, which have very important practical applications in many areas. Different kinds of algorithms are proposed to discover the functional dependencies. However, it is found in this paper that the existing algorithms cannot deal well with massive data which cannot be held entirely in memory due to high memory consumption and high computation cost. In this paper, a novel algorithm FSC is presented to compute functional dependencies on massive data. The two-step execution of FSC relies on a pre-computed update-friendly assistant structure of comparable pairs which reflect the identifier pairs for tuples with at least one equal attribute. In step 1, FSC determines the violated functional dependencies and introduces the selective comparison by comparable pairs to reduce the required pairwise comparison significantly. The direct value-combination compression strategy is devised to process attributes of small cardinality. In step 2, FSC induces the required functional dependencies by the results in step 1. The extensive experimental results, conducted on synthetic and real-life data sets, show that FSC can discover functional dependencies on massive data efficiently.},
  archive      = {J_TKDE},
  author       = {Xiaolong Wan and Xixian Han and Jinbao Wang and Jianzhong Li},
  doi          = {10.1109/TKDE.2023.3288209},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {107-121},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient discovery of functional dependencies on massive data},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CT-auth: Capacitive touchscreen-based continuous
authentication on smartphones. <em>TKDE</em>, <em>36</em>(1), 90–106.
(<a href="https://doi.org/10.1109/TKDE.2023.3277879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous authentication, which provides identity verification using behavioral biometrics in an implicit and transparent manner, has shown potentials for protecting privacy. As the most common way of human-computer interaction, touch behavior pattern of each user has been proven distinctive and widely adopted for continuous authentication. However, most touch based solutions rely on the touchscreen signals obtained from high-level application programming interfaces, which are hard to characterize fine-grained appearance and contour profile of contact fingertips as well as dynamic sliding information in a touch gesture. In this paper, we propose a continuous authentication framework called CT-Auth, which leverages raw capacitive value collected from capacitive touchscreen on smartphone as a descriptor of touch behavior for authentication. Specifically, we first develop a three-dimensional convolution neural network model for capturing intra-gesture spatial-temporal feature and a structure extraction model for capturing structural information between moving fingertips of a touch gesture and touchscreen. A recurrent neural network based model is also applied for capturing temporal patterns among a sequence of touch gestures. To evaluate the effectiveness of our framework, we recruit 100 volunteers over 2 months and collect a large-scale dataset in the unconstrained conditions. Extensive experiments reveal that CT-Auth provides the state-of-the-art authentication accuracy.},
  archive      = {J_TKDE},
  author       = {Zhihao Shen and Shun Li and Xi Zhao and Jianhua Zou},
  doi          = {10.1109/TKDE.2023.3277879},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {90-106},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CT-auth: Capacitive touchscreen-based continuous authentication on smartphones},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Convergence of a fast hierarchical alternating least squares
algorithm for nonnegative matrix factorization. <em>TKDE</em>,
<em>36</em>(1), 77–89. (<a
href="https://doi.org/10.1109/TKDE.2023.3279369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hierarchical alternating least squares (HALS) algorithms are powerful tools for nonnegative matrix factorization (NMF), among which the Fast-HALS, proposed in [A. Cichocki and A.-H. Phan, 2009], is one of the most efficient. This paper investigates the convergence of Fast-HALS. First, a more general weak convergence (converged subsequences exist and converge to the stationary point set) is established without any assumption, while most existing results assume all the columns of iterates are strictly away from the origin. Then, a simplified strong convergence (the entire sequence converges to a stationary point) proof is provided. The existing strong convergence is attributed to the block prox-linear (BPL) method, which is a more general framework including Fast-HALS as a special case. So, the convergence proof under BPL is quite complex. Our simplified proof explores the structure of Fast-HALS and can be regarded as a complement to the results under BPL. In addition, some numerical verifications are presented.},
  archive      = {J_TKDE},
  author       = {Liangshao Hou and Delin Chu and Li-Zhi Liao},
  doi          = {10.1109/TKDE.2023.3279369},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {77-89},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Convergence of a fast hierarchical alternating least squares algorithm for nonnegative matrix factorization},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CityTrans: Domain-adversarial training with knowledge
transfer for spatio-temporal prediction across cities. <em>TKDE</em>,
<em>36</em>(1), 62–76. (<a
href="https://doi.org/10.1109/TKDE.2023.3283520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the spatio-temporal data of a city is not always available, insufficient data would lead to poor performance in some urban prediction tasks. Existing works utilize transfer learning to solve the data scarcity problem, but they ignore the differences in data distributions across cities, which leads to the ineffectiveness of knowledge transfer. In this paper, we propose a domain adversarial model with knowledge transfer for spatio-temporal prediction across cities, entitled CityTrans . Specifically, 1) the self-adaptive spatio-temporal knowledge (namely ST-Knowledge) is mined, to learn the latent spatial and temporal patterns among cities; 2) the domain-adversarial training strategy is introduced to enhance domain invariance; 3) a knowledge attention mechanism is proposed to extract the transferable information from the ST-Knowledge. Note that our CityTrans is an end-to-end domain adversarial spatio-temporal network without two-stage training (i.e., pre-training and fine-tuning). Finally, we conduct extensive experiments on two spatio-temporal prediction tasks: traffic (flow and speed) prediction, and air quality prediction. Experimental results demonstrate that CityTrans outperforms state-of-the-art models on all tasks by a significant margin.},
  archive      = {J_TKDE},
  author       = {Xiaocao Ouyang and Yan Yang and Wei Zhou and Yiling Zhang and Hao Wang and Wei Huang},
  doi          = {10.1109/TKDE.2023.3283520},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {62-76},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CityTrans: Domain-adversarial training with knowledge transfer for spatio-temporal prediction across cities},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Broad learning autoencoder with graph structure for data
clustering. <em>TKDE</em>, <em>36</em>(1), 49–61. (<a
href="https://doi.org/10.1109/TKDE.2023.3283425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Broad learning system (BLS) is a simple yet efficient learning algorithm that only needs to train a three-layer feedforward neural network. Although various BLS variants have been designed for supervised learning, none have been used for unsupervised learning. This paper proposes BLS-AE, a novel data clustering scheme that seamlessly combines BLS and auto-encoder. Then, graph regularization is introduced into BLS-AE to increase the capability of learning intrinsic structures in data and adaptation to various data simultaneously, which is termed BLSg-AE. Moreover, different concatenation styles of feature and enhancement nodes are investigated for reusing the learned features, followed by designing two special strategies (i.e., pruning optimization and incremental learning) to reduce the parameter scale significantly and improve performance, which is termed xBLSg-AE. To address the performance instability issue caused by random subspace in a single xBLSg-AE, the x-cascade broad learning system graph regularization multi-auto-encoder (xBLSg-MAE) algorithm is proposed. Extensive experiments are conducted on multiple real data sets to demonstrate that the proposed methods are more effective and robust than competing approaches.},
  archive      = {J_TKDE},
  author       = {Zhiwen Yu and Zhijie Zhong and Kaixiang Yang and Wenming Cao and C. L. Philip Chen},
  doi          = {10.1109/TKDE.2023.3283425},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {49-61},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Broad learning autoencoder with graph structure for data clustering},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Balanced class-incremental 3D object classification and
retrieval. <em>TKDE</em>, <em>36</em>(1), 35–48. (<a
href="https://doi.org/10.1109/TKDE.2023.3284032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing 3D object classification and retrieval algorithms rely on one-off supervised learning on closed 3D object sets and tend to provide rigid convolutional neural networks with little scalability. Such limitations substantially restrict their potential to learn newly emerged 3D object classes continually in the real world. Aiming to go beyond these limitations, we innovatively propose two new and challenging tasks: class-incremental 3D object classification ( CI-3DOC ) and class-incremental 3D object retrieval ( CI-3DOR ), the key to which is class-incremental 3D representation learning. It expects the network to update continually to learn new 3D class representations without forgetting the previously learned ones. To this end, we design a novel balanced distillation network (BDNet) that uses a dual supervision mechanism to balance between consolidating old knowledge (stability) and adapting to new 3D object classes (plasticity) carefully. On the one hand, we employ stability-based supervision to retain the stable and discriminative information of old classes that greatly benefit both classification and retrieval tasks. On the other hand, we use plasticity-based supervision to improve the network&#39;s generalization for learning new class 3D representations by transferring knowledge from a temporary teacher network to the current model. By properly handling the relationship between the two modules, we achieve a surprising performance improvement. Furthermore, considering there is no available dataset for evaluation, we build two 3D datasets, INOR-1 and INOR-2, to evaluate these two new tasks. Extensive experimental results demonstrate that our method can significantly outperform other state-of-the-art class-incremental learning methods. Even if we store 500-1000 fewer 3D objects than SOTA methods, BDNet still achieves comparable performance.},
  archive      = {J_TKDE},
  author       = {An-An Liu and Haochun Lu and Heyu Zhou and Tianbao Li and Mohan Kankanhalli},
  doi          = {10.1109/TKDE.2023.3284032},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {35-48},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Balanced class-incremental 3D object classification and retrieval},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified framework for contextual and factoid question
generation. <em>TKDE</em>, <em>36</em>(1), 21–34. (<a
href="https://doi.org/10.1109/TKDE.2023.3280182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Question generation (QG) aims to automatically generate fluent and relevant questions, where the two most mainstream directions are generating questions from unstructured contextual texts (CQG), such as news articles, and generating questions from structured factoid texts (FQG), such as knowledge graphs or tables. Existing methods for these two tasks mainly face challenges of limited internal structural information as well as scarce background information, while these two tasks can benefit each other for alleviating these issues. For example, when meeting the entity mention “United Kingdom” in CQG, it can be inferred that it is a country in European continent based on the structural knowledge “(Europe, countries_within, United Kingdom)” in FQG. And when meeting the entity “Houston Rockets” in FQG, more background information, such as “an American professional basketball team based in Houston since 1971”, can be found in the related passages of CQG. To this end, we propose a unified framework for the tasks of CQG and FQG, where: (i) two types of task-sharing modules are developed to learn shared contextual and structural knowledge, where the task format is unified with a pseudo passage reformulation strategy; (ii) for the CQG task, a task-specific knowledge module with a knowledge selection and aggregation mechanism is introduced, so as to incorporate more factoid knowledge from external knowledge graphs and alleviate the word ambiguity problem; and (iii) for the FQG task, a task-specific passage module with a multi-level passage fusion mechanism is designed to extract fine-grained word-level knowledge. Experimental results in both automatic and human evaluation show the effectiveness of our proposed method.},
  archive      = {J_TKDE},
  author       = {Chenhe Dong and Ying Shen and Shiyang Lin and Zhenzhou Lin and Yang Deng},
  doi          = {10.1109/TKDE.2023.3280182},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {21-34},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A unified framework for contextual and factoid question generation},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of graph-based deep learning for anomaly detection
in distributed systems. <em>TKDE</em>, <em>36</em>(1), 1–20. (<a
href="https://doi.org/10.1109/TKDE.2023.3282898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is a crucial task in complex distributed systems. A thorough understanding of the requirements and challenges of anomaly detection is pivotal to the security of such systems, especially for real-world deployment. While there are many works and application domains that deal with this problem, few have attempted to provide an in-depth look at such systems. In this survey, we explore the potentials of graph-based algorithms to identify anomalies in distributed systems. These systems can be heterogeneous or homogeneous, which can result in distinct requirements. One of our objectives is to provide an in-depth look at graph-based approaches to conceptually analyze their capability to handle real-world challenges such as heterogeneity and dynamic structure. This study gives an overview of the State-of-the-Art (SotA) research articles in the field and compare and contrast their characteristics. To facilitate a more comprehensive understanding, we present three systems with varying abstractions as use cases. We examine the specific challenges involved in anomaly detection within such systems. Subsequently, we elucidate the efficacy of graphs in such systems and explicate their advantages. We then delve into the SotA methods and highlight their strength and weaknesses, pointing out the areas for possible improvements and future works.},
  archive      = {J_TKDE},
  author       = {Armin Danesh Pazho and Ghazal Alinezhad Noghre and Arnab A Purkayastha and Jagannadh Vempati and Otto Martin and Hamed Tabkhi},
  doi          = {10.1109/TKDE.2023.3282898},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {1},
  number       = {1},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey of graph-based deep learning for anomaly detection in distributed systems},
  volume       = {36},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
