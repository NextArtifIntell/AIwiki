<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tai---525">TAI - 525</h2>
<ul>
<li><details>
<summary>
(2024). Compact multitasking multichromosome genetic algorithm for
heuristic selection in ontology matching. <em>TAI</em>, <em>5</em>(12),
6752–6766. (<a href="https://doi.org/10.1109/TAI.2024.3442731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ontology matching (OM) is critical for knowledge integration and system interoperability on the semantic web, tasked with identifying semantically related entities across different ontologies. Despite its importance, the complexity of terminology semantics and the large number of potential matches present significant challenges. Existing methods often struggle to balance between accurately capturing the multifaceted nature of semantic relationships and computational efficiency. This work introduces a novel approach, a compact multitasking multichromosome genetic algorithm for Heuristic selection (HS) in OM, designed to navigate the nuanced hierarchical structure of ontologies and diverse entity mapping preferences. Our method combines compact genetic algorithms with multichromosome optimization for entity sequencing and assigning HS, alongside an adaptive knowledge transfer mechanism to finely balance exploration and exploitation efforts. Evaluated on the ontology alignment evaluation initiative&#39;s benchmark, our algorithm demonstrates superior ability to produce high-quality ontology alignments efficiently, surpassing comparative methods in both effectiveness and efficiency. These findings underscore the potential of advanced genetic algorithms in enhancing OM processes, offering significant contributions to the broader AI field by improving the interoperability and knowledge integration capabilities of semantic web technologies.},
  archive      = {J_TAI},
  author       = {Xingsi Xue and Jerry Chun-Wei Lin and Tong Su},
  doi          = {10.1109/TAI.2024.3442731},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6752-6766},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Compact multitasking multichromosome genetic algorithm for heuristic selection in ontology matching},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MTECC: A multitask learning framework for esophageal cancer
analysis. <em>TAI</em>, <em>5</em>(12), 6739–6751. (<a
href="https://doi.org/10.1109/TAI.2024.3485524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of esophageal cancer diagnostics, the accurate identification and classification of tumors and adjacent tissues within whole slide images (WSIs) are critical. However, this task is complicated by the difficulty in annotating normal tissue on tumor-bearing slides, as the infiltration results in a blend of different tissue types, making annotation difficult for pathologists. To overcome this challenge, we introduce the multitask esophageal cancer classification (MTECC) framework, featuring an innovative dual-branch architecture that operates at both global and local levels. The framework initially employs a masked autoencoder (MAE) for self-supervised learning. A distinctive feature of MTECC is the integration of RandoMix, an innovative image augmentation technique that randomly exchanges patches between different images. This method significantly enhances the model&#39;s generalization ability, especially for recognizing tissues within cancerous slides. MTECC ingeniously integrates two tasks: tumor detection using global tokens, and fine-grained tissue classification at the patch level using local tokens. The empirical evaluation of the MTECC on our extensive esophageal cancer dataset substantiates its efficacy. The performance metrics indicate robust results, with an accuracy of 0.811, an F1 score of 0.735, and an AUC of 0.957. The MTECC method represents a significant advancement in applying deep learning to complex pathological image analysis, offering valuable tools for pathologists in diagnosing and treating esophageal cancer.},
  archive      = {J_TAI},
  author       = {Jianpeng An and Wenqi Li and Yunhao Bai and Huazhen Chen and Gang Zhao and Qing Cai and Zhongke Gao},
  doi          = {10.1109/TAI.2024.3485524},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6739-6751},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {MTECC: A multitask learning framework for esophageal cancer analysis},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards better accuracy-efficiency trade-offs: Dynamic
activity inference via collaborative learning from various
width-resolution configurations. <em>TAI</em>, <em>5</em>(12),
6723–6738. (<a href="https://doi.org/10.1109/TAI.2024.3489532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep neural networks have triumphed over a large variety of human activity recognition (HAR) applications on resource-constrained mobile devices. However, most existing works are static and ignore the fact that the computational budget usually changes drastically across various devices, which prevent real-world HAR deployment. It still remains a major challenge: how to adaptively and instantly tradeoff accuracy and latency at runtime for on-device activity inference using time series sensor data? To address this issue, this article introduces a new collaborative learning scheme by training a set of subnetworks executed at varying network widths when fueled with different sensor input resolutions as data augmentation, which can instantly switch on-the-fly at different width-resolution configurations for flexible and dynamic activity inference under varying resource budgets. Particularly, it offers a promising performance-boosting solution by utilizing self-distillation to transfer the unique knowledge among multiple width-resolution configuration, which can capture stronger feature representations for activity recognition. Extensive experiments and ablation studies on three public HAR benchmark datasets validate the effectiveness and efficiency of our approach. A real implementation is evaluated on a mobile device. This discovery opens up the possibility to directly access accuracy-latency spectrum of deep learning models in versatile real-world HAR deployments. Code is available at https://github.com/Lutong-Qin/Collaborative_HAR .},
  archive      = {J_TAI},
  author       = {Lutong Qin and Lei Zhang and Chengrun Li and Chaoda Song and Dongzhou Cheng and Shuoyuan Wang and Hao Wu and Aiguo Song},
  doi          = {10.1109/TAI.2024.3489532},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6723-6738},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Towards better accuracy-efficiency trade-offs: Dynamic activity inference via collaborative learning from various width-resolution configurations},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Constrained multiobjective optimization via relaxations on
both constraints and objectives. <em>TAI</em>, <em>5</em>(12),
6709–6722. (<a href="https://doi.org/10.1109/TAI.2024.3454025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since most multiobjective optimization problems in real-world applications contain constraints, constraint-handling techniques (CHTs) are necessary for a multiobjective optimizer. However, existing CHTs give no relaxation to objectives, resulting in the elimination of infeasible dominated solutions that are promising (potentially useful but inferior) for detecting feasible regions and the constrained Pareto front (CPF). To overcome this drawback, in this work, we propose an objective relaxation technique that can preserve promising by relaxing objective function values, i.e., convergence, through an adaptively adjusted relaxation factor. Further, we develop a new constrained multiobjective optimization evolutionary algorithm (CMOEA) based on relaxations on both constraints and objectives. The proposed algorithm evolves one population by the constraint relaxation technique to preserve promising infeasible solutions and the other population by both objective and constraint relaxation techniques to preserve promising infeasible dominated solutions. In this way, our method can overcome the drawback of existing CHTs. Besides, an archive update strategy is designed to maintain encountered feasible solutions by the two populations to approximate the CPF. Experiments on challenging benchmark problems and real-world problems have demonstrated the superiority or at least competitiveness of our proposed CMOEA. Moreover, to verify the generality of the objective relaxation technique, we embed it into two existing CMOEA frameworks and the results show that it can significantly improve the performance in handling challenging problems.},
  archive      = {J_TAI},
  author       = {Fei Ming and Bing Xue and Mengjie Zhang and Wenyin Gong and Huixiang Zhen},
  doi          = {10.1109/TAI.2024.3454025},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6709-6722},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Constrained multiobjective optimization via relaxations on both constraints and objectives},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning for solving colored traveling
salesman problems: An entropy-insensitive attention approach.
<em>TAI</em>, <em>5</em>(12), 6699–6708. (<a
href="https://doi.org/10.1109/TAI.2024.3461630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The utilization of neural network models for solving combinatorial optimization problems (COPs) has gained significant attention in recent years and has demonstrated encouraging outcomes in addressing analogous problems such as the traveling salesman problem (TSP). The multiple TSP (MTSP) has sparked the interest of researchers as a special kind of COPs. The colored TSP (CTSP) is a variation of the MTSP, which utilizes colors to distinguish the accessibility of cities to salesmen. This article proposes a gated entropy-insensitive attention model (GEIAM) to solve CTSP. In specific, the original problem is first modeled as a sequence and preprocessed by the problem feature extraction network of the model, and then solved by the autoregressive solution constructor subsequently. The policy (parameters of the neural network model) is trained via reinforcement learning (RL). The proposed approach is compared with several commercial solvers as well as heuristics and demonstrates superior solving speed with comparable solution quality.},
  archive      = {J_TAI},
  author       = {Tianyu Zhu and Xinli Shi and Xiangping Xu and Jinde Cao},
  doi          = {10.1109/TAI.2024.3461630},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6699-6708},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Reinforcement learning for solving colored traveling salesman problems: An entropy-insensitive attention approach},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Get rid of your trail: Remotely erasing backdoors in
federated learning. <em>TAI</em>, <em>5</em>(12), 6683–6698. (<a
href="https://doi.org/10.1109/TAI.2024.3465441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) enables collaborative learning across multiple participants without exposing sensitive personal data. However, the distributed nature of FL and unvetted participants’ data makes it vulnerable to backdoor attacks . In these attacks, adversaries selectively inject malicious functionality into the centralized model during training, leading to intentional misclassifications for specific adversary-chosen inputs. While previous research has demonstrated successful injections of persistent backdoors in FL, the persistence also poses a challenge, as their existence in the centralized model can prompt the central aggregation server to take preventive measures for penalizing the adversaries. Therefore, this article proposes a method that enables adversaries to effectively remove backdoors from the centralized model upon achieving their objectives or upon suspicion of possible detection. The proposed approach extends the concept of machine unlearning and presents strategies to preserve the performance of the centralized model and simultaneously prevent over-unlearning of information unrelated to backdoor patterns, making adversaries stealthy while removing backdoors. To the best of our knowledge, this is the first work exploring machine unlearning in FL to remove backdoors to the benefit of adversaries. Exhaustive evaluation considering various image classification scenarios demonstrates the efficacy of the proposed method for efficient backdoor removal from the centralized model, injected by state-of-the-art attacks across multiple configurations.},
  archive      = {J_TAI},
  author       = {Manaar Alam and Hithem Lamri and Michail Maniatakos},
  doi          = {10.1109/TAI.2024.3465441},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6683-6698},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Get rid of your trail: Remotely erasing backdoors in federated learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RATs-NAS: Redirection of adjacent trails on graph
convolutional networks for predictor-based neural architecture search.
<em>TAI</em>, <em>5</em>(12), 6672–6682. (<a
href="https://doi.org/10.1109/TAI.2024.3465433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manually designed convolutional neural networks (CNNs) architectures such as visual geometry group network (VGG), ResNet, DenseNet, and MobileNet have achieved high performance across various tasks, but design them is time-consuming and costly. Neural architecture search (NAS) automates the discovery of effective CNN architectures, reducing the need for experts. However, evaluating candidate architectures requires significant graphics processing unit (GPU) resources, leading to the use of predictor-based NAS, such as graph convolutional networks (GCN), which is the popular option to construct predictors. However, we discover that, even though the ability of GCN mimics the propagation of features of real architectures, the binary nature of the adjacency matrix limits its effectiveness. To address this, we propose redirection of adjacent trails (RATs), which adaptively learns trail weights within the adjacency matrix. Our RATs-GCN outperform other predictors by dynamically adjusting trail weights after each graph convolution layer. Additionally, the proposed divide search sampling (DSS) strategy, based on the observation of cell-based NAS that architectures with similar floating point operations (FLOPs) perform similarly, enhances search efficiency. Our RATs-NAS, which combine RATs-GCN and DSS, shows significant improvements over other predictor-based NAS methods on NASBench-101, NASBench-201, and NASBench-301.},
  archive      = {J_TAI},
  author       = {Yu-Ming Zhang and Jun-Wei Hsieh and Chun-Chieh Lee and Kuo-Chin Fan},
  doi          = {10.1109/TAI.2024.3465433},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6672-6682},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {RATs-NAS: Redirection of adjacent trails on graph convolutional networks for predictor-based neural architecture search},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolution of web API cooperation network via exploring
community structure and popularity. <em>TAI</em>, <em>5</em>(12),
6659–6671. (<a href="https://doi.org/10.1109/TAI.2024.3472614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing popularity of the Internet, Web applications have become increasingly essential in our daily lives. Web application programming interfaces (Web APIs) play a crucial role in facilitating interaction between applications. However, most Web service platforms are suffering from the imbalance of Web services now, many services of good quality but low popularity are difficult to be invoked even once and do not create direct connections with the users. Some graph-based Web service recommendation methods also often present a long-tailed distribution of recommended Web services due to limited Mashup–API invocation relationships. To relieve this problem and promote service recommendation, in this article, we propose a community structure and popularity-based approach by constructing an evolving cooperation network for Web APIs. We leverage the Louvain algorithm in community detection to assign community structure to each Web API and consider both the popularity and community structure in constructing the network. By optimizing the Barabάsi–Albert (BA) evolving network model, we demonstrate that our approach outperforms the BA, Bianconi–Barabάsi (BB), and popularity-similarity optimization (PSO) models in Web service clustering. Based on our proposed evolutionary network model for the evolutionary extension of API cooperation network and used for downstream Web service recommendation tasks, the experimental results also show that our recommended approach outperforms some other baseline models for Web service recommendation.},
  archive      = {J_TAI},
  author       = {Guosheng Kang and Yang Wang and Jianxun Liu and Buqing Cao and Yong Xiao and Yu Xu},
  doi          = {10.1109/TAI.2024.3472614},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6659-6671},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Evolution of web API cooperation network via exploring community structure and popularity},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Partial domain adaptation for building borehole lithology
model under weaker geological prior. <em>TAI</em>, <em>5</em>(12),
6645–6658. (<a href="https://doi.org/10.1109/TAI.2024.3476434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lithology identification plays a pivotal role in stratigraphic characterization and reservoir exploration. The promising field of intelligent logging lithology identification, which employs machine learning algorithms to infer lithology from logging curves, is gaining significant attention. However, models trained on labeled wells currently face challenges in accurately predicting the lithologies of new unlabeled wells due to significant discrepancies in data distribution among different wells caused by the complex sedimentary environment and variations in logging equipment. Additionally, there is no guarantee that newly drilled wells share the same lithology classes as previously explored ones. Therefore, our research aims to leverage source logging and lithology data along with target logging data to train a model capable of directly discerning the lithologies of target wells. The challenges are centered around the disparities in data distribution and the lack of prior knowledge regarding potential lithology classes in the target well. To tackle these concerns, we have made concerted efforts: 1) proposing a novel lithology identification framework, sample transferability weighting based partial domain adaptation (ST-PDA), to effectively address the practical scenario of encountering an unknown label space in target wells; 2) designing a sample transferability weighting module to assign higher weights to shared-class samples, thus effectively mitigating the negative transfer caused by unshared-class source samples; 3) developing a module, convolutional neural network with integrated channel attention mechanism (CG ${}^{2}$ CA), to serve as the backbone network for feature extraction; and 4) incorporating a target sample reconstruction module to enhance the feature representation and further facilitating positive transfer. Extensive experiments on 16 real-world wells demonstrated the strong performance of ST-PDA and highlighted the necessity of each component in the framework.},
  archive      = {J_TAI},
  author       = {Jing Li and Jichen Wang and Zerui Li and Yu Kang and Wenjun Lv},
  doi          = {10.1109/TAI.2024.3476434},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6645-6658},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Partial domain adaptation for building borehole lithology model under weaker geological prior},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simplified kernel-based cost-sensitive broad learning system
for imbalanced fault diagnosis. <em>TAI</em>, <em>5</em>(12), 6629–6644.
(<a href="https://doi.org/10.1109/TAI.2024.3478191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of intelligent manufacturing, tackling the classification challenges caused by imbalanced data is crucial. Although the broad learning system (BLS) has been recognized as an effective and efficient method, its performance wanes with imbalanced datasets. Therefore, this article proposes a novel approach named simplified kernel-based cost-sensitive broad learning system (SKCSBLS) to address these issues. Based on the framework of cost-sensitive broad learning system (CSBLS) that assigns distinctive adjustment costs for individual classes, SKCSBLS emphasizes the importance of the minority class while mitigating the impact of data imbalance. Additionally, considering the complexity introduced by noisy or overlapping data points, we incorporate kernel mapping into the CSBLS. This improvement not only improves the system&#39;s capability to handle overlapping classes of samples, but also improves the overall classification effectiveness. Our experimental results highlight the potential of SKCSBLS in overcoming the challenges inherent in unbalanced data, providing a robust solution for advanced fault diagnosis in intelligent systems.},
  archive      = {J_TAI},
  author       = {Kaixiang Yang and Wuxing Chen and Yifan Shi and Zhiwen Yu and C. L. Philip Chen},
  doi          = {10.1109/TAI.2024.3478191},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6629-6644},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Simplified kernel-based cost-sensitive broad learning system for imbalanced fault diagnosis},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AugDiff: Diffusion-based feature augmentation for multiple
instance learning in whole slide image. <em>TAI</em>, <em>5</em>(12),
6617–6628. (<a href="https://doi.org/10.1109/TAI.2024.3454591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple instance learning (MIL), a powerful strategy for weakly supervised learning, is able to perform various prediction tasks on gigapixel whole slide images (WSIs). However, the tens of thousands of patches in WSIs usually incur a vast computational burden for image augmentation, limiting the performance improvement in MIL. Currently, the feature augmentation-based MIL framework is a promising solution, while existing methods such as mixup often produce unrealistic features. To explore a more efficient and practical augmentation method, we introduce the diffusion model (DM) into MIL for the first time and propose a feature augmentation framework called AugDiff. The diverse generation capabilities of DM guarantee a various range of feature augmentations, while its iterative generation approach effectively preserves semantic integrity during these augmentations. We conduct extensive experiments over four distinct cancer datasets, two different feature extractors, and three prevalent MIL algorithms to evaluate the performance of AugDiff. Ablation study and visualization further verify the effectiveness. Moreover, we highlight AugDiff&#39;s higher quality augmented feature over image augmentation and its superiority over self-supervised learning. The generalization over external datasets indicates its broader applications. The code is open-sourced on https://github.com/szc19990412/AugDiff .},
  archive      = {J_TAI},
  author       = {Zhuchen Shao and Liuxi Dai and Yifeng Wang and Haoqian Wang and Yongbing Zhang},
  doi          = {10.1109/TAI.2024.3454591},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6617-6628},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {AugDiff: Diffusion-based feature augmentation for multiple instance learning in whole slide image},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatio-temporal graph-based generation and detection of
adversarial false data injection evasion attacks in smart grids.
<em>TAI</em>, <em>5</em>(12), 6601–6616. (<a
href="https://doi.org/10.1109/TAI.2024.3464511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart power grids are vulnerable to security threats due to their cyber-physical nature. Existing data-driven detectors aim to address simple traditional false data injection attacks (FDIAs). However, adversarial false data injection evasion attacks (FDIEAs) present a more serious threat as adversaries, with different levels of knowledge about the system, inject adversarial samples to circumvent the grid&#39;s attack detection system. The robustness of state-of-the-art graph-based detectors has not been investigated against sophisticated FDIEAs. Hence, this article answers three research questions. 1) What is the impact of utilizing spatio-temporal features to craft adversarial samples and how to select attack nodes? 2) How can adversaries generate surrogate spatio-temporal data when they lack knowledge about the system topology? 3) What are the required model characteristics for a robust detection against adversarial FDIEAs? To answer the questions, we examine the robustness of several detectors against five attack cases and conclude the following: 1) Attack generation with full knowledge using spatio-temporal features leads to 5%–26% and 2%–5% higher degradation in detection rate (DR) compared to traditional FDIAs and using temporal features, respectively, whereas centrality analysis-based attack node selection leads to 3%–11% higher degradation in DR compared to a random selection; 2) Stochastic geometry-based graph generation to create surrogate adversarial topologies and samples leads to 3%–13% higher degradation in DR compared to traditional FDIAs; and 3) Adopting an unsupervised spatio-temporal graph autoencoder (STGAE)-based detector enhances the DR by 5 $-$ 53% compared to benchmark detectors against FDIEAs.},
  archive      = {J_TAI},
  author       = {Abdulrahman Takiddin and Muhammad Ismail and Rachad Atat and Erchin Serpedin},
  doi          = {10.1109/TAI.2024.3464511},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6601-6616},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Spatio-temporal graph-based generation and detection of adversarial false data injection evasion attacks in smart grids},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal control of stochastic markovian jump systems with
wiener and poisson noises: Two reinforcement learning approaches.
<em>TAI</em>, <em>5</em>(12), 6591–6600. (<a
href="https://doi.org/10.1109/TAI.2024.3471729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the infinite horizon optimal control problem for stochastic Markovian jump systems with Wiener and Poisson noises. First, a new policy iteration algorithm is designed by using integral reinforcement learning approach and subsystems transformation technique, which obtains the optimal solution without solving stochastic coupled algebraic Riccati equation (SCARE) directly. Second, through the transformation and substitution of the SCARE and feedback gain matrix, a policy iteration algorithm is devised to determine the optimal control strategy. This algorithm leverages only state trajectory information to obtain the optimal solution, and is updated in an unfixed form. Additionally, the algorithm remains unaffected by variations in Poisson jump intensity. Finally, an numerical example is given to verify the effectiveness and convergence of the proposed algorithms.},
  archive      = {J_TAI},
  author       = {Zhiguo Yan and Tingkun Sun and Guolin Hu},
  doi          = {10.1109/TAI.2024.3471729},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6591-6600},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Optimal control of stochastic markovian jump systems with wiener and poisson noises: Two reinforcement learning approaches},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event-triggered fuzzy adaptive stabilization of parabolic
PDE–ODE systems. <em>TAI</em>, <em>5</em>(12), 6580–6590. (<a
href="https://doi.org/10.1109/TAI.2024.3443011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) offers fuzzy logic system (FLS) technique as one of the popular AI agents and decision-making tools for control systems to deal with uncertain nonlinearities. This article is concerned with the event-triggered intelligent fuzzy adaptive stabilization of a class of reaction-diffusion systems based on parabolic partial differential equations-ordinary differential equations (PDE–ODEs). The studied system type is an ODE subsystem with nonlinear and unknown control coefficients for controlling PDEs. The original PDE is transformed into a new target system through the infinite-dimensional transformation method, and a state feedback controller for the transformed system is designed with the adaptive backstepping method to stabilize the system. An event-triggered strategy based on a relative threshold is designed into the backstepping framework. When the triggering condition of the system is met, the control signal of the ODE subsystem is updated. The designed control scheme ensures that all closed-loop signals are bounded; in addition, the original system states can converge to zero. Finally, the simulation example demonstrates that the event-triggered control (ETC)-based stability control technology has a good control effect.},
  archive      = {J_TAI},
  author       = {Yuan-Xin Li and Bo Xu and Xing-Yu Zhang},
  doi          = {10.1109/TAI.2024.3443011},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6580-6590},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Event-triggered fuzzy adaptive stabilization of parabolic PDE–ODE systems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-model-free learning versus learning with external
rewards in information constrained environments. <em>TAI</em>,
<em>5</em>(12), 6566–6579. (<a
href="https://doi.org/10.1109/TAI.2024.3433614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we provide a model-free reinforcement learning (RL) framework that relies on internal reinforcement signals, called self-model-free RL, for learning agents that experience loss of the reinforcement signals in the form of packet drops and/or jamming attacks by malicious agents. The framework embeds a correcting mechanism in the form of a goal network to compensate for information loss and produce optimal and stabilizing policies. It also provides a trade-off scheme that reconstructs the reward using a goal network whenever the reinforcement signals are lost but utilizes true reinforcement signals when they are available. The stability of the equilibrium point is guaranteed despite fractional information loss in the reinforcement signals. Finally, simulation results validate the efficacy of the proposed work.},
  archive      = {J_TAI},
  author       = {Prachi Pratyusha Sahoo and Kyriakos G. Vamvoudakis},
  doi          = {10.1109/TAI.2024.3433614},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6566-6579},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Self-model-free learning versus learning with external rewards in information constrained environments},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cost-efficient feature selection for horizontal federated
learning. <em>TAI</em>, <em>5</em>(12), 6551–6565. (<a
href="https://doi.org/10.1109/TAI.2024.3436664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Horizontal federated learning (HFL) exhibits substantial similarities in feature space across distinct clients. However, not all features contribute significantly to the training of the global model. Moreover, the curse of dimensionality delays the training. Therefore, reducing irrelevant and redundant features from the feature space makes training faster and inexpensive. This work aims to identify the common feature subset from the clients in federated settings. We introduce a hybrid approach called Fed-MOFS, 1 1This manuscript is an extension of Banerjee et al. [1]. utilizing mutual information (MI) and clustering for local FS at each client. Unlike the Fed-FiS, which uses a scoring function for global feature ranking, Fed-MOFS employs multiobjective optimization to prioritize features based on their higher relevance and lower redundancy. This article compares the performance of Fed-MOFS 2 2We share our code, data, and supplementary copy through https://github.com/DevBhuyan/Horz-FL/blob/main/README.md. with conventional and federated FS methods. Moreover, we tested the scalability, stability, and efficacy of both Fed-FiS and Fed-MOFS across diverse datasets. We also assessed how FS influenced model convergence and explored its impact in scenarios with data heterogeneity. Our results show that Fed-MOFS enhances global model performance with a 50% reduction in feature space and is at least twice as fast as the FSHFL method. The computational complexity for both approaches is O( $d^{2}$ ), which is lower than the state of the art.},
  archive      = {J_TAI},
  author       = {Sourasekhar Banerjee and Devvjiit Bhuyan and Erik Elmroth and Monowar Bhuyan},
  doi          = {10.1109/TAI.2024.3436664},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6551-6565},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Cost-efficient feature selection for horizontal federated learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Higher-order directed community detection by a
multiobjective evolutionary framework. <em>TAI</em>, <em>5</em>(12),
6536–6550. (<a href="https://doi.org/10.1109/TAI.2024.3436659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Higher-order community detection in real-life networks has recently gained significant attention, because motif-based communities reflect not only higher-order mesoscale structures but also functional characteristics. However, motif-based communities detected by existing methods for directed networks often disregard edge directionality (nonreciprocal directional arcs), so they typically fail to comprehensively reveal intrinsic characteristics of higher-order topology and information flow. To address this issue, first, we model higher-order directed community detection as a biobjective optimization problem, aiming to provide high-quality and diverse compromise partitions that capture both characteristics. Second, we introduce a multiobjective genetic algorithm based on motif density and information flow (MOGA-MI) to approximate the Pareto optimal higher-order directed community partitions. On the one hand, an arc-and-motif neighbor-based genetic generator (AMN-GA) is developed to generate high-quality and diverse offspring individuals; on the other hand, a higher-order directed neighbor community modification (HD-NCM) operation is designed to further improve generated partitions by modifying easily confused nodes into more appropriate motif-neighbor communities. Finally, experimental results demonstrate that the proposed MOGA-MI outperforms state-of-the-art algorithms in terms of higher-order topology and information flow indicators while providing more diverse community information.},
  archive      = {J_TAI},
  author       = {Jing Xiao and Jing Cao and Xiao-Ke Xu},
  doi          = {10.1109/TAI.2024.3436659},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6536-6550},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Higher-order directed community detection by a multiobjective evolutionary framework},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RADiff: Controllable diffusion models for radio astronomical
maps generation. <em>TAI</em>, <em>5</em>(12), 6524–6535. (<a
href="https://doi.org/10.1109/TAI.2024.3436538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Along with the nearing completion of the square kilometer array (SKA), comes an increasing demand for accurate and reliable automated solutions to extract valuable information from the vast amount of data it will allow acquiring. Automated source finding is a particularly important task in this context, as it enables the detection and classification of astronomical objects. Deep-learning-based object detection and semantic segmentation models have proven to be suitable for this purpose. However, training such deep networks requires a high volume of labeled data, which is not trivial to obtain in the context of radio astronomy. Since data needs to be manually labeled by experts, this process is not scalable to large dataset sizes, limiting the possibilities of leveraging deep networks to address several tasks. In this work, we propose RADiff, a generative approach based on conditional diffusion models trained over an annotated radio dataset to generate synthetic images, containing radio sources of different morphologies, to augment existing datasets and reduce the problems caused by class imbalances. We also show that it is possible to generate fully synthetic image-annotation pairs to automatically augment any annotated dataset. We evaluate the effectiveness of this approach by training a semantic segmentation model on a real dataset augmented in two ways: 1) using synthetic images obtained from real masks; and 2) generating images from synthetic semantic masks. Finally, we also show how the model can be applied to populate background noise maps for simulating radio maps for data challenges.},
  archive      = {J_TAI},
  author       = {Renato Sortino and Thomas Cecconello and Andrea De Marco and Giuseppe Fiameni and Andrea Pilzer and Daniel Magro and Andrew M. Hopkins and Simone Riggi and Eva Sciacca and Adriano Ingallinera and Cristobal Bordiu and Filomena Bufano and Concetto Spampinato},
  doi          = {10.1109/TAI.2024.3436538},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6524-6535},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {RADiff: Controllable diffusion models for radio astronomical maps generation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cooperative advantage actor–critic reinforcement learning
for multiagent pursuit-evasion games on communication graphs.
<em>TAI</em>, <em>5</em>(12), 6509–6523. (<a
href="https://doi.org/10.1109/TAI.2024.3432511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the distributed optimal strategy problem in multiagent pursuit-evasion (MPE) games, striving for Nash equilibrium through the optimization of individual benefit matrices based on observations. To this end, a novel collaborative control scheme for MPE games using communication graphs is proposed. This scheme employs cooperative advantage actor–critic (A2C) reinforcement learning to facilitate collaborative capture by pursuers in a distributed manner while maintaining bounded system signals. The strategy orchestrates the actions of pursuers through adaptive neural network learning, ensuring proximity-based collaboration for effective captures. Meanwhile, evaders aim to evade collectively by converging toward each other. Through extensive simulations involving five pursuers and two evaders, the efficacy of the proposed approach is demonstrated, and pursuers seamlessly organize into pursuit units and capture evaders, validating the collaborative capture objective. This article represents a promising step toward effective and cooperative control strategies in MPE game scenarios.},
  archive      = {J_TAI},
  author       = {Yizhen Meng and Chun Liu and Qiang Wang and Longyu Tan},
  doi          = {10.1109/TAI.2024.3432511},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6509-6523},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Cooperative advantage Actor–Critic reinforcement learning for multiagent pursuit-evasion games on communication graphs},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-stage representation refinement based on convex
combination for 3-d human poses estimation. <em>TAI</em>,
<em>5</em>(12), 6500–6508. (<a
href="https://doi.org/10.1109/TAI.2024.3432028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the human pose estimation task, on the one hand, 3-D pose always has difficulty in dividing different 2-D poses if the view is limited; on the other hand, it is hard to reduce the lifting ambiguity because of the lack of depth information, it is an important and challenging problem. Therefore, two-stage representation refinement based on the convex combination for 3-D human pose estimation is proposed, in which the two-stage method includes a dense-spatial-temporal convolutional network and a local-to-refine network. The former is applied to determine the features between each video frame; the latter is used to get the different scales of pose details. It aims to address the difficulty of estimating 3-D human pose from 2-D image sequences. In such a way, it can better use the relations between every frame in the sequence of the pose video to produce more accurate results. Finally, we combine the above network with a block called convex combination to help refine the 3-D pose location. We test the proposed approach on both Human3.6m and MPII datasets. The result confirms that our method can achieve better performance than improved CNN supervision, a simple yet effective baseline, and coarse-to-fine volumetric prediction. Besides, a robustness test experiment is carried out for the proposed method while the input is interrupted. The result verifies that our method shows better robustness.},
  archive      = {J_TAI},
  author       = {Luefeng Chen and Wei Cao and Biao Zheng and Min Wu and Witold Pedrycz and Kaoru Hirota},
  doi          = {10.1109/TAI.2024.3432028},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6500-6508},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Two-stage representation refinement based on convex combination for 3-D human poses estimation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FIMKD: Feature-implicit mapping knowledge distillation for
RGB-d indoor scene semantic segmentation. <em>TAI</em>, <em>5</em>(12),
6488–6499. (<a href="https://doi.org/10.1109/TAI.2024.3452052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth images are often used to improve the geometric understanding of scenes owing to their intuitive distance properties. Although there have been significant advancements in semantic segmentation tasks using red–green–blue-depth (RGB-D) images, the complexity of existing methods remains high. Furthermore, the requirement for high-quality depth images increases the model inference time, which limits the practicality of these methods. To address this issue, we propose a feature-implicit mapping knowledge distillation (FIMKD) method and a cross-modal knowledge distillation (KD) architecture to leverage deep modal information for training and reduce the model dependence on this information during inference. The approach comprises two networks: FIMKD-T, a teacher network that uses RGB-D data, and FIMKD-S, a student network that uses only RGB data. FIMKD-T extracts high-frequency information using the depth modality and compensates for the loss of RGB details due to a reduction in resolution during feature extraction by the high-frequency feature enhancement module, thereby enhancing the geometric perception of semantic features. In contrast, the FIMKD-S network does not employ deep learning techniques; instead, it uses a nonlearning approach to extract high-frequency information. To enable the FIMKD-S network to learn deep features, we propose a feature-implicit mapping KD for feature distillation. This mapping technique maps the features in channel and space to a low-dimensional hidden layer, which helps to avoid inefficient single-pattern student learning. We evaluated the proposed FIMKD-S* (FIMKD-S with KD) on the NYUv2 and SUN-RGBD datasets. The results demonstrate that both FIMKD-T and FIMKD-S* achieve state-of-the-art performance. Furthermore, FIMKD-S* provides the best performance balance.},
  archive      = {J_TAI},
  author       = {Wujie Zhou and Yuxiang Xiao and Yuanyuan Liu and Qiuping Jiang},
  doi          = {10.1109/TAI.2024.3452052},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6488-6499},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {FIMKD: Feature-implicit mapping knowledge distillation for RGB-D indoor scene semantic segmentation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep imbalanced learning for multimodal emotion recognition
in conversations. <em>TAI</em>, <em>5</em>(12), 6472–6487. (<a
href="https://doi.org/10.1109/TAI.2024.3445325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main task of multimodal emotion recognition in conversations (MERC) is to identify the emotions in modalities, e.g., text, audio, image, and video, which is a significant development direction for realizing machine intelligence. However, many data in MERC naturally exhibit an imbalanced distribution of emotion categories, and researchers ignore the negative impact of imbalanced data on emotion recognition. To tackle this problem, we systematically analyze it from three aspects: data augmentation, loss sensitivity, and sampling strategy, and propose the class boundary enhanced representation learning (CBERL) model. Concretely, we first design a multimodal generative adversarial network to address the imbalanced distribution of emotion categories in raw data. Second, a deep joint variational autoencoder is proposed to fuse complementary semantic information across modalities and obtain discriminative feature representations. Finally, we implement a multitask graph neural network with mask reconstruction and classification optimization to solve the problem of overfitting and underfitting in class boundary learning and achieve cross-modal emotion recognition. We have conducted extensive experiments on the interactive emotional dyadic motion capture (IEMOCAP) and multimodal emotion lines dataset (MELD) benchmark datasets, and the results show that CBERL has achieved a certain performance improvement in the effectiveness of emotion recognition. Especially on the minority class “fear” and “disgust” emotion labels, our model improves the accuracy and F1 value by 10% to 20%. Our code is publicly available at https://github.com/yuntaoshou/CBERL .},
  archive      = {J_TAI},
  author       = {Tao Meng and Yuntao Shou and Wei Ai and Nan Yin and Keqin Li},
  doi          = {10.1109/TAI.2024.3445325},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6472-6487},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep imbalanced learning for multimodal emotion recognition in conversations},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LSTM-based model compression for CAN security in intelligent
vehicles. <em>TAI</em>, <em>5</em>(12), 6457–6471. (<a
href="https://doi.org/10.1109/TAI.2024.3438110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid deployment and low-cost inference of controller area network (CAN) bus anomaly detection models on intelligent vehicles can drive the development of the Green Internet of Vehicles. Anomaly detection on intelligent vehicles often utilizes recurrent neural network models, but computational resources for these models are limited on small platforms. Model compression is essential to ensure CAN bus security with restricted computing resources while improving model computation efficiency. However, the existence of shared cyclic units significantly constrains the compression of recurrent neural networks. In this study, we propose a structured pruning method for long short-term memory (LSTM) based on the contribution values of shared vectors. By analyzing the contribution value of each dimension of shared vectors, the weight matrix of the model is structurally pruned, and the output value of the LSTM layer is supplemented to maintain the information integrity between adjacent network layers. We further propose an approximate matrix multiplication calculation module that runs in the whole process of model calculation and is deployed in parallel with the pruning module. Evaluated on a realistic public CAN bus dataset, our method effectively achieves highly structured pruning, improves model computing efficiency, and maintains performance stability compared to other compression methods.},
  archive      = {J_TAI},
  author       = {Yuan Feng and Yingxu Lai and Ye Chen and Zhaoyi Zhang and Jingwen Wei},
  doi          = {10.1109/TAI.2024.3438110},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6457-6471},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {LSTM-based model compression for CAN security in intelligent vehicles},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive radiogenomic feature characterization of
19/20 co-gain in glioblastoma. <em>TAI</em>, <em>5</em>(12), 6442–6456.
(<a href="https://doi.org/10.1109/TAI.2024.3440219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prognosis and treatment planning of glioblastoma multiforme (GBM) involves a holistic analysis of imaging, clinical, and molecular data. The correlation of imaging and molecular features has garnered much interest due to its potential to reduce the number of invasive procedures on a patient and resource utilization of the overall prognostic and treatment planning process. This article detects and characterizes the impact of tumor biomarkers (such as shape, texture, location, and the tissue surrounding the tumor) in detecting a prognostic mutation – the concurrent gain of 19 and 20 chromosomes, and proposes two novel ideas for this analysis. First, to address the challenges associated with the limited, diverse, and complex nature of medical data, this article proposes a novel generative model – the realistic radiogenomic design using disentanglement in generative adversarial networks (R2D2-GAN), designed to recreate highly subtle, unapparent manifestations of mutations in magnetic resonance imaging. It generates high-resolution, diverse data that captures the discriminatory visual features of the molecular markers while tackling the high diversity, unbalanced, and limited GBM data with rare mutations correlating with patient survival such as 19/20 co-gain. Second, this study proposes a quantitative metric called the synthetic image fidelity (SIF) score to evaluate the performance of GANs in learning visually unapparent prognostic features through the use of gradient-based model explanations. Results are compared with current methods.},
  archive      = {J_TAI},
  author       = {Padmaja Jonnalagedda and Brent Weinberg and Taejin L. Min and Shiv Bhanu and Bir Bhanu},
  doi          = {10.1109/TAI.2024.3440219},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6442-6456},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A comprehensive radiogenomic feature characterization of 19/20 co-gain in glioblastoma},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated <span class="math inline"><em>c</em></span>-means
and fuzzy <span class="math inline"><em>c</em></span>-means clustering
algorithms for horizontally and vertically partitioned data.
<em>TAI</em>, <em>5</em>(12), 6426–6441. (<a
href="https://doi.org/10.1109/TAI.2024.3426408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated clustering lets multiple data owners collaborate in discovering patterns from distributed data without violating privacy requirements. The federated versions of traditional clustering algorithms proposed so far are, however, “lossy” since they fail to identify exactly the same clusters as the original versions executed on the merged data stored in a centralized server, as would happen if no privacy constraint occurred. In this article, we propose federated procedures for losslessly executing the C-means (CM) and the fuzzy C-means (FCM) algorithms in both horizontally and vertically partitioned data scenarios, while preserving data privacy. We formally prove that the proposed federated procedures identify the same clusters determined by applying the algorithms to the union of all local data. Further, we present an extensive experimental analysis for characterizing the behavior of the proposed approach in a typical federated learning scenario, that is, as the fraction of participants in the federation changes. We focus on the federated FCM and the horizontally partitioned data, which is the most interesting scenario. We show that the proposed procedure is effective and is able to achieve competitive performance with respect to two recently proposed versions of federated FCM for horizontally partitioned data.},
  archive      = {J_TAI},
  author       = {José Luis Corcuera Bárcena and Francesco Marcelloni and Alessandro Renda and Alessio Bechini and Pietro Ducange},
  doi          = {10.1109/TAI.2024.3426408},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6426-6441},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Federated $c$-means and fuzzy $c$-means clustering algorithms for horizontally and vertically partitioned data},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generative representation learning in recurrent neural
networks for causal timeseries forecasting. <em>TAI</em>,
<em>5</em>(12), 6412–6425. (<a
href="https://doi.org/10.1109/TAI.2024.3446465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feed-forward deep neural networks (DNNs) are the state of the art in timeseries forecasting. A particularly significant scenario is the causal one: when an arbitrary subset of variables of a given multivariate timeseries is specified as forecasting target, with the remaining ones (exogenous variables) causing the target at each time instance. Then, the goal is to predict a temporal window of future target values, given a window of historical exogenous values. To this end, this article proposes a novel deep recurrent neural architecture, called generative-regressing recurrent neural network (GRRNN), which surpasses competing ones in causal forecasting evaluation metrics, by smartly combining generative learning and regression. During training, the generative module learns to synthesize historical target timeseries from historical exogenous inputs via conditional adversarial learning, thus internally encoding the input timeseries into semantically meaningful features. During a forward pass, these features are passed over as input to the regression module, which outputs the actual future target forecasts in a sequence-to-sequence fashion. Thus, the task of timeseries generation is synergistically combined with the task of timeseries forecasting, under an end-to-end multitask training setting. Methodologically, GRRNN contributes a novel augmentation of pure supervised learning, tailored to causal timeseries forecasting, which essentially forces the generative module to transform the historical exogenous timeseries to a more appropriate representation, before feeding it as input to the actual forecasting regressor. Extensive experimental evaluation on relevant public datasets obtained from disparate fields, ranging from air pollution data to sentiment analysis of social media posts, confirms that GRRNN achieves top performance in multistep long-term forecasting.},
  archive      = {J_TAI},
  author       = {Georgios Chatziparaskevas and Ioannis Mademlis and Ioannis Pitas},
  doi          = {10.1109/TAI.2024.3446465},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6412-6425},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Generative representation learning in recurrent neural networks for causal timeseries forecasting},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simultaneous learning and planning within sensing range: An
approach for local path planning. <em>TAI</em>, <em>5</em>(12),
6399–6411. (<a href="https://doi.org/10.1109/TAI.2024.3438094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes an approach for local path planning. Unlike traditional approaches, the proposed local path planner simultaneously learns and plans within the sensing range (SLPA-SR) during local path planning. SLPA-SR is the synergy between the local path planner, the dynamic window approach (DWA), the obstacle avoidance by velocity obstacle (VO) approach, and the proposed next-best reward learning (NBR) algorithms. In the proposed SLPA-SR, the DWA acts as an actuator and helps to balance exploration and exploitation in the proposed NBR. In the proposed NBR, dimensions of state and action do not need to be defined a priori ; rather, dimensions of state and action change dynamically. The proposed SLPA-SR is simulated and experimentally validated on the TurtleBot3 Waffle Pi. The performance of the proposed SLPA-SR is tested in several typical environments, both in simulation and hardware experiments. The proposed SLPA-SR outperforms the contender algorithms (i.e., DWA, DWA-RL, improved time elastic band, predictive artificial potential field, and artificial potential field) by a significant margin in terms of run-time, linear velocity, angular velocity, success rate, average trajectory length, and average velocity. The efficacy of the proposed NBR is established by analyzing the percentage of exploitation, average reward, and state-action pair count.},
  archive      = {J_TAI},
  author       = {Lokesh Kumar and Arup Kumar Sadhu and Ranjan Dasgupta},
  doi          = {10.1109/TAI.2024.3438094},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6399-6411},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Simultaneous learning and planning within sensing range: An approach for local path planning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal fusion induced attention network for industrial
VOCs detection. <em>TAI</em>, <em>5</em>(12), 6385–6398. (<a
href="https://doi.org/10.1109/TAI.2024.3436037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial volatile organic compounds (VOCs) emissions and leakage have caused serious problems to the environment and public safety. Traditional VOCs monitoring systems require professionals to carry gas sensors into the emission area to collect VOCs, which might cause secondary hazards. VOCs infrared (IR) imaging visual inspection technology is a convenient and low-cost method. However, current visual detection methods with VOCs IR imaging are limited due to blurred imaging and indeterminate gas shapes. Moreover, major works pay attention to only IR modality for VOCs emissions detection, which would neglect semantic expressions of VOCs. To this end, we propose a dual-stream fusion detection framework to deal with visible and IR features of VOCs. Additionally, a multimodal fusion induced attention (MFIA) module is designed to realize feature fusion across modalities. Specifically, MFIA uses the spatial attention fusion module (SAFM) to mine association among modalities in terms of spatial location and generates fused features by spatial location weighting. Then, the modality adapter (MA) and induced attention module (IAM) are proposed to weight latent VOCs regions in IR features, which alleviates the problem of noise interference and degradation of VOCs characterization caused by fusion. Finally, comprehensive experiments are carried out on the challenging VOCs dataset, and the mAP@0.5 and F1-score of the proposed model are 0.527 and 0.601, which outperforms the state-of-the-art methods by 3.3% and 3.4%, respectively.},
  archive      = {J_TAI},
  author       = {Yu Kang and Kehao Shi and Jifang Tan and Yang Cao and Lijun Zhao and Zhenyi Xu},
  doi          = {10.1109/TAI.2024.3436037},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6385-6398},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multimodal fusion induced attention network for industrial VOCs detection},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Differentially private and heterogeneity-robust federated
learning with theoretical guarantee. <em>TAI</em>, <em>5</em>(12),
6369–6384. (<a href="https://doi.org/10.1109/TAI.2024.3446759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a popular distributed paradigm where enormous clients collaboratively train a machine learning (ML) model under the orchestration of a central server without knowing the clients’ private raw data. The development of effective FL algorithms faces multiple practical challenges including data heterogeneity and clients’ privacy protection. Despite that numerous attempts have been made to deal with data heterogeneity or rigorous privacy protection, none have effectively tackled both issues simultaneously. In this article, we propose a differentially private and heterogeneity-robust FL algorithm, named DP-FedCVR to mitigate the data heterogeneity by following the client-variance-reduction strategy. Besides, it adopts a sophisticated differential privacy (DP) mechanism where the privacy-amplified strategy is applied, to achieve a rigorous privacy protection guarantee. We show that the proposed DP-FedCVR algorithm maintains its heterogeneity-robustness though DP noises are incorporated, while achieving a sublinear convergence rate for a nonconvex FL problem. Numerical experiments based on image classification tasks are presented to demonstrate that DP-FedCVR provides superior performance over the benchmark algorithms in the presence of data heterogeneity and various DP privacy budgets.},
  archive      = {J_TAI},
  author       = {Xiuhua Wang and Shuai Wang and Yiwei Li and Fengrui Fan and Shikang Li and Xiaodong Lin},
  doi          = {10.1109/TAI.2024.3446759},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6369-6384},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Differentially private and heterogeneity-robust federated learning with theoretical guarantee},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MSCS: Multiscale consistency supervision with
CNN-transformer collaboration for semisupervised histopathology image
semantic segmentation. <em>TAI</em>, <em>5</em>(12), 6356–6368. (<a
href="https://doi.org/10.1109/TAI.2024.3443794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a multiscale consistency supervision (MSCS) strategy that combines a semisupervised learning approach with multimagnification learning to ease the labeling load and improve the prediction accuracy of histopathology image semantic segmentation. The MSCS strategy incorporates multiview complementary information into the semisupervised learning process, where this information includes that obtained from multiscale views (i.e., cells and tissues) and encoders with different decision perspectives. The strategy is implemented through the collaboration between convolutional neural network (CNN) and Transformer encoders, where the former encoder excels at capturing local spatial relationships in the input images and the latter encoder excels at capturing global relationships. In the proposed approach, the learning process is performed using two asymmetric multiscale fusion networks, designated as MSUnetFusion and MSUSegFormer. MSUnetFusion learns the cell-level features using CNN and the tissue-level features using Transformer. In contrast, MSUSegFormer learns both features using only Transformer. MSCS enforces prediction consistency between the two networks to enhance the prediction performance for unlabeled training data. The experimental results show that MSCS outperforms both supervised and semisupervised methods for the segmentation of hepatocellular carcinoma (HCC) and colorectal cancer (CRC) datasets, even when only limited labeled data are available. Overall, MSCS appears to provide a promising solution for histopathology image semantic segmentation.},
  archive      = {J_TAI},
  author       = {Min-En Hsieh and Chien-Yu Chiou and Hung-Wen Tsai and Yu-Cheng Chang and Pau-Choo Chung},
  doi          = {10.1109/TAI.2024.3443794},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6356-6368},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {MSCS: Multiscale consistency supervision with CNN-transformer collaboration for semisupervised histopathology image semantic segmentation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning empirical inherited intelligent MPC for switched
systems with network security communication. <em>TAI</em>,
<em>5</em>(12), 6342–6355. (<a
href="https://doi.org/10.1109/TAI.2024.3486276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies learning empirical inherited intelligent model predictive control (LEII-MPC) for switched systems. For complex environments and systems, an intelligent control method design with learning ability is necessary and meaningful. First, a switching law that coordinates the iterative learning control action is devised according to the average dwell time approach. Second, an intelligent MPC mechanism with the iteration learning experience is designed to optimize the control action. With the designed LEII-MPC, sufficient conditions for the switched systems stability equipped with the event-triggering schemes (ETSs) in both the time domain and the iterative domain are presented. The ETS in the iterative domain is to solve unnecessary iterative updates. The ETS in the time domain is to deal with potential denial of service (DoS) attacks, which includes two parts: 1) for detection, an attack-dependent event-triggering method is presented to determine attack sequence and reduce lost packets; and 2) for compensation, a buffer is used to ensure system performance during the attack period. Last, a numerical example shows the effectiveness of the proposed method.},
  archive      = {J_TAI},
  author       = {Yiwen Qi and Yiwen Tang and Wenke Yu},
  doi          = {10.1109/TAI.2024.3486276},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6342-6355},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Learning empirical inherited intelligent MPC for switched systems with network security communication},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantum face recognition with multigate quantum
convolutional neural network. <em>TAI</em>, <em>5</em>(12), 6330–6341.
(<a href="https://doi.org/10.1109/TAI.2024.3419077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decade, quantum computing has showcased its unique mechanism across diverse fields, highlighting significant potential for data-driven applications requiring substantial computational resources. Within this landscape, quantum machine learning emerges as a promising frontier, poised to harness the unique advantages of quantum computing for machine learning tasks. Nonetheless, the current generation of quantum hardware, typified by noisy intermediate-scale quantum (NISQ) devices, grapples with severe resource constraints, particularly in terms of qubit availability. While quantum computing offers tantalizing capabilities such as superposition and entanglement, which can be strategically leveraged to optimize the performance of quantum neural networks, the challenge remains in mitigating the resource limitations while upholding high recognition accuracy. To address this imperative, we introduce a pioneering face recognition method christened the multigate quantum convolutional neural network (MG-QCNN). This innovation is engineered to surmount the resource bottleneck endemic to NISQ devices while preserving exceptional recognition accuracy. Our empirical investigations conducted on benchmark datasets, including the Yale face dataset and the ORL face database, illuminate the remarkable potential of this approach. Specifically, our proposed variational quantum circuit architecture consistently achieves an impressive average accuracy of 96%, which is better than the 95% of the classic CNN. Our model underscores the efficacy of quantum convolution operations in the extraction of feature maps, exhibiting a transformative stride toward unlocking the full potential of quantum-enhanced face recognition, and compared with other quantum models, our method has more advantages in accuracy and efficiency.},
  archive      = {J_TAI},
  author       = {Yijie Zhu and Ahmed Bouridane and M Emre Celebi and Debanjan Konar and Plamen Angelov and Qiang Ni and Richard Jiang},
  doi          = {10.1109/TAI.2024.3419077},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6330-6341},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Quantum face recognition with multigate quantum convolutional neural network},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal output feedback tracking control for takagi–sugeno
fuzzy systems. <em>TAI</em>, <em>5</em>(12), 6320–6329. (<a
href="https://doi.org/10.1109/TAI.2024.3443004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, an optimal output feedback tracking control approach with a Q-learning algorithm is presented for Takagi–Sugeno (T–S) fuzzy discrete-time systems with immeasurable states. First, a state reconstruction method based on the measured output data and input data is applied to handle immeasurable states problem. Then, the optimal output feedback tracking control input policy is designed and boiled down to the algebraic Riccati equations (AREs). To obtain the solution to AREs, a Q-learning value iteration (VI) algorithm is formulated, which directly learns each state-action value. Consequently, the sufficient conditions for the convergence of the proposed optimal algorithm are derived by constructing an approximate Q-function. It is proved that the presented optimal output feedback tracking control method can guarantee the controlled systems to be stable and output track the given reference signal. Finally, we take the truck-trailer system as the simulation example, the simulation results validate feasibility of the presented optimal control methodology.},
  archive      = {J_TAI},
  author       = {Wenting Song and Shaocheng Tong},
  doi          = {10.1109/TAI.2024.3443004},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6320-6329},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Optimal output feedback tracking control for Takagi–Sugeno fuzzy systems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A composite decomposition method for large-scale global
optimization. <em>TAI</em>, <em>5</em>(12), 6304–6319. (<a
href="https://doi.org/10.1109/TAI.2024.3373391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperative coevolution (CC) algorithms, based on the divide-and-conquer strategy, have emerged as the predominant approach to solving large-scale global optimization (LSGO) problems. The efficiency and accuracy of the grouping stage significantly impact the performance of the optimization process. While the general separability grouping (GSG) method has overcome the limitation of previous differential grouping (DG) methods by enabling the decomposition of nonadditively separable functions, it suffers from high computational complexity. To address this challenge, this article proposes a composite separability grouping (CSG) method, seamlessly integrating DG and GSG into a problem decomposition framework to utilize the strengths of both approaches. CSG introduces a step-by-step decomposition framework that accurately decomposes various problem types using fewer computational resources. By sequentially identifying additively, multiplicatively and generally separable variables, CSG progressively groups nonseparable variables by recursively considering the interactions between each nonseparable variable and the formed nonseparable groups. Furthermore, to enhance the efficiency and accuracy of CSG, we introduce two innovative methods: a multiplicatively separable variable detection method and a nonseparable variable grouping method. These two methods are designed to effectively detect multiplicatively separable variables and efficiently group nonseparable variables, respectively. Extensive experimental results demonstrate that CSG achieves more accurate variable grouping with lower computational complexity compared to GSG and state-of-the-art DG-series designs.},
  archive      = {J_TAI},
  author       = {Maojiang Tian and Minyang Chen and Wei Du and Yang Tang and Yaochu Jin and Gary G. Yen},
  doi          = {10.1109/TAI.2024.3373391},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6304-6319},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A composite decomposition method for large-scale global optimization},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A game-theoretic approach to containing artificial general
intelligence: Insights from highly autonomous aggressive malware.
<em>TAI</em>, <em>5</em>(12), 6290–6303. (<a
href="https://doi.org/10.1109/TAI.2024.3394392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial general intelligence (AGI) promises transformative societal changes but poses safety and containment challenges. Large language models such as ChatGPT have intensified public expectations and apprehensions regarding AGI capabilities and risks. Existing research underestimated replicating human intelligence and lacks effective containment strategies scaled for AGI&#39;s complexity. We developed a cybersecurity-inspired framework to reconceptualize AGI containment as securing critical infrastructure indispensable for its operation. We applied game theory to model the strategic interplay between AGI and humans, drawing parallels with highly autonomous malware, emphasizing infrastructural dependencies and human countermeasures. We introduced offensive/defensive containment strategies and an AGI Kill Chain model profiling escalating AGI threats. Our game-theoretic approach examined complex AGI-human interactions revealing insights for adaptive oversight mechanisms. Game simulations demonstrated AGI carefully manages resources and autonomy balancing benefits against risks, necessitating strategic human responses. Our findings provided detailed containment tactics, emphasizing flexibility to address AGI&#39;s dynamic evolution. We proposed comprehensive, multidisciplinary containment strategies, effective governance evaluating long-term efficacy, and emphasize ongoing innovation for aligning AGI progression with utility and security.},
  archive      = {J_TAI},
  author       = {Timothy R. McIntosh and Teo Susnjak and Tong Liu and Paul Watters and Alex Ng and Malka N. Halgamuge},
  doi          = {10.1109/TAI.2024.3394392},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6290-6303},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A game-theoretic approach to containing artificial general intelligence: Insights from highly autonomous aggressive malware},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A brain-inspired theory of collective mind model for
efficient social cooperation. <em>TAI</em>, <em>5</em>(12), 6280–6289.
(<a href="https://doi.org/10.1109/TAI.2024.3396124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social intelligence manifests the capability, often referred to as the theory of mind (ToM), to discern others’ behavioral intentions, beliefs, and other mental states. ToM is crucial in multiagent and human–machine interaction contexts, where each participant needs to grasp the mental states of others to respond, interact, and collaborate more effectively. Recent studies show that while the ToM model can infer beliefs, intentions, and predict future observations and actions, its application in complex tasks is significantly constrained. The challenges arise when the number of agents increases, the environment becomes more complex, and interacting with the environment and predicting the mental state of each other becomes difficult and time consuming. To overcome such limits, we take inspiration from the theory of collective mind (ToCM) mechanism, predicting observations of all other agents into a unified but plural representation and discerning how our own actions affect this mental state representation. Based on this foundation, we construct an imaginative space to simulate the multiagent interaction process, thus improving the efficiency of cooperation among multiple agents in complex decision-making environments. In various cooperative tasks with different numbers of agents, the experimental results highlight the superior cooperative efficiency and performance of our approach compared to the multiagent reinforcement learning (MARL) baselines. We achieve consistent boost on SNN- and ANN-based decision networks and demonstrate that ToCM&#39;s inferences about others’ mental states can be transferred to new tasks for quickly and flexible adaptation.},
  archive      = {J_TAI},
  author       = {Zhuoya Zhao and Feifei Zhao and Shiwen Wang and Yinqian Sun and Yi Zeng},
  doi          = {10.1109/TAI.2024.3396124},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6280-6289},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A brain-inspired theory of collective mind model for efficient social cooperation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge-guided evolutionary optimization for large-scale
air defense resource allocation. <em>TAI</em>, <em>5</em>(12),
6267–6279. (<a href="https://doi.org/10.1109/TAI.2024.3375263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the high-stakes domain of air defense, resource allocation is a challenging optimization problem that involves many constraints and requires fast solutions. Previous work mainly addressed the single-intercept sensor-weapon-target assignment problem, which assigns missiles and radars to targets without optimizing the launch time. However, this problem can become large-scale and difficult to solve when saturation attack tactics are employed. Therefore, we propose a more realistic model that allows multiple missiles to intercept the same target and considers the launch time as a decision variable. We also incorporate radar relay guidance to maximize the interception probability and minimize the missile cost. To solve this model efficiently, we present a novel evolutionary algorithm with linear population size reduction (LPSR) and knowledge-guided strategies, called LKEA. Specifically, we design a heuristic method to quickly find the optimal launch time for each missile and a knowledge-guided strategy to use the radar channel collision information to guide the evolution. Experimental results show that LKEA is competitive in solving small/medium/large air defense resource allocation problems (ADRAPs).},
  archive      = {J_TAI},
  author       = {Wenhua Li and Rui Wang and Yong Heng and Tao Zhang and Ling Wang},
  doi          = {10.1109/TAI.2024.3375263},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6267-6279},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Knowledge-guided evolutionary optimization for large-scale air defense resource allocation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiscale and multilayer contrastive learning for domain
generalization. <em>TAI</em>, <em>5</em>(12), 6253–6266. (<a
href="https://doi.org/10.1109/TAI.2024.3377173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the past decade, deep neural networks have led to fast-paced progress and significant achievements in computer vision problems, for both academia and industry. Yet despite their success, state-of-the-art image classification approaches fail to generalize well in previously unseen visual contexts, as required by many real-world applications. In this article, we focus on this domain generalization (DG) problem and argue that the generalization ability of deep convolutional neural networks (CNNs) can be improved by taking advantage of multilayer and multiscaled representations of the network. We introduce a framework that aims at improving DG of image classifiers by combining both low- and high-level features at multiple scales, enabling the network to implicitly disentangle representations in its latent space and learn domain-invariant attributes of the depicted objects. Additionally, to further facilitate robust representation learning, we propose a novel objective function, inspired by contrastive learning, which aims at constraining the extracted representations to remain invariant under distribution shifts. We demonstrate the effectiveness of our method by evaluating on the DG datasets of PACS, VLCS, Office–Home, and NICO. Through extensive experimentation, we show that our model is able to surpass the performance of previous DG methods and consistently produce competitive and state-of-the-art results in all datasets.},
  archive      = {J_TAI},
  author       = {Aristotelis Ballas and Christos Diou},
  doi          = {10.1109/TAI.2024.3377173},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6253-6266},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multiscale and multilayer contrastive learning for domain generalization},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An approach for privacy-aware mobile app package
recommendation. <em>TAI</em>, <em>5</em>(12), 6240–6252. (<a
href="https://doi.org/10.1109/TAI.2024.3443028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the prosperity of the mobile Internet, the abundance of data makes it difficult for users to choose their favorite app. Thus, mobile app recommendation as an emerging topic attracts lots of attention. However, existing methods for app recommendation rarely consider recommendation accuracy under the privacy representation of user preferences. To address this problem, we propose a privacy-aware app package recommendation method named APP-Rec. Specifically, in this method: 1) treat an app and its associated heterogeneous entities (APP-Rec considers not only the apps themselves but also a variety of related factors—collectively referred to as heterogeneous entities, such as app category and app neighbors) as an app package and extract comprehensive features from the app package using an intrapackage attention network and an interpackage attention network to improve app recommendation; and 2) design a privacy module utilizing Laplace noise to achieve privacy preservation of user preferences. Experimental results show that APP-Rec outperforms the state-of-the-art methods in terms of area under the curve (AUC). Moreover, the privacy preservation of user preferences in APP-Rec is proved by theoretical analysis and experimental results.},
  archive      = {J_TAI},
  author       = {Shanpeng Liu and Buqing Cao and Jianxun Liu and Guosheng Kang and Min Shi and Xiong Li and Kenneth K. Fletcher},
  doi          = {10.1109/TAI.2024.3443028},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6240-6252},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An approach for privacy-aware mobile app package recommendation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OAFuser: Toward omni-aperture fusion for light field
semantic segmentation. <em>TAI</em>, <em>5</em>(12), 6225–6239. (<a
href="https://doi.org/10.1109/TAI.2024.3457931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field cameras are capable of capturing intricate angular and spatial details. This allows for acquiring complex light patterns and details from multiple angles, significantly enhancing the precision of image semantic segmentation. However, two significant issues arise: 1) The extensive angular information of light field cameras contains a large amount of redundant data, which is overwhelming for the limited hardware resources of intelligent agents. 2) A relative displacement difference exists in the data collected by different microlenses. To address these issues, we propose an omni-aperture fusion model (OAFuser) that leverages dense context from the central view and extracts the angular information from subaperture images to generate semantically consistent results. To simultaneously streamline the redundant information from the light field cameras and avoid feature loss during network propagation, we present a simple yet very effective subaperture fusion module (SAFM) . This module efficiently embeds subaperture images in angular features, allowing the network to process each subaperture image with a minimal computational demand of only ( ${\sim}1\rm GFlops$ ). Furthermore, to address the mismatched spatial information across viewpoints, we present a center angular rectification module (CARM) to realize feature resorting and prevent feature occlusion caused by misalignment. The proposed OAFuser achieves state-of-the-art performance on four UrbanLF datasets in terms of all evaluation metrics and sets a new record of $84.93\%$ in mIoU on the UrbanLF-Real Extended dataset, with a gain of ${+}3.69\%$ . The source code for OAFuser is available at https://github.com/FeiBryantkit/OAFuser .},
  archive      = {J_TAI},
  author       = {Fei Teng and Jiaming Zhang and Kunyu Peng and Yaonan Wang and Rainer Stiefelhagen and Kailun Yang},
  doi          = {10.1109/TAI.2024.3457931},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6225-6239},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {OAFuser: Toward omni-aperture fusion for light field semantic segmentation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning multipursuit evasion for safe targeted navigation
of drones. <em>TAI</em>, <em>5</em>(12), 6210–6224. (<a
href="https://doi.org/10.1109/TAI.2024.3366871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Safe navigation of drones in the presence of adversarial physical attacks from multiple pursuers is a challenging task. This article proposes a novel approach, asynchronous multistage deep reinforcement learning (AMS-DRL), to train adversarial neural networks that can learn from the actions of multiple evolved pursuers and adapt quickly to their behavior, enabling the drone to avoid attacks and reach its target. Specifically, AMS-DRL evolves adversarial agents in a pursuit-evasion game (PEG) where the pursuers and the evader are asynchronously trained in a bipartite graph way during multiple stages. Our approach guarantees convergence by ensuring Nash equilibrium (NE) among agents from the game-theory analysis. We evaluate our method in extensive simulations and show that it outperforms baselines with higher navigation success rates (SRs). We also analyze how parameters such as the relative maximum speed affect navigation performance. Furthermore, we have conducted physical experiments and validated the effectiveness of the trained policies in real-time flights. A SR heatmap is introduced to elucidate how spatial geometry influences navigation outcomes.},
  archive      = {J_TAI},
  author       = {Jiaping Xiao and Mir Feroskhan},
  doi          = {10.1109/TAI.2024.3366871},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6210-6224},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Learning multipursuit evasion for safe targeted navigation of drones},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From behavior to natural language: Generative approach for
unmanned aerial vehicle intent recognition. <em>TAI</em>,
<em>5</em>(12), 6196–6209. (<a
href="https://doi.org/10.1109/TAI.2024.3376510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a novel cross-modal neural network model that aims to convert long-term temporal behavior data into natural language to achieve unmanned aerial vehicle (UAV) intent recognition. Our generative intent recognition model effectively utilizes the inherent redundancy present in long temporal behavior data by incorporating a sequence compression module, which enables the cross-modal generation and alignment of intents while preserving the integrity of the standard Transformer architecture. Importantly, we observe that this approach mitigates the negative impact of imbalanced database distribution by mapping intent categories onto the modality of natural language. Furthermore, we propose three comprehensive pretraining tasks specifically designed for time series data, thoroughly examining their interconnections and analyzing the impact of a hybrid pretraining framework on the accuracy of intent recognition. Our experimental results demonstrate the superiority of our proposed generative UAV intent recognition model, along with the hybrid pretraining initialization method, compared to conventional classification models. Simultaneously, the intent recognition method exhibits heightened temporal sensitivity and robust resilience, enabling it to deal with complex UAV confrontation and interference environment.},
  archive      = {J_TAI},
  author       = {Leyan Li and Rennong Yang and Maolong Lv and Ao Wu and Zilong Zhao},
  doi          = {10.1109/TAI.2024.3376510},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6196-6209},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {From behavior to natural language: Generative approach for unmanned aerial vehicle intent recognition},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Q-cogni: An integrated causal reinforcement learning
framework. <em>TAI</em>, <em>5</em>(12), 6186–6195. (<a
href="https://doi.org/10.1109/TAI.2024.3453230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Q-Cogni , an algorithmically integrated causal reinforcement learning framework that redesigns Q-Learning to improve the learning process with causal inference. Q-Cogni achieves improved policy quality and learning efficiency with a prelearned structural causal model of the environment, queried to guide the policy learning process with an understanding of cause-and-effect relationships in a state-action space. By doing so, we not only leverage the sample efficient techniques of reinforcement learning but also enable reasoning about a broader set of policies and bring higher degrees of interpretability to decisions made by the reinforcement learning agent. We apply Q-Cogni on vehicle routing problem (VRP) environments including a real-world dataset of taxis in New York City using the Taxi and Limousine Commission trip record data. We show Q-Cogni&#39;s capability to achieve an optimally guaranteed policy (total trip distance) in 76% of the cases when comparing to shortest-path-search methods and outperforming (shorter distances) state-of-the-art reinforcement learning algorithms in 66% of cases. Additionally, since Q-Cogni does not require a complete global map, we show that it can start efficiently routing with partial information and improve as more data is collected, such as traffic disruptions and changes in destination, making it ideal for deployment in real-world dynamic settings.},
  archive      = {J_TAI},
  author       = {Cristiano da Costa Cunha and Wei Liu and Tim French and Ajmal Mian},
  doi          = {10.1109/TAI.2024.3453230},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6186-6195},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Q-cogni: An integrated causal reinforcement learning framework},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DeepHGCN: Toward deeper hyperbolic graph convolutional
networks. <em>TAI</em>, <em>5</em>(12), 6172–6185. (<a
href="https://doi.org/10.1109/TAI.2024.3440223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperbolic graph convolutional networks (HGCNs) have demonstrated significant potential in extracting information from hierarchical graphs. However, existing HGCNs are limited to shallow architectures due to the computational expense of hyperbolic operations and the issue of oversmoothing as depth increases. Although treatments have been applied to alleviate oversmoothing in graph convolutional networks (GCNs), developing a hyperbolic solution presents distinct challenges since operations must be carefully designed to fit the hyperbolic nature. Addressing these challenges, we propose DeepHGCN, the first deep multilayer HGCN architecture with dramatically improved computational efficiency and substantially reduced oversmoothing. DeepHGCN features two key innovations: 1) a novel hyperbolic feature transformation layer that enables fast and accurate linear mappings; and 2) techniques such as hyperbolic residual connections and regularization for both weights and features, facilitated by an efficient hyperbolic midpoint method. Extensive experiments demonstrate that DeepHGCN achieves significant improvements in link prediction (LP) and node classification (NC) tasks compared to both Euclidean and shallow hyperbolic GCN variants.},
  archive      = {J_TAI},
  author       = {Jiaxu Liu and Xinping Yi and Xiaowei Huang},
  doi          = {10.1109/TAI.2024.3440223},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6172-6185},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {DeepHGCN: Toward deeper hyperbolic graph convolutional networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatiotemporal object detection for improved aerial vehicle
detection in traffic monitoring. <em>TAI</em>, <em>5</em>(12),
6159–6171. (<a href="https://doi.org/10.1109/TAI.2024.3454566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents advancements in multiclass vehicle detection using unmanned aerial vehicle (UAV) cameras through the development of spatiotemporal object detection models. The study introduces a spatiotemporal vehicle detection dataset (STVD) containing $6600$ annotated sequential frame images captured by UAVs, enabling comprehensive training and evaluation of algorithms for holistic spatiotemporal perception. A YOLO-based object detection algorithm is enhanced to incorporate temporal dynamics, resulting in improved performance over single frame models. The integration of attention mechanisms into spatiotemporal models is shown to further enhance performance. Experimental validation demonstrates significant progress, with the best spatiotemporal model exhibiting a 16.22% improvement over single frame models, while it is demonstrated that attention mechanisms hold the potential for additional performance gains.},
  archive      = {J_TAI},
  author       = {Kristina Telegraph and Christos Kyrkou},
  doi          = {10.1109/TAI.2024.3454566},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6159-6171},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Spatiotemporal object detection for improved aerial vehicle detection in traffic monitoring},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Curious feature selection-based clustering. <em>TAI</em>,
<em>5</em>(12), 6146–6158. (<a
href="https://doi.org/10.1109/TAI.2024.3407034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In tabular data, certain challenges can negatively affect the quality of machine learning models, such as high dimensionality, noisy, irrelevant, and repetitive features, interactions between features and the fact that instances often come from different sources or distributions. Feature selection, instance selection and clustering algorithms address some of these challenges. Here, we propose a new holistic framework that assists in clarifying the structure of tabular datasets and enables the production of higher quality machine learning models. The framework, based on intrinsic-reward deep reinforcement learning loops, uses curious feature selection as the basis for clustering data instances, effectively creating blocks within the tabular data with the most relevant features for each cluster. The framework results in a clustering algorithm, wherein the instances are clustered based on their predicted optimal informative features. We show that this framework makes it possible to improve the accuracy of learning models on artificial and real datasets and to provide important insights into the data themselves.},
  archive      = {J_TAI},
  author       = {Michal Moran and Goren Gordon},
  doi          = {10.1109/TAI.2024.3407034},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6146-6158},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Curious feature selection-based clustering},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning-based dual watermarking for image copyright
protection and authentication. <em>TAI</em>, <em>5</em>(12), 6134–6145.
(<a href="https://doi.org/10.1109/TAI.2024.3485519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advancements in digital technologies make it easy to modify the content of digital images. Hence, ensuring digital images’ integrity and authenticity is necessary to protect them against various attacks that manipulate them. We present a deep learning (DL) based dual invisible watermarking technique for performing source authentication, content authentication, and protecting digital content copyright of images sent over the internet. Beyond securing images, the proposed technique demonstrates robustness to content-preserving image manipulation attacks. It is also impossible to imitate or overwrite watermarks because the cryptographic hash of the image and the dominant features of the image in the form of perceptual hash are used as watermarks. We highlighted the need for source authentication to safeguard image integrity and authenticity, along with identifying similar content for copyright protection. After exhaustive testing, our technique obtained a high peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM), which implies there is a minute change in the original image after embedding our watermarks. Our trained model achieves high watermark extraction accuracy and satisfies two different objectives of verification and authentication on the same watermarked image.},
  archive      = {J_TAI},
  author       = {Sudev Kumar Padhi and Archana Tiwari and Sk. Subidh Ali},
  doi          = {10.1109/TAI.2024.3485519},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6134-6145},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep learning-based dual watermarking for image copyright protection and authentication},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised domain adaptation on point clouds via
high-order geometric structure modeling. <em>TAI</em>, <em>5</em>(12),
6121–6133. (<a href="https://doi.org/10.1109/TAI.2024.3483199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point clouds can capture the precise geometric information of objects and scenes, which are an important source of 3-D data and one of the most popular 3-D geometric data structures for cognitions in many real-world applications like automatic driving and remote sensing. However, due to the influence of sensors and varieties of objects, the point clouds obtained by different devices may suffer obvious geometric changes, resulting in domain gaps that are prone to the neural networks trained in one domain failing to preserve the performance in other domains. To alleviate the above problem, this article proposes an unsupervised domain adaptation framework, named HO-GSM, as the first attempt to model high-order geometric structures of point clouds. First, we construct multiple self-supervised tasks to learn the invariant semantic and geometric features of the source and target domains, especially to capture the feature invariance of high-order geometric structures of point clouds. Second, the discriminative feature space of target domain is acquired by using contrastive learning to refine domain alignment to specific class level. Experiments on the PointDA-10 and GraspNetPC-10 collection of datasets show that the proposed HO-GSM can significantly outperform the state-of-the-art counterparts.},
  archive      = {J_TAI},
  author       = {Jiang-Xing Cheng and Huibin Lin and Chun-Yang Zhang and C. L. Philip Chen},
  doi          = {10.1109/TAI.2024.3483199},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6121-6133},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Unsupervised domain adaptation on point clouds via high-order geometric structure modeling},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing aerial object detection with selective frequency
interaction network. <em>TAI</em>, <em>5</em>(12), 6109–6120. (<a
href="https://doi.org/10.1109/TAI.2024.3381096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aerial object detection is a crucial task in computer vision because it plays a pivotal role in understanding remote images. However, most convolutional neural network (CNN) methods primarily focus on the spatial/channel interactions, overlooking the significance of frequency domain information. To overcome these limitations, we introduce an innovative method named the selective frequency interaction (SFI) network for the task of aerial object detection. Our method comprises two essential modules: the selective frequency-domain feature extraction (SFFE) module and the selective frequency-domain features interaction (SFFI) module. In the first module, SFFE, we focus on the extraction of frequency-domain information from the feature maps. This extraction process significantly enriches the feature information, spanning various frequencies. The subsequent module, SFFI, plays a crucial role in facilitating efficient interaction and fusion of the frequency-domain feature maps obtained from the SFFE module across channels. This interaction is essential for optimizing the utilization of frequency-domain information. Final, we integrate these frequency-domain weights with the time-domain feature maps. By enabling full and efficient interaction and fusion of SFFE feature weights across channels, the SFFI module ensures the effective utilization of frequency-domain information. We conduct extensive experiments on the DOTA V1.0, DOTA V1.5, and HRSC2016 datasets to demonstrate the competitive performance of the proposed SFI network in aerial object detection.},
  archive      = {J_TAI},
  author       = {Weijie Weng and Mengwan Wei and Junchi Ren and Fei Shen},
  doi          = {10.1109/TAI.2024.3381096},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6109-6120},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Enhancing aerial object detection with selective frequency interaction network},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Epi-curriculum: Episodic curriculum learning for
low-resource domain adaptation in neural machine translation.
<em>TAI</em>, <em>5</em>(12), 6095–6108. (<a
href="https://doi.org/10.1109/TAI.2024.3396125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural machine translation (NMT) models have achieved comparable results to human translation with a large number of parallel corpora available. However, their performance remains poor when translating on new domains with a limited number of data. Recent studies either only show the model&#39;s robustness to domain shift or the superiority in adapting to new domains with a limited number of data. A solution for addressing both the model&#39;s robustness and adaptability is underexplored. In this article, we present a novel approach Epi-Curriculum to address low-resource domain adaptation (DA), which contains a new episodic training framework along with a denoised curriculum learning. Our episodic training framework enhances the model&#39;s robustness to domain shift by episodically exposing the encoder/decoder to an inexperienced decoder/encoder. The denoised curriculum learning filters the noised data and further improves the model&#39;s adaptability by gradually guiding the learning process from easy to more difficult tasks. Extensive experiments have been conducted on English–German (En-De), English–Romanian (En-Ro), and English–French (En-Fr) translation tasks. Our results show that: 1) Epi-Curriculum outperforms the baseline on unseen and seen domains by 2.28 and 3.64 BLEU score on En-De task, and 3.32 and 2.23 on En-Ro task; and 2) our episodic training framework outperforms the recent popular metalearning framework in terms of robustness to domain shift and achieves comparable adaptability to new domains.},
  archive      = {J_TAI},
  author       = {Keyu Chen and Di Zhuang and Mingchen Li and J. Morris Chang},
  doi          = {10.1109/TAI.2024.3396125},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6095-6108},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Epi-curriculum: Episodic curriculum learning for low-resource domain adaptation in neural machine translation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Game theory meets data augmentation. <em>TAI</em>,
<em>5</em>(12), 6080–6094. (<a
href="https://doi.org/10.1109/TAI.2024.3384129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation is a critical component in building modern deep-learning systems. In this article, we propose MFG Augment , a novel data augmentation method based on the mean-field game (MFG) theory that can synthesize a sequence of data between every two images or features. The central idea is to consider every image as a distribution over its pixel or feature space. Using MFG theory, we can generate a time-continuous “path” from one distribution to another so that the points along the “path” are augmented images or features. Empirically, the experiment results on MNIST, CIFAR-10, and ImageNet demonstrate that the proposed technology has better generalization ability and higher classification accuracy as compared to several benchmark methods. More importantly, our MFG Augment improves the test accuracy significantly when the dataset size is small. MFG Augment consistently shows better affinity and diversity scores, two important empirical metrics for evaluating the generalization of data augmentation techniques.},
  archive      = {J_TAI},
  author       = {Yuhan Kang and Samira Zare and Alex Tong Lin and Zhu Han and Stanley Osher and Hien Van Nguyen},
  doi          = {10.1109/TAI.2024.3384129},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6080-6094},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Game theory meets data augmentation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-based offline reinforcement learning with uncertainty
estimation and policy constraint. <em>TAI</em>, <em>5</em>(12),
6066–6079. (<a href="https://doi.org/10.1109/TAI.2024.3372939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explicit uncertainty estimation is an effective method for addressing the overestimation problem caused by distribution shifts in offline reinforcement learning (RL). However, the common bootstrapped ensemble network method fails to obtain reliable uncertainty estimation, which will decrease the performance of offline RL. Compared with model-free offline RL, model-based offline RL provides better generalizability although it is limited by the model-bias problem. The adverse effects of model bias will be aggravated by the state mismatch phenomenon that will ultimately disrupt policy learning. In this article, we propose the model-based offline RL with uncertainty estimation and policy constraint (MOUP) algorithm to obtain reliable uncertainty estimation and bounded state mismatch. First, we introduce Monte Carlo (MC) dropout to ensemble networks and propose ensemble dropout networks for uncertainty estimation. Second, a novel policy constraint method is given that incorporates the maximum mean discrepancy constraint into policy optimization, and we prove that such a method can generate bounded state mismatch. Finally, we evaluate the MOUP algorithm on the MuJoCo control toolkit. Experimental results show that the proposed MOUP algorithm is competitive compared with existing offline RL algorithms.},
  archive      = {J_TAI},
  author       = {Jin Zhu and Chunhui Du and Geir E. Dullerud},
  doi          = {10.1109/TAI.2024.3372939},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6066-6079},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Model-based offline reinforcement learning with uncertainty estimation and policy constraint},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Steganography in style transfer. <em>TAI</em>,
<em>5</em>(12), 6054–6065. (<a
href="https://doi.org/10.1109/TAI.2024.3379946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Steganography entails concealing secret data within a given medium for covert communication. In recent years, style-transferred images have been widely disseminated on social media, offering a novel multimedia carrier for steganography. However, there is currently a lack of steganographic techniques specifically designed for style-transferred images. In this article, we propose disguising the steganographic tool as a deep neural network (DNN) performing style transfer tasks. In our method, a neural network is manipulated to transfer the style of a given image to a target style, while also embedding secret data into the given image. Meanwhile, a trained receiving network is used to extract the embedded data. The same pretrained network used by the processing network and the receiving network matches the feature maps of secret data at the same layers. Under the guidance of secret data, a stego image is generated after being trained by the processing network and the receiving network an appropriate number of times, and secret data can be extracted from stego image. Experimental results demonstrate that our method is more effective and secure compared to existing steganographic algorithms that achieve data embedding by modifying image content.},
  archive      = {J_TAI},
  author       = {Ruolan Shi and Zichi Wang and Yunlong Hao and Xinpeng Zhang},
  doi          = {10.1109/TAI.2024.3379946},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6054-6065},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Steganography in style transfer},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Direct adversarial latent estimation to evaluate decision
boundary complexity in black box models. <em>TAI</em>, <em>5</em>(12),
6043–6053. (<a href="https://doi.org/10.1109/TAI.2024.3455308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A trustworthy artificial intelligence (AI) model should be robust to perturbed data, where robustness correlates with the dimensionality and linearity of feature representations in the model latent space. Existing methods for evaluating feature representations in the latent space are restricted to white-box models. In this work, we introduce direct adversarial latent estimation (DALE) for evaluating the robustness of feature representations and decision boundaries for target black-box models. A surrogate latent space is created using a variational autoencoder (VAE) trained on a disjoint dataset from an object classification backbone, then the VAE latent space is traversed to create sets of adversarial images. An object classification model is trained using transfer learning on the VAE image reconstructions, then classifies instances in the adversarial image set. We propose that the number of times the classification changes in an image set indicates the complexity of the decision boundaries in the classifier latent space; more complex decision boundaries are found to be more robust. This is confirmed by comparing the DALE distributions to the degradation of the classifier F1 scores in the presence of adversarial attacks. This work enables the first comparisons of latent-space complexity between black box models by relating model robustness to complex decision boundaries.},
  archive      = {J_TAI},
  author       = {Ashley S. Dale and Lauren Christopher},
  doi          = {10.1109/TAI.2024.3455308},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6043-6053},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Direct adversarial latent estimation to evaluate decision boundary complexity in black box models},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Label-efficient time series representation learning: A
review. <em>TAI</em>, <em>5</em>(12), 6027–6042. (<a
href="https://doi.org/10.1109/TAI.2024.3430236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label-efficient time series representation learning, which aims to learn effective representations with limited labeled data, is crucial for deploying deep learning models in real-world applications. To address the scarcity of labeled time series data, various strategies, e.g., transfer learning, self-supervised learning, and semisupervised learning, have been developed. In this survey, we introduce a novel taxonomy for the first time, categorizing existing approaches as in-domain or cross domain based on their reliance on external data sources or not. Furthermore, we present a review of the recent advances in each strategy, conclude the limitations of current methodologies, and suggest future research directions that promise further improvements in the field.},
  archive      = {J_TAI},
  author       = {Emadeldeen Eldele and Mohamed Ragab and Zhenghua Chen and Min Wu and Chee-Keong Kwoh and Xiaoli Li},
  doi          = {10.1109/TAI.2024.3430236},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6027-6042},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Label-efficient time series representation learning: A review},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neuro-symbolic AI for military applications. <em>TAI</em>,
<em>5</em>(12), 6012–6026. (<a
href="https://doi.org/10.1109/TAI.2024.3444746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) plays a significant role in enhancing the capabilities of defense systems, revolutionizing strategic decision-making, and shaping the future landscape of military operations. Neuro-Symbolic AI is an emerging approach that leverages and augments the strengths of neural networks and symbolic reasoning. These systems have the potential to be more impactful and flexible than traditional AI systems, making them well-suited for military applications. This article comprehensively explores the diverse dimensions and capabilities of Neuro-Symbolic AI, aiming to shed light on its potential applications in military contexts. We investigate its capacity to improve decision-making, automate complex intelligence analysis, and strengthen autonomous systems. We further explore its potential to solve complex tasks in various domains, in addition to its applications in military contexts. Through this exploration, we address ethical, strategic, and technical considerations crucial to the development and deployment of Neuro-Symbolic AI in military and civilian applications. Contributing to the growing body of research, this study represents a comprehensive exploration of the extensive possibilities offered by Neuro-Symbolic AI.},
  archive      = {J_TAI},
  author       = {Desta Haileselassie Hagos and Danda B. Rawat},
  doi          = {10.1109/TAI.2024.3444746},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {6012-6026},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Neuro-symbolic AI for military applications},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient evaluation methods for neural architecture search:
A survey. <em>TAI</em>, <em>5</em>(12), 5990–6011. (<a
href="https://doi.org/10.1109/TAI.2024.3477457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search (NAS) has received increasing attention because of its exceptional merits in automating the design of deep neural network (DNN) architectures. However, the performance evaluation process, as a key part of NAS, often requires training a large number of DNNs. This inevitably makes NAS computationally expensive. In past years, many efficient evaluation methods (EEMs) have been proposed to address this critical issue. In this article, we comprehensively survey these EEMs published up to date, and provide a detailed analysis to motivate the further development of this research direction. Specifically, we divide the existing EEMs into four categories based on the number of DNNs trained for constructing these EEMs. The categorization can reflect the degree of efficiency in principle, which can in turn help quickly grasp the methodological features. In surveying each category, we further discuss the design principles and analyze the strengths and weaknesses to clarify the landscape of existing EEMs, thus making easily understanding the research trends of EEMs. Furthermore, we also discuss the current challenges and issues to identify future research directions in this emerging topic. In summary, this survey provides a convenient overview of EEM for interested users, and they can easily select the proper EEM method for the tasks at hand. In addition, the researchers in the NAS field could continue exploring the future directions suggested in the article.},
  archive      = {J_TAI},
  author       = {Xiaotian Song and Xiangning Xie and Zeqiong Lv and Gary G. Yen and Weiping Ding and Jiancheng Lv and Yanan Sun},
  doi          = {10.1109/TAI.2024.3477457},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {5990-6011},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Efficient evaluation methods for neural architecture search: A survey},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring machine learning for semiconductor process
optimization: A systematic review. <em>TAI</em>, <em>5</em>(12),
5969–5989. (<a href="https://doi.org/10.1109/TAI.2024.3429479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As machine learning (ML) continues to find applications, extensive research is currently underway across various domains. This study examines the current methodologies of ML being investigated to optimize semiconductor manufacturing processes. Our research involved searching the SPIE Digital Library, IEEE Xplore, and ArXiv databases, identifying 58 publications in the field of ML-based semiconductor process optimization. These investigations employ ML techniques such as feature extraction, feature selection, and neural network architecture are analyzed using different algorithms. These models find applications in advanced process control, virtual metrology, and quality control, critical aspects in semiconductor manufacturing for enhancing throughput and reducing production costs. We categorize the articles based on the methods and applications employed, summarizing the primary findings. Furthermore, we discuss the general conclusion of several studies. Overall, the reviewed literature suggests that ML-based semiconductor manufacturing is rapidly gaining popularity and advancing at a swift pace.},
  archive      = {J_TAI},
  author       = {Ying-Lin Chen and Sara Sacchi and Bappaditya Dey and Victor Blanco and Sandip Halder and Philippe Leray and Stefan De Gendt},
  doi          = {10.1109/TAI.2024.3429479},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {5969-5989},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Exploring machine learning for semiconductor process optimization: A systematic review},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Games for artificial intelligence research: A review and
perspectives. <em>TAI</em>, <em>5</em>(12), 5949–5968. (<a
href="https://doi.org/10.1109/TAI.2024.3410935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Games have been the perfect test beds for artificial intelligence (AI) research for the characteristics that widely exist in real-world scenarios. Learning and optimization, decision-making in dynamic and uncertain environments, game theory, planning and scheduling, design, and education are common research areas shared between games and real-world problems. Numerous open-source games or game-based environments have been implemented to study AI. In addition to single- or multiplayer, collaborative, and adversarial games, there has also been growing interest in implementing platforms for creative design in recent years. Those platforms provide ideal benchmarks for exploring and comparing AI ideas and techniques. This article reviews the games and game-based platforms for AI research, provides guidance on matching particular types of AI with suitable games for testing and matching particular needs in games with suitable AI techniques, discusses the research trend induced by the evolution of those games and platforms, and gives an outlook.},
  archive      = {J_TAI},
  author       = {Chengpeng Hu and Yunlong Zhao and Ziqi Wang and Haocheng Du and Jialin Liu},
  doi          = {10.1109/TAI.2024.3410935},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {5949-5968},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Games for artificial intelligence research: A review and perspectives},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on symbolic knowledge distillation of large
language models. <em>TAI</em>, <em>5</em>(12), 5928–5948. (<a
href="https://doi.org/10.1109/TAI.2024.3428519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This survey article delves into the emerging and critical area of symbolic knowledge distillation in large language models (LLMs). As LLMs such as generative pretrained transformer-3 (GPT-3) and bidirectional encoder representations from transformers (BERT) continue to expand in scale and complexity, the challenge of effectively harnessing their extensive knowledge becomes paramount. This survey concentrates on the process of distilling the intricate, often implicit knowledge contained within these models into a more symbolic, explicit form. This transformation is crucial for enhancing the interpretability, efficiency, and applicability of LLMs. We categorize the existing research based on methodologies and applications, focusing on how symbolic knowledge distillation can be used to improve the transparency and functionality of smaller, more efficient artificial intelligence (AI) models. The survey discusses the core challenges, including maintaining the depth of knowledge in a comprehensible format, and explores the various approaches and techniques that have been developed in this field. We identify gaps in current research and potential opportunities for future advancements. This survey aims to provide a comprehensive overview of symbolic knowledge distillation in LLMs, spotlighting its significance in the progression toward more accessible and efficient AI systems.},
  archive      = {J_TAI},
  author       = {Kamal Acharya and Alvaro Velasquez and Houbing Herbert Song},
  doi          = {10.1109/TAI.2024.3428519},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {5928-5948},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A survey on symbolic knowledge distillation of large language models},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive exploration of real-time 3-d view
reconstruction methods. <em>TAI</em>, <em>5</em>(12), 5915–5927. (<a
href="https://doi.org/10.1109/TAI.2024.3477425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time 3-D view reconstruction in an unfamiliar environment poses complexity for various applications due to varying conditions such as occlusion, latency, precision, etc. This article thoroughly examines and tests contemporary methodologies addressing challenges in 3-D view reconstruction. The methods being explored in this article are categorized into volumetric and mesh, generative adversarial network based, and open source library based methods. The exploration of these methods undergoes detailed discussions, encompassing methods, advantages, limitations, and empirical results. The real-time testing of each method is done on benchmarked datasets, including ShapeNet, Pascal 3D+, Pix3D, etc. The narrative highlights the crucial role of 3-D view reconstruction in domains such as robotics, virtual and augmented reality, medical imaging, cultural heritage preservation, etc. The article also anticipates future scopes by exploring generative models, unsupervised learning, and advanced sensor fusion to increase the robustness of the algorithms.},
  archive      = {J_TAI},
  author       = {Arya Agrawal and Teena Sharma and Nishchal K. Verma},
  doi          = {10.1109/TAI.2024.3477425},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {5915-5927},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A comprehensive exploration of real-time 3-D view reconstruction methods},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review on transferability estimation in deep transfer
learning. <em>TAI</em>, <em>5</em>(12), 5894–5914. (<a
href="https://doi.org/10.1109/TAI.2024.3445892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep transfer learning has become increasingly prevalent in various fields such as industry and medical science in recent years. To ensure the successful implementation of target tasks and improve the transfer performance, it is meaningful to prevent negative transfer. However, the dissimilarity between the data from source domain and target domain can pose challenges to transfer learning. Additionally, different transfer models exhibit significant variations in the performance of target tasks, potentially leading to a negative transfer phenomenon. To mitigate the adverse effects of the above factors, transferability estimation methods are employed in this field to evaluate the transferability of the data and the models of various deep transfer learning methods. These methods ascertain transferability by incorporating mutual information between the data or models of the source domain and the target domain. This article furnishes a comprehensive overview of four categories of transferability estimation methods in recent years. It employs qualitative analysis to evaluate various transferability estimation approaches, assisting researchers in selecting appropriate methods. Furthermore, this article evaluates the open problems associated with transferability estimation methods, proposing potential emerging areas for further research. Last, the open-source datasets commonly used in transferability estimation studies are summarized in this study.},
  archive      = {J_TAI},
  author       = {Yihao Xue and Rui Yang and Xiaohan Chen and Weibo Liu and Zidong Wang and Xiaohui Liu},
  doi          = {10.1109/TAI.2024.3445892},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {5894-5914},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A review on transferability estimation in deep transfer learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recent advances in generative AI and large language models:
Current status, challenges, and perspectives. <em>TAI</em>,
<em>5</em>(12), 5873–5893. (<a
href="https://doi.org/10.1109/TAI.2024.3444742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of generative artificial intelligence (AI) and large language models (LLMs) has marked a new era of natural language processing (NLP), introducing unprecedented capabilities that are revolutionizing various domains. This article explores the current state of these cutting-edge technologies, demonstrating their remarkable advancements and wide-ranging applications. Our article contributes to providing a holistic perspective on the technical foundations, practical applications, and emerging challenges within the evolving landscape of generative AI and LLMs. We believe that understanding the generative capabilities of AI systems and the specific context of LLMs is crucial for researchers, practitioners, and policymakers to collaboratively shape the responsible and ethical integration of these technologies into various domains. Furthermore, we identify and address main research gaps, providing valuable insights to guide future research endeavors within the AI research community.},
  archive      = {J_TAI},
  author       = {Desta Haileselassie Hagos and Rick Battle and Danda B. Rawat},
  doi          = {10.1109/TAI.2024.3444742},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {5873-5893},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Recent advances in generative AI and large language models: Current status, challenges, and perspectives},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ensuring ethical standards in the development of autonomous
and intelligent systems. <em>TAI</em>, <em>5</em>(12), 5863–5872. (<a
href="https://doi.org/10.1109/TAI.2024.3387403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering ethical values, maintaining ethical requirements, and addressing ethical concerns is a crucial aspect of developing systems based on artificial intelligence (AI). Developers need to prioritize ethical considerations throughout the development process, from the initial concept to the final implementation. Despite the issuance of numerous ethical guidelines for technology developers in recent years, research results have shown no significant impact of ethical requirements on the human decision-making of software engineers from the tech community. It is therefore important to have well-defined processes to ensure that ethical standards are maintained throughout the development process. This is essential to fostering trust in AI-based systems and reducing their potential risks and negative impacts. This article provides an overview of the processes that can integrate ethics into development methodologies by introducing a certification program that focuses on ethical accountability, transparency, privacy, and the avoidance of unacceptable algorithmic bias. The certification method specifies detailed criteria for the independent evaluation, conformity assessment, and certification of AI systems. The article also underlines factors that can affect ethical accountability, transparency, privacy, and unacceptable bias. It provides examples of the necessary documented evidence for conformity assessment against the suite of certification criteria that enable evaluation of the organizations’ ability to meet ethical requirements.},
  archive      = {J_TAI},
  author       = {Anetta Jedlickova},
  doi          = {10.1109/TAI.2024.3387403},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {5863-5872},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Ensuring ethical standards in the development of autonomous and intelligent systems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Editorial: Future directions in artificial intelligence
research. <em>TAI</em>, <em>5</em>(12), 5858–5862. (<a
href="https://doi.org/10.1109/TAI.2024.3501912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TAI},
  author       = {Hussein Abbass},
  doi          = {10.1109/TAI.2024.3501912},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {12},
  pages        = {5858-5862},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Editorial: Future directions in artificial intelligence research},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hybrid relational approach toward stock price prediction
and profitability. <em>TAI</em>, <em>5</em>(11), 5844–5854. (<a
href="https://doi.org/10.1109/TAI.2024.3408129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An accurate estimation of future stock prices can help investors maximize their profits. The current advancements in the area of artificial intelligence (AI) have proven prevalent in the financial sector. Besides, stock market prediction is difficult owing to the considerable volatility and unpredictability induced by numerous factors. Recent approaches have considered fundamental, technical, or macroeconomic variables to find hidden complex patterns in financial data. At the macro level, there exists a spillover effect between stock pairs that can explain the variance present in the data and boost the prediction performance. To address this interconnectedness defined by intrasector stocks, we propose a hybrid relational approach to predict the future price of stocks in the American, Indian, and Korean economies. We collected market data of large-, mid-, and small-capitalization peer companies in the same industry as the target firm, considering them as relational features. To ensure efficient feature selection, we have utilized a data-driven approach, i.e., random forest feature permutation (RF2P), to remove noise and instability. A hybrid prediction module consisting of temporal convolution and linear model (TCLM) is proposed that considers irregularities and linear trend components of the financial data. We found that RF2P-TCLM gave the superior performance. To demonstrate the real-world applicability of our approach in terms of profitability, we created a trading method based on the predicted results. This technique generates a higher profit than the existing approaches.},
  archive      = {J_TAI},
  author       = {Manali Patel and Krupa Jariwala and Chiranjoy Chattopadhyay},
  doi          = {10.1109/TAI.2024.3408129},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5844-5854},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A hybrid relational approach toward stock price prediction and profitability},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiscale bilateral attention fusion network for
pansharpening. <em>TAI</em>, <em>5</em>(11), 5828–5843. (<a
href="https://doi.org/10.1109/TAI.2024.3418378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-resolution multispectral (HRMS) images combine spatial and spectral information originating from panchromatic (PAN) and reduced-resolution multispectral (LRMS) images. Pansharpening performs well and is widely used to obtain HRMS images. However, most pansharpening approaches determine the ratio of PAN and LRMS images through direct interpolation, which may introduce artifacts and distort the color of the fused results. To address this issue, an unsupervised progressive pansharpening framework, MSBANet, is proposed, which adopts a multistage fusion strategy. Each stage contains an attention interactive extraction module (AIEM) and a multiscale bilateral fusion module (MBFM). The AIEM extracts spatial and spectral features from input images and captures the correlations between features. The MBFM can efficiently integrate information from the AIEM and improve MSBANet context awareness. We design a hybrid loss function that enhances the ability of the fusion network to store spectral and texture details. In qualitative and quantitative experimental studies on four datasets, MSBANet outperformed state-of-the-art pansharpening techniques. The code will be released.},
  archive      = {J_TAI},
  author       = {Zhongyuan Guo and Jiawei Li and Jia Lei and Jinyuan Liu and Shihua Zhou and Bin Wang and Nikola K. Kasabov},
  doi          = {10.1109/TAI.2024.3418378},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5828-5843},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multiscale bilateral attention fusion network for pansharpening},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An improved continuous-encoding-based multiobjective
evolutionary algorithm for community detection in complex networks.
<em>TAI</em>, <em>5</em>(11), 5815–5827. (<a
href="https://doi.org/10.1109/TAI.2024.3442153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection is a fundamental and widely studied field in network science. To perform community detection, various competitive multiobjective evolutionary algorithms (MOEAs) have been proposed. It is worth noting that the latest continuous encoding (CE) method transforms the original discrete problem into a continuous one, which can achieve better community partitioning. However, the original CE ignored important structural features of nodes, such as the clustering coefficient (CC), resulting in poor initial solutions and reduced the performance of community detection. Therefore, we propose a simple scheme to effectively utilize node structure feature vectors to enhance community detection. Specifically, a CE and CC-based (CE-CC) MOEA called CECC-Net is proposed. In CECC-Net, the CC vector performs the Hadamard product with a continuous vector (i.e., a concatenation of the continuous variables $\mathbf{x}$ associated with the edges), resulting in an improved initial individual. Then, applying the nonlinear transformation to the continuous-valued individual yields a discrete-valued community grouping solution. Furthermore, a corresponding adaptive operator is designed as an essential part of this scheme to mitigate the negative effects of feature vectors on population diversity. The effectiveness of the proposed scheme was validated through ablation and comparative experiments. Experimental results on synthetic and real-world networks demonstrate that the proposed algorithm has competitive performance in comparison with several state-of-the-art EA-based community detection algorithms.},
  archive      = {J_TAI},
  author       = {Jun Fu and Yan Wang},
  doi          = {10.1109/TAI.2024.3442153},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5815-5827},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An improved continuous-encoding-based multiobjective evolutionary algorithm for community detection in complex networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical spatial-temporal masked contrast for skeleton
action recognition. <em>TAI</em>, <em>5</em>(11), 5801–5814. (<a
href="https://doi.org/10.1109/TAI.2024.3430260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of 3-D action recognition, self-supervised learning has shown promising results but remains a challenging task. Previous approaches to motion modeling often relied on selecting features solely from the temporal or spatial domain, which limited the extraction of higher-level semantic information. Additionally, traditional one-to-one approaches in multilevel comparative learning overlooked the relationships between different levels, hindering the learning representation of the model. To address these issues, we propose the hierarchical spatial-temporal masked network (HSTM) for learning 3-D action representations. HSTM introduces a novel masking method that operates simultaneously in both the temporal and spatial dimensions. This approach leverages semantic relevance to identify meaningful regions in time and space, guiding the masking process based on semantic richness. This guidance is crucial for learning useful feature representations effectively. Furthermore, to enhance the learning of potential features, we introduce cross-level distillation (CLD) to extend the comparative learning approach. By training the model with two types of losses simultaneously, each level of the multilevel comparative learning process can be guided by levels rich in semantic information. This allows for more effective supervision of comparative learning, leading to improved performance. Extensive experiments conducted on the NTU-60, NTU-120, and PKU-MMD datasets demonstrate the effectiveness of our proposed framework. The learned action representations exhibit strong transferability and achieve state-of-the-art results.},
  archive      = {J_TAI},
  author       = {Wenming Cao and Aoyu Zhang and Zhihai He and Yicha Zhang and Xinpeng Yin},
  doi          = {10.1109/TAI.2024.3430260},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5801-5814},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Hierarchical spatial-temporal masked contrast for skeleton action recognition},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Broad siamese network for facial beauty prediction.
<em>TAI</em>, <em>5</em>(11), 5786–5800. (<a
href="https://doi.org/10.1109/TAI.2024.3429293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial beauty prediction (FBP) aims to automatically predict beauty scores of facial images according to human perception. Usually, facial images contain lots of information irrelevant to facial beauty, such as information about pose, emotion, and illumination, which interferes with the prediction of facial beauty. To overcome interferences, we develop a broad Siamese network (BSN) to focus more on the task of beauty prediction. Specifically, BSN consists mainly of three components: a multitask Siamese network (MTSN), a multilayer attention (MLA) module, and a broad representation learning (BRL) module. First, MTSN is proposed with different tasks about facial beauty to fully mine knowledge about attractiveness and guide the network to neglect interference information. In the subnetwork of MTSN, the MLA module is proposed to focus more on salient features about facial beauty and reduce the impact of interference information. Then, the BRL module based on broad learning system (BLS) is developed to learn discriminative features with the guidance of beauty scores. It further releases facial features from the impact of interference information. Comparisons with state-of-the-art methods demonstrate the effectiveness of BSN.},
  archive      = {J_TAI},
  author       = {Yikai Li and Tong Zhang and C. L. Philip Chen},
  doi          = {10.1109/TAI.2024.3429293},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5786-5800},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Broad siamese network for facial beauty prediction},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An integrated fusion framework for ensemble learning
leveraging gradient-boosting and fuzzy rule-based models. <em>TAI</em>,
<em>5</em>(11), 5771–5785. (<a
href="https://doi.org/10.1109/TAI.2024.3424427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of different learning paradigms has long been a focus of machine learning research, aimed at overcoming the inherent limitations of individual methods. Fuzzy rule-based models excel in interpretability and have seen widespread application across diverse fields. However, they face challenges such as complex design specifications and scalability issues with large datasets. The fusion of different techniques and strategies, particularly gradient boosting, with fuzzy rule-based models offers a robust solution to these challenges. This article proposes an integrated fusion framework that merges the strengths of both paradigms to enhance model performance and interpretability. At each iteration, a fuzzy rule-based model is constructed and controlled by a dynamic factor to optimize its contribution to the overall ensemble. This control factor serves multiple purposes: it prevents model dominance, encourages diversity, acts as a regularization parameter, and provides a mechanism for dynamic tuning based on model performance, thus mitigating the risk of overfitting. Additionally, the framework incorporates a sample-based correction mechanism that allows for adaptive adjustments based on feedback from a validation set. Experimental results substantiate the efficacy of the presented gradient-boosting framework for fuzzy rule-based models, demonstrating performance enhancement, especially in terms of mitigating overfitting and complexity typically associated with many rules. By leveraging an optimal factor to govern the contribution of each model, the framework improves performance, maintains interpretability, and simplifies the maintenance and update of the models.},
  archive      = {J_TAI},
  author       = {Jinbo Li and Peng Liu and Long Chen and Witold Pedrycz and Weiping Ding},
  doi          = {10.1109/TAI.2024.3424427},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5771-5785},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An integrated fusion framework for ensemble learning leveraging gradient-boosting and fuzzy rule-based models},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Brain-inspired evolutionary architectures for spiking neural
networks. <em>TAI</em>, <em>5</em>(11), 5760–5770. (<a
href="https://doi.org/10.1109/TAI.2024.3407033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intricate and distinctive evolutionary topology of the human brain enables it to execute multiple cognitive tasks simultaneously, and this automated evolutionary process of biological networks motivates our investigation into efficient architecture optimization for spiking neural networks (SNNs). Diverging from traditional manual-designed and hierarchical network architecture search (NAS), we advance the evolution of SNN architecture by integrating local, brain region-inspired modular structures with global cross-module connectivity. Locally, the brain region-inspired module consists of multiple neural motifs with excitatory and inhibitory connections; globally, free connections among modules, including long-term cross-module feedforward and feedback connections are evolved. We introduce an efficient multiobjective evolutionary algorithm that leverages a few-shot predictor, endowing SNNs with high performance and low energy consumption. Extensive experiments across both static (CIFAR10, CIFAR100) and neuromorphic (CIFAR10-DVS, DVS128-Gesture) datasets reveal that the proposed model significantly exhibits robustness while maintaining consistent and exceptional performance. This study pioneers in searching for optimal neural architectures for SNNs by integrating the human brain&#39;s advanced connectivity and modular organization into SNN optimization, thereby contributing valuable perspectives to the development of brain-inspired artificial intelligence.},
  archive      = {J_TAI},
  author       = {Wenxuan Pan and Feifei Zhao and Zhuoya Zhao and Yi Zeng},
  doi          = {10.1109/TAI.2024.3407033},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5760-5770},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Brain-inspired evolutionary architectures for spiking neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online continual learning benefits from large number of task
splits. <em>TAI</em>, <em>5</em>(11), 5746–5759. (<a
href="https://doi.org/10.1109/TAI.2024.3405404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work tackles the significant challenges inherent in online continual learning (OCL), a domain characterized by its handling of numerous tasks over extended periods. OCL is designed to adapt evolving data distributions and previously unseen classes through a single-pass analysis of a data stream, mirroring the dynamic nature of real-world applications. Despite its promising potential, existing OCL methodologies often suffer from catastrophic forgetting (CF) when confronted with a large array of tasks, compounded by substantial computational demands that limit their practical utility. At the heart of our proposed solution is the adoption of a kernel density estimation (KDE) learning framework, aimed at resolving the task prediction (TP) dilemma and ensuring the separability of all tasks. This is achieved through the incorporation of a linear projection head and a probability density function (PDF) for each task, while a shared backbone is maintained across tasks to provide raw feature representation. During the inference phase, we leverage an ensemble of PDFs, which utilizes a self-reporting mechanism based on maximum PDF values to identify the most appropriate model for classifying incoming instances. This strategy ensures that samples with identical labels are cohesively grouped within higher density PDF regions, effectively segregating dissimilar instances across the feature space of different tasks. Extensive experimental validation across diverse OCL datasets has underscored our framework&#39;s efficacy, showcasing remarkable performance enhancements and significant gains over existing methodologies, all achieved with minimal time-space overhead. Our approach introduces a scalable and efficient paradigm for OCL, addressing both the challenge of CF and computational efficiency, thereby extending the applicability of OCL to more realistic and demanding scenarios.},
  archive      = {J_TAI},
  author       = {Shilin Zhang and Chenlin Yi},
  doi          = {10.1109/TAI.2024.3405404},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5746-5759},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Online continual learning benefits from large number of task splits},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep learning-based method for crowd counting using
shunting inhibition mechanism. <em>TAI</em>, <em>5</em>(11), 5733–5745.
(<a href="https://doi.org/10.1109/TAI.2024.3443789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-based crowd counting has gained significant attention due to its widespread applications in security and surveillance. Recent advancements in deep learning have led to the development of numerous methods that have achieved remarkable success in accurately counting crowds. However, many of the existing deep learning methods, which have large model sizes, are unsuitable for deployment on edge devices. This article introduces a novel network architecture and processing element designed to create an efficient and compact deep learning model for crowd counting. The processing element, referred to as the shunting inhibitory neuron, generates complex decision boundaries, making it more powerful than the traditional perceptron. It is employed in both the encoder and decoder modules of the proposed model for feature extraction. Furthermore, the decoder includes alternating convolutional and transformer layers, which provide local receptive fields and global self-attention, respectively. This design captures rich contextual information that is used for generating accurate segmentation and density maps. The self-attention mechanism is implemented using convolution modulation instead of matrix multiplication to reduce computational costs. Experiments conducted on three challenging crowd counting datasets demonstrate that the proposed deep learning network, which comprises a small model size, achieves crowd counting performance comparable to that of state-of-the-art techniques. Codes are available at https://github.com/ftivive/SINet .},
  archive      = {J_TAI},
  author       = {Fok Hing Chi Tivive and Abdesselam Bouzerdoum and Son Lam Phung and Hoang Thanh Le and Hamza Baali},
  doi          = {10.1109/TAI.2024.3443789},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5733-5745},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A deep learning-based method for crowd counting using shunting inhibition mechanism},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning robust global-local representation from EEG for
neural epilepsy detection. <em>TAI</em>, <em>5</em>(11), 5720–5732. (<a
href="https://doi.org/10.1109/TAI.2024.3406289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epilepsy is a life-threatening and challenging neurological disorder, and applying an electroencephalogram (EEG) is a commonly used clinical approach for its detection. Neuropsychological research indicates that epilepsy seizures are highly associated with distinct ranges of temporal EEG patterns. Although previous attempts to automatically detect epilepsy have achieved high classification performance, one crucial challenge still remains: how to effectively learn the robust global-local representation associated with epilepsy in the signals? To address the above challenge, we propose global-local neural epilepsy detection network (GlepNet), a novel architecture for automatic EEG epilepsy detection. We interleave the temporal convolution model together with the multihead attention mechanism within the GlepNet&#39;s encoder blocks to jointly capture the interlaced epilepsy seizure local and global features in EEG signals. Meanwhile, the interpretable method, gradient-weighted class activation mapping (Grad-CAM), is applied to visually confirm that the GlepNet acquires the ability to accord significant weight to EEG segments containing epileptiform abnormalities such as spike-wave complexes. Specifically, the Grad-CAM heatmaps are generated by backpropagating the gradients from the encoder blocks to highlight the epilepsy seizure-related parts. Extensive experiments show the superiority of the GlepNet over state-of-the-art methods on multiple EEG epilepsy datasets. The code will soon be open-sourced on GitHub.},
  archive      = {J_TAI},
  author       = {Xinliang Zhou and Chenyu Liu and Ruizhi Yang and Liangwei Zhang and Liming Zhai and Ziyu Jia and Yang Liu},
  doi          = {10.1109/TAI.2024.3406289},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5720-5732},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Learning robust global-local representation from EEG for neural epilepsy detection},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial intelligence driven predictive analysis of
acoustic and linguistic behaviors for ASD identification. <em>TAI</em>,
<em>5</em>(11), 5709–5719. (<a
href="https://doi.org/10.1109/TAI.2024.3439288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification of autism spectrum disorder (ASD) faces challenges due to the lack of reliable biomarkers and the subjectivity in diagnostic procedures, necessitating improved tools for objectivity and efficiency. Being a key characteristic of autism, language impairments are regarded as potential markers for identifying ASD. However, current research predominantly focuses on analyzing language characteristics in English, overlooking linguistic and contextual specificities in other resource-constrained languages. Motivated by these, we developed an artificial intelligence (AI)-based system to detect ASD, utilizing a range of acoustic and linguistic features extracted from dyadic conversations between a child and their communication partner. Validating our model on 76 English-speaking children [35 ASD and 41 typically developing (TD)] and 33 Hindi-speaking children (15 ASD and 18 TD), our extensive analysis of a diverse and comprehensive set of acoustic and linguistic speech attributes, including lexical, syntactic, semantic, and pragmatic elements revealed reliable speech attributes as predictors of ASD. This comprehensive analysis achieved a remarkable macro F1-score of approximately $\boldsymbol{\sim}$ 91.30%. We further addressed the influence of linguistic diversity on speech-based ASD assessment by examining speech behaviors in both English and the low-resource language, Hindi. Specific features such as adverbs and distinct roots contributed significantly to ASD classification in English, while the proportion of unintelligible utterances and adposition use held greater importance in Hindi. This study underscores the reliability of speech-based biomarkers in ASD assessment, emphasizing their effectiveness across diverse linguistic backgrounds and highlighting the need for language-specific research in this domain.},
  archive      = {J_TAI},
  author       = {Ashwini B. and Deeptanshu and Sheffali Gulati and Jainendra Shukla},
  doi          = {10.1109/TAI.2024.3439288},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5709-5719},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Artificial intelligence driven predictive analysis of acoustic and linguistic behaviors for ASD identification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weighted concept factorization based incomplete multi-view
clustering. <em>TAI</em>, <em>5</em>(11), 5699–5708. (<a
href="https://doi.org/10.1109/TAI.2024.3433379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary objective of classical multiview clustering (MVC) is to categorize data into separate clusters under the assumption that all perspectives are completely available. However, in practical situations, it is common to encounter cases where not all viewpoints of the data are accessible. This limitation can impede the effectiveness of traditional MVC methods. The incompleteness of the clustering of multiview data has witnessed substantial progress in recent years due to its promising applications. In response to the aforementioned issue, we have tackled it by introducing an inventive MVC algorithm that is tailored to handle incomplete data from various views. Additionally, we have proposed a distinct objective function that leverages a weighted concept factorization technique to address the absence of data instances within each incomplete perspective. To address inconsistencies between different views, we introduced a coregularization factor, which operates in conjunction with a shared consensus matrix. It is important to highlight that the proposed objective function is intrinsically nonconvex, presenting challenges in terms of optimization. To secure the optimal solution for this objective function, we have implemented an iterative optimization approach to reach the local minima for our method. To underscore the efficacy and validation of our approach, we experimented with real-world datasets and used state-of-the-art methods to perform comparative assessments.},
  archive      = {J_TAI},
  author       = {Ghufran Ahmad Khan and Jalaluddin Khan and Taushif Anwar and Zubair Ashraf and Mohammad Hafeez Javed and Bassoma Diallo},
  doi          = {10.1109/TAI.2024.3433379},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5699-5708},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Weighted concept factorization based incomplete multi-view clustering},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collision-free grasp detection from color and depth images.
<em>TAI</em>, <em>5</em>(11), 5689–5698. (<a
href="https://doi.org/10.1109/TAI.2024.3420848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient and reliable grasp pose generation plays a crucial role in robotic manipulation tasks. The advancement of deep learning techniques applied to point cloud data has led to rapid progress in grasp detection. However, point cloud data has limitations: no appearance information and susceptibility to sensor noise. In contrast, color Red, Green, Blue (RGB) images offer high-resolution and intricate textural details, making them a valuable complement to the 3-D geometry offered by point clouds or depth (D) images. Nevertheless, the effective integration of appearance information to enhance point cloud-based grasp detection remains an open question. In this study, we extend the concepts of VoteGrasp [1] and introduce an innovative deep learning approach referred to as VoteGrasp Red, Green, Blue, Depth (RGBD). To build robustness to occlusion, the proposed model generates candidates by casting votes and accumulating evidence for feasible grasp configurations. This methodology revolves around fuzing votes extracted from images and point clouds. To further enhance the collaborative effect of merging appearance and geometry features, we introduce a context learning module. We exploit contextual information by encoding the dependency of objects in the scene into features to boost the performance of grasp generation. The contextual information enables our model to increase the likelihood that the generated grasps are collision-free. The efficacy of our model is verified through comprehensive evaluations on the demanding GraspNet-1Billion dataset, leading to a significant improvement of 9.3 in average precision (AP) over the existing state-of-the-art results. Additionally, we provide extensive analyses through ablation studies to elucidate the contributions of each design decision.},
  archive      = {J_TAI},
  author       = {Dinh-Cuong Hoang and Anh-Nhat Nguyen and Chi-Minh Nguyen and An-Binh Phi and Quang-Tri Duong and Khanh-Duong Tran and Viet-Anh Trinh and Van-Duc Tran and Hai-Nam Pham and Phuc-Quan Ngo and Duy-Quang Vu and Thu-Uyen Nguyen and Van-Duc Vu and Duc-Thanh Tran and Van-Thiep Nguyen},
  doi          = {10.1109/TAI.2024.3420848},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5689-5698},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Collision-free grasp detection from color and depth images},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight parallel convolutional neural network with SVM
classifier for satellite imagery classification. <em>TAI</em>,
<em>5</em>(11), 5676–5688. (<a
href="https://doi.org/10.1109/TAI.2024.3423813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite image classification is crucial for various applications, driving advancements in convolutional neural networks (CNNs). While CNNs have proven effective, deep models often encounter overfitting issues as the network&#39;s depth increases since the model has to learn many parameters. Besides this, traditional CNNs have the inherent difficulty of extracting fine-grained details and broader patterns simultaneously. To overcome these challenges, this article presents a novel approach using a lightweight parallel CNN (LPCNN) architecture with a support vector machine (SVM) classifier to classify satellite images. At first, preprocessing such as resizing and sharpening is used to improve image quality. Each branch within the parallel network is designed for specific resolution characteristics, spanning from low (emphasizing broader patterns) to high (capturing fine-grained details), enabling the simultaneous extraction of a comprehensive set of features without increasing network depth. The LPCNN incorporates a dilation factor to expand the network&#39;s receptive field without increasing parameters, and a dropout layer is introduced to mitigate overfitting. SVM is used alongside LPCNN because it is effective at handling high-dimensional features and defining complex decision boundaries, which improves overall classification accuracy. Evaluation of two public datasets (EuroSAT dataset and RSI-CB256 dataset) demonstrates remarkable accuracy rates of 97.91% and 99.8%, surpassing previous state-of-the-art models. Finally, LPCNN, with less than 1 million parameters, outperforms high-parameter models by effectively addressing overfitting issues, showcasing exceptional performance in satellite image classification.},
  archive      = {J_TAI},
  author       = {Priyanti Paul Tumpa and Md. Saiful Islam},
  doi          = {10.1109/TAI.2024.3423813},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5676-5688},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Lightweight parallel convolutional neural network with SVM classifier for satellite imagery classification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). StackAMP: Stacking-based ensemble classifier for
antimicrobial peptide identification. <em>TAI</em>, <em>5</em>(11),
5666–5675. (<a href="https://doi.org/10.1109/TAI.2024.3421176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Antimicrobial peptides (AMPs) play a vital role in the immune defence systems of various organisms and have garnered significant attention for their potential applications in biotechnology and medicine. There are several approaches to identifying AMPs including clinical isolation and characterization, functional genomics, microbiology techniques, and others. However, these methods are mostly expensive, time-consuming, and require well-equipped labs. To overcome these challenges, machine learning models are a potential solution due to their robustness and high predictive capability with less time and cost. In this study, we explored the efficacy of stacking-based ensemble machine-learning techniques to identify AMPs with higher accuracy and precision. Five distinct feature extraction methods, namely amino acid composition, dipeptide composition, moran autocorrelation, geary autocorrelation, and pseudoamino acid composition, were employed to represent the sequence characteristics of peptides. To build robust predictive models, different traditional machine learning algorithms were applied. Additionally, we developed a novel stacking classifier, aptly named StackAMP, to harness the collective power of these algorithms. Our results demonstrated the exceptional performance of the proposed StackAMP ensemble method in AMP identification, achieving an accuracy of 99.97%, 99.93% specificity, and 100% sensitivity. This high accuracy underscores the effectiveness of our approach, which has promising outcomes for the rapid and accurate identification of AMPs in various biological contexts. This study not only contributes to the growing body of knowledge in the field of AMP recognition but also offers a practical tool with potential applications in drug discovery, biotechnology, and disease prevention.},
  archive      = {J_TAI},
  author       = {Tasmin Karim and Md. Shazzad Hossain Shaon and Md. Mamun Ali and Kawsar Ahmed and Francis M. Bui and Li Chen},
  doi          = {10.1109/TAI.2024.3421176},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5666-5675},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {StackAMP: Stacking-based ensemble classifier for antimicrobial peptide identification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning security breach by evolutionary universal
perturbation attack (EUPA). <em>TAI</em>, <em>5</em>(11), 5655–5665. (<a
href="https://doi.org/10.1109/TAI.2024.3429473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The potential for sabotaging deep convolutions neural networks classifiers by universal perturbation attack (UPA) has proved itself as an effective threat to fool deep learning models in sensitive applications such as autonomous vehicles, clinical diagnosis, face recognition, and so on. The prospective application of UPA is for adversarial training of deep convolutional networks against the attacks. Although evolutionary algorithms have already shown their tremendous ability in solving nonconvex complex problems, the literature has limited exploration of evolutionary techniques and strategies for UPA, thus, it needs to be explored on evolutionary algorithms to minimize the magnitude and number of perturbation pixels while maximizing the misclassification of maximum data samples. In this research. This work focuses on utilizing an integer coded genetic algorithm within an evolutionary framework to evolve the UPA. The evolutionary UPA has been structured, analyzed, and compared for two evolutionary optimization structures: 1) constrained single-objective evolutionary UPA; and 2) Pareto double-objective evolutionary UPA. The efficiency of the methodology is analyzed on GoogleNet convolution neural network for its effectiveness on the Imagenet dataset. The results show that under the same experimental conditions, the constrained single objective technique outperforms the Pareto double objective one, and manages a successful breach on a deep network wherein the average detection score falls to $0.446429$ . It is observed that besides the minimization of the detection rate score, the constraint of invisibility of noise is much more effective rather than having a conflicting objective of noise power minimization.},
  archive      = {J_TAI},
  author       = {Neeraj Gupta and Mahdi Khosravy and Antoine Pasquali and Olaf Witkowski},
  doi          = {10.1109/TAI.2024.3429473},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5655-5665},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep learning security breach by evolutionary universal perturbation attack (EUPA)},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated bundle branch block detection using multivariate
fourier–bessel series expansion-based empirical wavelet transform.
<em>TAI</em>, <em>5</em>(11), 5643–5654. (<a
href="https://doi.org/10.1109/TAI.2024.3420259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bundle branch block (BBB) refers to cardiac condition that causes a delay in the path of electrical impulses, which makes it difficult for the heart to pump blood efficiently throughout the body. Early diagnosing BBB is important in cases where prior heart anomalies exist. Generally, the 12-lead electrocardiogram (ECG) is used to detect the BBB. To ease the ECG recording procedure, vectorcardiography (VCG) has been proposed with three leads ECG system. Manual diagnosis of BBB using ECG is subjective to the expertise of the doctor. To facilitate the doctors, in the present study, we have proposed a novel framework to automatically detect BBB from VCG signals using multivariate Fourier–Bessel series expansion-based empirical wavelet transform (MVFBSE-EWT). The MVFBSE-EWT is applied over the three channels of VCG signal, which results in the varying number of multivariate Fourier–Bessel intrinsic mode functions (MVFBIMFs). To process further, first six number of MVFBIMFs are selected due to their presence in the entire dataset. Each MVFBIMF is represented in higher dimensional phase space. From each phase space trajectory, fractal dimension (FD) is computed with three scales. The feature space is reduced with metaheuristic feature selection algorithm.},
  archive      = {J_TAI},
  author       = {Sibghatullah Inayatullah Khan and Ram Bilas Pachori},
  doi          = {10.1109/TAI.2024.3420259},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5643-5654},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Automated bundle branch block detection using multivariate Fourier–Bessel series expansion-based empirical wavelet transform},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating negative sampling approaches for neural topic
models. <em>TAI</em>, <em>5</em>(11), 5630–5642. (<a
href="https://doi.org/10.1109/TAI.2024.3432857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Negative sampling has emerged as an effective technique that enables deep learning models to learn better representations by introducing the paradigm of “learn-to-compare.” The goal of this approach is to add robustness to deep learning models to learn better representation by comparing the positive samples against the negative ones. Despite its numerous demonstrations in various areas of computer vision and natural language processing, a comprehensive study of the effect of negative sampling in an unsupervised domain such as topic modeling has not been well explored. In this article, we present a comprehensive analysis of the impact of different negative sampling strategies on neural topic models. We compare the performance of several popular neural topic models by incorporating a negative sampling technique in the decoder of variational autoencoder-based neural topic models. Experiments on four publicly available datasets demonstrate that integrating negative sampling into topic models results in significant enhancements across multiple aspects, including improved topic coherence, richer topic diversity, and more accurate document classification. Manual evaluations also indicate that the inclusion of negative sampling into neural topic models enhances the quality of the generated topics. These findings highlight the potential of negative sampling as a valuable tool for advancing the effectiveness of neural topic models.},
  archive      = {J_TAI},
  author       = {Suman Adhya and Avishek Lahiri and Debarshi Kumar Sanyal and Partha Pratim Das},
  doi          = {10.1109/TAI.2024.3432857},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5630-5642},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Evaluating negative sampling approaches for neural topic models},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CycleGAN*: Collaborative AI learning with improved
adversarial neural networks for multimodalities data. <em>TAI</em>,
<em>5</em>(11), 5616–5629. (<a
href="https://doi.org/10.1109/TAI.2024.3432856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread adoption of generative adversarial networks (GANs) for sample generation, this article aims to enhance adversarial neural networks to facilitate collaborative artificial intelligence (AI) learning which has been specifically tailored to handle datasets containing multimodalities. Currently, a significant portion of the literature is dedicated to sample generation using GANs, with the objective of enhancing the detection performance of machine learning (ML) classifiers through the incorporation of these generated data into the original training set via adversarial training. The quality of the generated adversarial samples is contingent upon the sufficiency of training data samples. However, in the multimodal domain, the scarcity of multimodal data poses a challenge due to resource constraints. In this article, we address this challenge by proposing a new multimodal dataset generation approach based on the classical audio–visual speech recognition (AVSR) task, utilizing CycleGAN, DiscoGAN, and StyleGAN2 for exploration and performance comparison. AVSR experiments are conducted using the LRS2 and LRS3 corpora. Our experiments reveal that CycleGAN, DiscoGAN, and StyleGAN2 do not effectively address the low-data state problem in AVSR classification. Consequently, we introduce an enhanced model, CycleGAN*, based on the original CycleGAN, which efficiently learns the original dataset features and generates high-quality multimodal data. Experimental results demonstrate that the multimodal datasets generated by our proposed CycleGAN* exhibit significant improvement in word error rate (WER), indicating reduced errors. Notably, the images produced by CycleGAN* exhibit a marked enhancement in overall visual clarity, indicative of its superior generative capabilities. Furthermore, in contrast to traditional approaches, we underscore the significance of collaborative learning. We implement co-training with diverse multimodal data to facilitate information sharing and complementary learning across modalities. This collaborative approach enhances the model’s capability to integrate heterogeneous information, thereby boosting its performance in multimodal environments.},
  archive      = {J_TAI},
  author       = {Yibo He and Kah Phooi Seng and Li Minn Ang},
  doi          = {10.1109/TAI.2024.3432856},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5616-5629},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {CycleGAN*: Collaborative AI learning with improved adversarial neural networks for multimodalities data},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A reliable clinical decision support system for
posttraumatic stress disorder using functional magnetic resonance
imaging data. <em>TAI</em>, <em>5</em>(11), 5605–5615. (<a
href="https://doi.org/10.1109/TAI.2024.3411596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been an upsurge in artificial intelligence (AI) systems. These systems, along with efficient performance and predictability, also need to incorporate the power of explainability and interpretability. This can significantly aid clinical decision support by providing explainable predictions to assist clinicians. Explainability generally involves uncovering key input features important for classification. However, characterizing the uncertainty underlying the decisions of the AI system is an important aspect needed for interpreting the decisions. This is especially important in clinical decision support systems, given considerations of medical ethics such as nonmaleficence and beneficence. In this study, we develop methods for characterizing the decision certainty of machine learning (ML)-based clinical decision support systems. As an illustrative example, we introduce a framework for ML-based posttraumatic stress disorder (PTSD) diagnostic classification that classifies the subjects into pure and mixed classes. Accordingly, a clinician can have very high confidence ( $\geq$ 95% probability) about the diagnosis of a subject in a pure PTSD or combat control class. Remaining sample points for which the AI classification tool does not have very high confidence ( $&amp;lt;$ 95% probability) are grouped into a mixed class. Such a scheme will address ethical considerations of nonmaleficence and beneficence since the clinicians can use the AI system to identify those subjects whose diagnosis has a very high degree of confidence (and proceed with treatment accordingly), and refer those in the uncertain/mixed group to further tests. This is a novel approach, in contrast to the existing framework which aims to maximize classification.},
  archive      = {J_TAI},
  author       = {J. Bhattacharya and A. Gupta and M. N. Dretsch and T. S. Denney and G. Deshpande},
  doi          = {10.1109/TAI.2024.3411596},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5605-5615},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A reliable clinical decision support system for posttraumatic stress disorder using functional magnetic resonance imaging data},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A study of enhancing federated learning on non-IID data with
server learning. <em>TAI</em>, <em>5</em>(11), 5589–5604. (<a
href="https://doi.org/10.1109/TAI.2024.3430250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has emerged as a means of distributed learning using local data stored at clients with a coordinating server. Recent studies showed that FL can suffer from poor performance and slower convergence when training data at the clients are not independent and identically distributed (IID). Here, we consider auxiliary server learning (SL) as a complementary approach to improving the performance of FL on non-IID data. Our analysis and experiments show that this approach can achieve significant improvements in both model accuracy and convergence time even when the dataset utilized by the server is small and its distribution differs from that of the clients’ aggregate data. Moreover, experimental results suggest that auxiliary SL delivers benefits when employed together with other techniques proposed to mitigate the performance degradation of FL on non-IID data.},
  archive      = {J_TAI},
  author       = {Van Sy Mai and Richard J. La and Tao Zhang},
  doi          = {10.1109/TAI.2024.3430250},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5589-5604},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A study of enhancing federated learning on non-IID data with server learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Social NSTransformers: Low-quality pedestrian trajectory
prediction. <em>TAI</em>, <em>5</em>(11), 5575–5588. (<a
href="https://doi.org/10.1109/TAI.2024.3421175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a novel model for low-quality pedestrian trajectory prediction, the social nonstationary transformers (NSTransformers), that merges the strengths of NSTransformers and spatiotemporal graph transformer (STAR). The model can capture social interaction cues among pedestrians and integrate features across spatial and temporal dimensions to enhance the precision and resilience of trajectory predictions. We also propose an enhanced loss function that combines diversity loss with logarithmic root mean squared error (log-RMSE) to guarantee the reasonableness and diversity of the generated trajectories. This design adapts well to complex pedestrian interaction scenarios, thereby improving the reliability and accuracy of trajectory prediction. Furthermore, we integrate a generative adversarial network (GAN) to model the randomness inherent in pedestrian trajectories. Compared to the conventional standard Gaussian distribution, our GAN approach better simulates the intricate distribution found in pedestrian trajectories, enhancing the trajectory prediction&#39;s diversity and robustness. Experimental results reveal that our model outperforms several state-of-the-art methods. This research opens the avenue for future exploration in low-quality pedestrian trajectory prediction.},
  archive      = {J_TAI},
  author       = {Zihan Jiang and Yiqun Ma and Bingyu Shi and Xin Lu and Jian Xing and Nuno Gonçalves and Bo Jin},
  doi          = {10.1109/TAI.2024.3421175},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5575-5588},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Social NSTransformers: Low-quality pedestrian trajectory prediction},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel incentive mechanism for federated learning over
wireless communications. <em>TAI</em>, <em>5</em>(11), 5561–5574. (<a
href="https://doi.org/10.1109/TAI.2024.3419757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies a federated learning system over wireless communications, where a parameter server shares a global model trained by distributed devices. Due to limited communication resources, not all devices can participate in the training process. To encourage suitable devices to participate, this article proposes a novel incentive mechanism, where the parameter server assigns rewards to the devices, and the devices make participation decisions to maximize their overall profit based on the obtained rewards and their energy costs. Based on the interaction between the parameter server and the devices, the proposed incentive mechanism is formulated as a bilevel optimization problem (BOP), in which the upper level optimizes reward factors for the parameter server and the lower level makes participation decisions for the devices. Note that each device needs to make an independent participation decision due to limited communication resources and privacy concerns. To solve this BOP, a bilevel optimization approach called BIMFL is proposed. BIMFL adopts multiagent reinforcement learning (MARL) to make independent participation decisions with local information at the lower level, and introduces multiagent meta-reinforcement learning to accelerate the training by incorporating meta-learning into MARL. Moreover, BIMFL utilizes covariance matrix adaptation evolutionary strategy to optimize reward factors at the upper level. The effectiveness of BIMFL is demonstrated on different datasets using multilayer perceptron and convolutional neural networks.},
  archive      = {J_TAI},
  author       = {Yong Wang and Yu Zhou and Pei-Qiu Huang},
  doi          = {10.1109/TAI.2024.3419757},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5561-5574},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A novel incentive mechanism for federated learning over wireless communications},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Epileptic seizure prediction using stacked CNN-BiLSTM: A
novel approach. <em>TAI</em>, <em>5</em>(11), 5553–5560. (<a
href="https://doi.org/10.1109/TAI.2024.3410928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a novel hybrid architecture for epileptic seizure prediction, utilizing a deep learning approach by stacking the convolutional neural network (CNN) and bidirectional long short-term memory (Bi-LSTM) layers. The proposed approach employs a series of 1-D convolution layers, each with several filters with lengths varying exponentially. The deep Bi-LSTM layers are subsequently integrated to the design to create a densely connected feed-forward structure. The model effectively prioritizes spatiotemporal information, thus extracting key insights for identification of interictal and preictal features. The Boston Children’s Hospital–MIT datasets (Children’s Hospital Boston-Massachusetts Institute of Technology (CHB-MIT)) are utilized and fivefold cross validation is applied for training the model. The proposed model has undergone comprehensive evaluations, with sensitivity of 97.63%, precision of 98.30%, F1-Score of 98.25%, and an area under curve (AUC)-receiver operating characteristic (ROC) of 0.9 across six patients. It can predict seizures 30 min before their onset, allowing individuals ample time to take preventive measures. Compared to the state-of-the-art approach, our model achieves a higher accuracy by 3.44% and demonstrating improved prediction times.},
  archive      = {J_TAI},
  author       = {Zeenat Firdosh Quadri and M. Saqib Akhoon and Sajad A. Loan},
  doi          = {10.1109/TAI.2024.3410928},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5553-5560},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Epileptic seizure prediction using stacked CNN-BiLSTM: A novel approach},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-view masked model for self-supervised graph
representation learning. <em>TAI</em>, <em>5</em>(11), 5540–5552. (<a
href="https://doi.org/10.1109/TAI.2024.3419749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-structured data plays a foundational role in knowledge representation across various intelligent systems. Self-supervised graph representation learning (SSGRL) has emerged as a key methodology for processing such data efficiently. Recent advances in SSGRL have introduced the masked graph model (MGM), which achieves state-of-the-art performance by masking and reconstructing node features. However, the effectiveness of MGM-based methods heavily relies on the information density of the original node features. Performance deteriorates notably when dealing with sparse node features, such as one-hot and degree-hot encodings, commonly found in social and chemical graphs. To address this challenge, we propose a novel cross-view node feature reconstruction method that circumvents direct reliance on the original node features. Our approach generates four distinct views (graph view, masked view, diffusion view, and masked diffusion view) from the original graph through node masking and diffusion. These views are then encoded into representations with high information density. The reconstruction process operates across these representations, enabling self-supervised learning without direct reliance on the original features. Extensive experiments are conducted on 26 real-world graph datasets, including those with sparse and high information density environments. This cross-view reconstruction method represents a promising direction for effective SSGRL, particularly in scenarios with sparse node feature information.},
  archive      = {J_TAI},
  author       = {Haoran Duan and Beibei Yu and Cheng Xie},
  doi          = {10.1109/TAI.2024.3419749},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5540-5552},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Cross-view masked model for self-supervised graph representation learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised exploration via temporal inconsistency in
reinforcement learning. <em>TAI</em>, <em>5</em>(11), 5530–5539. (<a
href="https://doi.org/10.1109/TAI.2024.3413692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In sparse extrinsic reward settings, reinforcement learning remains a challenge despite increasing interest in this field. Existing approaches suggest that intrinsic rewards can alleviate issues caused by reward sparsity. However, many studies overlook the critical role of temporal information, essential for human curiosity. This article introduces a novel intrinsic reward mechanism inspired by human learning processes, where curiosity is evaluated by comparing current observations with historical knowledge. Our method involves training a self-supervised prediction model, periodically saving snapshots of the model parameters, and employing the nuclear norm to assess the temporal inconsistency between predictions from different snapshots as intrinsic rewards. Additionally, we propose a variational weighting mechanism to adaptively assign weights to the snapshots, enhancing the model&#39;s robustness and performance. Experimental results across various benchmark environments demonstrate the efficacy of our approach, which outperforms other state-of-the-art methods without incurring additional training costs and exhibits higher noise tolerance. Our findings indicate that leveraging temporal information in intrinsic rewards can significantly improve exploration performance, motivating future research to develop more robust and accurate reward systems for reinforcement learning.},
  archive      = {J_TAI},
  author       = {Zijian Gao and Kele Xu and Yuanzhao Zhai and Bo Ding and Dawei Feng and Xinjun Mao and Huaimin Wang},
  doi          = {10.1109/TAI.2024.3413692},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5530-5539},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Self-supervised exploration via temporal inconsistency in reinforcement learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring weight distributions and dependence in neural
networks with <span class="math inline"><em>α</em></span>-stable
distributions. <em>TAI</em>, <em>5</em>(11), 5519–5529. (<a
href="https://doi.org/10.1109/TAI.2024.3409673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fundamental use of neural networks is in providing a nonlinear mapping between input and output data with possibly a high number of parameters that can be learned from data directly. Consequently, studying the model&#39;s parameters, particularly the weights, is of paramount importance. The distribution and interdependencies of these weights have a direct impact on the model&#39;s generalizability, compressibility, initialization, and convergence speed. By fitting the weights of pretrained neural networks using the $\alpha$ -stable distributions and conducting statistical tests, we discover widespread heavy-tailed phenomena in neural network weights, with a few layers exhibiting asymmetry. Additionally, we employ a multivariate $\alpha$ -stable distribution to model the weights and explore the relationship between weights within and across layers by calculating the signed symmetric covariation coefficient. The results reveal a strong dependence among certain weights. Our findings indicate that the Gaussian assumption, symmetry assumption, and independence assumption commonly used in neural network research might be inconsistent with reality. In conclusion, our research shows three properties observed in neural network weights: heavy-tailed phenomena, asymmetry, and dependence on certain weights.},
  archive      = {J_TAI},
  author       = {Jipeng Li and Xueqiong Yuan and Ercan Engin Kuruoglu},
  doi          = {10.1109/TAI.2024.3409673},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5519-5529},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Exploring weight distributions and dependence in neural networks with $\alpha$-stable distributions},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tailor-made reinforcement learning approach with advanced
noise optimization for soft continuum robots. <em>TAI</em>,
<em>5</em>(11), 5509–5518. (<a
href="https://doi.org/10.1109/TAI.2024.3440225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advancements in the fusion of reinforcement learning (RL) and soft robotics are presented in this study, with a focus on refining training methodologies for soft planar continuum robots (SPCRs). The proposed modifications to the twin-delayed deep deterministic (TD3) policy gradient algorithm introduce the innovative dynamic harmonic noise (DHN) to enhance exploration adaptability. Additionally, a tailored adaptive task achievement reward (ATAR) is introduced to balance goal achievement, time efficiency, and trajectory smoothness, thereby improving precision in SPCR navigation. Evaluation metrics, including mean squared distance (MSD), mean error (ME), and mean episodic reward (MER), demonstrate robust generalization capabilities. Significant improvements in average reward, success rate, and convergence speed for the proposed modified TD3 algorithm over traditional TD3 are highlighted in the comparative analysis. Specifically, a 45.17% increase in success rate and a 4.92% increase in convergence speed over TD3 are demonstrated by the proposed TD3. Beyond insights into RL and soft robotics, potential applicability of RL in diverse scenarios is underscored, laying the foundation for future breakthroughs in real-world applications.},
  archive      = {J_TAI},
  author       = {Jino Jayan and Lal Priya P.S. and Hari Kumar R.},
  doi          = {10.1109/TAI.2024.3440225},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5509-5518},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Tailor-made reinforcement learning approach with advanced noise optimization for soft continuum robots},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solving orienteering problems by hybridizing evolutionary
algorithm and deep reinforcement learning. <em>TAI</em>, <em>5</em>(11),
5493–5508. (<a href="https://doi.org/10.1109/TAI.2024.3409520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The orienteering problem (OP) is widely applied in real life. However, as the scale of real-world problem scenarios grows quickly, traditional exact, heuristics, and learning-based methods have difficulty balancing optimization accuracy and efficiency. This study proposes a problem decomposition-based double-layer optimization framework named DEA-DYPN to solve OPs. Using a diversity evolutionary algorithm (DEA) as the external optimizer and a dynamic pointer network (DYPN) as the inner optimizer, we significantly reduce the difficulty of solving large-scale OPs. Several targeted optimization operators are innovatively designed for stronger search ability, including a greedy population initialization heuristic, an elite strategy, a population restart mechanism, and a fitness-sharing selection strategy. Moreover, a dynamic embedding mechanism is introduced to DYPN to improve its characteristic learning ability. Extensive comparative experiments on OP instances with sizes from 20 to 500 are conducted for algorithmic performance validation. More experiments and analyses, including the significance test, stability analysis, complexity analysis, sensitivity analysis, and ablation experiments, are also conducted for comprehensive algorithmic evaluation. Experimental results show that our proposed DEA-DYPN ranks first according to the Friedman test and outperforms the competitor algorithms by 69%.},
  archive      = {J_TAI},
  author       = {Rui Wang and Wei Liu and Kaiwen Li and Tao Zhang and Ling Wang and Xin Xu},
  doi          = {10.1109/TAI.2024.3409520},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5493-5508},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Solving orienteering problems by hybridizing evolutionary algorithm and deep reinforcement learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Communication-efficient federated learning for decision
trees. <em>TAI</em>, <em>5</em>(11), 5478–5492. (<a
href="https://doi.org/10.1109/TAI.2024.3433419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing concerns about data privacy and security have driven the emergence of federated learning, which preserves privacy by collaborative learning across multiple clients without sharing their raw data. In this article, we propose a communication-efficient federated learning algorithm for decision trees (DTs), referred to as FL-DT. The key idea is to exchange the statistics of a small number of features among the server and all clients, enabling identification of the optimal feature to split each DT node without compromising privacy. To efficiently find the splitting feature based on the partially available information at each DT node, a novel formulation is derived to estimate the lower and upper bounds of Gini indexes of all features by solving a sequence of mixed-integer convex programming problems. Our experimental results based on various public datasets demonstrate that FL-DT can reduce the communication overhead substantially without surrendering any classification accuracy, compared to other conventional methods.},
  archive      = {J_TAI},
  author       = {Shuo Zhao and Zikun Zhu and Xin Li and Ying-Chi Chen},
  doi          = {10.1109/TAI.2024.3433419},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5478-5492},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Communication-efficient federated learning for decision trees},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous hypergraph embedding for node classification
in dynamic networks. <em>TAI</em>, <em>5</em>(11), 5465–5477. (<a
href="https://doi.org/10.1109/TAI.2024.3450658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs are a foundational way to represent scenarios where objects interact in pairs. Recently, graph neural networks (GNNs) have become widely used for modeling simple graph structures, either in homogeneous or heterogeneous graphs, where edges represent pairwise relationships between nodes. However, many real-world situations involve more complex interactions where multiple nodes interact simultaneously, as observed in contexts such as social groups and gene-gene interactions. Traditional graph embeddings often fail to capture these multifaceted nonpairwise dynamics. A hypergraph, which generalizes a simple graph by connecting two or more nodes via a single hyperedge, offers a more efficient way to represent these interactions. While most existing research focuses on homogeneous and static hypergraph embeddings, many real-world networks are inherently heterogeneous and dynamic. To address this gap, we propose a GNN-based embedding for dynamic heterogeneous hypergraphs, specifically designed to capture nonpairwise interactions and their evolution over time. Unlike traditional embedding methods that rely on distance or meta-path-based strategies for node neighborhood aggregation, a $k$ -hop neighborhood strategy is introduced to effectively encapsulate higher-order interactions in dynamic networks. Furthermore, the information aggregation process is enhanced by incorporating semantic hyperedges, further enriching hypergraph embeddings. Finally, embeddings learned from each timestamp are aggregated using a mean operation to derive the final node embeddings. Extensive experiments on five real-world datasets, along with comparisons against homogeneous, heterogeneous, and hypergraph-based baselines (both static and dynamic), demonstrate the robustness and superiority of our model.},
  archive      = {J_TAI},
  author       = {Malik Khizar Hayat and Shan Xue and Jia Wu and Jian Yang},
  doi          = {10.1109/TAI.2024.3450658},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5465-5477},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Heterogeneous hypergraph embedding for node classification in dynamic networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal trajectory-based control of 3-d dual rotary cranes
for payload dynamic regulation in complex environments. <em>TAI</em>,
<em>5</em>(11), 5452–5464. (<a
href="https://doi.org/10.1109/TAI.2024.3421172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With flexible payload adjustment ability and large load capacity, dual rotary cranes (DRCs) provide effective solutions for various complex hoisting tasks. At present, the control research for DRCs mostly focuses on two-dimensional space (restricting workspace and efficiency), or lacks the consideration of DRC dynamic characteristics and the practical demands for the dynamic regulation of payload positions and attitudes, which makes it difficult to handle hoisting tasks in complex environments. To tackle these issues, this article proposes an optimal trajectory-based motion control method for three-dimensional (3-D) DRCs in complex environments, effectively tackling key challenges encountered by DRCs operating in 3-D space. The proposed method achieves dynamic regulation of payload position and attitude by DRCs in 3-D space for the first time, constraining payload velocity and acceleration within reasonable ranges while avoiding obstacles, which represents an advancement in enhancing the efficiency and safety of 3-D DRC operations in complex environments. Specifically, the coupling relationship between the actuated boom motions and the non-actuated payload motions in 3-D space is mathematically solved, which provides the foundation of indirect payload regulation through boom control. Moreover, by introducing multiple performance indicators during optimization, the proposed method ensures satisfactory payload transient performance while maintaining a safe distance from obstacles. Additionally, by the analysis of steady-state equilibrium conditions and the reasonable passing time allocation of virtual via-points, coordinated boom motions with payload swing suppression are realized, ensuring transportation smoothness. Finally, hardware experiments are conducted considering collision-free payload transportation through reciprocating boom pitch/rotation motions, which verifies the effectiveness and practical performance of the proposed method.},
  archive      = {J_TAI},
  author       = {Zhuoqing Liu and Tong Yang and Yongchun Fang and Ning Sun},
  doi          = {10.1109/TAI.2024.3421172},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5452-5464},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Optimal trajectory-based control of 3-D dual rotary cranes for payload dynamic regulation in complex environments},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differentiation of benign and malignant masses in mammogram
using 2D-fourier–bessel intrinsic band functions and improved feature
space. <em>TAI</em>, <em>5</em>(11), 5442–5451. (<a
href="https://doi.org/10.1109/TAI.2024.3396800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a framework based on the 2D-Fourier–Bessel decomposition method (2D-FBDM) and improved feature space for the automatic diagnosis of benign and malignant masses in mammograms. For analysis purposes, a curated breast imaging subset of the digital database for screening mammography (CBIS-DDSM) is used. Haralick texture features are used to extract finesse, coarse or smoothness, and irregularities in 2D-Fourier–Bessel intrinsic band functions which are obtained by 2D-FBDM. Linear regression-based improved feature space is produced and effects on classification performance are analyzed after ensembling them with old feature space. For CBIS-DDSM, the accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve obtained by the proposed framework are 99.06%, 98.48%, 99.74%, and 0.99, respectively. The mini-mammographic image analysis society database is also analyzed to show the robustness of the proposed framework.},
  archive      = {J_TAI},
  author       = {Pradeep Kumar Chaudhary and Ram Bilas Pachori},
  doi          = {10.1109/TAI.2024.3396800},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5442-5451},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Differentiation of benign and malignant masses in mammogram using 2D-Fourier–Bessel intrinsic band functions and improved feature space},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Alternating excitation–inhibition dendritic computing for
classification. <em>TAI</em>, <em>5</em>(11), 5431–5441. (<a
href="https://doi.org/10.1109/TAI.2024.3416236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The addition of dendritic inhibition has been shown to significantly enhance the computational and representational capabilities of neurons. However, this inhibitory mechanism is mostly ignored in the existing artificial neural networks (ANNs). In this article, we propose the alternating excitatory and inhibitory mechanisms and use them to construct an ANN-based dendritic neuron, the alternating excitation–inhibition dendritic neuron model (ADNM). Subsequently, a comprehensive multilayer neural system named the alternating excitation–inhibition dendritic neuron system (ADNS) is constructed by networking multiple ADNMs. To evaluate the performance of ADNS, a series of extensive experiments are implemented to compare it with other state-of-the-art networks on a diverse set consisting of 47 feature-based classification datasets and two image-based classification datasets. The experimental results demonstrate that ADNS outperforms its competitors in classification tasks. In addition, the impact of different hyperparameters on the performance of the neural model is analyzed and discussed. In summary, the study provides a novel dendritic neuron model (DNM) with better performance and interpretability for practical classification tasks.},
  archive      = {J_TAI},
  author       = {Jiayi Li and Zhenyu Lei and Zhiming Zhang and Haotian Li and Yuki Todo and Shangce Gao},
  doi          = {10.1109/TAI.2024.3416236},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5431-5441},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Alternating Excitation–Inhibition dendritic computing for classification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CrackLens: Automated sidewalk crack detection and
segmentation. <em>TAI</em>, <em>5</em>(11), 5418–5430. (<a
href="https://doi.org/10.1109/TAI.2024.3435608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic sidewalk crack detection is necessary for urban infrastructure maintenance to ensure pedestrian safety. Such a task becomes complex on overgrown sidewalks, where crack detection usually misjudges vegetation as cracks. A lack of automated crack detection targets overgrown sidewalk problems; most crack detection focuses on vehicular roadway cracks that are recognizable even at the aerial photography level. Hence, this article introduces CrackLens, an automated sidewalk crack detection framework capable of detecting cracks even on overgrown sidewalks. We include several contributions as follows. First, we designed an automatic data parser using a red, green, and blue (RGB)-depth fusion sidewalk dataset we collected. The RGB and depth information are combined to create depth-embedded matrices, which are used to prelabel and separate the collected dataset into two categories (with and without crack). Second, we created an automatic annotation process using image processing methods and tailored the tool only to annotate cracks on overgrown sidewalks. This process is followed by a binary classification for verification, allowing the tool to target overgrown problems on sidewalks. Lastly, we explored the robustness of our framework by experimenting with it using 8,000 real sidewalk images with some overgrown problems. The evaluation leveraged several transformer-based neural network models. Our framework achieves substantial crack detection and segmentation in overgrown sidewalks by addressing the challenges of limited data and subjective manual annotations.},
  archive      = {J_TAI},
  author       = {Chan Young Koh and Mohamed Ali and Abdeltawab Hendawi},
  doi          = {10.1109/TAI.2024.3435608},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5418-5430},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {CrackLens: Automated sidewalk crack detection and segmentation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SSpose: Self-supervised spatial-aware model for human pose
estimation. <em>TAI</em>, <em>5</em>(11), 5403–5417. (<a
href="https://doi.org/10.1109/TAI.2024.3440220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation (HPE) relies on the anatomical relationships among different body parts to locate keypoints. Despite the significant progress achieved by convolutional neural networks (CNN)-based models in HPE, they typically fail to explicitly learn the global dependencies among various body parts. To overcome this limitation, we propose a spatial-aware HPE model called SSpose that explicitly captures the spatial dependencies between specific key points and different locations in an image. The proposed SSpose model adopts a hybrid CNN-Transformer encoder to simultaneously capture local features and global dependencies. To better preserve image details, a multiscale fusion module is introduced to integrate coarse- and fine-grained image information. By establishing a connection with the activation maximization (AM) principle, the final attention layer of the Transformer aggregates contributions (i.e., attention scores) from all image positions and forms the maximum position in the heatmap, thereby achieving keypoint localization in the head structure. Additionally, to address the issue of visible information leakage in convolutional reconstruction, we have devised a self-supervised training framework for the SSpose model. This framework incorporates mask autoencoder (MAE) technology into SSpose models by utilizing masked convolution and hierarchical masking strategy, thereby facilitating efficient self-supervised learning. Extensive experiments demonstrate that SSpose performs exceptionally well in the pose estimation task. On the COCO val set, it achieves an AP and AR of 77.3% and 82.1%, respectively, while on the COCO test-dev set, the AP and AR are 76.4% and 81.5%. Moreover, the model exhibits strong generalization capabilities on MPII.},
  archive      = {J_TAI},
  author       = {Linfang Yu and Zhen Qin and Liqun Xu and Zhiguang Qin and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TAI.2024.3440220},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5403-5417},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {SSpose: Self-supervised spatial-aware model for human pose estimation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 360° high-resolution depth estimation via uncertainty-aware
structural knowledge transfer. <em>TAI</em>, <em>5</em>(11), 5392–5402.
(<a href="https://doi.org/10.1109/TAI.2024.3427068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To predict high-resolution (HR) omnidirectional depth maps, existing methods typically leverage HR omnidirectional image (ODI) as the input via fully supervised learning. However, in practice, taking HR ODI as input is undesired due to resource-constrained devices. In addition, depth maps are often with lower resolution than color images. Therefore, in this article, we explore for the first time to estimate the HR omnidirectional depth directly from a low-resolution (LR) ODI, when no HR depth ground truth (GT) map is available. Our key idea is to transfer the scene structural knowledge from the HR image modality and the corresponding LR depth maps to achieve the goal of HR depth estimation without any extra inference cost. Specifically, we introduce ODI super-resolution (SR) as an auxiliary task and train both tasks collaboratively in a weakly supervised manner to boost the performance of HR depth estimation. The ODI SR task extracts the scene structural knowledge via uncertainty estimation. Buttressed by this, a scene structural knowledge transfer (SSKT) module is proposed with two key components. First, we employ a cylindrical implicit interpolation function (CIIF) to learn cylindrical neural interpolation weights for feature up-sampling and share the parameters of CIIFs between the two tasks. Then, we propose a feature distillation (FD) loss that provides extra structural regularization to help the HR depth estimation task learn more scene structural knowledge. Extensive experiments demonstrate that our weakly supervised method outperforms baseline methods, and even achieves comparable performance with the fully supervised methods.},
  archive      = {J_TAI},
  author       = {Zidong Cao and Hao Ai and Athanasios V. Vasilakos and Lin Wang},
  doi          = {10.1109/TAI.2024.3427068},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5392-5402},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {360° high-resolution depth estimation via uncertainty-aware structural knowledge transfer},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intelligent multigrade brain tumor identification in MRI: A
metaheuristic-based uncertain set framework. <em>TAI</em>,
<em>5</em>(11), 5381–5391. (<a
href="https://doi.org/10.1109/TAI.2024.3441520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research intends to address the critical need for precise brain tumor prediction through the development of an automated method that entwines the Firefly (FF) algorithm and the interval type-II fuzzy (IT2FLS) technique. The proposed method improves tumor delineation in complex brain tissue by using the FF algorithm to find possible cluster positions and the IT2FLS system for final clustering. This algorithm demonstrates its versatility by processing diverse image sequences from BRATS challenge datasets (2017, 2018, and 2020), which encompass varying levels of complexity. Through comprehensive evaluation metrics such as sensitivity, specificity, and dice-overlap index (DOI), the proposed algorithm consistently yields improved segmentation results. Ultimately, this research aims to augment oncologists&#39; perceptual acumen, facilitating enhanced intuition and comprehension of patients&#39; conditions, thereby advancing decision-making capabilities in medical research.},
  archive      = {J_TAI},
  author       = {Saravanan Alagarsamy and Vishnuvarthanan Govindaraj and A. Shahina and D. Nagarajan},
  doi          = {10.1109/TAI.2024.3441520},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5381-5391},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Intelligent multigrade brain tumor identification in MRI: A metaheuristic-based uncertain set framework},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy scheduling optimization for microgrids based on
partially observable markov game. <em>TAI</em>, <em>5</em>(11),
5371–5380. (<a href="https://doi.org/10.1109/TAI.2024.3428510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microgrids (MGs) are essential for enhancing energy efficiency and minimizing power usage through the regulation of energy storage systems. Nevertheless, privacy-related concerns obstruct the real-time precise regulation of these systems due to unavailable state-of-charge (SOC) data. This article introduces a self-adaptive energy scheduling optimization framework for MGs that operates without SOC information, utilizing a partially observable Markov game (POMG) to decrease energy usage. Furthermore, to develop an optimal energy scheduling strategy, a MG system optimization approach using recurrent multiagent deep deterministic policy gradient (RMADDPG) is presented. This method is evaluated against other existing techniques such as MADDPG, deterministic recurrent policy gradient (DRPG), and independent Q-learning (IQL), demonstrating reductions in electrical energy consumption by 4.29%, 5.56%, and 12.95%, respectively, according to simulation outcomes.},
  archive      = {J_TAI},
  author       = {Jiakai Gong and Nuo Yu and Fen Han and Bin Tang and Haolong Wu and Yuan Ge},
  doi          = {10.1109/TAI.2024.3428510},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {11},
  number       = {11},
  pages        = {5371-5380},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Energy scheduling optimization for microgrids based on partially observable markov game},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Expert knowledge driven human-AI collaboration for medical
imaging: A study on epileptic seizure onset zone identification.
<em>TAI</em>, <em>5</em>(10), 5352–5368. (<a
href="https://doi.org/10.1109/TAI.2024.3396421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised artificial intelligence (AI) techniques are good at learning class specific characteristic properties despite variance across samples. However, for rare class classification, challenges arise due to class imbalance. On the contrary, knowledge-based techniques encode class specific information without class labels but face difficulties in parsing knowledge which is often vague, uncertain, and result in high intraclass variability. This manuscript presents a human-AI collaboration methodology to integrate AI with expert knowledge for rare class classification, mitigating class imbalance and intraclass variability effects. We present a formal framework for expert knowledge representation using logical connectives of atomic propositions, a rule refinement strategy to derive class specific machine checkable formulas, and a rule implementation strategy that extracts explainable partitions of rare class expert rules for its recognition. A knowledge-AI integration strategy is presented that uses entropy imbalance gain and Gini index to quantify class imbalance and intraclass variability, and orchestrates supervised AI and expert knowledge machines to effectively identify rare class through human-AI collaboration with reduced human effort. We apply the proposed integration framework to develop DeepXSOZ that identifies seizure onset zones (SOZ) in focal epilepsy patients from resting state functional magnetic resonance imaging. DeepXSOZ&#39;s performance is validated on multicenter datasets against anatomical MRI-based manual SOZ identification, and Engel outcomes after surgical SOZ alteration. This human-AI collaboration demonstrates increased F1 score compared with state-of-the-art “AI-only” techniques, minimal data leakage effect with statistically similar performance across multicenter datasets without fine tuning, consistent results across age and gender, and reduced manual effort.},
  archive      = {J_TAI},
  author       = {Payal Kamboj and Ayan Banerjee and Sandeep K. S. Gupta},
  doi          = {10.1109/TAI.2024.3396421},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5352-5368},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Expert knowledge driven human-AI collaboration for medical imaging: A study on epileptic seizure onset zone identification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward correlated sequential rules. <em>TAI</em>,
<em>5</em>(10), 5340–5351. (<a
href="https://doi.org/10.1109/TAI.2024.3429306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of high-utility sequential pattern mining (HUSPM) is to efficiently discover profitable or useful sequential patterns in a large number of sequences. However, simply being aware of utility-eligible patterns is insufficient for making predictions. To compensate for this deficiency, high-utility sequential rule mining (HUSRM) is designed to explore the confidence or probability of predicting the occurrence of consequence sequential patterns based on the appearance of premise sequential patterns. It has numerous applications, such as product recommendation and weather prediction. However, the existing algorithm, known as HUSRM, is limited to extracting all eligible rules while neglecting the correlation between the generated sequential rules. To address this issue, we propose a novel algorithm called correlated high-utility sequential rule miner (CoUSR) to integrate the concept of correlation into HUSRM. The proposed algorithm requires not only that each rule be correlated but also that the patterns in the antecedent and consequent of the high-utility sequential rule be correlated. The algorithm adopts a utility-list structure to avoid multiple database scans. Additionally, several pruning strategies are used to improve the algorithm&#39;s efficiency and performance. Based on several real-world datasets, subsequent experiments demonstrated that CoUSR is effective and efficient in terms of operation time and memory consumption. All codes are accessible on GitHub: https://github.com/DSI-Lab1/CoUSR .},
  archive      = {J_TAI},
  author       = {Lili Chen and Wensheng Gan and Chien-Ming Chen},
  doi          = {10.1109/TAI.2024.3429306},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5340-5351},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Toward correlated sequential rules},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IN-GFD: An interpretable graph fraud detection model for
spam reviews. <em>TAI</em>, <em>5</em>(10), 5325–5339. (<a
href="https://doi.org/10.1109/TAI.2024.3420262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of the e-commerce platform, more and more reviews of its various formats continue to appear. Reviews help people buy the right item faster, and instead, spam reviews reduce the user experience. To be able to detect spam reviews, statistical machine learning-based methods were commonly used in the past, but these approaches ignored the correlation between reviews. With the development of the graph fraud detection model, people have started to graph model the review data. However, typical graph fraud detection models still have problems with interpretability. Therefore, we propose here an interpretable graph fraud detection model for spam reviews, which is also named IN-GFD. As for the interpretability issue, we leveraged the relationship against the predicted score and whether a review is spam or not to build a loss function on top of the feature-embedding matrix, and introduced a scoring difference threshold mechanism, which can allow our model to have antehoc interpretability. In addition, to address class imbalance issues, IN-GFD utilizes the oversampling of the spam nodes to balance them with normal nodes and introduces an edge-loss function to learn new edge relationships. After extensive experiments, our method proves to be better than other state-of-the-arts (SOTA) models in terms of fraud detection and offers the benefit of interpretability. Finally, our study combines detection models with antehoc interpretability, offering a promising direction in review detection. Our approach has wide applicability, detecting spam reviews in datasets with user reviews and providing reasonable interpretations.},
  archive      = {J_TAI},
  author       = {Hang Yu and Weixu Liu and Nengjun Zhu and Pengbo Li and Xiangfeng Luo},
  doi          = {10.1109/TAI.2024.3420262},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5325-5339},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {IN-GFD: An interpretable graph fraud detection model for spam reviews},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Disentangled cross-modal fusion for event-guided image
super-resolution. <em>TAI</em>, <em>5</em>(10), 5314–5324. (<a
href="https://doi.org/10.1109/TAI.2024.3418376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event cameras detect the intensity changes and produce asynchronous events with high dynamic range and no motion blur. Recently, several attempts have been made to superresolve the intensity images guided by events. However, these methods directly fuse the event and image features without distinguishing the modality difference and achieve image superresolution (SR) in multiple steps, leading to error-prone image SR results. Also, they lack quantitative evaluation of real-world data. In this article, we present an end-to-end framework, called event-guided image (EGI)-SR to narrow the modality gap and subtly integrate the event and RGB modality features for effective image SR. Specifically, EGI-SR employs three crossmodality encoders (CME) to learn modality-specific and modality-shared features from the stacked events and the intensity image, respectively. As such, EGI-SR can better mitigate the negative impact of modality varieties and reduce the difference in the feature space between the events and the intensity image. Subsequently, a transformer-based decoder is deployed to reconstruct the SR image. Moreover, we collect a real-world dataset, with temporally and spatially aligned events and color image pairs. We conduct extensive experiments on the synthetic and real-world datasets, showing EGI-SR favorably surpassing the existing methods by a large margin.},
  archive      = {J_TAI},
  author       = {Minjie Liu and Hongjian Wang and Kuk-Jin Yoon and Lin Wang},
  doi          = {10.1109/TAI.2024.3418376},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5314-5324},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Disentangled cross-modal fusion for event-guided image super-resolution},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-driven model predictive control for hybrid charging
stations using ensemble learning. <em>TAI</em>, <em>5</em>(10),
5304–5313. (<a href="https://doi.org/10.1109/TAI.2024.3404913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An increased demand in electric vehicle (EV) charging facilities has necessitated intelligent energy management systems (EMSs), to control and monitor the available energy sources in these charging stations. The goal is to create a charging schedule for EVs that minimizes the operating cost of the charging station while ensuring all connected EV&#39;s charging demands. Model predictive control (MPC) has been widely used for EMS. The challenge with MPC is that a precise representation of the underlying physical system&#39;s dynamics is essential. In this study, machine learning methods are combined with conventional MPC to build a data-driven MPC (DMPC) which can adapt to the changes in the system&#39;s behavior over time. As new data become available, the data-driven model can be updated and the MPC algorithm can be reoptimized to reflect the current behavior of the system. Ensemble learning is an effective machine learning technique that increases the effectiveness and accuracy of decision making by utilizing the combined knowledge of several models. Out of the several methods available for implementing ensemble learning, adaptive random forest (ARF) algorithm with affine functions and convex optimization is selected. The results show comparable performance of DMPC with respect to MPC implemented on a well-established mathematical model of the system.},
  archive      = {J_TAI},
  author       = {G. S. Asha Rani and P. S. Lal Priya},
  doi          = {10.1109/TAI.2024.3404913},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5304-5313},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Data-driven model predictive control for hybrid charging stations using ensemble learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparative evaluation in the wild: Systems for the
expressive rendering of music. <em>TAI</em>, <em>5</em>(10), 5290–5303.
(<a href="https://doi.org/10.1109/TAI.2024.3408717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There have been many attempts to model the ability of human musicians to take a score and perform or render it expressively, by adding tempo, timing, loudness, and articulation changes to nonexpressive music data. While expressive rendering models exist in academic research, most of these are not open source or accessible, meaning they are difficult to evaluate empirically and have not been widely adopted in professional music software. Systematic comparative evaluation of such algorithms stopped after the last performance rendering contest (RENCON) in 2013, making it difficult to compare newer models to existing work in a fair and valid way. In this article, we introduce the first transformer-based model for expressive rendering, cue-free express + pedal (CFE + P), which predicts expressive attributes such as notewise dynamics and micro-timing adjustments, and beatwise tempo and sustain pedal use based only on the start and end times and pitches of notes (e.g., inexpressive musical instrument digital interface (MIDI) input). We perform two comparative evaluations on our model against a nonmachine learning baseline taken from professional music software and two open-source algorithms—a feedforward neural network (FFNN) and hierarchical recurrent neural network (HRNN). The results of two listening studies indicate that our model renders passages that outperform what can be done in professional music software such as Logic Pro and Ableton Live. 1 1All data and preexisting hypotheses can be accessed via the Open Science Foundation: https://osf.io/6uwjk/.},
  archive      = {J_TAI},
  author       = {Kyle Worrall and Zongyu Yin and Tom Collins},
  doi          = {10.1109/TAI.2024.3408717},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5290-5303},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Comparative evaluation in the wild: Systems for the expressive rendering of music},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic combination forecasting for short-term photovoltaic
power. <em>TAI</em>, <em>5</em>(10), 5277–5289. (<a
href="https://doi.org/10.1109/TAI.2024.3404408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate short-term photovoltaic (PV) power prediction can be crucial for fault detection of the control system and reducing the fault of the PV output control system. However, PV power is highly volatile, and significant power fluctuations cannot be adapted to by the combined model when predicting, thus affecting the stable operation of the PV output control system. In response to this issue, a dynamic combination short-term PV power prediction model of temporal convolutional network (TCN)-bidirectional gated recurrent unit network (BiGRU) and TCN-bidirectional long-short term memory network (BiLSTM) based on complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN) is proposed. CEEMDAN is employed to decompose the original PV power data to reduce the volatility of the original data. Constructing two combined models, TCN-BiGRU and TCN-BiLSTM, and training them separately. Introducing ElasticNet, which utilizes both L1 and L2 regularization terms. This approach preserves the sparsity from least absolute shrinkage and selection operator (LASSO) regression regularization while incorporating the smoothness from Ridge regression regularization, effectively avoiding the issue of the combined model getting trapped in a local optimum. In the end, experimental verification is conducted using actual measurement data from a solar power facility in Gansu, China, and another in Xinjiang, China. The simulation results illustrate that the accuracy of PV power prediction can be significantly improved by the proposed forecasting approach. In comparison with the control experiment, the R2 of the Gansu dataset increased by 0.32% at least, and the R2 of the Xinjiang dataset increased by 0.66% at least.},
  archive      = {J_TAI},
  author       = {Yu Huang and Jiaxing Liu and Zongshi Zhang and Dui Li and Xuxin Li and Guang Wang},
  doi          = {10.1109/TAI.2024.3404408},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5277-5289},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Dynamic combination forecasting for short-term photovoltaic power},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stable learning via triplex learning. <em>TAI</em>,
<em>5</em>(10), 5267–5276. (<a
href="https://doi.org/10.1109/TAI.2024.3404411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stable learning aims to learn a model that generalizes well to arbitrary unseen target domain by leveraging a single source domain. Recent advances in stable learning have focused on balancing the distribution of confounders for each feature to eliminate spurious correlations. However, previous studies treat all features equally without considering the difficulties of confounder balancing associated with different features, and regard irrelevant features as confounders, deteriorating generalization performance. To tackle these issues, this article proposes a novel triplex learning (TriL) based stable learning algorithm, which performs sample reweighting, causal feature selection, and representation learning to remove spurious correlations. Specifically, first, TriL adaptively assigns weights to the confounder balancing term of each feature in accordance with the difficulties of confounder balancing, and aligns the confounder distribution of each feature by learning a group of sample weights. Second, TriL integrates the sample weights into a weighted cross-entropy model to compute causal effects of features for excluding irrelevant features from the confounder set. Finally, TriL relearns a set of sample weights and uses them to guide a new supervised dual-autoencoder containing two classifiers to learn feature representations. TriL forces the results of two classifiers to remain consistent for removing spurious correlations by using a cross-classifier consistency regularization. Extensive experiments on synthetic and two real-world datasets show the superiority of TriL compared with seven methods.},
  archive      = {J_TAI},
  author       = {Shuai Yang and Tingting Jiang and Qianlong Dang and Lichuan Gu and Xindong Wu},
  doi          = {10.1109/TAI.2024.3404411},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5267-5276},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Stable learning via triplex learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel technique of synthetic data generation for asset
administration shells in industry 4.0 scenarios. <em>TAI</em>,
<em>5</em>(10), 5258–5266. (<a
href="https://doi.org/10.1109/TAI.2024.3409516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manufacturing plants are highly dependent on machines and involve a significant number of equipment to produce a finished product. Industry 4.0 helps structure the processes involved in such setups and enables the functionalities of how the equipment and machines interact with each other. With the advancement of visualizing these types of equipment as digital twins, multiple opportunities have developed for automating processes and optimizing various aspects of the assembly, especially for original equipment manufacturers (OEMs). One problem that concerns a network of manufacturers is the availability of equipment and spare parts data which are sometimes confidential but are required by a new member in the network for several analytical applications. This article looks at this problem statement to turn this into an opportunity by introducing a novel concept of AASGAN that combines the knowledge representation of a digital twin data in the asset administration shell (AAS) and a synthetic data generation technique of generative adversarial network (GAN) to generate fake data that is identical to real data. This article also explains how this concept helps perform analytical operations using industry grade solutions for the automotive industry available for managing digital twins and other scenarios for industrial automation.},
  archive      = {J_TAI},
  author       = {Suman De and Pabitra Mitra},
  doi          = {10.1109/TAI.2024.3409516},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5258-5266},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A novel technique of synthetic data generation for asset administration shells in industry 4.0 scenarios},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid intelligent optimization of nonlinear switched
systems with guaranteed feasibility. <em>TAI</em>, <em>5</em>(10),
5244–5257. (<a href="https://doi.org/10.1109/TAI.2024.3408130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the challenge of globally optimal control of path-constrained switched systems, a hybrid intelligent dynamic optimization method is proposed by combining the biobjective particle swarm optimization (PSO) method and a gradient descent method, which simultaneously obtains globally optimal switching instants and input and guarantees rigorous satisfaction of the path constraints over the continuous time horizon. First, the path constraint of switched systems is discretized into multiple point constraints, and then the right-hand side of the path constraint ( $\leq 0$ ) is substituted with a negative value ( $\leq-\varepsilon$ ). Second, the single-objective constrained dynamic program of switched systems is transformed into a biobjective unconstrained dynamic program where each particle intelligently adjusts its objectives to detect the global optimum area satisfying the constraints, depending on its current position in the search space by the search mechanism of PSO. Third, the deterministic optimization method is deployed in the detected global optimum area to locate a feasible solution satisfying the Karush–Kuhn–Tucker (KKT) conditions to a specified tolerance of dynamic optimization of switched systems. Moreover, it is proved that the hybrid intelligent dynamic optimization method can obtain the optimal solution satisfying the first-order approximation KKT conditions within a finite number of iterations. Finally, the results of numerical simulations show the effectiveness of the presented method in terms of improving the solution accuracy and guaranteeing rigorous satisfaction of the path constraint.},
  archive      = {J_TAI},
  author       = {Huan Li and Jun Fu and Tianyou Chai},
  doi          = {10.1109/TAI.2024.3408130},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5244-5257},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Hybrid intelligent optimization of nonlinear switched systems with guaranteed feasibility},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multivariate time-series modeling and forecasting with
parallelized convolution and decomposed sparse-transformer.
<em>TAI</em>, <em>5</em>(10), 5232–5243. (<a
href="https://doi.org/10.1109/TAI.2024.3410934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world scenarios require accurate predictions of time series, especially in the case of long sequence time-series forecasting (LSTF), such as predicting traffic flow and electricity consumption. However, existing time-series prediction models encounter certain limitations. First, they struggle with mapping the multidimensional information present in each time step to high dimensions, resulting in information coupling and increased prediction difficulty. Second, these models fail to effectively decompose the intertwined temporal patterns within the time series, which hinders their ability to learn more predictable features. To overcome these challenges, we propose a novel end-to-end LSTF model with parallelized convolution and decomposed sparse-Transformer (PCDformer). PCDformer achieves the decoupling of input sequences by parallelizing the convolutional layers, enabling the simultaneous processing of different variables within the input sequence. To decompose distinct temporal patterns, PCDformer incorporates a temporal decomposition module within the encoder–decoder structure, effectively separating the input sequence into predictable seasonal and trend components. Additionally, to capture the correlation between variables and mitigate the impact of irrelevant information, PCDformer utilizes a sparse self-attention mechanism. Extensive experimentation conducted on five diverse datasets demonstrates the superior performance of PCDformer in LSTF tasks compared to existing approaches, particularly outperforming encoder–decoder-based models.},
  archive      = {J_TAI},
  author       = {Shusen Ma and Yun-Bo Zhao and Yu Kang and Peng Bai},
  doi          = {10.1109/TAI.2024.3410934},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5232-5243},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multivariate time-series modeling and forecasting with parallelized convolution and decomposed sparse-transformer},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning-based time-synchronized optimized
control for affine systems. <em>TAI</em>, <em>5</em>(10), 5216–5231. (<a
href="https://doi.org/10.1109/TAI.2024.3420261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The approach of (fixed-) time-synchronized control (FTSC) aims at attaining the outcome where all the system state-variables converge to the origin simultaneously/synchronously. This type of outcome can be the highly essential performance desired in various real-world high-precision control applications. Toward this objective, this article proposes and investigates the development of a time-synchronized reinforcement learning algorithm (TSRL) applicable to a particular class of first- and second-order affine nonlinear systems. The approach developed here appropriately incorporates the norm-normalized sign function into the optimal system control design, leveraging on the special properties of this norm-normalized sign function in attaining time-synchronized stability and control. Concurrently, the actor–critic framework in reinforcement learning (RL) is invoked, and the dual quantities of system control and gradient term of the cost function are decomposed with appropriate time-synchronized control items and unknown actor/critic part and to be learned independently. By additionally employing the adaptive dynamic programming technique, the solution of the Hamilton–Jacobi–Bellman equation is iteratively approximated under this actor–critic framework. As an outcome, the proposed TSRL method optimizes the system control while attaining the notable time-synchronized convergence property. The performance and effectiveness of the proposed method are demonstrated to be effectively applicable via detailed numerical studies and on an autonomous vehicle nonlinear system motion control problem.},
  archive      = {J_TAI},
  author       = {Yuxiang Zhang and Xiaoling Liang and Dongyu Li and Shuzhi Sam Ge and Bingzhao Gao and Hong Chen and Tong Heng Lee},
  doi          = {10.1109/TAI.2024.3420261},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5216-5231},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Reinforcement learning-based time-synchronized optimized control for affine systems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial intelligence-driven framework for augmented
reality markerless navigation in knee surgery. <em>TAI</em>,
<em>5</em>(10), 5205–5215. (<a
href="https://doi.org/10.1109/TAI.2024.3429048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional orthopedic navigation systems depend on marker-based tracking, which may introduce additional skin incisions, increase the risk and discomfort for the patient, and entail increased workflow complexity. The guidance is conveyed via 2-D monitors, which may distract the surgeon and increase the cognitive burden. This study presents an artificial intelligence (AI)—driven surgical navigation framework for knee replacement surgery. The system comprises an augmented reality (AR) interface that combines an occlusions-robust deep learning-based markerless bone tracking and registration algorithm with a commercial HoloLens 2 headset calibrated for the user&#39;s perspective on both eyes. The feasibility of such a system in navigating a bone drilling task is investigated with an experienced orthopedic surgeon on three cadaveric knees under realistic operating room (OR) conditions. After registering an implant model to computed tomography (CT) scans, the preoperative plans are determined based on the location of the fixation pins. Navigation accuracy is quantified using a highly accurate optical tracking system. The achieved drilling error is 7.88 $\pm$ 2.41 mm in translation and 7.36 $\pm$ 1.77 ${}^{\boldsymbol{\circ}}$ in orientation. The results demonstrate the viability of integrating AI and AR technology to navigate knee surgery.},
  archive      = {J_TAI},
  author       = {Xue Hu and Fabrizio Cutolo and Hisham Iqbal and Johann Henckel and Ferdinando Rodriguez y Baena},
  doi          = {10.1109/TAI.2024.3429048},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5205-5215},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Artificial intelligence-driven framework for augmented reality markerless navigation in knee surgery},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive intelligent resilient bipartite formation control
for nonlinear multiagent systems with false data injection attacks on
actuators and sensors. <em>TAI</em>, <em>5</em>(10), 5194–5204. (<a
href="https://doi.org/10.1109/TAI.2024.3418938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An adaptive intelligent resilient distributed output bipartite time-varying formation protocol is proposed for a class of second-order uncertain nonlinear multiagent systems (MASs) with unknown attacks. Actuators and sensors are both vulnerable to unknown false data injection (FDI) attacks, and the proposed protocol does not require the removal of misbehaving agents or strong network connectivity restrictions. However, existing research methods are mainly limited to studying the complete cooperative relationship and attacks only on actuators or sensors. Network interactions are based on directed signed topologies, reflecting cooperation and competition between agents, and the corresponding adjacency matrix is no longer nonnegative, making traditional consensus controls strategy inapplicable and analyzed by gauge transformation matrix. Due to the uncertain nonlinear dynamics with unmeasurable states, unknown attacks would jeopardize the synchronization of bipartite formation control and even deteriorate entire systems. To address this issue, a security state estimator and adaptive intelligent state reconstruction technique are adopted. It not only can estimate and mitigate malicious unknown FDI attacks on both actuators and sensors simultaneously but also achieve uniform ultimate boundedness (UUB) for observer errors and prescribed time-varying bipartite group consistency formation performance. In particular, the proposed method overcomes the restriction that the dynamics must be linear or general Lipschitz-type nonlinear conditions. Finally, employing Riccati equation and linear matrix inequality, the theoretical method is validly proved by constructing proper Lyapunov through transformation matrix. The results of digital simulation can be effectively demonstrated.},
  archive      = {J_TAI},
  author       = {Jie Lan and Hao Wang and Yan-Jun Liu and Shaocheng Tong},
  doi          = {10.1109/TAI.2024.3418938},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5194-5204},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adaptive intelligent resilient bipartite formation control for nonlinear multiagent systems with false data injection attacks on actuators and sensors},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). U-park: A user-centric smart parking recommendation system
for electric shared micromobility services. <em>TAI</em>,
<em>5</em>(10), 5179–5193. (<a
href="https://doi.org/10.1109/TAI.2024.3428513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electric shared micromobility services (ESMSs) has become a vital element within the mobility as a service framework, contributing to sustainable transportation systems. However, existing ESMS face notable design challenges such as shortcomings in integration, transparency, and user-centered approaches, resulting in increased operational costs and decreased service quality. A key operational issue for ESMS revolves around parking, particularly ensuring the availability of parking spaces as users approach their destinations. For instance, a recent study illustrated that nearly 13% of shared e-bike users in Dublin, Ireland, encounter difficulties parking their e-bikes due to inadequate planning and guidance. In response, we introduce U-Park, a user-centric smart parking recommendation system designed for ESMS, providing tailored recommendations to users by analyzing their historical mobility data, trip trajectory, and parking space availability. We present the system architecture, implement it, and evaluate its performance using real-world data from an Irish-based shared e-bike provider, MOBY Bikes. Our results illustrate U-Park&#39;s ability to predict a user&#39;s destination within a shared e-bike system, achieving an approximate accuracy rate of over 97.60%, all without requiring direct user input. Experiments have proven that this predictive capability empowers U-Park to suggest the optimal parking station to users based on the availability of predicted parking spaces, improving the probability of obtaining a parking spot by 24.91% on average and 29.66% on maximum when parking availability is limited.},
  archive      = {J_TAI},
  author       = {Sen Yan and Noel E. O’Connor and Mingming Liu},
  doi          = {10.1109/TAI.2024.3428513},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5179-5193},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {U-park: A user-centric smart parking recommendation system for electric shared micromobility services},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global attention-guided dual-domain point cloud feature
learning for classification and segmentation. <em>TAI</em>,
<em>5</em>(10), 5167–5178. (<a
href="https://doi.org/10.1109/TAI.2024.3429050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous studies have demonstrated the effectiveness of point-based neural models on the point cloud analysis task. However, there remains a crucial issue on producing the efficient input embedding for raw point coordinates. Moreover, another issue lies in the limited efficiency of neighboring aggregations, which is a critical component in the network stem. In this paper, we propose a global attention-guided dual-domain feature learning network (GAD) to address the above-mentioned issues. We first devise the contextual position-enhanced transformer (CPT) module, which is armed with an improved global attention mechanism, to produces a global-aware input embedding that serves as the guidance to subsequent aggregations. Then, the dual-domain K-nearest neighbor feature fusion (DKFF) is cascaded to conduct effective feature aggregation through novel dual-domain feature learning which appreciates both local geometric relations and long-distance semantic connections. Extensive experiments on multiple point cloud analysis tasks (e.g., classification, part segmentation, and scene semantic segmentation) demonstrate the superior performance of the proposed method and the efficacy of the devised modules.},
  archive      = {J_TAI},
  author       = {Zihao Li and Pan Gao and Kang You and Chuan Yan and Manoranjan Paul},
  doi          = {10.1109/TAI.2024.3429050},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5167-5178},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Global attention-guided dual-domain point cloud feature learning for classification and segmentation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ethical decision-making for the inside of autonomous buses
moral dilemmas. <em>TAI</em>, <em>5</em>(10), 5153–5166. (<a
href="https://doi.org/10.1109/TAI.2024.3396415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of moral dilemmas inside autonomous buses can lead to unpredictable consequences for maneuver and amplify potential harms to passengers onboard. However, most existing approaches solely focus on ensuring ethical decision-making in scenarios outside of vehicles. To ensure ethical decision-making for autonomous buses when moral dilemmas occur inside, there are many urgent challenges that need to be addressed. First, the noncommensurability of ethical values presents difficulties in designing quantifiable environments and decision-making models. Moreover, ethical dilemmas involve multiple conflicting objectives, often necessitating the consideration of multiple moral theories to comprehensively evaluate different perspectives. Additionally, accurately representing these dilemmas and identifying optimal solutions that address conflicting objectives poses further challenges. This article proposes a general ethical decision-making system to handle ethical dilemmas inside autonomous buses. The system&#39;s design adheres to multiple ethical principles, and it comprises two stages: 1) develop a generative adversarial network (GAN) based human-value-aligned data collection scheme to gather representative moral values and generate comprehensive moral scenarios, which address the incommensurable ethical metrics issue; and 2) propose an ethical compliant multiobjective thresholded lexicographic Deep Q-learning method to ensure optimal policies that satisfy multiple ethical objectives. In a case study of autonomous bus route planning, our system outperforms benchmarks in showing a greater number and more evenly distributed policies in the Pareto Front and a 27% increase in coverage rate. Extensive experiments against nonethical systems show superior outcomes in convergence, average reward and cost. Finally, user studies demonstrate the system&#39;s accuracy and usability.},
  archive      = {J_TAI},
  author       = {Zijie Huang and Yulei Wu and Niccolò Tempini and Haina Tang},
  doi          = {10.1109/TAI.2024.3396415},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5153-5166},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Ethical decision-making for the inside of autonomous buses moral dilemmas},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human cognitive learning in shared control via differential
game with bounded rationality and incomplete information. <em>TAI</em>,
<em>5</em>(10), 5141–5152. (<a
href="https://doi.org/10.1109/TAI.2024.3415549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since human beings are of limited reasoning ability as well as the machines do not usually know human intentions, how to learn human cognitive levels in shared control to enhance the machines’ intelligence is a challenging issue. In this study, this issue is addressed in the context of human–machine shared control for a class of human-in-the-loop (HiTL) systems based on a differential game with bounded rationality and incomplete information. Initially, we formulate the human–machine shared control problem as a two-player nonzero-sum linear quadratic dynamic game (LQDG), where the weighting matrix of the cost function representing the human intention is unknown for the machine. To model the human bounded rationality, the level- $\boldsymbol{k}$ (LK) approach is employed to set up the LK control policies of two players performing the corresponding steps of strategic thinking. To infer the human intention, an online adaptive inverse optimal control (IOC) algorithm is then developed by using the system state data, so that the control policies of different cognitive levels can be computed. In addition, a reinforcement learning method is proposed for the machine to identify the distribution of the human cognitive levels while providing a proactive collaborative control to assist the human in a probabilistic switching way. Finally, simulation results on a cooperative shared control driver assistance system (DAS) illustrate the efficacy of the proposed approach.},
  archive      = {J_TAI},
  author       = {Huai-Ning Wu and Xiao-Yan Jiang and Mi Wang},
  doi          = {10.1109/TAI.2024.3415549},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5141-5152},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Human cognitive learning in shared control via differential game with bounded rationality and incomplete information},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multichannel long-term external attention network for
aeroengine remaining useful life prediction. <em>TAI</em>,
<em>5</em>(10), 5130–5140. (<a
href="https://doi.org/10.1109/TAI.2024.3400929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately estimating the remaining useful life (RUL) of aircraft engines can effectively prevent aircraft crashes and human casualties. In some RUL prediction methods, particularly for aircraft engines running under complex conditions, they are difficult to comprehensively characterize the engine degradation process, resulting in poor predicted RUL. To address the above challenge, a multichannel long-term external attention network (MLEAN) is proposed for the RUL prediction of turbofan engines. First, the preprocessed samples are transformed to enable MLEAN to focus on learning inter-sensor correlations within the same degradation stage. To improve the feature representation capability of the network, multichannel time attention network (MTANet) is then designed to realize multiscale and multifrequency feature learning, which effectively achieves multiperspective analysis of long-term dependencies in different channels. Then, external attention block (EAB) is introduced to memorize important degraded features from different samples, which can improve the ability of global feature extraction and generalization ability of the network. The performance of MLEAN is examined on the C-MAPSS public dataset. The evaluation metrics RMSE and score values are 13.71 and 680, respectively. In comparison experiments, the proposed MLEAN performs better than the listed state-of-the-art RUL prediction methods.},
  archive      = {J_TAI},
  author       = {Xuezhen Liu and Yongyi Chen and Dan Zhang and Ruqiang Yan and Hongjie Ni},
  doi          = {10.1109/TAI.2024.3400929},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5130-5140},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A multichannel long-term external attention network for aeroengine remaining useful life prediction},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interacting multiple model framework for incipient diagnosis
of interturn faults in induction motors. <em>TAI</em>, <em>5</em>(10),
5120–5129. (<a href="https://doi.org/10.1109/TAI.2024.3405468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces a novel online signal processing and machine learning (ML) framework designed for the incipient diagnosis of stator interturn faults (SITF) in three-phase squirrel cage induction motors. Addressing the critical need for incipient fault detection to prevent severe motor damage, the framework focuses on motor speed estimation, incipient fault detection, fault severity estimation, and faulty phase identification using only stator currents. A distinctive contribution lies in the proposed interacting multiple model (IMM) framework that leverages carefully selected motor current signatures as features, offering a comprehensive strategy for stator fault diagnosis not explored previously. The article pioneers the use of the selected harmonics with ML models to estimate a fault severity indicator, which is developed based on insights from the motor&#39;s physics of failure. Experimental validation showcases the fault indicator&#39;s effectiveness under diverse operating conditions, demonstrating its utility in fault severity assessment. Suitable standalone ML model is selected, or an ensemble is constructed from a pool of ML models at each stage of the IMM framework. Further, a feature relevance analysis is also performed to garner insights into the contributions of each handpicked feature in predicting the fault indicator.},
  archive      = {J_TAI},
  author       = {Akash C. Babu and Jeevanand Seshadrinath},
  doi          = {10.1109/TAI.2024.3405468},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5120-5129},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Interacting multiple model framework for incipient diagnosis of interturn faults in induction motors},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enclose and track a target of mobile robot with motion and
field of view constraints based on relative position measurement.
<em>TAI</em>, <em>5</em>(10), 5110–5119. (<a
href="https://doi.org/10.1109/TAI.2024.3403511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a systematic design approach to address the challenge of enclosing and tracking a moving target in multirobot systems while accounting for motion and field of view (FOV) constraints. First, a reference trajectory is designed based on relative position measurement which also conforms to the motion and FOV constraints. Subsequently, considering the uncertainty of mobile robots, and combining prescribed performance bound (PPB) technique, an adaptive tracking solutions are designed to force the fleet of robots track and enclose the moving target. Experimental results demonstrate that the robots can efficiently track the provided reference trajectory while ensuring guaranteed transient performance of position and direction tracking errors, account for the motion and FOV constraints, achieve rapid enclosing and tracking of target objects.},
  archive      = {J_TAI},
  author       = {Yu Wen and Jiangshuai Huang and Shaoxin Sun and Xiaojie Su},
  doi          = {10.1109/TAI.2024.3403511},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5110-5119},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Enclose and track a target of mobile robot with motion and field of view constraints based on relative position measurement},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous graph contrastive learning with augmentation
graph. <em>TAI</em>, <em>5</em>(10), 5100–5109. (<a
href="https://doi.org/10.1109/TAI.2024.3400751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous graph neural networks (HGNNs) have demonstrated promising capabilities in addressing various problems defined on heterogeneous graphs containing multiple types of nodes or edges. However, traditional HGNN models depend on label information and capture the local structural information of the original graph. In this article, we propose a novel heterogeneous graph contrastive learning method with augmentation graph (AHGCL). Specifically, we construct an augmentation graph by calculating the feature similarity of nodes to capture latent structural information. For the original graph and the augmentation graph, we employ a shared graph neural network (GNN) encoder to extract the semantic features of nodes with different meta-paths. The feature information is aggregated through a semantic-level attention mechanism to generate final node embeddings, which capture latent high-order semantic structural information. Considering the problems of label information for the real-world datasets, we adopt contrastive learning to train the GNN encoder for maximizing the common information between similar nodes from the original graph and the augmentation graph views. We conduct node classification experiments on four real-world datasets, AMiner, Freebase, digital bibliography &amp; library project (DBLP), and association for computing machinery (ACM), to evaluate the performance of AHGCL. The results show that the proposed AHGCL demonstrates excellent stability and capability compared to existing graph representation learning methods.},
  archive      = {J_TAI},
  author       = {Zijuan Zhao and Zequn Zhu and Yuan Liu and Jinli Guo and Kai Yang},
  doi          = {10.1109/TAI.2024.3400751},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5100-5109},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Heterogeneous graph contrastive learning with augmentation graph},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Observer-based adaptive fuzzy control for singular systems
with nonlinear perturbation and actuator saturation. <em>TAI</em>,
<em>5</em>(10), 5090–5099. (<a
href="https://doi.org/10.1109/TAI.2024.3429052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the adaptive fuzzy control problem for singular systems with actuator saturation and nonlinear perturbation, where the system consists of two coupled differential and algebraic subsystems. To cope with the actuator saturation, a new auxiliary system whose order is the same as the differential subsystem is introduced. With the help of the backstepping method and adaptive fuzzy control method, an observer-based adaptive output feedback tracking control approach is utilized. Under the designed controller, it is proved that the closed-loop system is impulse-free and regular, and all the involved signals are bounded. Furthermore, it is ensured that the tracking error can be adjusted by the errors between the control inputs and the corresponding saturated inputs, as well as the design parameters. Finally, simulation studies demonstrate the validity of the control approach.},
  archive      = {J_TAI},
  author       = {Qingtan Meng and Qian Ma},
  doi          = {10.1109/TAI.2024.3429052},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5090-5099},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Observer-based adaptive fuzzy control for singular systems with nonlinear perturbation and actuator saturation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ConvBLS: An effective and efficient incremental
convolutional broad learning system combining deep and broad
representations. <em>TAI</em>, <em>5</em>(10), 5075–5089. (<a
href="https://doi.org/10.1109/TAI.2024.3403953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Broad learning system (BLS) has to undergo a vectorization operation before modeling image data, which makes it challenging for BLS to learn local semantic features. Thus, various convolutional-based broad learning systems (C-BLSs) have been introduced to address these challenges. Regrettably, the existing C-BLS variants either lack an efficient training algorithm and incremental learning capability or suffer from poor performance. To this end, we propose a novel convolutional broad learning system (ConvBLS) based on the spherical K-means (SKM) algorithm and two-stage multiscale (TSMS) feature fusion, which consists of the convolutional feature layer (CFL), convolutional enhancement layer (CEL), TSMS feature fusion layer, and output layer. First, unlike the current C-BLS, the simple yet efficient SKM algorithm is utilized to learn the weights of CFLs. Compared with random filters, the SKM algorithm enables the CFL to learn more comprehensive spatial features. Second, to further mine the local semantic features, CELs are established to expand the feature space. Third, the TSMS feature fusion layer is proposed to extract more effective multiscale features by integrating deep and broad representations. Thanks to the above elaborate design and the pseudoinverse calculation of the output layer weights, our proposed ConvBLS method is unprecedentedly efficient and effective. Finally, the corresponding incremental learning algorithms are presented for rapid remodeling if the model deems to expand. Experiments and comparisons demonstrate the superiority of our method.},
  archive      = {J_TAI},
  author       = {Chunyu Lei and Jifeng Guo and C. L. Philip Chen},
  doi          = {10.1109/TAI.2024.3403953},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5075-5089},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {ConvBLS: An effective and efficient incremental convolutional broad learning system combining deep and broad representations},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Selective depth attention networks for adaptive multiscale
feature representation. <em>TAI</em>, <em>5</em>(10), 5064–5074. (<a
href="https://doi.org/10.1109/TAI.2024.3401652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing multiscale methods lead to a risk of just increasing the receptive field sizes while neglecting small receptive fields. Thus, it is a challenging problem to effectively construct adaptive neural networks for recognizing various spatial-scale objects. To tackle this issue, we first introduce a new attention dimension, i.e., depth, in addition to existing attentions such as channel-attention, spatial-attention, branch-attention, and self-attention. We present a novel selective depth attention network to treat multiscale objects symmetrically in various vision tasks. Specifically, the blocks within each stage of neural networks, including convolutional neural networks (CNNs), e.g., ResNet, SENet, and Res2Net, and vision transformers (ViTs), e.g., PVTv2, output the hierarchical feature maps with the same resolution but different receptive field sizes. Based on this structural property, we design a depthwise building module, namely an selective depth attention (SDA) module, including a trunk branch and a SE-like attention branch. The block outputs of the trunk branch are fused to guide their depth attention allocation through the attention branch globally. According to the proposed attention mechanism, we dynamically select different depth features, which contributes to adaptively adjusting the receptive field sizes for the variable-sized input objects. Moreover, our method is orthogonal to multiscale networks and attention networks, so-called SDA- $x$ Net. Extensive experiments demonstrate that the proposed SDA method significantly improves the original performance as a lightweight and efficient plug-in on numerous computer vision tasks, e.g., image classification, object detection, and instance segmentation.},
  archive      = {J_TAI},
  author       = {Qingbei Guo and Xiao-Jun Wu and Tianyang Xu and Tongzhen Si and Cong Hu and Jinglan Tian},
  doi          = {10.1109/TAI.2024.3401652},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5064-5074},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Selective depth attention networks for adaptive multiscale feature representation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DecGAN: Decoupling generative adversarial network for
detecting abnormal neural circuits in alzheimer’s disease. <em>TAI</em>,
<em>5</em>(10), 5050–5063. (<a
href="https://doi.org/10.1109/TAI.2024.3416420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main reasons for Alzheimer&#39;s disease (AD) is the disorder of some neural circuits. Existing methods for AD prediction have achieved great success, however, detecting abnormal neural circuits from the perspective of brain networks is still a big challenge. In this work, a novel decoupling generative adversarial network (DecGAN) is proposed to detect abnormal neural circuits for AD. Concretely, a decoupling module is designed to decompose a brain network into two parts: one part is composed of a few sparse graphs that represent the neural circuits largely determining the development of AD; the other part is a supplement graph, whose influence on AD can be ignored. Furthermore, the adversarial strategy is utilized to guide the decoupling module to extract the feature more related to AD. Meanwhile, by encoding the detected neural circuits to hypergraph data, an analytic module associated with the hyperedge neurons algorithm is designed to identify the neural circuits. More importantly, a novel sparse capacity loss based on the spatial-spectral hypergraph similarity is developed to minimize the intrinsic topological distribution of neural circuits, which can significantly improve the accuracy and robustness of the proposed model. Experimental results demonstrate that the proposed model can effectively detect the abnormal neural circuits at different stages of AD, which is helpful for pathological study and early treatment.},
  archive      = {J_TAI},
  author       = {Junren Pan and Qiankun Zuo and Bingchuan Wang and C.L. Philip Chen and Baiying Lei and Shuqiang Wang},
  doi          = {10.1109/TAI.2024.3416420},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5050-5063},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {DecGAN: Decoupling generative adversarial network for detecting abnormal neural circuits in alzheimer&#39;s disease},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CNN-based metrics for performance evaluation of generative
adversarial networks. <em>TAI</em>, <em>5</em>(10), 5040–5049. (<a
href="https://doi.org/10.1109/TAI.2024.3401650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose two convolutional neural network (CNN) based metrics, classification score (CS) and distribution score (DS), for performance evaluation of generative adversarial networks (GANs). Though GAN-generated images can be evaluated through manual assessment of visual fidelity, it is prolonged, subjective, challenging, tiresome, and can be misleading. Existing quantitative methods are biased toward memory GAN and fail to detect overfitting. CS and DS allow us to experimentally prove that training of GANs is actually guided by the dataset that it improves with every epoch and gets closer to following the distribution of the dataset. Both methods are based on GAN-generated image classification by CNN. CS is the root mean square (rms) value of three different classification techniques, direct classification (DC), indirect classification (IC), and blind classification (BC). It exhibits the degree to which GAN can learn the features and generate fake images similar to real datasets. DS shows the contrast between the mean distribution of GAN-generated data and the real data. It indicates the extent to which GANs can create synthetic images with similar distribution to real datasets. We evaluated CS and DS metrics for different variants of GANs and compared their performances with existing metrics. Results show that CS and DS can evaluate the different variants of GANs quantitatively and qualitatively while detecting overfitting and mode collapse.},
  archive      = {J_TAI},
  author       = {Adarsh Prasad Behera and Satya Prakash and Siddhant Khanna and Shivangi Nigam and Shekhar Verma},
  doi          = {10.1109/TAI.2024.3401650},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5040-5049},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {CNN-based metrics for performance evaluation of generative adversarial networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adjusting logit in gaussian form for long-tailed visual
recognition. <em>TAI</em>, <em>5</em>(10), 5026–5039. (<a
href="https://doi.org/10.1109/TAI.2024.3401102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is not uncommon that real-world data are distributed with a long tail. For such data, the learning of deep neural networks becomes challenging because it is hard to classify tail classes correctly. In the literature, several existing methods have addressed this problem by reducing classifier bias, provided that the features obtained with long-tailed data are representative enough. However, we find that training directly on long-tailed data leads to uneven embedding space. That is, the embedding space of head classes severely compresses that of tail classes, which is not conducive to subsequent classifier learning. This article therefore studies the problem of long-tailed visual recognition from the perspective of feature level. We introduce feature augmentation to balance the embedding distribution. The features of different classes are perturbed with varying amplitudes in Gaussian form. Based on these perturbed features, two novel logit adjustment methods are proposed to improve model performance at a modest computational overhead. Subsequently, the distorted embedding spaces of all classes can be calibrated. In such balanced-distributed embedding spaces, the biased classifier can be eliminated by simply retraining the classifier with class-balanced sampling data. Extensive experiments conducted on benchmark datasets demonstrate the superior performance of the proposed method over the state-of-the-art ones.},
  archive      = {J_TAI},
  author       = {Mengke Li and Yiu-ming Cheung and Yang Lu and Zhikai Hu and Weichao Lan and Hui Huang},
  doi          = {10.1109/TAI.2024.3401102},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5026-5039},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adjusting logit in gaussian form for long-tailed visual recognition},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonlinear regression with hierarchical recurrent neural
networks under missing data. <em>TAI</em>, <em>5</em>(10), 5012–5025.
(<a href="https://doi.org/10.1109/TAI.2024.3404414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study regression (or prediction) of sequential data, which may have missing entries and/or different lengths. This problem is heavily investigated in the machine learning literature since such missingness is a common occurrence in most real-life applications due to data corruption, measurement errors, and similar. To this end, we introduce a novel hierarchical architecture involving a set of long short-term memory (LSTM) networks, which use only the existing inputs in the sequence without any imputations or statistical assumptions on the missing data. To incorporate the missingness information, we partition the input space into different regions in a hierarchical manner based on the “presence-pattern” of the previous inputs and then assign different LSTM networks to these regions. In this sense, we use the LSTM networks as our experts for these regions and adaptively combine their outputs to generate our final output. Our method is generic so that the set of partitioned regions (presence-patterns) that are modeled by the LSTM networks can be customized, and one can readily use other sequential architectures such as gated recurrent unit (GRU) networks and recurrent neural networks (RNNs) as shown in the article. We also provide the computational complexity analysis of the proposed architecture, which is in the same order as a conventional LSTM architecture. In our experiments, our algorithm achieves significant performance improvements on the well-known financial and real-life datasets with respect to the state-of-the-art methods. We also share the source code of our algorithm to facilitate other research and the replicability of our results.},
  archive      = {J_TAI},
  author       = {S. Onur Sahin and Suleyman S. Kozat},
  doi          = {10.1109/TAI.2024.3404414},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {5012-5025},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Nonlinear regression with hierarchical recurrent neural networks under missing data},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable learning for multiagent route planning: Adapting to
diverse task scales. <em>TAI</em>, <em>5</em>(10), 4996–5011. (<a
href="https://doi.org/10.1109/TAI.2024.3402193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When utilizing end-to-end learn-to-construct methods to solve routing problems for multiagent systems, the model is usually trained individually for different problem scales (i.e., the number of customers to be concurrently served within a map) to make the model adaptive to the corresponding scale, ensuring good solution quality. Otherwise, the model trained for one specific scale can lead to poor performance when applied to another different scale, and this situation can get worse when the scale discrepancy increases. Such a separate training strategy is inefficient and time-intensive. In this article, we propose a mix-scale learning framework that requires only a single training session, enabling the model to effectively plan high-quality routes for various problem scales. Based on the capacitated vehicle routing problem (CVRP), the test results reveal that: for problem scales which are no matter seen or unseen during training, our once-trained model can produce solution routes with performance comparable or even superior to those of individually trained models, and offer the highest average solution quality with improvement ratio ranging from 2.28% to 8.07%, which effectively spares the separate training session for each specific scale. Additionally, the extended comparison analysis with individually trained models on real-world benchmark dataset from CVRPLib further highlights our once-trained model&#39;s generalization performance across various problem scales and diverse node distributions.},
  archive      = {J_TAI},
  author       = {Site Qu and Guoqiang Hu},
  doi          = {10.1109/TAI.2024.3402193},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {4996-5011},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Scalable learning for multiagent route planning: Adapting to diverse task scales},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bidirectional influence and interaction for multiagent
reinforcement learning. <em>TAI</em>, <em>5</em>(10), 4984–4995. (<a
href="https://doi.org/10.1109/TAI.2024.3401649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, multiagent reinforcement learning (MARL) has demonstrated considerable potential across diverse applications. However, in reinforcement learning environments characterized by sparse rewards, the scarcity of reward signals may give rise to reward conflicts among agents. In these scenarios, each agent tends to compete to obtain limited rewards, deviating from collaborative efforts aimed at achieving collective team objectives. This not only amplifies the learning challenge but also imposes constraints on the overall learning performance of agents, ultimately compromising the attainment of team goals. To mitigate the conflicting competition for rewards among agents in MARL, we introduce the bidirectional influence and interaction (BDII) MARL framework. This innovative approach draws inspiration from the collaborative ethos observed in human social cooperation, specifically the concept of “sharing joys and sorrows.” The fundamental concept behind BDII is to empower agents to share their individual rewards with collaborators, fostering a cooperative rather than competitive behavioral paradigm. This strategic shift aims to resolve the pervasive issue of reward conflicts among agents operating in sparse-reward environments. BDII incorporates two key factors—namely, the Gaussian kernel distance between agents (physical distance) and policy diversity among agents (logical distance). The two factor collectively contribute to the dynamic adjustment of reward allocation coefficients, culminating in the formation of reward distribution weights. The incorporation of these weights facilitates the equitable sharing of agents’ contributions to rewards, promoting a cooperative learning environment. Through extensive experimental evaluations, we substantiate the efficacy of BDII in addressing the challenge of reward conflicts in MARL. Our research findings affirm that BDII significantly mitigates reward conflicts, ensuring that agents consistently align with the original team objectives, thereby achieving state-of-the-art performance. This validation underscores the potential of the proposed framework in enhancing the collaborative nature of multiagent systems, offering a promising avenue for advancing the field of reinforcement learning.},
  archive      = {J_TAI},
  author       = {Shaoqi Sun and Kele Xu and Dawei Feng and Bo Ding},
  doi          = {10.1109/TAI.2024.3401649},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {4984-4995},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Bidirectional influence and interaction for multiagent reinforcement learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Redefining real-time road quality analysis with vision
transformers on edge devices. <em>TAI</em>, <em>5</em>(10), 4972–4983.
(<a href="https://doi.org/10.1109/TAI.2024.3394797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Road infrastructure is essential for transportation safety and efficiency. However, the current methods for assessing road conditions, crucial for effective planning and maintenance, suffer from high costs, time-intensive procedures, infrequent data collection, and limited real-time capabilities. This article presents an efficient lightweight system to analyze road quality from video feeds in real time. The backbone of the system is EdgeFusionViT, a novel vision transformer (ViT)-based architecture that uses an attention-based late fusion mechanism. The proposed architecture outperforms lightweight convolutional neural network (CNN)-based and ViT-based models. Its practicality is demonstrated by its deployment on an edge device, the Nvidia Jetson Orin Nano, enabling real-time road analysis at 12 frames per second. EdgeFusionViT outperforms existing benchmarks, achieving an impressive accuracy of 89.76% on the road surface condition dataset (RSCD). Notably, the model maintains a commendable accuracy of 76.89% even when trained with only 2% of the dataset, demonstrating its robustness and efficiency. These findings highlight the system&#39;s potential in road infrastructure management. It aids in creating safer, more efficient transport systems through timely, accurate road condition assessments. The study sets a new benchmark and opens up possibilities for advanced machine learning in infrastructure management.},
  archive      = {J_TAI},
  author       = {Tasnim Ahmed and Naveed Ejaz and Salimur Choudhury},
  doi          = {10.1109/TAI.2024.3394797},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {4972-4983},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Redefining real-time road quality analysis with vision transformers on edge devices},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GOAL: Generalized jointly sparse linear discriminant
regression for feature extraction. <em>TAI</em>, <em>5</em>(10),
4959–4971. (<a href="https://doi.org/10.1109/TAI.2024.3412862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ridge regression (RR)-based methods aim to obtain a low-dimensional subspace for feature extraction. However, the subspace&#39;s dimensionality does not exceed the number of data categories, hence compromising its capability of feature representation. Moreover, these methods with $L_{2}$ -norm metric and regularization cannot extract highly robust features from data with corruption. To address these problems, in this article, we propose generalized jointly sparse linear discriminant regression (GOAL), a novel regression method based on joint $L_{2,1}$ -norm and capped- $L_{2}$ -norm, which can integrate sparsity, locality, and discriminability into one model to learn a full-rank robust feature extractor. The sparsely selected discriminative features are robust enough to characterize the decision boundary between classes. Locality is related to manifold structure and Laplacian smoothing, which can enhance the robustness of the model. By using the multinorm metric and regularization regression framework, the proposed method obtains the projection with joint sparsity and guarantees that the rank of the projection matrix will not be limited by the number of classes. An iterative algorithm is proposed to compute the optimal solution. Complexity analysis and proofs of convergence are also given in the article. Experiments on well-known datasets demonstrate our model&#39;s superiority and generalization ability.},
  archive      = {J_TAI},
  author       = {Haoquan Lu and Zhihui Lai and Junhong Zhang and Zhuozhen Yu and Jiajun Wen},
  doi          = {10.1109/TAI.2024.3412862},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {4959-4971},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {GOAL: Generalized jointly sparse linear discriminant regression for feature extraction},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive prescribed-time neural control of nonlinear systems
via dynamic surface technique. <em>TAI</em>, <em>5</em>(10), 4948–4958.
(<a href="https://doi.org/10.1109/TAI.2024.3404914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adaptive practical prescribed-time (PPT) neural control is studied for multiinput multioutput (MIMO) nonlinear systems with unknown nonlinear functions and unknown input gain matrices. Unlike existing PPT design schemes based on backstepping, this study proposes a novel PPT control framework using the dynamic surface control (DSC) approach. First, a novel nonlinear filter (NLF) with an adaptive parameter estimator and a piecewise function is constructed to effectively compensate for filter errors and facilitate prescribed-time convergence. Based on this, a unified DSC-based adaptive PPT control algorithm, augmented with a neural networks (NNs) approximator, is developed, where NNs are used to approximate unknown nonlinear system functions. This algorithm not only addresses the inherent computational complexity explosion associated with traditional backstepping methods but also reduces the constraints on filter design parameters compared to the DSC algorithm that relies on linear filters. The simulation showcases the effectiveness and superiority of the devised scheme by employing a two-degree-of-freedom robot manipulator.},
  archive      = {J_TAI},
  author       = {Ping Wang and Chengpu Yu and Maolong Lv and Zilong Zhao},
  doi          = {10.1109/TAI.2024.3404914},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {4948-4958},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adaptive prescribed-time neural control of nonlinear systems via dynamic surface technique},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A robust deep-learning model to detect major depressive
disorder utilizing EEG signals. <em>TAI</em>, <em>5</em>(10), 4938–4947.
(<a href="https://doi.org/10.1109/TAI.2024.3394792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Major depressive disorder (MDD), commonly called depression, is a prevalent psychiatric condition diagnosed via questionnaire-based mental status assessments. However, this method often yields inconsistent and inaccurate results. Furthermore, there is currently a lack of a comprehensive diagnostic framework for MDD that assesses various brainwaves (alpha, theta, gamma, etc.) of electroencephalogram (EEG) signals as potential biomarkers, aiming to identify the most effective one for achieving accurate and robust diagnostic outcomes. To address this issue, we propose an innovative approach employing a deep convolutional neural network (DCNN) for MDD diagnosis utilizing the brainwaves present in EEG signals. Our proposed model, an extended 11-layer 1-D convolutional neural network (Ex-1DCNN), is designed to automatically learn from input EEG signals, foregoing the need for manual feature selection. By harnessing intrinsic brainwave patterns, our model demonstrates adaptability in classifying EEG signals into depressive and healthy categories. We have conducted an extensive analysis to identify optimal brainwave features and epoch duration for accurate MDD diagnosis. Leveraging EEG data from 34 MDD patients and 30 healthy subjects, we have identified that the Gamma brainwave at a 15-s epoch duration is the most effective configuration, achieving an accuracy of 99.60%, sensitivity of 100%, specificity of 99.21%, and an F1 score of 99.60%. This study highlights the potential of deep-learning techniques in streamlining the diagnostic process for MDD and offering a reliable aid to clinicians in MDD diagnosis.},
  archive      = {J_TAI},
  author       = {Israq Ahmed Anik and A. H. M. Kamal and Muhammad Ashad Kabir and Shahadat Uddin and Mohammad Ali Moni},
  doi          = {10.1109/TAI.2024.3394792},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {4938-4947},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A robust deep-learning model to detect major depressive disorder utilizing EEG signals},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hedge-embedded linguistic fuzzy neural networks for systems
identification and control. <em>TAI</em>, <em>5</em>(10), 4928–4937. (<a
href="https://doi.org/10.1109/TAI.2024.3395416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of natural language processing, hedge-embedded structures have contributed considerably by appreciating linguistic variables and distinguishing overlapped classes. This aspect of natural languages considerably affects the building of linguistically interpretable architectures for fuzzy neural networks (FNNs). Here, we propose extending the idea of hedge-embedded linguistic fuzzy neural networks (LiFNNs) to the systems identification and control paradigm. This perspective leads us to the universal approximation property for this mathematical construct using the Stone–Weierstrass theorem and the proof of stability for the resulting nonlinear system identification process using the Lyapunov function. Furthermore, the power activation functions in the membership degrees of the proposed network enable linguistic hedge interpretation and more precise learning. Finally, the proposed LiFNN, optimized using a backpropagation learning algorithm, is evaluated on several problems in function approximation (periodic functions and quadratic Hermite function), system identification (a nonlinear system), and direct adaptive control fields. Results show that memberships are more distinguishable in the proposed LiFNN, leading to $\sim$ 50% less error on the average and higher granulation and interpretability.},
  archive      = {J_TAI},
  author       = {Hamed Rafiei and Mohammad-R. Akbarzadeh-T.},
  doi          = {10.1109/TAI.2024.3395416},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {4928-4937},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Hedge-embedded linguistic fuzzy neural networks for systems identification and control},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CS-mixer: A cross-scale vision multilayer perceptron with
spatial–channel mixing. <em>TAI</em>, <em>5</em>(10), 4915–4927. (<a
href="https://doi.org/10.1109/TAI.2024.3415551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite simpler architectural designs compared with vision transformers (ViTs) and convolutional neural networks, vision multilayer perceptrons (MLPs) have demonstrated strong performance and high data efficiency for image classification and semantic segmentation. Following pioneering works such as MLP-Mixers and gMLPs, later research proposed a plethora of vision MLP architectures that achieve token-mixing with specifically engineered convolution- or attentionlike mechanisms. However, existing methods such as $\text{S}^{\text{2}}$ -MLPs and PoolFormers typically model spatial information in equal-sized spatial regions and do not consider cross-scale spatial interactions, thus delivering subpar performance compared with transformer models that employ global token mixing. Further, these MLP token-mixers, along with most ViTs, only model one- or two-axis correlations among space and channels, avoiding simultaneous three-axis spatial–channel mixing due to its computational demands. We, therefore, propose CS-Mixer, a hierarchical vision MLP that learns dynamic low-rank transformations for tokens aggregated across scales, both locally and globally. Such aggregation allows for token-mixing that explicitly models spatial–channel interactions, made computationally possible by a multihead design that projects to low-dimensional subspaces. The proposed methodology achieves competitive results on popular image recognition benchmarks without incurring substantially more computing. Our largest model, CS-Mixer-L, reaches 83.2% top-1 accuracy on ImageNet-1k with 13.7 GFLOPs and 94 M parameters.},
  archive      = {J_TAI},
  author       = {Jonathan Cui and David A. Araujo and Suman Saha and Md Faisal Kabir},
  doi          = {10.1109/TAI.2024.3415551},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {4915-4927},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {CS-mixer: A cross-scale vision multilayer perceptron with Spatial–Channel mixing},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive decentralized policies with attention for
large-scale multiagent environments. <em>TAI</em>, <em>5</em>(10),
4905–4914. (<a href="https://doi.org/10.1109/TAI.2024.3415550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiagent reinforcement learning (MARL) poses unique challenges in real-world applications, demanding the adaptation of reinforcement learning principles to scenarios where agents interact in dynamically changing environments. This article presents a novel approach, “decentralized policy with attention” (ADPA), designed to address these challenges in large-scale multiagent environments. ADPA leverages an attention mechanism to dynamically select relevant information for estimating critics while training decentralized policies. This enables effective and scalable learning, supporting both cooperative and competitive settings, and scenarios with nonglobal states. In this work, we conduct a comprehensive evaluation of ADPA across a range of multiagent environments, including cooperative treasure collection and rover-tower communication. We compare ADPA with existing centralized training methods and ablated variants to showcase its advantages in terms of scalability, adaptability to various environments, and robustness. Our results demonstrate that ADPA offers a promising solution for addressing the complexities of large-scale MARL, providing the flexibility to handle diverse multiagent scenarios. By combining decentralized policies with attention mechanisms, we contribute to the advancement of MARL techniques, offering a powerful tool for real-world applications in dynamic and interactive multiagent systems.},
  archive      = {J_TAI},
  author       = {Youness Boutyour and Abdellah Idrissi},
  doi          = {10.1109/TAI.2024.3415550},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {4905-4914},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adaptive decentralized policies with attention for large-scale multiagent environments},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Test-time adaptation for nighttime color-thermal semantic
segmentation. <em>TAI</em>, <em>5</em>(10), 4893–4904. (<a
href="https://doi.org/10.1109/TAI.2023.3336611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to scene understanding in adverse visual conditions, e.g. , nighttime, has sparked active research for color-thermal semantic segmentation. However, it is essentially hampered by two critical problems: 1) the day-night gap of color images is larger than that of thermal images; and 2) the classwise performance of color images at night is not consistently higher or lower than that of thermal images. We propose the first test-time adaptation (TTA) framework, dubbed Night-TTA , to address the problems for nighttime color-thermal semantic segmentation without access to the source (daytime) data during adaptation. Our method enjoys three key technical parts. First, as one modality ( e . g ., color) suffers from a larger domain gap than that of the other ( e . g ., thermal), imaging heterogeneity refinement ( IHR ) employs an interaction branch on the basis of color and thermal branches to prevent cross-modal discrepancy and performance degradation. Then, class aware refinement ( CAR ) is introduced to obtain reliable ensemble logits based on pixel-level distribution aggregation of the three branches. In addition, we also design a specific learning scheme for our TTA framework, which enables the ensemble logits and three student logits to collaboratively learn to improve the quality of predictions during the testing phase of our Night-TTA. Extensive experiments show that our method achieves state-of-the-art (SoTA) performance with a 13.07% boost in mean intersection over union (mIoU).},
  archive      = {J_TAI},
  author       = {Yexin Liu and Weiming Zhang and Guoyang Zhao and Jinjing Zhu and Athanasios V. Vasilakos and Lin Wang},
  doi          = {10.1109/TAI.2023.3336611},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {4893-4904},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Test-time adaptation for nighttime color-thermal semantic segmentation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A human-in-the-middle attack against object detection
systems. <em>TAI</em>, <em>5</em>(10), 4884–4892. (<a
href="https://doi.org/10.1109/TAI.2024.3428520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection systems using deep learning models have become increasingly popular in robotics thanks to the rising power of central processing units (CPUs) and graphics processing units (GPUs) in embedded systems. However, these models are susceptible to adversarial attacks. While some attacks are limited by strict assumptions on access to the detection system, we propose a novel hardware attack inspired by Man-in-the-Middle attacks in cryptography. This attack generates a universal adversarial perturbations (UAPs) and injects the perturbation between the universal serial bus (USB) camera and the detection system via a hardware attack. Besides, prior research is misled by an evaluation metric that measures the model accuracy rather than the attack performance. In combination with our proposed evaluation metrics, we significantly increased the strength of adversarial perturbations. These findings raise serious concerns for applications of deep learning models in safety-critical systems, such as autonomous driving.},
  archive      = {J_TAI},
  author       = {Han Wu and Sareh Rowlands and Johan Wahlström},
  doi          = {10.1109/TAI.2024.3428520},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {4884-4892},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A human-in-the-middle attack against object detection systems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-driven technology applications in planning, demand-side
management, and cybersecurity for smart household community.
<em>TAI</em>, <em>5</em>(10), 4868–4883. (<a
href="https://doi.org/10.1109/TAI.2024.3417389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need for data-driven technologies such as artificial intelligence (AI), machine learning (ML), and deep learning (DL) in various sectors has been soaring for over a decade. The amount of data released by the smart grid itself has been enormous, making these cutting-edge technologies highly efficient and reliable. This article proposes an orderly review of data-driven technology applications for smart residential households. It underpins the importance of forecasting studies with demand-side management (DSM)-aided tools such as demand response (DR), over a secure energy transaction platform. For the publications reviewed, the outcomes suggest the urgent need for household-level forecasting as it accounts for only 21% of the publications reviewed while DL dominates the forecasting studies (57%) with scope towards its hybridization with decomposition techniques. Similarly, the DSM/DR domain needs to be actively implemented at the retail level over a secure network. The outcomes suggest that baseline prediction (4.76%) and self-learning DR (19%) are crucial but the least focused issues, hence AI/ML/DL could be the solutions. Likewise, scalability (24.3%) turns out to be the major issue for assessing the security of the utility grid. However, deep reinforcement learning (DRL) could be a suitable tool as it is adaptive, independent of the system dynamics, and works best in a model-free dynamic environment. The overall findings suggest that the smart household community is the least focused entity and needs prompt attention to address the associated challenges. Additionally, several distinct insights such as dataset features, model parameters, performance metrics, customer-centricity, customer diversity, and mitigation are mapped with applications. Besides, this article points out various shortcomings and tries to postulate probable solutions to the best of capacity.},
  archive      = {J_TAI},
  author       = {Dipanshu Naware and Arghya Mitra},
  doi          = {10.1109/TAI.2024.3417389},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {4868-4883},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Data-driven technology applications in planning, demand-side management, and cybersecurity for smart household community},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transformer-based generative adversarial networks in
computer vision: A comprehensive survey. <em>TAI</em>, <em>5</em>(10),
4851–4867. (<a href="https://doi.org/10.1109/TAI.2024.3404910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial networks (GANs) have been very successful for synthesizing the images in a given dataset. The artificially generated images by GANs are very realistic. The GANs have shown potential usability in several computer vision applications, including image generation, image-to-image translation, and video synthesis. Conventionally, the generator network is the backbone of GANs, which generates the samples, and the discriminator network is used to facilitate the training of the generator network. The generator and discriminator networks are usually a convolutional neural network (CNN). The convolution-based networks exploit the local relationship in a layer, which requires the deep networks to extract the abstract features. However, recently developed transformer networks are able to exploit the global relationship with tremendous performance improvement for several problems in computer vision. Motivated from the success of transformer networks and GANs, recent works have tried to exploit the transformers in GAN framework for the image/video synthesis. This article presents a comprehensive survey on the developments and advancements in GANs utilizing the transformer networks for computer vision applications. The performance comparison for several applications on benchmark datasets is also performed and analyzed. The conducted survey will be very useful to understand the research trends and gaps related with transformer-based GANs and to develop the advanced GAN architectures by exploiting the global and local relationships for different applications.},
  archive      = {J_TAI},
  author       = {Shiv Ram Dubey and Satish Kumar Singh},
  doi          = {10.1109/TAI.2024.3404910},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {10},
  pages        = {4851-4867},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Transformer-based generative adversarial networks in computer vision: A comprehensive survey},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel grades prediction method for undergraduate students
by learning explicit conditional distribution. <em>TAI</em>,
<em>5</em>(9), 4837–4848. (<a
href="https://doi.org/10.1109/TAI.2024.3416077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Educational data mining (EDM) offers an effective solution to predict students’ course grades in the next term. Conventional grade prediction methods can be viewed as regressing an expectation of the probability distribution of the student&#39;s grade, typically called single-value grade prediction. The reliable prediction outcomes of these methods depend on the complete input information related to students. However, next-term grade prediction often encounters the challenge of incomplete input information due to the inaccessibility of future data and the privacy of data. In this scenario, single-value grade prediction struggles to assess students’ academic status, as it may not be represented and assessed by relying on a singular expectation value. This limitation increases the risk of misjudgment, and may lead to errors in educational decision-making. Considering the challenge of collecting complete input information, we shift from traditional single-value predictions to forecasting the explicit probability distribution of the course grade. The probability distribution of the grade can assess the students’ academic status by providing probabilities corresponding to all possible grade values rather than relying solely on an expectation value, which offers the foundation to support the educators’ decision-making. In this article, the course grade distribution prediction (CGDP) model is proposed, aiming to estimate an explicit conditional probability distribution of course grades in the next term. This model can identify at-risk students, offering comprehensive decision-making information for educators and students. To ensure precise distribution predictions, a calibration method is also employed to improve the alignment between predicted and actual probabilities. Experimental results verify the effectiveness of the proposed model in early grade warning for undergraduates, based on real university data.},
  archive      = {J_TAI},
  author       = {Na Zhang and Ming Liu and Lin Wang and Shuangrong Liu and Runyuan Sun and Bo Yang and Shenghui Zhu and Chengdong Li and Cheng Yang and Yuhu Cheng},
  doi          = {10.1109/TAI.2024.3416077},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4837-4848},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A novel grades prediction method for undergraduate students by learning explicit conditional distribution},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-modality calibration in multi-input network for
axillary lymph node metastasis evaluation. <em>TAI</em>, <em>5</em>(9),
4823–4836. (<a href="https://doi.org/10.1109/TAI.2024.3397246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of deep neural networks (DNNs) in medical images has enabled the development of solutions characterized by the need of leveraging information coming from multiple sources, raising the multimodal deep learning. DNNs are known for their ability to provide hierarchical and high-level representations of input data. This capability has led to the introduction of methods performing data fusion at an intermediate level, preserving the distinctiveness of the heterogeneous sources in modality-specific paths, while learning the way to define an effective combination in a shared representation. However, modeling the intricate relationships between different data remains an open issue. In this article, we aim to improve the integration of data coming from multiple sources. We introduce between layers belonging to different modality-specific paths a transfer module (TM) able to perform the cross-modality calibration of the extracted features, reducing the effects of the less discriminative ones. As case of study, we focus on the axillary lymph nodes (ALNs) metastasis evaluation in malignant breast cancer (BC), a crucial prognostic factor, affecting patient&#39;s survival. We propose a multi-input single-output 3-D convolutional neural network (CNN) that considers both images acquired with multiparametric magnetic resonance and clinical information. In particular, we assess the proposed methodology using four architectures, namely BasicNet and three ResNet variants, showing the improvement of the performance obtained by including the TM in the network configuration. Our results achieve up to 90% and 87% of accuracy and area under ROC curve, respectively when the ResNet10 is considered, surpassing various fusion strategies proposed in the literature.},
  archive      = {J_TAI},
  author       = {Michela Gravina and Domiziana Santucci and Ermanno Cordelli and Paolo Soda and Carlo Sansone},
  doi          = {10.1109/TAI.2024.3397246},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4823-4836},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Cross-modality calibration in multi-input network for axillary lymph node metastasis evaluation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforced reweighting for self-supervised partial domain
adaptation. <em>TAI</em>, <em>5</em>(9), 4813–4822. (<a
href="https://doi.org/10.1109/TAI.2024.3397288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation enables the reduction of distribution differences across domains, allowing for effective knowledge transfer from one domain to a different domain. In recent years, partial domain adaptation (PDA) has attracted growing interest due to its focus on a more realistic scenario, where the target label space is a subset of the source label space. As the source and target domains do not possess the same label space in the PDA setting, it is challenging but crucial to mitigate the domain gap without incurring negative transfer. In this article, we propose a reinforced reweighting united with self-supervised adaptation (R2SA) method to address the challenges in PDA by leveraging the merits of deep reinforcement learning (DRL) and self-supervised learning (SSL) simultaneously in a cooperative way. Reinforced reweighting aims to learn a source reweighting policy automatically based on information provided by the PDA model, while self-supervised adaptation aims to boost the adaptability of the PDA model through an additional self-supervised objective on the target domain. Extensive experiments on several cross-domain benchmarks demonstrate that our method achieves state-of-the-art results, with larger performance gains on more challenging tasks.},
  archive      = {J_TAI},
  author       = {Keyu Wu and Shengkai Chen and Min Wu and Shili Xiang and Ruibing Jin and Yuecong Xu and Xiaoli Li and Zhenghua Chen},
  doi          = {10.1109/TAI.2024.3397288},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4813-4822},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Reinforced reweighting for self-supervised partial domain adaptation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MTPret: Improving x-ray image analytics with multitask
pretraining. <em>TAI</em>, <em>5</em>(9), 4799–4812. (<a
href="https://doi.org/10.1109/TAI.2024.3400750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While deep neural networks (DNNs) have been widely used in various X-ray image analytics tasks such as classification, segmentation, detection, etc., there frequently needs to collect and annotate a huge amount of training data to train a model for every single task. In this work, we proposed a multitask self-supervised pretraining strategy MTPret to improve the performance of DNNs in various X-ray analytics tasks. MTPret first trains the backbone to learn visual representations from multiple datasets of different tasks through contrastive learning, then MTPret leverages a multitask continual learning to learn discriminative features from various downstream tasks. To evaluate the performance of MTPret , we collected eleven X-ray image datasets from different body parts, such as heads, chest, lungs, bones, and etc., for various tasks to pretrain backbones, and fine-tuned the networks on seven of the tasks. The evaluation results on top of the seven tasks showed MTPret outperformed a large number of baseline methods, including other initialization strategies, pretrained models, and task-specific algorithms in recent studies. In addition, we also performed experiments based on two external tasks, where the datasets of external tasks have not been used in pretraining. The excellent performance of MTPret further confirmed the generalizability and superiority of the proposed multitask self-supervised pretraining.},
  archive      = {J_TAI},
  author       = {Weibin Liao and Qingzhong Wang and Xuhong Li and Yi Liu and Zeyu Chen and Siyu Huang and Dejing Dou and Yanwu Xu and Haoyi Xiong},
  doi          = {10.1109/TAI.2024.3400750},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4799-4812},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {MTPret: Improving X-ray image analytics with multitask pretraining},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated detection of harmful insects in agriculture: A
smart framework leveraging IoT, machine learning, and blockchain.
<em>TAI</em>, <em>5</em>(9), 4787–4798. (<a
href="https://doi.org/10.1109/TAI.2024.3394799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Paddy cultivation is a significant global economic sector, with rice production playing a crucial role in influencing worldwide economies. However, insects in paddy farms predominantly impact the growth rate and ecological equilibrium of the agricultural field. Hence, the precise and timely identification of insects in agricultural settings presents a potential strategy for addressing this issue. This study aims to implement an automated system for paddy farming by employing a real-time framework that incorporates the Internet of Things (IoT), blockchain technology, and Deep Learning (DL) algorithms. The primary emphasis of the DL-based system is on the timely identification of pests. In contrast, integrating the IoT and blockchain technologies facilitates stablishing a fully automated system with security within the agricultural domain. The DL-based system includes a secondary dataset of paddy insects, and then preprocessing, feature extraction, and identification have been performed. Besides, an IoT-based system is embodied with a camera module and microprocessor, accompanied by some apparatus required to automate the whole system. In addition, the research also includes the blockchain to secure each individual data transmission among the several IoT components and the cloud server. While examining the proposed solution, various experimental data have been systematically documented and analyzed. The proposed framework attained a peak accuracy of 98.91% using the VGG19 model and ensemble classifiers to detect the pest with a specificity of 99.14% and a precision of 98.21%. The study additionally quantifies the mean duration of the cloud response when integrated with IoT, yielding an average time of 1.71 s after pest identification. Nevertheless, the system has exhibited a high level of efficacy in the context of real-time monitoring and automation of paddy farms.},
  archive      = {J_TAI},
  author       = {Wahidur Rahman and Muhammad Minoar Hossain and Md. Mahedi Hasan and Md. Sadiq Iqbal and Mohammad Motiur Rahman and Khondokar Fida Hasan and Mohammad Ali Moni},
  doi          = {10.1109/TAI.2024.3394799},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4787-4798},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Automated detection of harmful insects in agriculture: A smart framework leveraging IoT, machine learning, and blockchain},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving code summarization with tree transformer enhanced
by position-related syntax complement. <em>TAI</em>, <em>5</em>(9),
4776–4786. (<a href="https://doi.org/10.1109/TAI.2024.3395231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code summarization aims to generate natural language (NL) summaries automatically given the source code snippet, which aids developers in understanding source code faster and improves software maintenance. Recent approaches using NL techniques in code summarization fall short of adequately capturing the syntactic characteristics of programming languages (PLs), particularly the position-related syntax, from which the semantics of the source code can be extracted. In this article, we present Syntax transforMer (SyMer) based on the transformer architecture where we enhance it with position-related syntax complement (PSC) to better capture syntactic characteristics. PSC takes advantage of unambiguous relations among code tokens in abstract syntax tree (AST), as well as the gathered attention on crucial code tokens indicated by its syntactic structure. The experimental results demonstrate that SyMer outperforms state-of-the-art models by at least 2.4% bilingual evaluation understudy (BLEU), 1.0% metric for evaluation of translation with explicit ORdering (METEOR) on Java benchmark, and 4.8% (BLEU), 5.1% (METEOR), and 3.2% recall-oriented understudy for gisting evaluation - longest common subsequence (ROUGE-L) on Python benchmark.},
  archive      = {J_TAI},
  author       = {Jie Song and Zexin Zhang and Zirui Tang and Shi Feng and Yu Gu},
  doi          = {10.1109/TAI.2024.3395231},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4776-4786},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Improving code summarization with tree transformer enhanced by position-related syntax complement},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ClassLIE: Structure- and illumination-adaptive
classification for low-light image enhancement. <em>TAI</em>,
<em>5</em>(9), 4765–4775. (<a
href="https://doi.org/10.1109/TAI.2024.3405405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light images often suffer from limited visibility and multiple types of degradation, rendering low-light image enhancement (LIE) a nontrivial task. Some endeavors have been made to enhance low-light images using convolutional neural networks (CNNs). However, they have low efficiency in learning the structural information and diverse illumination levels at the local regions of an image. Consequently, the enhanced results are affected by unexpected artifacts, such as unbalanced exposure, blur, and color bias. This article proposes a novel framework, called ClassLIE, that combines the potential of CNNs and transformers. It classifies and adaptively learns the structural and illumination information from the low-light images in a holistic and regional manner, thus showing better enhancement performance. Our framework first employs a structure and illumination classification (SIC) module to learn the degradation information adaptively. In SIC, we decompose an input image into an illumination map and a reflectance map. A class prediction block is then designed to classify the degradation information by calculating the structure similarity scores on the reflectance map and mean square error (MSE) on the illumination map. As such, each input image can be divided into patches with three enhancement difficulty levels. Then, a feature learning and fusion (FLF) module is proposed to adaptively learn the feature information with CNNs for different enhancement difficulty levels while learning the long-range dependencies for the patches in a holistic manner. Experiments on five benchmark datasets consistently show our ClassLIE achieves new state-of-the-art performance, with 25.74 peak signal-to-noise ratio (PSNR) and 0.92 structural similarity (SSIM) on the LOw-Light (LOL) dataset.},
  archive      = {J_TAI},
  author       = {Zixiang Wei and Yiting Wang and Lichao Sun and Athanasios V. Vasilakos and Lin Wang},
  doi          = {10.1109/TAI.2024.3405405},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4765-4775},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {ClassLIE: Structure- and illumination-adaptive classification for low-light image enhancement},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variable curvature gabor convolution and multibranch
structures for finger vein recognition. <em>TAI</em>, <em>5</em>(9),
4753–4764. (<a href="https://doi.org/10.1109/TAI.2024.3397293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gabor filters are able to extract texture features from finger vein images from different directions and scales. However, manually crafted Gabor filters have problems such as relatively single direction and scale, and difficulties in parameter adjustment to adapt to specific datasets. To solve these problems, this article proposes a neural network with a learnable variable curvature Gabor (VC-Gabor) convolutional layer. First, the Gabor filter is improved by adding variable curvature to extract information about different curvature degrees in the vein curves. Second, the VC-Gabor filter is designed as a learnable convolutional filter, with parameters updated using neural network back-propagation. This facilitates the enrichment of learned VC-Gabor filter directions, scales, and curvatures, eliminating the need for intricate manual parameter tuning. Finally, we propose adaptive multibranch structures for feature extraction, which are used to enhance the feature extraction capability of the model. Experimental results on publicly available datasets FV-USM and SDUMLA demonstrate that the proposed algorithm improves recognition accuracy and reduces the equal error rate (EER), thereby substantiating the effectiveness of the approach.},
  archive      = {J_TAI},
  author       = {Jun Li and Huabin Wang and Shicheng Wei and Jian Zhou and Yuankang Shen and Liang Tao},
  doi          = {10.1109/TAI.2024.3397293},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4753-4764},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Variable curvature gabor convolution and multibranch structures for finger vein recognition},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic plane pose estimation for cardiac left ventricle
coverage estimation via deep adversarial regression network.
<em>TAI</em>, <em>5</em>(9), 4738–4752. (<a
href="https://doi.org/10.1109/TAI.2024.3394798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of the ventricles plays a crucial role in determining cardiac functional parameters such as ventricular volume, ventricular mass, and ejection fraction. However, poor image quality, such as inadequate coverage of the left ventricle (LV) and right ventricle (RV) in cardiac magnetic resonance (CMR) image sequences, can significantly affect the assessment of cardiac function. This study investigates issues related to missing or corrupted imaging planes, which often lead to incomplete ventricle coverage. To address the challenge of estimating ventricle coverage in CMR images regardless of variations in imaging parameters such as device type, magnetic field strength, and protocol execution, we introduce a novel convolutional neural network (CNN) based on adversarial learning. Additionally, we integrate supplementary information (e.g., cross-view image data) as privileged information to enhance the interpretability of our model&#39;s predictions and identify potential biases or inaccuracies. This research represents the first attempt to automatically estimate ventricular coverage by identifying missing slices and plane orientations in CMR images using a dataset-agnostic approach. The effectiveness of the proposed model is demonstrated through the evaluation of datasets from three diverse and sizable image acquisition cohorts, demonstrating superior performance compared to existing methods.},
  archive      = {J_TAI},
  author       = {Le Zhang and Kevin Bronik and Stefan K. Piechnik and Joao A. C. Lima and Stefan Neubauer and Steffen E. Petersen and Alejandro F. Frangi},
  doi          = {10.1109/TAI.2024.3394798},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4738-4752},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Automatic plane pose estimation for cardiac left ventricle coverage estimation via deep adversarial regression network},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quadratic neuron-empowered heterogeneous autoencoder for
unsupervised anomaly detection. <em>TAI</em>, <em>5</em>(9), 4723–4737.
(<a href="https://doi.org/10.1109/TAI.2024.3394795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the complexity and diversity of biological neurons, a quadratic neuron is proposed to replace the inner product in the current neuron with a simplified quadratic function. Employing such a novel type of neurons offers a new perspective on developing deep learning. When analyzing quadratic neurons, we find that there exists a function such that a heterogeneous network can approximate it well with a polynomial number of neurons but a purely conventional or quadratic network needs an exponential number of neurons to achieve the same level of error. Encouraged by this inspiring theoretical result on heterogeneous networks, we directly integrate conventional and quadratic neurons in an autoencoder to make a new type of heterogeneous autoencoders. To our best knowledge, it is the first heterogeneous autoencoder that is made of different types of neurons. Next, we apply the proposed heterogeneous autoencoder to unsupervised anomaly detection (AD) for tabular data and bearing fault signals. The AD faces difficulties such as data unknownness, anomaly feature heterogeneity, and feature unnoticeability, which is suitable for the proposed heterogeneous autoencoder. Its high feature representation ability can characterize a variety of anomaly data (heterogeneity), discriminate the anomaly from the normal (unnoticeability), and accurately learn the distribution of normal samples (unknownness). Experiments show that heterogeneous autoencoders perform competitively compared with other state-of-the-art models.},
  archive      = {J_TAI},
  author       = {Jing-Xiao Liao and Bo-Jian Hou and Hang-Cheng Dong and Hao Zhang and Xiaoge Zhang and Jinwei Sun and Shiping Zhang and Feng-Lei Fan},
  doi          = {10.1109/TAI.2024.3394795},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4723-4737},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Quadratic neuron-empowered heterogeneous autoencoder for unsupervised anomaly detection},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SBP-GCA: Social behavior prediction via graph contrastive
learning with attention. <em>TAI</em>, <em>5</em>(9), 4708–4722. (<a
href="https://doi.org/10.1109/TAI.2024.3395574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social behavior prediction on social media is attracting significant attention from researchers. Social e-commerce focuses on engagement marketing, which emphasizes social behavior because it effectively increases brand recognition. Currently, existing works on social behavior prediction suffer from two main problems: 1) they assume that social influence probabilities can be learned independently of each other, and their calculations do not include any influence probability estimations based on friends’ behavior; and 2) negative sampling of subgraphs is usually ignored in social behavior prediction work. To the best of our knowledge, introducing graph contrastive learning (GCL) to social behavior prediction is novel and interesting. In this article, we propose a framework, social behavior prediction via graph contrastive learning with attention named SBP-GCA, to promote social behavior prediction. First, two methods are designed to extract subgraphs from the original graph, and their structural features are learned by GCL. Then, it models how a user&#39;s behavior is influenced by neighbors and learns influence features via graph attention networks (GATs). Furthermore, it combines structural features, influence features, and intrinsic features to predict social behavior. Extensive and systematic experiments on three datasets validate the superiority of the proposed SBP-GCA.},
  archive      = {J_TAI},
  author       = {Yufei Liu and Jia Wu and Jie Cao},
  doi          = {10.1109/TAI.2024.3395574},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4708-4722},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {SBP-GCA: Social behavior prediction via graph contrastive learning with attention},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UPR-BP: Unsupervised photoplethysmography representation
learning for noninvasive blood pressure estimation. <em>TAI</em>,
<em>5</em>(9), 4696–4707. (<a
href="https://doi.org/10.1109/TAI.2024.3396126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents unsupervised photoplethysmography representation learning for noninvasive blood pressure (UPR-BP), a novel framework utilizing photoplethysmography (PPG) signals for accurate, noninvasive BP estimation. Leveraging readily available unlabeled PPG data, UPR-BP overcomes the limitations of data-driven models by effectively capturing discriminative BP features without requiring extensive paired measurements. Our framework employs a three-branch architecture with shared weights for joint optimization and incorporates preservation of invariance, variance, and covariance in the PPG temporal encoding, preventing information collapse and generating meaningful deep representations. Additionally, temporal neighborhood coding facilitates the identification of diverse physiological states within the PPG signals. We comprehensively validate UPR-BP on diverse datasets from bedside monitors and wearable wristwatches, encompassing over 4000 subjects. The proposed approach achieves medical-grade accuracy, demonstrating significant superiority to state-of-the-art techniques with a low estimation error of 0.30 ± 4.68 mmHg for systolic BP and 0.25 ± 2.66 mmHg for diastolic BP. These results highlight the potential of UPR-BP for significantly advancing continuous, noninvasive BP monitoring in clinical settings.},
  archive      = {J_TAI},
  author       = {Chenbin Ma and Peng Zhang and Fan Song and Zeyu Liu and Youdan Feng and Yufang He and Guanglei Zhang},
  doi          = {10.1109/TAI.2024.3396126},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4696-4707},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {UPR-BP: Unsupervised photoplethysmography representation learning for noninvasive blood pressure estimation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Remaining useful life prediction via frequency emphasizing
mix-up and masked reconstruction. <em>TAI</em>, <em>5</em>(9),
4686–4695. (<a href="https://doi.org/10.1109/TAI.2024.3396422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of the remaining useful lifetime (RUL) of machines and tools is crucial in the realm of modern manufacturing and in the framework of Industry 4.0. Recent deep learning techniques offer an opportunity for the utilization of data-driven methods in RUL prediction. However, the persistent data scarcity issue presents a thorny bottleneck in the machinery RUL prediction tasks due to the high cost of labeled training data collection. In this article, we propose an effective learning framework for RUL prediction consisting of two novel methodological modules to improve data utilization efficiency. The first module is to leverage the intrinsically decomposed frequency-domain information to boost the effective feature extraction with a novel-designed frequency emphasizing mix-up module (FEMM) . The second one involves incorporating semisupervised learning to make use of unrestricted domain unlabeled data to overcome the constraints associated with data scarcity where we introduced masked autoencoder reconstruction auxiliary learning (MARAL) to the model. In addition, to better obtain temporal information, an LSTM temporal projection layer is designed. The proposed method was evaluated through experiments conducted on the commercial modular aero-propulsion system simulation (C-MAPSS) datasets, and our results show that the proposed method outperforms existing other methods in terms of both accuracy and effectiveness in the RUL prediction. In addition, our experimental results demonstrate that the proposed method is able to leverage unrestricted domain datasets to significantly boost model performance under low-data scenarios.},
  archive      = {J_TAI},
  author       = {Haoren Guo and Haiyue Zhu and Jiahui Wang and Vadakkepat Prahlad and Weng Khuen Ho and Clarence W. de Silva and Tong Heng Lee},
  doi          = {10.1109/TAI.2024.3396422},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4686-4695},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Remaining useful life prediction via frequency emphasizing mix-up and masked reconstruction},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A causality-informed graph intervention model for pancreatic
cancer early diagnosis. <em>TAI</em>, <em>5</em>(9), 4675–4685. (<a
href="https://doi.org/10.1109/TAI.2024.3395586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pancreatic cancer is a highly fatal cancer type. Patients are typically in an advanced stage at their first diagnosis, mainly due to the absence of distinctive early stage symptoms and lack of effective early diagnostic methods. In this work, we propose an automated method for pancreatic cancer diagnosis using noncontrast computed tomography (CT), taking advantage of its widespread availability in clinic. Currently, a primary challenge limiting the clinical value of intelligent systems is low generalization, i.e., the difficulty of achieving stable performance across datasets from different medical sources. To address this challenge, a novel causality-informed graph intervention model is developed based on a multi-instance-learning framework integrated with graph neural network (GNN) for the extraction of local discriminative features. Within this model, we develop a graph causal intervention scheme with three levels of intervention for graph nodes, structures, and representations. This scheme systematically suppresses noncausal factors and thus lead to generalizable predictions. Specifically, first, a target node perturbation strategy is designed to capture target-region features. Second, a causal-structure separation module is developed to automatically identify the causal graph structures for obtaining stable representations of whole target regions. Third, a graph-level feature consistency mechanism is proposed to extract invariant features. Comprehensive experiments on large-scale datasets validated the promising early diagnosis performance of our proposed model. The model generalizability was confirmed on three independent datasets, where the classification accuracy reached 86.3%, 80.4%, and 82.2%, respectively. Overall, we provide a valuable potential tool for pancreatic cancer screening and early diagnosis.},
  archive      = {J_TAI},
  author       = {Xinyue Li and Rui Guo and Hongzhang Zhu and Tao Chen and Xiaohua Qian},
  doi          = {10.1109/TAI.2024.3395586},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4675-4685},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A causality-informed graph intervention model for pancreatic cancer early diagnosis},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised representation learning for 3-d magnetic
resonance imaging superresolution with degradation adaptation.
<em>TAI</em>, <em>5</em>(9), 4660–4674. (<a
href="https://doi.org/10.1109/TAI.2024.3397292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-resolution (HR) magnetic resonance imaging (MRI) is essential in aiding doctors in their diagnoses and image-guided treatments. However, acquiring HR images can be time-consuming and costly. Consequently, deep learning-based superresolution reconstruction (SRR) has emerged as a promising solution for generating superresolution (SR) images from low-resolution (LR) images. Unfortunately, training such neural networks requires aligned authentic HR and LR image pairs, which are challenging to obtain due to patient movements during and between image acquisitions. While rigid movements of hard tissues can be corrected with image registration, aligning deformed soft tissues is complex, making it impractical to train neural networks with authentic HR and LR image pairs. Previous studies have focused on SRR using authentic HR images and downsampled synthetic LR images. However, the difference in degradation representations between synthetic and authentic LR images suppresses the quality of SR images reconstructed from authentic LR images. To address this issue, we propose a novel unsupervised degradation adaptation network (UDEAN). Our network consists of a degradation learning network and an SRR network. The degradation learning network downsamples HR images using the degradation representation learned from misaligned or unpaired LR images. The SRR network then learns to map the downsampled HR images to the original ones. Experimental results show that our method outperforms state-of-the-art networks with an improvement of up to 0.051/3.52 dB in structural similarity (SSIM)/peak signal-to-noise ratio (PSNR) on two public datasets, thus is a promising solution to the challenges in clinical settings.},
  archive      = {J_TAI},
  author       = {Jianan Liu and Hao Li and Tao Huang and Euijoon Ahn and Kang Han and Adeel Razi and Wei Xiang and Jinman Kim and David Dagan Feng},
  doi          = {10.1109/TAI.2024.3397292},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4660-4674},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Unsupervised representation learning for 3-D magnetic resonance imaging superresolution with degradation adaptation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An explainable intellectual property protection method for
deep neural networks based on intrinsic features. <em>TAI</em>,
<em>5</em>(9), 4649–4659. (<a
href="https://doi.org/10.1109/TAI.2024.3388389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intellectual property (IP) protection for deep neural networks (DNNs) has raised serious concerns in recent years. Most existing works embed watermarks in the DNN model for IP protection, which need to modify the model and do not consider/mention interpretability. In this article, for the first time, we propose an interpretable IP protection method for DNN based on explainable artificial intelligence. Compared with existing works, the proposed method does not modify the DNN model, and the decision of the ownership verification is interpretable. We extract the intrinsic features of the DNN model by using deep Taylor decomposition. Since the intrinsic feature is composed of unique interpretation of the model&#39;s decision, the intrinsic feature can be regarded as fingerprint of the model. If the fingerprint of a suspected model is the same as the original model, the suspected model is considered as a pirated model. Experimental results demonstrate that the fingerprints can be successfully used to verify the ownership of the model and the test accuracy of the model is not affected. Furthermore, the proposed method is robust to fine-tuning attack, pruning attack, watermark overwriting attack, and adaptive attack.},
  archive      = {J_TAI},
  author       = {Mingfu Xue and Xin Wang and Yinghao Wu and Shifeng Ni and Leo Yu Zhang and Yushu Zhang and Weiqiang Liu},
  doi          = {10.1109/TAI.2024.3388389},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4649-4659},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An explainable intellectual property protection method for deep neural networks based on intrinsic features},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Strategic gradient transmission with targeted
privacy-awareness in model training: A stackelberg game analysis.
<em>TAI</em>, <em>5</em>(9), 4635–4648. (<a
href="https://doi.org/10.1109/TAI.2024.3389611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy-aware machine learning paradigms have sparked widespread concern due to their ability to safeguard the local privacy of data owners, preventing the leakage of private information to untrustworthy platforms or malicious third parties. This article focuses on characterizing the interactions between the learner and the data owner within this privacy-aware training process. Here, the data owner hesitates to transmit the original gradient to the learner due to potential cybersecurity issues, such as gradient leakage and membership inference. To address this concern, we propose a Stackelberg game framework that models the training process. In this framework, the data owner&#39;s objective is not to maximize the discrepancy between the learner&#39;s obtained gradient and the true gradient but rather to ensure that the learner obtains a gradient closely resembling one deliberately designed by the data owner, while the learner&#39;s objective is to recover the true gradient as accurately as possible. We derive the optimal encoder and decoder using mismatched cost functions and characterize the equilibrium for specific cases, balancing model accuracy and local privacy. Numerical examples illustrate the main results, and we conclude with expanding discussions to suggest future investigations into reliable countermeasure designs.},
  archive      = {J_TAI},
  author       = {Hezhe Sun and Yufei Wang and Huiwen Yang and Kaixuan Huo and Yuzhe Li},
  doi          = {10.1109/TAI.2024.3389611},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4635-4648},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Strategic gradient transmission with targeted privacy-awareness in model training: A stackelberg game analysis},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Linear regression-based autonomous intelligent optimization
for constrained multiobjective problems. <em>TAI</em>, <em>5</em>(9),
4620–4634. (<a href="https://doi.org/10.1109/TAI.2024.3391230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is very challenging to autonomously generate algorithms suitable for constrained multiobjective optimization problems due to the diverse performance of existing algorithms. In this article, we propose a linear regression (LR)-based autonomous intelligent optimization method. It first extracts typical features of a constrained multiobjective optimization problem by focused sampling to form a feature vector. Then, a LR model is designed to learn the relationship between optimization problems and intelligent optimization algorithms (IOAs). Finally, the trained model autonomously generates a suitable IOA by inputting the feature vector. The proposed method is applied to six constrained multiobjective benchmark test sets with various characteristics and compared with seven popular optimization algorithms. The experimental results verify the effectiveness of the proposed method. In addition, the proposed method is used to solve the operation optimization problems of an integrated coal mine energy system, and the experimental results show its practicability.},
  archive      = {J_TAI},
  author       = {Yan Wang and Xiaoyan Sun and Yong Zhang and Dunwei Gong and Hejuan Hu and Mingcheng Zuo},
  doi          = {10.1109/TAI.2024.3391230},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4620-4634},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Linear regression-based autonomous intelligent optimization for constrained multiobjective problems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning counterfactual explanation of graph neural networks
via generative flow network. <em>TAI</em>, <em>5</em>(9), 4607–4619. (<a
href="https://doi.org/10.1109/TAI.2024.3387406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Counterfactual subgraphs explain graph neural networks (GNNs) by answering the question: “How would the prediction change if a certain subgraph were absent in the input instance?” The differentiable proxy adjacency matrix is prevalent in current counterfactual subgraph discovery studies due to its ability to avoid exhaustive edge searching. However, a prediction gap exists when feeding the proxy matrix with continuous values and the thresholded discrete adjacency matrix to GNNs, compromising the optimization of the subgraph generator. Furthermore, the end-to-end learning schema adopted in the subgraph generator limits the diversity of counterfactual subgraphs. To this end, we propose CF-GFNExplainer, a flow-based approach for learning counterfactual subgraphs. CF-GFNExplainer employs a policy network with a discrete edge removal schema to construct counterfactual subgraph generation trajectories. Additionally, we introduce a loss function designed to guide CF-GFNExplainer&#39;s optimization. The discrete adjacency matrix generated in each trajectory eliminates the prediction gap, enhancing the validity of the learned subgraphs. Furthermore, the multitrajectories sampling strategy adopted in CF-GFNExplainer results in diverse counterfactual subgraphs. Extensive experiments conducted on synthetic and real-world datasets demonstrate the effectiveness of the proposed method in terms of validity and diversity.},
  archive      = {J_TAI},
  author       = {Kangjia He and Li Liu and Youmin Zhang and Ye Wang and Qun Liu and Guoyin Wang},
  doi          = {10.1109/TAI.2024.3387406},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4607-4619},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Learning counterfactual explanation of graph neural networks via generative flow network},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified conditional diffusion framework for dual protein
targets-based bioactive molecule generation. <em>TAI</em>,
<em>5</em>(9), 4595–4606. (<a
href="https://doi.org/10.1109/TAI.2024.3387402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in deep generative models shed light on de novo molecule generation with desired properties. However, molecule generation targeted for dual protein targets still faces formidable challenges including insufficient protein 3-D structure data requisition for conditioned model training, inflexibility of auto-regressive sampling, and model generalization to unseen targets. Here, this study proposed diffusion model for dual targets-based molecule generation (DiffDTM), a novel unified structure-free deep generative framework based on a diffusion model for dual-target based molecule generation to address the above issues. Specifically, DiffDTM receives representations of protein sequences and molecular graphs pretrained on large-scale datasets as inputs instead of protein and molecular conformations and incorporates an information fusion module to achieve conditional generation in a one-shot manner. We perform comprehensive multiview experiments to demonstrate that DiffDTM can generate druglike, synthesis-accessible, novel, and high-binding affinity molecules targeting specific dual proteins, outperforming the state-of-the-art (SOTA) models in terms of multiple evaluation metrics. Furthermore, DiffDTM could directly generate molecules toward dopamine receptor D2 (DRD2) and 5-hydroxytryptamine receptor 1A (HTR1A) as new antipsychotics. Experimental comparisons highlight the generalizability of DiffDTM to easily adapt to unseen dual targets and generate bioactive molecules, addressing the issues of insufficient active molecule data for model training when new targets are encountered.},
  archive      = {J_TAI},
  author       = {Lei Huang and Zheng Yuan and Huihui Yan and Rong Sheng and Linjing Liu and Fuzhou Wang and Weidun Xie and Nanjun Chen and Fei Huang and Songfang Huang and Ka-Chun Wong and Yaoyun Zhang},
  doi          = {10.1109/TAI.2024.3387402},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4595-4606},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A unified conditional diffusion framework for dual protein targets-based bioactive molecule generation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Stabilizing diffusion model for robotic control with
dynamic programming and transition feasibility. <em>TAI</em>,
<em>5</em>(9), 4585–4594. (<a
href="https://doi.org/10.1109/TAI.2024.3387401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its strong ability in distribution representation, the diffusion model has been incorporated into offline reinforcement learning (RL) to cover diverse trajectories of the complex behavior policy. However, this also causes several challenges. Training the diffusion model to imitate behavior from the collected trajectories suffers from limited stitching capability which derives better policies from suboptimal trajectories. Furthermore, the inherent randomness of the diffusion model can lead to unpredictable control and dangerous behavior for the robot. To address these concerns, we propose the value-learning-based decision diffuser (V-DD), which consists of the trajectory diffusion module (TDM) and the trajectory evaluation module (TEM). During the training process, the TDM combines the state-value and classifier-free guidance to bolster the ability to stitch suboptimal trajectories. During the inference process, we design the TEM to select a feasible trajectory generated by the diffusion model. Empirical results demonstrate that our method delivers competitive results on the D4RL benchmark and substantially outperforms current diffusion model-based methods on the real-world robot task.},
  archive      = {J_TAI},
  author       = {Haoran Li and Yaocheng Zhang and Haowei Wen and Yuanheng Zhu and Dongbin Zhao},
  doi          = {10.1109/TAI.2024.3387401},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4585-4594},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Stabilizing diffusion model for robotic control with dynamic programming and transition feasibility},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An unbiased fuzzy weighted relative error support vector
machine for reverse prediction of concrete components. <em>TAI</em>,
<em>5</em>(9), 4574–4584. (<a
href="https://doi.org/10.1109/TAI.2024.3385386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concrete is a vital component in modern construction, prized for its strength, durability, and versatility. Accurately determining the quantities of concrete components is crucial in civil engineering applications to optimize resources (e.g., manpower and financial resources). In this article, we propose an unbiased fuzzy-weighted relative error support vector machine (UFW-RE-SVM) for reverse prediction of concrete components. First, we add an unbiased term to the target function of UFW-RE-SVM for obtaining an unbiased model. Second, we design a fuzzy-weighted operation to indicate sample importance by incorporating the fuzzy membership values into the UFW-RE-SVM. The $n$ th root operation is introduced to address the exponential explosion issue in the fuzzy-weighted operation. Finally, considering the UFW-RE-SVM is sensitive to its hyperparameters for multioutput prediction, the whale optimization algorithm (WOA) is utilized for hyperparameter optimization for its effectiveness in optimization tasks. We design the fitness function based on the results from multiple components to balance the performance of multioutput predictions. Experimental results show that the performance of our proposed model outperforms existing works in predicting concrete components in terms of mean absolute relative error, standard deviation, and root mean square error. Further, the statistical test shows the WOA and two other metaheuristics can significantly improve the prediction performance. This indicates that the unbiased term, fuzzy-weighted operation, and WOA are effective for improving the proposed model for reverse prediction concrete components. With these promising results, the proposed model could provide decision-makers with a valuable tool for determining concrete component quantities based on desired concrete qualities.},
  archive      = {J_TAI},
  author       = {Zongwen Fan and Jin Gou and Shaoyuan Weng},
  doi          = {10.1109/TAI.2024.3385386},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4574-4584},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An unbiased fuzzy weighted relative error support vector machine for reverse prediction of concrete components},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IOTM: Iterative optimization trigger method—a runtime
data-free backdoor attacks on deep neural networks. <em>TAI</em>,
<em>5</em>(9), 4562–4573. (<a
href="https://doi.org/10.1109/TAI.2024.3384938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks are susceptible to various backdoor attacks, such as training time attacks, where the attacker can inject a trigger pattern into a small portion of the dataset to control the model&#39;s predictions at runtime. Backdoor attacks are dangerous because they do not degrade the model&#39;s performance. This article explores the feasibility of a new type of backdoor attack, a data-free backdoor. Unlike traditional backdoor attacks that require poisoning data and injection during training, our approach, the iterative optimization trigger method (IOTM), enables trigger generation without compromising the integrity of the models and datasets. We propose an attack based on an IOTM technique, guided by an adaptive trigger generator (ATG) and employing a custom objective function. ATG dynamically refines the trigger using feedback from the model&#39;s predictions. We empirically evaluated the effectiveness of IOTM with three deep learning models (CNN, VGG16, and ResNet18) using the CIFAR10 dataset. The achieved runtime-attack success rate (R-ASR) varies across different classes. For some classes, the R-ASR reached 100%; whereas, for others, it reached 62%. Furthermore, we conducted an ablation study to investigate critical factors in the runtime backdoor, including optimizer, weight, “REG,” and trigger visibility on R-ASR using the CIFAR100 dataset. We observed significant variations in the R-ASR by changing the optimizer, including Adam and SGD, with and without momentum. The R-ASR reached 81.25% with the Adam optimizer, whereas the SGD with momentum and without results reached 46.87% and 3.12%, respectively.},
  archive      = {J_TAI},
  author       = {Iram Arshad and Saeed Hamood Alsamhi and Yuansong Qiao and Brian Lee and Yuhang Ye},
  doi          = {10.1109/TAI.2024.3384938},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4562-4573},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {IOTM: Iterative optimization trigger Method—A runtime data-free backdoor attacks on deep neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prioritized local matching network for cross-category
few-shot anomaly detection. <em>TAI</em>, <em>5</em>(9), 4550–4561. (<a
href="https://doi.org/10.1109/TAI.2024.3385743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the rapid evolution of products in industrial inspection, this article introduces the cross-category few-shot anomaly detection (C-FSAD) task, aimed at efficiently detecting anomalies in new object categories with minimal normal samples. However, the diversity of defects and significant visual distinctions among various objects hinder the identification of anomalous regions. To tackle this, we adopt a pairwise comparison between query and normal samples, establishing an intimate correlation through fine-grained correspondence. Specifically, we propose the prioritized local matching network (PLMNet), emphasizing local analysis of correlation, which includes three primary components: 1) Local perception network refines the initial matches through bidirectional local analysis; 2) step aggregation strategy employs multiple stages of local convolutional pooling to aggregate local insights; and 3) defect-sensitive Weight Learner adaptively enhances channels informative for defect structures, ensuring more discriminative representations of encoded context. Our PLMNet deepens the interpretation of correlations, from geometric cues to semantics, efficiently extracting discrepancies in feature space. Extensive experiments on two standard industrial anomaly detection benchmarks demonstrate our state-of-the-art performance in both detection and localization, with margins of 9.8% and 5.4%, respectively.},
  archive      = {J_TAI},
  author       = {Huilin Deng and Hongchen Luo and Wei Zhai and Yanming Guo and Yang Cao and Yu Kang},
  doi          = {10.1109/TAI.2024.3385743},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4550-4561},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Prioritized local matching network for cross-category few-shot anomaly detection},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel applicable shadow resistant neural network model for
high-efficiency grid-level pavement crack detection. <em>TAI</em>,
<em>5</em>(9), 4535–4549. (<a
href="https://doi.org/10.1109/TAI.2024.3386149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address two key challenges—limited grid-level detection capability and difficulty in detecting pavement cracks in complex environments, this study proposes a novel neural network model called CrackcellNet. This innovative model incorporates an output structure that enables end-to-end grid recognition and a module that enhances shadow image data to enhance crack detection. The model relies on the design of consecutive pooling layers to achieve adaptive target size grid output. By utilizing image fusion techniques, it enhances the quantity of shadow data in road surface detection. The results of ablation experiments indicate that the optimal configuration for CrackcellNet includes V-block and shadow augmentation operations, dilation rates of 1 or 2, and a convolutional layer in the CBA module. Through extensive experimentation, we have demonstrated that our model achieved an accuracy rate of 94.5% for grid-level crack detection and a F1 value of 0.839. Furthermore, practical engineering validation confirms the model&#39;s efficacy with an average PCIe of 0.045, providing valuable guidance for road maintenance decisions.},
  archive      = {J_TAI},
  author       = {Handuo Yang and Ju Huyan and Tao Ma and Yitao Song and Chengjia Han},
  doi          = {10.1109/TAI.2024.3386149},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4535-4549},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A novel applicable shadow resistant neural network model for high-efficiency grid-level pavement crack detection},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An intelligent fingerprinting technique for low-power
embedded IoT devices. <em>TAI</em>, <em>5</em>(9), 4519–4534. (<a
href="https://doi.org/10.1109/TAI.2024.3386498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) has been a popular topic for research and development in the past decade. The resource-constrained and wireless nature of IoT devices presents a large surface of vulnerabilities, and traditional network security methods involving complex cryptography are not feasible. Studies show that Denial of Service (DoS), physical intrusion, spoofing, and node forgery are prevalent threats in the IoT, and there is a need for robust, lightweight device fingerprinting schemes. We identify eight criteria of effective fingerprinting methods for resource-constrained IoT devices and propose an intelligent, lightweight, whitelist-based fingerprinting method that satisfies these properties. The proposed method uses the power-up Static Random Access Memory (SRAM) stack as fingerprint features and autoencoder networks (AEN) for fingerprint registration and verification. We also present a threat mitigation framework based on network isolation levels to handle potential and identified threats. Experiments are conducted with a heterogeneous pool of 10 advanced virtual reduced instruction set computer (AVR) Harvard architecture prover devices from different vendors, and Dell Latitude and Dell XPS 13 laptops are used as verifier testbeds. The proposed method has a 99.9% accuracy, 100% precision, and 99.6% recall on known and unknown heterogeneous devices, which is an improvement over several past works. The independence of fingerprints stored in the AENs enables easy distribution and update, and the observed evaluation latency ( $\sim$ $10^{-4}$ s) and data collection latency ( $\sim$ $1$ s) make our method practical for real-world scenarios. Lastly, we analyze the proposed method with regard to the eight criteria and highlight its limitations for future improvement.},
  archive      = {J_TAI},
  author       = {Varun Kohli and Muhammad Naveed Aman and Biplab Sikdar},
  doi          = {10.1109/TAI.2024.3386498},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4519-4534},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An intelligent fingerprinting technique for low-power embedded IoT devices},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shuffled grouping cross-channel attention-based
bilateral-filter-interpolation deformable ConvNet with applications to
benthonic organism detection. <em>TAI</em>, <em>5</em>(9), 4506–4518.
(<a href="https://doi.org/10.1109/TAI.2024.3385387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, to holistically tackle underwater detection degradation due to unknown geometric variation arising from scale, pose, viewpoint, and occlusion under low-contrast and color-distortion circumstances, a shuffled grouping cross-channel attention-based bilateral-filter-interpolation deformable ConvNet (SGCA-BDC) framework is established for benthonic organism detection (BOD). Main contributions are as follows: 1) By comprehensively considering spatial and feature similarities between offset and integral coordinate positions, the BDC with modulation weight mechanism is created, such that sampling ability of convolutional kernel for BO with unknown geometric variation can be adaptively augmented from spatial perspective; 2) By utilizing 1-D convolution to recalibrate channel weight for grouped subfeature via information entropy statistic technique, an SGCA module is innovated, such that seabed background noise can be suppressed from channel aspect; 3) The proposed SGCA-BDC scheme is eventually built in an organic manner by incorporating BDC and SGCA modules. Comprehensive experiments and comparisons demonstrate that the SGCA-BDC scheme remarkably outperforms typical detection approaches including Faster RCNN, SSD, YOLOv6, YOLOv7, YOLOv8, RetinaNet, and CenterNet in terms of mean average precision by 8.54%, 4.4%, 5.18%, 3.1%, 3.01%, 12.53%, and 7.09%, respectively.},
  archive      = {J_TAI},
  author       = {Tingkai Chen and Ning Wang},
  doi          = {10.1109/TAI.2024.3385387},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4506-4518},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Shuffled grouping cross-channel attention-based bilateral-filter-interpolation deformable ConvNet with applications to benthonic organism detection},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prefetching-based multiproposal markov chain monte carlo
algorithm. <em>TAI</em>, <em>5</em>(9), 4493–4505. (<a
href="https://doi.org/10.1109/TAI.2024.3385384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our proposed algorithm is a prefetching-based multiproposal Markov Chain Monte Carlo (PMP-MCMC) method that efficiently explores the target distribution by combining multiple proposals with the concept of prefetching. In our method, not all proposals are directly derived from the current state; some are derived from future states. This approach breaks through the inherent sequential characteristics of traditional MCMC algorithms. Compared with single-proposal and multiproposal methods, our approach speeds up by $K$ times and the burn-in period is reduced by a factor of $1/\text{log}_{2}K$ maximally, where $K$ is the number of parallel computational units or processing cores. Compared with prefetching method, our method has increased the number of samples per iteration by a factor of $K/\text{log}_{2}K$ . Furthermore, the proposed method is general and can be integrated into MCMC variants such as Hamiltonian Monte Carlo (HMC). We have also applied this method to optimize the model parameters of neural networks and Bayesian inference and observed significant improvements in optimization performance.},
  archive      = {J_TAI},
  author       = {Guifeng Ye and Shaowen Lu},
  doi          = {10.1109/TAI.2024.3385384},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4493-4505},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Prefetching-based multiproposal markov chain monte carlo algorithm},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Retain and adapt: Online sequential EEG classification with
subject shift. <em>TAI</em>, <em>5</em>(9), 4479–4492. (<a
href="https://doi.org/10.1109/TAI.2024.3385390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large variance exists in Electroencephalogram (EEG) signals with its pattern differing significantly across subjects. It is a challenging problem to perform online sequential decoding of EEG signals across different subjects, where a sequence of subjects arrive in temporal order and no signal data is jointly available beforehand. The challenges include the following two aspects: 1) the knowledge learned from previous subjects does not readily fit to future subjects, and fast adaptation is needed in the process; and 2) the EEG classifier could drastically erase information of learnt subjects as learning progresses, namely catastrophic forgetting. Most existing EEG decoding explorations use sizable data for pretraining purposes, and to the best of our knowledge we are the first to tackle this challenging online sequential decoding setting. In this work, we propose a unified bi-level meta-learning framework that enables the EEG decoder to simultaneously perform fast adaptation on future subjects and retain knowledge of previous subjects. In addition, we extend to the more general subject-agnostic scenario and propose a subject shift detection algorithm for situations that subject identity and the occurrence of subject shifts are unknown. We conducted experiments on three public EEG datasets for both subject-aware and subject-agnostic scenarios. The proposed method demonstrates its effectiveness in most of the ablation settings, e.g. an improvement of 5.73% for forgetting mitigation and 3.50% for forward adaptation on SEED dataset for subject agnostic scenarios.},
  archive      = {J_TAI},
  author       = {Tiehang Duan and Zhenyi Wang and Li Shen and Gianfranco Doretto and Donald A. Adjeroh and Fang Li and Cui Tao},
  doi          = {10.1109/TAI.2024.3385390},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4479-4492},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Retain and adapt: Online sequential EEG classification with subject shift},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Building a robust and efficient defensive system using
hybrid adversarial attack. <em>TAI</em>, <em>5</em>(9), 4470–4478. (<a
href="https://doi.org/10.1109/TAI.2024.3384337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial attack is a method used to deceive machine learning models, which offers a technique to test the robustness of the given model, and it is vital to balance robustness with accuracy. Artificial intelligence (AI) researchers are constantly trying to find a better balance to develop new techniques and approaches to minimize loss of accuracy and increase robustness. To address these gaps, this article proposes a hybrid adversarial attack strategy by utilizing the Fast Gradient Sign Method and Projected Gradient Descent effectively to compute the perturbations that deceive deep neural networks, thus quantifying robustness without compromising its accuracy. Three distinct datasets—CelebA, CIFAR-10, and MNIST—were used in the extensive experiment, and six analyses were carried out to assess how well the suggested technique performed against attacks and defense mechanisms. The proposed model yielded confidence values of 99.99% for the MNIST dataset, 99.93% for the CelebA dataset, and 99.99% for the CIFAR-10 dataset. Defense study revealed that the proposed model outperformed previous models with a robust accuracy of 75.33% for the CelebA dataset, 55.4% for the CIFAR-10 dataset, and 98.65% for the MNIST dataset. The results of the experiment demonstrate that the proposed model is better than the other existing methods in computing the adversarial test and improvising the robustness of the system, thereby minimizing the accuracy loss.},
  archive      = {J_TAI},
  author       = {Rachel Selva Dhanaraj and M. Sridevi},
  doi          = {10.1109/TAI.2024.3384337},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4470-4478},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Building a robust and efficient defensive system using hybrid adversarial attack},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Universal transfer framework for urban spatiotemporal
knowledge based on radial basis function. <em>TAI</em>, <em>5</em>(9),
4458–4469. (<a href="https://doi.org/10.1109/TAI.2024.3382267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate and rapid transfer of complex urban spatiotemporal data is crucial for urban computing tasks such as urban planning and public transportation deployment for smart-city applications. Existing works consider auxiliary data or propose end-to-end models to process complex spatiotemporal information into more complex deep features. However, the latter is incapable of decoupling spatiotemporal knowledge, which means these end-to-end models lack modularity and substitutability. A general modular framework that can automatically capture simple representations of complex spatiotemporal information is required. In this article, we thus propose a universal framework for the transfer of spatiotemporal knowledge based on a radial basis function (RBF). We termed this approach spatial–temporal RBF transfer framework (STRBF-TF). The proposed STRBF-TF generates simple RBF representations of spatiotemporal flow distribution with an RBF transfer block and also leverages a channel attention mechanism. Moreover, we propose two RBF kernel initializers suitable for the source and the target domains, respectively. The framework retains important spatiotemporal knowledge in simple representations for the reconfiguration of spatiotemporal feature distribution for fast and accurate transfer. We conducted cross-domain learning experiments on a large real-world telecom dataset. The results demonstrate the efficiency and accuracy of the proposed approach, as well as its suitability for real-world applications.},
  archive      = {J_TAI},
  author       = {Sheng-Min Chiu and Yow-Shin Liou and Yi-Chung Chen and Chiang Lee and Rong-Kang Shang and Tzu-Yin Chang and Roger Zimmermann},
  doi          = {10.1109/TAI.2024.3382267},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4458-4469},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Universal transfer framework for urban spatiotemporal knowledge based on radial basis function},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A nonparametric split and kernel-merge clustering algorithm.
<em>TAI</em>, <em>5</em>(9), 4443–4457. (<a
href="https://doi.org/10.1109/TAI.2024.3382248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a novel split and kernel-merge clustering (S-KMC), a nonparametric clustering algorithm that combines the strengths of hierarchical clustering, partitional clustering, and density-based clustering. It consists of two main phases: splitting and merging. In the splitting phase, a ranking-based operator is used to divide the data into optimal subclusters. In the merging phase, a kernel function estimates the density of these subclusters after projecting them onto a straight line passing through their centers, facilitating the merging operation. S-KMC is fully nonparametric, eliminating the need for prior information about the data. It effectively handles 1) shape diversity, 2) density variability, 3) high dimensionality, 4) outliers, and 5) missing values. The algorithm offers easily tunable hyperparameters, enhancing its applicability to complex problems and robustness against data anomalies. Experimental analysis on 21 benchmark datasets demonstrates the improved performance of S-KMC in terms of cluster accuracy, handling high-dimensional data, and managing data anomalies and outliers. Comprehensive comparisons with state-of-the-art techniques further validate the superior or comparable performance of the proposed S-KMC algorithm.},
  archive      = {J_TAI},
  author       = {Khurram Khan and Atiq ur Rehman and Adnan Khan and Syed Rameez Naqvi and Samir Brahim Belhaouari and Amine Bermak},
  doi          = {10.1109/TAI.2024.3382248},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4443-4457},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A nonparametric split and kernel-merge clustering algorithm},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multimodal multiobjective evolutionary algorithm for
filter feature selection in multilabel classification. <em>TAI</em>,
<em>5</em>(9), 4428–4442. (<a
href="https://doi.org/10.1109/TAI.2024.3380590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilabel learning is an emergent topic that addresses the challenge of associating multiple labels with a single instance simultaneously. Multilabel datasets often exhibit high dimensionality with noisy, irrelevant, and redundant features. In recent years, multilabel feature selection (MLFS) has gained prominence as a crucial and emerging machine learning task due to its ability to handle such data effectively. However, existing approaches for MLFS often prioritize top-ranked features based on intrinsic data criteria, disregarding relationships within the feature subset. Additionally, compared with conventional feature selection, multiobjective evolutionary algorithms (MOEAs) have not been widely explored in the context of MLFS. This study aims to address these gaps by proposing a multimodal multiobjective evolutionary algorithm (MMOEA) called MMDE_SICD which incorporates a preelimination scheme, an improved initialization scheme, an exploration scheme inspired by genetic operations and a statistically inspired crowding distance scheme. The results show that the proposed MMDE_SICD algorithm can outperform a variety of MOEAs and MMOEAs as well as conventional MLFS algorithms. Notably, this study is the first of its kind to consider MLFS as a multimodal multiobjective problem.},
  archive      = {J_TAI},
  author       = {Emrah Hancer and Bing Xue and Mengjie Zhang},
  doi          = {10.1109/TAI.2024.3380590},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4428-4442},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A multimodal multiobjective evolutionary algorithm for filter feature selection in multilabel classification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lightweight multidendritic pyramidal neuron model with
neural plasticity on image recognition. <em>TAI</em>, <em>5</em>(9),
4415–4427. (<a href="https://doi.org/10.1109/TAI.2024.3379968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulating the method of neurons in the human brain that process signals is crucial for constructing a neural network with biological interpretability. However, existing deep neural networks simplify the function of a single neuron without considering dendritic plasticity. In this article, we present a multidendrite pyramidal neuron model (MDPN) for image classification, which mimics the multilevel dendritic structure of a nerve cell. Unlike the traditional feedforward network model, MDPN discards premature linear summation integration and employs a nonlinear dendritic computation such that improving the neuroplasticity. To model a lightweight and effective classification system, we emphasized the importance of single neuron and redefined the function of each subcomponent. Experimental results verify the effectiveness and robustness of our proposed MDPN in classifying 16 standardized image datasets with different characteristics. Compared to other state-of-the-art and well-known networks, MDPN is superior in terms of classifica-tion accuracy.},
  archive      = {J_TAI},
  author       = {Yu Zhang and Pengxing Cai and Yanan Sun and Zhiming Zhang and Zhenyu Lei and Shangce Gao},
  doi          = {10.1109/TAI.2024.3379968},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4415-4427},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A lightweight multidendritic pyramidal neuron model with neural plasticity on image recognition},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Progressively select and reject pseudolabeled samples for
open-set domain adaptation. <em>TAI</em>, <em>5</em>(9), 4403–4414. (<a
href="https://doi.org/10.1109/TAI.2024.3379940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation solves image classification problems in the target domain by taking advantage of the labeled source data and unlabeled target data. Usually, the source and target domains share the same set of classes. As a special case, open-set domain adaptation (OSDA) assumes there exist additional classes in the target domain but are not present in the source domain. To solve such a domain adaptation problem, our proposed method learns discriminative common subspaces for the source and target domains using a novel open-set locality preserving projection (OSLPP) algorithm. The source and target domain data are aligned in the learned common spaces classwise. To handle the open-set classification problem, our method progressively selects target samples to be pseudolabeled as known classes, rejects the outliers if they are detected as unknown classes, and leaves the remaining target samples as uncertain. The common subspace learning algorithm OSLPP simultaneously aligns the labeled source data and pseudolabeled target data from known classes and pushes the rejected target data away from the known classes. The common subspace learning and the pseudolabeled sample selection/rejection facilitate each other in an iterative learning framework and achieve state-of-the-art performance on four benchmark datasets Office-31, Office-Home, VisDA17, and Syn2Real-O with the average harmonic mean of open-set recognition accuracy (HOS) of 87.6%, 67.0%, 76.1%, and 65.6%, respectively.},
  archive      = {J_TAI},
  author       = {Qian Wang and Fanlin Meng and Toby P. Breckon},
  doi          = {10.1109/TAI.2024.3379940},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4403-4414},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Progressively select and reject pseudolabeled samples for open-set domain adaptation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regional ensemble for improving unsupervised outlier
detectors. <em>TAI</em>, <em>5</em>(9), 4391–4402. (<a
href="https://doi.org/10.1109/TAI.2024.3381102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier ensemble is an important methodology for improving outlier detection, but faces severe challenges in unsupervised settings. Unlike traditional outlier ensembles which revised scores by considering only the values of the scores from multiple detectors, we present a novel regional ensemble (RE). RE combines the scores from multiple objects and multiple detectors and simultaneously takes into consideration both the values and the distribution of these scores. RE specifically enhances the score of a given object by using the scores of neighboring objects of the given object, under the assumption that the scores of the majority of neighboring objects are reliable. RE provides many potential applications, particularly in data mining and machine learning. Compared to existing outlier ensembles with 30 real-world datasets tested, RE attained the best performance with 14 datasets, while the current standard achieves superior performance with only eight datasets. RE can significantly improve the best existing from 0.83 to 0.86 AUC on average.},
  archive      = {J_TAI},
  author       = {Jiawei Yang and Sylwan Rahardja and Susanto Rahardja},
  doi          = {10.1109/TAI.2024.3381102},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4391-4402},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Regional ensemble for improving unsupervised outlier detectors},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bilateral-head region-based convolutional neural networks: A
unified approach for incremental few-shot object detection.
<em>TAI</em>, <em>5</em>(9), 4376–4390. (<a
href="https://doi.org/10.1109/TAI.2024.3381919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Practical object detection systems are highly desired to be open-ended for learning on frequently evolved datasets. Moreover, learning with little supervision further adds flexibility for real-world applications such as autonomous driving and robotics, where large-scale datasets could be prohibitive or expensive to obtain. However, continual adaption with small training examples often results in catastrophic forgetting and dramatic overfitting. To address such issues, a compositional learning system is proposed to enable effective incremental object detection from nonstationary and few-shot data streams. First of all, a novel bilateral–head framework is proposed to decouple the representation learning of base (pretrained) and novel (few-shot) classes into separate embedding spaces, which takes care of novel concept integration and base knowledge retention simultaneously. Moreover, to enhance learning stability, a robust parameter updating rule, i.e., recall and progress mechanism, is carried out to constrain the optimization trajectory of sequential model adaption. Beyond that, to enforce intertask class discrimination with little memory burden, we present a between-class regularization method that expands the decision space of few-shot classes for constructing unbiased feature representation. Final, we deeply investigate the incomplete annotation issue considering the realistic scenario of incremental few-shot object detection (iFSOD) and propose a semisupervised object labeling mechanism to accurately recover the missing annotations for previously encountered classes, which further enhances the robustness of the target detector to counteract catastrophic forgetting. Extensive experiments conducted on both Pascal visual object classes dataset (VOC) and microsoft common objects in context dataset (MS-COCO) datasets demonstrate the effectiveness of our method.},
  archive      = {J_TAI},
  author       = {Yiting Li and Haiyue Zhu and Sichao Tian and Jun Ma and Cheng Xiang and Prahlad Vadakkepat},
  doi          = {10.1109/TAI.2024.3381919},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4376-4390},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Bilateral-head region-based convolutional neural networks: A unified approach for incremental few-shot object detection},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing reinforcement learning via transformer-based state
predictive representations. <em>TAI</em>, <em>5</em>(9), 4364–4375. (<a
href="https://doi.org/10.1109/TAI.2024.3379969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enhancing state representations can effectively mitigate the issue of low sample efficiency in reinforcement learning (RL) within high-dimensional input environments. Existing methods attempt to improve sample efficiency by learning predictive state representations from sequence data. However, there still remain significant challenges in achieving a comprehensive understanding and learning of information within long sequences. Motivated by this, we introduce a transformer-based state predictive representations (TSPR) 1 1Our code will be released at https://github.com/gourmet-liu/TSPR auxiliary task that promotes better representation learning through self-supervised goals. Specifically, we design a transformer-based predictive model to establish unidirectional and bidirectional prediction tasks for predicting state representations within the latent space. TSPR effectively exploits contextual information within sequences to learn more informative state representations, thereby contributing to the enhancement of policy training in RL. Extensive experiments demonstrate that the combination of TSPR with off-policy RL algorithms leads to a substantial improvement in the sample efficiency of RL. Furthermore, TSPR outperforms state-of-the-art sample-efficient RL methods on both the multiple continuous control (DMControl) and discrete control(Atari) tasks.},
  archive      = {J_TAI},
  author       = {Minsong Liu and Yuanheng Zhu and Yaran Chen and Dongbin Zhao},
  doi          = {10.1109/TAI.2024.3379969},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4364-4375},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Enhancing reinforcement learning via transformer-based state predictive representations},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A similarity-based positional attention-aided deep learning
model for copy–move forgery detection. <em>TAI</em>, <em>5</em>(9),
4354–4363. (<a href="https://doi.org/10.1109/TAI.2024.3379941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The process of modifying digital images has been made significantly easier by the availability of several image editing software. However, in a variety of contexts, including journalism, judicial processes, and historical documentation, the authenticity of images is of utmost importance. In particular, copy–move forgery is a distinct type of image manipulation, where a portion of an image is copied and pasted into another area of the same image, creating a fictitious or altered version of the original. In this research, we present a lightweight MultiResUnet architecture with the similarity-based positional attention module (SPAM) attention module for copy–move forgery detection (CMFD). By using a similarity measure across the patches of the features, this attention module identifies the patches, where a forged region is present. The lightweight network also aids in resource-efficient training and transforms the model into one that can be used in real time. We have employed four commonly used but extremely difficult CMFD datasets, namely CoMoFoD, COVERAGE, CASIA v2, and MICC-F600, to assess the effectiveness of our model. The proposed model significantly lowers false positives, thereby improving the pixel-level accuracy and dependability of CMFD tools.},
  archive      = {J_TAI},
  author       = {Ayush Roy and Sk Mohiuddin and Ram Sarkar},
  doi          = {10.1109/TAI.2024.3379941},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4354-4363},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A similarity-based positional attention-aided deep learning model for Copy–Move forgery detection},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A distributed conditional wasserstein deep convolutional
relativistic loss generative adversarial network with improved
convergence. <em>TAI</em>, <em>5</em>(9), 4344–4353. (<a
href="https://doi.org/10.1109/TAI.2024.3386500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial networks (GANs) excel in diverse applications such as image enhancement, manipulation, and generating images and videos from text. Yet, training GANs with large datasets remains computationally intensive for standalone systems. Synchronization issues between the generator and discriminator lead to unstable training, poor convergence, vanishing, and exploding gradient challenges. In decentralized environments, standalone GANs struggle with distributed data on client machines. Researchers have turned to federated learning (FL) for distributed-GAN (D-GAN) implementations, but efforts often fall short due to training instability and poor synchronization within GAN components. In this study, we present DRL-GAN, a lightweight Wasserstein conditional distributed relativistic loss-GAN designed to overcome existing limitations. DRL-GAN ensures training stability in the face of nonconvex losses by employing a single global generator on the central server and a discriminator per client. Utilizing Wasserstein-1 for relativistic loss computation between real and fake samples, DRL-GAN effectively addresses issues, such as mode collapses, vanishing, and exploding gradients, accommodating both iid and non-iid private data in clients and fostering strong convergence. The absence of a robust conditional distributed-GAN model serves as another motivation for this work. We provide a comprehensive mathematical formulation of DRL-GAN and validate our claims empirically on CIFAR-10, MNIST, EuroSAT, and LSUN-Bedroom datasets.},
  archive      = {J_TAI},
  author       = {Arunava Roy and Dipankar Dasgupta},
  doi          = {10.1109/TAI.2024.3386500},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4344-4353},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A distributed conditional wasserstein deep convolutional relativistic loss generative adversarial network with improved convergence},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial machine learning for social good: Reframing the
adversary as an ally. <em>TAI</em>, <em>5</em>(9), 4322–4343. (<a
href="https://doi.org/10.1109/TAI.2024.3383407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have been the driving force behind many of the recent advances in machine learning. However, research has shown that DNNs are vulnerable to adversarial examples—input samples that have been perturbed to force DNN-based models to make errors. As a result, adversarial machine learning (AdvML) has gained a lot of attention, and researchers have investigated these vulnerabilities in various settings and modalities. In addition, DNNs have also been found to incorporate embedded bias and often produce unexplainable predictions, which can result in antisocial AI applications. The emergence of new AI technologies that leverage large language models (LLMs), such as ChatGPT and GPT-4, increases the risk of producing antisocial applications at scale. AdvML for social good (AdvML4G) is an emerging field that repurposes the AdvML bug to invent prosocial applications. Regulators, practitioners, and researchers should collaborate to encourage the development of prosocial applications and hinder the development of antisocial ones. In this work, we provide the first comprehensive review of the emerging field of AdvML4G. This paper encompasses a taxonomy that highlights the emergence of AdvML4G, a discussion of the differences and similarities between AdvML4G and AdvML, a taxonomy covering social good-related concepts and aspects, an exploration of the motivations behind the emergence of AdvML4G at the intersection of ML4G and AdvML, and an extensive summary of the works that utilize AdvML4G as an auxiliary tool for innovating prosocial applications. Finally, we elaborate upon various challenges and open research issues that require significant attention from the research community.},
  archive      = {J_TAI},
  author       = {Shawqi Al-Maliki and Adnan Qayyum and Hassan Ali and Mohamed Abdallah and Junaid Qadir and Dinh Thai Hoang and Dusit Niyato and Ala Al-Fuqaha},
  doi          = {10.1109/TAI.2024.3383407},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4322-4343},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adversarial machine learning for social good: Reframing the adversary as an ally},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incomplete graph learning via partial graph convolutional
network. <em>TAI</em>, <em>5</em>(9), 4315–4321. (<a
href="https://doi.org/10.1109/TAI.2024.3386499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) gain increasing attention on graph data learning tasks in recent years. However, in many applications, graph may come with an incomplete form where attributes of graph nodes are partially unknown/missing. Existing graph convolutions (GCs) are generally designed on complete graphs which cannot deal with attribute-incomplete graph data directly. To address this problem, in this article, we extend standard GC and develop an explicit Partial Graph Convolution (PaGC) for attribute-incomplete graph data. Our PaGC is derived based on the observation that the core neighborhood aggregator in GC operation can be equivalently viewed as an energy minimization model. Based on it, we can define a novel partial aggregation function and derive PaGC for incomplete graph data. Experiments demonstrate the effectiveness and efficiency of the proposed PaGCN.},
  archive      = {J_TAI},
  author       = {Ziyan Zhang and Bo Jiang and Jin Tang and Jinhui Tang and Bin Luo},
  doi          = {10.1109/TAI.2024.3386499},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4315-4321},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Incomplete graph learning via partial graph convolutional network},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Editorial: From explainable artificial intelligence (xAI) to
understandable artificial intelligence (uAI). <em>TAI</em>,
<em>5</em>(9), 4310–4314. (<a
href="https://doi.org/10.1109/TAI.2024.3439048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TAI},
  author       = {Hussein Abbass and Keeley Crockett and Jonathan Garibaldi and Alexander Gegov and Uzay Kaymak and Joao Miguel C. Sousa},
  doi          = {10.1109/TAI.2024.3439048},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {4310-4314},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Editorial: From explainable artificial intelligence (xAI) to understandable artificial intelligence (uAI)},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feedback generative adversarial network with channel-space
attention for image-based optimal path search planning. <em>TAI</em>,
<em>5</em>(8), 4293–4307. (<a
href="https://doi.org/10.1109/TAI.2024.3373390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Path planning and seeking is one of the most challenging and interesting problems in the field of artificial intelligence. In this article, we propose a new image-based path planning algorithm to overcome traditional design concepts based on complex topological networks. Specifically, a novel feedback generative adversarial network learning model with channel-space attention (CSA-FGAN) is developed to search and plan an optimal path. It is worth noting that the input and output of the proposed feedback learning model are RGB images with complex spatial obstacle environment maps and RGB images with feasible paths, respectively. In particular, in order to further enhance the feature extraction ability and path prediction accuracy of image data, the feedback attention mechanism controls and improves the depth of the network by forming a closed loop of the proposed learning model. Final, three application scenario image examples are conducted to verify the superiority of the proposed path planning policy.},
  archive      = {J_TAI},
  author       = {Tao Sun and Jian-Sheng Li and Yi-Fan Zhang and Xin-Feng Ru and Ke Wang},
  doi          = {10.1109/TAI.2024.3373390},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {4293-4307},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Feedback generative adversarial network with channel-space attention for image-based optimal path search planning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiagent hierarchical deep reinforcement learning for
operation optimization of grid-interactive efficient commercial
buildings. <em>TAI</em>, <em>5</em>(8), 4280–4292. (<a
href="https://doi.org/10.1109/TAI.2024.3366869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The operation optimization of grid-interactive efficient commercial buildings (GECBs) contributes to overcoming the issues caused by increasing peak electricity demand. However, existing GECB operation optimization methods either rely on prior information of stochastic parameters or do not consider the multitimescale characteristics of heating, ventilation, and air conditioning (HVAC) systems. In this article, we investigate a GECB operation optimization problem considering HVAC multitimescale characteristics. Specifically, an expected GECB operation cost minimization problem is first formulated. To overcome the challenges in solving the formulated problem (e.g., uncertain parameters, inexplicit system models, multiple timescales, and high-dimensional discrete solution space), we reformulate the above problem as a Markov game and two Markov decision processes. Then, we design a GECB operation optimization algorithm based on multiagent hierarchical deep reinforcement learning (MAHDRL), which can support two kinds of smart coordination, i.e., the coordination among agents at slow timescales, and the coordination among agents at different timescales. Performance evaluation with real-world data-sets indicates that the designed algorithm could decrease total operation cost by 6.16%–45.52% while maintaining comfortable indoor environments and efficient grid services compared with adopted benchmarks.},
  archive      = {J_TAI},
  author       = {Zhiqiang Chen and Liang Yu and Shuang Zhang and Shushan Hu and Chao Shen},
  doi          = {10.1109/TAI.2024.3366869},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {4280-4292},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multiagent hierarchical deep reinforcement learning for operation optimization of grid-interactive efficient commercial buildings},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distilled gradual pruning with pruned fine-tuning.
<em>TAI</em>, <em>5</em>(8), 4269–4279. (<a
href="https://doi.org/10.1109/TAI.2024.3366497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks (NNs) have been driving machine learning progress in recent years, but their larger models present challenges in resource-limited environments. Weight pruning reduces the computational demand, often with performance degradation and long training procedures. This work introduces distilled gradual pruning with pruned fine-tuning (DG2PF), a comprehensive algorithm that iteratively prunes pretrained NNs using knowledge distillation. We employ a magnitude-based unstructured pruning function that selectively removes a specified proportion of unimportant weights from the network. This function also leads to an efficient compression of the model size while minimizing classification accuracy loss. Additionally, we introduce a simulated pruning strategy with the same effects of weight recovery but while maintaining stable convergence. Furthermore, we propose a multistep self-knowledge distillation strategy to effectively transfer the knowledge of the full, unpruned network to the pruned counterpart. We validate the performance of our algorithm through extensive experimentation on diverse benchmark datasets, including CIFAR-10 and ImageNet, as well as a set of model architectures. The results highlight how our algorithm prunes and optimizes pretrained NNs without substantially degrading their classification accuracy while delivering significantly faster and more compact models.},
  archive      = {J_TAI},
  author       = {Federico Fontana and Romeo Lanzino and Marco Raoul Marini and Danilo Avola and Luigi Cinque and Francesco Scarcello and Gian Luca Foresti},
  doi          = {10.1109/TAI.2024.3366497},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {4269-4279},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Distilled gradual pruning with pruned fine-tuning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic long-term time-series forecasting via meta
transformer networks. <em>TAI</em>, <em>5</em>(8), 4258–4268. (<a
href="https://doi.org/10.1109/TAI.2024.3365775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A reliable long-term time-series forecaster is highly demanded in practice but comes across many challenges such as low computational and memory footprints as well as robustness against dynamic learning environments. This article proposes meta-transformer networks (MANTRA) to deal with the dynamic long-term time-series forecasting tasks. MANTRA relies on the concept of fast and slow learners where a collection of fast learners learns different aspects of data distributions while adapting quickly to changes. A slow learner tailors suitable representations to fast learners. Fast adaptations to dynamic environments are achieved using the universal representation transformer (URT) layers producing task-adapted representations with a small number of parameters. Our experiments using four datasets with different prediction lengths demonstrate the advantage of our approach with at least $3\%$ improvements over the baseline algorithms for both multivariate and univariate settings.},
  archive      = {J_TAI},
  author       = {Muhammad Anwar Ma&#39;sum and MD Rasel Sarkar and Mahardhika Pratama and Savitha Ramasamy and Sreenatha Anavatti and Lin Liu and Habibullah Habibullah and Ryszard Kowalczyk},
  doi          = {10.1109/TAI.2024.3365775},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {4258-4268},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Dynamic long-term time-series forecasting via meta transformer networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-based online adaptive inverse noncooperative
linear-quadratic differential games via finite-time concurrent learning.
<em>TAI</em>, <em>5</em>(8), 4247–4257. (<a
href="https://doi.org/10.1109/TAI.2024.3368357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noncooperative differential games provide a basis for the study of coordination, conflict, and control for a single dynamical system with multiple players. Within the linear-quadratic differential games (LQDGs), the optimal feedback gain matrix and the weighting matrices of individual cost function depict each player&#39;s control policy and the tradeoff of various objectives, respectively. In this article, we investigate the inverse problem of LQDG with partial state observation via a model-based online method, i.e., recover the cost function of each player. First, a state observer is used to estimate the system state. Then, the feedback gain matrices of players are learned using a finite-time concurrent learning (FTCL)-based adaptive law relaxing the persistent excitation (PE) condition which is required in traditional adaptive estimation methods. With the learned feedback gain matrices, a semidefinite programming (SDP) problem with the quadratic objective function can be set up for determining the weighting matrices of the cost function. The applicability and effectiveness of the proposed method are demonstrated with a numerical example and a shared steering control simulation.},
  archive      = {J_TAI},
  author       = {Jie Lin and Huai-Ning Wu},
  doi          = {10.1109/TAI.2024.3368357},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {4247-4257},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Model-based online adaptive inverse noncooperative linear-quadratic differential games via finite-time concurrent learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multistream gaze estimation with anatomical eye region
isolation by synthetic to real transfer learning. <em>TAI</em>,
<em>5</em>(8), 4232–4246. (<a
href="https://doi.org/10.1109/TAI.2024.3366174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel neural pipeline, MSGazeNet, that learns gaze representations by taking advantage of the eye anatomy information through a multistream framework. Our proposed solution comprises two components, first a network for isolating anatomical eye regions, and a second network for multistream gaze estimation. The eye region isolation is performed with a U-Net style network which we train using a synthetic dataset that contains eye region masks for the visible eyeball and the iris region. The synthetic dataset used in this stage is procured using the UnityEyes simulator and consists of 80 000 eye images. Successive to training, the eye region isolation network is then transferred to the real domain for generating masks for the real-world eye images. In order to successfully make the transfer, we exploit domain randomization in the training process, which allows for the synthetic images to benefit from a larger variance with the help of augmentations that resemble artifacts. The generated eye region masks along with the raw eye images are then used together as a multistream input to our gaze estimation network, which consists of wide residual blocks. The output embeddings from these encoders are fused in the channel dimension before feeding into the gaze regression layers. We evaluate our framework on three gaze estimation datasets and achieve strong performances. Our method surpasses the state of the art by 7.57% and 1.85% on two datasets and obtains competitive results on the other. We also study the robustness of our method with respect to the noise in the data and demonstrate that our model is less sensitive to noisy data. Last, we perform a variety of experiments including ablation studies to evaluate the contribution of different components and design choices in our solution.},
  archive      = {J_TAI},
  author       = {Zunayed Mahmud and Paul Hungler and Ali Etemad},
  doi          = {10.1109/TAI.2024.3366174},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {4232-4246},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multistream gaze estimation with anatomical eye region isolation by synthetic to real transfer learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shapley value-based approaches to explain the quality of
predictions by classifiers. <em>TAI</em>, <em>5</em>(8), 4217–4231. (<a
href="https://doi.org/10.1109/TAI.2024.3365082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of algorithm-agnostic approaches for explainable machine learning (ML) is an emerging area of research. When explaining the contribution of features toward the predicted outcome, traditionally, the focus remains on explaining the prediction itself, however a little has been done on explaining the quality of prediction of these models, where the quality can be assessed by the algorithm performance when changing the thresholds for classification. In this article, we propose the use of Shapley values to explain the contribution of features toward the overall algorithm performance, measured in terms of receiver operating characteristics (ROC) curve and the area under the ROC curve (AUC). With the help of an illustrative example, we demonstrate the proposed idea of explaining the ROC curve, and visualizing the uncertainties in these curves. For imbalanced datasets, the use of precision-recall curve (PRC) is considered more appropriate, therefore we also demonstrate how to explain the PRCs with the help of Shapley values. The explanation of the model performance can help analysts in a number of ways, for example, in feature selection by identifying the irrelevant features that can be removed to reduce the computational complexity. It can also help in identifying the features having critical contributions toward the overall algorithm performance.},
  archive      = {J_TAI},
  author       = {Guilherme Dean Pelegrina and Sajid Siraj},
  doi          = {10.1109/TAI.2024.3365082},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {4217-4231},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Shapley value-based approaches to explain the quality of predictions by classifiers},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhance adversarial robustness via geodesic distance.
<em>TAI</em>, <em>5</em>(8), 4202–4216. (<a
href="https://doi.org/10.1109/TAI.2024.3364121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial training is an effective method to improve the model&#39;s adversarial robustness. To realize a considerable tradeoff between clean accuracy and adversarial robustness, surrogate loss minimization can be utilized to regularize for robustness. This study advances beyond previous research efforts by taking a further step. It is informed by two theoretical perspectives. First, adversarial examples are inevitable within a unit sphere surrounding clean data. Second, geodesics can characterize the shortest distance between two points in Riemannian geometry. Accordingly, this study employs geodesic distance as a regularized surrogate loss item to capture the minimal divergence between the distribution of natural examples and the distribution of adversarial examples. This approach yields a more compact upper bound of risk errors than previous studies, which is beneficial for improving adversarial robustness. Based on the theoretical insight, this study proposes a metric of geodesic loss and a framework of geodesic adversarial training to boost the adversarial robustness of neural networks. The empirical study on the diverse datasets demonstrates the considerable performance of the proposed method against diverse attacks, including white-box attacks, black-box corruptions, adaptive attacks, and auto attacks.},
  archive      = {J_TAI},
  author       = {Jun Yan and Huilin Yin and Ziming Zhao and Wancheng Ge and Jingfeng Zhang},
  doi          = {10.1109/TAI.2024.3364121},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {4202-4216},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Enhance adversarial robustness via geodesic distance},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive iterative learning control for nonlinear multiagent
systems with initial error compensation. <em>TAI</em>, <em>5</em>(8),
4192–4201. (<a href="https://doi.org/10.1109/TAI.2024.3366877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the consensus tracking problem of neural network (NN)-based adaptive iterative learning control (AILC) scheme is investigated for the nonlinear multiagent systems (MASs) with repetitive control tasks. To achieve the better tracking performance, the NN is developed to iteratively learn the uncertain nonlinearity. Meanwhile, the gradient descent algorithm is utilized to train NN weights, which is different from the traditional method based on $\sigma$ -modification. For initial error problems of AILC, an initial error compensation method is developed to deal with the initial consensus tracking error. During the AILC design using the dynamic surface control technique, an auxiliary control signal is introduced to handle the residual term. Then, the boundedness of all signals and the iterative convergence of tracking errors are analyzed by the Lyapunov theory. Final, a simulation of the multimanipulators is provided to validate the effectiveness of the proposed control algorithm.},
  archive      = {J_TAI},
  author       = {Zhiqiang Li and Qi Zhou and Yang Liu and Hongru Ren and Hongyi Li},
  doi          = {10.1109/TAI.2024.3366877},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {4192-4201},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adaptive iterative learning control for nonlinear multiagent systems with initial error compensation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Alternating direction method of multipliers-based parallel
optimization for multi-agent collision-free model predictive control.
<em>TAI</em>, <em>5</em>(8), 4176–4191. (<a
href="https://doi.org/10.1109/TAI.2024.3364127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the collision-free control problem for multi-agent systems. For such multi-agent systems, it is the typical situation where conventional methods using either the usual centralized model predictive control (MPC), or even the distributed counterpart, would suffer from substantial difficulty in balancing optimality and computational efficiency. Additionally, the nonconvex characteristics that invariably arise in such collision-free control and optimization problems render it difficult to effectively derive a reliable solution (and also to thoroughly analyze the associated convergence properties). To overcome these challenging issues, this work establishes a suitably novel parallel computation framework through an innovative mathematical problem formulation; and then with this framework and formulation, a parallel algorithm based on alternating direction method of multipliers (ADMM) is presented to solve the subproblems arising from the resulting parallel structure. Furthermore, an efficient and intuitive initialization procedure is developed to accelerate the optimization process, and the optimum is thus determined with significantly improved computational efficiency. As supported by rigorous proofs, the convergence of the proposed ADMM iterations for this nonconvex optimization problem is analyzed and discussed in detail. Finally, a simulation with a group of unmanned aerial vehicles (UAVs) serves as an illustrative example here to demonstrate the effectiveness and efficiency of the proposed approach. Also, the simulation results verify significant improvements in accuracy and computational efficiency compared to other baselines, including primal quadratic mixed integer programming (PQ-MIP), nonconvex quadratic mixed integer programming (NC-MIP), and nonconvex quadratically constrained quadratic programming (NC-QCQP).},
  archive      = {J_TAI},
  author       = {Zilong Cheng and Jun Ma and Wenxin Wang and Zicheng Zhu and Clarence W. de Silva and Tong Heng Lee},
  doi          = {10.1109/TAI.2024.3364127},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {4176-4191},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Alternating direction method of multipliers-based parallel optimization for multi-agent collision-free model predictive control},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An iterative optimizing framework for radiology report
summarization with ChatGPT. <em>TAI</em>, <em>5</em>(8), 4163–4175. (<a
href="https://doi.org/10.1109/TAI.2024.3364586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The “Impression” section of a radiology report is a critical basis for communication between radiologists and other physicians. Typically written by radiologists, this part is derived from the “Findings” section, which can be laborious and error-prone. Although deep-learning-based models, such as bidirectional encoder representation from transformers (BERT), have achieved promising results in automatic impression generation (AIG), such models often require substantial amounts of medical data and have poor generalization performance. Recently, large language models (LLMs) like Chat Generative Pre-trained Transformer (ChatGPT) have shown strong generalization capabilities and performance, but their performance in specific domains, such as radiology, remains under-investigated and potentially limited. To address this limitation, we propose ImpressionGPT, leveraging the contextual learning capabilities of LLMs through our dynamic prompt and iterative optimization algorithm to accomplish the AIG task. ImpressionGPT initially employs a small amount of domain-specific data to create a dynamic prompt, extracting contextual semantic information closely related to the test data. Subsequently, the iterative optimization algorithm automatically evaluates the output of LLMs and provides optimization suggestions, continuously refining the output results. The proposed ImpressionGPT model achieves superior performance of AIG task on both the Medical Information Mart for Intensive Care - Chest X-ray database (MIMIC-CXR) and Open Access Biomedical Image Search Engine (OpenI) datasets without requiring additional training data or fine-tuning the LLMs. This work presents a paradigm for localizing LLMs that can be applied in a wide range of similar application scenarios, bridging the gap between general-purpose LLMs and the specific language processing needs of various domains.},
  archive      = {J_TAI},
  author       = {Chong Ma and Zihao Wu and Jiaqi Wang and Shaochen Xu and Yaonai Wei and Zhengliang Liu and Fang Zeng and Xi Jiang and Lei Guo and Xiaoyan Cai and Shu Zhang and Tuo Zhang and Dajiang Zhu and Dinggang Shen and Tianming Liu and Xiang Li},
  doi          = {10.1109/TAI.2024.3364586},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {4163-4175},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An iterative optimizing framework for radiology report summarization with ChatGPT},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Text-guided portrait image matting. <em>TAI</em>,
<em>5</em>(8), 4149–4162. (<a
href="https://doi.org/10.1109/TAI.2024.3363120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image matting is a technique used to separate the foreground of an image from the background, which estimates an alpha matte that indicates pixel-wise degree of transparency. To precisely extract target objects and address the ambiguity of solutions in image matting, many existing approaches employ a trimap or background image provided by the user as additional input to guide the matting process. This article introduces a novel matting paradigm termed text-guided image matting, utilizing a textual description of the foreground object as a guiding element. In contrast to trimap or background-based methods, text-guided matting offers a user-friendly interface, providing semantic clues for the objects of interest. Moreover, it facilitates batch processing across multiple frames featuring the same objects of interest. The proposed text-guided matting approach is implemented through a deep neural network (NN) comprising three-stage cross-modal feature fusion and two-step alpha matte prediction. Experimental results on portrait matting demonstrate the competitive performance of our text-guided approach compared to existing trimap-based and background-based methods.},
  archive      = {J_TAI},
  author       = {Yong Xu and Xin Yao and Baoling Liu and Yuhui Quan and Hui Ji},
  doi          = {10.1109/TAI.2024.3363120},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {4149-4162},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Text-guided portrait image matting},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Remote sensing image semantic segmentation based on cascaded
transformer. <em>TAI</em>, <em>5</em>(8), 4136–4148. (<a
href="https://doi.org/10.1109/TAI.2024.3363685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-resolution (HR) remote sensing image semantic segmentation plays an important role in Earth&#39;s surface. Despite remote sensing image semantic segmentation methods have developed rapidly. However, the land objects types of HR remote sensing images are complex, the features are difficult to be extracted, and the existing deep learning methods are difficult to obtain enough effective features, but there is still room for further improvement in feature representation ability. In respect of the issues above, we propose novel global and local features aggregated network to simultaneously exploit boundary information and capture hierarchical semantic information for remote sensing image semantic segmentation. In addition, a novel loss module is designed according to generative adversarial network (GAN) to enhance the feature representation capability of the model in multiclass segmentation. In the meanwhile, the performance of CTrans $\_$ Net is verified on three public datasets, and good results are obtained.},
  archive      = {J_TAI},
  author       = {Falin Wang and Jian Ji and Yuan Wang},
  doi          = {10.1109/TAI.2024.3363685},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {4136-4148},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Remote sensing image semantic segmentation based on cascaded transformer},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CTRL: Clustering training losses for label error detection.
<em>TAI</em>, <em>5</em>(8), 4121–4135. (<a
href="https://doi.org/10.1109/TAI.2024.3365093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In supervised machine learning, use of correct labels is extremely important to ensure high accuracy. Unfortunately, most datasets contain corrupted labels. Machine learning models trained on such datasets do not generalize well. Thus, detecting the label errors can significantly increase their efficacy. We propose a novel framework, called CTRL 1 1CTRL is open-source: https://github.com/chang-yue/ctrl. (Clustering TRaining Losses for label error detection), to detect label errors in multiclass datasets. It detects label errors in two steps based on the observation that models learn clean and noisy labels in different ways. First, we train a neural network (NN) using the noisy training dataset and obtain the loss curve for each sample. Then, we apply clustering algorithms to the training losses to group samples into two categories: cleanly labeled and noisily labeled. After label error detection, we remove samples with noisy labels and retrain the model. Our experimental results demonstrate state-of-the-art error detection accuracy on both image and tabular datasets under labeling noise. We also use a theoretical analysis to provide insights into why CTRL performs so well.},
  archive      = {J_TAI},
  author       = {Chang Yue and Niraj K. Jha},
  doi          = {10.1109/TAI.2024.3365093},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {4121-4135},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {CTRL: Clustering training losses for label error detection},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Context-aware self-supervised learning of whole slide
images. <em>TAI</em>, <em>5</em>(8), 4111–4120. (<a
href="https://doi.org/10.1109/TAI.2024.3365779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presenting whole slide images (WSIs) as graph will enable a more efficient and accurate learning framework for cancer diagnosis. Due to the fact that a single WSI consists of billions of pixels and there is a lack of vast annotated datasets required for computational pathology, the problem of learning from WSIs using typical deep learning approaches such as convolutional neural network (CNN) is challenging. Additionally, WSIs downsampling may lead to the loss of data that is essential for cancer detection. A novel two-stage learning technique is presented in this work. Since context, such as topological features in the tumor surroundings, may hold important information for cancer grading and diagnosis, a graph representation capturing all dependencies among regions in the WSI is very intuitive. Graph convolutional network (GCN) is deployed to include context from the tumor and adjacent tissues, and self-supervised learning is used to enhance training through unlabeled data. More specifically, the entire slide is presented as a graph, where the nodes correspond to the patches from the WSI. The proposed framework is then tested using WSIs from prostate and kidney cancers.},
  archive      = {J_TAI},
  author       = {Milan Aryal and Nasim Yahya Soltani},
  doi          = {10.1109/TAI.2024.3365779},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {4111-4120},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Context-aware self-supervised learning of whole slide images},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-bidirectional decoupled distillation for time series
classification. <em>TAI</em>, <em>5</em>(8), 4101–4110. (<a
href="https://doi.org/10.1109/TAI.2024.3360180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the years, many deep learning algorithms have been developed for time series classification (TSC). A learning model&#39;s performance usually depends on the quality of the semantic information extracted from lower and higher levels within the representation hierarchy. Efficiently promoting mutual learning between higher and lower levels is vital to enhance the model&#39;s performance during model learning. To this end, we propose a self-bidirectional decoupled distillation (self-BiDecKD) method for TSC. Unlike most self-distillation algorithms that usually transfer the target-class knowledge from higher to lower levels, self-BiDecKD encourages the output of the output layer and the output of each lower level block to form a bidirectional decoupled knowledge distillation (KD) pair. The bidirectional decoupled KD promotes mutual learning between lower and higher level semantic information and extracts the knowledge hidden in the target and nontarget classes, helping self-BiDecKD capture rich representations from the data. Experimental results show that compared with a number of self-distillation algorithms, self-BiDecKD wins 35 out of 85 University of California, Riverside (UCR) 2018 datasets and achieves the smallest AVeraGe (AVG)_rank score, namely 3.2882. In particular, compared with a nonself-distillation baseline, self-BiDecKD results in 58/8/19 regarding “win”/“tie”/“lose.”},
  archive      = {J_TAI},
  author       = {Zhiwen Xiao and Huanlai Xing and Rong Qu and Hui Li and Li Feng and Bowen Zhao and Jiayi Yang},
  doi          = {10.1109/TAI.2024.3360180},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {4101-4110},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Self-bidirectional decoupled distillation for time series classification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep transfer learning for detecting electric vehicles
highly correlated energy consumption parameters. <em>TAI</em>,
<em>5</em>(8), 4087–4100. (<a
href="https://doi.org/10.1109/TAI.2024.3358796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implementation of advanced intelligent deep learning techniques for electric vehicles (EVs) energy consumption analysis is obstructed by two main subjects. First, the problem of finding a very similar collection of datasets to the actual EVs energy usage in terms of feature space and data distribution. Second, training a retrained model from scratch requires a massive amount of computational power; however, this does not guarantee to catch rare events included in datasets. To mitigate the aforementioned concerns, this article aims to present a model based on deep transfer learning (DTL) between domain-variant datasets, to reduce the need for the existence of a vast amount of EVs data, including driving characteristics and patterns. Also, this model applies a distributed cooperative learning approach to identify highly correlated energy consumption parameters by building the model on previously acquired knowledge from preceding learning phases in order to enhance the artificial intelligence (AI) accuracy level of the proposed energy management system.},
  archive      = {J_TAI},
  author       = {Zeinab Teimoori and Abdulsalam Yassine and Chaoru Lu},
  doi          = {10.1109/TAI.2024.3358796},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {4087-4100},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep transfer learning for detecting electric vehicles highly correlated energy consumption parameters},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Boundary-aware uncertainty suppression for semi-supervised
medical image segmentation. <em>TAI</em>, <em>5</em>(8), 4074–4086. (<a
href="https://doi.org/10.1109/TAI.2024.3359576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning (SSL) algorithms have received extensive attention in medical image segmentation because they can be trained with unlabeled data. However, most existing SSL methods underestimate the importance of small branches and boundary regions, resulting in unsatisfactory boundaries and nonsmooth objects. We observe that the voxels of the target boundary have relative uncertainty. When the foreground map and background map of an object have the same voxel, that voxel must be in the edge region. Therefore, in this study, we propose a novel SSL framework based on the uncertainty of bounding voxels, which we call the boundary-aware network (BoANet). Specifically, we use a dual-task network that predicts the segmentation map and background map of objects. For unlabeled data, because the geometric contour information of the target object is obtained by elementwise multiplication of the segmentation map and the background map, geometric constraints are imposed on the segmentation. Simultaneously, for labeled data, we propose a weighted cross-entropy (WCE) loss, which can synthesize the local structural information of voxels and guide the network to mine boundary details. We evaluated our method on publicly available benchmark datasets. The experimental results show that our method can outperform the current state-of-the-art approaches.},
  archive      = {J_TAI},
  author       = {Congcong Li and Jinshuo Zhang and Dongmei Niu and Xiuyang Zhao and Bo Yang and Caiming Zhang},
  doi          = {10.1109/TAI.2024.3359576},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {4074-4086},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Boundary-aware uncertainty suppression for semi-supervised medical image segmentation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed optimal formation control of multiple unmanned
surface vehicles with stackelberg differential graphical game.
<em>TAI</em>, <em>5</em>(8), 4058–4073. (<a
href="https://doi.org/10.1109/TAI.2024.3357795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, a cooperative formation control methodology is proposed for multiple six-degree-of-freedom (six-DoF) unmanned surface vehicles (USVs) subject to input saturation. Our method includes several key steps. To begin, we formulate the optimal formation control problem for USVs as Stackelberg differential graphical games. According to this design principle, each USV is partitioned into a primary follower and a secondary follower. The primary followers coalesce into one team, while the secondary followers constitute another team independently. Second, a new type of barrier Lyapunov function (BLF) with a hyperbolic tangent function is introduced to indirectly constrain velocity vector. Third, an auxiliary system is established to compensate for the input saturation. Then, the Lyapunov stability theory is employed to prove that all closed-loop signals of multiple USVs are semiglobal uniform ultimate boundedness (SGUUB). Finally, we present the simulation results of our proposed control scheme. These results demonstrate the effectiveness and feasibility of our proposed methodology for cooperative formation control of multiple USVs.},
  archive      = {J_TAI},
  author       = {Kunting Yu and Yongming Li and Maolong Lv and Shaocheng Tong},
  doi          = {10.1109/TAI.2024.3357795},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {4058-4073},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Distributed optimal formation control of multiple unmanned surface vehicles with stackelberg differential graphical game},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new causal inference framework for SAR target recognition.
<em>TAI</em>, <em>5</em>(8), 4042–4057. (<a
href="https://doi.org/10.1109/TAI.2024.3357664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In synthetic aperture radar (SAR) automatic target recognition (ATR) tasks, deep learning-based methods usually work with the assumption that the training and test target samples are independent and identically distributed. However, the performance of the deep model degrades dramatically when there exists a large distribution variation between training and test data. The collected target samples include not only the target entity but also the target&#39;s complicated surrounding environment. So it is difficult to accurately identify targets when they appear in a new background. In this article, we propose a causal inference framework for SAR ATR by removing the background-related bias. This framework can handle more challenging recognition scenarios, SAR background out of distribution (o.o.d) recognition task. First, the SAR ATR task is modeled as a causal graph from a causal inference perspective, and this graph clearly explains the sources of background-related bias in traditional deep models. Then, this graph is intervened to calculate the causal effect caused by background on prediction in accordance with the frontdoor adjustment. This postintervened graph cuts the spurious correlations between background and prediction. Finally, the total effect is used as the final unbiased prediction by removing background-related bias from the original prediction. Our framework does not impose constraints on the specific implementation of the model and does not add any new training parameters. Experimental results on the moving and stationary target acquisition and recognition (MSTAR) and synthetic and measured paired labeled experiment (SAMPLE) benchmarks demonstrate the effectiveness of our proposed framework in the background out-of-distribution case, and it scientifically improve the recognition capabilities of several baseline models.},
  archive      = {J_TAI},
  author       = {Jiaxiang Liu and Zhunga Liu and Zuowei Zhang and Longfei Wang and Meiqin Liu},
  doi          = {10.1109/TAI.2024.3357664},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {4042-4057},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A new causal inference framework for SAR target recognition},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive heterogeneous credit card fraud detection model
based on deep reinforcement training subset selection. <em>TAI</em>,
<em>5</em>(8), 4026–4041. (<a
href="https://doi.org/10.1109/TAI.2024.3359568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous fraud detection is an important means of credit card security assurance, which can utilize historical transaction records in a source and target domain to build an effective fraud detection model. Nevertheless, large feature distribution differences between source and target transaction instances and the complex intrinsic structure hidden behind transaction data make it difficult for existing credit card fraud detection (CCFD) models to capture and transfer the most informative feature representations and seriously hinder detection performance. In this work, we propose a novel adaptive heterogeneous CCFD model named adaptive heterogeneous credit card fraud detection model based on deep reinforcement training subset selection (RTAHC) based on deep reinforcement training subset selection, which mainly contains two components: selection distribution generator (SDG) and transaction fraud detector (TFD, including feature extractor with an attention mechanism and classifier). The SDG can generate the selection probability distribution vector via the reinforcement reward mechanism, and then transaction instances in the source domain relevant to the target domain are selected. The feature extractor with an attention mechanism can learn the abstract deep semantic feature representations of selected source transaction instances and the target domain. The joint training of SDG and TFD can provide more real-time and accurate transaction feature representations to reduce the distribution discrepancy between the two domains. We verify the detection performance of RTAHC across a large real-world credit card transaction dataset and four public datasets. Experimental results demonstrate that the RTAHC model can exhibit competitive CCFD performance.},
  archive      = {J_TAI},
  author       = {Kun Zhu and Nana Zhang and Weiping Ding and Changjun Jiang},
  doi          = {10.1109/TAI.2024.3359568},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {4026-4041},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An adaptive heterogeneous credit card fraud detection model based on deep reinforcement training subset selection},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Focal transfer graph network and its application in
cross-scene hyperspectral image classification. <em>TAI</em>,
<em>5</em>(8), 4013–4025. (<a
href="https://doi.org/10.1109/TAI.2024.3357658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Affected by the sensor, shooting environment, and other aspects, hyperspectral images (HSIs) in the source and target domains exhibit phenomenon of difficult feature extraction and domain shift. The above phenomena pose challenges to the cross-scene HSI classification task. Therefore, a focal transfer graph network (FTGN) for cross-scene HSI classification is proposed. First, FTGN leverages graph sample and aggregate (GraphSAGE) to capture spatial–spectral features by aggregating partial adjacency nodes, ensuring the acquisition of contextual information. The neighbor weighting strategy based on spatial–spectral information is proposed to solve the information interference caused by excessive node aggregation. Second, a pseudolabel trimming strategy based on class metrics is proposed to reduce the adverse effects of pseudolabel noise in the transfer process. Then, a specification subdomain adaptation (SSA) module is proposed, which helps to achieve effective distribution alignment by reducing the feature distance of intraclass samples and widening the feature distance of interclass samples during the subdomain adaptation process. Finally, the focal loss is utilized to help FTGN focus on hard-to-classify samples. The experimental results on eight data pairs show that the proposed method outperforms several state-of-the-art methods.},
  archive      = {J_TAI},
  author       = {Haoyu Wang and Xiaomin Liu},
  doi          = {10.1109/TAI.2024.3357658},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {4013-4025},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Focal transfer graph network and its application in cross-scene hyperspectral image classification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). X-fuzz: An evolving and interpretable neuro-fuzzy learner
for data streams. <em>TAI</em>, <em>5</em>(8), 4001–4012. (<a
href="https://doi.org/10.1109/TAI.2024.3363116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While evolving, neuro-fuzzy systems have shown promise for learning from nonstationary streaming data with concept drift, most existing models lack transparency due to the limited interpretability of Takagi–Sugeno (TS) fuzzy architecture&#39;s linear rule consequents. The lack of transparency limits the reliability of crucial applications. To address this limitation, this article proposes a new evolving neuro-fuzzy system (ENFS) called X-Fuzz that enhances interpretability by integrating the local interpretable model-agnostic explanations (LIME) technique to provide local explanations and evaluates them using faithfulness and monotonicity metrics. X-Fuzz is rigorously tested on streaming datasets with diverse concept drifts via prequential analysis. Experiments demonstrate X-Fuzz&#39;s capabilities in mining insights from large and dynamic data streams exhibiting diverse concept drifts including abrupt, gradual, recurring contextual, and cyclical drifts. In addition, for online runway exit prediction using real aviation data, X-Fuzz achieved 98.04% accuracy, significantly exceeding recent methods. With its balance of efficiency and transparency, X-Fuzz represents a promising approach for trustworthy evolving artificial intelligence (AI) that can handle complex, nonstationary data streams in critical real-world settings. We have made the X-Fuzz source code available at https://github.com/m-ferdaus/X_Fuzz for reproducibility and to facilitate future research.},
  archive      = {J_TAI},
  author       = {Md Meftahul Ferdaus and Tanmoy Dam and Sameer Alam and Duc-Thinh Pham},
  doi          = {10.1109/TAI.2024.3363116},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {4001-4012},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {X-fuzz: An evolving and interpretable neuro-fuzzy learner for data streams},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal inference of hidden markov models through
expert-acquired data. <em>TAI</em>, <em>5</em>(8), 3985–4000. (<a
href="https://doi.org/10.1109/TAI.2024.3358261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on inferring a general class of hidden Markov models (HMMs) using data acquired from experts. Expert-acquired data contain decisions/actions made by humans/users for various objectives, such as navigation data reflecting drivers’ behavior, cybersecurity data carrying defender decisions, and biological data containing the biologist&#39;s actions (e.g., interventions and experiments). Conventional inference methods rely on temporal changes in data without accounting for expert knowledge. This article incorporates expert knowledge into the inference of HMMs by modeling expert behavior as an imperfect reinforcement learning agent. The proposed method optimally quantifies experts’ perceptions about the system model, which, alongside the temporal changes in data, contributes to the inference process. The proposed inference method is derived through a combination of dynamic programming and optimal recursive Bayesian estimation. The applicability of this method is demonstrated to a wide range of inference criteria, such as maximum likelihood and maximum a posteriori. The performance of the proposed method is investigated through a comprehensive numerical experiment using a benchmark problem and biological networks.},
  archive      = {J_TAI},
  author       = {Amirhossein Ravari and Seyede Fatemeh Ghoreishi and Mahdi Imani},
  doi          = {10.1109/TAI.2024.3358261},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {3985-4000},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Optimal inference of hidden markov models through expert-acquired data},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving source tracking accuracy through learning-based
estimation methods in SH domain: A comparative study. <em>TAI</em>,
<em>5</em>(8), 3974–3984. (<a
href="https://doi.org/10.1109/TAI.2024.3355323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acoustic source tracking is significant across applications like surveillance, teleconferencing, and robot audition, yet the complexity introduced by reverberation, background noise, and overlapping sources impedes precise source localization. This article uses learning-based localization methods to introduce a resilient and intelligent acoustic source tracking approach in the spherical harmonics (SHs) domain. The tracking algorithms anticipate moving source locations by leveraging past predictions and direction of arrival (DOA) estimations. The prediction probability is computed through alpha–beta and Kalman filtering applied to the estimated DOAs, which are likelihood probabilities obtained from learning models. Utilizing the spatial attributes of sound sources encoded in SH signals, diverse learning-based frameworks are introduced to capture the intricate relationship between SH features and source locations. Supervised learning is utilized to train the models that minimize localization errors between predicted and ground truth positions. Experimental assessments underscore the efficacy and resilience of our proposed approach, conducted using LOCAlization and TrAcking (LOCATA) data, revealing a substantial enhancement in tracking accuracy compared to baseline methods.},
  archive      = {J_TAI},
  author       = {Priyadarshini Dwivedi and Gyanajyoti Routray and Devansh Kumar Jha and Rajesh M. Hegde},
  doi          = {10.1109/TAI.2024.3355323},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {3974-3984},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Improving source tracking accuracy through learning-based estimation methods in SH domain: A comparative study},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Encoder–decoder calibration for multimodal machine
translation. <em>TAI</em>, <em>5</em>(8), 3965–3973. (<a
href="https://doi.org/10.1109/TAI.2024.3354668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main purpose of multimodal machine translation (MMT) is to improve the quality of translation results by taking the corresponding visual context as an additional input. Recently many studies in neural machine translation have attempted to obtain high-quality multimodal representation of encoder or decoder via attention mechanism. However, attention mechanism does not always accurately identify the decisive input for each prediction, which leads to an unsatisfactory multimodal information fusion. To this end, we propose an encoder–decoder (Enc–Dec) calibration method which can automatically calibrate the image and text fusion representation in the encoder, and find the decisive input to the translation in the decoder. We validate our model on the MMT dataset Multi30K. Experimental results show that our method significantly outperforms several recent baselines for both English–German and English–French translation tasks in terms of BLEU and METEOR.},
  archive      = {J_TAI},
  author       = {Turghun Tayir and Lin Li and Bei Li and Jianquan Liu and Kong Aik Lee},
  doi          = {10.1109/TAI.2024.3354668},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {3965-3973},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Encoder–Decoder calibration for multimodal machine translation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An attention augmented convolution-based tiny-residual UNet
for road extraction. <em>TAI</em>, <em>5</em>(8), 3951–3964. (<a
href="https://doi.org/10.1109/TAI.2024.3357437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently remote sensing images have become more popular due to improved image quality and resolution. These images have been shown to be a valuable data source for road extraction applications like intelligent transportation systems, road maintenance, and road map making. In recent decades, the use of highly significant deep learning (DL) in automatic road extraction from these images has been a hot research area. However, highly accurate road extractions from remote sensing images remain a challenge because they are cluttered in the background and have widely different shapes and complex connectivities. This article proposes novel tiny attention augmented convolution-based residual UNet architecture (Tiny-AAResUNet) for road extraction, which adopts powerful features of self-attention mechanism and advantageous properties of residual UNet structure. The self-attention mechanism uses attention augmented convolutional (AttAug-Conv) operation to capture long-range global information; however, traditional convolution has a fundamental disadvantage: it only performs on local information. Therefore, we use the AttAug-Conv layer as an alternative to standard convolution layers to obtain more discriminant feature representations. It allows to develop a network with fewer parameters. We also adopt improved residual units in standard ResUNet to the speedup training process and enhance the segmentation accuracy of the network. Experimental results on Massachusetts, DeepGlobe Challenge, and UAV Road Dataset show that the Tiny-AAResUNet performs well in road extraction, with intersection over union (IoU) (94.27%), lower trainable parameters (1.20 M), and inference time (1.14 s). Comparative results on the proposed method have outperformed in road extraction with ten recently established DL approaches.},
  archive      = {J_TAI},
  author       = {Parmeshwar S. Patil and Raghunath S. Holambe and Laxman M. Waghmare},
  doi          = {10.1109/TAI.2024.3357437},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {3951-3964},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An attention augmented convolution-based tiny-residual UNet for road extraction},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantile-long short term memory: A robust, time series
anomaly detection method. <em>TAI</em>, <em>5</em>(8), 3939–3950. (<a
href="https://doi.org/10.1109/TAI.2024.3353163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomalies refer to the departure of systems and devices from their normal behavior in standard operating conditions. An anomaly in an industrial device can indicate an upcoming failure, often in the temporal direction. In this article, we make two contributions: 1) we estimate conditional quantiles in the popular long short term memory networks (LSTMs) architecture, propose a novel anomaly detection method, quantile-long short term memory (q-LSTM), and consider three different ways to define anomalies based on the estimated quantiles; and 2) we use a new learnable activation function (AF), parametric Elliot function (PEF), in q-LSTM architecture to model temporal long-range dependency. Unlike sigmoid and tanh , the derivative of the PEF depends on the input as well as on the parameter, which help in mitigating the vanishing gradient problem and therefore facilitates in escaping early saturation. The proposed algorithms are compared with other well-known anomaly detection algorithms, such as isolation forest (iForest), elliptic envelope, autoencoder, and modern deep learning models, namely deep autoencoding Gaussian mixture model (DAGMM) and generative adversarial networks (GANs). The algorithms are evaluated using various performance metrics, such as precision and recall. The algorithms have been tested on multiple industrial time-series datasets such as Yahoo, Amazon Web Services (AWS), General Electric (GE), and machine sensors. We have found that the LSTM-based quantile algorithms are very effective and outperformed the existing algorithms in identifying anomalies.},
  archive      = {J_TAI},
  author       = {Snehanshu Saha and Jyotirmoy Sarkar and Soma S. Dhavala and Preyank Mota and Santonu Sarkar},
  doi          = {10.1109/TAI.2024.3353163},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {3939-3950},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Quantile-long short term memory: A robust, time series anomaly detection method},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised forecasting in electronic health records
with attention-free models. <em>TAI</em>, <em>5</em>(8), 3926–3938. (<a
href="https://doi.org/10.1109/TAI.2024.3353164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the proven effectiveness of Transformer neural networks across multiple domains, their performance with electronic health records (EHRs) can be nuanced. The unique, multidimensional sequential nature of EHR data can sometimes make even simple linear models with carefully engineered features more competitive. Thus, the advantages of Transformers, such as efficient transfer learning and improved scalability, are not always fully exploited in EHR applications. Addressing these challenges, we introduce SANSformer, an attention-free sequential model designed with specific inductive biases to cater for the unique characteristics of EHR data. In this work, we aim to forecast the demand for healthcare services, by predicting the number of patient visits to healthcare facilities. The challenge amplifies when dealing with divergent patient subgroups, like those with rare diseases, which are characterized by unique health trajectories and are typically smaller in size. To address this, we employ a self-supervised pretraining strategy, generative summary pretraining (GSP), which predicts future summary statistics based on past health records of a patient. Our models are pretrained on a health registry of nearly one million patients, then fine-tuned for specific subgroup prediction tasks, showcasing the potential to handle the multifaceted nature of EHR data. In evaluation, SANSformer consistently surpasses robust EHR baselines, with our GSP pretraining method notably amplifying model performance, particularly within smaller patient subgroups. Our results illuminate the promising potential of tailored attention-free models and self-supervised pretraining in refining healthcare utilization predictions across various patient demographics.},
  archive      = {J_TAI},
  author       = {Yogesh Kumar and Alexander Ilin and Henri Salo and Sangita Kulathinal and Maarit K. Leinonen and Pekka Marttinen},
  doi          = {10.1109/TAI.2024.3353164},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {3926-3938},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Self-supervised forecasting in electronic health records with attention-free models},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Proximal policy optimization with advantage reuse
competition. <em>TAI</em>, <em>5</em>(8), 3915–3925. (<a
href="https://doi.org/10.1109/TAI.2024.3354694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, reinforcement learning (RL) has made great achievements in artificial intelligence. Proximal policy optimization (PPO) is a representative RL algorithm, which limits the magnitude of each policy update to achieve monotonic policy improvement. However, as an on-policy algorithm, PPO suffers from sample inefficiency and poor policy exploratory. To solve above problems, the off-policy advantage is proposed, which calculates the advantage function through the reuse of previous policy, and the proximal policy optimization with advantage reuse (PPO-AR) is proposed. Furthermore, to improve the sampling efficiency of policy update, the proximal policy optimization with advantage reuse competition (PPO-ARC) is proposed, which introduces PPO-AR into the policy calculation and uses the parallel competitive optimization, and it is shown to improve the performance of policy. Moreover, to improve the exploratory of policy update, the proximal policy optimization with generalized clipping (PPO-GC) is proposed, which relaxes the limits of policy update by changing the policy flat clipping boundary. Experimental results on OpenAI Gym demonstrate the effectiveness of our proposed PPO-ARC and PPO-GC.},
  archive      = {J_TAI},
  author       = {Yuhu Cheng and Qingbang Guo and Xuesong Wang},
  doi          = {10.1109/TAI.2024.3354694},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {3915-3925},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Proximal policy optimization with advantage reuse competition},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep learning-based cyber intrusion detection and
mitigation system for smart grids. <em>TAI</em>, <em>5</em>(8),
3902–3914. (<a href="https://doi.org/10.1109/TAI.2024.3354688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of power system digitalization initiatives is revolutionizing the way electricity grids are monitored and protected. However, the integration of cyber and physical electrical infrastructures leads to an increase in the risk of cyber intrusions. Attackers can gain access to the smart grid and inject falsified data, leading the protection schemes to activate unnecessary power outage actions. Such outages can be devastating to end users. In this article, an intrusion detection and mitigation system (IDMS) is proposed using deep learning neural networks (DLNNs) to detect, classify, and locate intrusions in smart grids. Once the disturbance is detected, the IDMS is designed to diagnose the intrusion and classify the attack into a single point or coordinated intrusion. Afterward, the algorithm locates and isolates the contaminated intelligent electronic device (IED) and predicts its current waveform utilizing long short-term memory (LSTM) to maintain power system observability. The proposed IDMS performs the required diagnosis on the modified IEEE 13-bus system. Simulation results demonstrate high accuracy in the proposed detection, classification, location, and prediction approach.},
  archive      = {J_TAI},
  author       = {Abdulaziz Aljohani and Mohammad AlMuhaini and H. Vincent Poor and Hamed M. Binqadhi},
  doi          = {10.1109/TAI.2024.3354688},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {3902-3914},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A deep learning-based cyber intrusion detection and mitigation system for smart grids},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Complexity-driven model compression for resource-constrained
deep learning on edge. <em>TAI</em>, <em>5</em>(8), 3886–3901. (<a
href="https://doi.org/10.1109/TAI.2024.3353157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in artificial intelligence (AI) on the Internet of Things (IoT) devices have realized edge AI in several applications by enabling low latency and energy efficiency. However, deploying state-of-the-art convolutional neural networks (CNNs) such as VGG-16 and ResNets on resource-constrained edge devices is practically infeasible due to their extensive parameter counts and floating-point operations (FLOPs). Thus, the concept of network pruning as a type of model compression is gaining attention for accelerating CNNs on low-power devices. State-of-the-art pruning approaches, either structured or unstructured , postulate a three-stage: training–pruning–retraining pipeline, which results in an inevitable retraining overhead. In this work, we posit an orthogonal conjecture on structured pruning at initialization to find sparse subnetworks realizing $\approx$ 60% less training time than conventional pruning. Moreover, conventional pruning focuses on identifying the saliency at filter granularity while ignoring the importance of characteristics at layer granularity. In contrast, the proposed complexity-driven approach leverages the intrinsic complexities of CNN layers to guide the filter pruning process without requiring dense pretraining of models. Particularly, we characterize the importance of CNN layers with respect to parameters, FLOPs, and memory-based complexities to work in tandem with filter pruning in a structured manner. Experiments show the competitive performance of our approach in terms of accuracy and acceleration for all three modes of pruning, namely parameter-aware (PA), FLOPs-aware (FA), and memory-aware (MA). For example, reducing $\approx$ 70% parameters, $\approx$ 50% FLOPs, and $\approx$ 50% memory from MobileNetV2 did not result in any accuracy loss, unlike state-of-the-art approaches. Lastly, we present a tradeoff between different resources and accuracy, which can be helpful for developers in making the right decisions in resource-constrained IoT environments.},
  archive      = {J_TAI},
  author       = {Muhammad Zawish and Steven Davy and Lizy Abraham},
  doi          = {10.1109/TAI.2024.3353157},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {3886-3901},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Complexity-driven model compression for resource-constrained deep learning on edge},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Securing user privacy in cloud-based whiteboard services
against health attribute inference attacks. <em>TAI</em>, <em>5</em>(8),
3872–3885. (<a href="https://doi.org/10.1109/TAI.2024.3352529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud-based whiteboard services have gained immense popularity, facilitating seamless collaboration and communication. However, the open-ended and persistent nature of whiteboard data exposes privacy vulnerabilities. In this article, we investigate potential health attribute inference attacks that leverage drawings to infer sensitive user information without consent. We develop a local differentially private algorithm that perturbs drawings by spatially deforming them, providing provable privacy guarantees. Our algorithm is implemented in a software tool called DP-WhiteBoard (DP-WB). Extensive experiments demonstrate the algorithm&#39;s ability to significantly reduce the accuracy of health attribute inference attacks while maintaining utility for benign recognition tasks. This work represents the first comprehensive study of emerging privacy threats in cloud-based whiteboards, proposing an scalable and adaptable solution with provable privacy guarantee. A demonstration of our work can be found at https://youtu.be/5gD1Te1Fgnw .},
  archive      = {J_TAI},
  author       = {Abdur R. Shahid and Ahmed Imteaj},
  doi          = {10.1109/TAI.2024.3352529},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {3872-3885},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Securing user privacy in cloud-based whiteboard services against health attribute inference attacks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Additive noise model structure learning based on spatial
coordinates. <em>TAI</em>, <em>5</em>(8), 3858–3871. (<a
href="https://doi.org/10.1109/TAI.2024.3351102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovering causal relationships from a large amount of observational data is an important research direction in data mining. To address the challenges of discovering and constructing causal networks on nonlinear and high-dimensional data, this article proposes a new structural learning algorithm called spatial coordinates based (SCB). The SCB algorithm effectively discovers causal networks from large scale nonlinear data. In this article, we make three main contributions. Firstly, based on the Hilbert–Schmidt independence criterion (HSIC), we propose new independence test coefficients: spatial coordinate (SC) coefficient and conditional spatial coordinate (CSC) coefficient. We also prove that the statistical distribution of the CSC coefficient follows a standard normal distribution. Secondly, using the statistical distribution of the CSC coefficient, we redefine the correlation of variables and combine it with local learning to propose the SCB algorithm. Finally, we demonstrate the effectiveness of the algorithm through experiments on data generated from various nonlinear function dependencies. Compared to existing algorithms, the causal network models constructed using the SCB algorithm exhibit significant improvements in accuracy and time performance. The effectiveness of the proposed method is further validated on real power plant data.},
  archive      = {J_TAI},
  author       = {Jing Yang and Ting Lu and Youjie Zhu},
  doi          = {10.1109/TAI.2024.3351102},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {3858-3871},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Additive noise model structure learning based on spatial coordinates},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Octant spherical harmonics features for source localization
using artificial intelligence based on unified learning framework.
<em>TAI</em>, <em>5</em>(8), 3845–3857. (<a
href="https://doi.org/10.1109/TAI.2024.3352530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in artificial intelligence (AI) have shown potential solutions to acoustic source localization in three-dimensional space. This article proposes a new low-complex AI-based framework in the spherical harmonics (SH) domain for efficient direction of arrival (DOA) estimation. The SH coefficients are the key features for the DOA estimation and are obtained from the SH decomposition (SHD) of the spherical microphone array (SMA) recordings. Subsequently, the unified convolutional neural network (UCNN) model is trained to estimate the source azimuth and elevation from the phase and magnitude of the SH coefficient. Since the relation between the azimuth and elevation with phase and magnitude of the SH coefficient is subjective, a high volume of data are required to train the model. In this context, the symmetric properties of the SH basis function are explored to obtain the SH implicit symmetric coefficients (SH-ISCs) that split the 3-D space into octant classes. Within each octant, the phase and magnitude of the SH coefficients exhibit one-to-one correspondence with the source azimuth and elevation and execute the data redundancy. This work can be divided into two parts, a multiclass support vector machine (M-SVM) is investigated to obtain the octant classes from the SH-ISC in the first part. In the second part, the UCNN model is developed to estimate the DOA angles in each octant class. Further, the proposed technique is computationally efficient compared to the baseline learning algorithms in terms of computational and run-time complexity.},
  archive      = {J_TAI},
  author       = {Priyadarshini Dwivedi and Gyanajyoti Routray and Rajesh M. Hegde},
  doi          = {10.1109/TAI.2024.3352530},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {3845-3857},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Octant spherical harmonics features for source localization using artificial intelligence based on unified learning framework},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A perceptual computing approach for learning interpretable
unsupervised fuzzy scoring systems. <em>TAI</em>, <em>5</em>(8),
3832–3844. (<a href="https://doi.org/10.1109/TAI.2023.3333762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scoring the driver&#39;s behavior through the analysis of his/her road trip data is an active area of research. However, such systems suffer from a lack of explainability, integration of expert bias in the calculated score, and ignoring the semantic uncertainty of various variables contributing to the score. To overcome these limitations, we have proposed a novel perceptual computing based unsupervised scoring system. The prowess of the proposed system has been exemplified in a case study of driver&#39;s scoring from telemetry data. Our proposed approach yields scores that showed a higher significant separability between drivers performing responsible or irresponsible (aggressive or drowsy) driving behaviors, than the formal method of computing these scores (a $p$ value of $3.94\boldsymbol{\times}10^{-4}$ and $3.42\boldsymbol{\times}10^{-3}$ , respectively, in a Kolmogorov–Smirnov test). Further, the proposed method displayed higher robustness in the bootstrap test (where 30% of original data were omitted at random) by providing scores that were 90% similar to the original ones for all results within a confidence interval of 95%.},
  archive      = {J_TAI},
  author       = {Prashant K. Gupta and Deepak Sharma and Javier Andreu-Perez},
  doi          = {10.1109/TAI.2023.3333762},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {3832-3844},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A perceptual computing approach for learning interpretable unsupervised fuzzy scoring systems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient structure slimming for spiking neural networks.
<em>TAI</em>, <em>5</em>(8), 3823–3831. (<a
href="https://doi.org/10.1109/TAI.2024.3352533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) are deeply inspired by biological neural information systems. Compared to convolutional neural networks (CNNs), SNNs are low power consumption because of their spike based information processing mechanism. However, most of the current structures of SNNs are fully connected or converted from deep CNNs which poses redundancy connections. While the structure and topology in human brain systems are sparse and efficient. This article aims at taking full advantage of sparse structure and low power consumption which lie in human brain and proposed efficient structure slimming methods. Inspired by the development of biological neural network structures, this article designed types of structure slimming methods including neuron pruning and channel pruning. In addition to pruning, this article also considers the growth and development of the nervous system. Through iterative application of the proposed neural pruning and rewiring algorithms, experimental evaluations on Canadian Institute for Advanced Research (CIFAR)-10, CIFAR-100, and dynamic vision sensor (DVS)-gesture datasets demonstrate the effectiveness of the structure slimming methods. When the parameter count is reduced to only about $10$ % of the original, the performance decreases by less than $1$ %.},
  archive      = {J_TAI},
  author       = {Yaxin Li and Xuanye Fang and Yuyuan Gao and Dongdong Zhou and Jiangrong Shen and Jian K. Liu and Gang Pan and Qi Xu},
  doi          = {10.1109/TAI.2024.3352533},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {3823-3831},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Efficient structure slimming for spiking neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on neural network hardware accelerators.
<em>TAI</em>, <em>5</em>(8), 3801–3822. (<a
href="https://doi.org/10.1109/TAI.2024.3377147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) hardware accelerator is an emerging research for several applications and domains. The hardware accelerator&#39;s direction is to provide high computational speed with retaining low-cost and high learning performance. The main challenge is to design complex machine learning models on hardware with high performance. This article presents a thorough investigation into machine learning accelerators and associated challenges. It describes a hardware implementation of different structures such as convolutional neural network (CNN), recurrent neural network (RNN), and artificial neural network (ANN). The challenges such as speed, area, resource consumption, and throughput are discussed. It also presents a comparison between the existing hardware design. Last, the article describes the evaluation parameters for a machine learning accelerator in terms of learning and testing performance and hardware design.},
  archive      = {J_TAI},
  author       = {Tamador Mohaidat and Kasem Khalil},
  doi          = {10.1109/TAI.2024.3377147},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {3801-3822},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A survey on neural network hardware accelerators},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive survey on graph summarization with graph
neural networks. <em>TAI</em>, <em>5</em>(8), 3780–3800. (<a
href="https://doi.org/10.1109/TAI.2024.3350545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As large-scale graphs become more widespread, more and more computational challenges with extracting, processing, and interpreting large graph data are being exposed. It is therefore natural to search for ways to summarize these expansive graphs while preserving their key characteristics. In the past, most graph summarization techniques sought to capture the most important part of a graph statistically. However, today, the high dimensionality and complexity of modern graph data are making deep learning techniques more popular. Hence, this article presents a comprehensive survey of progress in deep learning summarization techniques that rely on graph neural networks (GNNs). Our investigation includes a review of the current state-of-the-art approaches, including recurrent GNNs, convolutional GNNs, graph autoencoders, and graph attention networks. A new burgeoning line of research is also discussed where graph reinforcement learning is being used to evaluate and improve the quality of graph summaries. Additionally, the survey provides details of benchmark datasets, evaluation metrics, and open-source tools that are often employed in experimentation settings, along with a detailed comparison, discussion, and takeaways for the research community focused on graph summarization. Finally, the survey concludes with a number of open research challenges to motivate further study in this area.},
  archive      = {J_TAI},
  author       = {Nasrin Shabani and Jia Wu and Amin Beheshti and Quan Z. Sheng and Jin Foo and Venus Haghighi and Ambreen Hanif and Maryam Shahabikargar},
  doi          = {10.1109/TAI.2024.3350545},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {3780-3800},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A comprehensive survey on graph summarization with graph neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on verification and validation, testing and
evaluations of neurosymbolic artificial intelligence. <em>TAI</em>,
<em>5</em>(8), 3765–3779. (<a
href="https://doi.org/10.1109/TAI.2024.3351798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurosymbolic artificial intelligence (AI) is an emerging branch of AI that combines the strengths of symbolic AI and subsymbolic AI. Symbolic AI is based on the idea that intelligence can be represented using semantically meaningful symbolic rules and representations, while deep learning (DL), or sometimes called subsymbolic AI, is based on the idea that intelligence emerges from the collective behavior of artificial neurons that are connected to each other. A major drawback of DL is that it acts as a “black box,” meaning that predictions are difficult to explain, making the testing &amp; evaluation (T&amp;E) and validation &amp; verification (V&amp;V) processes of a system that uses subsymbolic AI a challenge. Since neurosymbolic AI combines the advantages of both symbolic and subsymbolic AI, this survey explores how neurosymbolic applications can ease the V&amp;V process. This survey considers two taxonomies of neurosymbolic AI, evaluates them, and analyzes which algorithms are commonly used as the symbolic and subsymbolic components in current applications. Additionally, an overview of current techniques for the T&amp;E and V&amp;V processes of these components is provided. Furthermore, it is investigated how the symbolic part is used for T&amp;E and V&amp;V purposes in current neurosymbolic applications. Our research shows that neurosymbolic AI has great potential to ease the T&amp;E and V&amp;V processes of subsymbolic AI by leveraging the possibilities of symbolic AI. Additionally, the applicability of current T&amp;E and V&amp;V methods to neurosymbolic AI is assessed, and how different neurosymbolic architectures can impact these methods is explored. It is found that current T&amp;E and V&amp;V techniques are partly sufficient to test, evaluate, verify, or validate the symbolic and subsymbolic part of neurosymbolic applications independently, while some of them use approaches where current T&amp;amp;E and V&amp;amp;V methods are not applicable by default, and adjustments or even new approaches are needed. Our research shows that there is great potential in using symbolic AI to test, evaluate, verify, or validate the predictions of a subsymbolic model, making neurosymbolic AI an interesting research direction for safe, secure, and trustworthy AI.},
  archive      = {J_TAI},
  author       = {Justus Renkhoff and Ke Feng and Marc Meier-Doernberg and Alvaro Velasquez and Houbing Herbert Song},
  doi          = {10.1109/TAI.2024.3351798},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {3765-3779},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A survey on verification and validation, testing and evaluations of neurosymbolic artificial intelligence},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Memory prompt for spatiotemporal transformer visual object
tracking. <em>TAI</em>, <em>5</em>(8), 3759–3764. (<a
href="https://doi.org/10.1109/TAI.2024.3353698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent transformer techniques have achieved promising performance boosts in visual object tracking, with their capability to exploit long-range dependencies among relevant tokens. However, a long-range interaction can be achieved only at the expense of huge computation, which is proportional to the square of the number of tokens. This becomes particularly acute in online visual tracking with a memory bank containing multiple templates, which is a widely used strategy to address spatiotemporal template variations. We address this complexity problem by proposing a memory prompt tracker (MPTrack) that enables multitemplate aggregation and efficient interactions among relevant queries and clues. The memory prompt gathers any supporting context from the historical templates in the form of learnable token queries, producing a concise dynamic target representation. The extracted prompt tokens are then fed into a transformer encoder–decoder to inject the relevant clues into the instance, thus achieving improved target awareness from the spatiotemporal perspective. The experimental results on standard benchmarking datasets, i.e., UAV123, TrackingNet, large-scale single object tracking benchmark (LaSOT), and generic object tracking benchmark (GOT)-10k, demonstrate the merit of the proposed memory prompt in obtaining an efficient and promising tracking performance, as compared with the state-of-the-art.},
  archive      = {J_TAI},
  author       = {Tianyang Xu and Xiao-Jun Wu and Xuefeng Zhu and Josef Kittler},
  doi          = {10.1109/TAI.2024.3353698},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {3759-3764},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Memory prompt for spatiotemporal transformer visual object tracking},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Defending against poisoning attacks in federated learning
with blockchain. <em>TAI</em>, <em>5</em>(7), 3743–3756. (<a
href="https://doi.org/10.1109/TAI.2024.3376651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of deep learning, federated learning (FL) presents a promising approach that allows multiinstitutional data owners, or clients, to collaboratively train machine learning models without compromising data privacy. However, most existing FL approaches rely on a centralized server for global model aggregation, leading to a single point of failure. This makes the system vulnerable to malicious attacks when dealing with dishonest clients. In this work, we address this problem by proposing a secure and reliable FL system based on blockchain and distributed ledger technology. Our system incorporates a peer-to-peer voting mechanism and a reward-and-slash mechanism, which are powered by on-chain smart contracts, to detect and deter malicious behaviors. Both theoretical and empirical analyses are presented to demonstrate the effectiveness of the proposed approach, showing that our framework is robust against malicious client-side behaviors.},
  archive      = {J_TAI},
  author       = {Nanqing Dong and Zhipeng Wang and Jiahao Sun and Michael Kampffmeyer and William Knottenbelt and Eric Xing},
  doi          = {10.1109/TAI.2024.3376651},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3743-3756},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Defending against poisoning attacks in federated learning with blockchain},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling and clustering of parabolic granular data.
<em>TAI</em>, <em>5</em>(7), 3728–3742. (<a
href="https://doi.org/10.1109/TAI.2024.3377172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, there exist some problems in granular clustering methods, such as lack of nonlinear membership description and global optimization of granular data boundaries. To address these issues, in this study, revolving around the parabolic granular data, we propose an overall architecture for parabolic granular modeling and clustering. To begin with, novel coverage and specificity functions are established, and then a parabolic granular data structure is proposed. The fuzzy c-means (FCM) algorithm is used to obtain the numeric prototypes, and then particle swarm optimization (PSO) is introduced to construct the parabolic granular data from the global perspective under the guidance of principle of justifiable granularity (PJG). Combining the advantages of FCM and PSO, we propose the parabolic granular modeling and optimization (PGMO) method. Moreover, we put forward attribute weights and sample weights as well as a distance measure induced by the Gaussian kernel similarity, and then come up with the algorithm of weighted kernel fuzzy clustering for parabolic granularity (WKFC-PG). In addition, the assessment mechanism of parabolic granular clustering is discussed. In summary, we set up an overall architecture including parabolic granular modeling, clustering, and assessment. Finally, comparative experiments on artificial, UCI, and high-dimensional datasets validate that our overall architecture delivers a good improvement over previous strategies. The parameter analysis and time complexity are also given for WKFC-PG. In contrast with related granular clustering algorithms, it is observed that WKFC-PG performs better than other granular clustering algorithms and has superior stability in handling outliers, especially on high-dimensional datasets.},
  archive      = {J_TAI},
  author       = {Yiming Tang and Jianwei Gao and Witold Pedrycz and Xianghui Hu and Lei Xi and Fuji Ren and Min Hu},
  doi          = {10.1109/TAI.2024.3377172},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3728-3742},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Modeling and clustering of parabolic granular data},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive learning for soil classification in laser-induced
breakdown spectroscopy streaming. <em>TAI</em>, <em>5</em>(7),
3714–3727. (<a href="https://doi.org/10.1109/TAI.2024.3375260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of machine learning (ML) has accelerated the development of laser-induced breakdown spectroscopy (LIBS) in soil analysis. However, analyzing remote LIBS data in real time using ML is challenging due to several factors. First, building robust ML models requires extensive calibration datasets, which are not always possible with limited LIBS experimental data. Second, matrix effects can worsen LIBS performance, and changes in sample physical properties or the apparatus can impact the distribution and intensity of emission lines. These issues may lead to concept drift in real-time/online data streaming, causing the relationship between the input and the target spectra to change over time. Consequently, an ML model designed for one LIBS system may not apply to another. To conquer these challenges, we propose a framework based on transfer learning (TL) to use limited experimental data and adapt to the emission line variation in the LIBS streaming. A model is first pretrained using a large labeled source dataset and then fine-tuned with new experimental measurements to classify soil samples. LIBS measurements are conducted with variations in sample properties and experimental parameters to simulate differences in remote LIBS sensors. The collected spectra are fed into the model by chunks, and data evolution is dynamically learned by self-balanced learning to self-adapt to the domain shift. The proposed framework is found effective in improving classification accuracy during data streaming by implementing TL and supporting adaptation compared to the literature.},
  archive      = {J_TAI},
  author       = {Yingchao Huang and Shubho Mohajan and Nicholas F. Beier and Ying Wan and Sadee Lamothe and Abdul Bais and Miles Dyck and Frank Hegmann and Amina E. Hussein},
  doi          = {10.1109/TAI.2024.3375260},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3714-3727},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adaptive learning for soil classification in laser-induced breakdown spectroscopy streaming},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diverse hazy image synthesis via coloring network.
<em>TAI</em>, <em>5</em>(7), 3703–3713. (<a
href="https://doi.org/10.1109/TAI.2024.3379113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural network (CNN)-based dehazing methods have achieved great success in single image dehazing. However, the absence of real-world haze image datasets hinders the deep development of single image dehazing. To address this issue, we propose a diverse hazy image synthesis method based on generative adversarial network (GAN) and matting. Specially, we train a GAN-based model that can transform a gray image into a hazy image. To boost the diversity of hazy images, we propose to simulate hazy images via image matting, which can fuse a real haze image with another image containing diverse objects. To evaluate the performance of dehazing methods, we propose two novel metrics: part-based peak signal-to-noise ratio (PSNR) and structure similarity index measure (SSIM). Extensive experiments are conducted to show the effectiveness of the proposed model, dataset, and criteria.},
  archive      = {J_TAI},
  author       = {Shengdong Zhang and Xiaoqin Zhang and Shaohua Wan and Wenqi Ren and Liping Zhao and Li Zhao and Linlin Shen},
  doi          = {10.1109/TAI.2024.3379113},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3703-3713},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Diverse hazy image synthesis via coloring network},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Short-term residential load forecasting via pooling-ensemble
model with smoothing clustering. <em>TAI</em>, <em>5</em>(7), 3690–3702.
(<a href="https://doi.org/10.1109/TAI.2024.3375833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short-term residential load forecasting is essential to demand side response. However, the frequent spikes in the load and the volatile daily load patterns make it difficult to accurately forecast the load. To deal with these problems, this article proposes a smoothing clustering method for daily load clustering and a pooling-ensemble model for one day ahead load forecasting. The whole short-term load forecasting framework in this article contains three steps. Specifically and first, the states of the residents are obtained by clustering the daily load curves with the proposed smoothing clustering method. Second, a weighted mixed Markov model is built to predict the probability distribution of the load state in the next day. Third, multiple predictors in the pooling-ensemble model are selected for different states and the load is forecasted by weighing the results of the multiple predictors based on the predicted states. Results of the case studies and comparison studies on two public datasets verify the advantages of the smoothing clustering method and the pooling-ensemble model.},
  archive      = {J_TAI},
  author       = {Jiang-Wen Xiao and Hongliang Fang and Yan-Wu Wang},
  doi          = {10.1109/TAI.2024.3375833},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3690-3702},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Short-term residential load forecasting via pooling-ensemble model with smoothing clustering},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible constraints-based adaptive intelligent
event-triggered control for slowly switched nonlinear systems using
reinforcement learning. <em>TAI</em>, <em>5</em>(7), 3678–3689. (<a
href="https://doi.org/10.1109/TAI.2024.3375828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this note, an adaptive event-triggered optimized tracking control problem is investigated for nonlinear switched systems with flexible output constraints under extended mode-dependent average dwell time (MDADT). Initially, a new shifting function and an improved barrier function are constructed to solve flexible output constraints under the practical background. Subsequently, a global performance function with the exponential discount factor based on the error variable is designed under optimized backstepping (OB), which not only ensures that the performance function converges, but also evaluates the tracking performance of the system, reflecting the energy consumption. The corresponding Hamilton–Jacobi–Bellman (HJB) equation is constructed to solve the optimal control strategy. To remove the restriction on the maximum asynchronous time, an event-triggered optimization strategy for subsystems is utilized to exclude Zeno behavior. Furthermore, we demonstrate that the signals of the closed-loop system are bounded and the flexible output constraints are strictly obeyed. Finally, the application of the above control technique to the manipulator system is validated.},
  archive      = {J_TAI},
  author       = {Chengyuan Yan and Jianwei Xia and Ju H. Park and Xiangpeng Xie},
  doi          = {10.1109/TAI.2024.3375828},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3678-3689},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Flexible constraints-based adaptive intelligent event-triggered control for slowly switched nonlinear systems using reinforcement learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Developing a reliable shallow supervised learning for
thermal comfort using multiple ASHRAE databases. <em>TAI</em>,
<em>5</em>(7), 3664–3677. (<a
href="https://doi.org/10.1109/TAI.2024.3376319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The artificial intelligence (AI) system faces the challenge of insufficient training datasets and the risk of an uncomfortable user experience during the data gathering and learning process. The unreliable training data leads to overfitting and poor system performance which will result in wasting operational energy. This work introduces a reliable data set for training the AI subsystem for thermal comfort. The most reliable current training data sets for thermal comfort are ASHRAE RP-884 and ASHRAE Global Thermal Comfort Database II, but the direct use of these data for learning will give a poor learning result of less than 60% accuracy. This article presents the algorithm for data filtering and semantic data augmentation for the multiple ASHRAE databases for the supervised learning process. The result was verified with the visual psychrometric chart method that can check for overfitting and verified by developing the Internet of Things (IoT) control system for residential usage based on shallow supervised learning. The AI system was a wide artificial neural network (ANN) which is simple enough to be implemented in a local node. The filtering and semantic augmentation method can increase the accuracy to 96.1%. The control algorithm that was developed based on the comfort zone identification can increase the comfort acknowledgement by 6.06% leading to energy saving for comfort. This work can contribute to 717.2 thousand tonnes of CO 2 equivalent per year which is beneficial for a more sustainable thermal comfort system and the development of a reinforced learning system for thermal comfort.},
  archive      = {J_TAI},
  author       = {Kanisius Karyono and Badr M. Abdullah and Alison J. Cotgrave and Ana Bras and Jeff Cullen},
  doi          = {10.1109/TAI.2024.3376319},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3664-3677},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Developing a reliable shallow supervised learning for thermal comfort using multiple ASHRAE databases},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scene text image superresolution through multiscale
interaction of structural and semantic priors. <em>TAI</em>,
<em>5</em>(7), 3653–3663. (<a
href="https://doi.org/10.1109/TAI.2024.3375836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text image superresolution (STISR) aims to enhance the resolution of images containing text within a scene, making the text more readable and easier to recognize. This technique has broad applications in numerous fields such as autonomous driving, document scanning, image retrieval, and so on. However, most existing STISR methods have not fully exploited the multiscale structural and semantic information within scene text images. As a result, the restored text image quality is not sufficient, significantly impacting subsequent tasks such as text detection and recognition. Hence, this article proposes a novel scheme that leverages multiscale structural and semantic priors to efficiently guide text semantic restoration, ultimately yielding high-quality text images. First, a multiscale interaction attention (MSIA) module is designed to capture location-specific details of various-scale structural features and facilitate the recovery of semantic information. Second, a multiscale prior learning module (MSPLM) is developed. Within this module, skip connections are employed among codecs to strengthen both structural and semantic prior features, thereby enhancing the up-sampling and reconstruction capabilities. Finally, building upon the MSPLM, cascaded encoders are connected through residual connections to further enrich the multiscale features and bolster the representational capacity of the prior. Experiments conducted on the standard TextZoom dataset demonstrate that the average recognition accuracies of three evaluators—attentional scene text recognizer (ASTER), convolutional recurrent neural network (CRNN), and multi-object rectified attention network (MORAN)—are 64.4%, 53.5%, and 60.8%, respectively, surpassing most existing methods, including the state-of-the-art ones.},
  archive      = {J_TAI},
  author       = {Zhongjie Zhu and Lei Zhang and Yongqiang Bai and Yuer Wang and Pei Li},
  doi          = {10.1109/TAI.2024.3375836},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3653-3663},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Scene text image superresolution through multiscale interaction of structural and semantic priors},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RGB-d fusion through zero-shot fuzzy membership learning for
salient object detection. <em>TAI</em>, <em>5</em>(7), 3638–3652. (<a
href="https://doi.org/10.1109/TAI.2024.3376640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant improvement has been achieved lately in color and depth data-based salient object detection (SOD) on images from varied datasets, which is mainly due to RGB-D fusion using modern machine learning techniques. However, little emphasis has been given recently on performing RGB-D fusion for SOD in the absence of ground truth data for training. This article proposes a zero-shot deep RGB-D fusion approach based on the novel concept of fuzzy membership learning, which does not require any data for training. The constituent salient object maps to be fused are represented using parametric fuzzy membership functions and the optimal parameter values are estimated through our zero-shot fuzzy membership learning (Z-FML) network. The optimal parameter values are used in a fuzzy inference system along with the constituent salient object maps to perform the fusion. A measure called the membership similarity measure (MSM) is proposed, and the Z-FML network is trained using it to devise a loss function that maximizes the similarity between the constituent salient object maps and the fused salient object map. The deduction of MSM and its properties are shown theoretically, and the gradients involved in the training of the Z-FML network are derived. Qualitative and quantitative evaluations using several datasets signify the effectiveness of our RGB-D fusion and our fusion-based RGB-D SOD in comparison with the state-of-the-art. We also empirically demonstrate the advantage of employing the novel MSM for training our Z-FML network.},
  archive      = {J_TAI},
  author       = {Sudipta Bhuyan and Aupendu Kar and Debashis Sen and Sankha Deb},
  doi          = {10.1109/TAI.2024.3376640},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3638-3652},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {RGB-D fusion through zero-shot fuzzy membership learning for salient object detection},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online reinforcement learning in periodic MDP. <em>TAI</em>,
<em>5</em>(7), 3624–3637. (<a
href="https://doi.org/10.1109/TAI.2024.3375258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study learning in periodic Markov decision process (MDP), a special type of nonstationary MDP where both the state transition probabilities and reward functions vary periodically, under the average reward maximization setting. We formulate the problem as a stationary MDP by augmenting the state space with the period index and propose a periodic upper confidence bound reinforcement learning-2 (PUCRL2) algorithm. We show that the regret of PUCRL2 varies linearly with the period $N$ and as $\mathcal{O}(\sqrt{T \text{log} T})$ with the horizon length $T$ . Utilizing the information about the sparsity of transition matrix of augmented MDP, we propose another algorithm [periodic upper confidence reinforcement learning with Bernstein bounds (PUCRLB) which enhances upon PUCRL2, both in terms of regret ( $O(\sqrt{N})$ dependency on period] and empirical performance. Finally, we propose two other algorithms U-PUCRL2 and U-PUCRLB for extended uncertainty in the environment in which the period is unknown but a set of candidate periods are known. Numerical results demonstrate the efficacy of all the algorithms.},
  archive      = {J_TAI},
  author       = {Ayush Aniket and Arpan Chattopadhyay},
  doi          = {10.1109/TAI.2024.3375258},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3624-3637},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Online reinforcement learning in periodic MDP},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variance-reduced deep actor–critic with an optimally
subsampled actor recursion. <em>TAI</em>, <em>5</em>(7), 3607–3623. (<a
href="https://doi.org/10.1109/TAI.2024.3379109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) algorithms combined with deep learning architectures have achieved tremendous success in many practical applications. However, the policies obtained by many deep reinforcement learning (DRL) algorithms are seen to suffer from high variance making them less useful in safety-critical applications. In general, it is desirable to have algorithms that give a low iterate-variance while providing a high long-term reward. In this work, we consider the actor–critic (AC) paradigm, where the critic is responsible for evaluating the policy while the feedback from the critic is used by the actor for updating the policy. The updates of both the critic and the actor in the standard AC procedure are run concurrently until convergence. It has been previously observed that updating the actor once after every $L&amp;gt;1$ steps of the critic reduces the iterate variance. In this article, we address the question of what optimal $L$ -value to use in the recursions and propose a data-driven $L$ -update rule that runs concurrently with the AC algorithm with the objective being to minimize the variance of the infinite horizon discounted reward. This update is based on a random search (discrete) parameter optimization procedure that incorporates smoothed functional (SF) estimates. We prove the convergence of our proposed multitimescale scheme to the optimal $L$ and optimal policy pair. Subsequently, through numerical evaluations on benchmark RL tasks, we demonstrate the advantages of our proposed algorithm over multiple state-of-the-art algorithms in the literature.},
  archive      = {J_TAI},
  author       = {Lakshmi Mandal and Raghuram Bharadwaj Diddigi and Shalabh Bhatnagar},
  doi          = {10.1109/TAI.2024.3379109},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3607-3623},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Variance-reduced deep Actor–Critic with an optimally subsampled actor recursion},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous dual-dynamic attention network for modeling
mutual interplay of stocks. <em>TAI</em>, <em>5</em>(7), 3595–3606. (<a
href="https://doi.org/10.1109/TAI.2024.3374269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern quantitative finance and portfolio-based investment hinge on the dependence structure among financial instruments (like stocks) for return prediction, risk management, and hedging. Several attempts have been made to leverage recent advancements in graph neural networks (GNNs) to achieve effective modeling to capture stock interactions. However, existing work suffers from the homogeneity and stationary of the predefined graph structure, and their reliance on hard-coded correlation-based relationships neglects the mutual intervention of different instruments. In this study, we propose the heterogeneous dual-dynamic attention network (HD 2 AT) to address these challenges in modeling the mutual interplay among stocks, across four key dimensions: heterogeneity, causality, structural dynamics, and lead-lag dynamics. More specifically, our method uncovers the causal interdependencies among instruments, dynamically inferring the heterogeneous network structure from real-time trading activities. Subsequently, we identify the cross-temporal propagation of interventions by integrating interactions at the node, time, and graph levels. Extensive experiments on real-world trading data demonstrate the superiority of the proposed framework over various baselines in terms of both return prediction and back-testing profitability.},
  archive      = {J_TAI},
  author       = {Haotian Liu and Yadong Zhou and Yuxun Zhou and Bowen Hu},
  doi          = {10.1109/TAI.2024.3374269},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3595-3606},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Heterogeneous dual-dynamic attention network for modeling mutual interplay of stocks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A self-aware digital memory framework powered by artificial
intelligence. <em>TAI</em>, <em>5</em>(7), 3579–3594. (<a
href="https://doi.org/10.1109/TAI.2024.3375834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing devices in Internet-of-Things (IoT) systems are being widely used in diverse application domains including industrial automation, surveillance, and smart housing. These applications typically employ a large array of sensors, store a high volume of data, and search within the stored data for specific patterns using machine intelligence. Due to this heavy reliance on data in these applications, optimizing the memory performance in edge devices has become an important research focus. In this work, we note (based on some preliminary quantitative studies) that the memory requirements of such application-specific systems tend to differ drastically from traditional general-purpose computing systems. Inspired by these findings and also through drawing inspiration from the human brain (which excels at being highly adaptive), we design a digital memory framework that can continually adapt to the specific needs of different edge devices. This adaption is made possible through a continual reinforcement-based learning methodology, and it aims at creating a digital memory framework that is always self-aware of the data it hold and queries being made. Through a methodical implementation of the framework, we demonstrate its effectiveness for different use-cases, settings, and hyperparameters in comparison with traditional content-addressable memory.},
  archive      = {J_TAI},
  author       = {Prabuddha Chakraborty and Swarup Bhunia},
  doi          = {10.1109/TAI.2024.3375834},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3579-3594},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A self-aware digital memory framework powered by artificial intelligence},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual-enhanced high-order self-learning tensor singular value
decomposition for robust principal component analysis. <em>TAI</em>,
<em>5</em>(7), 3564–3578. (<a
href="https://doi.org/10.1109/TAI.2024.3373388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, tensor singular value decomposition (TSVD) within high-order (Ho) algebra framework has shed new light on tensor robust principal component analysis (TRPCA) problem. However, HoTSVD lacks flexibility in handling the hidden correlations along different modes of large-scale multidimensional data. Moreover, the utilization of fixed or data-independent transformations in HoTSVD may result in suboptimality. For a relief, we propose a dual-enhanced self-learning TSVD along all modes to address computational flaws and learn a lossless transformation that induces a lower average-rank tensor. Specifically, we multiply the learnable semiorthogonal matrices obtained through Tucker compression with the original tensor along all modes, thus obtaining a core tensor with more inherent low rankness. Building upon this foundation, a new TNN is introduced by generalizing HoTSVD to Mode- k TSVD, followed by the facilitation to the core tensor, achieving dual-enhancement. Moreover, a reweighting scheme is imposed on the Mode- k HoTSVD to learn the global low-rank correlation and provide an efficient numerical solution. Finally, an alternating direction method of multipliers (ADMM)-based algorithm is developed as a solver. Experimental results on several types of multidimensional visual data, including light field images (LFI) and color videos, demonstrate the superiority of the proposal over previous state-of-the-art methods.},
  archive      = {J_TAI},
  author       = {Honghui Xu and Chuangjie Fang and Renfang Wang and Shengyong Chen and Jianwei Zheng},
  doi          = {10.1109/TAI.2024.3373388},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3564-3578},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Dual-enhanced high-order self-learning tensor singular value decomposition for robust principal component analysis},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A semisupervised aircraft fuselage defect detection network
with dynamic attention and class-aware adaptive pseudolabel assignment.
<em>TAI</em>, <em>5</em>(7), 3551–3563. (<a
href="https://doi.org/10.1109/TAI.2024.3372474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To track the problem of aircraft fuselage defect detection in complex environments and reduce aviation safety hazards such as careless observation and delayed reporting due to objective factors, a semisupervised aircraft fuselage defect detection network was proposed. First, we constructed a new baseline model that extends one-stage detector with dynamic head and partial convolution named as dynamic decoupled detector, which enhances the representation capability of the model and improves the detection accuracy of small defects. Second, to address the issue of inconsistent pseudolabel distribution in semisupervised learning, we propose a class-aware adaptive pseudolabel assignment strategy that adaptively obtains the pseudolabel filtering threshold during the training iteration to further optimize the pseudolabel assignment process. Finally, to validate the effectiveness of the proposed model, we construct a dataset for aircraft fuselage defect detection for semisupervised training. Experimental results show that the proposed semisupervised aircraft fuselage defect detection network outperforms the current state-of-the-art semisupervised object detection framework on the aircraft fuselage defect dataset. At the same time, the proposed model has better generalization performance and provides more reliable support for real-time visualization of aircraft fuselage defects.},
  archive      = {J_TAI},
  author       = {Xiaoyu Zhang and Jinping Zhang and Jiusheng Chen and Runxia Guo and Jun Wu},
  doi          = {10.1109/TAI.2024.3372474},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3551-3563},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A semisupervised aircraft fuselage defect detection network with dynamic attention and class-aware adaptive pseudolabel assignment},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inferring electrocardiography from optical sensing using
lightweight neural network. <em>TAI</em>, <em>5</em>(7), 3535–3550. (<a
href="https://doi.org/10.1109/TAI.2024.3400749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a computational solution that enables continuous cardiac monitoring through cross-modality inference of electrocardiogram (ECG). While some smartwatches now allow users to obtain a 30-s ECG test by tapping a built-in bio-sensor, these short-term ECG tests often miss intermittent and asymptomatic abnormalities of cardiac functions. It is also infeasible to expect persistently active user participation for long-term continuous cardiac monitoring in order to capture these and other types of cardiac abnormalities. To alleviate the need for continuous user attention and active participation, we design a lightweight neural network that infers ECG from the photoplethysmogram (PPG) signal sensed at the skin surface by a wearable optical sensor. We also develop a diagnosis-oriented training strategy to enable the neural network to capture the pathological features of ECG, aiming to increase the utility of reconstructed ECG signals for screening cardiovascular diseases (CVDs). We also leverage model interpretation to obtain insights from data-driven models, for example, to reveal some associations between CVDs and ECG/PPG and to demonstrate how the neural network copes with motion artifacts in the ambulatory application. The experimental results on three datasets demonstrate the feasibility of inferring ECG from PPG, achieving a high fidelity of ECG reconstruction with only about 40 000 parameters.},
  archive      = {J_TAI},
  author       = {Yuenan Li and Xin Tian and Qiang Zhu and Min Wu},
  doi          = {10.1109/TAI.2024.3400749},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3535-3550},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Inferring electrocardiography from optical sensing using lightweight neural network},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal knowledge sharing enable spiking neural network
learning from past and future. <em>TAI</em>, <em>5</em>(7), 3524–3534.
(<a href="https://doi.org/10.1109/TAI.2024.3374268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) have attracted significant attention from researchers across various domains due to their brain-inspired information processing mechanism. However, SNNs typically grapple with challenges such as extended time steps, low temporal information utilization, and the requirement for consistent time step between testing and training. These challenges render SNNs with high latency. Moreover, the constraint on time steps necessitates the retraining of the model for new deployments, reducing adaptability. To address these issues, this article proposed a novel perspective, viewing the SNN as a temporal aggregation model. We introduced the temporal knowledge sharing (TKS) method, facilitating information interact between different time points. TKS can be perceived as a form of temporal self-distillation. To validate the efficacy of TKS in information processing, we tested it on static datasets such as CIFAR10, CIFAR100, ImageNet-1k, and neuromorphic datasets such as DVS-CIFAR10 and NCALTECH101. Experimental results demonstrated that our method achieves state-of-the-art performance compared to other algorithms. Furthermore, TKS addresses the temporal consistency challenge, endowing the model with superior temporal generalization capabilities. This allows the network to train with longer time steps and maintain high performance during testing with shorter time steps. Such an approach considerably accelerates the deployment of SNNs on edge devices. Finally, we conducted ablation experiments and tested TKS on fine-grained tasks, with results showcasing TKS&#39;s enhanced capability to process information efficiently.},
  archive      = {J_TAI},
  author       = {Yiting Dong and Dongcheng Zhao and Yi Zeng},
  doi          = {10.1109/TAI.2024.3374268},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3524-3534},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Temporal knowledge sharing enable spiking neural network learning from past and future},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Context aware automatic polyp segmentation network with mask
attention. <em>TAI</em>, <em>5</em>(7), 3510–3523. (<a
href="https://doi.org/10.1109/TAI.2024.3375832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colorectal cancer stands out as a major factor in cancer-related fatalities. The prevention of colorectal cancer may be aided by early polyp diagnosis. Colonoscopy is a widely used procedure for the diagnosis of polyps, but it is highly dependent on the skills of the medical practitioner. Automatic polyp segmentation using computer-aided diagnosis can help medical practitioners detect even those polyps missed by humans, and this early detection of polyps can save precious human lives. Due to the lack of distinct edges, poor contrast between the foreground and background, and great variety of polyps, automatic segmentation of polyps is quite difficult. Although there are several deep learning-based strategies for segmenting polyps, typical convolutional neural network (CNN)-based algorithms lack long-range dependencies and lose spatial information because of consecutive convolution and pooling. In this research, a novel encoder–decoder-based segmentation architecture has been proposed in an effort to identify distinguishing features that can be used to precisely separate the polyps. The proposed architecture combines the strengths of a pretrained ResNet50 encoder, residual block, our proposed multiscale dilation block, and the mask attention block. Multiscale dilation block enables us to extract features at different scales for better feature representation. The mask attention block utilizes a generated auxiliary mask in order to concentrate on important image features. To evaluate the proposed architecture, several polyp segmentation datasets have been used. The obtained findings show that the suggested architecture performs better than several state-of-the-art (SOTA) approaches for segmenting the polyps.},
  archive      = {J_TAI},
  author       = {Praveer Saxena and Ashish Kumar Bhandari},
  doi          = {10.1109/TAI.2024.3375832},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3510-3523},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Context aware automatic polyp segmentation network with mask attention},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A subspace projective clustering approach for backdoor
attack detection and mitigation in deep neural networks. <em>TAI</em>,
<em>5</em>(7), 3497–3509. (<a
href="https://doi.org/10.1109/TAI.2024.3373720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backdoor attacks in deep neural networks (DNNs) involve an attacker inserting a backdoor into the network by manipulating the training dataset, which causes misclassification of inputs that contain a specific trigger. Detecting and mitigating such attacks are challenging, as only the attacker knows the trigger and target class. Our study demonstrates that the representations, i.e., the neuron activations for a given DNN, of poisoned and genuine data lie in different subspaces, which implies that there exists a certain subspace where the difference of projections from different data can be manifested. To this end, we propose a method based on subspace projective clustering (SPC), which learns a subspace as well as a projection-based weight vector by solving a projection maximization program, and the optimized weight vector can be utilized in a clustering framework to infer the group of data. Based on our theoretical analysis and experimental results, we demonstrate the effectiveness of our method in defending against backdoor attacks that use different settings of poisoned samples on GTSRB, Imagenet, VGGFace2, and PubFig datasets in comparison with the state-of-the-art methods. Our algorithm can detect more than 90% of the infected classes and identify 95% of the poisoned samples.},
  archive      = {J_TAI},
  author       = {Yue Wang and Wenqing Li and Esha Sarkar and Muhammad Shafique and Michail Maniatakos and Saif Eddin Jabari},
  doi          = {10.1109/TAI.2024.3373720},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3497-3509},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A subspace projective clustering approach for backdoor attack detection and mitigation in deep neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hardware-aware network for real-world single image
super-resolutions. <em>TAI</em>, <em>5</em>(7), 3482–3496. (<a
href="https://doi.org/10.1109/TAI.2024.3368372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most single image super-resolution (SISR) methods are developed on synthetic low-resolution (LR) and high-resolution (HR) image pairs, which are simulated by a predetermined degradation operation, such as bicubic downsampling. However, these methods only learn the inverse process of the predetermined operation, which fails to super resolve the real-world LR images, whose true formulation deviates from the predetermined operation. To address this, we propose a novel super-resolution (SR) framework named hardware-aware super-resolution (HASR) network that first extracts hardware information, particularly the camera degradation information. The LR images are then super resolved by integrating the extracted information. To evaluate the performance of HASR network, we build a dataset named Real-Micron from real-world micron-scale patterns. The paired LR and HR images are captured by changing the objectives and registered using a developed registration algorithm. Transfer learning is implemented during the training of Real-Micron dataset due to the lack of amount of data. Experiments demonstrate that by integrating the degradation information, our proposed network achieves state-of-the-art performance for the blind SR task on both synthetic and real-world datasets.},
  archive      = {J_TAI},
  author       = {Rui Ma and Xian Du},
  doi          = {10.1109/TAI.2024.3368372},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3482-3496},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A hardware-aware network for real-world single image super-resolutions},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Activation control of vision models for sustainable AI
systems. <em>TAI</em>, <em>5</em>(7), 3470–3481. (<a
href="https://doi.org/10.1109/TAI.2024.3372935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As artificial intelligence (AI) systems become more complex and widespread, they require significant computational power, increasing energy consumption. Addressing this challenge is essential for ensuring the long-term sustainability of AI technology. AI-on-AI control refers to a system with a set of AI functions controlled by an upper-level AI model. Previous work in AI-on-AI control focuses on boosting accuracy or expanding system capability by increasing overall system cost. Alternatively, we focus on applying AI-on-AI control to decrease system cost and increase the sustainability and viability of a system with multiple AI functions. Our supervised image classification evaluative controller (SICEC) is a cost-reduction oriented AI-on-AI controller that learns when vision models within an AI system should be activated based on input features. The function controller (FC) preprocesses an input and activates relevant functions, functions being distinct units of AI functionality within the system. Some functions have a set of same functional models (SFMs). These models take the same input and produce the same output but have architectural differences. We introduce a same functional controller to select a SFM using the FC&#39;s decision confidence. Results are promising, with a decrease of up to 48.9% in inference time, 67.8% in floating point operations (FLOPs), and 66.4% in energy usage. With SICEC showing significant reductions in inference time and energy cost, our work contributes to limited resource computing and sustainable AI technology.},
  archive      = {J_TAI},
  author       = {Jonathan Burton-Barr and Basura Fernando and Deepu Rajan},
  doi          = {10.1109/TAI.2024.3372935},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3470-3481},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Activation control of vision models for sustainable AI systems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual encoder–decoder shifted window-based transformer
network for polyp segmentation with self-learning approach.
<em>TAI</em>, <em>5</em>(7), 3456–3469. (<a
href="https://doi.org/10.1109/TAI.2024.3366146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to WHO reports, cancer is the leading cause of death worldwide. The second most prevalent cause of cancer-related death in both men and women is colorectal cancer (CRC). One potential approach for reducing the severity of colon cancer is to utilize automatic segmentation and detection of colorectal polyps in colonoscopy videos. This technology can assist endoscopists in quickly identifying colorectal disease, leading to earlier intervention and better patient Quality of Life (QoL). In this article, we propose a self-supervised transformer based dual encoder–decoder architecture named P-SwinNet for polyps segmentation in colonoscopy images. The P-SwinNet adapts the dual encoder–decoder type of model to enhance the feature maps by sharing multiscale information from the encoder to the decoder. The proposed model uses multiple dilated convolutions to enlarge the field of view to gather more information without increasing the computational cost and the loss of spatial information. We also leverage a large-scale unlabeled dataset for training our model using the self-learning strategy of Barlow twins. Additionally, to capture the long-range dependencies in the data, we used a shift window-based approach that computes global attention. We extensively evaluate our model against state-of-the-art algorithms. The quantitative results show that the proposed P-SwinNet achieves a mean dice score of 0.87 and a mean Intersection over Union (IoU) of 0.82 on five datasets used in our study. This performance demonstrates a substantial advancement over existing similar works, highlighting the advantage and novelty of our proposed approach in the field of medical image segmentation.},
  archive      = {J_TAI},
  author       = {Lijin P. and Mohib Ullah and Anuja Vats and Faouzi Alaya Cheikh and Santhosh Kumar G. and Madhu S. Nair},
  doi          = {10.1109/TAI.2024.3366146},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3456-3469},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Dual Encoder–Decoder shifted window-based transformer network for polyp segmentation with self-learning approach},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Long-term prediction on graph data with causal network
construction. <em>TAI</em>, <em>5</em>(7), 3445–3455. (<a
href="https://doi.org/10.1109/TAI.2024.3351105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph data encode unobservable complex spatiotemporal information of complex systems, which brings a great challenge for accurate and stable long-term prediction. Therefore, long-term prediction of such networked flow data has always been one of the bottlenecks of modern complex systems. In previous studies, most researchers only paid attention to short-term prediction of graph data. The accuracy of these models deteriorates rapidly when they are applied in long-term prediction. In this study, a causation-based spatiotemporal feature extraction method with a novel deep learning framework named pyramid spatiotemporal network (PSTN) is proposed for long-term networked flow prediction tasks, which can achieve stable long-term spatiotemporal feature extraction with the constructed causal network, and the multiple temporal extraction mechanism. PSTN has achieved prior performance in the tested PeMSD7(M) and PeMS-Bay datasets, where the long-term prediction accuracy of PSTN is highly better than other widely used baseline prediction models, which may provide some insight into spatiotemporal prediction researches.},
  archive      = {J_TAI},
  author       = {Xiaolu Liu and Qi Shao and Duxin Chen},
  doi          = {10.1109/TAI.2024.3351105},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3445-3455},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Long-term prediction on graph data with causal network construction},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semisupervised transfer boosting (SS-TrBoosting).
<em>TAI</em>, <em>5</em>(7), 3431–3444. (<a
href="https://doi.org/10.1109/TAI.2024.3350543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semisupervised domain adaptation (SSDA) aims at training a high-performance model for a target domain using few labeled target data, many unlabeled target data, and plenty of auxiliary data from a source domain. Previous works in SSDA mainly focused on learning transferable representations across domains. However, it is difficult to find a feature space where the source and target domains share the same conditional probability distribution. Additionally, there is no flexible and effective strategy extending existing unsupervised domain adaptation (UDA) approaches to SSDA settings. In order to solve the above two challenges, we propose a novel fine-tuning framework, semisupervised transfer boosting (SS-TrBoosting). Given a well-trained deep learning-based UDA or SSDA model, we use it as the initial model, generate additional base learners by boosting, and then use all of them as an ensemble. More specifically, half of the base learners are generated by supervised domain adaptation, and half by semisupervised learning. Furthermore, for more efficient data transmission and better data privacy protection, we propose a source data generation approach to extend SS-TrBoosting to semisupervised source-free domain adaptation (SS-SFDA). Extensive experiments showed that SS-TrBoosting can be applied to a variety of existing UDA, SSDA, and SFDA approaches to further improve their performance.},
  archive      = {J_TAI},
  author       = {Lingfei Deng and Changming Zhao and Zhenbang Du and Kun Xia and Dongrui Wu},
  doi          = {10.1109/TAI.2024.3350543},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3431-3444},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Semisupervised transfer boosting (SS-TrBoosting)},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-based dual-loop learning control of underactuated
systems with disturbance prediction and input–output constraints.
<em>TAI</em>, <em>5</em>(7), 3419–3430. (<a
href="https://doi.org/10.1109/TAI.2024.3355870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practice, many mechanical systems have fewer actuators than degrees of freedom (DOFs), such as transportation robots, aerial vehicles, and flexible structures. These systems are underactuated with high flexibility and low energy consumption. However, unexpected input/output constraints, unmeasurable nonlinear dynamics/disturbances, and complicated gain selections bring about more challenges in real operations. To this end, this article presents a dual-loop learning control framework for a class of multiinput–multioutput (MIMO) underactuated systems. The model-independent inner-loop controller accelerates error convergence and is derived from Lyapunov-based stability analysis. Moreover, the inner-loop controller and the underactuated system are integrated into an optimal reference model by a data-based learning method. The parameters and control gains are optimized online . The outer-loop prediction controller directly adapts the optimized reference model as a prediction model. Also, the reference trajectories and disturbance estimates are generated and transmitted to the inner-loop structure. Hence, the real-time performance of the proposed controller is not affected by model accuracy. As far as we know, this article designs the first controller for MIMO underactuated systems to simultaneously restrict actuated/unactuated motions and actual inputs, predict unknown disturbances, and optimize control gains. The closed-loop stability is theoretically guaranteed. Some hardware experiments provide performance verification.},
  archive      = {J_TAI},
  author       = {Tong Yang and Ning Sun and Meng Zhai and Yongchun Fang and Qingxiang Wu},
  doi          = {10.1109/TAI.2024.3355870},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3419-3430},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Data-based dual-loop learning control of underactuated systems with disturbance prediction and Input–Output constraints},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Soft attention mechanism based network to extract blood
vessels from retinal image modality. <em>TAI</em>, <em>5</em>(7),
3408–3418. (<a href="https://doi.org/10.1109/TAI.2024.3351589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of ophthalmology, most of the ocular diseases are associated with vessel abnormalities. Hence, vessel segmentation plays an important role to diagnose ocular disease accurately. Deep learning (DL) techniques are grabbing attention these days due to their accurate analysis and extraction of retinal vessels. However, the vessels with low contrast, thin vessels, and vessels with improper boundaries are still overlooked. To cover this gap, we have introduced an attention mechanism based DL network that includes encoder and decoder blocks. Basically, the architecture is an extension of U-net model in which soft attention blocks are introduced in between the down-sampling and up-sampling phases of the network. The attention blocks learn to focus on the target structures and, therefore, the relevant information gets effectively extracted. In our proposed approach, for the preprocessing of the image, we first extracted the green channel (GE) and then applied contrast limited adaptive histogram equalization (CLAHE) for contrast enhancement. The preprocessed image was then fed as the input to the model for vessel segmentation. For the quantitative analysis of our model, we have used four benchmark datasets [digital retinal image for vessel extraction (DRIVE), structural analysis of retina (STARE), child heart and health study in England (CHASE), and high resolution fundus (HRF)]. A comparative analysis is also done with the state-of-the-art techniques using several performance parameters. The method proposed in this study is found to be more efficient in extracting retinal vessels. The average accuracy achieved by our method is 98%.},
  archive      = {J_TAI},
  author       = {Preity and Ashish Kumar Bhandari and Syed Shahnawazuddin},
  doi          = {10.1109/TAI.2024.3351589},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3408-3418},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Soft attention mechanism based network to extract blood vessels from retinal image modality},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple resolutions detail enhancement network for
real-time image semantic segmentation. <em>TAI</em>, <em>5</em>(7),
3393–3407. (<a href="https://doi.org/10.1109/TAI.2024.3355354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time image semantic segmentation (ISS) draws the attentions of more and more researchers as a basis of scene understanding, and it has been applied in many fields that need fast interaction and response, such as autonomous driving and robot control. Considering the loss of low-level spatial information with the deepening network layer, we propose a multiple resolutions detail enhancement network (MRDENet) in this article, which adequately extracts and utilizes accurate low-level detail information from original images with different resolutions. MRDENet consists of three light-weight branch subnetworks, and designs dense oblique connections between adjacent branches to preserve the different level effective features of previous branch. Furthermore, a new multilevel information aggregation (MIA) module is presented to effectively fuse the low-level detail features and the high-level semantic features of different branches by employing group convolution (group conv) and channel shuffle with low computation cost, thus ensuring that MRDENet could achieve a favorable tradeoff between segmentation precision with inference speed. The experimental results show that MRDENet achieves 73.1% mean intersection over union (mIoU) with 93 frames per second (FPS) on Cityscapes dataset, and 68.5% mIoU with 112 FPS on CamVid dataset, which indicates the performance of MRDENet is competitive with the state-of-art methods.},
  archive      = {J_TAI},
  author       = {Jing Gu and Xinkai Sun and Jie Feng and Shuyuan Yang and Fang Liu and Licheng Jiao},
  doi          = {10.1109/TAI.2024.3355354},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3393-3407},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multiple resolutions detail enhancement network for real-time image semantic segmentation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring the predictive capabilities of AlphaFold using
adversarial protein sequences. <em>TAI</em>, <em>5</em>(7), 3384–3392.
(<a href="https://doi.org/10.1109/TAI.2024.3353708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Protein folding neural networks (PFNNs) such as AlphaFold predict remarkably accurate structures of proteins compared to other approaches. However, the robustness of such networks has heretofore not been fully explored. This is particularly relevant given the broad social implications of such technologies and the fact that biologically small perturbations to noncritical residues of a protein sequence do not typically lead to drastic changes in the protein structure. Our study demonstrates that, similar to adversarial methods in machine learning, small changes to protein sequences can result in significant differences in the predicted protein structures using AlphaFold as determined by large distance measures. Despite this, our findings using multiple protein sequences suggest that AlphaFold is able to accurately predict the domain structure and folding regions of a protein. To gauge structural differences, we employ two alignment-based measures [root-mean-square deviation (RMSD) and the global distance test (GDT) similarity], and one alignment-free measure, which is an effective graph-based structure representation (GraSR) method. We prove that the problem of minimally perturbing protein sequences is NP-complete. Based on the well-established block substitution matrices (BLOSUM62) sequence alignment scoring matrix, we generate adversarial sequences. In our experimental evaluation, we consider 111 proteins (including 29 COVID-19 sequences) in the Universal Protein resource (UniProt), a central resource for protein data. Our findings suggest that, despite the high RMSD values returned by AlphaFold, it is capable of handling the BLOSUM adversarial sequences considered in our analysis, as evidenced by the preservation of the folded regions and the GraSR results.},
  archive      = {J_TAI},
  author       = {Ismail R. Alkhouri and Sumit Jha and Andre Beckus and George Atia and Susmit Jha and Rickard Ewetz and Alvaro Velasquez},
  doi          = {10.1109/TAI.2024.3353708},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3384-3392},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Exploring the predictive capabilities of AlphaFold using adversarial protein sequences},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learn from safe experience: Safe reinforcement learning for
task automation of surgical robot. <em>TAI</em>, <em>5</em>(7),
3374–3383. (<a href="https://doi.org/10.1109/TAI.2024.3351797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surgical task automation in robotics can improve the outcomes, reduce quality-of-care variance among surgeons and relieve surgeons’ fatigue. Reinforcement learning (RL) methods have shown considerable performance in robot autonomous control in complex environments. However, the existing RL algorithms for surgical robots do not consider any safety requirements, which is unacceptable in automating surgical tasks. In this work, we propose an approach called safe experience reshaping (SER) that can be integrated into any offline RL algorithm. First, the method identifies and learns the geometry of constraints. Second, a safe experience is obtained by projecting an unsafe action to the tangent space of the learned geometry, which means that the action is in the safe space. Then, the collected safe experiences are used for safe policy training. We designed three tasks that closely resemble real surgical tasks including 2-D cutting tasks and a contact-rich debridement task in 3-D space to evaluate the safe RL framework. We compare our framework to five state-of-the-art (SOTA) RL methods including reward penalty and primal-dual methods. Results show that our framework gets a lower rate of constraint violations and better performance in task success, especially with a higher convergence speed.},
  archive      = {J_TAI},
  author       = {Ke Fan and Ziyang Chen and Giancarlo Ferrigno and Elena De Momi},
  doi          = {10.1109/TAI.2024.3351797},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3374-3383},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Learn from safe experience: Safe reinforcement learning for task automation of surgical robot},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep model intellectual property protection with
compression-resistant model watermarking. <em>TAI</em>, <em>5</em>(7),
3362–3373. (<a href="https://doi.org/10.1109/TAI.2024.3351116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) is considered a promising technology for empowering the industrial Internet of Things (IIoT) with intelligence. However, the application of DL in the industrial IoT is accompanied by significant security challenges. Therefore, it has become crucial to investigate effective measures to provide secure DL services in IoT applications. In particular, the issue of intellectual property rights (IPR) protection is of great concern due to the illegal copying, redistribution, or misuse of deep neural network (DNN) models, which is one of the common ways that attackers target DNNs. However, existing defense mechanisms are easily detectable by attackers, rendering them ineffective. To address this issue, this article presents a novel neural network model intellectual property protection scheme, called compression-resistant model watermarking (CRMW), which employs an image steganography algorithm and an image compression algorithm to generate a watermark dataset, which is subsequently embedded into the neural network model using feature consistency training. Compared with prior efforts, CRMW offers the advantage of being resistant to image compression and maintaining invisibility. The effectiveness of CRMW in providing secure DL services for IIoT has been validated through numerous experimental analyses.},
  archive      = {J_TAI},
  author       = {Hewang Nie and Songfeng Lu and Junjun Wu and Jianxin Zhu},
  doi          = {10.1109/TAI.2024.3351116},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3362-3373},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep model intellectual property protection with compression-resistant model watermarking},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Training value-aligned reinforcement learning agents using a
normative prior. <em>TAI</em>, <em>5</em>(7), 3350–3361. (<a
href="https://doi.org/10.1109/TAI.2024.3363122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Value alignment is a property of intelligent agents wherein they solely pursue non-harmful behaviors or human-beneficial goals. We introduce an approach to value-aligned reinforcement learning (RL), in which we train an agent with two reward signals: a standard task performance reward plus a normative behavior reward. The normative behavior reward is derived from a value-aligned prior model that we train using naturally occurring stories. These stories encode societal norms and can be used to classify text as normative or nonnormative. We show how variations on a policy shaping technique can balance these two sources of reward and produce policies that are both effective and perceived as more normative. We test our value-alignment technique on three interactive text-based worlds; each world is designed specifically to challenge agents with a task as well as provide opportunities to deviate from the task to engage in normative and/or altruistic behavior.},
  archive      = {J_TAI},
  author       = {Md Sultan al Nahian and Spencer Frazier and Mark Riedl and Brent Harrison},
  doi          = {10.1109/TAI.2024.3363122},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3350-3361},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Training value-aligned reinforcement learning agents using a normative prior},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vision intelligence assisted lung function estimation based
on transformer encoder–decoder network with invertible modeling.
<em>TAI</em>, <em>5</em>(7), 3336–3349. (<a
href="https://doi.org/10.1109/TAI.2023.3348428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung function evaluation is important to many medical applications, but conducting pulmonary function tests is constrained by different conditions. This article presents a pioneer study of an integrated invertible deep learning method for lung function estimation via using computed tomography (CT) images. First, the projection method is proposed to flatten the three-dimensional (3-D) image onto a two-dimensional (2-D) plane, with preserving location information in 3-D. Next, the MBConv transformer-based encoder–decoder structure is developed to extract latent features. Finally, we develop an invertible normalizing flow (NF) model to infer lung function based on the extracted features and design two loss functions for two directions. The method enables both estimating the lung function based on CT images and metadata as well as generating the corresponding simulated CT image according to the lung function. Computational studies show that the proposed regression model outperforms all state-of-the-art image regression models. A comprehensive comparative analysis also demonstrates the effectiveness of using generated images and confirms the superiority of the proposed method. To the best of our knowledge, this work is the first of its kind in combining encoder–decoder network with NFs to ensure the effectiveness of the fully invertible framework, especially in lung CT image analysis.},
  archive      = {J_TAI},
  author       = {Liuyin Chen and Di Lu and Jianxue Zhai and Kaican Cai and Long Wang and Zijun Zhang},
  doi          = {10.1109/TAI.2023.3348428},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3336-3349},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Vision intelligence assisted lung function estimation based on transformer Encoder–Decoder network with invertible modeling},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Composite learning adaptive intelligent self-triggered
fault-tolerant control with improved performance assurance for
autonomous surface vehicle. <em>TAI</em>, <em>5</em>(7), 3325–3335. (<a
href="https://doi.org/10.1109/TAI.2024.3353150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the trajectory tracking control issue of the autonomous surface vehicle (ASV) subject to unknown actuator failures, a composite learning adaptive intelligent self-triggered fault-tolerant control (FTC) design with improved performance assurance is proposed in this article. Initially, an enhanced fixed-time performance function is introduced to construct an expected tight feasible area such that the fixed-time convergence of the tracking errors can be achieved without satisfying the specific form of fixed-time stability. Then, with benefits from the outstanding fuzzy modeling and detail analysis capabilities of fuzzy wavelet neural networks (FWNNs), a nonlinear disturbance observer-based composite neural learning strategy is proposed for handling the unknown dynamics and compound disturbance, which provides a practicable method to improve approximate precision and robustness against the unknown disturbances. Furthermore, by constructing the self-triggered mechanism and fault-tolerant mechanism, an adaptive fault-tolerant trajectory tracking controller with the self-triggered feature is developed, which ensures that entire signals in the closed-loop system (CLS) are semiglobally uniformly ultimately bounded, and the tracking errors can converge to a predefined neighborhood of the zero satisfying a prespecified tracking accuracy even if actuator failures occur suddenly. Finally, the validity and superiority of the developed approach are verified through simulation results.},
  archive      = {J_TAI},
  author       = {Xiaona Song and Chenglin Wu and Shuai Song},
  doi          = {10.1109/TAI.2024.3353150},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3325-3335},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Composite learning adaptive intelligent self-triggered fault-tolerant control with improved performance assurance for autonomous surface vehicle},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exemplar-based continual learning via contrastive learning.
<em>TAI</em>, <em>5</em>(7), 3313–3324. (<a
href="https://doi.org/10.1109/TAI.2024.3355879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the impressive performance of deep learning models, they suffer from catastrophic forgetting, which refers to a significant decline in overall performance when trained with new classes added incrementally. The primary reason for this phenomenon is the overlapping or confusion between the feature space representations of old and new classes. In this study, we examine this issue and propose a model that can mitigate the problem by learning more transferable features. We employ contrastive learning, a recent breakthrough in deep learning, which can learn visual representations better than the task-specific supervision method. Specifically, we introduce an exemplar-based continual learning (CL) method using contrastive learning to learn a task-agnostic and continuously improved feature expression. However, the class imbalance between old and new samples in CL can affect the final learned features. To address this issue, we propose two approaches. First, we use a novel exemplar-based method, called determinantal point processes experience replay (DPPER), to improve buffer diversity during memory update. Second, we propose an old sample compensation weight (CW) to resist the corruption of the old model caused by new task learning during memory retrieval. Our experimental results on benchmark datasets demonstrate that our approach outperforms state-of-the-art methods in terms of comparable performance.},
  archive      = {J_TAI},
  author       = {Shuai Chen and Mingyi Zhang and Junge Zhang and Kaiqi Huang},
  doi          = {10.1109/TAI.2024.3355879},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3313-3324},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Exemplar-based continual learning via contrastive learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event-triggered distributed intelligent learning control of
six-rotor UAVs under FDI attacks. <em>TAI</em>, <em>5</em>(7),
3299–3312. (<a href="https://doi.org/10.1109/TAI.2024.3351095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the six-rotor unmanned aerial vehicles (UAVs) subjected to false data injection (FDI) attacks, an event-triggered-based distributed intelligent learning control strategy is proposed in this article. The reinforcement learning algorithm which is designed based on neural networks (NNs) is introduced to achieve intelligent optimal control. Under the actor–critic–identifier frame, three kinds of NNs are used to realize the control action, evaluate the system performance, and estimate the unknown dynamic, respectively. Then, an improved event-triggered strategy including a decreasing function of the consensus error is designed to reduce the waste of resources while decreasing the adverse impact on tracking control performance as far as possible, which also does not exhibit Zeno behavior. Furthermore, an adaptive compensation control scheme is given, which can effectively compensate for the negative impacts brought by the FDI attacks existing in information interaction among UAVs. Despite the impact of lumped disturbances and FDI attacks, the proposed disturbance-observer-based distributed intelligent learning control strategy can still guarantee that all signals in the closed-loop UAV systems are bounded, and all follower UAVs can track the signal of the leader UAV. Finally, some simulation verification diagrams are shown to test the feasibility of the designed control strategy.},
  archive      = {J_TAI},
  author       = {Ying Wu and Mou Chen and Hongyi Li and Mohammed Chadli},
  doi          = {10.1109/TAI.2024.3351095},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {3299-3312},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Event-triggered distributed intelligent learning control of six-rotor UAVs under FDI attacks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fuzzy neural network-based adaptive asymmetric constraint
control in wastewater treatment process. <em>TAI</em>, <em>5</em>(6),
3284–3296. (<a href="https://doi.org/10.1109/TAI.2023.3347182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wastewater treatment process (WWTP) is an important mean to prevent water pollution and improve ecological environment. Dissolved oxygen (DO) and nitrate nitrogen (NO ${}_{3}$ -N) concentrations are the main indicators to affect the effluent quality (EQ). In order to achieve high accuracy control, the artificial intelligence based asymmetric constraint control method with actuator saturation processing technology is proposed in WWTP. First, the fuzzy neural network (FNN) model is used to estimate the unknown situations, in which the maximum correlation entropy criterion is introduced into the adjustment of model structure to deal with the dynamic changes of WWTP. Second, the unified actuator saturation processing model is established to achieve the stable tracking performance. Third, the asymmetric barrier Lyapunov function (BLF) is introduced into the controller design. Not only the DO and NO ${}_{3}$ -N concentrations, but also the tracking error are kept within the asymmetric constraint range to guarantee the control performance. Finally, the effectiveness of the proposed method is verified via the dynamical and constant set-points simulation experiments of benchmark simulation model 1 (BSM1).},
  archive      = {J_TAI},
  author       = {Dingyuan Chen and Cuili Yang and Junfei Qiao},
  doi          = {10.1109/TAI.2023.3347182},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3284-3296},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Fuzzy neural network-based adaptive asymmetric constraint control in wastewater treatment process},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Empowering semantic segmentation with selective frequency
enhancement and attention mechanism for tampering detection.
<em>TAI</em>, <em>5</em>(6), 3270–3283. (<a
href="https://doi.org/10.1109/TAI.2023.3347178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, massive amounts of multimedia contents are exchanged in our daily life, while tampered images are also flooding the social networks. Tampering detection is therefore becoming increasingly important for multimedia integrity, and it is generally realized by designing specific convolutional neural networks. From a new perspective, this article proposes two pluggable modules for empowering existing semantic segmentation models for tampering detection. First, a selective frequency enhancement (SFE) module is developed to suppress the semantic information and selectively enhance the tamper information. Second, a boundary enhanced attention (BEA) module is designed to highlight the edge information of tempered area. Our SFE and BEA modules are combined with five mainstream semantic segmentation networks for performance evaluation. The experiment results demonstrate that our modules are able to empower the semantic segmentation networks for tampering detection, and their combinations even perform better than state-of-the-art algorithms in certain datasets.},
  archive      = {J_TAI},
  author       = {Xu Xu and Wenrui Lv and Wei Wang and Yushu Zhang and Junxin Chen},
  doi          = {10.1109/TAI.2023.3347178},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3270-3283},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Empowering semantic segmentation with selective frequency enhancement and attention mechanism for tampering detection},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Node-level graph regression with deep gaussian process
models. <em>TAI</em>, <em>5</em>(6), 3257–3269. (<a
href="https://doi.org/10.1109/TAI.2023.3347177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study node-level graph regression, which aims to predict an output vector for each node on a given graph. This task has a broad range of applications, including spatiotemporal forecasting and computational biology. We propose a model called deep Gaussian processes over graphs (DGPGs), which is composed of hierarchical Gaussian processes (GPs) and learns the mapping between input–output signals in graph domains. DGPG possesses several distinctive advantages, such as the ability in capturing uncertainty, effectiveness on small datasets, and requiring fewer efforts for selecting model architectures and hyperparameters. It is also more favorable than traditional GP models in terms of expressiveness and scalability, due to the hierarchical deep structure and the variational inference framework. Moreover, we generalize DGPG to a more challenging setting where the graph structure is time-varying. Our theoretical analysis shows that the graph information can improve convergence by reducing sampling variances when optimizing the evidence lower bound, and the challenge of time-varying graph structure can be addressed by a time-weighted sampling scheme. The performance of DGPG is demonstrated through extensive experiments in various synthetic and real-world datasets. Some appealing characteristics of DGPG are further discussed, such as its ability to capture prediction uncertainty and learn graph structures.},
  archive      = {J_TAI},
  author       = {Naiqi Li and Wenjie Li and Yinghua Gao and Yiming Li and Jigang Bao and Ercan E. Kuruoğlu and Yong Jiang and Shu-Tao Xia},
  doi          = {10.1109/TAI.2023.3347177},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3257-3269},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Node-level graph regression with deep gaussian process models},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight learning for partial and occluded person
re-identification. <em>TAI</em>, <em>5</em>(6), 3245–3256. (<a
href="https://doi.org/10.1109/TAI.2023.3346333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occluded and partial person re-identification (re-ID) problems have emerged as challenging research topics in the area of computer vision. Existing part-based models, with complex designs, fail to properly tackle these problems. The reasons for their failures are two-fold. First, individual body part appearances are not discriminative enough to distinguish between two closely appearing persons. Second, re-ID datasets typically lack detailed human body part annotations. To address these challenges, we present a lightweight yet accurate solution for partial person re-ID. Our proposed approach consists of two key components, namely, design of a lightweight unary–binary projective dictionary learning (UBDL) model, and construction of a similarity matrix for distilling knowledge from the deep omni-scale network (OSNet) to UBDL. The unary dictionary (UD) pair encodes patches horizontally, ignoring the viewpoints. The binary dictionary (BD) pairs, on the other hand, are learned between two views, giving more weight to less occluded vertical patches for improving the correspondence across the views. We formulate appropriate convex objective functions for unary and binary cases by incorporating the above knowledge similarity matrix. Closed form solutions are obtained for updating unary and BD components. Final matching scores are computed by fuzing unary and binary matching scores with adaptive weighting of relevant cross-view patches. Extensive experiments and ablation studies on a number of occluded and partial re-ID datasets like occluded-REID (O-REID), partial-REID (P-REID), and partial-iLIDS (P-iLIDS), clearly showcase the merits of our proposed solution.},
  archive      = {J_TAI},
  author       = {Arindam Sikdar and Ananda S. Chowdhury},
  doi          = {10.1109/TAI.2023.3346333},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3245-3256},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Lightweight learning for partial and occluded person re-identification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving 3-d medical image segmentation at boundary regions
using local self-attention and global volume mixing. <em>TAI</em>,
<em>5</em>(6), 3233–3244. (<a
href="https://doi.org/10.1109/TAI.2023.3346833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Volumetric medical image segmentation is a fundamental problem in medical image analysis where the objective is to accurately classify a given 3-D volumetric medical image with voxel-level precision. In this work, we propose a novel hierarchical encoder–decoder-based framework that strives to explicitly capture the local and global dependencies for volumetric 3-D medical image segmentation. The proposed framework exploits local volume-based self-attention to encode the local dependencies at high resolution and introduces a novel volumetric multi-layer perceptron (MLP)-mixer to capture the global dependencies at low-resolution feature representations, respectively. The proposed volumetric MLP-mixer learns better associations among volumetric feature representations. These explicit local and global feature representations contribute to better learning of the shape-boundary characteristics of the organs. Extensive experiments on three different datasets reveal that the proposed method achieves favorable performance compared to state-of-the-art approaches. On the challenging Synapse Multiorgan dataset, the proposed method achieves an absolute 3.82% gain over the state-of-the-art approaches in terms of HD95 evaluation metrics while a similar improvement pattern is exhibited in Medical Segmentation Decathlon (MSD) Liver and Pancreas tumor datasets. We also provide a detailed comparison between recent architectural design choices in the 2-D computer vision literature by adapting them for the problem of 3-D medical image segmentation. Finally, our experiments on the ZebraFish 3-D cell membrane dataset having limited training data demonstrate the superior transfer learning capabilities of the proposed vMixer model on the challenging 3-D cell instance segmentation task, where accurate boundary prediction plays a vital role in distinguishing individual cell instances.},
  archive      = {J_TAI},
  author       = {Daniya Najiha Abdul Kareem and Mustansar Fiaz and Noa Novershtern and Jacob Hanna and Hisham Cholakkal},
  doi          = {10.1109/TAI.2023.3346833},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3233-3244},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Improving 3-D medical image segmentation at boundary regions using local self-attention and global volume mixing},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Web API recommendation via combining adaptive multichannel
graph representation and xDeepFM quality prediction. <em>TAI</em>,
<em>5</em>(6), 3218–3232. (<a
href="https://doi.org/10.1109/TAI.2023.3345828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing number of Web services, how to provide developers with Web APIs that meet their Mashup requirements accurately and efficiently has become a challenge. Even though the existing methods show improvements in service recommendation, the efficiency and accuracy (ACC) of them still need to be improved due to their limited representation in fuzing network topology and node feature of Web service, and the neglected higher-order feature interactions of Web service. To address this problem, this article proposes a Web APIs recommendation method via combining adaptive multichannel (AMC) graph representation and eXtreme deep factorization machine (xDeepFM) quality prediction. In this method, firstly, specific embedding and shared embedding in Web API node isomorphic network are extracted from the nodes’ feature space, topology space, and the combination of the two spaces. Then, attention mechanism is used to adaptively learn the importance weight of each embedding. Next, these embeddings are adaptively integrated to generate the multichannel graph representation of Web APIs for service classification. Finally, aiming at the Web APIs in the service cluster, it utilizes xDeepFM to model and mine the complex feature interactions and predict and rank the scores of Web APIs for Mashup creation. The experimental results on the real datasets of ProgrammableWeb show that compared with DeepFM, wide and deep learning (WDL), FM supported neural network (FNN), neural factorization machine (NFM), and mixed logistic regression (MLR), the method proposed in this article has an average improvement in AUC of 2.3%, 7.9%, 8.0%, 9.6%, and 13.3%.},
  archive      = {J_TAI},
  author       = {Buqing Cao and Yueying Qing and Dong Zhou and Xiang Xie and Guosheng Kang and Jianxun Liu and Kenneth K. Fletcher},
  doi          = {10.1109/TAI.2023.3345828},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3218-3232},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Web API recommendation via combining adaptive multichannel graph representation and xDeepFM quality prediction},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A neural-reinforcement-learning-based guaranteed cost
control for perturbed tracking systems. <em>TAI</em>, <em>5</em>(6),
3205–3217. (<a href="https://doi.org/10.1109/TAI.2023.3346334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI)-based learning control plays a critical role in the evolution of intelligent control, particularly for complex network systems. Traditional intelligent control methods assume the agent can learn from safe data in the tasks. However, many application scenarios exist perturbations caused by noise and/or malicious attack, which make the received data unreliable and may cause the failure of the learning process. In this article, we focus on developing an intelligent guaranteed cost control method for nonlinear tracking systems subject to unknown matched and mismatched perturbations. By developing appropriate cost functions for the nominal plants, we transform the robust tracking control problem into a stabilization design for both kinds of perturbations. The explicit proofs are provided to show the equivalence of the transformation for these two situations respectively. Then, the neural-reinforcement-learning-based algorithm with guaranteed cost control is developed to learn the cost functions and optimal control laws adaptively. The designed method can also guarantee the boundedness of a given cost function. Three simulation studies are provided to demonstrate the effectiveness of the proposed method and also validate the theoretical analysis.},
  archive      = {J_TAI},
  author       = {Xiangnan Zhong and Zhen Ni},
  doi          = {10.1109/TAI.2023.3346334},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3205-3217},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A neural-reinforcement-learning-based guaranteed cost control for perturbed tracking systems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive intelligent output feedback consensus control for
nonlinear multiagent systems with switched topologies and dead-zone.
<em>TAI</em>, <em>5</em>(6), 3195–3204. (<a
href="https://doi.org/10.1109/TAI.2023.3344388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the neural network (NN) adaptive consensus control issue for nonlinear multiagent systems (MASs) with dead-zone (DZ) output and jointly connected topologies. NNs are utilized to identify unknown agents, and a state observer is established to handle the problem resulted from unmeasurable states. A smooth inverse model is designed to replace nonsmooth DZ such that the nonsmooth problem of the system output is avoided. To estimate the unknown leader and its high-order derivatives under jointly connected topologies, a distributed observer is constructed. By using backstepping control technique, an output feedback consensus control scheme is established. It is demonstrated that all the variables of the controlled MASs are bounded despite of DZ output, and the followers can track the trajectory of the leader. Furthermore, multiple unmanned surface vehicles (USVs) are given to verify the feasibility of the presented consensus control methodology.},
  archive      = {J_TAI},
  author       = {Jun Zhang and Shaocheng Tong},
  doi          = {10.1109/TAI.2023.3344388},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3195-3204},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adaptive intelligent output feedback consensus control for nonlinear multiagent systems with switched topologies and dead-zone},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforced knowledge distillation for time series
regression. <em>TAI</em>, <em>5</em>(6), 3184–3194. (<a
href="https://doi.org/10.1109/TAI.2023.3341854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the most popular and effective methods in model compression, knowledge distillation (KD) attempts to transfer knowledge from single or multiple large-scale networks (i.e., Teachers ) to a compact network (i.e., Student ). For the multiteacher scenario, existing methods either assign equal or fixed weights for different teacher models during distillation, which can be inefficient as teachers might perform variously or even oppositely on different training samples. To address this issue, we propose a novel reinforced knowledge distillation method with negatively correlated teachers which are generated via negative correlation learning. The negatively correlated teachers would encourage teachers to learn different aspects of data and thus the ensemble of them can be more comprehensive and suitable for multiteacher KD. Subsequently, a reinforced KD algorithm is proposed to dynamically employ proper teachers for different training instances via dueling double deep Q-network (DDQN). Our proposed method complements the existing KD procedure on teacher generation and selection. Extensive experimental results on two real-world time series regression tasks clearly demonstrate that the proposed approach could achieve superior performance over state-of-the-art (SOTA) methods. The PyTorch implementation of our proposed approach is available at https://github.com/xuqing88/RL-KD-for-time-series-regression .},
  archive      = {J_TAI},
  author       = {Qing Xu and Keyu Wu and Min Wu and Kezhi Mao and Xiaoli Li and Zhenghua Chen},
  doi          = {10.1109/TAI.2023.3341854},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3184-3194},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Reinforced knowledge distillation for time series regression},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural-based predefined-time distributed optimization of
high-order nonlinear multiagent systems. <em>TAI</em>, <em>5</em>(6),
3174–3183. (<a href="https://doi.org/10.1109/TAI.2023.3343684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses a predefined-time distributed optimization problem for high-order nonlinear multiagent systems (MASs). First, by means of a distributed proportional integration (PI) protocol, a reference model is constructed to evaluate the global optimal solution for MASs. Then, the resulting measurement is fed into a prefilter to produce a reconstructed optimal reference signal and its high-order derivatives. Instead of designing the updated law with ó-modification to deal with unknown nonlinearities, a gradient descent algorithm is developed to train the weights of neural networks (NNs) to achieve higher function approximation accuracy. Moreover, in the framework of prefiltering, an NN-based predefined-time control strategy is built using the backstepping technique to guarantee that all agents’ outputs can reach optimal consensus in predefined time. Finally, simulation examples validate the effectiveness of the presented approach.},
  archive      = {J_TAI},
  author       = {Xiaohong Zheng and Hui Ma and Deyin Yao and Hongyi Li},
  doi          = {10.1109/TAI.2023.3343684},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3174-3183},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Neural-based predefined-time distributed optimization of high-order nonlinear multiagent systems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep learning-based joint image super-resolution and
deblurring framework. <em>TAI</em>, <em>5</em>(6), 3160–3173. (<a
href="https://doi.org/10.1109/TAI.2023.3343319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep 4 learning (DL)-based single image super-resolution (SISR) for low-resolution (LR) images typically aims to recover a high-resolution (HR) image from its LR version due to downsampling and blurring imperfections of the imaging sensor. The existing DL SR networks reasonably solve the downsampling problem, however, they do not address the complex deblurring problem, simultaneously. To address the later, we propose a joint dual-branch convolutional neural network (CNN) for recovering sharp HR images from LR images degraded with Gaussian blur. The proposed method has two task-independent networks: 1) super-resolution (SR) and 2) deblurring. In particular, we adopt a residual spatial and channel squeeze-and-excitation (RSCSE) module incorporating concurrent spatial and channel squeeze-and-excitation (SCSE) attention mechanism and local feature fusion (LFF) concepts in the SR network. Furthermore, the deblurring network is designed based on a SCSE-based encoder-decoder module to retrieve sharp features from blurred LR images. The feature maps obtained from these networks are adaptively fused by learning a gated module with attention mechanism to generate a clear HR. Experimental results demonstrate that the proposed method outperforms other state-of-the-art DL techniques in visual results and quantitative metrics; peak signalto-noise ratio (PSNR) improves by 1.4 dB-4.9 dB and 0.4 dB-2.6 dB for zooming factors 2 and 4, respectively, on publicly available RGB remote sensing (RS) datasets. Similarly, for multispectral (MS) datasets, they are 1.4 dB-3.5 dB and 0.2 dB-1.4 dB for zooming factors 2 and 4, respectively. It also provides promising results for land cover classification in RS applications.},
  archive      = {J_TAI},
  author       = {Trishna Barman and Bhabesh Deka},
  doi          = {10.1109/TAI.2023.3343319},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3160-3173},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A deep learning-based joint image super-resolution and deblurring framework},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3-d dynamic multitarget detection algorithm based on
cross-view feature fusion. <em>TAI</em>, <em>5</em>(6), 3146–3159. (<a
href="https://doi.org/10.1109/TAI.2023.3342104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In autonomous driving, data degradation and insufficient feature-richness in the current single-modal algorithms cannot effectively perform dynamic multitarget detection. Therefore, a 3-D dynamic multitarget detection algorithm based on cross-view feature fusion is proposed. A two-stage parallel fusion framework is proposed, which simultaneously extracts point cloud and image features in the first stage. Additionally, a Lidar-Camera feature mapping module is designed to achieve pointwised correspondence between different data. Then, a feature weighted fusion module is designed to judge the weight of each point in the point cloud feature and image feature. In the second stage, a keypoint-based feature extraction module is designed to enrich the features, which integrates the multiscale features and image features in the first stage to improve the detection accuracy. The proposed algorithm was compared with other state-of-the-art (SOTA) methods on the Kitti, Waymo, and Nuscene datasets. The result showed that the accuracy of vehicle target has reached to 93.03%. The module ablation study and accuracy detection on self-made dataset showed that the proposed algorithm not only had good robustness, strong portability, and generalization ability but also had high detection accuracy.},
  archive      = {J_TAI},
  author       = {Feng Zhou and Chongben Tao and Zhen Gao and Zufeng Zhang and Sifa Zheng and Yuan Zhu},
  doi          = {10.1109/TAI.2023.3342104},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3146-3159},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {3-D dynamic multitarget detection algorithm based on cross-view feature fusion},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel framework for improved grasping of thin and stacked
objects. <em>TAI</em>, <em>5</em>(6), 3134–3145. (<a
href="https://doi.org/10.1109/TAI.2023.3341833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a novel top–down grasping approach for robots that combines a deep high resolution convolutional neural network (DHRNet) and a multiview perception-based trajectory planning controller (MP-PC). Unlike the traditional encoder–decoder architecture, DHRNet preserves the high-resolution feature maps and integrates feature maps at different scales to ensure maximum retention of spatial information and its fusion with high-level semantic information. The MP-PC continuously adapts the trajectory planning of the robotic end-effector based on the outputs of the DHRNet in order to respond to situations with low confidence in the grasping detection. As a result, it improves the smoothness and accuracy of the trajectory and avoids entering singularities. We evaluate the performance of the DHRNet on the Cornell and Jacquard grasping datasets, achieving accuracies of $99.50$ % and $94.80$ %, respectively. In addition, the framework outperformed other methods in real-world experiments using a 7 degrees of freedom Franka Emika Panda robot, especially in scenarios with stacked and thin objects. The codes for this approach are available at https://github.com/USTCzzl/DHRNet-MP-PC/tree/master .},
  archive      = {J_TAI},
  author       = {Zhangli Zhou and Shaochen Wang and Ziyang Chen and Mingyu Cai and Zhen Kan},
  doi          = {10.1109/TAI.2023.3341833},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3134-3145},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A novel framework for improved grasping of thin and stacked objects},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large-scale parallel cognitive diagnostic test assembly
using a dual-stage differential evolution-based approach. <em>TAI</em>,
<em>5</em>(6), 3120–3133. (<a
href="https://doi.org/10.1109/TAI.2023.3341916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel testing, which uses different test forms to assess examinees, is a necessary and important technique in both educational and psychometric assessments. A key but challenging problem for successful parallel testing lies in generating a high-quality parallel test set. Most existing parallel test assembly methods were developed for classic test theory and item response theory. In the context of cognitive diagnosis models, which is a new instrument featuring the ability to assess the examinee&#39;s status on fine-grained attributes, the investigation of parallel test assembly is limited, particularly for large parallel scale. This study aims to provide an efficient dual-stage solution for the large-scale parallel cognitive diagnostic test (CDT) assembly problem. In the first stage, the assembly of individual CDTs is treated as a multimodal optimization problem and a niching differential evolution algorithm is developed to find an elite set of CDTs with near-optimal diagnostic performance. By re-designing evolutionary operators, the efficient search mechanism in differential evolution is transferred to the binary context and suits the purpose of optimizing item assignment to a CDT. In the second stage, a graph representation is defined to capture the set of elite CDTs and their overlapping relationships. A deterministic algorithm is applied to the graph to find specific nodal maximum cliques and provide two types of parallel test sets that satisfy different examiner preferences. Simulation studies under a variety of conditions and real-data demonstration show that the proposed method outperforms the existing approaches on large-scale instances while remaining competitive on small-scale cases.},
  archive      = {J_TAI},
  author       = {Xi Cao and Ying Lin and Dong Liu and Henry Been-Lirn Duh and Jun Zhang},
  doi          = {10.1109/TAI.2023.3341916},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3120-3133},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Large-scale parallel cognitive diagnostic test assembly using a dual-stage differential evolution-based approach},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A rearrangement and restore-based mixer model for
target-oriented multimodal sentiment classification. <em>TAI</em>,
<em>5</em>(6), 3109–3119. (<a
href="https://doi.org/10.1109/TAI.2023.3341879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of fine-grained multimodal sentiment analysis tasks, target-oriented multimodal sentiment (TMSC) analysis has received more attention, which aims to classify the sentiment of target with the help of textual and associated image features. Existing methods focus on exploring fine-grained image features and incorporate transformer-based complex fusion strategies, while ignoring the heavy computational burden. Recently, some lightweight multilayer perceptrons (MLP)-based methods have been successfully applied to multimodal sentiment classification tasks. In this article, we propose an effective rearrangement and restore mixer model (RR-Mixer) for TMSC, which dedicates the interaction of image, text, and targets along the modal-axis , sequential-axis , and feature channel-axis through rearrangement and restore operations. Specifically, we take vision transformer (ViT) and robustly optimized BERT (RoBERTa) pretrained models to extract image and textual features, respectively. Further, we adopt cosine similarity to select the most semantically relevant image features. Then, an RR-Mixer module is designed for mixed multimodal features, with the core technology consisting of rolling, grouping rearrangement and restore operations. Moreover, we introduce MLP Unit to learn the information of different modalities for intermodal interaction. The results show that our model achieves superior performance on two benchmark multimodal datasets, TWITTER-15 and TWITTER-17, with a significant improvement of 4.66% and 1.26% in terms of macro-F1.},
  archive      = {J_TAI},
  author       = {Li Jia and Tinghuai Ma and Huan Rong and Victor S. Sheng and Xuejian Huang and Xintong Xie},
  doi          = {10.1109/TAI.2023.3341879},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3109-3119},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A rearrangement and restore-based mixer model for target-oriented multimodal sentiment classification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rethinking DABNet: Light-weight network for real-time
semantic segmentation of road scenes. <em>TAI</em>, <em>5</em>(6),
3098–3108. (<a href="https://doi.org/10.1109/TAI.2023.3341976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in autonomous driving and mobile devices have led to the development of real-time and lightweight semantic image segmentation models. However, these algorithms readily suffer from inherent accuracy loss compared to large networks. DABNet (Li et al., 2019) presented a highly efficient method to balance the accuracy-model size tradeoff. Nevertheless, the bottleneck structure and single-scale receptive field of its building block have limited performance for the given network size. To further improve the segmentation score and reduce the number of parameters, the basic block is redesigned using an inverted-residual and dilation pyramid structure (IRDP). The IRDP module can efficiently learn contextual features at multiple dilations within the block. Using the inverted-residual structure with an expansion layer prevents information loss due to the dimensionality reduction of the feature space. The IRDP block is utilized to rebuild the DABNet structure, working in real-time for resource-constrained devices. In addition, a fast and lightweight decoder-fast-lightweight decoder (FLD) is also proposed to improve the segmentation accuracy of the network. Experiments performed on Cityscapes and Cambridge-driving Labeled Video Database (CamVid) datasets demonstrate the effectiveness of the proposed approach. On Cityscapes, IRDPNet can achieve a mean Intersection-over-Union (mIOU) of 75.62%. At the same time, the lighter version gets an mIoU of 71.32% with only 0.32 million parameters, which is similar to the DABNet accuracy with half the number of parameters.},
  archive      = {J_TAI},
  author       = {Saquib Mazhar and Nadeem Atif and M. K. Bhuyan and Shaik Rafi Ahamed},
  doi          = {10.1109/TAI.2023.3341976},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3098-3108},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Rethinking DABNet: Light-weight network for real-time semantic segmentation of road scenes},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Manipulation attacks on learned image compression.
<em>TAI</em>, <em>5</em>(6), 3083–3097. (<a
href="https://doi.org/10.1109/TAI.2023.3340982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) techniques have shown promising results in image compression compared to conventional methods, with competitive bitrate and image reconstruction quality from compressed latent. However, whereas learned image compression has progressed toward a higher peak signal-to-noise ratio (PSNR) and fewer bits per pixel (bpp), its robustness to adversarial images has never received deliberation. In this work, we investigate the robustness of image compression systems where imperceptibly manipulated inputs can stealthily precipitate a significant increase in the compressed bitrate without compromising reconstruction quality. Such attacks can potentially exhaust the storage or network bandwidth of computing systems and lead to service denial. We term it as a denial-of-service attack on image compressors. To characterize the robustness of state-of-the-art learned image compression, we mount white-box and black-box attacks. Our white-box attack employs a gradient ascent approach on the entropy estimation of the bitstream as its bitrate approximation. We propose discrete cosine transform-Net simulating joint photographic experts group (JPEG) compression with architectural simplicity and lightweight training as the substitute in the black-box attack, enabling fast adversarial transferability. Our results on six image compression architectures, each with six different bitrate qualities (thirty-six models in total), show that they are surprisingly fragile, where the white-box attack achieves up to 55 ${\boldsymbol{\times}}$ and black-box 2 $\boldsymbol{\times}$ bpp increase, respectively, revealing the devastating fragility of DL-based compression models. To improve robustness, we propose a novel compression architecture factorAtn incorporating attention modules and a basic factorized entropy model that presents a promising tradeoff between rate-distortion performance and robustness to adversarial attacks and surpasses existing learned image compressors.},
  archive      = {J_TAI},
  author       = {Kang Liu and Di Wu and Yangyu Wu and Yiru Wang and Dan Feng and Benjamin Tan and Siddharth Garg},
  doi          = {10.1109/TAI.2023.3340982},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3083-3097},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Manipulation attacks on learned image compression},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic coherence guided multiview similarity for image
classification with varied supervision. <em>TAI</em>, <em>5</em>(6),
3072–3082. (<a href="https://doi.org/10.1109/TAI.2023.3339593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pairwise similarity has been widely used for image classification by propagating the class information from labeled images to unlabeled images and predicting the classes of unlabeled images accordingly. Although widely used, pairwise similarity based classification methods have two drawbacks. On one hand, visual-semantic similarity consistency does not always hold due to semantic gap. On the other hand, the reliability of pairwise similarity is also influenced by the number and underlying distribution of labeled images. A well-designed classification model cannot be easily generalized to few-shot classification tasks. To solve these two problems, in this article, we propose a semantic coherence guided multiview similarity (SCG-MV) for image classification method with varied supervision levels. We measure the semantic coherence between image pairs for classifier learning. This is achieved by first conducting line search along the line segment between each image pair, and then measure the semantics of each interval. The similarity of one image pair can then be obtained by aggregating the semantic coherence of all the intervals. In this way, we can model image similarities more finely and discriminatively. To make use of multiview information, we also use the semantic coherence guided image similarities to learn multiview classifiers. The proposed semantic coherence guided method can be combined with other similarity based methods for image classification with various levels of supervision. We conduct both few-shot and fully supervised classification experiments on four datasets with the results well prove the discriminative power and generalization ability of the proposed method.},
  archive      = {J_TAI},
  author       = {Chunjie Zhang and Ping Cao and Huihui Bai and Xiaolong Zheng},
  doi          = {10.1109/TAI.2023.3339593},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3072-3082},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Semantic coherence guided multiview similarity for image classification with varied supervision},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On skull-closed machine thinking based on emergent turing
machines. <em>TAI</em>, <em>5</em>(6), 3057–3071. (<a
href="https://doi.org/10.1109/TAI.2023.3337322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) has made much progress, but the existing paradigm for AI is still basically pattern recognition representations, including various neural networks. This work proposes an emergent turing machine (ETM)-based machine thinking model as a new paradigm to address Alan Turing&#39;s machine thinking question. The main practical motivation is to fundamentally raise the power of generalization. This is a journal version of [1] from which the major conceptual clarification is the skull-closure, which means that the representations inside the closed-skull is totally off-limit to human teachers in the skull-external environment. The machine learning inside the skull, like an animal, is fully autonomous throughout the lifetime during which a grand emergent TM is learnt by automatically integrating many incrementally learnt sub-TMs, as a process called scaffolding in developmental psychology. The model inside the closed-skull is our proposed developmental network (DN) that has been mathematically proven to be optimal in the sense of maximum likelihood. Therefore, the postselection problem in deep learning [2] is avoided since we develop only a single network for each life. Experiments in simulated mazes are conducted. With new mechanisms (e.g., $Z$ -to- $Z$ connections that learn “Where–What” concepts) added to this journal version, the success rate of disjoint tests increases from 35% to 62.5%, showing a high generalization power of “Where–What” abstraction to new settings. Instead of real-world scenes in our prior work [3] , simulations are necessary for this work because they provide arbitrary scenes and ground truths for precise and quantitative error measurements.},
  archive      = {J_TAI},
  author       = {Xiang Wu and Zejia Zheng and Juyang Weng},
  doi          = {10.1109/TAI.2023.3337322},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3057-3071},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {On skull-closed machine thinking based on emergent turing machines},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiclass counterfactual explanations using support vector
data description. <em>TAI</em>, <em>5</em>(6), 3046–3056. (<a
href="https://doi.org/10.1109/TAI.2023.3337053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainability has become crucial in artificial intelligence studies and, as the complexity of the model increases, so does the complexity of its explanation. However, the higher the complexity of the problem, the higher the amount of information it may provide, and this information can be exploited to generate a more precise explanation of how the model works. One of the most valuable ways to recover such input–output relation is to extract counterfactual explanations that allow us to find minimal changes from an observation to another one belonging to a different class. In this article, we propose a novel methodology to extract multiple counterfactual explanations [MUltiCounterfactual via Halton sampling (MUCH)] from an original multiclass support vector data description algorithm. To evaluate the performance of the proposed method, we extracted a set of counterfactual explanations from three state-of-the-art datasets achieving satisfactory results that pave the way to a range of real-world applications.},
  archive      = {J_TAI},
  author       = {Alberto Carlevaro and Marta Lenatti and Alessia Paglialonga and Maurizio Mongelli},
  doi          = {10.1109/TAI.2023.3337053},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3046-3056},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multiclass counterfactual explanations using support vector data description},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Brightness perceiving for recursive low-light image
enhancement. <em>TAI</em>, <em>5</em>(6), 3034–3045. (<a
href="https://doi.org/10.1109/TAI.2023.3339092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the wide dynamic range in real low-light scenes, there will be large differences in the degree of contrast degradation and detail blurring of captured images, making it difficult for existing end-to-end methods to enhance low-light images to normal exposure. To address the above issue, we decompose low-light image enhancement (LLIE) into a recursive enhancement task and propose a brightness perceiving-based recursive enhancement framework for high dynamic range LLIE. Specifically, our recursive enhancement framework consists of two parallel subnetworks: adaptive contrast and texture enhancement network (ACT-Net) and brightness perception network (BP-Net). The ACT-Net is proposed to adaptively enhance image contrast and details under the guidance of the brightness adjustment branch and gradient adjustment branch, which are proposed to perceive the degradation degree of contrast and details in low-light images. To adaptively enhance images captured under different brightness levels, BP-Net is proposed to control the recursive enhancement times of ACT-Net by exploring the image brightness distribution properties. Finally, in order to coordinate ACT-Net and BP-Net, we design a novel unsupervised training strategy to facilitate the training procedure. To further validate the effectiveness of the proposed method, we construct a new dataset with a broader brightness distribution by mixing three low-light datasets. Compared with eleven existing representative methods, the proposed method achieves new state-of-the-art (SOTA) performance on six reference and no-reference metrics. Specifically, the proposed method improves the peak signal-to-noise ratio (PSNR) by 0.9 dB compared to the existing SOTA method.},
  archive      = {J_TAI},
  author       = {Haodian Wang and Long Peng and Yuejin Sun and Zengyu Wan and Yang Wang and Yang Cao},
  doi          = {10.1109/TAI.2023.3339092},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3034-3045},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Brightness perceiving for recursive low-light image enhancement},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low quality retinal blood vessel image boosting using
fuzzified clustering. <em>TAI</em>, <em>5</em>(6), 3022–3033. (<a
href="https://doi.org/10.1109/TAI.2023.3336612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinal imaging can effectively diagnose diseases that manifest changes in the retinal anatomy. However, manual diagnosis paradigms are both error-prone and cost-intensive. Therefore, computer-aided technologies were developed for an exhaustive examination of retinal pathology and anatomy. In this article, a new retinal image enhancement method based on fuzzy c-means is proposed to enhance low quality retinal blood vessel images while preserving its brightness. Fuzzy c-means clustering groups the intensity levels into multiple clusters and assigns a cluster membership value to each intensity level. These values are subsequently modified and are then mapped to their corresponding initial values. The green channel of a modified image obtained above is equalized using the adaptive histogram equalization to yield the enhanced image. The results for the proposed algorithm were established using standard datasets consisting of 1000 fundus images with 39 categories. The proposed technique preserves the brightness and improves the contrast while improving vascular segmentation.},
  archive      = {J_TAI},
  author       = {Sanya Sinha and Ashish Kumar Bhandari and Reman Kumar},
  doi          = {10.1109/TAI.2023.3336612},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3022-3033},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Low quality retinal blood vessel image boosting using fuzzified clustering},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Metro flow prediction with hierarchical hypergraph attention
networks. <em>TAI</em>, <em>5</em>(6), 3012–3021. (<a
href="https://doi.org/10.1109/TAI.2023.3337052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional neural networks, which aim to learn the spatial correlation within multivariate time-series data, have achieved significant advancements in traffic prediction. Despite the proliferation of various spatiotemporal graph neural networks, a fundamental question of the predefined graphs still remain to be explored: how can the graph be dynamically optimized as time shifts? Especially for tasks such as rail transit where travel patterns of passengers change periodically. In other words, the spatial correlation of multivariate traffic time-series data is closely intertwined with temporal features, which are tricky to reflect through static graphs with handcrafted association. To adaptively learn the spatial correlation in feature domain, we propose hierarchical hypergraph attention networks (HHGATs). The hierarchical nature of the framework is reflected in both temporal and spatial dimensions. At different time spans, the model explores the hidden hyperedges with a local hypergraph attention mechanism and optimizes the hypergraph with a global attention mechanism. The nodelevel prediction results are then fused after the downstream tasks of spatiotemporal hypergraph convolutional networks. The effectiveness and accuracy of the proposed model are evaluated on historical datasets of Beijing and Hangzhou. Compared to the baselines, the proposed approach consistently demonstrates superior performance in various evaluation scenarios.},
  archive      = {J_TAI},
  author       = {Jingcheng Wang and Yong Zhang and Yongli Hu and Baocai Yin},
  doi          = {10.1109/TAI.2023.3337052},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3012-3021},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Metro flow prediction with hierarchical hypergraph attention networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Degradation-trend-aware deep neural network with attention
mechanism for bearing remaining useful life prediction. <em>TAI</em>,
<em>5</em>(6), 2997–3011. (<a
href="https://doi.org/10.1109/TAI.2023.3333767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remaining useful life (RUL) prediction of bearings has extraordinary significance for prognostics and health management (PHM) of rotating machinery. RUL prediction approaches based on deep learning have been dedicated to finding a nonlinear mapping relationship between nonstationary monitoring data and RUL. However, most existing approaches pay little attention to the degradation trend of diverse health stages of bearing and lack the discriminative power of crucial degradation features, resulting in the loss of some important information associated with RUL. To address this challenge, this article proposes a novel RUL prediction framework based on degradation-trend-aware deep neural network with attention mechanism (DTADAN). First the multidirection features with evident degradation trend are extracted via the analysis of bearing vibration signal from both time-domain and time-frequency domain. Next, the deep neural network architecture with attention mechanism is utilized to adaptively learn the critical degradation features beneficial for RUL prediction. Distinct from the existing approaches, the proposed framework is able to dynamically extract key degradation features of the bearing including degradation trend information and effectively fuse multidirection information to improve RUL prediction accuracy. The performance of the proposed approach is evaluated via case studies on XJTU-SY bearing dataset and PRONOSTIA bearing dataset. Compared with other state-of-the-art approaches, the proposed framework has better predictive accuracy and robustness. Additionally, interpretable analysis is provided to reveal the process of model learning and data characteristics, and the analysis results are helpful in guiding model learning.},
  archive      = {J_TAI},
  author       = {Yongkang Liu and Donghui Pan and Haifeng Zhang and Kai Zhong},
  doi          = {10.1109/TAI.2023.3333767},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2997-3011},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Degradation-trend-aware deep neural network with attention mechanism for bearing remaining useful life prediction},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical multiview top-k pooling with deep-q-networks.
<em>TAI</em>, <em>5</em>(6), 2985–2996. (<a
href="https://doi.org/10.1109/TAI.2023.3334261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) are extensions of deep neural networks to graph-structured data. It has already attracted widespread attention for various tasks such as node classification and link prediction. Existing research focuses more on graph convolutional neural networks (GCNs). However, it is usually overlooked that graph pooling can obtain graph representations by summarizing and down-sampling node information. Meanwhile, existing graph pooling methods mainly use top-k for node selection, but most of them consider only single view information when scoring nodes, and the k values in top-k are usually selected empirically. This work proposes the hierarchical multiview top-k pooling (HMTPool) with deep-Q-networks, which scores nodes taking into account multiview information (considering graph structure and features) and does not rely on the empirical adaptive selection of the best k value. HMTPool is a two-stage process. It first uses a variant GCN and multilayer perceptron to score the nodes from structural and feature perspectives, respectively and then performs fusion operations on the multiview information scores of the nodes. In addition, to select the optimal pooling ratio of top-k, we propose a deep-Q-network-based top-k (DTop-k) node selection method, which can adaptively select the best pooling ratio without prior knowledge. Experimental results on six TUDatasets and two Benchmarking GNNs datasets demonstrate the effectiveness of our proposed approach.},
  archive      = {J_TAI},
  author       = {Zhi-Peng Li and Hai-Long Su and Yong- Wu and Qin-Hu Zhang and Chang-An Yuan and Valeriya Gribova and Vladimir Fedorovich Filaretov and De-Shuang Huang},
  doi          = {10.1109/TAI.2023.3334261},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2985-2996},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Hierarchical multiview top-k pooling with deep-Q-networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiscale wasserstein shortest-path graph kernels for graph
classification. <em>TAI</em>, <em>5</em>(6), 2973–2984. (<a
href="https://doi.org/10.1109/TAI.2023.3333830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph kernels are conventional methods for computing graph similarities. However, the existing R-convolution graph kernels cannot resolve both of the two challenges: 1) comparing graphs at multiple different scales; and 2) considering the distributions of substructures when computing the kernel matrix. These two challenges limit their performances. To mitigate both of the two challenges, we propose a novel graph kernel called the multiscale Wasserstein shortest-path graph kernel (MWSP), at the heart of which is the multiscale shortest-path node feature map, of which each element denotes the number of occurrences of the shortest path around a node. The shortest path is represented by the concatenation of all the labels of nodes in it. Since the shortest-path node feature map can only compare graphs at local scales, we incorporate into it the multiple different scales of the graph structure, which are captured by the truncated BFS trees of different depths rooted at each node in a graph. We use the Wasserstein distance to compute the similarity between the multiscale shortest-path node feature maps of two graphs, considering the distributions of shortest paths. We empirically validate MWSP on various benchmark graph datasets and demonstrate that it achieves state-of-the-art performance on most datasets.},
  archive      = {J_TAI},
  author       = {Wei Ye and Hao Tian and Qijun Chen},
  doi          = {10.1109/TAI.2023.3333830},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2973-2984},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multiscale wasserstein shortest-path graph kernels for graph classification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structure-sensitive graph dictionary embedding for graph
classification. <em>TAI</em>, <em>5</em>(6), 2962–2972. (<a
href="https://doi.org/10.1109/TAI.2023.3334259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph structure expression plays a vital role in distinguishing various graphs. In this work, we propose a structure-sensitive graph dictionary embedding (SS-GDE) framework to transform input graphs into the embedding space of a graph dictionary for the graph classification task. Instead of a plain use of a base graph dictionary, we propose the variational graph dictionary adaptation (VGDA) to generate a personalized dictionary (named adapted graph dictionary) for catering to each input graph. In particular, for the adaptation, the Bernoulli sampling is introduced to adjust substructures of base graph keys according to each input, which increases the expression capacity of the base dictionary tremendously. To make cross-graph measurement sensitive as well as stable, multisensitivity Wasserstein encoding is proposed to produce the embeddings by designing multiscale attention on optimal transport. To optimize the framework, we introduce mutual information as the objective, which further deduces variational inference of the adapted graph dictionary. We perform our SS-GDE on multiple datasets of graph classification, and the experimental results demonstrate the effectiveness and superiority over the state-of-the-art methods.},
  archive      = {J_TAI},
  author       = {Guangbu Liu and Tong Zhang and Xudong Wang and Wenting Zhao and Chuanwei Zhou and Zhen Cui},
  doi          = {10.1109/TAI.2023.3334259},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2962-2972},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Structure-sensitive graph dictionary embedding for graph classification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bio-inspired intelligence-based multiagent navigation with
safety-aware considerations. <em>TAI</em>, <em>5</em>(6), 2946–2961. (<a
href="https://doi.org/10.1109/TAI.2023.3334227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple autonomous vehicles (MAVs) enhance efficiency and task execution compared to a single vehicle. Real-world applications necessitate MAVs to safely navigate in dynamic formation along planned trajectories, while sensing, mapping, and avoiding obstacles. Addressing the need for trajectory adaptation amidst real-world scenarios, a safety-aware bio-inspired framework is proposed in this article. Our approach employs a chaotic gravitational search algorithm (CGSA) for global trajectory generation in a predefined formation. A quadtree-driven variable resolution (QVR) algorithm using monocular cameras provides occupancy grid maps (OGMs) at different resolutions. A formation control with target tracking minimizes a potential function for MAVs to follow the CGSA trajectory. Additionally, a bio-inspired neural network (BNN) local navigator coupled with dynamic moving windows (DMW) advances obstacle avoidance and refines safe trajectories using QVR and OGMs. Simulation and comparative studies validate the framework&#39;s robustness and effectiveness for MAVs.},
  archive      = {J_TAI},
  author       = {Tingjun Lei and Chaomin Luo and Simon X. Yang and Daniel W. Carruth and Zhuming Bi},
  doi          = {10.1109/TAI.2023.3334227},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2946-2961},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Bio-inspired intelligence-based multiagent navigation with safety-aware considerations},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel visual transformer for long-distance pipeline
pattern recognition in complex environment. <em>TAI</em>, <em>5</em>(6),
2933–2945. (<a href="https://doi.org/10.1109/TAI.2023.3333821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effective warning of dangerous events along long-distance pipelines is critical to ensure the safety of oil and gas transportation. Distributed optical fiber sensing (DOFS) technology can assist operators to identify threat vibration signals. However, due to the complex and changeable environmental background noise along long-distance pipeline, most of the existing methods only extract one-dimensional features of the signal, making it difficult to distinguish various types of environmental noise, strong interference, and dangerous events. Besides, the samples of different classes in the actual scene are unbalanced. The sample size of dangerous events is often smaller than others. To address these problems, we use image encoding to transform the time series signals collected by the DOFS system into image data, and fully extract the time dependence and the correlation between different elements in the signal. Moreover, a visual transformer model PipelineADWinT is proposed. The self-attention mechanism of diagonal-axial window designed in this model can perfectly combine image encoding features, and obtain local to global multiscale features through hierarchical structure. By optimizing the loss function, the model&#39;s ability to handle the class imbalance problem is enhanced. The experimental results show that PipelineADWinT has more comprehensive classification performance and fewer false alarms than all the baseline models, which proves the effectiveness and superiority of the model.},
  archive      = {J_TAI},
  author       = {Chengyuan Zhu and Yanyun Pu and Kaixiang Yang and Qinmin Yang and C.L. Philip Chen},
  doi          = {10.1109/TAI.2023.3333821},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2933-2945},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A novel visual transformer for long-distance pipeline pattern recognition in complex environment},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated reinforcement learning with knowledge transfer for
network selection in hybrid WiFi-VLC networks. <em>TAI</em>,
<em>5</em>(6), 2921–2932. (<a
href="https://doi.org/10.1109/TAI.2023.3333267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dramatic growth of mobile data demand may impose a heavy traffic burden in indoor environments such that conventional radio frequency wireless networks might not be enough to ensure a high quality of service (QoS). Applications such as texting, 4k video streaming, and virtual reality have substantial differences in terms of data rate requirements. Visible light communication (VLC) has emerged recently as a complementary technology in heterogeneous networks, as it supports a high data rate and uses an unlicensed spectrum. In hybrid WiFi-VLC networks, users can be assigned to WiFi or VLC access points (APs) in downlink to maximize the minimum user data rate. In this article, we propose an intelligent network selection that is capable of reducing network complexity and enhancing users’ QoS. A federated Q-learning (FQL) algorithm is proposed, where each VLC AP performs local Q-learning and updates the global model at the WiFi AP. A new local reward function is proposed to maximize user satisfaction with limited shared information. Furthermore, a deep neural network (DNN) is proposed to reduce the complexity of the FQL. The output of the DNN is adjusted to assign some users directly to WiFi or VLC and create an initial policy for the FQL. To quantify the improvements achieved, the proposed federated Q-learning assisted by knowledge transfer (FQL-KT) is compared against state-of-the-art benchmark approaches, such as centralized Q-learning (CQL), FQL, signal strength strategy (SSS), and exhaustive search. Numerical simulation results show that the proposed FQL-KT significantly outperformed the other approaches in terms of convergence speed, maximizing the minimum user data rate, fairness, and minimizing the outage probability.},
  archive      = {J_TAI},
  author       = {Abdulmajeed M. Alenezi and Khairi A. Hamdi},
  doi          = {10.1109/TAI.2023.3333267},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2921-2932},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Federated reinforcement learning with knowledge transfer for network selection in hybrid WiFi-VLC networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-modal and cross-level attention interaction network
for salient object detection. <em>TAI</em>, <em>5</em>(6), 2907–2920.
(<a href="https://doi.org/10.1109/TAI.2023.3333827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing RGB-D salient object detection methods utilize the convolutional neural networks (CNNs) to extract features. However, they fail to extract global information due to the inherent defect of sliding window. On the other hand, with the emergence of depth clues, how to effectively incorporate cross-modal features has become an underlying challenge. In addition, in terms of cross-level feature fusion, most methods do not fully consider the complementarity between different layers and usually adopt simple fusion strategies, thereby leading to the missing of detailed information. To relieve these issues, a cross-modal and cross-level attention interaction network (CAINet) is proposed. First, different from most existing methods, we adopt a two-stream Swin Transformers to extract RGB and depth features. Second, a high-level context refinement module (HCRM) is designed to further extract refined features and give accurate guidance in early prediction stage. Third, we design a cross-modal interaction enhancement module (CIEM) to explore the complementarity of different modalities via coattention. In terms of fusion for high-level and low-level features in decoding, a multiscale attention induced decoder (MAID) is designed to extract and fuze the complementary information at different scales. Finally, the edge enhancement module (EEM) is employed to compensate the dilution of edge. Our proposed CAINet achieves excellent performance compared to other state-of-the-art (SOTA) methods on seven widely used datasets.},
  archive      = {J_TAI},
  author       = {Fasheng Wang and Yiming Su and Ruimin Wang and Jing Sun and Fuming Sun and Haojie Li},
  doi          = {10.1109/TAI.2023.3333827},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2907-2920},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Cross-modal and cross-level attention interaction network for salient object detection},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detection of sleep apnea from ECG signals using sliding
singular spectrum based subpattern principal component analysis.
<em>TAI</em>, <em>5</em>(6), 2897–2906. (<a
href="https://doi.org/10.1109/TAI.2023.3329455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sleep apnea (SA) is a potentially fatal sleep disorder where breathing regularly pauses and resumes during sleep, which results in regular awakenings. In this work, we introduced two efficient models which were tested on both the handcrafted and the latent features. To preprocess and segment the electrocardiogram (ECG) signals into multiple spectrums, this work uses a unique approach known as sliding singular spectrum analysis (SSSA). Later, we considered four time-frequency domain (TFD) features, such as spectral entropy (SE), signal energy (EN), dominant frequency (DF), and spike rhythmicity (SR) to precisely detect and classify SA from the ECG signals. To cope with the high-dimensional nature of the data, we have proposed a novel algorithm named subpattern-based principal component analysis (SPPCA), which can extract the most prominent features by delimiting the dimensions of the original features. To classify the ECG data, the low-dimensional TFD features were used to train and validate different machine learning (ML) models, such as extreme gradient boosting (XGB), support vector machine (SVM), Gaussian Naive Bayes (GNB), stochastic gradient descent (SGD), and K-nearest neighbor (KNN). Similarly, we implemented a deep learning (DL) framework named modified LeNet-5 CNN network (MLN-CNN), which extracts the hidden features to classify SA from the ECG signals. We used the Physionet Apnea ECG (PNEA) and St. Vincent&#39;s University Hospital/University College Dublin (UCD) databases for this study, which are publicly available. We evaluated both the proposed algorithms using various classification metrics. The metrics suggest that we achieved the highest accuracy of 100% and 97.1% on PNEA and 86.66% and 92.30% on UCD databases, respectively. The performance metrics of our proposed algorithms have shown a significant dominance over the latest state-of-the-art works.},
  archive      = {J_TAI},
  author       = {Muhammad Zubair and Umesh Kumar Naik M and Rajesh K. Tripathy and Mohammed Alhartomi and Saeed Alzahrani and Shaik Rafi Ahamed},
  doi          = {10.1109/TAI.2023.3329455},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2897-2906},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Detection of sleep apnea from ECG signals using sliding singular spectrum based subpattern principal component analysis},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Absorb and repel: Pseudo-label refinement for intra-camera
supervised person re-identification. <em>TAI</em>, <em>5</em>(6),
2884–2896. (<a href="https://doi.org/10.1109/TAI.2023.3327671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (ReID) aims to identify pedestrian images with the same identity across non-overlapping camera views. Intra-camera supervised person re-identification (ICS-ReID) is a new paradigm that trains a model using only intra-camera labels, thus reducing the cost of inter-camera identity association. Pseudo-label-based clustering algorithms perform well in the unsupervised ReID task, whereas they inevitably generate noisy pseudo labels through clustering, especially in the early training stage. Given this, we propose an unsupervised pseudo-labeling method to help in the semi-supervised ICS-ReID task. This method improves the clustering results by reassigning pseudo labels for the training data and consists of two modules, Absorb and Repel. The Absorb module aims to group all data with the same intra-camera identity into one cluster. The Repel module ensures that images under the same camera view but with different identities do not appear in the same cluster. Both modules are independent yet complementary to reduce the error rate of pseudo labels generated in each epoch. To our knowledge, this is the first attempt to refine pseudo labels for ICS-ReID. Our method is a simple, nonparametric, and effective strategy that can be easily integrated into existing clustering-based unsupervised ReID tasks. Extensive experiments demonstrate that our proposed method outperforms the state-of-the-art ICS-ReID approaches on three large-scale benchmark person ReID datasets.},
  archive      = {J_TAI},
  author       = {Wei Li and Chuyi Chen and Kaizhu Huang},
  doi          = {10.1109/TAI.2023.3327671},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2884-2896},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Absorb and repel: Pseudo-label refinement for intra-camera supervised person re-identification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive context based road accident risk prediction using
spatio-temporal deep learning. <em>TAI</em>, <em>5</em>(6), 2872–2883.
(<a href="https://doi.org/10.1109/TAI.2023.3328578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic accidents are common urban events that pose significant risks to human safety, traffic management, and economic stability; consequently, the research community is paying increasing attention toward accident risk prediction. However, accident risk prediction is a challenging problem because accident occurrences are sparse and influenced by multiple contextual factors (e.g., POI, road structure, road type, hour of the day, and month). Therefore, in this article, we propose a novel architecture named Topographic-Weighted Context Category (TWCCnet) that adapts heterogeneous contextual category weights based on spatial–temporal correlations across sectors. Specifically, the framework consists of two parallel components: one uses convolution and stacked bidirectional gated recurrent unit (Bi-GRU) to capture spatial–temporal relationships between neighborhood sectors, while the other uses multiple graph convolution network (GCN) over resemblance graphs to capture spatial–temporal relationships between semantic sectors. At last, temporal attention is utilized on top of parallel components to learn important spatiotemporal features that have a substantial impact on traffic accidents. The extensive experiments on two publicly available citywide datasets, i.e., New York City and Chicago demonstrate the effectiveness of the proposed approach and showed (7.21, 10.7) Root Mean Squared Error (RMSE), (34.09, 20.75) Mean Average Precision (MAP), and (0.19, 0.09) Recall approximately over both datasets, respectively, outperforming baseline as well as state-of-the-art models.},
  archive      = {J_TAI},
  author       = {Nishit Bhardwaj and Anupriya Pal and Bhumika and Debasis Das},
  doi          = {10.1109/TAI.2023.3328578},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2872-2883},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adaptive context based road accident risk prediction using spatio-temporal deep learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Critical sampling for robust evolution operator learning of
unknown dynamical systems. <em>TAI</em>, <em>5</em>(6), 2856–2871. (<a
href="https://doi.org/10.1109/TAI.2023.3327676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given an unknown dynamical system, what is the minimum number of samples needed for effective learning of its governing laws and accurate prediction of its future evolution behavior, and how to select these critical samples? In this work, we propose to explore this problem based on a design approach. Starting from a small initial set of samples, we adaptively discover critical samples to achieve increasingly accurate learning of the system evolution. One central challenge here is that we do not know the network modeling error since the ground-truth system state is unknown, which is, however, needed for critical sampling. To address this challenge, we introduce a multistep reciprocal prediction network where forward and backward evolution networks are designed to learn the temporal evolution behavior in the forward and backward time directions, respectively. Very interestingly, we find that the desired network modeling error is highly correlated with the multistep reciprocal prediction error, which can be directly computed from the current system state. This allows us to perform a dynamic selection of critical samples from regions with high network modeling errors for dynamical systems. In addition, a joint spatial-temporal evolution network is introduced, which incorporates spatial dynamics modeling into the temporal evolution prediction for robust learning of the system evolution operator with few samples. Our extensive experimental results demonstrate that our proposed method is able to dramatically reduce the number of samples needed for effective learning and accurate prediction of evolution behaviors of unknown dynamical systems by up to hundreds of times.},
  archive      = {J_TAI},
  author       = {Ce Zhang and Kailiang Wu and Zhihai He},
  doi          = {10.1109/TAI.2023.3327676},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2856-2871},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Critical sampling for robust evolution operator learning of unknown dynamical systems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An intelligent thyroid diagnosis system utilizing multiple
ensemble and explainable algorithms with medical supported attributes.
<em>TAI</em>, <em>5</em>(6), 2840–2855. (<a
href="https://doi.org/10.1109/TAI.2023.3327981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread impact of thyroid disease and its diagnosis is a challenging task for healthcare experts. The conventional technique for predicting such a vital disease is complex and time-consuming. A data-driven approach may offer predictive solutions, but it relies on all relevant attributes, which are computationally expensive. Hence, we propose a novel machine learning (ML) based disease prediction system that could potentially predict it by considering three crucial steps. First, to reduce the dimension of the dataset, three feature selection techniques were employed, including feature importance (FIS), information gain selections (IGS), and least absolute shrinkage and selection operator (LAS). Moreover, recommended medical references were considered while developing a feature set having the identical attributes as high-risk factors (HRF). Second, the models, including the three stage hybrid classifier ( 3SHC ) and the three stage hybrid artificial neural network ( 3SHANN ), are used as classifiers on the training data set. Third, a local interpretable model-agnostic explanations (LIME) to the 3SHC with the HRF samples was applied to individually explain the predictions. Then, the overall behaviors of both gender and age categories were explored with the help of a partial dependence plot (PDP). Finally, the proposed system is validated with extensive experiments, where the 3SHC achieves an accuracy (ACC) of 99.29%, which can play a crucial role in preventing thyroid disease and alleviating stress in the healthcare sector.},
  archive      = {J_TAI},
  author       = {Ananda Sutradhar and Mustahsin Al Rafi and Pronab Ghosh and F M Javed Mehedi Shamrat and Md. Moniruzzaman and Kawsar Ahmed and AKM Azad and Francis M. Bui and Li Chen and Mohammad Ali Moni},
  doi          = {10.1109/TAI.2023.3327981},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2840-2855},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An intelligent thyroid diagnosis system utilizing multiple ensemble and explainable algorithms with medical supported attributes},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Text-image scene graph fusion for multimodal named entity
recognition. <em>TAI</em>, <em>5</em>(6), 2828–2839. (<a
href="https://doi.org/10.1109/TAI.2023.3326416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularity and widespread use of social media platforms, such as Twitter and Facebook, massive amounts of text and image information posted by a variety of users have flooded these social media platforms. Thus, multimodal named entity recognition (MNER) has become a research hotspot for the task of extracting named entities from multimodal data. Empirically, the visual clues unrelated to text data may introduce uncertain or even negative impacts on the named entity recognition. The considerations of the relevance of multimodal data have been ignored in the previous studies. In this article, to effectively measure the relationship between text data and visual cues for improving the accuracy of named entities, we propose a text-image scene graph fusion (TISGF) approach with a text-image similarity assessment module (TISA) and a text-image fusion module (TIF) for MNER. Specifically, we first construct two (visual and textual) scene graphs to exploit the joint features of objects and relations in text and image and encode the two scene graphs separately using a specific encoder pair. In this way, we can obtain both object-level and relationship-level cross-modal features. Subsequently, TISA is used to compute the similarity of the image and text data and to determine the proportion of visual information that will be retained for fusion. Finally, we use TIF to achieve a unified multimodal representation of each word and predict the entity type using conditional random fields. Extensive experiment results on two public datasets demonstrate the effectiveness and competitiveness of our proposed method for the MNER task.},
  archive      = {J_TAI},
  author       = {Jian Cheng and Kaifang Long and Shuang Zhang and Tian Zhang and Lianbo Ma and Shi Cheng and Yinan Guo},
  doi          = {10.1109/TAI.2023.3326416},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2828-2839},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Text-image scene graph fusion for multimodal named entity recognition},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Electrocardiogram heartbeat classification using machine
learning and ensemble convolutional neural network-bidirectional long
short-term memory technique. <em>TAI</em>, <em>5</em>(6), 2816–2827. (<a
href="https://doi.org/10.1109/TAI.2023.3324627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated classification of cardiac rhythms from electrocardiogram (ECG) signals is significant for diagnosis of cardiovascular dysfunctioning. A biggest challenge in automated ECG classification is to address the task&#39;s specific characteristics, such as time dependencies between observations and a strong class imbalance. To address these issues, this article proposes machine learning ensemble techniques (random forest, support vector machine, Xgboost, Adaboost, and stacked ensemble classifier) and an ensemble of convolutional neural network (CNN) and bidirectional long short-term memory (Bi-LSTM) architecture for classification of cardiac arrhythmias in ECG signals. The proposed model has been trained and tested on the MIT-BIH arrhythmias database, which contains a total of 109 443 ECG beats with 90 589 normal beats, 8039 supraventricular beats (SB), 7236 ventricular beats (VB), 2776 fusion beats, and 803 unknown beats, respectively. Here, in this article, we incorporate a synthetic minority oversampling technique for balancing dataset along with time-series feature extraction library for extracting features and an ensemble of CNN-LSTM to predict fast features of upcoming ECG signals to enhance the performance of heartbeat classification as compared to the state-of-the-art techniques. The achieved average accuracy (Acc), sensitivity (SE), precision (PE), and F1-score using stacked ensemble machine learning algorithm for detection arrhythmias heartbeat classes are 98.67%, 96%, 97.80%, and 96.80%, respectively. With the ensemble CNN-BiLSTM method, an average accuracy of 99.88% has been achieved. The SE and PE values for SB and VB beats are 84.40%, 99.79%, 96.40%, and 98.54%, respectively. Experimental results demonstrate that an ensemble CNN-BiLSTM technique outperforms as compared to the existing classification techniques in terms of sensitivity, accuracy, and precision.},
  archive      = {J_TAI},
  author       = {Neenu Sharma and Ramesh Kumar Sunkaria and Amandeep Kaur},
  doi          = {10.1109/TAI.2023.3324627},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2816-2827},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Electrocardiogram heartbeat classification using machine learning and ensemble convolutional neural network-bidirectional long short-term memory technique},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vision-based PM2.5 concentration estimation with natural
scene statistical analysis. <em>TAI</em>, <em>5</em>(6), 2805–2815. (<a
href="https://doi.org/10.1109/TAI.2023.3324892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the primary pollutant in China&#39;s urban atmosphere, particulate matter (PM) $_{2.5}$ poses a great threat to the health of residents and ecological stability. An efficient and effective PM $_{2.5}$ concentration monitoring is essential. Nonetheless, the popular devices for PM $_{2.5}$ monitoring are developed based on two standards: the microoscillation balance method and the $\beta$ -ray method, which have high purchase and maintenance costs and slow calculation rates. To this end, we put forward a real-time and reliable vision-based estimation algorithm of PM $_{2.5}$ concentration. To be specific, the proposed method first develops two natural scene statistical analysis-based visual priors to measure saturation and structural information losses caused by the “haze” formed by PM $_{2.5}$ . Moreover, we develop a lightweight deep belief network-deep neural network-based PM $_{2.5}$ concentration estimation model, which learns the mapping from the designed visual priors to PM $_{2.5}$ concentrations. Experiments confirm the superiority of our vision-based PM $_{2.5}$ concentration estimation method by comparison with state-of-the-art photo-based PM $_{2.5}$ monitoring methods.},
  archive      = {J_TAI},
  author       = {Guangcheng Wang and Quan Shi and Han Wang and Ke Gu and Mengting Wei and Lai Kuan Wong and Mingxing Wang},
  doi          = {10.1109/TAI.2023.3324892},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2805-2815},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Vision-based PM2.5 concentration estimation with natural scene statistical analysis},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synthetic information toward maximum posterior ratio for
deep learning on imbalanced data. <em>TAI</em>, <em>5</em>(6),
2790–2804. (<a href="https://doi.org/10.1109/TAI.2023.3330949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work explores how class-imbalanced data affect deep learning and propose a data balancing technique for mitigation by generating more synthetic data for the minority class. In contrast to random-based oversampling techniques, our approach prioritizes balancing the most informative region by finding high entropy samples. This approach is opportunistic and challenging because well-placed synthetic data points can boost machine learning algorithms’ accuracy and efficiency, whereas poorly placed ones can cause a higher misclassification rate. In this study, we present an algorithm for maximizing the probability of generating a synthetic sample in the correct region of its class by placing it toward maximizing the class posterior ratio. In addition, to preserve data topology, synthetic data are closely generated within each minority sample neighborhood. Overall, experimental results on forty-one datasets show that our technique significantly outperforms experimental methods in terms of boosting deep-learning performance.},
  archive      = {J_TAI},
  author       = {Hung Nguyen and J. Morris Chang},
  doi          = {10.1109/TAI.2023.3330949},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2790-2804},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Synthetic information toward maximum posterior ratio for deep learning on imbalanced data},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Patch-to-sample reasoning for cervical cancer screening of
whole slide image. <em>TAI</em>, <em>5</em>(6), 2779–2789. (<a
href="https://doi.org/10.1109/TAI.2023.3323637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been instrumental in improving the accuracy of cervical cancer screening using whole-slide images (WSIs) in recent years. Due to the complexity of the computer-aided screening task, the pipeline typically involves detecting “abnormal” cervical cells, and runs classification at the patch and sample levels, respectively. While the patch-level classification for normal or abnormal cells cannot be perfect, the errors may accumulate across individual patches and make the subsequent sample-level classification even more difficult. To address these issues, we propose a patch-to-sample (P2S) reasoning method to screen the cervical abnormality in this article. We first improve the patch-level classifier by the hard patch mining strategy, such that the classifier is not only more accurate but also more powerful to represent suspicious cells in local patches. Then, we propose score embedding and token pooling to a transformer network, which aggregates multiple patches and derives the diagnosis result at the sample level. Experiments show that our P2S method can more effectively utilize the key patches in individual samples, and thus, outperforms existing methods.},
  archive      = {J_TAI},
  author       = {Maosong Cao and Manman Fei and Honglin Xiong and Xin Zhang and Xiangshan Fan and Lichi Zhang and Qian Wang},
  doi          = {10.1109/TAI.2023.3323637},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2779-2789},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Patch-to-sample reasoning for cervical cancer screening of whole slide image},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explain to me: Salience-based explainability for synthetic
face detection models. <em>TAI</em>, <em>5</em>(6), 2766–2778. (<a
href="https://doi.org/10.1109/TAI.2023.3333310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of convolutional neural networks has continued to improve over the last decade. At the same time, as model complexity grows, it becomes increasingly more difficult to explain model decisions. Such explanations may be of critical importance for reliable operation of human–machine pairing setups, or for model selection when the “best”’ model among many equally accurate models must be established. Saliency maps represent one popular way of explaining model decisions by highlighting image regions models deem important when making a prediction. However, examining salience maps at scale is not practical. In this article, we propose five novel methods of leveraging model salience to explain a model behavior at scale. These methods ask: 1) what is the average entropy for a model&#39;s salience maps; 2) how does model salience change when fed out-of-set samples; 3) how closely does model salience follow geometrical transformations; 4) what is the stability of model salience across independent training runs; and 5) how does model salience react to salience-guided image degradations. To assess the proposed measures on a concrete and topical problem, we conducted a series of experiments for the task of synthetic face detection with two types of models: those trained traditionally with cross-entropy loss, and those guided by human salience when training to increase model generalizability. These two types of models are characterized by different, interpretable properties of their salience maps, which allows for the evaluation of the correctness of the proposed measures. We offer source codes for each measure along with this article.},
  archive      = {J_TAI},
  author       = {Colton R. Crum and Patrick Tinsley and Aidan Boyd and Jacob Piland and Christopher Sweet and Timothy Kelley and Kevin Bowyer and Adam Czajka},
  doi          = {10.1109/TAI.2023.3333310},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2766-2778},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Explain to me: Salience-based explainability for synthetic face detection models},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event-triggered cooperative tracking control of multiagent
systems with a dynamic leader via approximate dynamic programming.
<em>TAI</em>, <em>5</em>(6), 2752–2765. (<a
href="https://doi.org/10.1109/TAI.2023.3327678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the optimal cooperative tracking control problem for unknown nonlinear multiagent systems with a real dynamic leader and uncertainties. Its main difficulty lies in eliminating effect of the unmatched uncertainty and noncooperation in the event-triggered manner. To overcome this difficulty, a distributed robust event-triggered control algorithm is developed with the aid of a disturbance observer and robust approximate dynamic programming (ADP). Specifically, this problem is converted into a cooperative multiplayer game for the distributed auxiliary systems employing observer-based feedforward compensation, which contributes to reducing conservatism; via an adaptive event-triggered mechanism and a new weight tuning law, a distributed robust ADP approach is proposed with hope to obviate judging the existence of the saddle point beforehand. Moreover, the local neighbor tracking error and the weight estimation error are demonstrated to be uniformly ultimately bounded by means of Lyapunov stability theory. We also prove that Zeno behavior is excluded. A comparative simulation example for multiquadrotor systems is carried out to verify the effectiveness and superiority of our algorithm.},
  archive      = {J_TAI},
  author       = {Hao Fu and Haodong He and Yang Chen},
  doi          = {10.1109/TAI.2023.3327678},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2752-2765},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Event-triggered cooperative tracking control of multiagent systems with a dynamic leader via approximate dynamic programming},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A culturally sensitive test to evaluate nuanced GPT
hallucination. <em>TAI</em>, <em>5</em>(6), 2739–2751. (<a
href="https://doi.org/10.1109/TAI.2023.3332837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generative pretrained transformer (GPT) models, renowned for generating human-like text, occasionally produce “hallucinations”—outputs that diverge from human expectations. Current mitigation strategies for these GPT hallucinations largely rely on algorithmic automation, thereby overlooking the complexities of human judgment and cultural influence, particularly in fact interpretation. Addressing this issue, we have introduced a Culturally Sensitive Test that integrates language subjectivity, cultural nuances, and GPT idiosyncrasies. We have applied this test to five GPT models—OpenAI&#39;s ChatGPT-3.5 and ChatGPT-4, Google&#39;s Bard, Perplexity AI, and TruthGPT—evaluating their responses to 70 questions across seven categories designed to provoke hallucinations. The evaluated models demonstrated varying performance, with controversial topics, those lacking clear scientific consensus and the brain teasers proving more susceptible to GPT hallucinations. Our study has paved the way for a nuanced assessment of GPT hallucinations.},
  archive      = {J_TAI},
  author       = {Timothy R. McIntosh and Tong Liu and Teo Susnjak and Paul Watters and Alex Ng and Malka N. Halgamuge},
  doi          = {10.1109/TAI.2023.3332837},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2739-2751},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A culturally sensitive test to evaluate nuanced GPT hallucination},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mathematical modeling and optimal stopping theory-based
extra layers for 30-day rate risk prediction of readmission to intensive
care units. <em>TAI</em>, <em>5</em>(6), 2723–2738. (<a
href="https://doi.org/10.1109/TAI.2023.3330136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The importance of $30$ -day patients’ readmissions (PRs) to intensive unit care stems from the significant cost and mortality risk when the patient&#39;s chosen class (i.e., readmitted or not to the hospital) is incorrect. The overall accuracy of the PRs classification obtained in the literature is still moderate, particularly for machine learning (ML) and deep artificial neural networks (ANNs), where the overall accuracy is around $65\%$ , resulting in $35\%$ of critical wrong decisions. The suggested technique for enhancing such an overall accuracy consists of three distinct phases. The first layer is an ML-assisted algorithm that uses support vector machines (SVMs) and ANNs techniques, while the second layer uses an arbitrary dataset&#39;s distribution and mathematical modeling of the problem to determine the optimal class probabilities interval that has the highest percentage of misclassified elements. Next, all items’ categories in that interval are altered, with the exception of those deemed by the generalized secretary problem (GSP) algorithm (the third layer) to be most likely correct. We provide a new theorem that applies to every distribution type and yields the GSP&#39;s optimum parameter. Starting with evidence that the Gilbrat and generalized logistic distributions suit the class probabilities produced by ANN and SVM, respectively, we demonstrate that our technique improves the overall accuracy by $5\%$ and $19\%$ , reaching $85\%$ and $87\%$ , respectively. It is important to point out that the suggested additional two layers may be utilized for any binary classification problem; this is independent of the dataset distribution and the stages that came before.},
  archive      = {J_TAI},
  author       = {Azeddine El Hassouny and Faissal El Bouanani and Khalid A. Qaraqe},
  doi          = {10.1109/TAI.2023.3330136},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2723-2738},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Mathematical modeling and optimal stopping theory-based extra layers for 30-day rate risk prediction of readmission to intensive care units},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bootstrap-based layerwise refining for causal structure
learning. <em>TAI</em>, <em>5</em>(6), 2708–2722. (<a
href="https://doi.org/10.1109/TAI.2023.3329786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning causal structures from observational data is critical for causal discovery and many machine learning tasks. Traditional constraint-based methods first adopt conditional independence (CI) tests to learn a global skeleton layer by layer and then orient the undirected edges to obtain a causal structure. However, the reliability of these statistical tests largely depends on the quality of data samples. In real-life scenarios, the presence of data noise or limited samples often makes many CI tests unreliable at each layer in the skeleton learning phase, leading to an inaccurate skeleton. As the number of layers increases, the inaccurate skeleton will continue to impair the skeleton construction of subsequent layers. Furthermore, an unreliable skeleton hampers the skeleton orientation procedure, resulting in an unsatisfactory causal structure. In this article, we propose a Bootstrap-based layerwise refining (BLR) algorithm for causal structure learning, which includes two new procedures to solve the above problems. First, BLR utilizes a novel layerwise skeleton refining procedure to construct the global skeleton layer by layer based on the bootstrap sampling. Second, BLR employs a collective skeleton orientation procedure that incorporates scoring techniques to collectively orient the global skeleton. The experimental results show that BLR outperforms the state-of-the-art methods on the benchmark Bayesian Network datasets.},
  archive      = {J_TAI},
  author       = {Guodu Xiang and Hao Wang and Kui Yu and Xianjie Guo and Fuyuan Cao and Yukun Song},
  doi          = {10.1109/TAI.2023.3329786},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2708-2722},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Bootstrap-based layerwise refining for causal structure learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Low-rank multilabel learning based on nonlinear mapping.
<em>TAI</em>, <em>5</em>(6), 2693–2707. (<a
href="https://doi.org/10.1109/TAI.2023.3329079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The relationship between features and labels, as well as the correlation between labels are two important factors that affect multilabel learning. Many existing studies assume that there is a linear mapping between sample space and label space. However, the linearity assumption often does not hold in practical applications, especially when characterizing the relationship between two high-dimensional spaces. In this article, a nonlinear mapping is used to describe the relationship between features and labels by mapping the original labels to a low-rank latent label space. With the help of nonlinear mapping, more compact and accurate label information can be mined, effectively overcoming the defect that linear models cannot fit real labels. In particular, the label space is firstly decomposed into a low-rank latent space to reduce the dimension of labels, and then a nonlinear mapping from the sample space to the low-rank latent space is constructed. Considering that the predicted values of similar labels should be closer, a comprehensive manifold regularization term is introduced to learn local and global label-correlation. The proposed multilabel learning method can effectively solve the problem of missing multilabel learning. The experimental results on ten multilabel databases show that the proposed method has good robustness and is superior to the most advanced multilabel learning methods.},
  archive      = {J_TAI},
  author       = {Changzhong Wang and Yan Wang and Tingquan Deng and Weiping Ding and Yuhua Qian},
  doi          = {10.1109/TAI.2023.3329079},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2693-2707},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Low-rank multilabel learning based on nonlinear mapping},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy efficient and fast CNN inference by exploring weight
approximation and computational reuse. <em>TAI</em>, <em>5</em>(6),
2678–2692. (<a href="https://doi.org/10.1109/TAI.2023.3324315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenges of convolutional neural networks (CNNs) based AI inference on edge devices include computing complexity, large memory requirements, and high power consumption. Researchers have pursued efficient hardware, dataflow optimization, and new algorithms to tackle these obstacles. This article introduced a novel acceleration methodology designed to significantly enhance the processing speed and power of existing CNN architectures. The proposed approach leverages the inherent static nature of pretrained CNN weights through the prism of linear approximation. Different weight approximation options are evaluated, including within and across kernels. By using the same weight index from different kernels for approximation, computation reuse becomes possible, as the same index from different kernels is sampled with the same data from the input feature map. The proposed approach is evaluated through the design and analysis of four CNN architectures, considering hardware resources, power consumption, and latency. The proposed algorithms are implemented on a gem5-based RISCV simulator, demonstrating a speedup of approximately 2× compared to the baseline. In addition, the proposed CNN accelerators are implemented on Xilinx Kintex 7 Field Programmable Gate Array (FPGA), resulting in a 50% reduction in FPGA hardware resources. When benchmarked with AlexNet, VGG16, SqueezeNet, and ResNet, the proposed approach achieves a 50% reduction in multiplications compared to previous works while maintaining accuracy with loss of 0.9% for VGG16 and 3.1% for AlexNet. This work can be used as a framework to evaluate the different tradeoffs between accuracy, latency, power, and cost of different CNN architectures and applications.},
  archive      = {J_TAI},
  author       = {Mohammed F. Tolba and Hani Saleh and Baker Mohammad and Mahmoud Al-Qutayri and Ayman Hroub and Thanos Stouraitis},
  doi          = {10.1109/TAI.2023.3324315},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2678-2692},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Energy efficient and fast CNN inference by exploring weight approximation and computational reuse},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sequential datum-wise feature acquisition and classifier
selection. <em>TAI</em>, <em>5</em>(6), 2663–2677. (<a
href="https://doi.org/10.1109/TAI.2023.3334707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a supervised machine learning framework for sequential datum-wise feature acquisition and classifier selection. The presented method sequentially acquires features during testing until it determines that additional features will not improve label assignment. At that stage, easy-to-classify examples are handled by a simple classifier, which assigns labels based on the lowest expected misclassification cost. On the contrary, difficult-to-classify examples are assigned a label using the acquired features along with one of a number of available complex classifiers. As more features are acquired, the presented framework continually assesses the difficulty of classifying each example. It controls both the feature acquisition and classifier selection processes through a carefully constructed optimization problem. We use eleven publicly available datasets to evaluate the presented framework with respect to accuracy and average number of acquired features, and obtain results when two and three complex classifiers are available, respectively. We compare the performance of the presented framework with both sequential feature acquisition methods and dynamic classifier selection methods, and observe improvements in accuracy as well as acquisition of less number of features on average. Moreover, we conduct experiments with popular ensemble classification methods and assess the performance of the proposed framework.},
  archive      = {J_TAI},
  author       = {Sachini Piyoni Ekanayake and Daphney-Stavroula Zois and Charalampos Chelmis},
  doi          = {10.1109/TAI.2023.3334707},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2663-2677},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Sequential datum-wise feature acquisition and classifier selection},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Representing multiview time-series graph structures for
multivariate long-term time-series forecasting. <em>TAI</em>,
<em>5</em>(6), 2651–2662. (<a
href="https://doi.org/10.1109/TAI.2023.3326796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate long-term time-series forecasting tasks are very challenging tasks in many real-world application areas. Recently, researchers focus on designing robust and effective methods, and have made considerable progress. However, there are several issues with existing models that need to be overcome. First, the lack of a relationship structure between multivariate variables needs to be addressed. Second, most models only have a weak ability to capture local dynamic changes across the entire long-term time-series. Third, current models suffer from high computational complexity and unsatisfactory accuracy. To figure out the abovementioned issues, we propose an effective and efficient method called multiview time-series graph structure representation (MTGSR). MTGSR uses GCNs to construct topological relationships in the multivariate time-series from three different perspectives: time, dimension, and crossing segments. Variation trends in different dimensions are extracted through a difference operation to construct topological maps that reflect the correlations between different dimensions. To capture the dynamically changing characteristics of fluctuation correlations between adjacent local sequences, MTGSR constructs a cross graph by calculating correlation coefficients between adjacent local sequences. Extensive experiments show that MTGSR reduces errors by 17.41% over the state of the art. In addition, memory use is decreased by 66.52% and the running time is reduced by 78.09%.},
  archive      = {J_TAI},
  author       = {Zehao Wang and Jin Fan and Huifeng Wu and Danfeng Sun and Jia Wu},
  doi          = {10.1109/TAI.2023.3326796},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2651-2662},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Representing multiview time-series graph structures for multivariate long-term time-series forecasting},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kullback–leibler divergence-based regularized normalization
for low-resource tasks. <em>TAI</em>, <em>5</em>(6), 2638–2650. (<a
href="https://doi.org/10.1109/TAI.2023.3323918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large pretrained models, like BERT, GPT, and Wav2Vec, have demonstrated their ability to learn transferable representations for various downstream tasks. However, obtaining a substantial amount of supervised data remains a challenge due to resource and time limitations. As a solution, researchers have turned their attention to using large pretrained datasets via techniques like fine tuning, linear probing, or prompt tuning in low-resource settings. Normalization techniques play a crucial role in speeding up training, style transfer, object detection, recurrent neural networks, and improving the generalization of deep neural networks. Despite their success in various domains, their effectiveness in low-resource NLP and speech tasks has been limited. A notable reason for this limitation is the difficulty in capturing expressiveness using affine parameters of normalization. To address this issue, we propose a novel approach called Kullback–Leibler (KL) regularized normalization or KL-Norm. The main objective of KL-Norm is to ensure that normalized data are well-behaved and to improve generalization by reducing overfitting by including a regularization loss function in the training process. It achieves this by promoting good performance on out-of-domain distributions and effectively filtering relevant features while eliminating superficial features or biases present in the dataset or pretrained model. Remarkably, KL-Norm accomplishes these objectives with minimal increase in model parameters and memory overheads. Through extensive experimental analysis, we showcase the improved accuracy and performance of KL-Norm in comparison to other normalization techniques on low-resource downstream NLP tasks. These tasks encompass a wide range of applications, including sentiment classification, semantic relationship characterization, semantic textual similarity, textual entailment, and paraphrase detection. Additionally, KL-Norm exhibits superior results in downstream speech tasks, specifically in keyword detection and emotion classification.},
  archive      = {J_TAI},
  author       = {Neeraj Kumar and Ankur Narang and Brejesh Lall},
  doi          = {10.1109/TAI.2023.3323918},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2638-2650},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Kullback–Leibler divergence-based regularized normalization for low-resource tasks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rule-based out-of-distribution detection. <em>TAI</em>,
<em>5</em>(6), 2627–2637. (<a
href="https://doi.org/10.1109/TAI.2023.3323923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Out-of-distribution detection is one of the most critical issue in the deployment of machine learning. The data analyst must assure that data in operation should be compliant with the training phase as well as understand if the environment has changed in a way that autonomous decisions would not be safe anymore. The method of the paper is based on eXplainable Artificial Intelligence (XAI); it takes into account different metrics to identify any resemblance between in-distribution and out of, as seen by the XAI model. The approach is nonparametric and distributional assumption free. The validation over complex scenarios (predictive maintenance, vehicle platooning, covert channels in cybersecurity) corroborates both precision in detection and evaluation of training-operation conditions proximity.},
  archive      = {J_TAI},
  author       = {Giacomo De Bernardi and Sara Narteni and Enrico Cambiaso and Maurizio Mongelli},
  doi          = {10.1109/TAI.2023.3323923},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2627-2637},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Rule-based out-of-distribution detection},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated learning for distribution skewed data using sample
weights. <em>TAI</em>, <em>5</em>(6), 2615–2626. (<a
href="https://doi.org/10.1109/TAI.2023.3348073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most challenging issues in federated learning (FL) is that the data are often not independent and identically distributed (non-IID). Clients are expected to contribute the same type of data and drawn from one global distribution. However, data are often collected in different ways from different resources. Thus, the data distributions among clients might be different from the underlying global distribution. This creates a weight divergence issue and reduces FL performance. This work focuses on improving FL performance for skewed data distribution across clients. The main idea is to adjust the client distribution closer to the global distribution using sample weights. Thus, the machine learning (ML) model converges faster with higher accuracy. We start from the fundamental concept of empirical risk minimization and theoretically derive a solution for adjusting the distribution skewness using sample weights. To determine sample weights, we implicitly exchange density information by leveraging a neural network-based density estimation model, MADE. The clients’ data distribution can then be adjusted without exposing their raw data. Our experiment results on three real-world datasets show that the proposed method not only improves FL accuracy but also significantly reduces communication costs compared to the other experimental methods.},
  archive      = {J_TAI},
  author       = {Hung Nguyen and Peiyuan Wu and J. Morris Chang},
  doi          = {10.1109/TAI.2023.3348073},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2615-2626},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Federated learning for distribution skewed data using sample weights},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Curriculum guided domain adaptation in the dark.
<em>TAI</em>, <em>5</em>(6), 2604–2614. (<a
href="https://doi.org/10.1109/TAI.2023.3330829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing the rising concerns of privacy and security, domain adaptation in the dark aims to adapt a black-box source trained model to an unlabeled target domain without access to any source data or source model parameters. The need for domain adaptation of black-box predictors becomes even more pronounced to protect intellectual property as deep learning based solutions are becoming increasingly commercialized. Current methods distill noisy predictions on the target data obtained from the source model to the target model, and/or separate clean/noisy target samples before adapting using traditional noisy label learning algorithms. However, these methods do not utilize the easy-to-hard learning nature of the clean/noisy data splits. Also, none of the existing methods are end-to-end, and require a separate fine-tuning stage and an initial warmup stage. In this work, we present C urriculum A daptation for B lack- B ox ( CABB ) which provides a curriculum guided adaptation approach to gradually train the target model, first on target data with high confidence (clean) labels, and later on target data with noisy labels. CABB utilizes Jensen–Shannon divergence as a better criterion for clean-noisy sample separation, compared to the traditional criterion of cross-entropy loss. Our method utilizes cotraining of a dual-branch network to suppress error accumulation resulting from confirmation bias. The proposed approach is end-to-end trainable and does not require any extra fine-tuning stage, unlike existing methods. Empirical results on standard domain adaptation datasets show that CABB outperforms existing state-of-the-art black-box DA models and is comparable to white-box domain adaptation models.},
  archive      = {J_TAI},
  author       = {Chowdhury Sadman Jahan and Andreas Savakis},
  doi          = {10.1109/TAI.2023.3330829},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2604-2614},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Curriculum guided domain adaptation in the dark},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Navigating data-centric artificial intelligence with
DC-check: Advances, challenges, and opportunities. <em>TAI</em>,
<em>5</em>(6), 2589–2603. (<a
href="https://doi.org/10.1109/TAI.2023.3345805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-centric artificial intelligence (AI) is an emerging paradigm that emphasizes the critical role of data in real-world machine learning (ML) systems—as a complement to model development. However, data-centric AI is still in its infancy, lacking a standardized framework that outlines necessary data-centric considerations at various stages of the ML pipeline: Data , Training , Testing , and Deployment . This lack of guidance hampers effective communication and design of data-centric driven ML systems. To address this critical gap, we introduce the Data-Centric Checklist (DC-Check), an actionable checklist-style framework that encapsulates data-centric considerations for ML systems. DC-Check is aimed at both practitioners and researchers to serve as a reference guide to data-centric AI development. Around each question in DC-Check, we discuss the applicability of different approaches, survey the state of the art, and highlight specific data-centric AI challenges and research opportunities. While developing DC-Check, we also undertook an analysis of the current data-centric AI landscape. The insights obtained from this exploration support the DC-Check framework, reinforcing its utility and relevance in the rapidly evolving field. To make DC-Check and related resources easily accessible, we provide a DC-Check companion website ( https://www.vanderschaar-lab.com/dc-check/ ), which will serve as a living resource, updated as methods and tools evolve.},
  archive      = {J_TAI},
  author       = {Nabeel Seedat and Fergus Imrie and Mihaela van der Schaar},
  doi          = {10.1109/TAI.2023.3345805},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2589-2603},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Navigating data-centric artificial intelligence with DC-check: Advances, challenges, and opportunities},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning for smart agriculture: A comprehensive
survey. <em>TAI</em>, <em>5</em>(6), 2568–2588. (<a
href="https://doi.org/10.1109/TAI.2023.3345278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As communication technologies and equipment evolve, smart assets become smarter. The agricultural industry is also evolving in line with the implementation of modern communication protocols, intelligent sensors, and equipment. This evolution is enabling large-scale agricultural production processes to operate independently, thus, securing the food supply chain for an ever-growing population. Data processing for such a system with multiple heterogeneous sources requires proper management for effective agricultural operations. Recognizing the advantages of machine learning (ML) in performing large-scale data processing, researchers are investigating the implementation of ML to design an effective intelligent agricultural architecture. The aim of this article is to provide a thorough analysis of the state-of-the-art in smart agriculture, open challenges, and guidelines for the development of further enhanced smart agriculture systems. Specifically, we describe how ML is used to create intelligent agricultural systems supported by state-of-the-art technology.},
  archive      = {J_TAI},
  author       = {M. Rezwanul Mahmood and Mohammad Abdul Matin and Sotirios K. Goudos and George Karagiannidis},
  doi          = {10.1109/TAI.2023.3345278},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2568-2588},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Machine learning for smart agriculture: A comprehensive survey},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robotic perception of transparent objects: A review.
<em>TAI</em>, <em>5</em>(6), 2547–2567. (<a
href="https://doi.org/10.1109/TAI.2023.3326120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transparent object perception is a rapidly developing research problem in artificial intelligence. The ability to perceive transparent objects enables robots to achieve higher levels of autonomy, unlocking new applications in various industries, such as healthcare, services, and manufacturing. Despite numerous datasets and perception methods being proposed in recent years, there is still a lack of in-depth understanding of these methods and the challenges in this field. To address this gap, this article provides a comprehensive survey of the platforms and recent advances for robotic perception of transparent objects. We highlight the main challenges and propose future directions of various transparent object perception tasks, i.e., segmentation, reconstruction, and pose estimation. We also discuss the limitations of existing datasets in diversity and complexity, and the benefits of employing multimodal sensors, such as RGB-D cameras, thermal cameras, and polarized imaging, for transparent object perception. Furthermore, we identify perception challenges in complex and dynamic environments, as well as for objects with changeable geometries. Finally, we provide an interactive online platform to navigate each reference.},
  archive      = {J_TAI},
  author       = {Jiaqi Jiang and Guanqun Cao and Jiankang Deng and Thanh-Toan Do and Shan Luo},
  doi          = {10.1109/TAI.2023.3326120},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2547-2567},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Robotic perception of transparent objects: A review},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continual learning: A review of techniques, challenges, and
future directions. <em>TAI</em>, <em>5</em>(6), 2526–2546. (<a
href="https://doi.org/10.1109/TAI.2023.3339091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual learning (CL), or the ability to acquire, process, and learn from new information without forgetting acquired knowledge, is a fundamental quality of an intelligent agent. The human brain has evolved into gracefully dealing with ever-changing circumstances and learning from experience with the help of complex neurophysiological mechanisms. Even though artificial intelligence takes after human intelligence, traditional neural networks do not possess the ability to adapt to dynamic environments. When presented with new information, an artificial neural network (ANN) often completely forgets its prior knowledge, a phenomenon called catastrophic forgetting or catastrophic interference. Incorporating CL capabilities into ANNs is an active field of research and is integral to achieving artificial general intelligence. In this review, we revisit CL approaches and critically examine their strengths and limitations. We conclude that CL approaches should look beyond mitigating catastrophic forgetting and strive for systems that can learn, store, recall, and transfer knowledge, much like the human brain. To this end, we highlight the importance of adopting alternative brain-inspired data representations and learning algorithms and provide our perspective on promising new directions where CL could play an instrumental role.},
  archive      = {J_TAI},
  author       = {Buddhi Wickramasinghe and Gobinda Saha and Kaushik Roy},
  doi          = {10.1109/TAI.2023.3339091},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2526-2546},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Continual learning: A review of techniques, challenges, and future directions},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GhostViT: Expediting vision transformers via cheap
operations. <em>TAI</em>, <em>5</em>(6), 2517–2525. (<a
href="https://doi.org/10.1109/TAI.2023.3326795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Transformers (ViTs) have recently achieved promising results in various computer vision tasks. However, ViTs have high computation costs and a large number of parameters due to the stacked multihead self-attention (MHSA) and expanded feed-forward network (FFN) modules. Since the complexity of Transformer-based models is quadratic with the length of the input tokens, most current efforts focus on reducing the number of tokens in ViTs to improve the model efficiency. Unlike previous studies, we argue that diverse redundant features help ViTs understand the data comprehensively. In this article, we propose ghost vision Transformer (GhostViT), which achieves both computation and storage efficiency. The key concept of GhostViT is to generate more diverse features using cheap operations in the MHSA and FFN modules. We experimentally demonstrate that our GhostViT can significantly reduce both the parameters and floating point operations (FLOPs) of ViTs while achieving similar or better accuracy. For example, about 14% of parameters and 17% of FLOPs of the DeiT-tiny model are reduced without any accuracy loss on the ImageNet-1 K dataset.},
  archive      = {J_TAI},
  author       = {Hu Cao and Zhongnan Qu and Guang Chen and Xinyi Li and Lothar Thiele and Alois Knoll},
  doi          = {10.1109/TAI.2023.3326795},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2517-2525},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {GhostViT: Expediting vision transformers via cheap operations},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable attention pruning: A metalearning-based
approach. <em>TAI</em>, <em>5</em>(6), 2505–2516. (<a
href="https://doi.org/10.1109/TAI.2024.3363686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pruning, as a technique to reduce the complexity and size of transformer-based models, has gained significant attention in recent years. While various models have been successfully pruned, pruning bidirectional encoder representations from transformers (BERT) poses unique challenges due to their fine-grained structure and overparameterization. However, by carefully considering these factors, it is possible to prune BERT without significantly degrading its pretrained loss. In this article, we propose a metalearning-based pruning approach that can adaptively identify and eliminate insignificant attention weights. The performance of the proposed model is compared with several baseline models, as well as the default fine-tuned BERT model. The baseline pruning strategies employed low-level pruning techniques, targeting the removal of only 20% of the connections. The experimental results show that the proposed model outperforms the other baseline models, in terms of lower inference latency, higher MCC, and lower loss. However, there is no significant improvement observed in terms of average floating-point operations per second (FLOPs). Furthermore, we conduct a comparative evaluation of the baseline models and our proposed model using two explainable artificial intelligence (XAI) approaches. While other models allocate reasonable attention to less significant words for sentiment classification, our model assigns higher probabilities to the most significant sentimental words.},
  archive      = {J_TAI},
  author       = {Praboda Rajapaksha and Noel Crespi},
  doi          = {10.1109/TAI.2024.3363686},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2505-2516},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Explainable attention pruning: A metalearning-based approach},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dense multicross self-attention and adaptive gated
perceptual unit method for few-shot semantic segmentation. <em>TAI</em>,
<em>5</em>(6), 2493–2504. (<a
href="https://doi.org/10.1109/TAI.2024.3369553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot semantic segmentation (FSSS) is a pivotal and prevalent research task for advancing the field of artificial intelligence. The task entails learning to differentiate between various classes in a support set and leveraging this knowledge on samples within a query set. However, traditional deep learning methods tend to underperform in this context due to limited training samples and subtle correlations between query and support images that are inadequately utilized. Existing methods for FSSS often compress support information into prototype categories or utilize only partial pixel-level support information, resulting in a significant impact. In this article, we propose a novel auto FSSS method that employs dense multicross self-attention and adaptive gate perception units to tackle this challenge. Specifically, our proposed method treats each query pixel as a label and predicts its segmentation label as the sum of labels of all support pixels. The method fully utilizes foreground and background support information through multilevel pixel correlations between paired query and support features to achieve state-of-the-art performance with only 1–5 annotated images. Moreover, our proposed adaptive gating perception unit filters and weighs each support image information by adaptively learning the gating values. This ensures the model selects only the most relevant support image information to the current query image. The proposed method is evaluated on several popular FSSS datasets and compared with state-of-the-art methods. Additionally, a visual analysis of our method is conducted to demonstrate its ability to distinguish different semantic categories and exhibit robustness at segmentation boundaries.},
  archive      = {J_TAI},
  author       = {Feng Xiao and Ruyu Liu and Yunrui Zhu and Haoyu Zhang and Jianhua Zhang and Shengyong Chen},
  doi          = {10.1109/TAI.2024.3369553},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2493-2504},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A dense multicross self-attention and adaptive gated perceptual unit method for few-shot semantic segmentation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An automated few-shot learning for time-series forecasting
in smart grid under data scarcity. <em>TAI</em>, <em>5</em>(6),
2482–2492. (<a href="https://doi.org/10.1109/TAI.2024.3358795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microgrid can improve greenhouse gas emissions and reduce operational costs. To forecast both energy generation and load demand, time-series prediction has been a key tool in real-time control and optimization. Developing an adequate predictive model is difficult when there is a lack of historical data. Moreover, hyperparameters have a tangible impact on the performance of machine learning models. Bearing these considerations in mind, this article develops a BiLO-Auto-TSF/ML framework that automates the optimal design of a few-shot learning pipeline from a bilevel programming perspective. Specifically, a lower level metalearner helps mitigate the small data challenge, whereas an upper level optimization optimizes both hyperparameters for lower and upper level learners. Note that the proposed framework is designed to accommodate a wide range of machine learning methods, allowing for easy integration through a plug-in mechanism. Comprehensive experiments demonstrate the effectiveness of our proposed BiLO-Auto-TSF/ML to search for a high-performance few-shot learning pipeline for various energy sources.},
  archive      = {J_TAI},
  author       = {Jiangjiao Xu and Ke Li and Dongdong Li},
  doi          = {10.1109/TAI.2024.3358795},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2482-2492},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An automated few-shot learning for time-series forecasting in smart grid under data scarcity},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning-driven dynamic multimodal optimization algorithm
for real-time traceability of water pollution. <em>TAI</em>,
<em>5</em>(6), 2472–2481. (<a
href="https://doi.org/10.1109/TAI.2024.3355027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, water quality safety problems caused by sudden urban drinking water contamination events have attracted the attention of experts in China and abroad. After an occurrence of urban water pollution, it is challenging to locate the pollution source in real time according to the information collected by water quality sensors and then quickly deduce the injection location, injection concentration quality, and other characteristics of the pollution source. In this article, we propose a learning-driven dynamic multimodal optimization algorithm framework that combines various machine learning algorithms. First, it uses the support vector machine (SVM) model to scale down and perform node probability estimation for a large-scale water supply pipeline network. Second, by predicting the uncertainty parameters of the pipe network when setting the pipe network simulation parameters, the framework can narrow the gap between simulation and real conditions, giving the pollution source characteristics obtained by the algorithm solution a higher confidence level. The experimental results show that the algorithm framework can achieve real-time traceability of water pollution for large-scale, uncertain pipe network environments, and can obtain better accuracy and real-time performance than other dynamic algorithms.},
  archive      = {J_TAI},
  author       = {Xuesong Yan and Xing Guo and Jin Chen and Chengyu Hu and Wenyin Gong and Liang Gao},
  doi          = {10.1109/TAI.2024.3355027},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2472-2481},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Learning-driven dynamic multimodal optimization algorithm for real-time traceability of water pollution},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-guided autoencoders for unsupervised change detection
in heterogeneous remote sensing images. <em>TAI</em>, <em>5</em>(6),
2458–2471. (<a href="https://doi.org/10.1109/TAI.2024.3357667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the problem of enormous differences in two heterogeneous images, the traditional unsupervised frameworks are most normally realized by converting two images into a common domain with various auxiliary strategies, such as transformation and alignment, which requires extensive calculation and has difficulty balancing the training tasks. To achieve a concise framework, this article proposes self-guided autoencoders (SGAE) for unsupervised change detection (CD) in heterogeneous remote sensing images (RSIs). Unlike traditional methods that aim to narrow the differences of heterogeneous images to highlight the changed information, SGAE forces the flow of identification in formation generated in unlabeled data through self-guided iterations. First, initial unsupervised networks output an elementary change map that will be screened to obtain reliable pseudolabels. The selected pseudolabeled samples will be used as the input of a supervised network to obtain another change map. Then, multiple change maps will be fused to refine the confidence of pseudolabels again, obtaining new fused pseudolabeled samples for the self-guided network, which will be trained with pseudolabeled samples and unlabeled samples. Finally, the above operations will be repeated to continuously optimize the net, which helps itself to extract the discriminative features for classification in self-guided iterations. Experiments compared with several algorithms on four datasets demonstrate the effectiveness and robustness of our method, which can help unsupervised models improve discriminative feature extraction and classification performance with a more flexible learning method.},
  archive      = {J_TAI},
  author       = {Jiao Shi and Tiancheng Wu and Alex Kai Qin and Yu Lei and Gwanggil Jeon},
  doi          = {10.1109/TAI.2024.3357667},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2458-2471},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Self-guided autoencoders for unsupervised change detection in heterogeneous remote sensing images},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guest editorial: AutoML for nonstationary data.
<em>TAI</em>, <em>5</em>(6), 2456–2457. (<a
href="https://doi.org/10.1109/TAI.2024.3387583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The five papers in this special section address different aspects of automated machine learning (AutoML) from fundamental algorithms to real-world applications. Developing high-performance machine learning models is a difficult task that usually requires expertise from data scientists and knowledge from domain experts. To make machine learning more accessible and ease the labor-intensive trial-and-error process of searching for the most appropriate machine learning algorithm and the optimal hyperparameter setting, AutoML was developed and has become a rapidly growing area in recent years. AutoML aims at automation and efficiency of the machine learning process across domains and applications. Nowadays, data is commonly collected over time and susceptible to changes, such as in Internet-of-Things (IoT) systems, mobile phone applications and healthcare data analysis. It poses new challenges to the traditional AutoML with the assumption of data stationarity. Interesting research questions arise around whether, when and how to effectively and efficiently deal with non-stationary data in AutoML.},
  archive      = {J_TAI},
  author       = {Ran Cheng and Hugo Jair Escalante and Wei-Wei Tu and Jan N. Van Rijn and Shuo Wang and Yun Yang},
  doi          = {10.1109/TAI.2024.3387583},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {2456-2457},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Guest editorial: AutoML for nonstationary data},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Broadcast gossip algorithm for nash equilibrium seeking of
nonaggregative games. <em>TAI</em>, <em>5</em>(5), 2444–2452. (<a
href="https://doi.org/10.1109/TAI.2023.3326113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we focus on seeking the Nash equilibrium (NE) of nonaggregative games over a directed communication network. An asynchronous broadcast-based gossip algorithm is proposed, where each player broadcasts its own decision and estimates of other players&#39; decisions to its out-neighbors. Then, the neighboring players update its respective decisions using its independent stepsize and estimates of other players&#39; decisions by weighting its own and the received decision estimates from $(0,\,1)$ . The almost surely convergence of the proposed algorithm with the diminishing stepsizes is presented by using a lemma and the property of the Kronecker product. Furthermore, the expected error bound between the NE and decisions for the proposed algorithm with constant stepsizes is presented with the increasing active probability of each player. Finally, two numerical examples are provided to illustrate the effectiveness of the proposed algorithm and the convergence rate comparison.},
  archive      = {J_TAI},
  author       = {Dong Wang and Feiyue Wu and Xiaopeng Xu and Changyun Wen and Jie Lian and Jun Zhou},
  doi          = {10.1109/TAI.2023.3326113},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2444-2452},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Broadcast gossip algorithm for nash equilibrium seeking of nonaggregative games},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A framework for modeling and optimization of data-driven
energy systems using machine learning. <em>TAI</em>, <em>5</em>(5),
2434–2443. (<a href="https://doi.org/10.1109/TAI.2023.3322395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces an innovative framework for solar energy optimization. This approach delves into the multifaceted layers and components of neural networks (NNs), elucidating their complexities and interconnections. The proposed framework strategically combines tailored algorithms and processes to address the optimization problem. The methodology ultimately leads to selecting an intelligent model that guarantees superior performance, accuracy, and adaptability across a broad spectrum of interdisciplinary applications. It offers valuable insights to researchers and practitioners striving to leverage NNs for complex data-driven solutions. Furthermore, this study introduces an online simulation tool powered by a new algorithm to monitor the optimal generation capacity of solar systems. This tool employs a uniquely designed optimization model architecture to track alterations in system output in response to environmental and input variable changes. This study presents a mathematical model of the system in Python to enhance accessibility and adaptability, allowing for easy adoption across various applications. This work is a straightforward reference, bridging the gap between research-oriented and tutorial methodologies.},
  archive      = {J_TAI},
  author       = {Mir Sayed Shah Danish},
  doi          = {10.1109/TAI.2023.3322395},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2434-2443},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A framework for modeling and optimization of data-driven energy systems using machine learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leverage learning behaviour data for students’ learning
performance prediction and influence factor analysis. <em>TAI</em>,
<em>5</em>(5), 2422–2433. (<a
href="https://doi.org/10.1109/TAI.2023.3320118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online education has become increasingly significant for university students and faculty, especially in the context of the modern remote education landscape. However, the inherent space-time separation in online education can create communication delays between teachers and students, making it challenging to monitor students&#39; behaviors effectively. In an effort to understand the connection between students&#39; online engagement and their learning performance, data annotation has been implemented to address the issue of accurately representing students&#39; learning behaviors. Taking the online teaching data of Shanghai Normal University platform as the research object, the primary online education problem is explored through data mining, which includes correlation analysis, Gini importance ranking, and principal component analysis (PCA). Then, constructing the learning performance prediction model using random forest (RF) based on PCA by comparing various machine learning algorithms. As a consequence, the most influential online learning behaviors are course duration time, document learning time, test average score, and video completion rate. The overall classification accuracy of the learning performance prediction model is 87.45%, and the highest prediction accuracy for a single category is 96.52%.},
  archive      = {J_TAI},
  author       = {Qin Ni and Yanjin Zhu and Lingying Zhang and Xiaochen Lu and Lei Zhang},
  doi          = {10.1109/TAI.2023.3320118},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2422-2433},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Leverage learning behaviour data for students&#39; learning performance prediction and influence factor analysis},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nuclear norm maximization-based curiosity-driven
reinforcement learning. <em>TAI</em>, <em>5</em>(5), 2410–2421. (<a
href="https://doi.org/10.1109/TAI.2023.3323628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) has achieved promising results in solving numerous challenging sequential decision problems. To address the issue of sparse extrinsic rewards, researchers have proposed intrinsic rewards, enabling the agent to acquire skills that may prove valuable in the pursuit of future rewards. One representative approach for generating intrinsic rewards involves constructing a predictive model to assess the novelty of states. However, due to the stochastic nature of complex environments, intrinsic rewards can be noisy. Directly employing noisy forward predictions to supervise policies can be detrimental to learning performance and efficiency. Many recent studies utilize the $\ell _{2}$ norm or variance to measure novelty, which further amplifies the noise through squaring operations. In this article, we aim to tackle these challenges by leveraging nuclear norm maximization (NNM). Specifically, we propose a novel curiosity reward that accurately quantifies the novelty of the exploration environment while exhibiting a high tolerance for noise and outliers. Our extensive experiments in various benchmark environments demonstrate that NNM achieves state-of-the-art performance compared with previous curiosity-based methods. When trained solely with intrinsic rewards, NNM achieves a human-normalized score of 1.09 on a subset of 26 Atari games, twice the performance of methods based on competitive intrinsic rewards.},
  archive      = {J_TAI},
  author       = {Chao Chen and Yuanzhao Zhai and Zijian Gao and Kele Xu and Sen Yang and Yiying Li and Bo Ding and Dawei Feng and Huaimin Wang},
  doi          = {10.1109/TAI.2023.3323628},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2410-2421},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Nuclear norm maximization-based curiosity-driven reinforcement learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event-triggered fixed-time adaptive fuzzy control for
nontriangular nonlinear systems with unknown control directions.
<em>TAI</em>, <em>5</em>(5), 2397–2409. (<a
href="https://doi.org/10.1109/TAI.2023.3318895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the problem of fixed-time adaptive fuzzy control for a class of nontriangular nonlinear systems with unknown control directions under the event-triggered framework. To tackle the algebraic loop issue arising from the nontriangular structure, the property of the fuzzy logic system is employed. The command filter and the error compensation signals, which have the feature of fixed-time convergence, are constructed to eliminate the “explosion of complexity” phenomenon and the effect of filtered error, respectively. By integrating the Nussbaum gain technique into the relative threshold-based event-triggered mechanism, a singularity-free fixed-time adaptive fuzzy control scheme is devised to ensure that the closed-loop system is practically fixed-time stable, and the tracking error converges to a small region near the origin in a fixed time. Finally, simulation examples are presented to demonstrate the effectiveness and superiority of the proposed fixed-time control algorithm.},
  archive      = {J_TAI},
  author       = {Guozeng Cui and Hui Xu and Jinpeng Yu and Qian Ma and Muwei Jian},
  doi          = {10.1109/TAI.2023.3318895},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2397-2409},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Event-triggered fixed-time adaptive fuzzy control for nontriangular nonlinear systems with unknown control directions},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Double-quantitative feature selection approach for
multigranularity ordered decision systems. <em>TAI</em>, <em>5</em>(5),
2385–2396. (<a href="https://doi.org/10.1109/TAI.2023.3319301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Double-quantitative-based granular computing implies the systematic perspective, completeness, and accuracy of rough approximation. However, most of the existing research works only focus on the case of single quantification, and there are few research study on the simultaneous computing method of double quantification. In this article, we explore feature selection with double quantification in multigranularity ordered decision systems (MG-ODSs). First, the related concepts of quantitative functions are interpreted from different viewpoints of relative and absolute quantification. Then, the multigranularity double-quantitative rough sets in an ordered decision system (ODS) from optimistic and pessimistic cases, the related properties, and three-way decisions based on the presented quantitative levels are discussed. Furthermore, the greedy algorithm for feature selection is derived. By using 12 datasets from a public repository, evaluations and comparisons are made on the parameter setting and classification accuracy. From these comparative experiments, the advantages and effectiveness of the proposed feature selection algorithm could be demonstrated over the existing approaches.},
  archive      = {J_TAI},
  author       = {Wentao Li and Chaojun Deng and Witold Pedrycz and Oscar Castillo and Chao Zhang and Tao Zhan},
  doi          = {10.1109/TAI.2023.3319301},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2385-2396},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Double-quantitative feature selection approach for multigranularity ordered decision systems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An end-to-end two-branch network towards robust video
fingerprinting. <em>TAI</em>, <em>5</em>(5), 2371–2384. (<a
href="https://doi.org/10.1109/TAI.2023.3318888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing number of edited videos, many robust video fingerprinting schemes have been proposed to solve the problem of video content authentication. However, most of them either deal with the temporal and spatial features symmetrically or insufficiently consider the temporal information. In this work, an end-to-end two-branch network toward robust video fingerprinting (RVFNet) is proposed, where the two branches focus on the temporal and spatial information, respectively. The temporal branch aims to comprehensively capture complex motion patterns by combining subtle motion changes with the overall motion trend. The spatial branch exploits the pixel-level information obtained by multiple receptive fields while preserving significant structural features. Deep metric learning is employed in the training process, and we adopt hard triplet loss to constrain the generation of fingerprints. Furthermore, we construct a large-scale and complex dataset for the robust video fingerprinting task based on multiple video content-preserving manipulations in actual scenarios. The size of our dataset exceeds most datasets adopted in the current robust video fingerprinting schemes. Based on the proposed dataset, experimental results demonstrate that our scheme achieves outstanding performance improvements compared with the state of the art.},
  archive      = {J_TAI},
  author       = {Yingying Xu and Yuanding Zhou and Xinran Li and Gejian Zhao and Chuan Qin},
  doi          = {10.1109/TAI.2023.3318888},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2371-2384},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An end-to-end two-branch network towards robust video fingerprinting},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting nash equilibria in bimatrix games using a robust
bichannel convolutional neural network. <em>TAI</em>, <em>5</em>(5),
2358–2370. (<a href="https://doi.org/10.1109/TAI.2023.3321584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider the problem of finding a Nash equilibrium in bimatrix games. Traditional solution methods, including the Lemke–Howson (LH) algorithm and enumeration methods, rely on iterative approaches, which leads to computational inefficiency, especially when dealing with multiple instances. To address this problem, we introduce a bichannel convolutional neural network (BiCNN) that receives a bimatrix game as an input and generates two mixed strategies as a predicted Nash equilibrium. To improve the robustness of the BiCNN model, we propose a novel training algorithm that uses bimatrix games generated from different game sizes and probability distributions. Finally, our experimental results show that the proposed approach offers a significant computational advantage while maintaining acceptable prediction accuracy.},
  archive      = {J_TAI},
  author       = {Dawen Wu and Abdel Lisser},
  doi          = {10.1109/TAI.2023.3321584},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2358-2370},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Predicting nash equilibria in bimatrix games using a robust bichannel convolutional neural network},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy-preserving human activity recognition system for
assisted living environments. <em>TAI</em>, <em>5</em>(5), 2342–2357.
(<a href="https://doi.org/10.1109/TAI.2023.3323272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic human activity recognition has numerous applications, especially in elderly support and healthcare. Several approaches for human activity recognition (HAR) using a variety of sensors are available in literature. While such frameworks are effective, each has limitations related to privacy, convenience, cost, and performance. In this article, a robust framework for automatic human activity recognition is proposed that uses depth sensors that preserve privacy and are cost-effective. The depth sensors provide two data modalities, namely, depth maps and skeleton sequences, used together for activity recognition. Two novel descriptors, joint position descriptor based on the position of joints; and bone angle descriptor based on bone inclination, are generated from the skeleton sequence data. The descriptors convey both spatial and temporal information and are scale and view-point invariant. Depth video clips are used along with the descriptors to deal with the issue of noisy and missing skeleton sequences. The data modalities and descriptors are fused using a two-level fusion strategy for a multichannel convolutional neural network (CNN) framework. The proposed system is validated and shown to be superior to the existing state of the art through comparisons over four widely used public datasets. A computational complexity analysis of the system confirms its efficacy in real time. A prototypical implementation of the proposed system further validates its practicability.},
  archive      = {J_TAI},
  author       = {Ankit Jain and Rajendra Akerkar and Abhishek Srivastava},
  doi          = {10.1109/TAI.2023.3323272},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2342-2357},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Privacy-preserving human activity recognition system for assisted living environments},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incremental object detection with image-level labels.
<em>TAI</em>, <em>5</em>(5), 2331–2341. (<a
href="https://doi.org/10.1109/TAI.2023.3318891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incremental object detection (IOD) aims to achieve simultaneous prediction of old and new samples on localization and classification when new concepts are provided. It is a challenging task due to the need for a joint interpretation of semantic and spatial information. While the existing work accomplishes the detector generalization with the help of annotated sample classes and bounding boxes, this article presents one astonishing finding on the unnecessity of new bounding boxes, which will significantly reduce the annotation cost and condition constraints in IOD. To enable the incremental detection process with image-level labels, we propose a multibranch decoupling scheme in which the representation, classification, and regression branches are customized to accommodate new concepts with class labels with different semantic levels. The regression branch is first frozen to ensure generalizable localization while maintaining the stability of representation optimization. Then, the representation branch is gradient-inverted to estimate the distribution drift from the old object-level set to the new image-level one, improving the effectiveness of feature updates in the absence of homogeneous semantic and spatial supervision. Finally, the classification branch is calibrated to balance the contribution of different images to the gradient, in which the frequency ratios of old instances and new images are calculated as a postprocessing factor for the logits margin. Extensive experiments on two standard incremental detection benchmarks demonstrate the above-par recognition and localization performance while reducing the burden on new class annotation, outperforming state-of-the-art methods by 2% and 3%, respectively.},
  archive      = {J_TAI},
  author       = {Qianzi Yu and Kai Zhu and Wen Wang and Yang Cao and Yu Kang},
  doi          = {10.1109/TAI.2023.3318891},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2331-2341},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Incremental object detection with image-level labels},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Appearance enhancement for camera-captured document images
in the wild. <em>TAI</em>, <em>5</em>(5), 2319–2330. (<a
href="https://doi.org/10.1109/TAI.2023.3321257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camera-captured document images usually suffer from various appearance degradations, which hamper the clarity of content and preclude subsequent analysis and recognition systems. Most of the existing methods are tailored for one or relatively few degradations, making them feasible only in limited scenarios. However, in real-world applications, degradations are more diverse, and different degradations may arise simultaneously in a single image. To remedy this limitation, we aimed to achieve appearance enhancement for camera-captured document images in the wild, where degradations exhibit more diversity and may coexist simultaneously within the same image. To realize this, we propose a new end-to-end neural network called GCDRNet, which consists of two cascaded subnets, global context learning network (GC-Net) and detail restoration network (DR-Net). The GC-Net is used for global context modeling, and the DR-Net is used for detail restoration through a multiscale and multiloss training strategy. To train and validate GCDRNet in real-world scenarios, we constructed a new benchmark called real-world document image appearance enhancement (RealDAE), which contains 600 real-world degraded document images that are carefully annotated with pixelwise alignment. To the best of our knowledge, RealDAE is the first dataset that targets multiple degradations in the wild. Extensive experiments validated the superiority and advancement of our GCDRNet and RealDAE compared to the existing methods and datasets, respectively. In addition, experiments also demonstrated that image appearance enhancement as a preprocessing procedure can effectively improve the performance of downstream tasks, such as text detection and recognition.},
  archive      = {J_TAI},
  author       = {Jiaxin Zhang and Lingyu Liang and Kai Ding and Fengjun Guo and Lianwen Jin},
  doi          = {10.1109/TAI.2023.3321257},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2319-2330},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Appearance enhancement for camera-captured document images in the wild},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A knowledge flow empowered cognitive framework for decision
making with task-agnostic data regulation. <em>TAI</em>, <em>5</em>(5),
2304–2318. (<a href="https://doi.org/10.1109/TAI.2023.3322391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extreme complexity of many real-world tasks poses considerable challenges to agents&#39; decision making. Most existing models only rely on task-related data for knowledge learning, while ignoring the important influence of potential task-agnostic factors. Effective learning coupled with both task-related and task-agnostic data can strongly enrich the agent&#39;s knowledge and improve its decision making performance. Furthermore, many existing learning models simply leverage data to learn knowledge but fail to express the thought process of decision making as humans do, which significantly limits their explanatory capability. To this end, we propose a novel knowledge flow empowered cognitive framework for real-world tasks. To obtain more reliable and trustworthy knowledge, a bottom-up knowledge learning model is developed, which incorporates both task-related data and task-agnostic data for comprehensive knowledge accumulation and value assessment of influencing factors. To demonstrate the thought process of decision making, a top-down knowledge expression model is proposed to coordinate different influencing factors by a knowledge flow structure. Two real-world case studies, including traffic anomaly detection and vehicle following anomaly detection, are introduced, where task-agnostic data are presented for the first time in both tasks. Experimental evaluation demonstrates the strong necessity of incorporating task-agnostic data in knowledge accumulation for real-world tasks and the effectiveness of our cognitive framework.},
  archive      = {J_TAI},
  author       = {Liming Huang and Yulei Wu and Niccolò Tempini},
  doi          = {10.1109/TAI.2023.3322391},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2304-2318},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A knowledge flow empowered cognitive framework for decision making with task-agnostic data regulation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse vicious attacks on graph neural networks.
<em>TAI</em>, <em>5</em>(5), 2293–2303. (<a
href="https://doi.org/10.1109/TAI.2023.3319306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we introduce SAVAGE, a novel framework for sparse vicious adversarial link prediction attacks in graph neural networks (GNNs). While GNNs have been successful in link prediction tasks, they are susceptible to adversarial attacks where malicious nodes attempt to manipulate recommendations for a target victim. SAVAGE optimizes the attacker&#39;s goal to maximize attack effectiveness while minimizing the required malicious resources. Unlike existing methods with static resource-based upper bounds, SAVAGE employs a sparsity-enforcing mechanism to reduce the number of malicious nodes needed for the attack. Extensive experiments on real-world and synthetic datasets demonstrate the optimal tradeoff achieved by SAVAGE between a high attack success rate and the number of malicious nodes utilized. Furthermore, we demonstrate that SAVAGE can successfully target non-GNN-based link prediction systems, even those unknown at the time of the attack. This showcases the transferability of SAVAGE-generated attacks to other black-box methods for link prediction, highlighting its applicability across different real-world scenarios.},
  archive      = {J_TAI},
  author       = {Giovanni Trappolini and Valentino Maiorca and Silvio Severino and Emanuele Rodolà and Fabrizio Silvestri and Gabriele Tolomei},
  doi          = {10.1109/TAI.2023.3319306},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2293-2303},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Sparse vicious attacks on graph neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiperson activity recognition and tracking based on
skeletal keypoint detection. <em>TAI</em>, <em>5</em>(5), 2279–2292. (<a
href="https://doi.org/10.1109/TAI.2023.3318575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, most action recognition networks have deep overall structures, large model parameters, and high requirements for computer hardware equipment. As a result, it is easy to overfit in the recognition process for too deep network layers. Furthermore, it is also difficult to extract features because of the video&#39;s interference information, such as illumination and occlusion. To solve the above problems, we propose a multiperson action recognition and tracking algorithm based on skeletal keypoint detection. First, the n network combining the improved dense convolutional network and part affinity field is used to extract the skeletal information points of the human body. Then, we present an improved DeepSort network for multiperson target tracking, which contains a Hungarian matching algorithm based on the generalized intersection over union and a pedestrian reidentification network combining GhostNet and feature pyramid network. Finally, we construct a deep neural network model to classify the extracted human skeletal information and realize action recognition. Experimental results show that the multiperson action recognition and tracking algorithm achieves an action recognition accuracy of 98%. In addition, the multitarget tracking accuracy of the proposed algorithm is improved by 4.2% on the MOT16 dataset. Compared with other common algorithms, the proposed algorithm can achieve high accuracy in detecting keypoints of the human body and improve the accuracy of multiperson action recognition with fewer parameters and complexity of operations.},
  archive      = {J_TAI},
  author       = {Hai-Sheng Li and Jing-Yin Chen and Hai-ying Xia},
  doi          = {10.1109/TAI.2023.3318575},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2279-2292},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multiperson activity recognition and tracking based on skeletal keypoint detection},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Time-aware and context-sensitive ensemble learning for
sequential data. <em>TAI</em>, <em>5</em>(5), 2264–2278. (<a
href="https://doi.org/10.1109/TAI.2023.3319308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate sequential time series data through ensemble learning. Conventional ensemble algorithms and the recently introduced ones have provided significant performance improvements in widely publicized time series prediction competitions for stationary data. However, recent studies are inadequate in capturing the temporally varying statistics for nonstationary data. To this end, we introduce a novel approach using a metalearner that effectively combines base learners in both a time-varying and context-dependent manner. Our approach is based on solving a weight optimization problem that minimizes a specific loss function with constraints on the linear combination of the base learners. The constraints are theoretically analyzed under known statistics and integrated into the learning procedure of the metalearner as part of the optimization in an automated manner. We demonstrate significant performance improvements on real-life data and well-known competition datasets over the widely used conventional ensemble methods and the state-of-the-art forecasting methods in the machine learning literature. Furthermore, we openly share the source code of our method to facilitate further research and comparison.},
  archive      = {J_TAI},
  author       = {Arda Fazla and Mustafa E. Aydin and Suleyman S. Kozat},
  doi          = {10.1109/TAI.2023.3319308},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2264-2278},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Time-aware and context-sensitive ensemble learning for sequential data},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incremental reinforcement learning via performance
evaluation and policy perturbation. <em>TAI</em>, <em>5</em>(5),
2253–2263. (<a href="https://doi.org/10.1109/TAI.2023.3316637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid adaptation to the environment is the long-term task of reinforcement learning. However, reinforcement learning faces great challenges in dynamic environments, especially with continuous state–action spaces. In this article, we propose a systematic incremental reinforcement learning method via performance evaluation and policy perturbation (IRL-PEPP) to improve the adaptability of reinforcement learning algorithms in dynamic environments with continuous state–action spaces, which mainly includes three parts, i.e., performance evaluation, policy perturbation, and importance weighting. First, in performance evaluation, we apply the learned optimal policy to sample a few episodes in the original environment and use these samples to evaluate the policy applicability in the new environment. Then, in policy perturbation, the policy is perturbed according to the policy applicability to balance the tradeoff between exploration and exploitation in the new environment. Finally, importance weighting is applied to weight the information to speed up the adjustment process of the policy. Experimental results demonstrate the feasibility and efficiency of the proposed IRL-PEPP method for continuous control tasks in comparison with the existing state-of-the-art methods.},
  archive      = {J_TAI},
  author       = {Guizhou Deng and Huiqiao Fu and Xinpeng Wang and Canghai Liu and Kaiqiang Tang and Chunlin Chen},
  doi          = {10.1109/TAI.2023.3316637},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2253-2263},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Incremental reinforcement learning via performance evaluation and policy perturbation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). An accurate and interpretable framework for trustworthy
process monitoring. <em>TAI</em>, <em>5</em>(5), 2241–2252. (<a
href="https://doi.org/10.1109/TAI.2023.3319606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trustworthy process monitoring seeks to build an accurate and interpretable monitoring framework, which is critical for ensuring the safety of energy conversion plant (ECP) that operates under extreme working conditions, such as high pressure and temperature. Contemporary self-attentive models, however, fall short in this domain for two main reasons. First, they rely on stepwise correlations that fail to involve physically meaningful semantics in ECP logs, resulting in suboptimal accuracy and interpretability. Second, attention matrices are frequently cluttered with spurious correlations that obscure physically meaningful ones, further impeding effective interpretation. To overcome these issues, we propose AttentionMixer, a framework aimed at improving both accuracy and interpretability of the existing methods, and establish a trustworthy ECP monitoring framework. Specifically, to tackle the first issue, we employ a spatial adaptive message passing block to capture variatewise correlations. This block is coupled with a temporal adaptive message passing block through a mixing operator, yielding a multifaceted representation of ECP logs accounting for both the stepwise and variatewise correlations. Concurrently, to tackle the second issue, we employ a sparse message passing regularizer to filter out spurious correlations. We validate the efficacy of AttentionMixer using two real-world datasets from the radiation monitoring network for Chinese nuclear power plants.},
  archive      = {J_TAI},
  author       = {Hao Wang and Zhiyu Wang and Yunlong Niu and Zhaoran Liu and Haozhe Li and Yilin Liao and Yuxin Huang and Xinggao Liu},
  doi          = {10.1109/TAI.2023.3319606},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2241-2252},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An accurate and interpretable framework for trustworthy process monitoring},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive federated relevance framework for
spatial–temporal graph learning. <em>TAI</em>, <em>5</em>(5), 2227–2240.
(<a href="https://doi.org/10.1109/TAI.2023.3316629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial–temporal data contains rich information and has been widely studied recently due to the rapid development of relevant applications. For instance, medical institutions often use electrodes attached to different parts of a patient to analyse the electroencephalogram data with rich spatial and temporal features for health assessment and disease diagnosis. Existing research has mainly used deep learning techniques such as convolutional neural network (CNN) or recurrent neural network (RNN) to extract spatial–temporal features. Yet, it is challenging to incorporate both inter–dependencies spatial information and dynamic temporal changes simultaneously. In reality, for a model that leverages these spatial–temporal features to fulfil complex prediction tasks, it often requires considerable training data to obtain satisfactory model performance. To address the challenges at hand, we propose an adaptive federated relevance framework called FedRel for spatial–temporal graph learning. After transforming the raw spatial–temporal data into high–quality features, the core Dynamic Inter–Intra Graph (DIIG) module in the framework leverages these features to generate the spatial–temporal graphs capable of capturing both the hidden topological and long–term temporal correlation information. To further enhance the model&#39;s generalization ability and performance, while still prioritizing local data privacy, we have incorporated a relevance–driven federated learning module into our framework. This module leverages the diverse data distributions from different participants and applies attentive aggregations of respective models. We then conducted extensive experiments on two real–world spatial–temporal datasets. The results demonstrate the framework&#39;s efficacy in spatial–temporal interpretation, collaborative model training, and handling divergent data distributions across various comparison settings.},
  archive      = {J_TAI},
  author       = {Tiehua Zhang and Yuze Liu and Zhishu Shen and Rui Xu and Xin Chen and Xiaowei Huang and Xi Zheng},
  doi          = {10.1109/TAI.2023.3316629},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2227-2240},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An adaptive federated relevance framework for Spatial–Temporal graph learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal knowledge graph for food risk prediction.
<em>TAI</em>, <em>5</em>(5), 2217–2226. (<a
href="https://doi.org/10.1109/TAI.2023.3321590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of food safety risk is an effective measure to improve food risk prevention. The complex and continuous characteristics of food safety influence various factors; thus, a food safety risk prediction model based on a temporal knowledge graph is proposed in this article. First, a food safety risk dataset is constructed by collecting food supervision and management sampling data used in daily life from 2018 to 2021 from the State Administration for Market Regulation. The dataset contains five categories: fruits, vegetables, meat, aquatic products, and dairy products. Then, a novel food safety temporal knowledge graph is designed based on the proposed index system because food safety data have temporal characteristics. A temporal knowledge graph network is proposed to build a food safety risk prediction model, which comprises historical learning and generation methods. The proposed food safety temporal knowledge graph can predict the food risk level and types of hazardous substances in a certain period. Finally, the comparative experiments on the food safety dataset constructed in this article showed that the proposed model achieved the accuracy of 86.15%, the mean reciprocal rank of 88.64%, and the recall of 85.13%. This demonstrated that the proposed food safety risk prediction method based on temporal knowledge graph networks has higher accuracy and stronger interpretability compared to some of the existing data prediction methods.},
  archive      = {J_TAI},
  author       = {Yuntao Shi and Kai Zhou and Meng Zhou and Shuqin Li and Weichuan Liu},
  doi          = {10.1109/TAI.2023.3321590},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2217-2226},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Temporal knowledge graph for food risk prediction},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial graph disentanglement with component-specific
aggregation. <em>TAI</em>, <em>5</em>(5), 2204–2216. (<a
href="https://doi.org/10.1109/TAI.2023.3316202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A real-world graph has a complex topological structure, which is often formed by the interaction of different latent factors. Disentanglement of these latent factors can effectively improve the robustness and expressiveness of the node representation of a graph. However, most existing methods lack consideration of the intrinsic differences in relations between nodes caused by factor entanglement. In this article, we propose an adversarial disentangled graph convolutional network (ADGCN) for disentangled graph representation learning. To begin with, we point out two aspects of graph disentanglement that need to be considered, i.e., microdisentanglement and macrodisentanglement. For them, a component-specific aggregation approach is proposed to achieve microdisentanglement by inferring latent components that caused the links between nodes. On the basis of microdisentanglement, we further propose a macrodisentanglement adversarial regularizer to improve the separability among component distributions, thus restricting the interdependence among components. In addition, to reveal the topological graph structure, a diversity-preserving node sampling approach is proposed, by which the graph structure can be progressively refined in a way of local structure awareness. The experimental results on various real-world graph data verify that our ADGCN obtains more favorable performance over currently available alternatives.},
  archive      = {J_TAI},
  author       = {Shuai Zheng and Zhenfeng Zhu and Zhizhe Liu and Jian Cheng and Yao Zhao},
  doi          = {10.1109/TAI.2023.3316202},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2204-2216},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adversarial graph disentanglement with component-specific aggregation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive optimal consensus control of multiagent systems
with unknown dynamics and disturbances via reinforcement learning.
<em>TAI</em>, <em>5</em>(5), 2193–2203. (<a
href="https://doi.org/10.1109/TAI.2023.3324895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An adaptive optimal consensus control design technique is presented for uncertain multiagent systems with prescribed performance guarantees using reinforcement learning (RL) algorithm. First, an adaptive neural network identifier is employed to learn the knowledge of uncertain system dynamics, and a disturbance observer is developed to compensate for time-varying disturbances. Second, a critic-network learning structure is established to obtain the approximate solution of Hamilton–Jacobi–Bellman (HJB) equations of multiagent systems. Then, an experience replay method is applied to update the critic network weights without requiring persistence of excitation condition. Third, RL-based optimized consensus controllers are developed such that 1) the cost function is minimized, 2) transient and steady-state performances of consensus error systems are guaranteed, and 3) uniform ultimate boundedness of the closed-loop systems is achieved. Finally, application to consensus control of unmanned surface vehicles with uncertain hydrodynamic dampings is given to demonstrate the effectiveness of the optimal control design technique.},
  archive      = {J_TAI},
  author       = {Lin Chen and Chao Dong and Shi-Lu Dai},
  doi          = {10.1109/TAI.2023.3324895},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2193-2203},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adaptive optimal consensus control of multiagent systems with unknown dynamics and disturbances via reinforcement learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combination of information in labeled and unlabeled data via
evidence theory. <em>TAI</em>, <em>5</em>(5), 2179–2192. (<a
href="https://doi.org/10.1109/TAI.2023.3316194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For classification with few labeled and massive unlabeled patterns, co-training, which uses information in labeled and unlabeled data to classify query patterns, is often employed to train classifiers in two distinct views. The classifiers teach each other by adding high-confidence unlabeled patterns to training dataset of the other view. Whereas, the direct adding often leads to some negative influence when retraining classifiers because some patterns with wrong predictions are added into training dataset. The wrong predictions must be considered for performance improvement. To this end, we present a method called Combination of Information in Labeled and Unlabeled (CILU) data based on evidence theory to effectively extract and fuse complementary knowledge in labeled and unlabeled data. In CILU, patterns are characterized by two distinct views, and the unlabeled patterns with high-confidence predictions are first added into the other view. We can train two classifiers by few labeled training data and high-confidence unlabeled patterns in each view. The classifiers are fused by evidence theory, and their weights which aim to reduce the harmful influence of wrong predictions are learnt by constructing an objection function on labeled data. There exist some complementary information between two distinct views, so the fused classifiers in two views are also combined. In order to extract more useful information in unlabeled data, semi-supervised Fuzzy C-mean clustering paradigm is also employed to yield clustering results. For a query pattern, the classification results and clustering results obtained by combined classifiers and clustering partition are integrated to make final class decision.},
  archive      = {J_TAI},
  author       = {Linqing Huang},
  doi          = {10.1109/TAI.2023.3316194},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2179-2192},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Combination of information in labeled and unlabeled data via evidence theory},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attention-based multiagent graph reinforcement learning for
service restoration. <em>TAI</em>, <em>5</em>(5), 2163–2178. (<a
href="https://doi.org/10.1109/TAI.2023.3314395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the ongoing integration of distributed energy resources, modern distribution systems are getting sufficient generation capacity to perform active restoration after outages without transmission system support. The model-based approaches are widely used in resolving service restoration problems, relying on accurate system models. Deep reinforcement learning is believed as an alternative solution for problem solving, although it has not been sufficiently explored. In this article, the service restoration process is described as a partially observable Markov decision process and a multiagent graph reinforcement learning approach based on attention is proposed to train multiple agents to coachieve the restoration goal to reinforce the system resilience in coping with extreme events. To consider the connections and correlations between nodes during the service restoration, the state of the active distribution network is defined by graph data that contains features of both topology and nodes. The perceived ability of the agents is empowered by graph convolutional networks during the feature extraction, supplying agents with more comprehensive data to learn more reasonable restoration strategies. In addition, the centralized training with attention is developed for multiagent systems, focusing on the relations between the agents to strengthen the teamwork capability. The performance of the proposed method is verified by a set of comparative studies on the IEEE-118 system with dispatchable generators, rooftop photovoltaics, and energy storage systems simultaneously.},
  archive      = {J_TAI},
  author       = {Bangji Fan and Xinghua Liu and Gaoxi Xiao and Yu Kang and Dianhui Wang and Peng Wang},
  doi          = {10.1109/TAI.2023.3314395},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2163-2178},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Attention-based multiagent graph reinforcement learning for service restoration},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identification of drug–side-effect association via multiview
semisupervised sparse model. <em>TAI</em>, <em>5</em>(5), 2151–2162. (<a
href="https://doi.org/10.1109/TAI.2023.3314405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The association between drugs and side effects encompasses information about approved medications and their documented adverse drug reactions. Traditional experimental approaches for studying this association tend to be time consuming and expensive. To represent all drug–side-effect associations, a bipartite network framework is employed. Consequently, numerous computational methods have been devised to tackle this problem, focusing on predicting new potential associations. However, a significant gap lies in the neglect of the multiview learning algorithm, which has the ability to integrate diverse information sources and enhance prediction accuracy. In our study, we have developed a novel predictor named multiview semisupervised sparse model (Mv3SM) to address the drug side effect prediction problem. Our approach aims to explore the distinctive characteristics of various view features obtained from fully paired multiview data and mitigate the influence of noisy data. To test the performance of Mv3SM and other computational approaches, we conducted experiments using three benchmark datasets. The obtained results clearly demonstrate that our proposed method achieves superior predictive performance compared to alternative approaches.},
  archive      = {J_TAI},
  author       = {Yijie Ding and Fei Guo and Prayag Tiwari and Quan Zou},
  doi          = {10.1109/TAI.2023.3314405},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2151-2162},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Identification of Drug–Side-effect association via multiview semisupervised sparse model},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved demonstration-knowledge utilization in
reinforcement learning. <em>TAI</em>, <em>5</em>(5), 2139–2150. (<a
href="https://doi.org/10.1109/TAI.2023.3328848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) has made great success in recent years. Generally, the learning process requires a huge amount of interaction with the environment before an agent can achieve acceptable performance. This motivates many techniques, such as incorporating prior knowledge which is usually presented as experts’ demonstration, and using a probability distribution to represent state-and-action values, to accelerate the learning process. The methods perform well when the prior knowledge is genuinely correct and no much change occurs to the learning environment. However, the requirement is not perfectly realistic in many complex applications. The demonstration knowledge may not reflect the true environment and even be full of noise. In this article, we introduce a dynamic distribution merging method to improve knowledge utilization in a general RL algorithm, namely Q-learning. The new method adapts a normal distribution to represent state-action values and merges the prior and learned knowledge in a discriminative way. We theoretically analyze the new learning method and demonstrate its empirical performance over multiple problem domains.},
  archive      = {J_TAI},
  author       = {Yanyu Liu and Yifeng Zeng and Biyang Ma and Yinghui Pan and Huifan Gao and Yuting Zhang},
  doi          = {10.1109/TAI.2023.3328848},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2139-2150},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Improved demonstration-knowledge utilization in reinforcement learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Designing new blood-brain barrier penetrating molecules
using novel hybridized gravitational search algorithm and explainable
AI. <em>TAI</em>, <em>5</em>(5), 2127–2138. (<a
href="https://doi.org/10.1109/TAI.2023.3313130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) has emerged as a powerful tool in computational biology, where it is being used to analyze large datasets to detect difficult biological patterns. This has enabled the design of new drug molecules. In this article, a novel method called hybridized gravitational search algorithm (HyGSA) has been proposed to design novel blood-brain barrier penetrating peptides (B3P2s) with desirable characteristics that enable them to cross the blood-brain barrier (BBB) and deliver neurological drugs directly to the brain. The HyGSA has two important modules in the form of an explainable machine learning classifier (with an accuracy, f1-score, and area under the ROC curve (AUROC) of 84%, 84%, and 91%, respectively) and an explainable deep learning-based B3P2 classifier (with an accuracy, f1-score, and AUROC of 89%, 91%, and 95%, respectively). The former was used to determine the crucial hand-engineered features, and the latter was designed to determine the critical amino acids that play an important role in the BBB penetrability of a peptide. Moreover, the population of particles was sampled at the beginning of each iteration to ensure a good mix of particles with low, average, and high fitness. This was achieved using a novel method that takes inspiration from the subset-sum problem and uses the mean and variance of the distribution of particles. For the pilot study, some B3P2s were discovered and optimized from a set of cell-penetrating peptides. Lastly, a free online tool has been deployed at https://b3p2design.anvil.app to help the scientific community discover and optimize B3P2s in protein sequences.},
  archive      = {J_TAI},
  author       = {Vishakha Singh and Ritesh Sharma and Sanjay Kumar Singh},
  doi          = {10.1109/TAI.2023.3313130},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2127-2138},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Designing new blood-brain barrier penetrating molecules using novel hybridized gravitational search algorithm and explainable AI},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal tracking neuro-control of continuous stirred tank
reactor systems: A dynamic event-driven approach. <em>TAI</em>,
<em>5</em>(5), 2117–2126. (<a
href="https://doi.org/10.1109/TAI.2023.3313105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is often challenging to design an optimal tracking controller for the continuous stirred tank reactor (CSTR) system due to its nonlinear nature and physical limitations. This article presents a dynamic event-driven optimal tracking neruo-control scheme for the CSTR system with asymmetric input constraints. Initially, an improved nonquadratic cost function is introduced for the CSTR system to tackle asymmetric control restrictions. Then, a dynamic event-driven mechanism together with the event-driven Hamilton-Jacobi-Bellman equation (ED-HJBE) is proposed. To solve the ED-HJBE, a critic neural network (CNN) is constructed within the critic learning framework. The CNN&#39;s weight vector is tuned by combining the gradient descent method and the concurrent learning technique. After that, uniform ultimate boundedness of the tracking error and the CNN&#39;s weight estimation error is assured based on the Lyapunov method. Finally, experiment studies are conducted to validate the present optimal tracking neuro-control strategy.},
  archive      = {J_TAI},
  author       = {Xiong Yang and Yingjiang Zhou},
  doi          = {10.1109/TAI.2023.3313105},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2117-2126},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Optimal tracking neuro-control of continuous stirred tank reactor systems: A dynamic event-driven approach},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rethinking deep supervision for brain tumor segmentation.
<em>TAI</em>, <em>5</em>(5), 2103–2116. (<a
href="https://doi.org/10.1109/TAI.2023.3313131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of brain tumors is crucial for diagnostic evaluation and clinical planning. Convolutional-based and Transformer-based models have shown promising results in automatic brain tumor segmentation. In these models, a deep supervision strategy has been widely adopted for parameter optimization. As a key part of this strategy, the segmentation head is responsible for generating early segmentation in the training phase. However, although containing informative cues valuable for decoder refinement, the segmentation head is usually discarded during inference. In this work, we propose a novel approach called deep supervision guided transformer (DSGT) for brain tumor segmentation. DSGT leverages informative cues within the segmentation head to guide decoding by developing guided heads upon a Transformer-based decoder. Specifically, we first extract semantic features from the segmentation head and then design two guided modules for feature refinement and noisy removal to generate sample-wise guiding maps. The resulting maps are fed back to the decoder for auxiliary guidance. Our experiments on two publicly available brain tumor segmentation datasets, BraTS2019 and BraTS2020, demonstrate that DSGT achieves superior segmentation performance compared with state-of-the-art approaches.},
  archive      = {J_TAI},
  author       = {Jiale Li and Aiping Liu and Yu Li and Wei Wei and Ruobing Qian and Qingguo Xie and Bensheng Qiu and Xun Chen},
  doi          = {10.1109/TAI.2023.3313131},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2103-2116},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Rethinking deep supervision for brain tumor segmentation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A self-labeling method for adaptive machine learning by
interactive causality. <em>TAI</em>, <em>5</em>(5), 2093–2102. (<a
href="https://doi.org/10.1109/TAI.2023.3311782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from unlabeled data or self-learning, can substantially reduce the complexity of machine learning (ML) utilization in real-time deployment. While the development of un/semisupervised algorithms shows promising results in learning with reduced labels, the fundamental assumption of data smoothness restricts its scope of application, especially with nonstationary data distributions in different domains. Leveraging transdomain invariant causal relationships, causality has recently been employed to foster robust ML. In this article, we have developed a generic method for self-labeling data that relies on known causality among interactive objects and learned temporal relations among causal events for identifying and associating labels and input data. The causal time-interval between asynchronous cause and effect events is studied to achieve self-labeling. We utilize dynamical system theory in a 1- $d$ setting to demonstrate that our proposed method outperforms traditional feature similarity-based semisupervised learning. A computer simulation experiment was conducted to generate high-dimensional data, and the comprehensive results reveal the potential of learning adaptation in dynamic environments to improve ML robustness against shifts in data distribution.},
  archive      = {J_TAI},
  author       = {Yutian Ren and Aaron Haohua Yen and Guann-Pyng Li},
  doi          = {10.1109/TAI.2023.3311782},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2093-2102},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A self-labeling method for adaptive machine learning by interactive causality},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safe adaptive dynamic programming for multiplayer systems
with static and moving no-entry regions. <em>TAI</em>, <em>5</em>(5),
2079–2092. (<a href="https://doi.org/10.1109/TAI.2023.3325780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the use of adaptive dynamic programming algorithm to solve the Nash equilibrium problem of multiplayer differential games has received extensive attention. Although approximate solutions can be obtained, the current algorithms have such a premise that the operation domain of the system is completely safe and the probing noise is required to excite the system during learning. To deal with these challenges, this article considers the optimal avoidance control problem that the system needs to avoid multiple static or dynamic no-entry regions while reaching the target point, and thus proposes a safe adaptive dynamic programming approach. First, the optimal avoidance control problem is formulated and multiple no-entry regions are encoded into each player&#39;s cost function using the barrier function. Then, a safe adaptive dynamic programming approach is proposed with several novel features, including actor–critic neural networks composed of state-following kernel function, state extrapolation for achieving virtual excitation, and weight tuning laws for executing adaptive learning. Next, this approach is extended to the case of moving regions and some theoretical results are provided. Finally, the proposed safe learning scheme is demonstrated on three simulation examples, and is also compared with other control methods.},
  archive      = {J_TAI},
  author       = {Chaoxu Mu and Ke Wang and Xin Xu and Changyin Sun},
  doi          = {10.1109/TAI.2023.3325780},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2079-2092},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Safe adaptive dynamic programming for multiplayer systems with static and moving no-entry regions},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural tree decoder for interpretation of vision
transformers. <em>TAI</em>, <em>5</em>(5), 2067–2078. (<a
href="https://doi.org/10.1109/TAI.2023.3312645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a novel vision transformer neural tree decoder (ViT-NeT) that is interpretable and highly accurate in terms of fine-grained visual categorization (FGVC). A ViT acts as a backbone, and to overcome the limitations of ViT, the output context image patch is fed to the proposed NeT. NeT aims to more accurately classify fine-grained objects using similar interclass correlations and different intra-class correlations. ViT-NeT can also describe decision-making processes and visually interpret the results through tree structures and prototypes. Because the proposed ViT-NeT is designed not only to improve FGVC classification performance, but also to provide human-friendly interpretation, it is effective in resolving the tradeoff between performance and interpretability. We compared the performance of ViT-NeT with other state-of-the-art (SoTA) methods using the widely applied FGVC benchmark datasets CUB-200-2011, Stanford Dogs, Stanford Cars, NABirds, and iNaturalist. The proposed method shows a promising quantitative and qualitative performance in comparison to previous SoTA methods as well as an excellent interpretability.},
  archive      = {J_TAI},
  author       = {Sangwon Kim and Byoung Chul Ko},
  doi          = {10.1109/TAI.2023.3312645},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2067-2078},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Neural tree decoder for interpretation of vision transformers},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multilevel explainable artificial intelligence: Visual and
linguistic bonded explanations. <em>TAI</em>, <em>5</em>(5), 2055–2066.
(<a href="https://doi.org/10.1109/TAI.2023.3308555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applications of deep neural networks (DNNs) are booming in more and more fields but lack transparency due to their black-box nature. Explainable artificial intelligence (XAI) is, therefore, of paramount importance, where strategies are proposed to understand how these black-box models function. The research so far mainly focuses on producing, for example, class-wise saliency maps, highlighting parts of a given image that affect the prediction the most. However, this method does not fully represent the way humans explain their reasoning, and awkwardly, validating these maps is quite complex and generally requires subjective interpretation. In this article, we conduct XAI differently by proposing a new XAI methodology in a multilevel (i.e., visual and linguistic) manner. By leveraging the interplay between the learned representations, i.e., image features and linguistic attributes, the proposed approach can provide salient attributes and attribute-wise saliency maps, which are far more intuitive than the class-wise maps, without requiring per-image ground-truth human explanations. It introduces self-interpretable attributes to overcome the current limitations in XAI and bring the XAI closer to a human-like explanation. The proposed architecture is simple in use and can reach surprisingly good performance in both prediction and explainability for deep neural networks thanks to the low-cost per-class attributes.},
  archive      = {J_TAI},
  author       = {Halil Ibrahim Aysel and Xiaohao Cai and Adam Prugel-Bennett},
  doi          = {10.1109/TAI.2023.3308555},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2055-2066},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multilevel explainable artificial intelligence: Visual and linguistic bonded explanations},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient real-time recognition model of plant diseases for
low-power consumption platform. <em>TAI</em>, <em>5</em>(5), 2040–2054.
(<a href="https://doi.org/10.1109/TAI.2023.3307662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognition and early warning of plant diseases is one of the keys to agricultural disaster prevention and mitigation. Deep learning-based image recognition methods give us a new idea for plant disease identification. Due to the harsh conditions in agricultural environment, recent research has focused on exploring ways to lightweight the recognition model for deployment on low-power devices. In this article, we propose an efficient and feature-guided real-time plant disease recognition model with a multiclassifier architecture, specifically designed for low-power devices. By comparing with other advanced methods, our model reaches the state of the art in the combined metrics of recognition accuracy, the number of parameters, and inference speed. First, we propose AMI-NanoNet based on Roofline theory to significantly reduce the number of parameters and computational complexity. This model can achieve 99.8343% accuracy on PlantVillage by using a feature-guided curriculum learning with stepwise training strategy. Moreover, we design another training strategy suitable for lightweight ensemble models. Based on this strategy, our model only needs to integrate the classifiers at the end of the network to achieve 99.8708% identification accuracy, and it hardly increases the number of operations and parameters of the network. Extensive evaluations on this dataset demonstrate the effectiveness of our ensemble learning method. Furthermore, we tested our proposed methods on another dataset from other domains to validate its applicability to different scenarios. Overall, our research provides a basis for rapid and intelligent identification of plant diseases.},
  archive      = {J_TAI},
  author       = {Songyun Deng and Wanneng Wu and Kunlin Zou and Hai Qin and Lekai Cheng and Qiaokang Liang},
  doi          = {10.1109/TAI.2023.3307662},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2040-2054},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Efficient real-time recognition model of plant diseases for low-power consumption platform},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated multi-phase curriculum learning to synchronously
correlate user heterogeneity. <em>TAI</em>, <em>5</em>(5), 2026–2039.
(<a href="https://doi.org/10.1109/TAI.2023.3307664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a decentralized learning method used to train machine learning algorithms. In FL, a global model iteratively collects the parameters of local models without accessing their local data. However, a significant challenge in FL is handling the heterogeneity of local data distribution, which often results in a drifted global model that is difficult to converge. To address this issue, current methods employ different strategies, such as knowledge distillation, weighted model aggregation, and multitask learning. These approaches are referred to as asynchronous FL, as they align user models either locally or posthoc, where model drift has already occurred or has been underestimated. In this article, we propose an active and synchronous correlation approach to address the challenge of user heterogeneity in FL. Specifically, our approach aims to approximate FL as standard deep learning by actively and synchronously scheduling user learning pace in each round with a dynamic multiphase curriculum. A global curriculum is formed by an autoregressive autoencoder that integrates all user curricula on the server. This global curriculum is then divided into multiple phases and broadcast to users to measure and align the domain-agnostic learning pace. Empirical studies demonstrate that our approach outperforms existing asynchronous approaches in terms of generalization performance, even in the presence of severe user heterogeneity.},
  archive      = {J_TAI},
  author       = {Mingjie Wang and Jianxiong Guo and Weijia Jia},
  doi          = {10.1109/TAI.2023.3307664},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2026-2039},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Federated multi-phase curriculum learning to synchronously correlate user heterogeneity},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Memory-augmented graph neural networks: A brain-inspired
review. <em>TAI</em>, <em>5</em>(5), 2011–2025. (<a
href="https://doi.org/10.1109/TAI.2023.3329454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have been extensively used for many domains where data are represented as graphs, including social networks, recommender systems, biology, chemistry, etc. Despite promising empirical results achieved by GNNs for many applications, there are some limitations of GNNs that hinder their performance on some tasks. For example, since GNNs update node features mainly based on local information, they have limited expressive power for capturing long-range dependencies between nodes. To address some of these limitations, several recent works have started to explore augmenting GNNs with memory to improve their performance and expressivity. We provide a comprehensive review of the existing literature on memory-augmented GNNs. We review these works through the lens of psychology and neuroscience, which has several established theories on how multiple memory systems and mechanisms operate in biological brains. We propose a taxonomy of memory-augmented GNNs and a set of criteria for comparing their memory mechanisms. We also provide critical discussions on the limitations of these works. Finally, we discuss the challenges and future directions for this area.},
  archive      = {J_TAI},
  author       = {Guixiang Ma and Vy A. Vo and Theodore L. Willke and Nesreen K. Ahmed},
  doi          = {10.1109/TAI.2023.3329454},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2011-2025},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Memory-augmented graph neural networks: A brain-inspired review},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Monocular visual simultaneous localization and mapping:
(R)evolution from geometry to deep learning-based pipelines.
<em>TAI</em>, <em>5</em>(5), 1990–2010. (<a
href="https://doi.org/10.1109/TAI.2023.3321032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of deep learning, there is a fundamental change in visual simultaneous localization and mapping (SLAM) algorithms toward developing different modules trained as end-to-end pipelines. However, regardless of the implementation domain, visual SLAM&#39;s performance is subject to diverse environmental challenges, such as dynamic elements in outdoor environments, harsh imaging conditions in underwater environments, or blurriness in high-speed setups. These environmental challenges need to be identified to study the real-world viability of SLAM implementations. Motivated by the aforementioned challenges, this article surveys the current state of visual SLAM algorithms according to the two main frameworks: geometry-based and learning-based SLAM. First, we introduce a general formulation of the SLAM pipeline that includes most of the implementations in the literature. Second, those implementations are classified and surveyed for geometry and learning-based SLAM. After that, environment-specific challenges are formulated to enable experimental evaluation of the resilience of different visual SLAM classes to varying imaging conditions. We address two significant issues in surveying visual SLAM, providing a consistent classification of visual SLAM pipelines and a robust evaluation of their performance under different deployment conditions. Finally, we give our take on future opportunities for visual SLAM implementations.},
  archive      = {J_TAI},
  author       = {Olaya Álvarez-Tuñón and Yury Brodskiy and Erdal Kayacan},
  doi          = {10.1109/TAI.2023.3321032},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1990-2010},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Monocular visual simultaneous localization and mapping: (R)Evolution from geometry to deep learning-based pipelines},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-centric green artificial intelligence: A survey.
<em>TAI</em>, <em>5</em>(5), 1973–1989. (<a
href="https://doi.org/10.1109/TAI.2023.3315272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the exponential growth of computational power and the availability of large-scale datasets in recent years, remarkable advancements have been made in the field of artificial intelligence (AI), leading to complex models and innovative applications. However, these models consume a significant unprecedented amount of energy, contributing to greenhouse gas emissions and a growing carbon footprint in the AI industry. In response, the concept of green AI has emerged, prioritizing energy efficiency and sustainability alongside accuracy and related measures. To this end, data-centric approaches are very promising to reduce the energy consumption of AI algorithms. This article presents a comprehensive overview of data-centric technologies and their impact on the energy efficiency of AI algorithms. Specifically, it focuses on methods that utilize training data in an efficient manner to improve the energy efficiency of AI algorithms. We have identified multiple data-centric approaches, such as active learning, knowledge transfer/sharing, dataset distillation, data augmentation, and curriculum learning that can contribute to the development of environmentally-friendly implementations of machine learning algorithms. Finally, the practical applications of these approaches are highlighted, and the challenges and future directions in the field are discussed.},
  archive      = {J_TAI},
  author       = {Shirin Salehi and Anke Schmeink},
  doi          = {10.1109/TAI.2023.3315272},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1973-1989},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Data-centric green artificial intelligence: A survey},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Embedded deep learning accelerators: A survey on recent
advances. <em>TAI</em>, <em>5</em>(5), 1954–1972. (<a
href="https://doi.org/10.1109/TAI.2023.3311776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential increase in generated data as well as the advances in high-performance computing has paved the way for the use of complex machine learning methods. Indeed, the availability of graphical processing units and tensor processing units has made it possible to train and prototype deep neural networks (DNNs) on large-scale datasets and for a variety of applications, i.e., vision, robotics, biomedical, etc. The popularity of these DNNs originates from their efficacy and state-of-the-art inference accuracy. However, this is obtained at the cost of a considerably high computational complexity. Such drawbacks rendered their implementation on limited resources, edge devices, without a major loss in inference speed and accuracy, a dire and challenging task. To this extent, it has become extremely important to design innovative architectures and dedicated accelerators to deploy these DNNs to embedded and reconfigurable processors in a high-performance low-complexity structure. In this study, we present a survey on recent advances in deep learning accelerators for heterogeneous systems and Reduced Instruction Set Computer processors given their open-source nature, accessibility, customizability, and universality. After reading this article, the readers should have a comprehensive overview of the recent progress in this domain, cutting edge knowledge of recent embedded machine learning trends, and substantial insights for future research directions and challenges.},
  archive      = {J_TAI},
  author       = {Ghattas Akkad and Ali Mansour and Elie Inaty},
  doi          = {10.1109/TAI.2023.3311776},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1954-1972},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Embedded deep learning accelerators: A survey on recent advances},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neurosymbolic reinforcement learning and planning: A survey.
<em>TAI</em>, <em>5</em>(5), 1939–1953. (<a
href="https://doi.org/10.1109/TAI.2023.3311428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The area of neurosymbolic artificial intelligence (Neurosymbolic AI) is rapidly developing and has become a popular research topic, encompassing subfields, such as neurosymbolic deep learning and neurosymbolic reinforcement learning (Neurosymbolic RL). Compared with traditional learning methods, Neurosymbolic AI offers significant advantages by simplifying complexity and providing transparency and explainability. Reinforcement learning (RL), a long-standing artificial intelligence (AI) concept that mimics human behavior using rewards and punishment, is a fundamental component of Neurosymbolic RL, a recent integration of the two fields that has yielded promising results. The aim of this article is to contribute to the emerging field of Neurosymbolic RL by conducting a literature survey. Our evaluation focuses on the three components that constitute Neurosymbolic RL: neural, symbolic, and RL. We categorize works based on the role played by the neural and symbolic parts in RL, into three taxonomies: learning for reasoning, reasoning for learning, and learning–reasoning. These categories are further divided into subcategories based on their applications. Furthermore, we analyze the RL components of each research work, including the state space, action space, policy module, and RL algorithm. In addition, we identify research opportunities and challenges in various applications within this dynamic field.},
  archive      = {J_TAI},
  author       = {Kamal Acharya and Waleed Raza and Carlos Dourado and Alvaro Velasquez and Houbing Herbert Song},
  doi          = {10.1109/TAI.2023.3311428},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1939-1953},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Neurosymbolic reinforcement learning and planning: A survey},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An overview of swarm coordinated control. <em>TAI</em>,
<em>5</em>(5), 1918–1938. (<a
href="https://doi.org/10.1109/TAI.2023.3314581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, with the rapid development of swarm control technology, the swarm system has shown broad application prospects in military, civil, and other fields, which has gradually become a research hotspot in the area of automatic control. To further promote the development of swarm technology, this article reviews the detail research on the swarm control, especially the results after 2018. In order to cover all the important and hot topics in the field of swarm control research, we conducted a literature search based on the Google Scholar and Web of Science databases and collected hundreds of articles on the swarm control for review. The topics include swarm, formation, consistency, topology switching, delay control, convergence time control, obstacle avoidance, path planning, etc. Besides, the evaluation methods and applications such as vicinagearth security are introduced. Then, based on the latest existing research methods and results, the future research directions are proposed, including survival-intelligence-based, learning-based, and heterogeneous-control-based. In conclusion, the main purpose of this article is to sort out the latest achievements in the development of swarm control technology, providing important research references for scholars.},
  archive      = {J_TAI},
  author       = {Dengxiu Yu and Jiacheng Li and Zhen Wang and Xuelong Li},
  doi          = {10.1109/TAI.2023.3314581},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1918-1938},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An overview of swarm coordinated control},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving the feature selection stability of the delta test
in regression. <em>TAI</em>, <em>5</em>(5), 1911–1917. (<a
href="https://doi.org/10.1109/TAI.2023.3313129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is an important preprocessing step that helps to improve model performance and to extract knowledge about important features in a dataset. However, feature selection methods are known to be adversely impacted by changes in the training dataset: even small differences between input datasets can result in the selection of different feature sets. This letter tackles this issue in the particular case of the delta test (DT), a well-known feature relevance criterion that approximates the noise variance for regression tasks. A new feature selection criterion is proposed, the delta test bar, which is shown to be more stable than its close competitors. Impact Statement Feature selection makes it possible to identify the attributes that play an important role in predicting a target. However, some feature selection methods, such as the delta test, suffer from instability. As a result, it is difficult to trust that the features selected are the most relevant ones. The method we present in this letter improves the stability of the delta test, thereby increasing the trustworthiness of the feature selection procedure.},
  archive      = {J_TAI},
  author       = {Rebecca Marion and Benoît Frénay},
  doi          = {10.1109/TAI.2023.3313129},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1911-1917},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Improving the feature selection stability of the delta test in regression},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust and verifiable privacy federated learning.
<em>TAI</em>, <em>5</em>(4), 1895–1908. (<a
href="https://doi.org/10.1109/TAI.2023.3309273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) safeguards user privacy by uploading gradients instead of raw data. However, inference attacks can reconstruct raw data using gradients uploaded by users in FL. To mitigate this issue, researchers have combined privacy computing techniques with FL. However, these techniques may not ensure the Byzantine robustness of aggregation or the integrity of the aggregated outcomes. Most current robust privacy FL methods assess differences between gradients and benchmarks in the direction, allowing adversaries to poison the aggregation against the magnitude. Furthermore, these methods cannot ensure the integrity of the aggregation results. To overcome these challenges, this study proposes a novel algorithm, robust and verifiable privacy federated learning (RVPFL), which can more effectively eliminate the poisoning attack of the opponent by measuring the direction and magnitude of the gradient in the ciphertext state. The proposed algorithm guarantees the integrity of server aggregation results while safeguarding user privacy. In this study, comprehensive theoretical analysis and experimental validation of RVPFL are conducted to demonstrate its superiority. The proposed RVPFL algorithm solves the Byzantine robustness problem of aggregation and the integrity problem of aggregation results, which helps to research and develop more robust and effective privacy-preserving federal learning techniques.},
  archive      = {J_TAI},
  author       = {Zhi Lu and Songfeng Lu and Xueming Tang and Junjun Wu},
  doi          = {10.1109/TAI.2023.3309273},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1895-1908},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Robust and verifiable privacy federated learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning for selecting custom instructions
under area constraint. <em>TAI</em>, <em>5</em>(4), 1882–1894. (<a
href="https://doi.org/10.1109/TAI.2023.3308099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extensible processors, which combine programmability and efficiency, are emerging as a promising approach in the field of embedded computing. Automated synthesis of custom instructions from high-level application descriptions is a vital step involved in the design of extensible processors. In automated custom instruction synthesis, selecting custom instructions from a large set of candidates under area constraint is essentially a difficult combinatorial optimization problem. In this article, we show that the custom instruction selection problem can be formulated as a sequential decision-making problem. Based on this formulation, we present three reinforcement learning (RL)-based approaches, namely state–action–reward–state–action (SARSA), Q-learning, and double Q-learning, for solving the custom instruction selection problem. Moreover, we also perform a comprehensive analysis and comparison of various combinations of learning specifications: the algorithm type and the update strategy the for $\epsilon$ -greedy policy. The experiments with 45 test instances reveal that the SARSA, Q-learning, and double Q-learning algorithms outperform the metaheuristic algorithm in terms of the overall performance gains by 26.9%, 26.1%, and 26.4%, respectively. Among the three RL algorithms, the SARSA algorithm slightly overwhelms the other two RL algorithms. Furthermore, the experimental results suggest that the strategy $F_{3}:\epsilon = \kappa ^{i}, 0&amp;lt; \kappa &amp;lt; 1$ is generally the most effective one for controlling the exploration and the exploitation of the learning processes.},
  archive      = {J_TAI},
  author       = {Shanshan Wang and Chenglong Xiao},
  doi          = {10.1109/TAI.2023.3308099},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1882-1894},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Reinforcement learning for selecting custom instructions under area constraint},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An end-to-end neuromorphic radio classification system with
an efficient sigma-delta-based spike encoding scheme. <em>TAI</em>,
<em>5</em>(4), 1869–1881. (<a
href="https://doi.org/10.1109/TAI.2023.3306334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid advancements in 5G communication and the Internet-of-things have prompted the development of cognitive radio sensing for spectrum monitoring and malicious attack detection. An end-to-end radio classification system is essential to realize efficient real-time monitoring at the edge. This work presents an end-to-end neuromorphic system enabled by an efficient spiking neural network (SNN) for radio classification. A novel hardware-efficient spiking encoding method is proposed leveraging the Sigma-Delta modulation mechanism in analog-to-digital converters. It requires no additional hardware components, simplifies the system design, and helps reduce conversion latency. Following a designed hardware-emulating conversion process, the classification performance is verified on two benchmark radio modulation datasets. A comparable accuracy to an artificial neural network baseline with a difference of 0.30% is achieved on the dataset RADIOML 2018 with more realistic conditions. Further analysis reveals that the proposed method requires less power-intensive computational operations, leading to 22× lower computational energy consumption. In addition, this method exhibits more than 99% accuracy on the dataset when the signal-to-noise ratio is above 0 dB. The SNN-based classification module is realized on field programmable gate array (FPGA) with a heterogeneous streaming architecture, achieving high throughput and low resource utilization. Therefore, this work demonstrates a promising solution for constructing an efficient high-performance end-to-end radio classification system.},
  archive      = {J_TAI},
  author       = {Wenzhe Guo and Kuilian Yang and Haralampos-G. Stratigopoulos and Hassan Aboushady and Khaled Nabil Salama},
  doi          = {10.1109/TAI.2023.3306334},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1869-1881},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An end-to-end neuromorphic radio classification system with an efficient sigma-delta-based spike encoding scheme},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discriminative deep generalized dependency analysis for
multi-view data. <em>TAI</em>, <em>5</em>(4), 1857–1868. (<a
href="https://doi.org/10.1109/TAI.2023.3306739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, a surging interest is noted for combining the information of multiple views to obtain a joint representation of the given data. In multi-view data analysis, the joint representation should be learned from the given input views in such a way that the view-specific information as well as the cross-view dependency are preserved properly. In the context of cross-view dependency, it is expected that both view-consistency and view-discrepancy are addressed simultaneously. Discriminability of the joint representation is also an important aspect in the classification problem. In this regard, a novel deep learning model is proposed to efficiently encapsulate the underlying data distribution over the space of input views. Considering both consensus and complementary principles, a loss function is introduced, based on the concept of the Hilbert–Schmidt independence criterion, to capture the relevant cross-view information from the given multi-view data. Incorporating the supervised information of sample categories not only enhances the discriminative ability of the model but also allows it to classify the given samples into different categories. An upper bound on the error probability of the proposed deep model is estimated in terms of the model architecture. It facilitates determining the optimal architecture of the proposed model for each database. The proficiency of the model is studied on numerous application domains with reference to several state-of-the-art multi-view classification algorithms.},
  archive      = {J_TAI},
  author       = {Debamita Kumar and Pradipta Maji},
  doi          = {10.1109/TAI.2023.3306739},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1857-1868},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Discriminative deep generalized dependency analysis for multi-view data},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptation: Blessing or curse for higher way meta-learning.
<em>TAI</em>, <em>5</em>(4), 1844–1856. (<a
href="https://doi.org/10.1109/TAI.2023.3301826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevailing literature typically assesses the effectiveness of meta-learning (ML) approaches on tasks that involve no more than 20 classes. However, we challenge this convention by conducting a more complex and natural task setup to test the fundamental initialization, metric, and optimization approaches. In particular, we increase the number of classes in the Omniglot and tieredImagenet datasets to 200 and 90, respectively. Interestingly, we observe that as the number of classes increases, ML approaches perform in reverse order of their degree of adaptation, with prototypical network (ProtoNet) outperforming almost no inner loop (ANIL) and model-agnostic meta-learning (MAML). ProtoNet, which does not require adaptation, is marginally affected by the increase in task complexity, while ANIL and MAML are highly affected. Despite performing full feature backbone and classifier adaptation, Meta Long Short-term Memory (MetaLSTM++) exhibits an intriguing behavior of performing well. To this end, we analyze the backbone learned by different algorithms and the influence of adaptation from different perspectives. We uncover that ProtoNet learns better, and MetaLSTM++ learns the worst backbone, but the generalizability to unseen data for MetaLSTM++ is compensated by powerful adaptation to the meta-test support sets, due to its learned data-driven optimizer.},
  archive      = {J_TAI},
  author       = {Aroof Aimen and Sahil Sidheekh and Bharat Ladrecha and Hansin Ahuja and Narayanan C Krishnan},
  doi          = {10.1109/TAI.2023.3301826},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1844-1856},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adaptation: Blessing or curse for higher way meta-learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain-centroid-guided progressive teacher-based knowledge
distillation for source-free domain adaptation of histopathological
images. <em>TAI</em>, <em>5</em>(4), 1831–1843. (<a
href="https://doi.org/10.1109/TAI.2023.3305331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks are commonly used for histopathology image analysis. However, such data-driven models are sensitive to style variances across scanners and suffer a significant performance degradation as a result. Although the network performance can be improved by using domain adaptation methods, the source dataset required to perform the adaptation process is generally unavailable. This study shows that the performance degradation of deep neural networks when applied to histopathology images is the result partly of the wide distribution of the features generated when inferring the features of the target model using the feature centers of the source model. To address this problem, a teacher–student framework, designated as domain-centroid-guided progressive teacher-based knowledge distillation (DCGP-KD), is proposed which aims to learn compact target features in order to provide more accurate pseudo labels for the target model without the need for the original source dataset. In the proposed framework, the class-wise feature centers of the source data are progressively adapted to the distribution of the target data, and compact target features are then generated by gathering the features based on their class-wise centers. A strategy is additionally proposed to prevent catastrophic forgetting during the progressive adaption process. Finally, a prediction consistency loss function is introduced to improve the robustness of the target dataset. The feasibility of the proposed framework is demonstrated experimentally for the illustrative case of the tumor classification of histopathological images with staining variations. The results show that DCGP-KD provides a promising assistive tool for pathologists in various histopathological analysis tasks.},
  archive      = {J_TAI},
  author       = {Kuo-Sheng Cheng and Qiong-Wen Zhang and Hung-Wen Tsai and Nien-Tsu Li and Pau-Choo Chung},
  doi          = {10.1109/TAI.2023.3305331},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1831-1843},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Domain-centroid-guided progressive teacher-based knowledge distillation for source-free domain adaptation of histopathological images},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fine-grained detector of face mask wearing status based on
improved YOLOX. <em>TAI</em>, <em>5</em>(4), 1816–1830. (<a
href="https://doi.org/10.1109/TAI.2023.3300668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fast outbreak of coronavirus disease 2019 (COVID-19) and rapid proliferation of its variants have continued to pose a huge challenge to people around the world. Wearing medical masks properly in public and private settings can protect people from COVID-19, which brings a growing demand for automatic detection services of face mask wearing conditions. In this article, we propose a fine-grained detector called ECA_YOLOX-S to identify the wearing status of face masks. Efficient channel attention is introduced into YOLOX-S to reach a tradeoff between effectiveness and efficiency. To demonstrate the performance of our proposed method, a Fine-grained Face Mask (FineFM) dataset is created, which covers four classes of mask wearing status. The proposed FineFM dataset has 16 955 annotated images and covers multiple realistic scenarios. To the best of authors&#39; knowledge, it has the largest number of improper mask wearing images among all similar datasets for realistic scenes. Experiments conducted on the FineFM dataset demonstrate that ECA_YOLOX-S achieves an overall mean average precision (mAP)@.50:95 of 86.80% for moderate scenes and an overall mAP@.50:95 of 73.20% for complex scenes, outperforming its benchmark model. Moreover, experiments conducted on other realistic and simulated datasets indicate that the proposed detector has advantages over other methods.},
  archive      = {J_TAI},
  author       = {Hongli Xiao and Bingshu Wang and Jiangbin Zheng and Licheng Liu and C. L. Philip Chen},
  doi          = {10.1109/TAI.2023.3300668},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1816-1830},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A fine-grained detector of face mask wearing status based on improved YOLOX},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Social self-attention generative adversarial networks for
human trajectory prediction. <em>TAI</em>, <em>5</em>(4), 1805–1815. (<a
href="https://doi.org/10.1109/TAI.2023.3299899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting accurate human future trajectories is of critical importance for self-driving vehicles if they are to navigate complex scenarios. Trajectories of humans are not only dependent on the humans themselves, but also the interactions with surrounding agents. Previous works mainly model interactions among agents by using a diversity of polymerization methods that integrate various learned agent states hit or miss. In this article, we propose social self-attention generative adversarial networks (Social SAGAN), which generate socially acceptable multimodal trajectory predictions. Social SAGAN incorporates a generator that predicts future trajectories of pedestrians, a discriminator that classifies trajectory predictions as real or fake, and a social self-attention mechanism that selectively refines the most interactive information and helps the overall model to capture what to pay attention to. Through extensive experiments, we demonstrate that our model achieves competitive prediction accuracy and computational complexity compared with previous state-of-the-art methods on all trajectory forecasting benchmarks.},
  archive      = {J_TAI},
  author       = {Changzhi Yang and Huihui Pan and Weichao Sun and Huijun Gao},
  doi          = {10.1109/TAI.2023.3299899},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1805-1815},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Social self-attention generative adversarial networks for human trajectory prediction},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Facilitating sim-to-real by intrinsic stochasticity of
real-time simulation in reinforcement learning for robot manipulation.
<em>TAI</em>, <em>5</em>(4), 1791–1804. (<a
href="https://doi.org/10.1109/TAI.2023.3299252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulation is essential to reinforcement learning (RL) before implementation in the real world, especially for safety-critical applications like robot manipulation. Conventionally, RL agents are sensitive to the discrepancies between the simulation and the real world, known as the sim-to-real gap. The application of domain randomization, a technique used to fill this gap, is limited to the imposition of heuristic-randomized models. We investigate the properties of intrinsic stochasticity of real-time simulation (RT-IS) of off-the-shelf simulation software and its potential to improve the RL performance. This improvement includes a higher tolerance to noise and model imprecision and superiority to conventional domain randomization in terms of ease of use and automation. First, we conduct analytical studies to measure the correlation of RT-IS with the utilization of computer hardware and validate its comparability with the natural stochasticity of a physical robot. Then, we exploit the RT-IS feature in the training of an RL agent. The simulation and physical experiment results verify the feasibility and applicability of RT-IS to robust agent training for robot manipulation tasks. The RT-IS-powered RL agent outperforms conventional agents on robots with modeling uncertainties. RT-IS requires less heuristic randomization is not task-dependent, and achieves better generalizability than the conventional domain-randomization-powered agents. Our findings provide a new perspective on the sim-to-real problem in practical applications like robot manipulation tasks.},
  archive      = {J_TAI},
  author       = {Amir M. Soufi Enayati and Ram Dershan and Zengjie Zhang and Dean Richert and Homayoun Najjaran},
  doi          = {10.1109/TAI.2023.3299252},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1791-1804},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Facilitating sim-to-real by intrinsic stochasticity of real-time simulation in reinforcement learning for robot manipulation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prescribed-time adaptive intelligent formation controller
for nonlinear multiagent systems based on time-domain mapping.
<em>TAI</em>, <em>5</em>(4), 1778–1790. (<a
href="https://doi.org/10.1109/TAI.2023.3299439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates an adaptive fuzzy prescribed-time formation control problem for nonstrict feedback nonlinear multi-agent systems subjected to uncertain external disturbances. Firstly, through time-domain mapping, the prescribed-time formation control problem of the original system is converted to asymptotic convergence problem of the transformed system. Secondly, the fuzzy logic systems are used to approximate the unknown nonlinear dynamics and the bounded estimation algorithm is utilized to design the intermediate controllers and parameter adaptive laws. Finally, by introducing an integrable function into the backstepping recursive design, an adaptive fuzzy prescribed-time formation control method is developed. Based on the Lyapunov stability theory, it is proved that the closed-loop system is stable, and the formation error converges asymptotically to zero in a prescribed time. Additionally, a numerical simulation is given to further illustrate the feasibility of the presented formation control method and theory.},
  archive      = {J_TAI},
  author       = {Yongming Li and Xingyan Zheng and Kewen Li},
  doi          = {10.1109/TAI.2023.3299439},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1778-1790},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Prescribed-time adaptive intelligent formation controller for nonlinear multiagent systems based on time-domain mapping},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast <span class="math inline"><em>K</em></span>-medoids
with the <span class="math inline"><em>l</em><sub>1</sub></span>-norm.
<em>TAI</em>, <em>5</em>(4), 1769–1777. (<a
href="https://doi.org/10.1109/TAI.2023.3298752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {K -medoids clustering is one of the most popular techniques in exploratory data analysis. The most commonly used algorithms to deal with this problem are quadratic on the number of instances, $n$ , and usually the quality of the obtained solutions strongly depends upon their initialization phase. In this work, we propose an algorithm for the $K$ -medoids problem on the $l_{1}$ -norm/Manhattan distance with a computational complexity of ${\mathcal {O}}(n \cdot \max \lbrace \log n, K\rbrace \cdot d)$ , along with theoretical guarantees in terms of the accuracy of the obtained approximation. In addition, we propose a cheap split–merge mechanism that can be used to restart the proposed algorithm after its convergence to a fixed point. Under some mild assumptions, we prove that such a restart procedure reduces the error of the given fixed point. The work also includes an extensive experimentation, in which we compare our method to the most popular approaches for the K -medoids problem: PAM, CLARA, and Park&#39;s $K$ -medoids. The obtained empirical results show the proposed algorithm to consistently converge to the solutions with the lowest errors, up to two orders of magnitude of relative error lower than the previously mentioned methods, while also requiring the lowest computational running times among them: up to three orders of magnitude lower.},
  archive      = {J_TAI},
  author       = {Marco Capó and Aritz Pérez and Jose A. Lozano},
  doi          = {10.1109/TAI.2023.3298752},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1769-1777},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Fast $K$-medoids with the $l_{1}$-norm},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A visual and textual information fusion-based zero-shot
framework for hazardous material placard detection and recognition.
<em>TAI</em>, <em>5</em>(4), 1755–1768. (<a
href="https://doi.org/10.1109/TAI.2023.3298588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatically detecting and recognizing hazardous material placards using computer vision-based methods ensures safe operations and proper management of dangerous freight transportation. Deep learning-based object detection methods provide viable and practical solutions to varied applications. However, contemporary deep learning-based methods suffer from imbalanced and unseen classes, which are very common in real-life data. Thus, this study, drawing attention to this hitherto neglected challenge in real-world applications, proposes a deep learning-based zero-shot framework to detect and recognize the hazardous material placards of both imbalanced and open classes. A logarithmic weighted cross-entropy is proposed to balance the closed classes during training. In addition, a logarithmic weighted confidence fusion strategy is designed to fuse the separately extracted visual and textual information. The experiments on real-world transportation data demonstrated the proposed framework&#39;s effectiveness and superiority over other state-of-the-art methods. Notably, our framework outperforms the previous method with a remarkable margin of 12.8% in the F1 score on the placard dataset. This study solves the imbalanced and open class problem by fusing object visual information and text information, providing a practical industrial application of the zero-shot learning concept.},
  archive      = {J_TAI},
  author       = {Ran Zhang and Zhila Bahrami and Ke Feng and Zheng Liu},
  doi          = {10.1109/TAI.2023.3298588},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1755-1768},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A visual and textual information fusion-based zero-shot framework for hazardous material placard detection and recognition},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Antenna parameter measurement network with dual attention
and focus loss using UAV. <em>TAI</em>, <em>5</em>(4), 1743–1754. (<a
href="https://doi.org/10.1109/TAI.2023.3297991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although deep learning has proven to be a successful and widely used technology across various industries, its drawbacks such as large models and difficulties in layout and maintenance in practical tasks have gradually become prominent. In view of the limitations and issues with the traditional method of measuring antenna parameters for mobile communication base stations, we are exploring the development of a new system that is designed to overcome the inefficiencies and potential risks associated with conventional labor-intensive methods. An effective measurement system, which is composed of a novel instance segmentation network with dual attention and focal loss, can accurately fathom out the antenna parameters in mobile communication base stations and completely subvert traditional measurement methods. To begin with, antenna video data are collected by an unmanned aerial vehicle (UAV), which flies around the base station; then a designed instance segmentation network is employed to process and segment mobile communication base station antennas. At last, we implement real-time adjustments to control the actions of the UAV based on algorithmic measurements displayed on the accompanying mobile application. Our measurement system has been shown to greatly enhance measurement efficiency and accuracy, as evidenced by the results of our experiments. Quantitative results that are in line with industry standards show that our measurement system has strong robustness and reproducibility.},
  archive      = {J_TAI},
  author       = {Ying Xu and Qirui Ke and Ziyi Jiang and Yikui Zhai and Angelo Genovese and Vincenzo Piuri and Fabio Scotti},
  doi          = {10.1109/TAI.2023.3297991},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1743-1754},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Antenna parameter measurement network with dual attention and focus loss using UAV},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bagging and boosting fine-tuning for ensemble learning.
<em>TAI</em>, <em>5</em>(4), 1728–1742. (<a
href="https://doi.org/10.1109/TAI.2023.3296685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble learning aggregates outputs from multiple base learners for better performance. Bootstrap aggregating (bagging) and boosting are two popular such approaches. They are suitable for integrating unstable base learners with large variance and weak base learners with large bias, respectively, but not base learners with small variance and/or bias, e.g., support vector machine, regularized logistic regression, and ridge regression. This article proposes two novel ensemble-learning-based fine-tuning approaches, boosting fine-tuning (BF) and bagging and boosting fine-tuning (BBF), to fine-tune learners with small variance and/or bias for better performance. BF embeds boosting in a single hidden layer neural network. In each iteration, BF first uses the Newton&#39;s method to generate a temporary training set, and then trains a boosting learner on it. BBF combines BF and bagging. It first uses bootstrap to obtain multiple replicas of the training set, and then trains a BF learner on each replica. Extensive experiments on 46 real-world datasets demonstrated that BBF is flexible, robust, and effective, and can fine-tune many popular classifiers to achieve better generalization performance.},
  archive      = {J_TAI},
  author       = {Changming Zhao and Ruimin Peng and Dongrui Wu},
  doi          = {10.1109/TAI.2023.3296685},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1728-1742},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Bagging and boosting fine-tuning for ensemble learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ImageNet classification using WordNet hierarchy.
<em>TAI</em>, <em>5</em>(4), 1718–1727. (<a
href="https://doi.org/10.1109/TAI.2023.3297086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (ConvNets) have become increasingly popular for image classification tasks. All contemporary computer vision problems are being dominated by ConvNets. Conventional training methods using cross-entropy loss for training have constantly outperformed the state-of-the-art technique to set a new standard in the ImageNet classification challenge. However, growing accuracy come at the cost of enormous number of parameters and computations. Further, classical learning algorithms do not utilize the semantic relationship between the classes present in the dataset. Thus, interpreting the behavior of the model become difficult even though the results may be desirable. Hence, we demonstrate a classification method by leveraging the WordNet hierarchy on the ImageNet dataset to establish class relationships and label embedding. The model is trained using cross entropy with soft labels based on the semantic similarity between the generated output and the ground truth. Unlike categorical cross entropy, it does not treat every predicted label as equally erroneous. The method generates meaningful neighboring classes in the feature space of the true label.},
  archive      = {J_TAI},
  author       = {Ankita Chatterjee and Jayanta Mukherjee and Partha Pratim Das},
  doi          = {10.1109/TAI.2023.3297086},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1718-1727},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {ImageNet classification using WordNet hierarchy},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A practical recipe for federated learning under statistical
heterogeneity experimental design. <em>TAI</em>, <em>5</em>(4),
1708–1717. (<a href="https://doi.org/10.1109/TAI.2023.3297090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has been an area of active research in recent years. There have been numerous studies in FL to make it more successful in the presence of data heterogeneity. However, despite the existence of many publications, the state of progress in the field is unknown. Many of the works use inconsistent experimental settings and there are no comprehensive studies on the effect of FL-specific experimental variables on the results and practical insights for a more comparable and consistent FL experimental setup. Furthermore, the existence of several benchmarks and confounding variables has further complicated the issue of inconsistency and ambiguity. In this work, we present the first comprehensive study on the effect of FL-specific experimental variables in relation to each other and performance results, bringing several insights and recommendations for designing a meaningful and well-incentivized FL experimental setup. We further aid the community by releasing FedZoo-Bench, an open-source library based on PyTorch with pre-implementation of 22 state-of-the-art methods, and a broad set of standardized and customizable features. We also provide a comprehensive comparison of several SOTA ethods to better understand the current state of the field and existing limitations.},
  archive      = {J_TAI},
  author       = {Mahdi Morafah and Weijia Wang and Bill Lin},
  doi          = {10.1109/TAI.2023.3297090},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1708-1717},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A practical recipe for federated learning under statistical heterogeneity experimental design},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dilation-supervised learning: A novel strategy for scale
difference in retinal vessel segmentation. <em>TAI</em>, <em>5</em>(4),
1693–1707. (<a href="https://doi.org/10.1109/TAI.2023.3296687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinal fundus image segmentation based on deep learning is an important method for auxiliary diagnosis of ophthalmic diseases. Due to the scale difference of the blood vessels and the imbalance between foreground and background pixels in the fundus image, the deep learning network will inevitably ignore thin vessels when downsampling and feature learning. For the scale difference problem, this article aims to tackle its limitation from two perspectives: changing the supervised approach and adapting the feature learning. Correspondingly, a dilation-supervised learning method and an adaptive scale dimensional attention mechanism which are used to construct a two-stage segmentation model is proposed. Moreover, we introduce a quantitative approach to evaluate the scale difference of the blood vessels. With the help of the proposed weighted loss function, the segmentation results are refined, and the class imbalance problem between foreground and background pixels is resolved. Finally, the proposed adaptive threshold selection method is used in the postprocessing of segmentation results. The experiments on DRIVE, STARE, CHASE_DB1, and HRF datasets show that the proposed method achieves better segmentation performance compared with other state-of-the-art methods, and has good generalization ability and robustness.},
  archive      = {J_TAI},
  author       = {Huadeng Wang and Wenbin Zuo and Bingbing Li and Xipeng Pan and Zhenbing Liu and Rushi Lan and Xiaonan Luo},
  doi          = {10.1109/TAI.2023.3296687},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1693-1707},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Dilation-supervised learning: A novel strategy for scale difference in retinal vessel segmentation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Empirical analysis of squeeze and excitation-based densely
connected CNN for chili leaf disease identification. <em>TAI</em>,
<em>5</em>(4), 1681–1692. (<a
href="https://doi.org/10.1109/TAI.2024.3364126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chili is one of the world&#39;s most extensively used spices. The changes in weather and environmental circumstances cause several diseases in chili leaves, which have an impact on yield of the spice. An optimized squeeze and excitation densely connected convolutional neural network (SEDCNN) architecture is proposed for early detection of chili leaf diseases. In the proposed research work, integration of SE block including pre-SE, post-SE, identity-SE, and standard-SE is individually performed and among these standard SE integrated with SEDCNN provides better performance. This architecture is evaluated with artificial neural network (ANN) conventional CNN, dense CNN, ResNet, ResNet with SE, separable standard SE, grouped standard SE, dilated standard SE, and nine different transfer learning networks. The standard-SE integrated with SEDCNN architecture has achieved higher disease identification accuracy of 97%. The empirical analysis of various block SEDCNN architectures and hyperparameter fine tuning resulted in the best accuracy with eight block SEDCNN, Adam optimizer with learning rate 0.001, batch size of 16, reduction ratio of 64, global average pooling squeeze operator, sigmoid excitation operator, Xavier normal kernel initializer, and categorical cross-entropy loss function. The proposed SEDCNN architecture integrated with standard SE identifies chili leaf disease with high accuracy, due to the incorporation of skip connections, SE blocks, and reduction ratio in the proposed architecture. The skip connection is utilized for feature forwarding, and the convolution layer weight update is being used for squeeze and excitation. Further, chili leaf disease identification accuracy is improved to 98.86% with augmentation.},
  archive      = {J_TAI},
  author       = {Naresh V. and Yogeswararao G. and Nageswararao Naik B. and Malmathanraj R. and Palanisamy P.},
  doi          = {10.1109/TAI.2024.3364126},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1681-1692},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Empirical analysis of squeeze and excitation-based densely connected CNN for chili leaf disease identification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generating long financial report using conditional
variational autoencoders with knowledge distillation. <em>TAI</em>,
<em>5</em>(4), 1669–1680. (<a
href="https://doi.org/10.1109/TAI.2024.3351594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating financial reports from a piece of news is a challenging task due to the lack of sufficient background knowledge to effectively generate long financial reports. To address this issue, this article proposes a conditional variational autoencoders (CVAEs)-based approach that distills external knowledge from a set of news–report data. Specifically, we design an encoder–decoder architecture to learn the latent variable distribution from this set of news–report data to provide background knowledge. Next, a teacher–student network is employed to distill knowledge to refine the output of the decoder component. To evaluate the model performance, extensive experiments have been performed on two public datasets using evaluation criteria like BLEU, ROUGE, METEOR, and human evaluation. Our promising experimental results demonstrate that our proposed approach outperforms existing state-of-the-art approaches.},
  archive      = {J_TAI},
  author       = {Ziao Wang and Yunpeng Ren and Xiaofeng Zhang and Yiyuan Wang},
  doi          = {10.1109/TAI.2024.3351594},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1669-1680},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Generating long financial report using conditional variational autoencoders with knowledge distillation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decomposing visual and semantic correlations for both fully
supervised and few-shot image classification. <em>TAI</em>,
<em>5</em>(4), 1658–1668. (<a
href="https://doi.org/10.1109/TAI.2023.3329457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most image classification methods are designed to either boost the classification accuracies with abundant supervision, or cope with the shortage of supervision information. This is often achieved by using the visual and semantic information of other sources. However, these methods use the visual and semantic information within the same class independently, leaving their intrinsic correlations unconsidered. Objects and disturbing components as well as noise exist on the same image. Besides, semantic representations also contain noisy information. To solve the problems mentioned above, we propose a novel method for both fully supervised image classification and few-shot image classification by decomposing visual and semantic correlations. We jointly explore the intrinsic correlations of visual and semantic information of images within the same class. For each class, we decompose its visual and semantic correlations using low-rank and sparse constraint respectively. The decomposed low-rank parts character the intrinsic correlations of images can be used in a linear transformation way. Using the decomposed parts of each class, Classification can be achieved by reconstruction error minimization. We conduct experiments on several datasets for both fully supervised image classification and few-shot image classification. Experimental results well show the effectiveness of the proposed method.},
  archive      = {J_TAI},
  author       = {Chunjie Zhang and Xiaolong Zheng},
  doi          = {10.1109/TAI.2023.3329457},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1658-1668},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Decomposing visual and semantic correlations for both fully supervised and few-shot image classification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A simple yet effective framelet-based graph neural network
for directed graphs. <em>TAI</em>, <em>5</em>(4), 1647–1657. (<a
href="https://doi.org/10.1109/TAI.2023.3316628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a spectral-based graph convolutional network for directed graphs. The proposed model employs the classic singular value decomposition (SVD) to perform signal decomposition directly on the asymmetric adjacency matrix. This strategy is simple, which allows many existing spectral-based methods to be adapted to directed graphs. We particularly utilize framelets-based filtering, which significantly enhances the learning capacity due to the separated modeling of information at different frequencies. We empirically observe the proposed model achieves the state-of-the-art results on various datasets. We also show that the model is robust to feature perturbation.},
  archive      = {J_TAI},
  author       = {Chunya Zou and Andi Han and Lequan Lin and Ming Li and Junbin Gao},
  doi          = {10.1109/TAI.2023.3316628},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1647-1657},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A simple yet effective framelet-based graph neural network for directed graphs},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Minimizing parameter overhead in self-supervised models for
target task. <em>TAI</em>, <em>5</em>(4), 1635–1646. (<a
href="https://doi.org/10.1109/TAI.2023.3322394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised deep learning models encounter two major challenges: labeled datasets for training and parameter overhead, which leads to extensive GPU usage and other computational resource requirements. Several CNN models show state-of-the-art performance while compromising with either of the challenges. Self-supervised models reduce the requirement for labeled training data; however, the problems of parameter overhead and GPU usage are rarely addressed. This article proposes a method to address the two challenges of the image classification task. We introduce a transfer learning approach for a target dataset, in which we take the learned features from a self-supervised model after minimizing its parameters by removing the final layer. The learned features are then fed into a CNN, followed by a multilayer perceptron (MLP), where the hyperparameters of both the CNN and the MLP are automatically tuned (autotuned) using a Bayesian-optimization-based technique. Furthermore, we reduce giga floating point operations per second (GFLOPs) by limiting the search space for the hyperparameters, not compromising the performance. The first challenge is addressed by utilizing the learned representations from the self-supervised model as a foundation for knowledge transfer in the proposed model. Rather than relying solely on labeled data, we employ the insights from unlabeled data by transferring knowledge from self-supervised models to the target task, hence reducing the cost and effort associated with data annotation. We address the second challenge by utilizing a minimized self-supervised backbone model and constraining the search space. We experiment with a wide variety of benchmark datasets, such as CIFAR-10, CIFAR-100, Oxford-IIIT-Pet, Oxford-102-flowers and Caltech-101, to establish the efficacy.},
  archive      = {J_TAI},
  author       = {Jaydeep Kishore and Snehasis Mukherjee},
  doi          = {10.1109/TAI.2023.3322394},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1635-1646},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Minimizing parameter overhead in self-supervised models for target task},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive intelligent control-based consensus tracking for a
class of switched nonstrict feedback nonlinear multiagent systems with
unmodeled dynamics. <em>TAI</em>, <em>5</em>(4), 1624–1634. (<a
href="https://doi.org/10.1109/TAI.2023.3300818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on the adaptive intelligent control technique, we develop an adaptive consensus tracking control scheme for the distributed switched nonstrict feedback nonlinear multiagent systems (MASs) with unmodeled dynamics. According to the properties of Gaussian functions, a fresh scheme is utilized to handle the nonstrict feedback form. Based on the characters of unmodeled dynamics, a dynamic signal is designed to resolve the design difficulties caused by the unmodeled dynamics. In the design process of the adaptive switching controllers, the backstepping technique is adopted and the unknown nonlinear functions are estimated by utilizing the radial basis function neural networks&#39; intelligent technique. Besides, a common Lyapunov function is constructed to analyze the system&#39;s stability. Under the Lyapunov stability theory, it is rigorously demonstrated that all signals of the closed-loop switched MAS are cooperatively semiglobal uniformly ultimately bounded and the outputs of all the followers eventually track the leader&#39;s output. Finally, the simulation results of a practical example is presented to confirm the efficiency of the proposed control scheme.},
  archive      = {J_TAI},
  author       = {Ben Niu and Xinyu Liu and Zhihua Guo and Hao Jiang and Huanqing Wang},
  doi          = {10.1109/TAI.2023.3300818},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1624-1634},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adaptive intelligent control-based consensus tracking for a class of switched nonstrict feedback nonlinear multiagent systems with unmodeled dynamics},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised representation learning for knee injury
diagnosis from magnetic resonance data. <em>TAI</em>, <em>5</em>(4),
1613–1623. (<a href="https://doi.org/10.1109/TAI.2023.3299883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical image analysis, the cost of acquiring high-quality data and annotation by experts is a barrier in many medical applications. Most of the techniques used are based on a supervised learning framework and require a large amount of annotated data to achieve satisfactory performance. As an alternative, in this article, we propose a self-supervised learning approach for learning the spatial anatomical representations from the frames of magnetic resonance (MR) video clips for the diagnosis of knee medical conditions. The pretext model learns meaningful context-invariant spatial representations. The downstream task in our article is a class-imbalanced multilabel classification. Different experiments show that the features learned by the pretext model provide competitive performance in the downstream task. Moreover, the efficiency and reliability of the proposed pretext model in learning representations of minority classes without applying any strategy toward imbalance in the dataset can be seen from the results. To the best of our knowledge, this work is the first of its kind in showing the effectiveness and reliability of self-supervised learning algorithms in imbalanced multilabel classification tasks on MR scans.},
  archive      = {J_TAI},
  author       = {Siladittya Manna and Saumik Bhattacharya and Umapada Pal},
  doi          = {10.1109/TAI.2023.3299883},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1613-1623},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Self-supervised representation learning for knee injury diagnosis from magnetic resonance data},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modified ResNet-152 network with hybrid pyramidal pooling
for local change detection. <em>TAI</em>, <em>5</em>(4), 1599–1612. (<a
href="https://doi.org/10.1109/TAI.2023.3299903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we put forth a unique attempt to detect the local changes in challenging video scenes by exploring the capabilities of an encoder-decoder type network that employs a modified ResNet-152 architecture with a multi-scale feature extraction (MFE) framework. The proposed encoder network consists of a modified ResNet-152 network where the initial two blocks are freeze and the weights of the last blocks are learned using a transfer learning mechanism. The said encoder can reduce the computational complexity and extract fine as well as coarse-scale features. We have proposed an MFE mechanism block which is a hybridization of pyramidal pooling architecture (PPA), and various atrous convolutional layers where the high-level features from the encoder network are utilized to extract multi-scale features. The use of PPA in the MFE block preserves maximum value in every pooling area, to retain the contextual relationship between the pixels in the complex video frames that can handle various challenging scenes. The proposed decoder network consists of stacked transposed convolution layers that learn a mapping from feature space to image space, predicting a score map. Then, a threshold is applied on the score map to get the binary class labels as the background and foreground. The performance of the proposed scheme is validated by testing it against 31 state-of-the-art techniques. The results obtained by the proposed method are corroborated qualitatively as well as quantitatively. Further, the efficacy of the proposed algorithm is verified with an unseen video setup and is found to provide better performance.},
  archive      = {J_TAI},
  author       = {Manoj Kumar Panda and Badri Narayan Subudhi and Thangaraj Veerakumar and Vinit Jakhetiya},
  doi          = {10.1109/TAI.2023.3299903},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1599-1612},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Modified ResNet-152 network with hybrid pyramidal pooling for local change detection},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal refinement graph convolutional network for
skeleton-based action recognition. <em>TAI</em>, <em>5</em>(4),
1586–1598. (<a href="https://doi.org/10.1109/TAI.2023.3329799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human skeleton data, which has served in the aspect of human activity recognition, ought to be the most representative biometric characteristics due to its intuitivity and visuality. The state-of-the-art approaches mainly focus on improving modeling spatial correlations within graph topologies. However, the interframes motional representations are also of vital importance, and we argue that they are worth paying attention to and exploring. Therefore, a temporal refinement module with contrastive learning mechanism is proposed, fuzing as a complementary to the conventional spatial graph convolution layer. In addition, in order to further exploiting the interframe variances and generalizing graph convolutional network (GCN) operation to temporal dimension, a temporal-correlation matrix is introduced to effectively capture dynamic dependencies within frame-pairs, enhancing semantic feature representation. Moreover, since GCN is a typical local operator which lacks of capability to fully model the long-term relations along spatial and temporal variation, to move beyond the limitation, a spatial-temporal cascaded aggregation (STCA) module is designed to enlarge the receptive filter scale. The overall recognition framework consists of three above novelties, which is capable of achieving remarkable performance by evaluating on benchmark datasets (i.e., NTU RGB+D 60, NTU RGB+D 120, PKU-MMD, and Kinetics Skeleton 400). Extensive experiments demonstrate the effectiveness of the proposed framework, e.g., performing recognition accuracy rate of 90.9% and 96.8% on NTU RGB+D 60, 87.9% and 88.9% on NTU RGB+D 120.},
  archive      = {J_TAI},
  author       = {Tianming Zhuang and Zhen Qin and Yi Ding and Fuhu Deng and Leduo Chen and Zhiguang Qin and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TAI.2023.3329799},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1586-1598},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Temporal refinement graph convolutional network for skeleton-based action recognition},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low-quality deepfake detection via unseen artifacts.
<em>TAI</em>, <em>5</em>(4), 1573–1585. (<a
href="https://doi.org/10.1109/TAI.2023.3299894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of manipulated media over the Internet has become a major source of concern in recent times. With the wide variety of techniques being used to create fake media, it has become increasingly difficult to identify such occurrences. While existing algorithms perform well on the detection of such media, limited algorithms take the impact of compression into account. Different social media platforms use different compression factors and algorithms before sharing such images and videos, which amplifies the issues in their identification. Therefore, it has become imperative that fake media detection algorithms work well for data compressed at different factors. To this end, the focus of this article is detecting low-quality fake videos in the compressed domain. The proposed algorithm distinguishes real images and videos from altered ones by using a learned visibility matrix, which enforces the model to see unseen imperceptible artifacts in the data. As a result, the learned model is robust to loss of information due to data compression. The performance is evaluated on three publicly available datasets, namely Celeb-DF, FaceForensics, and FaceForensics++, with three manipulation techniques, viz., Deepfakes, Face2Face, and FaceSwap. Experimental results show that the proposed approach is robust under different compression factors and yields state-of-the-art performance on the FaceForensics++ and Celeb-DF datasets with 97.14% classification accuracy and 74.45% area under the curve, respectively.},
  archive      = {J_TAI},
  author       = {Saheb Chhabra and Kartik Thakral and Surbhi Mittal and Mayank Vatsa and Richa Singh},
  doi          = {10.1109/TAI.2023.3299894},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1573-1585},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Low-quality deepfake detection via unseen artifacts},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized maximum entropy reinforcement learning via
reward shaping. <em>TAI</em>, <em>5</em>(4), 1563–1572. (<a
href="https://doi.org/10.1109/TAI.2023.3297988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entropy regularization is a commonly used technique in reinforcement learning to improve exploration and cultivate a better pre-trained policy for later adaptation. Recent studies further show that the use of entropy regularization can smooth the optimization landscape and simplify the policy optimization process, indicating the value of integrating entropy into reinforcement learning. However, existing studies only consider the policy&#39;s entropy at the current state as an extra regularization term in the policy gradient or in the objective function without formally integrating the entropy in the reward function. In this article, we propose a shaped reward that includes the agent&#39;s policy entropy into the reward function. In particular, the agent&#39;s expected entropy over a distribution of the next state is added to the immediate reward associated with the current state. The addition of the agent&#39;s expected policy entropy at the next state distribution is shown to yield new soft Q -function and state function that are concise and modular. Moreover, the new reinforcement learning framework can be easily applied to the existing standard reinforcement learning algorithms, such as deep q-network (DQN) and proximal policy optimization (PPO), while inheriting the benefits of employing entropy regularization. We further present a soft stochastic policy gradient theorem based on the shaped reward and propose a new practical reinforcement learning algorithm. Finally, a few experimental studies are conducted in MuJoCo environment to demonstrate that our method can outperform an existing state-of-the-art off-policy maximum entropy reinforcement learning approach soft actor-critic by 5%–150% in terms of average return.},
  archive      = {J_TAI},
  author       = {Feng Tao and Mingkang Wu and Yongcan Cao},
  doi          = {10.1109/TAI.2023.3297988},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1563-1572},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Generalized maximum entropy reinforcement learning via reward shaping},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised graph information bottleneck for multiview
molecular embedding learning. <em>TAI</em>, <em>5</em>(4), 1554–1562.
(<a href="https://doi.org/10.1109/TAI.2023.3297576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of computer-aided drug discovery, identifying promising drug candidates from small molecule libraries requires meaningful molecular embeddings for downstream tasks, such as property prediction. However, obtaining experimentally determined molecular property measurements is often expensive and time-consuming, making it challenging to train molecular encoders with limited supervision. In addition, molecules can be represented in two ways: as 2-D chemical-bond structures and 3-D geometry structures. Molecular embedding learning using only one of these representations can result in information loss, and effective fusion of the two views has not been fully explored. To address these challenges, we propose a new approach called the self-supervised multiview graph neural network (SMV-GNN) for molecular embedding learning. Our approach involves a self-supervised task that promotes the representation ability of the molecular encoder without requiring extra human-annotation data. Specifically, we use chemical-bond-based graph structures as inputs to predict interatom distances from the 2-D view and randomly shuffle a ratio of atoms in the 3-D coordinate-based graphs to predict atom rationality from the 3-D view. We further improve the representation ability of the molecular embedding by using information bottleneck to learn essential shared feature representations by discarding superfluous information from the 2-D/3-D views for downstream tasks. We evaluate our proposed SMV-GNN approach on seven benchmark datasets for molecule property-prediction tasks, and demonstrate that it outperforms the current state-of-the-art methods.},
  archive      = {J_TAI},
  author       = {Changsheng Li and Kaihang Mao and Shiye Wang and Ye Yuan and Guoren Wang},
  doi          = {10.1109/TAI.2023.3297576},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1554-1562},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Self-supervised graph information bottleneck for multiview molecular embedding learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of security protection methods for deep learning
model. <em>TAI</em>, <em>5</em>(4), 1533–1553. (<a
href="https://doi.org/10.1109/TAI.2023.3314398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning (DL) models have attracted widespread concern. Due to its own characteristics, DL has been successfully applied in the fields of object detection, superresolution reconstruction, speech recognition, natural language processing, etc., bringing high efficiency to industrial production and daily life. With the Internet of Things, 6G and other new technologies have been proposed, leading to an exponential growth in data volume. DL models currently suffer from some security issues, such as privacy issues during data collection, defense issues during model training and deployment, etc. The sensitive data of users and special institutions that are directly used as training data of DL models may lead to information leakage and serious privacy problems. In addition, DL models have encountered many malicious attacks in the real world, such as poisoning attack, exploratory attack, adversarial attack, etc., which caused model security problems. Therefore, this article discusses ways of ensuring the security and data privacy of DL models under diversified attack methods and the ways of ensuring the privacy security of edge mobile devices equipped with pretrained deep neural networks. Alternatively, this article analyzes the privacy security of DL models for typical deployment platforms such as server/cloud, edge mobile device, and web browser and, then, summarizes future research direction.},
  archive      = {J_TAI},
  author       = {Haipeng Peng and Shuang Bao and Lixiang Li},
  doi          = {10.1109/TAI.2023.3314398},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1533-1553},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A survey of security protection methods for deep learning model},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contrastive-enhanced domain generalization with federated
learning. <em>TAI</em>, <em>5</em>(4), 1525–1532. (<a
href="https://doi.org/10.1109/TAI.2023.3298297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain generalization (DG) aims to train a global model from different but related domains, which can be generalized to an unseen out-of-distribution domain. Most existing DG methods are based on the centralized learning paradigm, raising the privacy leakage concern. In this article, we propose a contrastive-enhanced domain generalization framework in the federated learning paradigm, where there are a server and multiple clients. Each client owns data from one domain and builds a local model consisting of a domain-invariant feature extractor and a classifier head. The server generates a global model through aggregating and broadcasting local models&#39; parameters, thus achieving knowledge sharing and keeping data confidential. To enhance the discrimination and generalization ability of the local model, we build an improved instance normalization module that focuses on task-relevant features with less domain-specific information. Moreover, for better classwise alignment in the embedding space, we propose a prototype-based contrastive loss. Given the limited annotation budget in practice, we also extend the proposed framework into the semisupervised DG setting (i.e., only ten labeled samples per class). Experimental results on three benchmarks and different backbones show that the proposed framework yields promising performances for both DG and semisupervised DG in the federated learning paradigm.},
  archive      = {J_TAI},
  author       = {Xinhui Yu and Dan Wang and Martin J. McKeown and Z. Jane Wang},
  doi          = {10.1109/TAI.2023.3298297},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1525-1532},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Contrastive-enhanced domain generalization with federated learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interpreting tangled program graphs under partially
observable dota 2 invoker tasks. <em>TAI</em>, <em>5</em>(4), 1511–1524.
(<a href="https://doi.org/10.1109/TAI.2023.3279057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interpretable learning agents directly construct models that provide insight into the relationships learnt. Moreover, to date, there has been a lot of emphasis on interpreting reactive models developed for supervised learning tasks. In this article, we consider the case of models developed to address a suite of six partially observable tasks defined in the Dota 2 Online Battle Arena game engine. This means that learning agents need to make decisions based on the previous state as developed by the learning agent&#39;s memory, in addition to a 310-D state vector provided by the game engine. Interpretability is addressed by adopting the tangled program graph approach to developing learning agents. Thus, decision making is explicitly divide-and-conquer, with different parts of the resulting graph visited depending on the task context. We demonstrate that programs comprising the tangled program graph approach self-organize such that: 1) small subsets of task features are identified to define conditions under which index memory is written and 2) the subset of programs responsible for defining actions typically query indexed memory rather than task features. Particular preferences emerge for different tasks; thus, the blocking (or evasion) tasks result in a preference for specific actions, whereas more open-ended tasks assume policies based on combinations of behaviors. In short, the ability to evolve the topology of the learning agent provides insights into how the policies are being constructed for addressing partially observable tasks.},
  archive      = {J_TAI},
  author       = {Robert J. Smith and Malcolm I. Heywood},
  doi          = {10.1109/TAI.2023.3279057},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1511-1524},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Interpreting tangled program graphs under partially observable dota 2 invoker tasks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised interpretable basis extraction for
concept-based visual explanations. <em>TAI</em>, <em>5</em>(4),
1496–1510. (<a href="https://doi.org/10.1109/TAI.2023.3338169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important line of research attempts to explain convolutional neural network (CNN) image classifier predictions and intermediate layer representations in terms of human understandable concepts. In this work, we expand on previous works in the literature that use annotated concept datasets to extract interpretable feature space directions and propose an unsupervised post-hoc method to extract a disentangling interpretable basis by looking for the rotation of the feature space that explains sparse one-hot thresholded transformed representations of pixel activations. We do experimentation with existing popular CNNs and demonstrate the effectiveness of our method in extracting an interpretable basis across network architectures and training datasets. We make extensions to the existing basis interpretability metrics found in the literature and show that intermediate layer representations become more interpretable when transformed to the bases extracted with our method. Finally, using the basis interpretability metrics, we compare the bases extracted with our method with the bases derived with a supervised approach and find that, in one aspect, the proposed unsupervised approach has a strength that constitutes a limitation of the supervised one and give potential directions for future research.},
  archive      = {J_TAI},
  author       = {Alexandros Doumanoglou and Stylianos Asteriadis and Dimitrios Zarpalas},
  doi          = {10.1109/TAI.2023.3338169},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1496-1510},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Unsupervised interpretable basis extraction for concept-based visual explanations},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prototype-based interpretable graph neural networks.
<em>TAI</em>, <em>5</em>(4), 1486–1495. (<a
href="https://doi.org/10.1109/TAI.2022.3222618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks have proved to be a key tool for dealing with many problems and domains, such as chemistry, natural language processing, and social networks. While the structure of the layers is simple, it is difficult to identify the patterns learned by the graph neural network. Several works propose post hoc methods to explain graph predictions, but few of them try to generate interpretable models. Conversely, the topic of the interpretable models is highly investigated in image recognition. Given the similarity between image and graph domains, we analyze the adaptability of prototype-based neural networks for graph and node classification. In particular, we investigate the use of two interpretable networks, ProtoPNet and TesNet, in the graph domain. We show that the adapted networks manage to reach better or higher accuracy scores than their respective black-box models and comparable performances with state-of-the-art self-explainable models. Showing how to extract ProtoPNet and TesNet explanations on graph neural networks, we further study how to obtain global and local explanations for the trained models. We then evaluate the explanations of the interpretable models by comparing them with post hoc approaches and self-explainable models. Our findings show that the application of TesNet and ProtoPNet to the graph domain produces qualitative predictions while improving their reliability and transparency.},
  archive      = {J_TAI},
  author       = {Alessio Ragno and Biagio La Rosa and Roberto Capobianco},
  doi          = {10.1109/TAI.2022.3222618},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1486-1495},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Prototype-based interpretable graph neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An explainable and actionable mistrust scoring framework for
model monitoring. <em>TAI</em>, <em>5</em>(4), 1473–1485. (<a
href="https://doi.org/10.1109/TAI.2023.3272876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous monitoring of trained ML models to determine when their predictions should and should not be trusted is essential for their safe deployment. Such a framework ought to be high-performing, explainable, post hoc , and actionable. We propose TRUST-LAPSE, a “mistrust” scoring framework for continuous model monitoring. We assess the trustworthiness of each input sample&#39;s model prediction using a sequence of latent-space embeddings. Specifically, 1) our latent-space mistrust score estimates mistrust using distance metrics (Mahalanobis distance) and similarity metrics (cosine similarity) in the latent-space, and 2) our sequential mistrust score determines deviations in correlations over the sequence of past input representations in a nonparametric, sliding-window-based algorithm for actionable continuous monitoring. We evaluate TRUST-LAPSE via two downstream tasks: 1) distributionally shifted input detection; and 2) data drift detection. We evaluate across diverse domains—audio and vision using public datasets and further benchmark our approach on challenging, real-world electroencephalograms (EEG) datasets for seizure detection. Our latent-space mistrust scores achieve state-of-the-art results with AUROCs of 84.1 (vision), 73.9 (audio), and 77.1 (clinical EEGs), outperforming baselines by over 10 points. We expose critical failures in popular baselines that remain insensitive to input semantic content, rendering them unfit for real-world model monitoring. We show that our sequential mistrust scores achieve high drift detection rates; over 90% of the streams show $&amp;lt;\! 20{\%}$ error for all domains. Through extensive qualitative and quantitative evaluations, we show that our mistrust scores are more robust and provide explainability for easy adoption into practice.},
  archive      = {J_TAI},
  author       = {Nandita Bhaskhar and Daniel L. Rubin and Christopher Lee-Messer},
  doi          = {10.1109/TAI.2023.3272876},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1473-1485},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An explainable and actionable mistrust scoring framework for model monitoring},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Why should i trust your explanation? An evaluation approach
for XAI methods applied to predictive process monitoring results.
<em>TAI</em>, <em>5</em>(4), 1458–1472. (<a
href="https://doi.org/10.1109/TAI.2024.3357041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a use case of process mining, predictive process monitoring (PPM) aims to provide information on the future course of running business process instances. A large number of available PPM approaches adopt predictive models based on machine learning (ML). With the improved efficiency and accuracy of ML models usually being coupled with increasing complexity, their understandability becomes compromised. Having the user at the center of attention, various eXplainable artificial intelligence (XAI) methods emerged to provide users with explanations of the reasoning process of an ML model. Though there is a growing interest in applying XAI methods to PPM results, various proposals have been made to evaluate explanations according to different criteria. In this article, we propose an approach to quantitatively evaluate XAI methods concerning their ability to reflect the facts learned from the underlying stores of business-related data, i.e., event logs. Our approach includes procedures to extract features that are crucial for generating predictions. Moreover, it computes ratios that have proven to be useful in differentiating XAI methods. We conduct experiments that produce useful insights into the effects of the various choices made through a PPM workflow. We can show that underlying data and model issues can be highlighted using the applied XAI methods. Furthermore, we could penalize and reward XAI methods for achieving certain levels of consistency with the facts learned about the underlying data. Our approach has been applied to different real-life event logs using different configurations of the PPM workflow.},
  archive      = {J_TAI},
  author       = {Ghada Elkhawaga and Omar M. Elzeki and Mervat Abu-Elkheir and Manfred Reichert},
  doi          = {10.1109/TAI.2024.3357041},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1458-1472},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Why should i trust your explanation? an evaluation approach for XAI methods applied to predictive process monitoring results},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explain the explainer: Interpreting model-agnostic
counterfactual explanations of a deep reinforcement learning agent.
<em>TAI</em>, <em>5</em>(4), 1443–1457. (<a
href="https://doi.org/10.1109/TAI.2022.3223892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Counterfactual examples (CFs) are one of the most popular methods for attaching post hoc explanations to machine learning models. However, existing CF generation methods either exploit the internals of specific models or depend on each sample&#39;s neighborhood; thus, they are hard to generalize for complex models and inefficient for large datasets. This article aims to overcome these limitations and introduces ReLAX , a model-agnostic algorithm to generate optimal counterfactual explanations. Specifically, we formulate the problem of crafting CFs as a sequential decision-making task. We then find the optimal CFs via deep reinforcement learning (DRL) with discrete-continuous hybrid action space. In addition, we develop a distillation algorithm to extract decision rules from the DRL agent&#39;s policy in the form of a decision tree to make the process of generating CFs itself interpretable. Extensive experiments conducted on six tabular datasets have shown that ReLAX outperforms existing CF generation baselines, as it produces sparser counterfactuals, is more scalable to complex target models to explain, and generalizes to both the classification and regression tasks. Finally, we show the ability of our method to provide actionable recommendations and distill interpretable policy explanations in two practical real-world use cases.},
  archive      = {J_TAI},
  author       = {Ziheng Chen and Fabrizio Silvestri and Gabriele Tolomei and Jia Wang and He Zhu and Hongshik Ahn},
  doi          = {10.1109/TAI.2022.3223892},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1443-1457},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Explain the explainer: Interpreting model-agnostic counterfactual explanations of a deep reinforcement learning agent},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review on explainable artificial intelligence for
healthcare: Why, how, and when? <em>TAI</em>, <em>5</em>(4), 1429–1442.
(<a href="https://doi.org/10.1109/TAI.2023.3266418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) models are increasingly finding applications in the field of medicine. Concerns have been raised about the explainability of the decisions that are made by these AI models. In this article, we give a systematic analysis of explainable artificial intelligence (XAI), with a primary focus on models that are currently being used in the field of healthcare. The literature search is conducted following the preferred reporting items for systematic reviews and meta-analyses standards for relevant work published from 1 January 2012 to 2 February 2022. The review analyzes the prevailing trends in XAI and lays out the major directions in which research is headed. We investigate the why, how, and when of the uses of these XAI models and their implications. We present a comprehensive examination of XAI methodologies as well as an explanation of how a trustworthy AI can be derived from describing AI models for healthcare fields. The discussion of this work will contribute to the formalization of the XAI field.},
  archive      = {J_TAI},
  author       = {Subrato Bharati and M. Rubaiyat Hossain Mondal and Prajoy Podder},
  doi          = {10.1109/TAI.2023.3266418},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1429-1442},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A review on explainable artificial intelligence for healthcare: Why, how, and when?},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guest editorial: New developments in explainable and
interpretable artificial intelligence. <em>TAI</em>, <em>5</em>(4),
1427–1428. (<a href="https://doi.org/10.1109/TAI.2024.3356669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This special issue brings together seven articles that address different aspects of explainable and interpretable artificial intelligence (AI). Over the years, machine learning (ML) and AI models have posted strong performance across several tasks. This has sparked interest in deploying these methods in critical applications like health and finance. However, to be deployable in the field, ML and AI models must be trustworthy. Explainable and interpretable AI are two areas of research that have become increasingly important to ensure trustworthiness and hence deployability of advanced AI and ML methods. Interpretable AI are models that obey some domain-specific constraints so that they are better understandable by humans. In essence, they are not black-box models. On the other hand, explainable AI refers to models and methods that are typically used to explain another black-box model.},
  archive      = {J_TAI},
  author       = {K. P. Suba Subbalakshmi and Wojciech Samek and Xia Ben Hu},
  doi          = {10.1109/TAI.2024.3356669},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1427-1428},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Guest editorial: New developments in explainable and interpretable artificial intelligence},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multicomponent similarity graphs for cross-network node
classification. <em>TAI</em>, <em>5</em>(3), 1411–1424. (<a
href="https://doi.org/10.1109/TAI.2023.3307105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-network node classification aims to train a classifier for an unlabeled target network using a source network with rich labels. In applications, the degree of nodes mostly conforms to the long-tail distribution, i.e., most nodes in the network are tail nodes with sparse neighborhoods. The established methods focus on either the discrepancy cross network or the long tail in a single network. As for the cross-network node classification under long tail, the coexistence of sparsity of tail nodes and the discrepancy cross-network challenges existing methods for long tail or methods for the cross-network node classification. To this end, a multicomponent similarity graphs for cross-network node classification (MS-CNC) is proposed in this article. Specifically, in order to address the sparsity of the tail nodes, multiple component similarity graphs, including attribute and structure similarity graphs, are constructed for each network to enrich the neighborhoods of the tail nodes and alleviate the long-tail phenomenon. Then, multiple representations are learned from the multiple similarity graphs separately. Based on the multicomponent representations, a two-level adversarial model is designed to address the distribution difference across networks. One level is used to learn the invariant representations cross network in view of structure and attribute components separately, and the other level is used to learn the invariant representations in view of the fused structure and attribute graphs. Extensive experimental results show that the MS-CNC outperforms the state-of-the-art methods.},
  archive      = {J_TAI},
  author       = {Yuhong Zhang and Congmei Shi and Xinzheng Li and Zan Zhang and Xuegang Hu},
  doi          = {10.1109/TAI.2023.3307105},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1411-1424},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multicomponent similarity graphs for cross-network node classification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intelligent proximate analysis of coal based on
near-infrared spectroscopy and multioutput deep learning. <em>TAI</em>,
<em>5</em>(3), 1398–1410. (<a
href="https://doi.org/10.1109/TAI.2023.3296714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proximate analysis of coal indicates the moisture, ash, volatile content, and calorific value, which has been widely utilized as the basis for coal characterization. It involves heating the coal under various conditions until a constant weight is obtained. Although it is a relatively simple process that does not require expensive analytical equipment, determining these characteristics is time consuming. An alternative way for proximate analysis is spectral analysis in combination with various machine learning methods. However, most previous works analyze individual characteristics and fail to explore the relationship among them. In this study, we propose a method for proximate analysis based on near-infrared spectroscopy and a multioutput attention Unet (MOA-Unet), which can predict multiple characteristics simultaneously. First, an attention-based Unet is designed as the shared feature extraction subnetwork, including an encoder, a decoder, convolutional block attention modules, and multiscale feature fusion modules, which can improve the representation power of the U-shape network through aggregating features of shallower layers and concatenating features of deeper layers. Second, four individual subnetworks with fully connected layers, designed for four outputs, are utilized for regressing those four characteristics. We employ the gradient normalization algorithm to alleviate the gradient magnitude masking effect caused by training imbalance among different tasks. The proposed MOA-Unet is compared with classical chemometric methods on 670 coal samples from on-site test. The experimental results demonstrate that the proposed model achieves state-of-the-art performance with correlation coefficients of 0.9015, 0.9538, 0.8986, and 0.8884, corresponding to moisture, ash, volatile content, and calorific value, respectively.},
  archive      = {J_TAI},
  author       = {Liang Zou and Jiahui Qiao and Xinhui Yu and Xun Chen and Meng Lei},
  doi          = {10.1109/TAI.2023.3296714},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1398-1410},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Intelligent proximate analysis of coal based on near-infrared spectroscopy and multioutput deep learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rethinking data heterogeneity in federated learning:
Introducing a new notion and standard benchmarks. <em>TAI</em>,
<em>5</em>(3), 1386–1397. (<a
href="https://doi.org/10.1109/TAI.2023.3293068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though successful, federated learning (FL) presents new challenges for machine learning, especially when the issue of data heterogeneity, also known as Non-IID data, arises. To cope with the statistical heterogeneity, previous works incorporated a proximal term in local optimization or modified the model aggregation scheme at the server side or advocated clustered federated learning approaches, where the central server groups agent population into clusters with jointly trainable data distributions to take the advantage of a certain level of personalization. While effective, they lack a deep elaboration on what kind of data heterogeneity and how the data heterogeneity impacts the accuracy performance of the participating clients. In contrast to many of the prior FL approaches, we demonstrate not only the issue of data heterogeneity in current setups is not necessarily a problem but also in fact it can be beneficial for the FL participants. Our observations are intuitive: 1) Dissimilar labels of clients (label skew) are not necessarily considered data heterogeneity, and 2) the principal angle between the clients&#39; data subspaces spanned by their corresponding principal vectors of data is a better estimate of the data heterogeneity.},
  archive      = {J_TAI},
  author       = {Saeed Vahidian and Mahdi Morafah and Chen Chen and Mubarak Shah and Bill Lin},
  doi          = {10.1109/TAI.2023.3293068},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1386-1397},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Rethinking data heterogeneity in federated learning: Introducing a new notion and standard benchmarks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel angular-based unsupervised domain adaptation
framework for image classification. <em>TAI</em>, <em>5</em>(3),
1373–1385. (<a href="https://doi.org/10.1109/TAI.2023.3293077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation (DA) deals with the common problem of distribution mismatch between the training and the target data. A model tested on data that comes from a different source than the training data will not perform well. DA methods aim to utilize the available training and testing data to model a target domain classifier. Domain invariant features are extracted and used to minimize the distribution divergence between the source and target domains. At the same time, there exist some imperative limitations. The angle between the subspaces of the source and target domains is not reduced during distribution alignment, and the formulation of an objective function, that reduces geometric and statistical deviations, must be used. None of the DA strategies in use today address these constraints simultaneously. Therefore, we propose a novel DA framework, called angular-based unsupervised domain adaptation framework for image classification (AUDAF) to address these limitations. AUDAF first obtains the pseudo labels for the unlabeled target domain data using a simple k-Nearest Neighbor classifier (trained on source domain data) and then formulates a robust objective function to reduce domain discrepancy. The objective function can be solved efficiently in a closed form by considering a couple of projection vector matrices (one for source, the other for target). Extensive testing on benchmark DA datasets shows that AUDAF performs better than existing DA methods in terms of classification accuracy.},
  archive      = {J_TAI},
  author       = {Shreyash Mishra and Rakesh Kumar Sanodiya},
  doi          = {10.1109/TAI.2023.3293077},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1373-1385},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A novel angular-based unsupervised domain adaptation framework for image classification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of character recognition model inspired by
visual explanations. <em>TAI</em>, <em>5</em>(3), 1362–1372. (<a
href="https://doi.org/10.1109/TAI.2023.3289167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) currently constitute the best-performing artificial vision systems. However, humans are still better at recognizing many characters, especially distorted, ornamental, or calligraphic characters compared with the highly sophisticated recognition models. Understanding the mechanism of character recognition by humans may give some cues for building better recognition models. However, the appropriate methodological approach to using these cues has not been much explored for developing character recognition models. Therefore, this paper tries to understand the process of character recognition by humans and DNNs by generating visual explanations for their respective decisions. We have used eye tracking to assay the spatial distribution of information hotspots for humans via fixation maps. We have proposed a gradient-based method for visualizing the reasoning behind the model&#39;s decision through visualization maps and have proved that our method is better than the other class activation mapping methods. Qualitative comparison between visualization maps and fixation maps reveals that both model and humans focus on similar regions in character in the case of correctly classified characters. However, when the focused regions are different for humans and model, the characters are typically misclassified by the latter. Hence, we propose to use the fixation maps as a supervisory input to train the model that ultimately results in improved recognition performance and better generalization. As the proposed model gives some insights about the reasoning behind its decision, it can find applications in fields, such as surveillance and medical applications, where explainability helps to determine system fidelity.},
  archive      = {J_TAI},
  author       = {Chetan Ralekar and Shubham Choudhary and Tapan Kumar Gandhi and Santanu Chaudhury},
  doi          = {10.1109/TAI.2023.3289167},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1362-1372},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Development of character recognition model inspired by visual explanations},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning multitask commonness and uniqueness for multimodal
sarcasm detection and sentiment analysis in conversation. <em>TAI</em>,
<em>5</em>(3), 1349–1361. (<a
href="https://doi.org/10.1109/TAI.2023.3298328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sarcasm is a form of figurative language device to express human inner feelings, where the author writes the positive sentence on surface form, while he/she actually expresses negative sentiment, vice versa. Sentiment, thus, comes into sight, and is closely related with sarcasm, leading to the recent popularity of multimodal sarcasm and sentiment joint detection in conversation (dialogue). The key challenges involve multimodal fusion and multitask interaction. Most of the existing studies have focused on building multimodal fused representation, while the commonness and uniqueness across related tasks has not received attention. To fill this gap, we propose a multimodal multitask interaction learning framework, termed MIL, for joint detection of sarcasm and sentiment. Specifically, a cross-modal target attention mechanism is proposed to automatically learn the alignment between texts and images/speeches. In addition, a multimodal interaction learning paradigm consisting of a dual-gating network, three separate fully connected layers that simultaneously capture the commonness and uniqueness. Comprehensive experiments on two benchmarking datasets (i.e., Memotion and MUStARD) show the effectiveness of the proposed model over state-of-the-art baselines with a significant improvement of 1.9%, 2.4% in terms of F1.},
  archive      = {J_TAI},
  author       = {Yazhou Zhang and Yang Yu and Dongming Zhao and Zuhe Li and Bo Wang and Yuexian Hou and Prayag Tiwari and Jing Qin},
  doi          = {10.1109/TAI.2023.3298328},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1349-1361},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Learning multitask commonness and uniqueness for multimodal sarcasm detection and sentiment analysis in conversation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Emotion flip reasoning in multiparty conversations.
<em>TAI</em>, <em>5</em>(3), 1339–1348. (<a
href="https://doi.org/10.1109/TAI.2023.3289937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a conversational dialogue, speakers may have different emotional states and their dynamics play an important role in understanding dialogue&#39;s emotional discourse. However, simply detecting emotions is not sufficient to entirely comprehend the speaker-specific changes in emotion that occur during a conversation. To understand the emotional dynamics of speakers in an efficient manner, it is imperative to identify the rationale or instigator behind any changes or flips in emotion expressed by the speaker. In this article, we explore the task called instigator-based emotion flip reasoning (EFR), which aims to identify the instigator behind a speaker&#39;s emotion flip within a conversation. For example, an emotion flip from joy to anger could be caused by an instigator like threat . To facilitate this task, we present MELD-I, a dataset that includes ground-truth EFR instigator labels, which are in line with emotional psychology. To evaluate the dataset, we propose a novel neural architecture called TGIF , which leverages Transformer encoders and stacked GRUs to capture the dialogue context, speaker dynamics, and emotion sequence in a conversation. Our evaluation demonstrates the state-of-the-art performance (+4%–12% increase in F1-score) against five baselines used for the task. Further, we establish the generalizability of TGIF on an unseen dataset in a zero-shot setting. In addition, we provide a detailed analysis of the competing models, highlighting the advantages and limitations of our neural architecture.},
  archive      = {J_TAI},
  author       = {Shivani Kumar and Shubham Dudeja and Md Shad Akhtar and Tanmoy Chakraborty},
  doi          = {10.1109/TAI.2023.3289937},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1339-1348},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Emotion flip reasoning in multiparty conversations},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Impulsive consensus of one-sided lipschitz multi-agent
systems with deception attacks and stochastic perturbation.
<em>TAI</em>, <em>5</em>(3), 1328–1338. (<a
href="https://doi.org/10.1109/TAI.2023.3289163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the consensus of one-sided Lipschitz multi-agent systems (MASs) subject to deception attacks and stochastic perturbation by using impulsive protocols. First, the one-sided Lipschitz condition is considered because it has a wider application range and less conservatism than general Lipschitz condition. Second, the impact of stochastic perturbation is considered, which is more consistent with the practical situation. Third, deception attacks are considered in controller-to-actuator channels and modeled as the Bernoulli distribution. In addition, two kinds of deception attacks are considered, namely injection attacks and replacement attacks. Then, based on Lyapunov stability theory, some results are obtained to accomplish consensus of MASs with both deception attacks and stochastic perturbations where the mean-squared consensus errors are bounded. What is more, the relationship between impulsive interval, convergence rate, and error bound are presented in this article. Based on this relationship, this article gives the rules for selecting these parameters, which can make a better control effectiveness. Finally, the validity of obtained results are proved via simulation examples.},
  archive      = {J_TAI},
  author       = {Zhengle Zhang and Tiedong Ma and Xiaojie Su and Xiaoyu Ma},
  doi          = {10.1109/TAI.2023.3289163},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1328-1338},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Impulsive consensus of one-sided lipschitz multi-agent systems with deception attacks and stochastic perturbation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An explainable coarse-to-fine survival analysis method on
multi-center whole slide images. <em>TAI</em>, <em>5</em>(3), 1316–1327.
(<a href="https://doi.org/10.1109/TAI.2023.3285855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survival models based on whole slide images (WSIs) are widely used in precision medicine to treat cancer patients better. Most previous studies attempted to address the challenge of WSIs&#39; gigapixel resolutions to survival models, but they failed in terms of computational efficiency and interpretability of models. This study proposes a coarse-to-fine survival model called WSISur based on graph neural networks, which not only solves the above two problems but also achieves the best survival prediction performance. To solve the issue of computational efficiency, coarse WSI graphs are first constructed on low-resolution images in WSIs, and then fine WSI graphs are built with high-resolution images on the basis of coarse WSI graphs. Subsequent survival analysis is performed on the constructed WSI graphs. To solve the issue of interpretability of the model, WSIs&#39; regions most relevant to patients&#39; lifetimes are identified by gradient-weighted class activation mapping. Nevertheless, due to the imbalance of data labels, there is a problem of probability failure in end-to-end mini-batch training. To this end, a survival event sampling strategy is proposed to balance data labels. In addition, a series of experiments are carried out on multicenter datasets to evaluate the proposed model. Experimental results show that WSISur achieves the state-of-the-art result compared with other existing related methods, and has stronger domain invariance. Results also demonstrate the interpretability of WSISur.},
  archive      = {J_TAI},
  author       = {Han Wang and Dan Jiang and Haixian Zhang and Yujue Wang and Li Yang and David J. Kerr and Yi Zhang},
  doi          = {10.1109/TAI.2023.3285855},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1316-1327},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An explainable coarse-to-fine survival analysis method on multi-center whole slide images},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model access control based on hidden adversarial examples
for automatic speech recognition. <em>TAI</em>, <em>5</em>(3),
1302–1315. (<a href="https://doi.org/10.1109/TAI.2023.3285858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have achieved remarkable success across various domains, and their commercial value has led to their classification as intellectual property (IP) for their creators. While model watermarking is commonly employed for DNN IP protection, it is limited to post hoc forensics. In contrast, model access control offers a more effective proactive approach to prevent IP infringement through authentication. However, existing model access control methods primarily focus on image classification models and are not suitable for automatic speech recognition (ASR) models, which are also widely used in commercial applications. To address the above limitation, inspired by audio adversarial examples, we propose the first model access control scheme for the IP protection of ASR models, which utilizes audio adversarial examples with target labels as user identity information, serving as identity-proof samples. However, a unique challenge arises in the form of interception attacks, in which an attacker detects and hijacks an authorized sample to bypass the authentication process. To remedy it, we introduce the hidden adversarial examples (HAEs) for authentication, which embed the authorized information by slightly modifying the logits and behaving like clean audios, thereby making them difficult to be detected by analyzing the predicted results. To further evade detection by steganalysis, which can be employed for adversarial example detection, we design a distortion cost function inspired by adaptive steganography to guide the generation of HAEs. We conduct extensive experiments on the open-source ASR system DeepSpeech, demonstrating that our proposed scheme effectively protects ASR models proactively and is resistant to unique interception attacks.},
  archive      = {J_TAI},
  author       = {Haozhe Chen and Jie Zhang and Kejiang Chen and Weiming Zhang and Nenghai Yu},
  doi          = {10.1109/TAI.2023.3285858},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1302-1315},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Model access control based on hidden adversarial examples for automatic speech recognition},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust graph autoencoder-based detection of false data
injection attacks against data poisoning in smart grids. <em>TAI</em>,
<em>5</em>(3), 1287–1301. (<a
href="https://doi.org/10.1109/TAI.2023.3286831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning-based detection of false data injection attacks (FDIAs) in smart grids relies on labeled measurement data for training and testing. The majority of existing detectors are developed assuming that the adopted datasets for training have correct labeling information. However, such an assumption is not always valid as training data might include measurement samples that are incorrectly labeled as benign, namely, adversarial data poisoning samples, which have not been detected before. Neglecting such an aspect makes detectors susceptible to data poisoning. Our investigations revealed that detection rates (DRs) of existing detectors significantly deteriorate by up to $\text{9}\text{--}\text{29}{\%}$ when subject to data poisoning in generalized and topology-specific settings. Thus, we propose a generalized graph neural network-based anomaly detector that is robust against FDIAs and data poisoning. It requires only benign datasets for training and employs an autoencoder with Chebyshev graph convolutional recurrent layers with attention mechanism to capture the spatial and temporal correlations within measurement data. The proposed convolutional recurrent graph autoencoder model is trained and tested on various topologies (from 14, 39, and 118-bus systems). Due to such factors, it yields stable generalized detection performance that is degraded by only $\text{1.6}\text{--}\text{3.7}{\%}$ in DR against high levels of data poisoning and unseen FDIAs in unobserved topologies.},
  archive      = {J_TAI},
  author       = {Abdulrahman Takiddin and Muhammad Ismail and Rachad Atat and Katherine R. Davis and Erchin Serpedin},
  doi          = {10.1109/TAI.2023.3286831},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1287-1301},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Robust graph autoencoder-based detection of false data injection attacks against data poisoning in smart grids},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated multitask learning for complaint identification
using graph attention network. <em>TAI</em>, <em>5</em>(3), 1277–1286.
(<a href="https://doi.org/10.1109/TAI.2023.3285196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior study on automatically identifying complaints on social media has relied on extensive feature engineering in centralized settings, with no consideration for the decentralized, nonidentically independently distributed (Non-IID), and privacy-conscious aspect of complaints, which can hinder data collection, distribution, and learning. In this work, we propose a graph attention network (GAT)-based multitask framework that intends to learn two closely related tasks, complaint detection (primary task) and sentiment classification (auxiliary task), simultaneously in federated learning scenarios. We propose the Federated Combination (FedComb) algorithm, a two-sided adaptive optimization technique that simultaneously optimizes global and local models. The proposed methodology outperforms several baselines for the intended task of recognizing complaints in decentralized settings, according to quantitative and qualitative studies on two benchmark datasets.},
  archive      = {J_TAI},
  author       = {Apoorva Singh and Siddarth Chandrasekar and Tanmay Sen and Sriparna Saha},
  doi          = {10.1109/TAI.2023.3285196},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1277-1286},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Federated multitask learning for complaint identification using graph attention network},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning visual representation for autonomous drone
navigation via a contrastive world model. <em>TAI</em>, <em>5</em>(3),
1263–1276. (<a href="https://doi.org/10.1109/TAI.2023.3283488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visuomotor policy learning for vision-based navigation tasks is still challenging and necessary for autonomous systems. Learning a task-specific policy from scratch simplifies the training pipeline while suffering from poor data efficiency and transfer ability. This problem intends to be more intractable under a low-data regime. In this work, we present a self-supervised representation learning architecture that incorporates Spatial and Temporal information via a Contrastive world model (STC) to extract image representation for vision-based navigation tasks. Specifically, STC leverages the dynamics transition model based on a recurrent neural network to construct a joint low-dimensional latent space for spatial and temporal representations. We simultaneously optimize all components of this architecture using a multiobjective contrastive training loss. The resulting pretrained encoder model acts as a standalone feature extractor to promote the policy learning procedure. We evaluate the final optimized visuomotor policy on both the simulated drone navigation environment and the out-of-domain dataset. Experimental results demonstrate that our proposed method outperforms task-specific and representative contrastive learning baselines in challenging complex visual environments with more than half the improvement in data efficiency and provides significant gains in learning speed as well as the final performance. Code and video are available at: https://github.com/yibow-wang/cwm4drone .},
  archive      = {J_TAI},
  author       = {Jiang Zhao and Yibo Wang and Zhihao Cai and Ningjun Liu and Kun Wu and Yingxun Wang},
  doi          = {10.1109/TAI.2023.3283488},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1263-1276},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Learning visual representation for autonomous drone navigation via a contrastive world model},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mutual adaptation: Learning from prototype for time-series
prediction. <em>TAI</em>, <em>5</em>(3), 1247–1262. (<a
href="https://doi.org/10.1109/TAI.2023.3282201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-series prediction is a current research hotspot in deep learning. However, due to the complex nature of time-series data, the modeling in this task is often highly nonconvex, which can make the final convergence unstable. To address this challenge, recent works have proposed deep mutual learning frameworks that allow models to learn from both ground truth and knowledge of other models in order to locate a better convergence point. However, a key disadvantage of deep mutual learning is that models that converge to poor local optima may still share their knowledge, limiting the overall performance. To overcome this limitation, in this article, we propose a new learning framework called mutual adaptation, which selects a prototype model that has the least error among all the models in the framework as the common teacher model. In addition, we incorporate a strategy of learning from each individual model&#39;s best local optimum in the history of training. Our experimental results show that, on average across multiple datasets, our method improves the performance of both Informer and long short-term memory (LSTM) models compared to deep mutual learning by 4.73% in mean absolute error (MAE) and 6.99% in mean squared error (MSE) for Informer, and 11.54% in MAE and 18.15% in MSE for LSTM. We also demonstrate the importance of memory of individual best local optima and provide sensitivity analysis and visualization of error and the loss descending process. Our method represents a new state-of-the-art in group learning for time-series prediction.},
  archive      = {J_TAI},
  author       = {Jinyu Chen and Xiaodan Shi and Haoran Zhang and Wenjing Li and Peiran Li and Yuhao Yao and Xuan Song and Ryosuke Shibasaki},
  doi          = {10.1109/TAI.2023.3282201},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1247-1262},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Mutual adaptation: Learning from prototype for time-series prediction},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mean-shift based differentiable architecture search.
<em>TAI</em>, <em>5</em>(3), 1235–1246. (<a
href="https://doi.org/10.1109/TAI.2023.3329792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differentiable architecture search (DARTS) is an effective continuous relaxation-based network architecture search (NAS) method with low search cost. It has attracted significant attention in AutoML research and has become one of the most effective paradigms in NAS. Although DARTS comes with great efficiency over traditional NAS approaches in handling the complex parameter search process, it often suffers from stabilization issues in producing deteriorating architectures when discretizing the found continuous architecture. To address this issue, we propose a mean-shift based DARTS (MS-DARTS) to improve the stability based on architecture sampling, perturbation, and shifting. The proposed mean-shift approach in MS-DARTS can effectively improve the stability and accuracy of DARTS by smoothing the loss landscape and sampling the architecture parameters within a suitable bandwidth. We investigate the convergence of our mean-shift approach as well as the effects of bandwidth selection toward stability and accuracy optimization. Evaluations on CIFAR-10, CIFAR-100, and ImageNet show that MS-DARTS archives competitive performance among state-of-the-art NAS methods with reduced search cost.},
  archive      = {J_TAI},
  author       = {Jun-Wei Hsieh and Cheng-Han Chou and Ming-Ching Chang and Ping-Yang Chen and Santanu Santra and Chih-Sheng Huang},
  doi          = {10.1109/TAI.2023.3329792},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1235-1246},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Mean-shift based differentiable architecture search},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiview adaptive k-nearest neighbor classification.
<em>TAI</em>, <em>5</em>(3), 1221–1234. (<a
href="https://doi.org/10.1109/TAI.2023.3296092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The k-nearest neighbor (KNN) classifies unlabeled samples according to the parameter $k$ , which is a user-defined constant and usually depends on prior knowledge. The selection of $k$ is crucial, as the size of the sample neighborhood affects the classification accuracy. To tackle this issue, we introduce the adaptive KNN (AKNN), which constructs a decision tree to assign different numbers of $k$ -values to different samples. In AKNN, we use the sample label information to calculate the weight between samples. Furthermore, to extend AKNN to a multiview scenario, we propose a method namely multiview adaptive KNN (MVAKNN), which integrates information from every single view by using the Dempster–Shafer theory. We conduct experiments on three benchmark multiview image datasets and the results show that MVAKNN exhibits desirable classification accuracy, outperforming some single-view and multiview methods. Experiments with Gaussian noises show the robustness of the proposed method.},
  archive      = {J_TAI},
  author       = {Zizhu Fan and Yijing Huang and Chao Xi and Qiang Liu},
  doi          = {10.1109/TAI.2023.3296092},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1221-1234},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multiview adaptive K-nearest neighbor classification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). O-minus decomposition for multiview tensor subspace
clustering. <em>TAI</em>, <em>5</em>(3), 1207–1220. (<a
href="https://doi.org/10.1109/TAI.2023.3293479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the powerful ability to exploit the latent structure of self-representation information, multiple off-the-shelf low rank tensor constraints have been employed in multiview tensor subspace clustering (MTSC) for achieving significant performance. However, current approaches mainly suffer from a series of problems, such as the deficient exploration of self-representation due to the unbalanced unfolding matrices, the inability to simultaneously capture both intraview and interview information, and so forth. All these will lead to MTSC with insufficient access to global information, which is contrary to the target of multiview clustering. To alleviate these problems, we propose a new tensor decomposition called O-minus decomposition (OMD) for multiview clustering. Specifically, based on the tensor ring format, we present the O-minus structure, which consists of a circle with an efficient bridge linking two weakly correlated factors. In this way, the information from intraview and interview can be better obtained simultaneously. Moreover, the enhanced capacity to capture global low-rank information will be achieved. The alternating direction method of multipliers is used to solve the proposed optimization model for OMD-MVC. Numerical experiments on six benchmark datasets demonstrate the superiority of our proposed method in terms of F-score, precision, recall, normalized mutual information, adjusted rand index, and accuracy.},
  archive      = {J_TAI},
  author       = {Yingcong Lu and Yipeng Liu and Zhen Long and Zhangxin Chen and Ce Zhu},
  doi          = {10.1109/TAI.2023.3293479},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1207-1220},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {O-minus decomposition for multiview tensor subspace clustering},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel approach for fair principal component analysis based
on eigendecomposition. <em>TAI</em>, <em>5</em>(3), 1195–1206. (<a
href="https://doi.org/10.1109/TAI.2023.3298291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal component analysis (PCA), a ubiquitous dimensionality reduction technique in signal processing, searches for a projection matrix that minimizes the mean squared error between the reduced dataset and the original one. As the classical PCA is not tailored to address concerns related to fairness, its application to actual problems may lead to disparity in the reconstruction errors of different groups (e.g., men and women, whites and blacks, etc.), with potentially harmful consequences. For instance, in terms of quality of representation in the projected space, one may retain more information from a specific group (e.g., men) instead of another one (e.g., women), which may introduce bias towards sensitive groups. Although several fair versions of PCA have been proposed recently, there still remains a fundamental gap in the search for algorithms that are simple enough to be deployed in real systems. Moreover, the considered fairness measure does not minimize, necessarily, the reconstruction errors of different groups. To address this, we propose a novel PCA algorithm which tackles fairness issues by means of a simple strategy comprising a 1-D search which exploits the closed-form solution of PCA. As attested by numerical experiments, the proposal can significantly improve fairness, by reducing disparities in reconstruction errors, with a very small loss in the overall reconstruction error and without resorting to complex optimization schemes. Moreover, our findings are consistent in several real situations as well as in scenarios with both unbalanced and balanced datasets.},
  archive      = {J_TAI},
  author       = {Guilherme Dean Pelegrina and Leonardo Tomazeli Duarte},
  doi          = {10.1109/TAI.2023.3298291},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1195-1206},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A novel approach for fair principal component analysis based on eigendecomposition},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Contrastive domain adaptation for time-series via temporal
mixup. <em>TAI</em>, <em>5</em>(3), 1185–1194. (<a
href="https://doi.org/10.1109/TAI.2023.3293473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) has emerged as a powerful solution for the domain shift problem via transferring the knowledge from a labeled source domain to a shifted unlabeled target domain. Despite the prevalence of UDA for visual applications, it remains relatively less explored for time-series applications. In this work, we propose a novel lightweight contrastive domain adaptation framework called CoTMix for time-series data. Unlike existing approaches that either use statistical distances or adversarial techniques, we leverage contrastive learning solely to mitigate the distribution shift across the different domains. Specifically, we propose a novel temporal mixup strategy to generate two intermediate augmented views for the source and target domains. Subsequently, we leverage contrastive learning to maximize the similarity between each domain and its corresponding augmented view. The generated views consider the temporal dynamics of time-series data during the adaptation process while inheriting the semantics among the two domains. Hence, we gradually push both domains toward a common intermediate space, mitigating the distribution shift across them. Extensive experiments conducted on five real-world time-series datasets show that our approach can significantly outperform all state-of-the-art UDA methods.},
  archive      = {J_TAI},
  author       = {Emadeldeen Eldele and Mohamed Ragab and Zhenghua Chen and Min Wu and Chee-Keong Kwoh and Xiaoli Li},
  doi          = {10.1109/TAI.2023.3293473},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1185-1194},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Contrastive domain adaptation for time-series via temporal mixup},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Deep curious feature selection: A recurrent,
intrinsic-reward reinforcement learning approach to feature selection.
<em>TAI</em>, <em>5</em>(3), 1174–1184. (<a
href="https://doi.org/10.1109/TAI.2023.3282564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection (FS) is an important step in the process of building machine learning-based models. The goal of the FS step is to find a small subset of features that will provide good prediction results by removing noisy, irrelevant, or repetitive features. Commonly used wrapper methods use a machine learning model as a black box and its performance as the goal function for evaluating different features&#39; subsets and selecting the best one. To avoid examining all possible subsets (an NP-hard problem), search algorithms are used to find the subsets to be examined, in a heuristic way. As exhaustive search methods are computationally complicated, most methods use simple and greedy search methods that yield only locally optimal results and are not sensitive to possible features interactions, which means that a feature may be chosen at the expense of two others that are more informative together. We analyze the problem of searching the features subset space with reference to two dimensions, namely, memory of past selected features subset and future selected features. We propose a new wrapper FS method based on the deep artificial curiosity framework, which implements intrinsic reward reinforcement learning, with long short-term memory unit. This novel algorithm integrates these two elements of memory and future step. We show that our method, called the deep curious FS algorithm, deals with feature interactions, and provides a feature subset which improves the accuracy of learning models on artificial and real datasets.},
  archive      = {J_TAI},
  author       = {Michal Moran and Goren Gordon},
  doi          = {10.1109/TAI.2023.3282564},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1174-1184},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep curious feature selection: A recurrent, intrinsic-reward reinforcement learning approach to feature selection},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian proper orthogonal decomposition for learnable
reduced-order models with uncertainty quantification. <em>TAI</em>,
<em>5</em>(3), 1162–1173. (<a
href="https://doi.org/10.1109/TAI.2023.3268609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing and/or controlling complex systems in science and engineering relies on appropriate mathematical modeling of systems dynamics. Classical differential equation-based solutions in applied/computational mathematics are often computationally demanding. Recently, the connection between reduced-order models of high-dimensional differential equation systems and surrogate machine learning models has been explored. However, the focus of both existing reduced-order and machine learning models has been how to best approximate the high-fidelity model of choice. Due to high complexity and often limited training data, it is critical for the models to have reliable uncertainty quantification. In this paper, we propose such a novel framework of Bayesian reduced-order models naturally equipped with uncertainty quantification as it learns the distributions of the parameters of the reduced-order models instead of their point estimates. Specifically, we develop learnable Bayesian proper orthogonal decomposition (BayPOD) that learns the distributions of both the POD projection bases and the mapping from the system input parameters to the projected scores/coefficients so that the learned BayPOD can help predict high-dimensional systems dynamics/fields with reliable uncertainty estimates. The developed BayPOD inherits the capability of embedding physics constraints when learning the POD-based surrogate models, a desirable feature when studying complex systems with limited available data. Furthermore, the proposed BayPOD is an end-to-end solution, which unlike other surrogate-based methods, does not require separate POD and machine learning steps. The results from a real-world case study of the pressure field around an airfoil shows the potential of learnable BayPOD as a new family of reduced-order models with reliable uncertainty estimates.},
  archive      = {J_TAI},
  author       = {Shahin Boluki and Siamak Zamani Dadaneh and Edward R. Dougherty and Xiaoning Qian},
  doi          = {10.1109/TAI.2023.3268609},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1162-1173},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Bayesian proper orthogonal decomposition for learnable reduced-order models with uncertainty quantification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Symmetry-informed reinforcement learning and its application
to low-level attitude control of quadrotors. <em>TAI</em>,
<em>5</em>(3), 1147–1161. (<a
href="https://doi.org/10.1109/TAI.2023.3249683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symmetry is ubiquitous in nature, physics, and mathematics. However, a classical symmetry-agnostic reinforcement learning (RL) approach cannot guarantee to respect symmetry. Researchers have shown that if the symmetry of a system cannot be respected, the performance of a symmetry-agnostic RL approach can be inhibited. To this end, this article develops a generally applicable neural network (NN) module with symmetry that can enforce the symmetry of a system to be respected. Based on the NN module with symmetry, this article proposes a symmetry-informed model-based RL (MBRL) approach that respects symmetry and improves data efficiency. The symmetry-informed MBRL approach is applied to the attitude control of a quadrotor in simulation to evaluate the effectiveness of the approach. The simulation results show that the data efficiency of the symmetry-informed MBRL approach is much superior to that of a symmetry-agnostic MBRL approach. An NN module with symmetry can respect the symmetry of a quadrotor while a naive NN cannot enforce the symmetry of a quadrotor to be respected.},
  archive      = {J_TAI},
  author       = {Junchang Huang and Weifeng Zeng and Hao Xiong and Bernd R. Noack and Gang Hu and Shugao Liu and Yuchen Xu and Huanhui Cao},
  doi          = {10.1109/TAI.2023.3249683},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1147-1161},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Symmetry-informed reinforcement learning and its application to low-level attitude control of quadrotors},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Auto-differentiable transfer mapping architecture for
physics-infused learning of acoustic field. <em>TAI</em>, <em>5</em>(3),
1132–1146. (<a href="https://doi.org/10.1109/TAI.2023.3248561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Opportunistic physics-mining transfer mapping architecture (OPTMA) is a hybrid architecture that combines fast simplified physics models with neural networks in order to provide significantly improved generalizability and explainability compared to pure data-driven machine learning (ML) models. However, training OPTMA remains computationally inefficient due to its dependence on gradient-free solvers or back-propagation with supervised learning over expensively pregenerated labels. This article presents two extensions of OPTMA that are not only more efficient to train through standard back-propagation, but are readily deployable through the state-of-the-art library, PyTorch. The first extension, OPTMA-Net, presents novel manual reprogramming of the simplified physics model, expressing it in Torch tensor compatible form, thus naturally enabling PyTorch&#39;s in-built autodifferentiation to be used for training. Since manual reprogramming can be tedious for some physics models, a second extension called OPTMA-Dual is presented, where a highly accurate internal neural net is trained a priori on the fast simplified physics model (which can be generously sampled), and integrated with the transfer model. Both new architectures are tested on analytical test problems and the problem of predicting the acoustic field of an unmanned aerial vehicle. The interference of the acoustic pressure waves produced by multiple monopoles form the basis of the simplified physics for this problem statement. An indoor noise monitoring setup in motion capture environment provided the ground truth for target data. Compared to sequential hybrid and pure ML models, OPTMA-Net/Dual demonstrate several fold improvement in performing extrapolation, while providing orders of magnitude faster training times compared to the original OPTMA.},
  archive      = {J_TAI},
  author       = {Rayhaan Iqbal and Amir Behjat and Revant Adlakha and Jesse Callanan and Mostafa Nouh and Souma Chowdhury},
  doi          = {10.1109/TAI.2023.3248561},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1132-1146},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Auto-differentiable transfer mapping architecture for physics-infused learning of acoustic field},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physics-constrained adversarial training for neural networks
in stochastic power grids. <em>TAI</em>, <em>5</em>(3), 1121–1131. (<a
href="https://doi.org/10.1109/TAI.2023.3236377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growth in distributed energy resources (DERs) is an important step toward solving the global climate crisis. However, many DERs, such as wind and solar power, are random and intermittent, causing the data in power grids to be perturbed with high uncertainty. Such perturbations degrade the performance of data-driven algorithms introduced in power grids for sensing and control. Existing approaches can strengthen machine learning models against well-designed malicious attacks. However, such physics-agnostic efforts fail to ensure the robustness of natural perturbations in power grids, that occur due to varying loads and changes in control inputs. This article proposes a novel physics-constrained adversarial training method for robustifying neural networks for the crucial problem of locating edge-faults in power grids. Our approach relies on first deriving analytical physical laws that are satisfied by state perturbations in realistic grids and then using the descriptive sets during adversarial training. Compared to state-of-the-art methods, our perturbation-robust neural networks have higher robust accuracy as well as training efficiency. Also, we demonstrate that the proposed approach achieves a tighter upper bound of robust risks than traditional efforts. The numerical experimental results in the IEEE 68-bus benchmark system validate our adversarial training in two scenarios when loads and control inputs vary randomly. The proposed method shows superior performance in accuracy and efficiency for different perturbed datasets. In addition, we testify the influence of different inputs on the robustness.},
  archive      = {J_TAI},
  author       = {Wenting Li and Deepjyoti Deka and Ren Wang and Mario R. Arrieta Paternina},
  doi          = {10.1109/TAI.2023.3236377},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1121-1131},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Physics-constrained adversarial training for neural networks in stochastic power grids},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A memory-efficient neural ordinary differential equation
framework based on high-level adjoint differentiation. <em>TAI</em>,
<em>5</em>(3), 1110–1120. (<a
href="https://doi.org/10.1109/TAI.2022.3230632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural ordinary differential equations (neural ODEs) have emerged as a novel network architecture that bridges dynamical systems and deep learning. However, the gradient obtained with the continuous adjoint method in the vanilla neural ODE is not reverse-accurate. Other approaches suffer either from an excessive memory requirement due to deep computational graphs or from limited choices for the time integration scheme, hampering their application to large-scale complex dynamical systems. To achieve accurate gradients without compromising memory efficiency and flexibility, we present a new neural ODE framework, PNODE, based on high-level discrete adjoint algorithmic differentiation. By leveraging discrete adjoint time integrators and advanced checkpointing strategies tailored for these integrators, PNODE can provide a balance between memory and computational costs, while computing the gradients consistently and accurately. In this article, we provide an open-source implementation based on PyTorch and PETSc, one of the most commonly used portable, scalable scientific computing libraries. We demonstrate the performance through extensive numerical experiments on image classification and continuous normalizing flow problems. We show that PNODE achieves the highest memory efficiency when compared with other reverse-accurate methods. On the image classification problems, PNODE is up to two times faster than the vanilla neural ODE and up to 2.3 times faster than the best existing reverse-accurate method. We also show that PNODE enables the use of the implicit time integration methods that are needed for stiff dynamical systems.},
  archive      = {J_TAI},
  author       = {Hong Zhang and Wenjun Zhao},
  doi          = {10.1109/TAI.2022.3230632},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1110-1120},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A memory-efficient neural ordinary differential equation framework based on high-level adjoint differentiation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward deep generation of guided wave representations for
composite materials. <em>TAI</em>, <em>5</em>(3), 1102–1109. (<a
href="https://doi.org/10.1109/TAI.2022.3229653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Laminated composite materials are widely used in most fields of engineering. Wave propagation analysis plays an essential role in understanding the short-duration transient response of composite structures. The forward physics-based models are utilized to map from elastic properties space to wave propagation behavior in a laminated composite material. Due to the high-frequency, multimodal, and dispersive nature of the guided waves, the physics-based simulations are computationally demanding. It makes property prediction, generation, and material design problems more challenging. In this work, a forward physics-based simulator, such as the stiffness matrix method is utilized to collect group velocities of guided waves for a set of composite materials. A variational autoencoder (VAE)-based deep generative model is proposed for the generation of new and realistic polar group velocity representations. It is observed that the deep generator is able to reconstruct unseen representations with very low mean square reconstruction error. Global Monte Carlo and directional equally spaced samplers are used to sample the continuous, complete, and organized low-dimensional latent space of VAE. The sampled point is fed into the trained decoder to generate new polar representations. The network has shown exceptional generation capabilities. It is also seen that the latent space forms a conceptual space where different directions and regions show inherent patterns related to the generated representations and their corresponding material properties.},
  archive      = {J_TAI},
  author       = {Mahindra Rautela and J. Senthilnath and Armin Huber and S. Gopalakrishnan},
  doi          = {10.1109/TAI.2022.3229653},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1102-1109},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Toward deep generation of guided wave representations for composite materials},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep reinforcement learning with reward design for quantum
control. <em>TAI</em>, <em>5</em>(3), 1087–1101. (<a
href="https://doi.org/10.1109/TAI.2022.3225256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (DRL) has been recognized as a powerful tool in quantum physics, where DRL&#39;s reward design is nontrivial but crucial for quantum control tasks. To address the problem of over-reliance on human empirical knowledge to design DRL&#39;s rewards, we propose a DRL with a novel reward paradigm designed by the learning process information (DRL-LPI), where the learning process information (LPI) comprises the state information and the experiences. In DRL-LPI, the state information after being classified by a fidelity threshold, and the experiences are first stored simultaneously in the respective sequences, and this process is repeated until a similar-segment ends. Then, the stored state information is converted to the real value and used to design the reward value by applying a self-amplitude function. Next, the designed reward values are integrated with the stored experiences to compose transitions for DRL&#39;s training. Through comparisons to five representative reward schemes, the proposed DRL-LPI is evaluated on two typical quantum control tasks, i.e., the spin-(1/2) quantum state control and many-coupled qubits state control, and the experimental results show the superior learning efficiency and control performance of the proposed approach. More results show that DRL-LPI exhibits the ability to learn the control strategy with few control actions compared to stochastic gradient descent (SGD) and genetic algorithm (GA).},
  archive      = {J_TAI},
  author       = {Haixu Yu and Xudong Zhao},
  doi          = {10.1109/TAI.2022.3225256},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1087-1101},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep reinforcement learning with reward design for quantum control},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy efficient quantum-informed ant colony optimization
algorithms for industrial internet of things. <em>TAI</em>,
<em>5</em>(3), 1077–1086. (<a
href="https://doi.org/10.1109/TAI.2022.3220186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most prominent research areas in information technology is the Internet of Things (IoT) as its applications are widely used, such as structural monitoring, health care management systems, agriculture and battlefield management, and so on. Due to its self-organizing network and simple installation of the network, the researchers have been attracted to pursue research in the various fields of IoTs. However, a huge amount of work has been addressed on various problems confronted by IoT. The nodes densely deploy over critical environments and those are operated on tiny batteries. Moreover, the replacement of dead batteries in the nodes is almost impractical. Therefore, the problem of energy preservation and maximization of IoT networks has become the most prominent research area. However, numerous state-of-the-art algorithms have addressed this issue. Thus, it has become necessary to gather the information and send it to the base station in an optimized method to maximize the network. Therefore, in this article, we propose a novel quantum-informed ant colony optimization (ACO) routing algorithm with the efficient encoding scheme of cluster head selection and derivation of information heuristic factors. The algorithm has been tested by simulation for various network scenarios. The simulation results of the proposed algorithm show its efficacy over a few existing evolutionary algorithms using various performance metrics, such as residual energy of the network, network lifetime, and the number of live IoT nodes.},
  archive      = {J_TAI},
  author       = {Srikanth Jannu and Suresh Dara and Chaitanya Thuppari and Ankit Vidyarthi and Debjani Ghosh and Prayag Tiwari and Ghulam Muhammad},
  doi          = {10.1109/TAI.2022.3220186},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1077-1086},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Energy efficient quantum-informed ant colony optimization algorithms for industrial internet of things},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural network methods based on efficient optimization
algorithms for solving impulsive differential equations. <em>TAI</em>,
<em>5</em>(3), 1067–1076. (<a
href="https://doi.org/10.1109/TAI.2022.3217207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In view of the fact that impulsive differential equations have the discreteness due to the impulse phenomenon, this article proposes a single hidden layer neural network method-based extreme learning machine and a physics-informed neural network method combined with learning rate attenuation strategy to solve linear impulsive differential equations and nonlinear impulsive differential equations, respectively. For the linear impulsive differential equations, first, the interval is segmented according to the impulse points, and a single hidden layer neural network model is constructed, the weight parameters of the hidden layer are randomly set, the optimal output parameters, and solution of the first segment are obtained by the extreme learning machine algorithm, then we calculate the initial value of the second segment according to the jumping equation and the remaining segments are solved in turn in the same way. Although the single hidden layer neural network method proposed can solve linear equations with high accuracy, it is not suitable for solving nonlinear equations. Therefore, we propose the physics-informed neural network combined with a learning rate attenuation strategy to solve the nonlinear impulsive differential equations, then the Adam algorithm and L-BFGS algorithm are combined to find the optimal approximate solution of each segment. Numerical examples show that the single hidden layer neural network method with Legendre polynomials as the activation function and the physics-informed neural network method combined with learning rate attenuation strategy can solve linear and nonlinear impulsive differential equations with higher accuracy.},
  archive      = {J_TAI},
  author       = {Baixue Xing and Hongliang Liu and Xiao Tang and Long Shi},
  doi          = {10.1109/TAI.2022.3217207},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1067-1076},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Neural network methods based on efficient optimization algorithms for solving impulsive differential equations},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tensor-based forward–backward algorithms in the
physics-informed coupled hidden markov model. <em>TAI</em>,
<em>5</em>(3), 1052–1066. (<a
href="https://doi.org/10.1109/TAI.2022.3227222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imposing data-driven with physical laws for user activity prediction could effectively solve various physical problems such as smart care, surveillance, and human–robot. In the growing field of artificial intelligence, the application of activity prediction based on the physical coupled hidden Markov model (CHMM) and tensor theory with physical properties has attracted increasing attentions. However, existing CHMMs usually only consider the time-series characteristic of data, while ignoring physical characteristics of user activity such as periodicity, timing, and correlation. Moreover, they are all matrix-based models, which could not holistically analyze the dependencies among physical states. The aforementioned disadvantages lead to lower prediction accuracy of the CHMM. To remove these disadvantages, three physics-informed tensor-based CHMMs are first constructed by incorporating prior physical knowledge. Then, the corresponding forward–backward algorithms are designed for resolving the evaluation problem of the CHMM. These algorithms could overall model multiple physical features by imposing physics and prior knowledge into the CHMM during training to improve the precision of probabilistic computing. The algorithms reduce the dependence of the model on training data by adding physical features. Finally, the comparative experiments show that our algorithms have better performances than existing prediction methods in precision and efficiency. In addition, further self-comparison experiments verify that our algorithms are effective and practical.},
  archive      = {J_TAI},
  author       = {Shunli Zhang and Laurence T. Yang and Yue Zhang and Zhixing Lu and Zongmin Cui},
  doi          = {10.1109/TAI.2022.3227222},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1052-1066},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Tensor-based Forward–Backward algorithms in the physics-informed coupled hidden markov model},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ultrasound COVID-19 classification based on the novel
module-based dual-path network. <em>TAI</em>, <em>5</em>(3), 1040–1051.
(<a href="https://doi.org/10.1109/TAI.2022.3208217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With a large-scale novel coronavirus pneumonia (COVID-19) outbreak, more and more researchers have acquired convenient and efficient COVID-19 infection status through medical imaging. Here, due to the excellent features of zero-radiation and rapid clinical examination, ultrasound images have been used to assist doctors in COVID-19 diagnosis. To effectively identify the pathological differences between common pneumonia and novel coronavirus pneumonia, an ultrasound COVID-19 classification based on the novel module-based dual-path network (MD-DPNet) is proposed. Specifically, this article effectively improves the generalization ability by adding the progressive heatmaps intuitively representing the lesion density with the original ultrasound images. Meanwhile, the proposed algorithm creates regular modular sets to reduce the calculating loads and the coupling between each module, which takes advantage of the fact that most pathological features are concentrated in relatively small regions. Furthermore, the proposed MD-DPNet model divides the complete task into multiple subtasks to alleviate interclass disequilibrium, and the lesion artifact localization is obtained by an unsupervised method. In this article, experiments show the effectiveness of the proposed innovation, and its sensitivity, specificity, positive predictive value, negative predictive value, and Dice reach 0.99, 0.99, 0.99, 0.98, and 0.99, respectively. Experiments on the proposed mixed dataset demonstrate satisfactory results on all the considered tasks, which can provide a new idea for effectively assisting doctors in COVID-19 diagnosis and follow-up treatment.},
  archive      = {J_TAI},
  author       = {Lingling Fang and Xin Wang},
  doi          = {10.1109/TAI.2022.3208217},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1040-1051},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Ultrasound COVID-19 classification based on the novel module-based dual-path network},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonintrusive operator inference for chaotic systems.
<em>TAI</em>, <em>5</em>(3), 1026–1039. (<a
href="https://doi.org/10.1109/TAI.2022.3207449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work explores the physics-driven machine learning (ML) technique operator inference (OpInf) for predicting the state of chaotic dynamical systems. OpInf provides a nonintrusive approach to infer approximations of polynomial operators in reduced space without having access to the full order operators appearing in discretized models. Datasets for the physics systems are generated using conventional numerical solvers, and then, projected to a low-dimensional space via principal component analysis (PCA). In latent space, a least-squares problem is set to fit a quadratic polynomial operator, which is subsequently employed in a time-integration scheme in order to produce extrapolations in the same space. Once solved, the inverse PCA operation is applied to reconstruct the extrapolations in the original space. The quality of the OpInf predictions is assessed via the normalized root mean squared errormetric from which the valid prediction time (VPT) is computed. Numerical experiments considering the chaotic systems Lorenz 96 and the Kuramoto–Sivashinsky equation show promising forecasting capabilities of the OpInf reduced order models with VPT ranges that outperform state-of-the-art ML methods such as backpropagation and reservoir computing recurrent neural networks (Vlachas et al., 2020), as well as Markov neural operators (Li et al., 2021).},
  archive      = {J_TAI},
  author       = {João Lucas de Sousa Almeida and Arthur Cancellieri Pires and Klaus Feine Vaz Cid and Alberto Costa Nogueira Junior},
  doi          = {10.1109/TAI.2022.3207449},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1026-1039},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Nonintrusive operator inference for chaotic systems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bellman neural networks for the class of optimal control
problems with integral quadratic cost. <em>TAI</em>, <em>5</em>(3),
1016–1025. (<a href="https://doi.org/10.1109/TAI.2022.3206735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces Bellman neural networks (BeNNs) and employs them to learn the optimal control actions for the class of optimal control problems (OCPs) with integral quadratic cost. BeNNs represent a particular family of physics-informed neural networks specifically designed and trained to tackle OCPs via applying the Bellman principle of optimality (BPO). The BPO provides necessary and sufficient optimality conditions, which result in a nonlinear partial differential equation known as the Hamilton–Jacobi–Bellman (HJB) equation. BeNNs learn the optimal control actions from the unknown solution of the arising HJB equation (i.e., the value function), where the unknown solution is modeled using a neural network. In addition, this article shows how to estimate the upper bounds on the generalization error of BeNNs while learning the solutions for the OCP class under consideration. The generalization error estimate is provided in terms of the choice and number of the training points as well as the training error. Numerical studies show that BeNNs can be successfully applied to learn the feedback control actions for the class of OCPs considered and, after the training is completed, deployed to control the system in a closed-loop fashion.},
  archive      = {J_TAI},
  author       = {Enrico Schiassi and Andrea D&#39;Ambrosio and Roberto Furfaro},
  doi          = {10.1109/TAI.2022.3206735},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1016-1025},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Bellman neural networks for the class of optimal control problems with integral quadratic cost},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physics-informed neural networks for modeling water flows in
a river channel. <em>TAI</em>, <em>5</em>(3), 1001–1015. (<a
href="https://doi.org/10.1109/TAI.2022.3200028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The impacts incurred by floods regularly affect the planet&#39;s population, inflicting social and economic problems. Optimal control strategies based on reservoir management may aid in controlling floods and mitigating the resulting damage. To this end, an accurate dynamic representation of water systems is needed. In practice, flood control strategies rely on hydrological forecasting models obtained from conceptual or data-driven methods. Encouraged by recent works, this research proposes a novel surrogate model for water flow in a river channel based on physics-informed neural networks (PINNs). This approach achieved promising results regarding the assimilation of real-data measurements and the parameter identification of differential equations that govern the underlying dynamics. This article investigates PINN performance in a simulated environment built directly from a configuration of the Saint-Venant equations. The objective is to create a suitable model with high prediction accuracy and scientifically consistent behavior for use in real-time applications. The experiments revealed promising results for hydrological modeling and presented alternatives to solve the main challenges found in conventional methods while assisting in synthesizing real-world representations.},
  archive      = {J_TAI},
  author       = {Luis Fernando Nazari and Eduardo Camponogara and Laio Oriel Seman},
  doi          = {10.1109/TAI.2022.3200028},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1001-1015},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Physics-informed neural networks for modeling water flows in a river channel},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning in sinusoidal spaces with physics-informed neural
networks. <em>TAI</em>, <em>5</em>(3), 985–1000. (<a
href="https://doi.org/10.1109/TAI.2022.3192362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A physics-informed neural network (PINN) uses physics-augmented loss functions, e.g., incorporating the residual term from governing partial differential equations (PDEs), to ensure its output is consistent with fundamental physics laws. However, it turns out to be difficult to train an accurate PINN model for many problems in practice. In this article, we present a novel perspective of the merits of learning in sinusoidal spaces with PINNs. By analyzing behavior at model initialization, we first show that a PINN of increasing expressiveness induces an initial bias around flat output functions . Notably, this initial solution can be very close to satisfying many physics PDEs, i.e., falling into a local minimum of the PINN loss that only minimizes PDE residuals, while still being far from the true solution that jointly minimizes PDE residuals and the initial and/or boundary conditions. It is difficult for gradient descent optimization to escape from such a local minimum trap, often causing the training to stall. We then prove that the sinusoidal mapping of inputs—in an architecture we label as sf -PINN—is effective to increase input gradient variability, thus avoiding being trapped in such deceptive local minimum. The level of variability can be effectively modulated to match high-frequency patterns in the problem at hand. A key facet of this article is the comprehensive empirical study that demonstrates the efficacy of learning in sinusoidal spaces with PINNs for a wide range of forward and inverse modeling problems spanning multiple physics domains.},
  archive      = {J_TAI},
  author       = {Jian Cheng Wong and Chin Chun Ooi and Abhishek Gupta and Yew-Soon Ong},
  doi          = {10.1109/TAI.2022.3192362},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {985-1000},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Learning in sinusoidal spaces with physics-informed neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantum-assisted activation for supervised learning in
healthcare-based intrusion detection systems. <em>TAI</em>,
<em>5</em>(3), 977–984. (<a
href="https://doi.org/10.1109/TAI.2022.3187676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrusion detection systems (IDSs) are amongst the most important automated defense mechanisms in modern industry. It is guarding against many attack vectors, especially in healthcare, where sensitive information (patient’s medical history, prescriptions, electronic health records, medical bills/debts, and many other sensitive data points) is open to compromise from adversaries. In the big data era, classical machine learning has been applied to train IDS. However, classical IDS tend to be complex: either using several hidden layers susceptible to overfitting on training data or using overly complex architectures such as convolutional neural networks, long-short term memory systems, and recurrent neural networks. This article explored the combination of principles of quantum mechanics and neural networks to train IDS. A hybrid classical-quantum neural architecture is proposed with a quantum-assisted activation function that successfully captures patterns in the dataset while having less architectural memory footprint than classical solutions. The experimental results are demonstrated on the popular KDD99 dataset while comparing our solution to other classical models.},
  archive      = {J_TAI},
  author       = {Nikhil Laxminarayana and Nimish Mishra and Prayag Tiwari and Sahil Garg and Bikash K. Behera and Ahmed Farouk},
  doi          = {10.1109/TAI.2022.3187676},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {977-984},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Quantum-assisted activation for supervised learning in healthcare-based intrusion detection systems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Thermodynamics-informed graph neural networks. <em>TAI</em>,
<em>5</em>(3), 967–976. (<a
href="https://doi.org/10.1109/TAI.2022.3179681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a deep learning method to predict the temporal evolution of dissipative dynamic systems. We propose using both geometric and thermodynamic inductive biases to improve accuracy and generalization of the resulting integration scheme. The first is achieved with graph neural networks, which induces a non-Euclidean geometrical prior with permutation-invariant node and edge update functions. The second bias is forced by learning the GENERIC structure of the problem, an extension of the Hamiltonian formalism, to model more general nonconservative dynamics. Several examples are provided in both Eulerian and Lagrangian description in the context of fluid and solid mechanics, respectively, achieving relative mean errors of less than 3% in all the tested examples. Two ablation studies are provided based on recent works in both Physics-informed and geometric deep learning.},
  archive      = {J_TAI},
  author       = {Quercus Hernández and Alberto Badías and Francisco Chinesta and Elías Cueto},
  doi          = {10.1109/TAI.2022.3179681},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {967-976},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Thermodynamics-informed graph neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guest editorial: Special issue on physics-informed machine
learning. <em>TAI</em>, <em>5</em>(3), 964–966. (<a
href="https://doi.org/10.1109/TAI.2023.3342563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The special issue delves into the tantalizing prospects of machine learning for multiscale modeling, a domain where the traditional methodologies often encounter scalability issues. Here, Physics-informed machine learning (PIML) promises to bridge scales from the microscopic to the macroscopic, creating models that are not only scalable but also more accurate and less resource-intensive. Furthermore, the contributors have taken on the challenge of machine learning model interpretability. They have explored how these models can provide insights into physical systems, thus serving a dual purpose of solving complex problems while also contributing to the body of knowledge in physics. The integration of physical laws with machine learning is not just an innovation; it is a renaissance of understanding. The papers in this issue showcase the pioneering works that merge the robustness of physics with the flexibility of machine learning. Here, we provide an overview of the significant contributions made by our authors in advancing the field of PIML.},
  archive      = {J_TAI},
  author       = {Francesco Piccialli and Maizar Raissi and Felipe A. C. Viana and Giancarlo Fortino and Huimin Lu and Amir Hussain},
  doi          = {10.1109/TAI.2023.3342563},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {964-966},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Guest editorial: Special issue on physics-informed machine learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual relationship detection for workplace safety
applications. <em>TAI</em>, <em>5</em>(2), 956–961. (<a
href="https://doi.org/10.1109/TAI.2023.3266183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applications of object and visual relationship detection for safety and security applications are in its infancy. The state-of-the-art computer vision research is largely focused on improving mean average precision and mean average recall performance on standard, general datasets, such as the verbs in common objects in context and the visual genome dataset and rarely mention the potential of such models in safety and security scenarios. We propose to train and develop an object and visual relationship detection neural network to be used as part of the backend model for a decision support system. We use a naive Bayesian network to determine scenarios where our proposed object and visual relationship detection network is error prone. We also release a graphical user interface, which demonstrates how our backend neural network and naive Bayesian network can be used for hazardous workplace safety and security applications.},
  archive      = {J_TAI},
  author       = {Thomas Truong and Svetlana Yanushkevich},
  doi          = {10.1109/TAI.2023.3266183},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {956-961},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Visual relationship detection for workplace safety applications},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generative augmentation-driven prediction of diverse visual
scanpaths in images. <em>TAI</em>, <em>5</em>(2), 940–955. (<a
href="https://doi.org/10.1109/TAI.2023.3278650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual scanpaths of multiple humans on an image represent the process by which they capture the information in it. State-of-the-art models to predict visual scanpaths on images learn directly from recorded human visual scanpaths. However, the generation of multiple visual scanpaths on an image having diversity like human visual scanpaths has not been explicitly considered. In this article, we propose a deep network for predicting multiple diverse visual scanpaths on an image. Image-specific hidden Markov model-based generative data augmentation is performed in the beginning to increase the number of image-visual scanpath training pairs. Considering a similarity between our generative data augmentation process and the use of long short-term memory (LSTM) for prediction, we propose an LSTM-based visual scanpath predictor. A network to predict a single visual scanpath on an image is designed first. The network is then modified to predict multiple diverse visual scanpaths representing different viewer varieties by using a parameter indicating the uniqueness of a viewer. A random vector is also employed for subtle variations within scanpaths of the same viewer variety. Our models are evaluated on three standard datasets using multiple performance measures, which demonstrate the superiority of the proposed approach over the state of the art. Empirical studies are also given indicating the significance of our generative data augmentation method and our multiple scanpath prediction strategy producing diverse visual scanpaths.},
  archive      = {J_TAI},
  author       = {Ashish Verma and Debashis Sen},
  doi          = {10.1109/TAI.2023.3278650},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {940-955},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Generative augmentation-driven prediction of diverse visual scanpaths in images},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attacking-distance-aware attack: Semi-targeted model
poisoning on federated learning. <em>TAI</em>, <em>5</em>(2), 925–939.
(<a href="https://doi.org/10.1109/TAI.2023.3280155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing model poisoning attacks on federated learning (FL) assume that an adversary has access to the full data distribution. In reality, an adversary usually has limited prior knowledge about clients&#39; data. A poorly chosen target class renders an attack less effective. This article considers a semitargeted situation where the source class is predetermined but the target class is not. The goal is to cause the misclassification of the global classifier on data from the source class. Approaches such as label flipping have been used to inject malicious parameters into FL. Nevertheless, it has been shown that their performances are usually class sensitive, varying with different target classes. Typically, an attack becomes less effective when shifting to a different target class. To overcome this challenge, we propose the attacking-distance-aware attack (ADA) that enhances model poisoning in FL by finding the optimized target class in the feature space. ADA deduces pairwise class attacking distances using a fast layer gradient method. Extensive evaluations were performed on five benchmark image classification tasks and three model architectures using varying attacking frequencies. Furthermore, ADA&#39;s robustness to conventional defenses of Byzantine-robust aggregation and differential privacy was validated. The results showed that ADA succeeded in increasing attack performance to 2.8 times in the most challenging case with an attacking frequency of 0.01 and bypassed existing defenses, where differential privacy that was the most effective defense still could not reduce the attack performance to below 50%.},
  archive      = {J_TAI},
  author       = {Yuwei Sun and Hideya Ochiai and Jun Sakuma},
  doi          = {10.1109/TAI.2023.3280155},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {925-939},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Attacking-distance-aware attack: Semi-targeted model poisoning on federated learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual-event-triggered intelligence security control for
multiagent systems against DoS attacks with applications in mobile robot
systems. <em>TAI</em>, <em>5</em>(2), 916–924. (<a
href="https://doi.org/10.1109/TAI.2023.3275938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intelligence security control problem of multiagent systems in the presence of actuator faults is investigated against denial-of-service (DoS) attacks. Noticing that the DoS attacks usually make the actuator signal not available for distributed control protocols, a novel intelligence dual-event-triggered control strategy is developed to eliminate the impact of DoS attacks. This control strategy includes two related event triggers, respectively, set on different channels, in which the second event-triggered condition is related to the triggering sequence of the previous event. Through the coordination between two nested event triggers set on different channels, the controller is avoided by being affected by wrong signals caused by DoS attacks. The developed distributed control protocol can not only guarantee the realization of the tracking control task, but also ensure that no “Zeno behavior” occurs in the process of the event triggering. Moreover, the obtained results are applied to the mobile robot systems. Finally, a numerical simulation example and a mobile robot system application are given to verify the feasibility and effectiveness of the proposed control strategy.},
  archive      = {J_TAI},
  author       = {Hongjing Liang and Zhenyu Chang and Yingnan Pan},
  doi          = {10.1109/TAI.2023.3275938},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {916-924},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Dual-event-triggered intelligence security control for multiagent systems against DoS attacks with applications in mobile robot systems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mitigating bias in bayesian optimized data while designing
MacPherson suspension architecture. <em>TAI</em>, <em>5</em>(2),
904–915. (<a href="https://doi.org/10.1109/TAI.2023.3274100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of artificial intelligence, the automotive industry searched for novel ways to improve future product design. We focus on designing automatic MacPherson suspension architecture for the automotive sector. It takes time for an automotive engineer to design vehicle parts and thus slows the pace of innovation in this field. Given the car&#39;s particular kinematic characteristics, we propose to predict an architecture by positioning the hardpoints. This work deals with the biased data generated using the discipline models using the dataset shift learning paradigm. The optimized data are created with random and uniform sampling, with more samples with random sampling. We resolve the bias in the data, using a novel criterion for tuning the kernel mean matching and a weight estimation algorithm and designing the required target characteristics.},
  archive      = {J_TAI},
  author       = {Sinnu Susan Thomas and Guillaume Lamine and Jacopo Palandri and Mohsen Lakehal-ayat and Punarjay Chakravarty and Friedrich Wolf-Monheim and Matthew B. Blaschko},
  doi          = {10.1109/TAI.2023.3274100},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {904-915},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Mitigating bias in bayesian optimized data while designing MacPherson suspension architecture},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weighted fuzzy system for identifying DNA n4-methylcytosine
sites with kernel entropy component analysis. <em>TAI</em>,
<em>5</em>(2), 895–903. (<a
href="https://doi.org/10.1109/TAI.2023.3266191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {N4-methylcytosine (4mC) is a common DNA methylation that has been implicated in epigenetic regulation and host defense. Accurate prediction of 4mC sites in DNA sequences will help to better explore the biological processes and mechanisms. For this problem, computational methods based on machine learning and deep learning are faster, less complex, and less expensive than experimental detection methods. However, the existing computational methods are still unsatisfactory in terms of prediction accuracy, so we propose a new method with better performance. In this work, we propose a weighted fuzzy system for identifying DNA 4mC sites by kernel entropy component analysis (KECA). We named it as W-TSK-FS-KECA. This model is improved based on the Takagi–Sugeuo–Kang fuzzy system (TSK-FS). We use position-specific trinucleotide propensity to construct feature vectors on representative benchmark datasets. Then we use KECA to get the reconstruct error. Finally, we put the calculated reconstruction error add to the regular term of TSK-FS as the weights to enhance the model performance. Comparative experiments with other methods show that it has good classification perfor- mance.},
  archive      = {J_TAI},
  author       = {Leyao Wang and Prayag Tiwari and Yijie Ding and Fei Guo},
  doi          = {10.1109/TAI.2023.3266191},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {895-903},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Weighted fuzzy system for identifying DNA n4-methylcytosine sites with kernel entropy component analysis},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiobjective multiverse optimizer for multirobotic
u-shaped disassembly line balancing problems. <em>TAI</em>,
<em>5</em>(2), 882–894. (<a
href="https://doi.org/10.1109/TAI.2023.3266187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of technology accelerates the upgrade of products, which results in a significant number of obsolete products. This research aims to solve the multirobotic multiproduct U-shaped disassembly line balancing problem (M2UDP), in which different products are disassembled on a U-shaped model in a preset cycle time and assigned tasks to robots in each workstation reasonably. A linear mixed-integer model is established to maximize disassembly profit and minimize carbon emissions. An improved multiobjective multiverse optimizer (IMMO) that utilizes the sigmoid activation function in neural networks is proposed to find the optimal plan for the model. The improved algorithm is verified via a set of real-life instances and compared with three classical multiobjective optimization algorithms. The experimental results show that the proposed IMMO performs better than those peers in solving the M2UDP problems.},
  archive      = {J_TAI},
  author       = {Shujin Qin and Shancheng Zhang and Jiacun Wang and Shixin Liu and Xiwang Guo and Liang Qi},
  doi          = {10.1109/TAI.2023.3266187},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {882-894},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multiobjective multiverse optimizer for multirobotic U-shaped disassembly line balancing problems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A similarity matrix low-rank approximation and inconsistency
separation fusion approach for multiview clustering. <em>TAI</em>,
<em>5</em>(2), 868–881. (<a
href="https://doi.org/10.1109/TAI.2023.3271964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, multiview clustering algorithms have achieved promising performance by exploiting the complementarity and consistency of different views. However, many multiview spectral clustering methods only focus on the consistent information of views, and the time cost of feature decomposition is expensive. Moreover, these methods also require postprocessing (e.g., k-means) to obtain the final clustering results. To overcome these limitations simultaneously, we propose a novel multiview clustering algorithm. First, the method removes the inconsistent information of the views through cross-view measurement to maintain consistent information. These inconsistencies may be caused by noise, corruptions, or view-specific properties and will affect the quality of the similarity matrix. Then, we learn a consensus embedding matrix with nonnegative constraints by performing a low-rank decomposition of the consistency information. In this way, we can replace the eigendecomposition of the n $\bm {\times }$ n Laplacian matrix in spectral clustering with the singular value decomposition of a n $\bm {\times }$ c low-rank matrix to reduce the computational burden, where c $\bm {\ll }$ n . Furthermore, due to the nonnegative constraint, we can directly obtain the clustering results. Also, to consider the diversity of views, adaptive weighting is applied to different view data. Compared to state-of-the-art multiview clustering methods on five benchmark multiview datasets, we demonstrate the superiority and effectiveness of our approach. We release the source code at https://github.com/hulu88/FAMvC .},
  archive      = {J_TAI},
  author       = {Ziqiang He and Shaohua Wan and Marco Zappatore and Hu Lu},
  doi          = {10.1109/TAI.2023.3271964},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {868-881},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A similarity matrix low-rank approximation and inconsistency separation fusion approach for multiview clustering},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supervised local training with backward links for deep
neural networks. <em>TAI</em>, <em>5</em>(2), 854–867. (<a
href="https://doi.org/10.1109/TAI.2023.3251313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The restricted training pattern in the standard backpropagation (BP) requires end-to-end error propagation, causing large memory costs and prohibiting model parallelization. Existing local training methods aim to resolve the training obstacles by completely cutting off the backward path between modules and isolating their gradients. These methods prevent information exchange between modules and result in inferior performance. This work proposes a novel local training algorithm, BackLink, which introduces intermodule backward dependence and facilitates information to flow backward along with the network. To preserve the computational advantage of local training, BackLink restricts the error propagation length within the module. Extensive experiments performed in various deep convolutional neural networks demonstrate that our method consistently improves the classification performance of local training algorithms over other methods. For example, our method can surpass the conventional greedy local training method by 6.45% in accuracy in ResNet32 classifying CIFAR100 and recent work by 2.58% in ResNet110 classifying STL-10 with much lower complexity. Analysis of computational costs reveals that small overheads are incurred in GPU memory costs and runtime on multiple GPUs. Our method can lead up to a 79% reduction in memory cost and 52% in simulation runtime in ResNet110 compared to the standard BP. Therefore, our method could create new opportunities for improving training algorithms toward better efficiency for real-time learning applications.},
  archive      = {J_TAI},
  author       = {Wenzhe Guo and Mohammed E. Fouda and Ahmed M. Eltawil and Khaled Nabil Salama},
  doi          = {10.1109/TAI.2023.3251313},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {854-867},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Supervised local training with backward links for deep neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Blockchain and federated reinforcement learning for
vehicle-to-everything energy trading in smart grids. <em>TAI</em>,
<em>5</em>(2), 839–853. (<a
href="https://doi.org/10.1109/TAI.2023.3262597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of electric vehicles (EVs) and the advancement of vehicle-to-everything energy trading systems are expected to play a crucial role in alleviating the stress on the electric grid during peak hours. However, the wide adoption of these paradigms requires intelligent mechanisms that protect the security and privacy of EV users. This article proposes a novel federated reinforcement learning system combined with blockchain technology to maximize EV users&#39; utility while preserving the security and privacy of trading transactions. Furthermore, we develop the concept of proof of state of charge as a consensus mechanism to determine the winning EVs and reward them as block miners in the blockchain. The proposed system is validated through comprehensive simulation experiments utilizing a real-world dataset. The model is implemented on the Avalanche blockchain platform to demonstrate its real-world feasibility. The test results show that the proposed scheme improves EV users&#39; utility significantly compared to the existing studies. The obtained simulation results indicate the effectiveness and robustness of the proposed system.},
  archive      = {J_TAI},
  author       = {Md Moniruzzaman and Abdulsalam Yassine and Rachid Benlamri},
  doi          = {10.1109/TAI.2023.3262597},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {839-853},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Blockchain and federated reinforcement learning for vehicle-to-everything energy trading in smart grids},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Encoding hierarchical information in neural networks helps
in subpopulation shift. <em>TAI</em>, <em>5</em>(2), 827–838. (<a
href="https://doi.org/10.1109/TAI.2023.3261861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decade, deep neural networks have proven to be adept in image classification tasks, often surpassing humans in terms of accuracy. However, standard neural networks often fail to understand the concept of hierarchical structures and dependencies among different classes for vision-related tasks. Humans on the other hand, seem to intuitively learn categories conceptually, progressively growing from understanding high-level concepts down to granular levels of categories. One of the issues arising from the inability of neural networks to encode such dependencies within its learned structure is that of subpopulation shift—where models are queried with novel unseen classes taken from a shifted population of the training set categories. Since the neural network treats each class as independent from all others, it struggles to categorize shifting populations that are dependent at higher levels of the hierarchy. In this work, we study the aforementioned problems through the lens of a novel conditional supervised training framework. We tackle subpopulation shift by a structured learning procedure that incorporates hierarchical information conditionally through labels. Furthermore, we introduce a notion of hierarchical distance to model the catastrophic effect of mispredictions. We show that learning in this structured hierarchical manner results in networks that are more robust against subpopulation shifts, with an improvement up to 3% in terms of accuracy and up to 11% in terms of hierarchical distance over standard models on subpopulation shift benchmarks.},
  archive      = {J_TAI},
  author       = {Amitangshu Mukherjee and Isha Garg and Kaushik Roy},
  doi          = {10.1109/TAI.2023.3261861},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {827-838},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Encoding hierarchical information in neural networks helps in subpopulation shift},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Capacity abuse attack of deep learning models without need
of label encodings. <em>TAI</em>, <em>5</em>(2), 814–826. (<a
href="https://doi.org/10.1109/TAI.2023.3266419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, machine learning (ML) models, especially deep learning models, have become commodities. In this context, data centers which hold a lot of data often buy ML models from ML model providers, train them on their data locally and use the trained models to provide intelligent services. Existing work has shown that there is a risk of data leakage, which could cause incalculable consequences. Even under the black-box condition, there are still some attacks that can steal the private data held by data centers, and the capacity abuse attack (CAA) is the state-of-the-art attack method. CAA attackers steal the training data by labeling malicious samples with the data to be stolen. However, the label encodings are usually mapped into other output forms, such as categories, and it is impossible for the adversary to know the mapping relationship between the form output by the trained model and the label encodings. Without the mapping relationship, CAA becomes invalid. Aiming at the limitation of CAA, this study proposes a novel practical attack method, i.e., capacity abuse attack II (CAAII), which can find the mapping relationship between the output in the arbitrary form returned by the trained model and the values of the stolen data. Experiments are conducted on MNIST, Fashion-MNIST, and CIFAR10 datasets, and experimental results show that no matter what forms are returned by the model, our attack method can always find the mapping relationship and successfully steals the training data.},
  archive      = {J_TAI},
  author       = {Wenjian Luo and Licai Zhang and Yulin Wu and Chuanyi Liu and Peiyi Han and Rongfei Zhuang},
  doi          = {10.1109/TAI.2023.3266419},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {814-826},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Capacity abuse attack of deep learning models without need of label encodings},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Performance-driven safe bayesian optimization for
intelligent tuning of high-order cascade systems. <em>TAI</em>,
<em>5</em>(2), 801–813. (<a
href="https://doi.org/10.1109/TAI.2023.3267030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic parameter tuning of high-order cascade controllers suffers from sampling inefficiency and strong couplings. This work presents a performance-driven, systematic, and safe intelligent parameter-tuning framework for high-order cascade systems. To achieve data-efficient and noise-robust hyperparameter calibration, an intelligent tuning framework based on Bayesian optimization is proposed to calibrate the control parameters from the buffer of performance-metric measurements. Furthermore, we improve the Bayesian-optimization-based framework in three aspects, involving effectiveness, security, and sampling efficiency. First, a comprehensive control performance assessment combining the error-integral and statistical performance criteria is designed to evaluate the cost of a sampling point in terms of final precision, response rapidity, and vibration. Meanwhile, the security of sampling exploration is heightened by imposing composite hard parametric and soft performance-metric (maximum input, overshoot, etc.) constraints on the acquisition points. Additionally, a hierarchical optimization strategy is proposed to further boost sampling exploitation by alternatively tuning the control parameters of subsystems and refining the constraint on the sampling space. The proposed framework is applied to automatic parameter tuning of high-order dynamic-surface-based backstepping control combined with antidisturbance rejection control. The comparative simulation results demonstrate that the proposed intelligent tuning possesses superiorities in sampling efficiency and security for parameter calibration problems.},
  archive      = {J_TAI},
  author       = {Tao Jiang and Xiaojie Su and Jiangshuai Huang and Zhenshan Bing and Alois Knoll},
  doi          = {10.1109/TAI.2023.3267030},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {801-813},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Performance-driven safe bayesian optimization for intelligent tuning of high-order cascade systems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimization of patient specific stimulus for deep brain
stimulation using spatially distributed neural sources. <em>TAI</em>,
<em>5</em>(2), 786–800. (<a
href="https://doi.org/10.1109/TAI.2023.3282199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep brain stimulation (DBS) becomes the therapy of choice in the later stages of Parkinson&#39;s disease (PD) due to the medication&#39;s side effects. For effective DBS treatment, it is important to have a controlled dosage of DBS. DBS dosage is administered using the tuning of electrical parameters of the stimulus signal. Since this tuning process is tedious, time-consuming, and patient-specific, there is a need to study the properties of DBS stimulation signal for proper dose administration. We propose a simulation framework to define an optimized DBS stimulus using electroencephalogram (EEG) signals. The objective is to provide a simulation environment inspired by a realistic brain. The framework uses spiking neurons in a reservoir modeled after real brain anatomy and is trained using a biologically inspired spike-time-dependent-plasticity learning algorithm. This reservoir is initially set to OFF-medication state and forced to drift to the ON-medication state by optimizing the synaptic changes. In later testing, the generalization of this framework is verified with EEG-inverse solutions, such as standardized low-resolution electromagnetic tomography, which utilize time-domain EEG signals to estimate neural activations. The stimulus signal is generated by accumulating the variations in synaptic weights in the neural reservoir in the target brain region. We analyze this signal and show that the application of this signal as stimulus results in decreased $\beta$ -band power in subthalamic-nucleus local field potential compared to OFF-medication local field potential without stimulation. Using SIM4LIFE simulation software, we show that the simulation increases chaos in the local field potential of subthalamic-nucleus neurons and shows that neuron weight variations follow specific trajectories in reconstructed state space.},
  archive      = {J_TAI},
  author       = {Syed Aamir Ali Shah and Abdul Bais and Lei Zhang},
  doi          = {10.1109/TAI.2023.3282199},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {786-800},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Optimization of patient specific stimulus for deep brain stimulation using spatially distributed neural sources},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Margin-aware adaptive-weighted-loss for deep learning based
imbalanced data classification. <em>TAI</em>, <em>5</em>(2), 776–785.
(<a href="https://doi.org/10.1109/TAI.2023.3275133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In supervised learning algorithms, the class imbalance problem often leads to generating results biased towards the majority classes. Present methods used to deal with the class imbalance problem ignore a principal aspect of separating the overlapping classes. This is the reason why most of these methods are prone to overfit on the training data. To this end, we propose a novel loss function, namely margin-aware adaptive-weighted loss. Here, we first use the large margin softmax to leverage intraclass compactness and interclass separability. Further to learn an unbiased representation of the classes, we put forward a dynamically weighted loss for imbalanced data classification. This weight dynamically adapts on every minibatch based on the inverse class frequencies. In addition, it takes care of the hard-to-train samples by using the confidence scores to learn discriminative hidden representations of the data. The overall framework is found to be effective when evaluated on the following two widely used datasets: 1) Canadian Institute for Advanced Research (CIFAR)-10 and 2) Fashion-MNIST. Additional experiments on human against machine and Asia Pacific tele-ophthalmology society 2019 blindness detection datasets prove the robustness of our methodology.},
  archive      = {J_TAI},
  author       = {Debasmit Roy and Rishav Pramanik and Ram Sarkar},
  doi          = {10.1109/TAI.2023.3275133},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {776-785},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Margin-aware adaptive-weighted-loss for deep learning based imbalanced data classification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Noisy label detection and counterfactual correction.
<em>TAI</em>, <em>5</em>(2), 763–775. (<a
href="https://doi.org/10.1109/TAI.2023.3271963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data quality is of paramount importance to the training of any machine learning model. Recently proposed approaches for noisy learning focus on detecting noisy labeled data instances by using a fixed loss value threshold and excluding detected noisy data instances in subsequent training steps. However, a predefined, fixed loss value threshold may not be optimal for detecting noisy labeled data, whereas excluding the detected noisy data instances can reduce the size of the training set to such an extent that accuracy can be negatively affected. In this article, we propose Noisy label Detection and Counterfactual Correction (NDCC), a new approach that automatically selects a loss value threshold to identify noisy labeled data instances, and uses counterfactual learning to correct the noisy labels. To the best of our knowledge, NDCC is the first work to explore the use of counterfactual learning in the noisy learning domain. We demonstrate the performance of NDCC on several datasets under a variety of label noise environments. Experimental results show the superiority of the proposed approach compared to the state of the art, especially in the presence of severe label noise.},
  archive      = {J_TAI},
  author       = {Wenting Qi and Charalampos Chelmis},
  doi          = {10.1109/TAI.2023.3271963},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {763-775},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Noisy label detection and counterfactual correction},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sample efficient reinforcement learning using graph-based
memory reconstruction. <em>TAI</em>, <em>5</em>(2), 751–762. (<a
href="https://doi.org/10.1109/TAI.2023.3268612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) algorithms typically require orders of magnitude more interactions than humans to learn effective policies. Research on memory in neuroscience suggests that humans&#39; learning efficiency benefits from associating their experiences and reconstructing potential events. Inspired by this finding, we introduce a human brainlike memory structure for agents and build a general learning framework based on this structure to improve the RL sampling efficiency. Since this framework is similar to the memory reconstruction process in psychology, we name the newly proposed RL framework as graph-based memory reconstruction (GBMR). In particular, GBMR first maintains an attribute graph on the agent&#39;s memory and then retrieves its critical nodes to build and update potential paths among these nodes. This novel pipeline drives the RL agent to learn faster with its memory-enhanced value functions and reduces interactions with the environment by reconstructing its valuable paths. Extensive experimental analyses and evaluations in the grid maze and some challenging Atari environments demonstrate GBMRs superiority over traditional RL methods. We will release the source code and trained models to facilitate further studies in this research direction.},
  archive      = {J_TAI},
  author       = {Yongxin Kang and Enmin Zhao and Yifan Zang and Lijuan Li and Kai Li and Pin Tao and Junliang Xing},
  doi          = {10.1109/TAI.2023.3268612},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {751-762},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Sample efficient reinforcement learning using graph-based memory reconstruction},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neighboring envelope embedded stacked autoencoder for deep
learning on hierarchically structured samples. <em>TAI</em>,
<em>5</em>(2), 737–750. (<a
href="https://doi.org/10.1109/TAI.2023.3266190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A stacked autoencoder (SAE) is a widely used deep network. However, existing deep SAEs focus on original samples without considering the hierarchical structural information between samples. This limits the accuracy of the SAE. In recent years, state-of-the-art SAEs have suggested improvements in network structure, cost function, and parameter optimization, thereby the accuracy has been enhanced. However, the problem mentioned above is still not solved. Therefore, this article is concerned with how to design an SAE that can conduct deep learning on hierarchically structured samples. This proposed SAE—neighboring envelope embedded stacked autoencoder (NE_ESAE)—mainly consists of two parts. The first is the neighboring sample envelope learning mechanism (NSELM) that constructs sample pairs by combining neighboring samples. In addition, the NSELM constructs multilayer sample spaces by multilayer iterative mean clustering, which considers similar samples and generates layers of envelope samples with hierarchical structural information. The second is an embedded stacked autoencoder (ESAE) to consider the original samples during training and in network structure, thereby finding the relationship of the samples with original features and deep features in a better manner. The experimental results show that our method has significantly better performance than some representative methods. Different from existing SAEs, the proposed NE_ESAE realizes deep learning on hierarchically structured samples and makes SAE able to conduct cooperative deep sampling and feature learning. The advantage that has been gained can be applied to other deep neural networks.},
  archive      = {J_TAI},
  author       = {Chuanyan Zhou and Jie Ma and Fan Li and Yongming Li and Pin Wang},
  doi          = {10.1109/TAI.2023.3266190},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {737-750},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Neighboring envelope embedded stacked autoencoder for deep learning on hierarchically structured samples},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A radial basis function-based graph attention network with
squeeze loss optimization for link prediction. <em>TAI</em>,
<em>5</em>(2), 724–736. (<a
href="https://doi.org/10.1109/TAI.2023.3265947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph attention networks are a popular method to deal with link prediction tasks, but the weight assigned to each sample is not focusing on the sample&#39;s own performance in training. Moreover, since the number of links is much larger than nodes in a graph, mapping functions are usually used to map the learned node features to link features, whose expression of node similarity determines the quality of link feature learning. To tackle the above issues, a new model graph attention networks based on the radial basis function (RBF) with squeeze loss are proposed, including two improvements. First, RBF function with extended parameters is used to transform the node features output by attention layer into link features. The result of link feature embedding can be improved by shortening the distance of nodes with links and enlarging the distance of nodes without links in vector space. Second, squeeze loss is designed to adjust the loss according to the performance of samples in training and change the proportion of sample loss in the loss function to allocate training resources reasonably. The link prediction task performed on datasets shows that the performance of the proposed method is better than baselines.},
  archive      = {J_TAI},
  author       = {Jiusheng Chen and Chengyuan Fang and Xiaoyu Zhang and Jun Wu and Runxia Guo},
  doi          = {10.1109/TAI.2023.3265947},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {724-736},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A radial basis function-based graph attention network with squeeze loss optimization for link prediction},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pseudo-shot learning for soil classification with
laser-induced breakdown spectroscopy. <em>TAI</em>, <em>5</em>(2),
709–723. (<a href="https://doi.org/10.1109/TAI.2023.3262503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Laser-induced breakdown spectroscopy (LIBS) has become an emerging analytical technique for soil analysis. The application of machine learning for quantitative and qualitative analysis has made LIBS more promising. However, the emission line distribution can be highly variable due to the soil samples&#39; varied physical properties and/or chemical composition. It may cause spectra distribution change and make the training spectra not accurately reflect the test spectra distribution. Hence, the test performance is deteriorated by applying an ML model trained on samples from the training distribution to the test distribution. To handle the spectra distribution problem, we propose using pseudoshot learning with Siamese networks, a domain adaptation (DA) method to incorporate pseudolabeled samples based on similarity confidence into the parameter estimation procedure. Considering the domain transfer differences among classes, we categorize the classes as hard, normal, and easy to reflect the class transfer difficulties in DA. We mainly focus on the hard classes as samples from these classes are not representative of the source domain and can easily be misclassified in the prediction phase. Few-shot learning is used to find the spectra from hard classes but misclassified into their similar classes. These spectra are included to cotrain the model with source samples to improve the test performance of hard classes. Our framework is tested with the EMSLIBS dataset, which shows that it can effectively overcome the spectra distribution shift and achieves 94.12% test accuracy. It beats the current best-performing model using the same dataset.},
  archive      = {J_TAI},
  author       = {Yingchao Huang and Abdul Bais and Amina E. Hussein},
  doi          = {10.1109/TAI.2023.3262503},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {709-723},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Pseudo-shot learning for soil classification with laser-induced breakdown spectroscopy},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hand gesture-operated system for rehabilitation using an
end-to-end detection framework. <em>TAI</em>, <em>5</em>(2), 698–708.
(<a href="https://doi.org/10.1109/TAI.2023.3251309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a hand gesture-operated system as an AI application to relieve discomfort and restore function in hand and arm movements caused by injuries and nerve and muscle complications. The system trains patients with hand exercises, such as performing hand gestures accurately, traversing within specified bounds, and operating a hand gesture calculator. However, the system requires accurate hand gesture detection, which is impeded by background clutter and variations in illumination and in the hand&#39;s size and angle. To address this, we developed a robust hand detection module that uses a single-stage transformer deep network. The transformer network encodes global information and uses bipartite matching to reduce the frequency of spurious detections. It drives a regression head and a classification head to localize the hand gesture in a bounding box and assign it a class label. Hand keypoints are also detected to support drawing, path traversal, and calculator use. The approach is evaluated on two benchmark datasets: 1) OUHANDS and 2) NUS. The method yields 89.6% accuracy for OUHANDS and 100% for NUS. These results indicate that precise hand detection can support a robust system for rehabilitation through hand exercises. Our experiments confirm that the users&#39; hand function progressively improved.},
  archive      = {J_TAI},
  author       = {H Pallab Jyoti Dutta and M. K. Bhuyan and Debanga Raj Neog and Karl F. MacDorman and R. H. Laskar},
  doi          = {10.1109/TAI.2023.3251309},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {698-708},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A hand gesture-operated system for rehabilitation using an end-to-end detection framework},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Query attack by multi-identity surrogates. <em>TAI</em>,
<em>5</em>(2), 684–697. (<a
href="https://doi.org/10.1109/TAI.2023.3257276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) are acknowledged as vulnerable to adversarial attacks while the existing black-box attacks require extensive queries on the victim DNN to achieve high success rates. For query-efficiency, surrogate models of the victim are used to generate transferable adversarial examples (AEs) because of their gradient similarity (GS), i.e., surrogates&#39; attack gradients are similar to the victim&#39;s ones. However, it is generally neglected to exploit their similarity on outputs, namely the prediction similarity (PS), to filter out inefficient queries by surrogates without querying the victim. To jointly utilize and also optimize surrogates&#39; GS and PS, we develop QueryNet, a unified attack framework that can significantly reduce queries. QueryNet creatively attacks by multi-identity surrogates, i.e., crafts several AEs for one sample by different surrogates and also uses surrogates to decide on the most promising AE for the query. After that, the victim&#39;s query feedback is accumulated to optimize not only surrogates&#39; parameters but also their architectures, enhancing both the GS and the PS. Although QueryNet has no access to pretrained surrogates&#39; prior, it reduces queries by averagely about an order of magnitude compared to alternatives within an acceptable time, according to our comprehensive experiments: 11 victims (including two commercial models) on MNIST/CIFAR10/ImageNet, allowing only 8-b image queries, and no access to the victim&#39;s training data. The code is available at https://github.com/Sizhe-Chen/QueryNet .},
  archive      = {J_TAI},
  author       = {Sizhe Chen and Zhehao Huang and Qinghua Tao and Xiaolin Huang},
  doi          = {10.1109/TAI.2023.3257276},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {684-697},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Query attack by multi-identity surrogates},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). <span class="math inline">log </span>-sigmoid
activation-based long short-term memory for time-series data
classification. <em>TAI</em>, <em>5</em>(2), 672–683. (<a
href="https://doi.org/10.1109/TAI.2023.3265641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the enhanced usage of artificial-intelligence-driven applications, the researchers often face challenges in improving the accuracy of data classification models, while trading off the complexity. In this article, we address the classification of time-series data using the long short-term memory (LSTM) network while focusing on the activation functions. While the existing activation functions, such as sigmoid and $\tanh$ , are used as LSTM internal activations, the customizability of these activations stays limited. This motivates us to propose a new family of activation functions, called $\log$ -sigmoid, inside the LSTM cell for time-series data classification and analyze its properties. We also present the use of a linear transformation (e.g., $\log \tanh$ ) of the proposed $\log$ -sigmoid activation as a replacement of the traditional $\tanh$ function in the LSTM cell. Both the cell activation and recurrent activation functions inside the LSTM cell are modified with $\log$ -sigmoid activation family while tuning the $\log$ bases. Furthermore, we report a comparative performance analysis of the LSTM model using the proposed and the state-of-the-art activation functions on multiple public time-series databases.},
  archive      = {J_TAI},
  author       = {Priyesh Ranjan and Pritam Khan and Sudhir Kumar and Sajal K. Das},
  doi          = {10.1109/TAI.2023.3265641},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {672-683},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {$\log$-sigmoid activation-based long short-term memory for time-series data classification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pyramid dynamic bayesian networks for key performance
indicator prediction in long time-delay industrial processes.
<em>TAI</em>, <em>5</em>(2), 661–671. (<a
href="https://doi.org/10.1109/TAI.2023.3258938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building dynamic Bayesian networks (DBNs) for time-delay industrial processes has always been tough, since the structure learning of the DBN is a NP-hard problem. In this article, a pyramid DBN (PDBN) framework is proposed to speed up modeling and improve feature learning for industrial processes with large time delays. In the PDBN framework, a sequence of small-sized DBNs are established, each of which is a progressively simpler representation of the previous layer, and the feature information learned by the DBN sequence corresponds to the layers in the pyramid. With information fusion of the feature pyramid and a further feature filtering with hill climbing based on Bayesian information criterion, we can restore the feature information of the time-delay industrial processes. Regression models are built based on the features learned by the PDBN framework for further key performance indicator estimation. Advantages of the method have been effectively validated on two actual industrial cases. With multilevel feature learning and filtering, both modeling speed and prediction accuracy have been greatly improved. The proposed method outperforms other similar works on identifying important features in industrial processes and existing DBN-based soft sensors.},
  archive      = {J_TAI},
  author       = {Lingquan Zeng and Zhiqiang Ge},
  doi          = {10.1109/TAI.2023.3258938},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {661-671},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Pyramid dynamic bayesian networks for key performance indicator prediction in long time-delay industrial processes},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synergetic focal loss for imbalanced classification in
federated XGBoost. <em>TAI</em>, <em>5</em>(2), 647–660. (<a
href="https://doi.org/10.1109/TAI.2023.3254519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applying sparsity- and overfitting-aware eXtreme Gradient Boosting (XGBoost) for classification in federated learning allows many participants to train a series of trees collaboratively. Since various local multiclass distributions and global aggregation diversity, model performance plummets as convergence slowly and accuracy decreases. Worse still, neither the participants nor the server can detect this problem and make timely adjustments. In this article, we provide a new local-global class imbalance inconsistency quantification and utilize softmax as the activation and focal loss, a dynamically scaled cross-entropy loss, in federated XGBoost to mitigate local class imbalance. Moreover, we propose a simple but effective hyperparameter determination strategy based on local data distribution to adjust the sample weights among noncommunicating participants, synergetic focal loss, to solve the inconsistency of local and global class imbalance, a unique characteristic of federated learning. This strategy is perfectly integrated into the original classification algorithm. It requires no additional detectors or information transmission. Furthermore, a dynamical for loop is designed to capture an optimum hyperparameter combination. Finally, we conduct comprehensive tabular- and image-based experiments to show that synergetic focal loss used in federated XGBoost achieves faster convergency and significant accuracy improvement. Simulation results prove the effectiveness of the proposed principle of configuring sample weights.},
  archive      = {J_TAI},
  author       = {Jiao Tian and Pei-Wei Tsai and Kai Zhang and Xinyi Cai and Hongwang Xiao and Ke Yu and Wenyu Zhao and Jinjun Chen},
  doi          = {10.1109/TAI.2023.3254519},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {647-660},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Synergetic focal loss for imbalanced classification in federated XGBoost},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep q-learning-based molecular graph generation for
chemical structure prediction from infrared spectra. <em>TAI</em>,
<em>5</em>(2), 634–646. (<a
href="https://doi.org/10.1109/TAI.2023.3287947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a novel approach to predicting chemical structures from their infrared (IR) spectra using deep Q-learning. IR spectra measurements are widely used in chemical analysis because they provide information on the types and characteristics of chemical bonds present within compounds. However, there are currently no algorithms to predict the entire chemical structure of a broad range of compounds relying solely on IR spectra, unless there is an exact or closely matched spectrum in an existing reference spectra library. To address this, we apply double deep Q-learning for automated prediction of the entire chemical structures of organic compounds based on IR spectra. Our method builds predicted structures by starting from a single carbon atom and subsequently adding an atom and bond step-by-step by ranking the rewards of each possible addition based on Q-values. We devised new structural similarity metrics, atom bond count and substructure count metrics to achieve our goal. Compared to the commonly used structural similarity score, the Jaccard index of extended-connectivity fingerprints, the devised metrics exhibit more suitable properties for Q-learning. The deep Q-model, which uses the combination of our two proposed metrics, gives the overall best performance and can generate structures similar to the actual structures in terms of their structural features and molecular weight in most tested cases.},
  archive      = {J_TAI},
  author       = {Joshua Dean Ellis and Razib Iqbal and Keiichi Yoshimatsu},
  doi          = {10.1109/TAI.2023.3287947},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {634-646},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep Q-learning-based molecular graph generation for chemical structure prediction from infrared spectra},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Masked siamese prompt tuning for few-shot natural language
understanding. <em>TAI</em>, <em>5</em>(2), 624–633. (<a
href="https://doi.org/10.1109/TAI.2023.3275132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, prompt-based learning has shown excellent performance on few-shot scenarios. Using frozen language models to tune trainable continuous prompt embeddings has become a popular and powerful methodology. For few-shot natural language understanding, even if we freeze the parameters of the pretrained language model, the learned pseudo-prompts might still be overfitted. In this article, we propose a novel masked Siamese prompt tuning (MSP-tuning) to improve few-shot natural language understanding. Concretely, MSP-tuning masks randomly out part of the prompt tokens to get a pair of masked Siamese prompts for each sample. Each training sample is then fed to the model twice with the masked Siamese prompts. Finally, MSP-tuning minimizes the Jensen–Shannon-divergence (JSD) between the two output probability distributions of the pretrained language model to regularize the model further. Experiment results on the few-shot GLUE benchmark and SuperGLUE benchmark show that MSP-tuning outperforms previous approaches. Numerically, our MSP-tuning achieves an average improvement of 1.79% (BERT-base) and 1.39% (BERT-large) of the GLUE benchmark and 1.90% (RoBERTa-base) and 1.71% (RoBERTa-large) of the SuperGLUE benchmark compared to state-of-the-art method P-tuning. Our method facilitates applying large pretrained language models in natural language understanding.},
  archive      = {J_TAI},
  author       = {Shiwen Ni and Hung-Yu Kao},
  doi          = {10.1109/TAI.2023.3275132},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {624-633},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Masked siamese prompt tuning for few-shot natural language understanding},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Video-based depth estimation autoencoder with weighted
temporal feature and spatial edge guided modules. <em>TAI</em>,
<em>5</em>(2), 613–623. (<a
href="https://doi.org/10.1109/TAI.2023.3324624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks with encoder and decoder structures, generally referred to as autoencoders, are used in many pixelwise transformation, detection, segmentation, and estimation applications, for example, which can be applied for face swapping, lane detection, semantic segmentation, and depth estimation, respectively. However, traditional autoencoders, which are based on single-frame inputs, ignore the temporal consistency between consecutive frames, and may, hence, produce unsatisfactory results. Accordingly, in this article, a video-based depth estimation (VDE) autoencoder is proposed to improve the quality of depth estimation through the inclusion of two weighted temporal feature (WTF) modules in the encoder and a single spatial edge guided (SEG) module in the decoder. The WTF modules designed with channel weighted block submodule effectively extract the temporal similarities in consecutive frames, whereas the SEG module provides spatial edge guidance of the object contours. Through the collaboration of the proposed modules, the accuracy of the depth estimation is greatly improved. The experimental results confirm that the proposed VDE autoencoder achieves a better monocular depth estimation performance than the existing autoencoders with only a slight increase in the computational cost.},
  archive      = {J_TAI},
  author       = {Wei-Jong Yang and Wan-Nung Tsung and Pau-Choo Chung},
  doi          = {10.1109/TAI.2023.3324624},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {613-623},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Video-based depth estimation autoencoder with weighted temporal feature and spatial edge guided modules},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pseudoinvertible neural networks. <em>TAI</em>,
<em>5</em>(2), 602–612. (<a
href="https://doi.org/10.1109/TAI.2023.3296573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces novel architecture components and training procedures to create augmented neural networks with the ability to process data bidirectionally via an end-to-end approximate inverse. We develop pseudoinvertible neural network (PsI-NN) layers which function as drop-in replacements for corresponding convolutional and fully connected layers; by using these, existing architectures gain a pseudoinverse function which, with training, approximately reverses the forward function. For cases where learning both a task and its inverse are necessary or desirable, we show that PsI-NN enabled models match or exceed the quality of results generated by systems that use two separate models while drastically reducing system parameter count. We demonstrate this on two tasks: unpaired image translation and semisupervised classification. In both, PsI-NN greatly reduces parameter count without any loss of output quality; in semisupervised image classification, PsI-NN improves classification performance beyond the baseline autoencoder method. Our approach creates pseudoinvertible versions of existing architectures, circumventing the stringent constraints required to create a true inverse while allowing a single neural network to learn a task in two directions simultaneously.},
  archive      = {J_TAI},
  author       = {Elijah D. Bolluyt and Cristina Comaniciu},
  doi          = {10.1109/TAI.2023.3296573},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {602-612},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Pseudoinvertible neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Constrained nonnegative matrix factorization based on label
propagation for data representation. <em>TAI</em>, <em>5</em>(2),
590–601. (<a href="https://doi.org/10.1109/TAI.2023.3267029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonnegative matrix factorization (NMF) algorithms are a series of dimensional reduction techniques widely used in data preprocessing. To improve the performance of clustering and the discrimination of the low-dimensional representation in NMF, we proposed a novel semisupervised constrained nonnegative matrix factorization based on label propagation (LpCNMF). Specifically, the proposed LpCNMF adopts graph and label propagation as regularization terms, then makes use of a small amount of labeled data to predict the label information of the unlabeled data and finally obtains a predictive membership matrix with more label information. At the same time, we introduce an efficient alternating iterative algorithm to solve the optimization problem of the objective function in the LpCNMF. Unlike other NMF algorithms that only update the basis and coefficient matrices, the LpCNMF algorithm increases the update of the predictive membership matrix obtained by label propagation. Experimental results on various benchmark datasets demonstrate the superiority of our algorithm over existing state-of-the-art NMF algorithms.},
  archive      = {J_TAI},
  author       = {Junmin Liu and Yicheng Wang and Jing Ma and Di Han and Yifan Huang},
  doi          = {10.1109/TAI.2023.3267029},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {590-601},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Constrained nonnegative matrix factorization based on label propagation for data representation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online aware synapse weighted autoencoder for recovering
random missing data in wastewater treatment process. <em>TAI</em>,
<em>5</em>(2), 578–589. (<a
href="https://doi.org/10.1109/TAI.2023.3266742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing values in wastewater treatment process (WWTP) data hinder the monitoring and prediction of operational status. Therefore, various online imputation methods have been proposed to recover missing values from streaming data collected from WWTP from real time. However, existing methods tend to ignore previous learned knowledge. In this article, an online aware synapse weighted autoencoder imputation method (OASI) is proposed to impute random missing values. First, an online stacked autoencoder (OSAE) framework is constructed to capture the nonlinear structure of the recently collected data. The framework decreases the computational and storage consumption of the model training. Second, an aware synapses weighted parameter regularization strategy is presented to guide the update of model parameters and alleviate the forgetting of historical information in an online continual setup. In this way, the learned features offer a more comprehensive representation of the overall information and help enhance imputation performance. Third, two real WWTP datasets with strong nonstationarity, high-noise level and high-dimensionality are used to validate the performance of the proposed OASI. Experimental results show that the proposed OASI achieves superior performances over the existing methods even in the presence of random missing values with different missing ratios, and only costs a short running time.},
  archive      = {J_TAI},
  author       = {Honggui Han and Meiting Sun and Fangyu Li},
  doi          = {10.1109/TAI.2023.3266742},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {578-589},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Online aware synapse weighted autoencoder for recovering random missing data in wastewater treatment process},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Datum-wise inference in structured environments.
<em>TAI</em>, <em>5</em>(2), 566–577. (<a
href="https://doi.org/10.1109/TAI.2023.3271616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various application domains (e.g., health psychology), experts use Bayesian networks to represent relationships among variables. However, these variables are not, in practice, directly observable but can be instead inferred via noisy but costly features. Herein, we study the problem of datum-wise feature selection and classification in the case where the label of each data instance is described by a known Bayesian network, and features are available at a cost. The goal is to accurately classify each data instance, while keeping the feature acquisition cost minimum. To this end, we first propose a forward pass algorithm that sequentially acquires features to infer the label of each variable in the Bayesian network. During this process, the proposed algorithm uses both the acquired features and the Bayesian network relationships. In an effort to improve classification accuracy, we also devise a backward pass algorithm, which exploits Bayesian network relationships along with evidence. We discuss the computational complexity of both the algorithms and experimentally assess their performance on 11 datasets. We observe that the forward pass algorithm achieves higher accuracy using a small fraction of features compared to state of the art, while the backward pass algorithm enhances accuracy without acquiring additional features.},
  archive      = {J_TAI},
  author       = {Sachini Piyoni Ekanayake and Daphney-Stavroula Zois},
  doi          = {10.1109/TAI.2023.3271616},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {566-577},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Datum-wise inference in structured environments},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dual-stage semi-supervised pre-training approach for
medical image segmentation. <em>TAI</em>, <em>5</em>(2), 556–565. (<a
href="https://doi.org/10.1109/TAI.2023.3272533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have played a vital role in developing automated methods for addressing medical image segmentation. However, their reliance on labeled data impedes the practicability. Semi-Supervised learning is gaining attention for its intrinsic ability to extract valuable information from labeled and unlabeled data with improved performance. Recently, consistency regularization methods have gained interest due to their efficient learning procedures. They are, however, confined to data or network-level perturbations, negating the benefit of having both forms in a single framework. In light of this, we ask an intriguing but unexplored question: Can we have both network-level and data-level perturbation in the semi-supervised framework? To this end, we present a holistic approach that integrates data-level perturbation in the model pre-training stage, followed by implicit network-level perturbation in the fine-tuning stage. Furthermore, we incorporate networks with manifold learning paradigms throughout the training to facilitate the formation of robust data representations by ensuring local and global semantic affinities adhering to the theory of consensus. Notably, this may be the first attempt in the semi-supervised medical image segmentation archetype to use data and network-level perturbation with a model pre-training strategy. We extensively validated the efficacy of the proposed framework on three benchmark datasets, namely the Automated Cardiac Diagnosis Challenge, ISIC-2018, and Left Atrial Segmentation Challenge datasets, subjected to severely low-sampled labeled data. Notably, in ACDC (4%), ISIC-2018 (5%), and LA (6%) labeled cases, the proposed method outperforms the second-best method by 2.95%, 1.31%, and 0.71% in the Dice Similarity Metric.},
  archive      = {J_TAI},
  author       = {Rajath C Aralikatti and S. J. Pawan and Jeny Rajan},
  doi          = {10.1109/TAI.2023.3272533},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {556-565},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A dual-stage semi-supervised pre-training approach for medical image segmentation},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical patch selection: An improved patch sampling for
no reference image quality assessment. <em>TAI</em>, <em>5</em>(2),
541–555. (<a href="https://doi.org/10.1109/TAI.2023.3262623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quality degradation due to the compression and the transmission of images is a significant threat to multimedia applications. Blind image quality assessment (BIQA) is a principal technique to measure the distortion and dynamically set the optimal parameters for developing image compression standards, image restoration algorithms, etc. Insufficient training data with quality scores are a challenge for image quality assessment (IQA) tasks. The existing solutions to deep-convolution-neural-network-based BIQA, which rely on patchwise training, struggle to find an ideal set of patches consistent with the human visual system. To address these issues, HIerarchical paTch Selection (HITS) is proposed. HITS keeps the patches with considerable details in each quadrant according to their intensity variance scatter ratio (IVSR). IVSR identifies the best nonhomogeneous patches by computing patch intensity variance. Extensive trials are conducted, and the performance reveals that the proposed approach achieves excellent performance on synthetic and authentic distortion datasets with less memory and processing power. Moreover, the proposed approach outperforms other BIQA methods that adapt patchwise training strategies.},
  archive      = {J_TAI},
  author       = {C. Nandhini and M. Brindha},
  doi          = {10.1109/TAI.2023.3262623},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {541-555},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Hierarchical patch selection: An improved patch sampling for no reference image quality assessment},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fast attention network for joint intent detection and slot
filling on edge devices. <em>TAI</em>, <em>5</em>(2), 530–540. (<a
href="https://doi.org/10.1109/TAI.2023.3309272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intent detection and slot filling are two main tasks in natural language understanding and play an essential role in task-oriented dialogue systems. The joint learning of both tasks can improve inference accuracy and is popular in recent works. However, most joint models ignore the inference latency and cannot meet the need to deploy dialogue systems at the edge. In this article, we propose a fast attention network (FAN) for joint intent detection and slot filling tasks, guaranteeing both accuracy and latency. Specifically, we introduce a clean and parameter-refined attention module to enhance the information exchange between intent and slot, improving semantic accuracy by more than 2%. The FAN can be implemented on different encoders and delivers more accurate models at every speed level. Our experiments on the Jetson Nano platform show that the FAN inferences 15 utterances per second with a small accuracy drop, showing its effectiveness and efficiency on edge devices.},
  archive      = {J_TAI},
  author       = {Liang Huang and Senjie Liang and Feiyang Ye and Nan Gao},
  doi          = {10.1109/TAI.2023.3309272},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {530-540},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A fast attention network for joint intent detection and slot filling on edge devices},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lab to multiscene generalization for non-line-of-sight
identification with small-scale datasets. <em>TAI</em>, <em>5</em>(2),
516–529. (<a href="https://doi.org/10.1109/TAI.2023.3262763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-wideband (UWB) wireless indoor positioning systems rely on time-of-flight to estimate distances but can be biased and miscalculated due to non-line-of-sight (NLOS) transmission channels in complex environments. Therefore, to remove errors, several machine learning (ML) techniques have been proposed for identifying NLOS signals from channel impulse responses (CIRs). However, as CIR signals could be heavily influenced by various environments, current NLOS classifiers are not universal to provide satisfactory accuracy for new scenarios and require detailed measurements on a large number of CIRs for training. Hence, we propose a generalization method based on the data augmentation via noise injection and transfer learning to allow the deep neural network (DNN) trained under a lab condition to be applied to various and even harsh practical scenarios with the need to measure massive training data minimized. This article presents the first demonstration that it is effective to utilize a lab-based pretrained DNN for real-world transfer and white Gaussian noise data augmentation for ML-based NLOS identification on UWB CIRs to address the problem when it is not feasible to measure sufficient training data. Our testing results show that, in two scenarios, corridor and parking lot, with only 50 CIR signals as the training set, the accuracy of the NLOS identification model after applying the proposed method is increased from 84.4% to 98.8% and from 81.1% to 97.1%, respectively.},
  archive      = {J_TAI},
  author       = {Qirui Hua and Martin Hedegaard Nielsen and Zeliang An and Jian Ren and Rafal Wisniewski and Søren Kold and Ole Rahbek and Ming Shen},
  doi          = {10.1109/TAI.2023.3262763},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {516-529},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Lab to multiscene generalization for non-line-of-sight identification with small-scale datasets},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Portfolio selection via graph-aware gaussian processes with
generalized gaussian likelihood. <em>TAI</em>, <em>5</em>(2), 505–515.
(<a href="https://doi.org/10.1109/TAI.2023.3262456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portfolio selection aims to manage the allocation of wealth among different assets, which remains to be a fundamental and challenging financial task. Markowitz&#39;s mean-variance analysis is one of the most well-known and widely adopted techniques for this problem. However, it requires accurate estimations of both the mean and variance of the return, while predicting the variance is particularly difficult. In this article, we propose a novel portfolio selection strategy based on a graph-aware Gaussian process model equipped with generalized Gaussian distribution (GGD) likelihood. Our method unleashes the potential of mean-variance analysis by exploiting the Gaussian process model&#39;s ability in capturing uncertainty, and the graph information is also incorporated so that correlation among different assets can be utilized. We notice that most existing financial models assume the returns follow the log-normal distribution, whereas we observe that it cannot perfectly explain real-world market data. Based on this discovery, we introduce the GGD as the likelihood function. We further improve our method by proposing a passive variant to address transaction fees, and designing the mean function of the Gaussian process with the mean reversion principle. Thorough experiments were performed on synthetic and real-world financial datasets to verify the effectiveness of our method.},
  archive      = {J_TAI},
  author       = {Naiqi Li and Zhikang Xia and Yiming Li and Ercan E. Kuruoğlu and Yong Jiang and Shu-Tao Xia},
  doi          = {10.1109/TAI.2023.3262456},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {505-515},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Portfolio selection via graph-aware gaussian processes with generalized gaussian likelihood},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A type-2 fuzzy-based explainable AI system for predictive
maintenance within the water pumping industry. <em>TAI</em>,
<em>5</em>(2), 490–504. (<a
href="https://doi.org/10.1109/TAI.2023.3279808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial maintenance has undergone a paradigm shift due to the emergence of artificial intelligence (AI), the Internet of things, and cloud computing. Rather than accepting the drawbacks of reactive maintenance, leading firms worldwide are embracing “predict-and-prevent” maintenance. However, opaque box AI models are sophisticated and complex for the average user to comprehend and explain. This limits the AI employment in predictive maintenance, where it is vital to understand and evaluate the model before deployment. In addition, it is also important to comprehend the maintenance system&#39;s decisions. This article presents a type-2 fuzzy-based explainable AI (XAI) system for predictive maintenance within the water pumping industry. The proposed system is optimized via big-bang big-crunch, which maximizes the model accuracy for predicting faults while maximizing model interpretability. We evaluated the proposed system on water pumps using real-time data obtained by our hardware placed at real-world locations around the United Kingdom and compared our model with Type-1 fuzzy logic system (T1FLS), a multilayer perceptron (MLP) neural network, an effective deep learning method known as stacked autoencoders (SAEs), and an interpretable model such as decision trees (DT). The proposed system predicted water pumping equipment failures with good accuracy (outperforming the T1FLS accuracy by 8.9% and DT by 529.2% while providing comparable results to SAEs and MLPs) and interpretability. The system predictions comprehend why a specific problem may occur, which leads to better and more informed customer visits to reduce equipment failure disturbances. It will be shown that 80.3% of water industry specialists strongly agree with the model&#39;s explanation, determining its acceptance.},
  archive      = {J_TAI},
  author       = {Shreyas J. Upasane and Hani Hagras and Mohammad Hossein Anisi and Stuart Savill and Ian Taylor and Kostas Manousakis},
  doi          = {10.1109/TAI.2023.3279808},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {490-504},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A type-2 fuzzy-based explainable AI system for predictive maintenance within the water pumping industry},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How artificial intelligence helped the humanity during the
COVID-19 pandemic: A review. <em>TAI</em>, <em>5</em>(2), 480–489. (<a
href="https://doi.org/10.1109/TAI.2023.3300991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unprecedented circumstances encountered during the COVID-19 pandemic in a variety of life aspects such as health, economy, and environment have urged the humanity to devise new solutions to control and mitigate the impact of the pandemic. Motivated by the high-accuracy decision-making capabilities of artificial intelligence, this article aims to highlight the effectiveness of artificial intelligence applications in the areas of virus detection, health monitoring, face mask detection, crowd sensing, and satellite-based environment monitoring. One of the most promising deep learning techniques presented in the literature is the convolutional neural network (CNN) that has shown remarkable classification accuracy. The reason for using artificial intelligence in this area is the inherent difficulty of these applications such as detecting COVID-19 infections using lung images due to its resemblance with other respiratory diseases. While the lack of sufficient training data is considered one of the main difficulties, it has been alleviated with the aid of specialized artificial intelligence techniques such as generative adversarial networks and transfer learning. Satellite-based imagery along with deep learning have shown an improvement in the air quality due to the imposed mobility restrictions during the pandemic. Apart from the technical challenges, some applications faced social and ethical challenges that are mainly related to the patient privacy. The latter factor have made dataset availability more limited, and restricted the implementation of some applications such as contact tracing. This work examines state-of-the-art studies and shows the effectiveness of artificial intelligence in solving the most challenging technical problems encountered during the pandemic.},
  archive      = {J_TAI},
  author       = {Ali Alnoman},
  doi          = {10.1109/TAI.2023.3300991},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {480-489},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {How artificial intelligence helped the humanity during the COVID-19 pandemic: A review},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning methods for small molecule drug discovery: A
survey. <em>TAI</em>, <em>5</em>(2), 459–479. (<a
href="https://doi.org/10.1109/TAI.2023.3251977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of computer-assisted techniques, research communities, including biochemistry and deep learning, have been devoted into the drug discovery field for over a decade. Various applications of deep learning have drawn great attention in drug discovery, such as molecule generation, molecular property prediction, retrosynthesis prediction, and reaction prediction. While most of the existing surveys only focus on one of the applications, limiting the view of researchers in the community, in this article, we present a comprehensive review on the aforementioned four aspects and discuss the relationships among different applications. The latest literature and classical benchmarks are presented for better understanding the development of a variety of approaches. We commence by summarizing the molecule representation format in these works, followed by an introduction of recent proposed approaches for each of the four tasks. Furthermore, we review a variety of commonly used datasets and evaluation metrics and compare the performance of deep-learning-based models. Finally, we conclude by identifying remaining challenges and discussing the future trend for deep learning methods in drug discovery.},
  archive      = {J_TAI},
  author       = {Wenhao Hu and Yingying Liu and Xuanyu Chen and Wenhao Chai and Hangyue Chen and Hongwei Wang and Gaoang Wang},
  doi          = {10.1109/TAI.2023.3251977},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {459-479},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep learning methods for small molecule drug discovery: A survey},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Audio representation learning by distilling video as
privileged information. <em>TAI</em>, <em>5</em>(1), 446–456. (<a
href="https://doi.org/10.1109/TAI.2023.3243596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep audio representation learning using multimodal audiovisual data often leads to a better performance compared to unimodal approaches. However, in real-world scenarios, both modalities are not always available at the time of inference, leading to performance degradation by models trained for multimodal inference. In this article, we propose a novel approach for deep audio representation learning using audiovisual data when the video modality is absent at inference. For this purpose, we adopt teacher–student knowledge distillation under the framework of learning using privileged information (LUPI). While the previous methods proposed for LUPI use soft labels generated by the teacher, in our proposed method, we use embeddings learned by the teacher to train the student network. We integrate our method in two different settings: sequential data where the features are divided into multiple segments throughout time, and nonsequential data where the entire features are treated as one whole segment. In the nonsequential setting, both the teacher and student networks are comprised of an encoder component and a task header. We use the embeddings produced by the encoder component of the teacher to train the encoder of the student, while the task header of the student is trained using ground-truth labels. In the sequential setting, the networks have an additional aggregation component that is placed between the encoder and the task header. We use two sets of embeddings produced by the encoder and the aggregation component of the teacher to train the student. Similar to the nonsequential setting, the task header of the student network is trained using ground-truth labels. We test our framework on two different audiovisual tasks, namely, speaker recognition and speech emotion recognition. Through these experiments, we show that by treating the video modality as privileged information for the main goal of audio representation learning, our method results in considerable improvements over sole audio-based recognition as well as prior works that use LUPI.},
  archive      = {J_TAI},
  author       = {Amirhossein Hajavi and Ali Etemad},
  doi          = {10.1109/TAI.2023.3243596},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {446-456},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Audio representation learning by distilling video as privileged information},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A long short-term memory-based interconnected architecture
for classification of grasp types using surface-electromyography
signals. <em>TAI</em>, <em>5</em>(1), 434–445. (<a
href="https://doi.org/10.1109/TAI.2023.3244177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable classification of grasp types from human limbs has become an important aspect used by applications with humanoid robotic systems, because of their high-accuracy implementations in human movement replication and detection. Biomedical features extracted from muscular signals are commonly used for this purpose, however, their extraction and usage have been targeted independently, with time series features not even considered in the classification stage. Recently, studies show deep neural networks could obtain the signal&#39;s features in their internal architecture and use them directly over a classification task, avoiding all preprocessing steps and improving the obtained accuracy. Therefore, the current study proposes a deep architecture based on long short-term memory networks for the classification of six grasp types as an end-to-end deep model approach, working with raw surface electromyography signals. Classification accuracy of 99.12% was obtained and compared with previous studies which use different machine learning techniques over the same dataset. Results obtained showed that our model&#39;s architecture improves previous results as well as provides a robust solution avoiding overfitting, with an F1-score higher than 99% for all grasp types.},
  archive      = {J_TAI},
  author       = {Andres Erazo and Seok-Bum Ko},
  doi          = {10.1109/TAI.2023.3244177},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {434-445},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A long short-term memory-based interconnected architecture for classification of grasp types using surface-electromyography signals},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extensible machine learning for encrypted network traffic
application labeling via uncertainty quantification. <em>TAI</em>,
<em>5</em>(1), 420–433. (<a
href="https://doi.org/10.1109/TAI.2023.3244168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing prevalence of encrypted network traffic, cybersecurity analysts have been turning to machine learning (ML) techniques to elucidate the traffic on their networks. However, ML models can become stale as new traffic emerges that is outside of the distribution of the training set. In order to reliably adapt in this dynamic environment, ML models must additionally provide contextualized uncertainty quantification to their predictions, which has received little attention in the cybersecurity domain. Uncertainty quantification is necessary both to signal when the model is uncertain about which class to choose in its label assignment and when the traffic is not likely to belong to any pretrained classes. We present a new public dataset of network traffic that includes labeled virtual-private-network-encrypted network traffic generated by ten applications and corresponding to five application categories. We also present an ML framework that is designed to rapidly train with modest data requirements and provide both calibrated predictive probabilities and an interpretable “out-of-distribution” (OOD) score to flag novel traffic samples. We describe calibrating OOD scores using $p$ -values of the relative Mahalanobis distance. We demonstrate that our framework achieves an F1-score of 0.98 on our dataset and that it can extend to an enterprise network by testing the model: 1) on data from similar applications; 2) on dissimilar application traffic from an existing category; and 3) on application traffic from a new category. The model correctly flags uncertain traffic and, upon retraining, accurately incorporates the new data.},
  archive      = {J_TAI},
  author       = {Steven Jorgensen and John Holodnak and Jensen Dempsey and Karla de Souza and Ananditha Raghunath and Vernon Rivet and Noah DeMoes and Andrés Alejos and Allan Wollaber},
  doi          = {10.1109/TAI.2023.3244168},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {420-433},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Extensible machine learning for encrypted network traffic application labeling via uncertainty quantification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Crafting transferable adversarial examples against face
recognition via gradient eroding. <em>TAI</em>, <em>5</em>(1), 412–419.
(<a href="https://doi.org/10.1109/TAI.2023.3253083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep neural networks (DNNs) have made significant progress on face recognition (FR). However, DNNs have been found to be vulnerable to adversarial examples, leading to fatal consequences in real-world applications. This article focuses on improving the transferability of adversarial examples against FR models. We propose gradient eroding (GE) to make the gradient of the residual blocks more diverse, by eroding the back-propagation dynamically. We also propose a novel black-box adversarial attack named corrasion attack based on GE. Extensive experiments demonstrate that our approach can effectively improve the transferability of adversarial attacks against FR models. Our approach overperforms 29.35% in fooling rate than state-of-the-art black-box attacks. Leveraging adversarial training with adversarial examples generated by us, the robustness of models can be improved by up to 43.2%. Besides, corrasion attack successfully breaks two online FR systems, achieving a highest fooling rate of 89.8%.},
  archive      = {J_TAI},
  author       = {Huipeng Zhou and Yajie Wang and Yu-an Tan and Shangbo Wu and Yuhang Zhao and Quanxin Zhang and Yuanzhang Li},
  doi          = {10.1109/TAI.2023.3253083},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {412-419},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Crafting transferable adversarial examples against face recognition via gradient eroding},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prototype enhancement-based incremental evolution learning
for urban garbage classification. <em>TAI</em>, <em>5</em>(1), 398–411.
(<a href="https://doi.org/10.1109/TAI.2023.3250207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network (DNN) based on incremental learning provides support for efficient garbage classification tasks. However, it is always challenging to accurately learn and preserve the information of known classes for updating DNN while new tasks are continuously emerging, which also affects the generalization performance of the model. To solve these issues, an incremental evolution learning (IEL) method based on prototype enhancement is proposed to accurately preserve data and improve the model generalization ability. First, a prototype enhancement method based on multidimensional Gaussian kernel density estimation is designed, which extends the prototype of each sample based on high-dimensional nonlinear data distribution. Then, the prototype enhancement accurately represents the known class data. Second, a contrastive feature method is proposed to constrain the consistency of features between tasks, which reduces the deviation between different tasks. Then, the extraction preference caused by the class sample imbalance is balanced and the generalization ability is improved. Third, the proposed IEL is used for garbage classification with imbalanced class samples. IEL implements garbage classification by effectively adapting to the differences between known classes and new classes. Finally, experiments on four standard datasets and one public garbage dataset verify that IEL has a strong classification ability in the learning tasks with the increasing number of classes.},
  archive      = {J_TAI},
  author       = {Honggui Han and Xiaoye Fan and Fangyu Li},
  doi          = {10.1109/TAI.2023.3250207},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {398-411},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Prototype enhancement-based incremental evolution learning for urban garbage classification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Patch-mixing contrastive regularization for few-label
semi-supervised learning. <em>TAI</em>, <em>5</em>(1), 384–397. (<a
href="https://doi.org/10.1109/TAI.2023.3247975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, consistency regularization has become a fundamental component in semi-supervised learning, which tries to make the network&#39;s predictions on unlabeled data to be invariant to perturbations. However, its performance decreases drastically when there are scarce labels, e.g., two labels per category. In this article, we analyze the semantic bias problem in consistency regularization for semi-supervised learning and find that this problem stems from imposing consistency regularization on some semantically biased positive sample pairs derived from indispensable data augmentation. Based on the above analysis, we propose a patch-mixing contrastive regularization approach called $p$ -Mix for semi-supervised learning with scarce labels. In $p$ -Mix, the magnitude of semantic bias is estimated by weighting augmented samples in the embedding space. Specifically, the samples are mixed in both sample space and embedding space, respectively, to construct more reliable and task-relevant positive sample pairs. Then, a patch-mixing contrastive objective is designed to indicate the magnitude of semantic bias by utilizing a mixed embedding weighted by virtual soft labels. Extensive experiments were conducted, demonstrating that $p$ -Mix significantly outperforms current state-of-the-art approaches. Especially, $p$ -Mix achieves an accuracy of 91.95% on the CIFAR-10 benchmark with only two labels available for each category, which exceeds the second-best method ICL-SSL by 3.22%.},
  archive      = {J_TAI},
  author       = {Xiaochang Hu and Xin Xu and Yujun Zeng and Xihong Yang},
  doi          = {10.1109/TAI.2023.3247975},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {384-397},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Patch-mixing contrastive regularization for few-label semi-supervised learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PpNNT: Multiparty privacy-preserving neural network training
system. <em>TAI</em>, <em>5</em>(1), 370–383. (<a
href="https://doi.org/10.1109/TAI.2023.3247554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By leveraging smart devices [e.g., industrial Internet of Things (IIoT)] and real-time data analytics, organizations, such as production plants can benefit from increased productivity, reduced costs, enhanced self-monitoring, and autonomous decision-making. In such a setting, machine learning plays an important role in data analytics, but the use of conventional centralized machine learning solutions may raise uncomfortable concerns about data privacy. Hence, one can explore the use of federated learning. In this article, we propose privacy- p reserving deep neural network training (PpNNT), which is designed to support federated learning in the multiparty setting. To minimize the overall costs, we further design a hybrid architecture to fully maximize resource utilization. Our proposed design allows the PpNNT system to provide high security, efficiency, and scalability for IIoT data analytics, as evidenced by our theoretical security proof and experimental results on the CIFAR10 dataset.},
  archive      = {J_TAI},
  author       = {Qi Feng and Debiao He and Jian Shen and Min Luo and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TAI.2023.3247554},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {370-383},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {PpNNT: Multiparty privacy-preserving neural network training system},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning-observer-based adaptive tracking control of
multiagent systems using compensation mechanism. <em>TAI</em>,
<em>5</em>(1), 358–369. (<a
href="https://doi.org/10.1109/TAI.2023.3247550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the adaptive consensus tracking problem for nonlinear multiagent systems with unmeasurable state and input saturation. For unmeasurable state and nonlinearities, a learning-observer inspired by the idea of extended state observer and the learning ability of neural network are established to observe the reconstructed system. In view of input saturation, a nonlinear auxiliary system with the same order is constructed to obtain the compensation signals. Different from the previous auxiliary system, the proposed method simultaneously compensates the change of nonlinearity caused by input saturation. Subsequently, a consensus tracking error is designed to ensure that the follower tracks the neighbor compensated output. Moreover, an adaptive consensus tracking control method based on learning-observer and compensation mechanism is proposed under the backstepping framework. The stability analysis is given by virtue of the Lyapunov theory. Finally, the effectiveness and superiority of the proposed control algorithm are verified through the simulation of unmanned air vehicles with one virtual leader and four followers.},
  archive      = {J_TAI},
  author       = {Zhiqiang Li and Yang Liu and Hui Ma and Hongyi Li},
  doi          = {10.1109/TAI.2023.3247550},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {358-369},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Learning-observer-based adaptive tracking control of multiagent systems using compensation mechanism},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-free quantum gate design and calibration using deep
reinforcement learning. <em>TAI</em>, <em>5</em>(1), 346–357. (<a
href="https://doi.org/10.1109/TAI.2023.3243187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-fidelity quantum gate design is important for various quantum technologies, such as quantum computation and quantum communication. Numerous control policies for quantum gate design have been proposed given a dynamical model of the quantum system of interest. However, a quantum system is often highly sensitive to noise, and obtaining its accurate modeling can be difficult for many practical applications. Thus, the control policy based on a quantum system model may be unpractical for quantum gate design. Also, quantum measurements collapse quantum states, which makes it challenging to obtain information through measurements during the control process. In this article, we propose a novel training framework using deep reinforcement learning for model-free quantum control. The proposed framework relies only on the measurement at the end of the control process and offers the ability to find the optimal control policy without access to quantum systems during the learning process. The effectiveness of the proposed technique is numerically demonstrated for model-free quantum gate design and quantum gate calibration using off-policy reinforcement learning algorithms.},
  archive      = {J_TAI},
  author       = {Omar Shindi and Qi Yu and Parth Girdhar and Daoyi Dong},
  doi          = {10.1109/TAI.2023.3243187},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {346-357},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Model-free quantum gate design and calibration using deep reinforcement learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interpretable prediction modeling for froth flotation via
stacked graph convolutional network. <em>TAI</em>, <em>5</em>(1),
334–345. (<a href="https://doi.org/10.1109/TAI.2023.3240114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time prediction of key quality variables based on data-driven soft sensor modeling is an important way to monitor flotation status and product quality in the froth flotation process. However, the existing data-driven methods have limitations in terms of nonlinear feature extraction and interpretability. In addition, the prevalent correlations between process variables can help improve the model interpretability and feature extraction of the model but there are still challenges in exploring the potential correlations between process variables due to the complexity of the industrial process mechanism and the presence of high noise in process data. These relationships can be abstracted as edges between nodes in a graph representation. To fully explore the deep process-related structural features of data, a novel stacked graph convolutional network (S-GCN) is proposed. First, S-GCN aggregates the features of neighboring nodes by an adaptive adjacency matrix to achieve the extraction of structural features among process variables. Then, the labeled quality variable data detected by a laboratory assay analysis are utilized to pretrain the network layer by layer to obtain better initial parameters and adjacency matrices for quality prediction. Finally, the prediction model is constructed based on the learned parameters to explore the interpretability of the model and perform prediction tasks. Experimental results on real industrial potassium chloride froth flotation process data show a significant improvement in the prediction accuracy and interpretability of the proposed method.},
  archive      = {J_TAI},
  author       = {Yalin Wang and Qingkai Sui and Chenliang Liu and Kai Wang and Xiaofeng Yuan and Guangfeng Dong},
  doi          = {10.1109/TAI.2023.3240114},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {334-345},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Interpretable prediction modeling for froth flotation via stacked graph convolutional network},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multivariate time-series representation learning via
hierarchical correlation pooling boosted graph neural network.
<em>TAI</em>, <em>5</em>(1), 321–333. (<a
href="https://doi.org/10.1109/TAI.2023.3241896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning is vital for the performance of multivariate time series (MTS)-related tasks. Given high-dimensional MTS data, researchers generally rely on deep learning models to learn representative features. Among them, the methods that can capture the spatial–temporal dependencies within MTS data generally achieve better performance. However, they ignored hierarchical relations and the dynamic property within MTS data, hindering their performance. To address these problems, we propose a hierarchical correlation pooling boosted graph neural network for MTS data representation learning. First, we propose a novel correlation pooling scheme to learn and capture hierarchical correlations between sensors. Meanwhile, a new assignment matrix is designed to ensure the effective learning of hierarchical correlations by adaptively combining both sensor features and correlations. Second, we learn sequential graphs to represent the dynamic property within MTS data, so that this property can be captured for learning decent representations. We conducted extensive experiments to test our model on various MTS tasks, including remaining useful life prediction, human activity recognition, and sleep stage classification. Experimental results have shown the effectiveness of our proposed model.},
  archive      = {J_TAI},
  author       = {Yucheng Wang and Min Wu and Xiaoli Li and Lihua Xie and Zhenghua Chen},
  doi          = {10.1109/TAI.2023.3241896},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {321-333},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multivariate time-series representation learning via hierarchical correlation pooling boosted graph neural network},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An automated identification approach for partial discharge
detection using density-based clustering without user inputs.
<em>TAI</em>, <em>5</em>(1), 310–320. (<a
href="https://doi.org/10.1109/TAI.2023.3237648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recently published state-of-the-art density-based clustering technique called ICFSFDP for partial discharge (PD) detection requires various sensitive user-defined input parameters. This article presents a parameter-free clustering technique called PDAutoClust for PD detection. The PDAutoClust algorithm can produce high-quality clusters for PD datasets without requiring user-defined input parameters. PDAutoClust produces high-quality clustering results by utilizing a vein-based density clustering approach. The vein of a cluster is produced by using multivariate kernel density estimation and a unique neighborhood set. We compared the performance of PDAutoClust against ICFSFDP and seven other state-of-the-art density-based and non-density-based clustering techniques by using four PD datasets in terms of adjusted rand index, normalized mutual information, F1-score, and purity. Another contribution of the article is a novel merging technique used with PDAutoClust to merge small non-viable clusters that a clustering technique may produce. PDAutoClust produces the final clusters for a dataset by merging the non-viable clusters that a clustering technique may produce. We also evaluate the performance of PDAutoClust with merging technique versus PDAutoClust without merging technique using four datasets. Simulation results for PDAutoClust with the merging technique show good performance compared to ICFSFDP and seven other state-of-the-art clustering techniques. We also performed an ablation study to demonstrate the importance of the steps involved in PDAutoClust.},
  archive      = {J_TAI},
  author       = {Md Anisur Rahman and Li-Minn Ang and Kah Phooi Seng},
  doi          = {10.1109/TAI.2023.3237648},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {310-320},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An automated identification approach for partial discharge detection using density-based clustering without user inputs},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A universal metric for robust evaluation of synthetic
tabular data. <em>TAI</em>, <em>5</em>(1), 300–309. (<a
href="https://doi.org/10.1109/TAI.2022.3229289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic tabular data generation becomes crucial when real data are limited, expensive to collect, or simply cannot be used due to privacy concerns. However, producing good quality synthetic data is challenging. Several probabilistic, statistical, generative adversarial networks and variational autoencoder-based approaches have been presented for synthetic tabular data generation. Once generated, evaluating the quality of the synthetic data is quite challenging. Some of the traditional metrics have been used in the literature, but there is lack of a common, robust, and single metric. This makes it difficult to properly compare the effectiveness of different synthetic tabular data generation methods. In this article, we propose a new universal metric, TabSynDex, for the robust evaluation of synthetic data. The proposed metric assesses the similarity of synthetic data with real data through different component scores, which evaluate the characteristics that are desirable for “high-quality” synthetic data. Being a single score metric and having an implicit bound, TabSynDex can also be used to observe and evaluate the training of neural network-based approaches. This would help in obtaining insights that was not possible earlier. We present several baseline models for comparative analysis of the proposed evaluation metric with existing generative models. We also give a comparative analysis between TabSynDex and existing synthetic tabular data evaluation metrics. This shows the effectiveness and universality of our metric over the existing metrics.},
  archive      = {J_TAI},
  author       = {Vikram S Chundawat and Ayush K Tarun and Murari Mandal and Mukund Lahoti and Pratik Narang},
  doi          = {10.1109/TAI.2022.3229289},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {300-309},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A universal metric for robust evaluation of synthetic tabular data},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Class-separation preserving pruning for deep neural
networks. <em>TAI</em>, <em>5</em>(1), 290–299. (<a
href="https://doi.org/10.1109/TAI.2022.3228511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural network pruning has been deemed essential in the deployment of deep neural networks on resource-constrained edge devices, greatly reducing the number of network parameters without drastically compromising accuracy. A class of techniques proposed in the literature assigns an importance score to each parameter and prunes those of the least importance. However, most of these methods are based on generalized estimations of the importance of each parameter, ignoring the context of the specific task at hand. In this article, we propose a task specific pruning approach, CSPrune, which is based on how efficiently a neuron or a convolutional filter is able to separate classes. Our axiomatic approach assigns an importance score based on how separable different classes are in the output activations or feature maps, preserving the separation of classes which avoids the reduction in classification accuracy. Additionally, most pruning algorithms prune individual connections or weights leading to a sparse network without taking into account whether the hardware the network is deployed on can take advantage of that sparsity or not. CSPrune prunes whole neurons or filters which results in a more structured pruned network whose sparsity can be more efficiently utilized by the hardware. We evaluate our pruning method against various benchmark datasets, both small and large, and network architectures and show that our approach outperforms comparable pruning techniques.},
  archive      = {J_TAI},
  author       = {Inder Preet and Oisín Boydell and Deepu John},
  doi          = {10.1109/TAI.2022.3228511},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {290-299},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Class-separation preserving pruning for deep neural networks},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial modality alignment network for cross-modal
molecule retrieval. <em>TAI</em>, <em>5</em>(1), 278–289. (<a
href="https://doi.org/10.1109/TAI.2023.3254518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cross-modal molecule retrieval (Text2Mol) task aims to bridge the semantic gap between molecules and natural language descriptions. A solution to this nontrivial problem relies on a graph convolutional network (GCN) and cross-modal attention with contrastive learning for reasonable results. However, there exist the following issues. First, the cross-modal attention mechanism is only in favor of text representations and cannot provide helpful information for molecule representations. Second, the GCN-based molecule encoder ignores edge features and the importance of various substructures of a molecule. Finally, the retrieval learning loss function is rather simplistic. This article further investigates the Text2Mol problem and proposes a novel adversarial modality alignment network (AMAN) based method to sufficiently learn both description and molecule information. Our method utilizes a SciBERT as a text encoder and a graph transformer network as a molecule encoder to generate multimodal representations. Then, an adversarial network is used to align these modalities interactively. Meanwhile, a triplet loss function is leveraged to perform retrieval learning and further enhance the modality alignment. Experiments on the ChEBI-20 dataset show the effectiveness of our AMAN method compared with baselines.},
  archive      = {J_TAI},
  author       = {Wenyu Zhao and Dong Zhou and Buqing Cao and Kai Zhang and Jinjun Chen},
  doi          = {10.1109/TAI.2023.3254518},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {278-289},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adversarial modality alignment network for cross-modal molecule retrieval},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy-preserving multiview matrix factorization for
recommender systems. <em>TAI</em>, <em>5</em>(1), 267–277. (<a
href="https://doi.org/10.1109/TAI.2023.3240700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With an increasing focus on data privacy, there have been pilot studies on recommender systems in a federated learning (FL) framework, where multiple parties collaboratively train a model without sharing their data. Most of these studies assume that the conventional FL framework can fully protect user privacy. However, there are serious privacy risks in matrix factorization in federated recommender systems based on our study. This article first provides a rigorous theoretical analysis of the server reconstruction attack in four scenarios in federated recommender systems, followed by comprehensive experiments. The empirical results demonstrate that the FL server could infer users&#39; information with accuracy $&amp;gt;80\%$ based on the uploaded gradients from FL nodes. The robustness analysis suggests that our reconstruction attack analysis outperforms the random guess by $&amp;gt;30\%$ under Laplace noises with $b\leq 0.5$ for all scenarios. Then, the article proposes a new privacy-preserving framework based on a threshold variant of homomorphic encryption, privacy-preserving multiview matrix factorization (PrivMVMF), to enhance user data privacy protection in federated recommender systems. The proposed PrivMVMF is successfully implemented and tested thoroughly with the MovieLens dataset.},
  archive      = {J_TAI},
  author       = {Peihua Mai and Yan Pang},
  doi          = {10.1109/TAI.2023.3240700},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {267-277},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Privacy-preserving multiview matrix factorization for recommender systems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accelerating point-voxel representation of 3-d object
detection for automatic driving. <em>TAI</em>, <em>5</em>(1), 254–266.
(<a href="https://doi.org/10.1109/TAI.2023.3237787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current point-voxel fusion methods for 3-D object detection could not make full use of complementary information in the field of autonomous driving. Therefore, a novel two-stage 3-D object detection method, called accelerating point-voxel representation (APVR), is proposed. The advantages of point-based feature and voxel-based feature can be integrated into a single 3-D representation. Thereby, the proposed method retains more fine-grained information of an object while maintaining high efficiency. Specifically, the computational cost is reduced by adding offsets to query neighboring voxels of key-points. More fine-grained information can be obtained by calculating the matching probability between neighboring voxels and key-points. During the optimization of the prediction boxes, virtual grid points are set to capture the spatial information between key-points. The constraint of minimum enclosing rectangle is also added to optimize the directions of the prediction boxes. A large number of experiments on the KITTI, NuScenes, and Waymo datasets demonstrate great generalizability and portability of the proposed approach. The effectiveness and efficiency of APVR have been proved by comparisons with the state-of-the-art methods. APVR makes the real-time processing frame rate reach 40.4 Hz while ensuring high prediction accuracy.},
  archive      = {J_TAI},
  author       = {JieCheng Cao and Chongben Tao and Zufeng Zhang and Zhen Gao and Xizhao Luo and Sifa Zheng and Yuan Zhu},
  doi          = {10.1109/TAI.2023.3237787},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {254-266},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Accelerating point-voxel representation of 3-D object detection for automatic driving},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robotic control in adversarial and sparse reward
environments: A robust goal-conditioned reinforcement learning approach.
<em>TAI</em>, <em>5</em>(1), 244–253. (<a
href="https://doi.org/10.1109/TAI.2023.3237665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With deep neural networks-based function approximators, reinforcement learning holds the promise of learning complex end-to-end robotic controllers that can map high-dimensional sensory information directly to control policies. However, a common challenge, especially for robotics, is sample-efficient learning from sparse rewards, in which an agent is required to find a long sequence of “correct” actions to achieve a desired outcome. Unfortunately, inevitable perturbations on observations may make this task trickier to solve. Here, this article advances a novel robust goal-conditioned reinforcement learning approach for end-to-end robotic control in adversarial and sparse reward environments. Specifically, a mixed adversarial attack scheme is presented to generate diverse adversarial perturbations on observations by combining white-box and black-box attacks. Meanwhile, a hindsight experience replay technique considering observation perturbations is developed to turn a failed experience into a successful one and generate the policy trajectories perturbed by the mixed adversarial attacks. Additionally, a robust goal-conditioned actor–critic method is proposed to learn goal-conditioned policies and keep the variations of the perturbed policy trajectories within bounds. Finally, the proposed method is evaluated on three tasks with adversarial attacks and sparse reward settings. The results indicate that our scheme can ensure robotic control performance and policy robustness on the adversarial and sparse reward tasks.},
  archive      = {J_TAI},
  author       = {Xiangkun He and Chen Lv},
  doi          = {10.1109/TAI.2023.3237665},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {244-253},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Robotic control in adversarial and sparse reward environments: A robust goal-conditioned reinforcement learning approach},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature selection for unbalanced distribution hybrid data
based on <span class="math inline"><em>k</em></span>-nearest
neighborhood rough set. <em>TAI</em>, <em>5</em>(1), 229–243. (<a
href="https://doi.org/10.1109/TAI.2023.3237203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neighborhood rough sets are now widely used to process numerical data. Nevertheless, most of the existing neighborhood rough sets are not able to distinguish class mixture samples well when dealing with classification problems. That is, it cannot effectively classify categories when dealing with data with an unbalanced distribution. Because of this, in this article, we propose a new feature selection method that takes into consideration both heterogeneous data and feature interaction. The proposed model well integrates the ascendancy of ${\delta }$ -neighborhood and ${k}$ -nearest neighbor. Such heterogeneous data can be handled better than existing neighborhood models. We utilize information entropy theories such as mutual information and conditional mutual information and employ an iterative strategy to define the importance of each feature in decision making. Furthermore, we design a feature extraction algorithm based on the above idea. Experimental results display that the raised algorithm has superior effect than some existing algorithms, particularly the ${\delta }$ -neighborhood rough set model and the ${k}$ -nearest neighborhood rough set model.},
  archive      = {J_TAI},
  author       = {Weihua Xu and Ziting Yuan and Zheng Liu},
  doi          = {10.1109/TAI.2023.3237203},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {229-243},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Feature selection for unbalanced distribution hybrid data based on ${k}$-nearest neighborhood rough set},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint learning and channel coding for error-tolerant IoT
systems based on machine learning. <em>TAI</em>, <em>5</em>(1), 217–228.
(<a href="https://doi.org/10.1109/TAI.2023.3235778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In several machine learning (ML) based Internet of Things (IoT) systems, data are captured by IoT devices and then transmitted over a wireless channel for remote processing. Since noise often appears on the channel (so causing data corruption and consequently an incorrect ML result), channel protection must be provided to guarantee an acceptable error rate for the transmitted data, especially in safety-critical applications. An often-used protection technique employs error correction codes (ECCs); however, even with some improved designs, the power dissipation required by an ECC implementation may still not meet the strict requirements of hardware-constrained platforms. To address this issue, a “joint learning and channel coding” (JLCC) scheme is proposed in this article. In such a scheme, the ML model is retrained using two methods to tolerate some channel errors, such that the system requires an ECC with significantly lower protection capability. Since ML training is executed remotely, JLCC achieves a significant power reduction for ECC without introducing any additional overhead to the IoT device. An electrocardiogram (ECG) system is taken as a case study to illustrate the proposed JLCC scheme and evaluate its effectiveness. A low-density parity-check code is employed for the protection of the system with/without JLCC; its analysis and implementation are presented. Simulation results show that, when employing JLCC with the proposed two retraining methods, an average reduction of 29.15% and 34.82% in the dissipated power is achieved for the ECG sensor when compared to the original system.},
  archive      = {J_TAI},
  author       = {Xiaochen Tang and Pedro Reviriego and Wei Tang and David G. M. Mitchell and Fabrizio Lombardi and Shanshan Liu},
  doi          = {10.1109/TAI.2023.3235778},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {217-228},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Joint learning and channel coding for error-tolerant IoT systems based on machine learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Blind image despeckling using a multiscale attention-guided
neural network. <em>TAI</em>, <em>5</em>(1), 205–216. (<a
href="https://doi.org/10.1109/TAI.2023.3235342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coherent imaging systems have been applied in the detection of target of interest, natural resource exploration, ailment diagnosis, etc. However, it is easy to generate speckle-degraded images due to the coherent interference of reflected echoes, restricting these practical applications. Speckle noise is a granular interference that affects the observed reflectivity. It is often modeled as multiplicative noise with a negative exponential distribution. This nonlinear property makes despeckling of imaging data an intractable problem. To enhance the despeckling performance, we propose to blindly remove speckle noise using an intelligent computing-enabled multiscale attention-guided neural network (termed MSANN). In particular, we first introduce the logarithmic transformation to convert the multiplicative speckle noise model to an additive version. Our MSANN, essentially a feature pyramid network, is then exploited to restore degraded images in the logarithmic domain. To enhance the generalization ability of the MSANN, a multiscale feature enhancement attention module is incorporated into the MSANN to extract multiscale features for improving the imaging quality. A multiscale mixed loss function is further presented to increase the network robustness during training. The final despeckled images are naturally equivalent to the exponential versions of the output of the MSANN. Experimental results have shown that the MSANN has the capacity of effectively removing the speckle noise while preserving essential structures. It can achieve superior despeckling results in terms of visual quality and quantitative measures.},
  archive      = {J_TAI},
  author       = {Yu Guo and Yuxu Lu and Ryan Wen Liu and Fenghua Zhu},
  doi          = {10.1109/TAI.2023.3235342},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {205-216},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Blind image despeckling using a multiscale attention-guided neural network},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A transistor operations model for deep learning energy
consumption scaling law. <em>TAI</em>, <em>5</em>(1), 192–204. (<a
href="https://doi.org/10.1109/TAI.2022.3229280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have transformed the automation of a wide range of industries and found increasing ubiquity in society. The high complexity of DNN models and its widespread adoption has led to global energy consumption doubling every 3–4 months. Current energy consumption measures largely monitor system wide consumption or make linear assumptions of DNN models. The former approach captures other unrelated energy consumption anomalies, whilst the latter does not accurately reflect nonlinear computations. In this article, we are the first to develop a bottom-up transistor operations (TOs) approach to expose the role of nonlinear activation functions and neural network structure. As there will be inevitable energy measurement errors at the core level, we statistically model the energy scaling laws as opposed to absolute consumption values. We offer models for both feedforward DNNs and convolution neural networks on a variety of datasets and hardware configurations—achieving a 93.6%–99.5% precision. This outperforms existing floating-point operations (FLOPs)-based methods and our TOs method can be further extended to other DNN models.},
  archive      = {J_TAI},
  author       = {Chen Li and Antonios Tsourdos and Weisi Guo},
  doi          = {10.1109/TAI.2022.3229280},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {192-204},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A transistor operations model for deep learning energy consumption scaling law},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image super-resolution with content-aware feature
processing. <em>TAI</em>, <em>5</em>(1), 179–191. (<a
href="https://doi.org/10.1109/TAI.2022.3225784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image super-resolution (SR) is currently a very active research topic with applications spanning from computer vision to videos and graphic industries. The top performers in SR field usually employ deep or wide convolutional neural networks (CNNs) to restore the lost textures from low-resolution images. However, most of these methods adopt pixel shuffle and deconvolution as their upsampling techniques and often generate conspicuous artifacts in the reconstructed image. In addition, the ongoing trend of directly portraying the degraded low-resolution image to a high-resolution image via complex deep CNNs improves the reconstruction performance, but at the cost of high computational complexity. In this article, we propose a multilevel bicubic upsampler network for reconstructing high-quality SR image with a restricted number of parameters. A novel content-aware feature difference (CAFD) block is presented to reform the network by focusing on contextual information. The proposed CAFD block consists of four multilevel attention blocks for a better extraction of low-level features at different scales. Furthermore, we design an innovative upsampling layer that consistently outperforms the traditional upsampling methods. These components collaboratively endow our proposed network with a great performance boost, helping it achieve state-of-the-art accuracy on five synthetic benchmark datasets, both qualitatively and quantitatively. In addition, a detailed ablation study has been accomplished to scrutinize the improvements obtained by different modules in the proposed method.},
  archive      = {J_TAI},
  author       = {Nancy Mehta and Subrahmanyam Murala},
  doi          = {10.1109/TAI.2022.3225784},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {179-191},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Image super-resolution with content-aware feature processing},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continual unsupervised domain adaptation in data-constrained
environments. <em>TAI</em>, <em>5</em>(1), 167–178. (<a
href="https://doi.org/10.1109/TAI.2022.3233791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation (DA) techniques aim to overcome the domain shift between the source domain used for training and the target domain where testing takes place. However, current DA methods assume that the entire target domain is available during adaptation, which may not hold in practice. We introduce a new, data-constrained DA paradigm where unlabeled target samples are received in batches and adaptation is performed continually. We propose a novel source-free method for continual unsupervised domain adaptation (UDA) that utilizes a buffer for selective replay of previously seen samples. In our continual DA framework, we selectively mix samples from incoming batches with data stored in a buffer using buffer management strategies and use the combination to incrementally update our model. We evaluate and compare the classification performance of the continual DA approach with state-of-the-art (SOTA) DA methods based on the entire target domain. Our results on three popular DA datasets demonstrate the benefits of our method when operating in data constrained environments. We further extend our experiments to adapting over multiple target domains and our method performs favorably with the SOTA methods.},
  archive      = {J_TAI},
  author       = {Abu Md Niamul Taufique and Chowdhury Sadman Jahan and Andreas Savakis},
  doi          = {10.1109/TAI.2022.3233791},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {167-178},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Continual unsupervised domain adaptation in data-constrained environments},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tensorial evolutionary computation for spatial optimization
problems. <em>TAI</em>, <em>5</em>(1), 154–166. (<a
href="https://doi.org/10.1109/TAI.2022.3229297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial optimization problems (SOPs) refer to a class of problems where the decision variables require spatial organization. Existing methods based on evolutionary algorithms (EAs) fit conventional evolutionary operators by flattening the spatial representations of SOPs to one dimension, which hence loses crucial spatial structures of the potential solutions. To address this issue, this article proposes a tensorial evolutionary algorithm (TEA) by formulating a tensor space of the SOPs, where the population of candidate solutions is represented as a third-order tensor. Accordingly, we design a tensor-product crossover operator and a two-phase mutation consisting of a local convolutional mutation operator and a global Gaussian mutation operator in the tensor space. These operators are all implemented based on tensor operations. Under the consideration of spatial context on individual solutions, we devise three types of evolutionary tensors, namely, the crossover tensor, the convolutional mutant tensor, and the Gaussian mutant tensor for the three tensorial evolutionary operators correspondingly. The resultant TEA is tested using two typical real-world SOPs, namely, the facility layout problem and the image steganography problem. Comparative experimental studies validate the effectiveness and robustness of TEA.},
  archive      = {J_TAI},
  author       = {Si-Chao Lei and Xiaolin Xiao and Yue-Jiao Gong and Yun Li and Jun Zhang},
  doi          = {10.1109/TAI.2022.3229297},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {154-166},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Tensorial evolutionary computation for spatial optimization problems},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Syntactic graph attention network for aspect-level sentiment
analysis. <em>TAI</em>, <em>5</em>(1), 140–153. (<a
href="https://doi.org/10.1109/TAI.2022.3227535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-level sentiment classification (ASC) is designed to identify the sentiment orientation of given aspect terms in a sentence. Previous neural networks have used attention mechanisms to align context words with the appropriate aspect terms. Without considering syntactic dependencies, these models may erroneously focus on context words that are not related to the aspect terms. To address this issue, the graph convolution network (GCN) and the graph attention network (GAT) are proposed to build a graph based on the dependency parse tree, allowing the representations of context words to be propagated to the aspect terms according to their syntactic dependencies. However, these models consider all syntactic dependencies to be of the same type, and thus may result in inappropriate propagation of word representations in the graph. To further distinguish between the syntactic dependencies, this study proposes a syntactic graph attention network (SGAN) to incorporate the knowledge of dependency types into the GAT. The dependency types are modeled as edge embeddings to learn the attention weight of each edge according to its dependency type. By considering different dependency types and their weights, the proposed method can block inappropriate propagation to better associate the context words to aspect terms. To increase training process stability and enrich the diversity of graph representations, a weighted multihead attention is applied to compose the graph representations generated by different heads. The experimental results on five benchmark datasets show that the SGAN yields more accurate results than existing methods.},
  archive      = {J_TAI},
  author       = {Li Yuan and Jin Wang and Liang-Chih Yu and Xuejie Zhang},
  doi          = {10.1109/TAI.2022.3227535},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {140-153},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Syntactic graph attention network for aspect-level sentiment analysis},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning multiresolution features for unsupervised anomaly
localization on industrial textured surfaces. <em>TAI</em>,
<em>5</em>(1), 127–139. (<a
href="https://doi.org/10.1109/TAI.2022.3227142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In industrial quality assessment, monitoring whether the textured product contains defects is a critical step. Compared to a large number of defect-free images that are easy to obtain, anomaly samples are limited and vary randomly in size and type. It is challenging to develop an automatic and accurate texture defect localization system that only uses normal images for training. In this article, a multiresolution feature learning network is proposed to detect various texture defects in an unsupervised manner. A robust pretrained model is first employed to extract the perceptual features from the input image, then the perceptual features of various layers are fed to the corresponding multiscale autoencoder framework. This hierarchical alignment strategy aids in receiving multilevel information for locating anomalies of various sizes. Moreover, a residual attention module is embedded in the architecture to further improve the detection performance. Our proposed method has achieved state-of-the-art performance on the texture dataset of MVTecAD. We also extended the experiment to the real industrial texture datasets, and its detection result is better than the major existing advanced techniques.},
  archive      = {J_TAI},
  author       = {Xian Tao and Shaohua Yan and Xinyi Gong and Chandranath Adak},
  doi          = {10.1109/TAI.2022.3227142},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {127-139},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Learning multiresolution features for unsupervised anomaly localization on industrial textured surfaces},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hybrid regularized multilayer perceptron for input noise
immunity. <em>TAI</em>, <em>5</em>(1), 115–126. (<a
href="https://doi.org/10.1109/TAI.2022.3225124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The immunity of multilayer perceptron (MLP) is less effective toward input noise. In this article, we have focused on the robustness of MLP with respect to input noise where noise can be additive or multiplicative. Here, we have proposed a DropConnect-based regularized MLP to reduce the coadaptation among the neurons of the hidden layer. At first, we have empirically and statistically shown that by reducing coadaptation among the hidden neurons, an MLP can achieve better noise immunity. We have also empirically shown that an MLP with input noise injection and $l_{2}$ regularizer is an effective approach to improve its noise immunity. However, the results indicate that it does not adjust the coadaptation among the hidden neurons. Therefore, for further improvement, we have proposed a hybrid regularized MLP (HRMLP), where DropConnect is combined with the noise injection and $l_{2}$ regularizer. In addition to input noise, we have also verified the robustness of HRMLP with respect to $\mathbf {20\%}$ outliers in the dataset. To justify the effectiveness of the proposed HRMLP, we have compared it with MLP, MLP with noise injection, MLP with $l_{2}$ regularizer, MLP with noise injection and $l_{2}$ regularizer, and MLP with DropConnect along with two state-of-the-art works based on $\mathbf {20}$ standard datasets. The experimental results for both noisy inputs and outliers confirm that the performance of HRMLP is significant compared to other methods.},
  archive      = {J_TAI},
  author       = {Rahul Mondal and Tandra Pal and Prasenjit Dey},
  doi          = {10.1109/TAI.2022.3225124},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {115-126},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A hybrid regularized multilayer perceptron for input noise immunity},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep dual attention network for precise diagnosis of
COVID-19 from chest CT images. <em>TAI</em>, <em>5</em>(1), 104–114. (<a
href="https://doi.org/10.1109/TAI.2022.3225372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic diagnosis of Coronavirus disease 2019 (COVID-19) using chest computed tomography (CT) images is of great significance for preventing its spread. However, it is difficult to precisely identify COVID-19 due to the following problems: 1) the location and size of lesions can vary greatly in CT images; 2) its unique characteristics are often imperceptible in imaging findings. To solve these problems, a Deep Dual Attention Network ( $\text {D}^{\text {2}}\text {ANet}$ ) is proposed for accurate diagnosis of COVID-19 by integrating dual attention modules (DAMs) with different scales of the feature extractor, where DAM can adaptively detect relevant lesion regions to extract discriminative imaging features of COVID-19. Specifically, DAM is implemented by two parallel blocks: 1) global attention block (GAB) and 2) local attention block (LAB), in which GAB is designed to roughly locate the infected regions from the entire image by modeling global contexts while LAB is developed to explicitly highlight subtle differences of COVID-19 from other viral pneumonia in the infected regions by learning detailed lesion information. Experimental results on several public datasets show that $\text {D}^{\text {2}}\text {ANet}$ outperforms the state-of-the-art approaches in various performance metrics.},
  archive      = {J_TAI},
  author       = {Zhijie Lin and Zhaoshui He and Ruoyu Yao and Xu Wang and Taiheng Liu and Yamei Deng and Shengli Xie},
  doi          = {10.1109/TAI.2022.3225372},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {104-114},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep dual attention network for precise diagnosis of COVID-19 from chest CT images},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incremental weighted ensemble for data streams with concept
drift. <em>TAI</em>, <em>5</em>(1), 92–103. (<a
href="https://doi.org/10.1109/TAI.2022.3224416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a popular strategy to tackle concept drift, chunk-based ensemble method adapts a new concept by adjusting the weights of historical classifiers. However, most previous approaches normally evaluate the historical classifier based on an entire chunk newly arrived, which may cause delayed adaptation. To address the issue, two novel ensemble models, named incremental weighted ensemble (IWE) and incremental weighted ensemble for multi-classification (IWE-M), are proposed. At each time step, all base classifiers are incrementally updated on a newly arrived instance. Following that, the instance is collected into a cache array. Once a data chunk is formed, a new base classifier is created. More specially, a forgetting mechanism based on variable-size window is designed to adjust the weight of each base classifier in IWE in terms of its classification accuracy on the latest instances in an online manner. IWE-M, an extension of IWE, aims to solve multiclass problems with local concept drifts. In IWE-M, the weight of a base classifier is expanded to a weight vector. In this way, this ensemble model can retain specific historical information about nondrift regions from a local drift. Experimental results show that the proposed ensemble frameworks outperform six competitive approaches on accuracy and G-mean.},
  archive      = {J_TAI},
  author       = {Botao Jiao and Yinan Guo and Cuie Yang and Jiayang Pu and Zhiji Zheng and Dunwei Gong},
  doi          = {10.1109/TAI.2022.3224416},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {92-103},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Incremental weighted ensemble for data streams with concept drift},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A generative neighborhood-based deep autoencoder for robust
imbalanced classification. <em>TAI</em>, <em>5</em>(1), 80–91. (<a
href="https://doi.org/10.1109/TAI.2023.3249685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models perform remarkably well on many classification tasks recently. The superior performance of deep neural networks relies on the large number of training data, which at the same time must have an equal class distribution in order to be efficient. However, in most real-world applications, the labeled data may be limited with high imbalance ratios among the classes, and thus, the learning process of most classification algorithms is adversely affected resulting in unstable predictions and low performance. Three main categories of approaches address the problem of imbalanced learning, i.e., data-level, algorithmic level, and hybrid methods, which combine the two aforementioned approaches. Data generative methods are typically based on generative adversarial networks, which require significant amounts of data, while model-level methods entail extensive domain expert knowledge to craft the learning objectives, thereby being less accessible for users without such knowledge. Moreover, the vast majority of these approaches are designed and applied to imaging applications, less to time series, and extremely rare to both of them. To address the above issues, we introduce GENDA, a generative neighborhood-based deep autoencoder, which is simple yet effective in its design and can be successfully applied to both image and time-series data. GENDA is based on learning latent representations that rely on the neighboring embedding space of the samples. Extensive experiments, conducted on a variety of widely-used real datasets demonstrate the efficacy of the proposed method.},
  archive      = {J_TAI},
  author       = {Eirini Troullinou and Grigorios Tsagkatakis and Attila Losonczy and Panayiota Poirazi and Panagiotis Tsakalides},
  doi          = {10.1109/TAI.2023.3249685},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {80-91},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A generative neighborhood-based deep autoencoder for robust imbalanced classification},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). One-stage deep edge detection based on dense-scale feature
fusion and pixel-level imbalance learning. <em>TAI</em>, <em>5</em>(1),
70–79. (<a href="https://doi.org/10.1109/TAI.2022.3223893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge detection, a basic task in the field of computer vision, is an important preprocessing operation for the recognition and understanding of a visual scene. In conventional models, the edge image generated is ambiguous, and the edge lines are also very thick, which typically necessitates the use of nonmaximum suppression (NMS) and morphological thinning operations to generate clear and thin edge images. In this article, we aim to propose a one-stage neural network model that can generate high-quality edge images without postprocessing. The proposed model adopts a classic encoder–decoder framework in which a pretrained neural model is used as the encoder and a multifeature-fusion mechanism that merges the features of each level with each other functions as a learnable decoder. Further, we propose a new loss function that addresses the pixel-level imbalance in the edge image by suppressing the false positive edge information near the true positive edge and the false negative nonedge. The results of experiments conducted on several benchmark datasets indicate that the proposed method achieves state-of-the-art (SOTA) results without using NMS and morphological thinning operations.},
  archive      = {J_TAI},
  author       = {Chunjie Wang and Dawei Dai and Shuyin Xia and Yingge Liu and Guoyin Wang},
  doi          = {10.1109/TAI.2022.3223893},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {70-79},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {One-stage deep edge detection based on dense-scale feature fusion and pixel-level imbalance learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Offline–online actor–critic. <em>TAI</em>, <em>5</em>(1),
61–69. (<a href="https://doi.org/10.1109/TAI.2022.3225251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offline–online reinforcement learning (RL) can effectively address the problem of missing data (commonly known as transition) in offline RL. However, due to the effect of distribution shift, the performance of policy may degrade when an agent moves from offline to online training phases. In this article, we first analyze the problems of distribution shift and policy performance degradation in offline–online RL. Then, in order to alleviate these problems, we propose a novel RL algorithm offline–online actor–critic (O2AC) algorithm. In O2AC, a behavior clone constraint term is introduced into the policy objective function to address the distribution shift in offline training phase. In addition, in online training phase, the influence of the behavior clone constraint term is gradually reduced, which alleviates the policy performance degradation. Experiments show that O2AC outperforms existing offline–online RL algorithms.},
  archive      = {J_TAI},
  author       = {Xuesong Wang and Diyuan Hou and Longyang Huang and Yuhu Cheng},
  doi          = {10.1109/TAI.2022.3225251},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {61-69},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Offline–Online Actor–Critic},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Credal valuation networks for machine reasoning under
uncertainty. <em>TAI</em>, <em>5</em>(1), 51–60. (<a
href="https://doi.org/10.1109/TAI.2023.3247971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contemporary undertakings provide limitless opportunities for widespread application of machine reasoning and artificial intelligence in situations characterized by uncertainty, hostility, and sheer volume of data. The article develops a valuation network as a graphical system for higher-level fusion and reasoning under uncertainty in support of the human operators. Valuations, which are mathematical representation of (uncertain) knowledge and collected data, are expressed as credal sets, defined as coherent interval probabilities in the framework of imprecise probability theory. The basic operations with such credal sets, combination, and marginalization, are defined to satisfy the axioms of a valuation algebra. A practical implementation of the credal valuation network is discussed and its utility demonstrated on a small scale example.},
  archive      = {J_TAI},
  author       = {Branko Ristic and Alessio Benavoli and Sanjeev Arulampalam},
  doi          = {10.1109/TAI.2023.3247971},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {51-60},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Credal valuation networks for machine reasoning under uncertainty},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gaussian switch sampling: A second-order approach to active
learning. <em>TAI</em>, <em>5</em>(1), 38–50. (<a
href="https://doi.org/10.1109/TAI.2023.3246959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In active learning, acquisition functions define informativeness directly on the representation position within the model manifold. However, for most machine learning models (in particular neural networks) this representation is not fixed due to the training pool fluctuations in between active learning rounds. Therefore, several popular strategies are sensitive to experiment parameters (e.g., architecture) and do not consider model robustness to out-of-distribution settings. To alleviate this issue, we propose a grounded second-order definition of information content and sample importance within the context of active learning. Specifically, we define importance by how often a neural network “forgets” a sample during training artifacts of second-order representation shifts. We show that our definition produces highly accurate importance scores even when the model representations are constrained by the lack of training data. Motivated by our analysis, we develop the Gaussian switch sampling ( GauSS ). We show that GauSS is setup agnostic and robust to anomalous distributions with exhaustive experiments on three in-distribution benchmarks, three out-of-distribution benchmarks, and three different architectures. We report an improvement of up to 5% when compared against four popular query strategies.},
  archive      = {J_TAI},
  author       = {Ryan Benkert and Mohit Prabhushankar and Ghassan AlRegib and Armin Pacharmi and Enrique Corona},
  doi          = {10.1109/TAI.2023.3246959},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {38-50},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Gaussian switch sampling: A second-order approach to active learning},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic review of echo state networks from design to
application. <em>TAI</em>, <em>5</em>(1), 23–37. (<a
href="https://doi.org/10.1109/TAI.2022.3225780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recurrent neural network (RNN) has demonstrated its outstanding ability in sequence tasks and has achieved state of the art in many applications, such as industrial and medical. An echo state network (ESN) is a simple type of RNN and has emerged in the last decade as an alternative to gradient descent training-based RNN. The ESN is practical, conceptually simple, and easy to implement with a strong theoretical ground. It can avoid nonconverging and computationally expensive issues in gradient descent RNN methods. Since the ESN was put forward in 2002, abundant existing works have promoted the progress of ESN, and the recently introduced deep ESN opened the way to uniting the merits of deep learning and reservoir computing. Besides, the combinations of ESNs with other machine learning models have also overperformed baselines in some applications. However, the apparent simplicity of ESNs can sometimes be deceptive. Successfully applying ESNs needs some experience. Thus, we reviewed over 300 related papers and provided a systematic overview for the first time. In this article, we categorize the related methods into classical ESN, DeepESN, and combination. Then, we analyze them from the perspective of network designs and specific applications. Finally, we discuss the challenges and opportunities by proposing open problems and future work.},
  archive      = {J_TAI},
  author       = {Chenxi Sun and Moxian Song and Derun Cai and Baofeng Zhang and Shenda Hong and Hongyan Li},
  doi          = {10.1109/TAI.2022.3225780},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {23-37},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A systematic review of echo state networks from design to application},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward handling uncertainty-at-source in AI—a review and
next steps for interval regression. <em>TAI</em>, <em>5</em>(1), 3–22.
(<a href="https://doi.org/10.1109/TAI.2023.3234930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of statistics and AI draw insights through modeling discord or variance between sources (i.e., intersource) of information. Increasingly however, research is focusing on uncertainty arising at the level of individual measurements (i.e., within- or intrasource), such as for a given sensor output or human response. Here, adopting intervals rather than numbers as the fundamental data-type provides an efficient, powerful, yet challenging way forward—offering systematic capture of uncertainty-at-source, increasing informational capacity, and ultimately potential for additional insight. Following progress in the capture of interval-valued data in particular from human participants, conducting machine learning directly upon intervals is a crucial next step. This article focuses on linear regression for interval-valued data as a recent growth area, providing an essential foundation for broader use of intervals in AI. We conduct an in-depth analysis of state-of-the-art methods, elucidating their behavior, advantages, and pitfalls when applied to synthetic and real-world datasets with different properties. Specific emphasis is given to the challenge of preserving mathematical coherence, i.e., models maintain fundamental mathematical properties of intervals. In support of real-world applicability of the regression methods, we introduce and demonstrate a novel visualization approach, the interval regression graph (IRG), which effectively communicates the impact of both position and range of variables within the regression models—offering a leap in their interpretability. Finally, this article provides practical recommendations concerning regression-method choice for interval data and highlights remaining challenges and important next steps for developing AI with the capacity to handle uncertainty-at-source.},
  archive      = {J_TAI},
  author       = {Shaily Kabir and Christian Wagner and Zack Ellerby},
  doi          = {10.1109/TAI.2023.3234930},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {3-22},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Toward handling uncertainty-at-source in AI—A review and next steps for interval regression},
  volume       = {5},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
