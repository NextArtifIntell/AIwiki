<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TCDS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tcds---175">TCDS - 175</h2>
<ul>
<li><details>
<summary>
(2024). Estimation of the cyclopean eye from binocular smooth
pursuit tests. <em>TCDS</em>, <em>16</em>(6), 2125–2137. (<a
href="https://doi.org/10.1109/TCDS.2024.3410110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In binocular vision, the visual system combines images in the retina to generate a single perception, which triggers a sensorimotor process that forces the eyes to point to the same target. Thus, following a moving target, both eyes are expected to move synchronously following identical motor triggers but, in practise, significant differences between eyes are found due to the presence of certain artifacts and effects. Thus, a better indirect characterization of the underlying neurological behavior during eye motion would require new automatic preprocessing methods applied to the eye-tracking sequences for rendering the common and most significant movements of both eyes. To address this need, the present study proposes an automatic method for extracting the common components of the left- and right-eye motions from a set of Smooth Pursuit tests by applying an independent component analysis. To do so, both sequences are decomposed into two independent latent components: the first presumably correlates with the common motor triggering at the brain, while the second collects artifacts introduced during the recording process and small effects due to convergence deficits and eye dominance biases. The evaluations were carried out using data corresponding to 12 different smooth pursuit eye movements tests, which were collected using an infrared high-speed video-based eye-tracking device from 41 parkinsonian patients and 47 controls. The results show that the automatic method can separate the aforementioned components in 99.50% of cases, extracting a latent component correlated with the common motor triggering at the brain, which we hypothesize is characterizing the movements of the cyclopean eye. The estimated component could be used to simplify any other potential automatic analysis.},
  archive      = {J_TCDS},
  author       = {Elisa Luque-Buzo and Mehdi Bejani and Julián D. Arias-Londoñ and Jorge A. Gómez-García and Francisco Grandas-Pérez and Juan I. Godino-Llorente},
  doi          = {10.1109/TCDS.2024.3410110},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {6},
  pages        = {2125-2137},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Estimation of the cyclopean eye from binocular smooth pursuit tests},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automotive object detection via learning sparse events by
spiking neurons. <em>TCDS</em>, <em>16</em>(6), 2110–2124. (<a
href="https://doi.org/10.1109/TCDS.2024.3410371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event-based sensors, distinguished by their high temporal resolution of $1 {\boldsymbol{\mu}}\text{s}$ and a dynamic range of $120 \mathrm{dB}$ , stand out as ideal tools for deployment in fast-paced settings such as vehicles and drones. Traditional object detection techniques that utilize artificial neural networks (ANNs) face challenges due to the sparse and asynchronous nature of the events these sensors capture. In contrast, spiking neural networks (SNNs) offer a promising alternative, providing a temporal representation that is inherently aligned with event-based data. This article explores the unique membrane potential dynamics of SNNs and their ability to modulate sparse events. We introduce an innovative spike-triggered adaptive threshold mechanism designed for stable training. Building on these insights, we present a specialized spiking feature pyramid network (SpikeFPN) optimized for automotive event-based object detection. Comprehensive evaluations demonstrate that SpikeFPN surpasses both traditional SNNs and advanced ANNs enhanced with attention mechanisms. Evidently, SpikeFPN achieves a mean average precision (mAP) of 0.477 on the GEN1 automotive detection (GAD) benchmark dataset, marking significant increases over the selected SNN baselines. Moreover, the efficient design of SpikeFPN ensures robust performance while optimizing computational resources, attributed to its innate sparse computation capabilities.},
  archive      = {J_TCDS},
  author       = {Hu Zhang and Yanchen Li and Luziwei Leng and Kaiwei Che and Qian Liu and Qinghai Guo and Jianxing Liao and Ran Cheng},
  doi          = {10.1109/TCDS.2024.3410371},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {6},
  pages        = {2110-2124},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Automotive object detection via learning sparse events by spiking neurons},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decoding joint-level hand movements with intracortical
neural signals in a human brain–computer interface. <em>TCDS</em>,
<em>16</em>(6), 2100–2109. (<a
href="https://doi.org/10.1109/TCDS.2024.3409555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine movements of hands play an important role in everyday life. While existing studies have successfully decoded hand gestures or finger movements from brain signals, direct decoding of single-joint kinematics remains challenging. This study aims to investigate the decoding of fine hand movements at the single-joint level. Neural activities were recorded from the motor cortex (MC) of a human participant while imagining eleven different hand movements. We comprehensively evaluated the decoding efficiency of various brain signal features, neural decoding algorithms, and single-joint kinematic variables for decoding. Results showed that using the spiking band power (SBP) signals, we could faithfully decode the single-joint angles with an average correlation coefficient of 0.77, outperforming other brain signal features. Nonlinear approaches that incorporate temporal context information, particularly recurrent neural networks, significantly outperformed traditional methods. Decoding joint angles yielded superior results compared to joint angular velocities. Our approach facilitates the construction of high-performance brain–computer interfaces for dexterous hand control.},
  archive      = {J_TCDS},
  author       = {Huaqin Sun and Yu Qi and Xiaodi Wu and Junming Zhu and Jianmin Zhang and Yueming Wang},
  doi          = {10.1109/TCDS.2024.3409555},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {6},
  pages        = {2100-2109},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Decoding joint-level hand movements with intracortical neural signals in a human Brain–Computer interface},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal strategies and cooperative teaming for 3-d
multiplayer reach-avoid games. <em>TCDS</em>, <em>16</em>(6), 2085–2099.
(<a href="https://doi.org/10.1109/TCDS.2024.3406889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies multiplayer reach-avoid games with a plane being the goal in 3-D space. Due to the difficulty that directly analyzing multipursuer multievader scenarios brings the curse of dimensionality, the whole problem is decomposed to distinct subgames. In the subgames, a single pursuer or multiple pursuers, which have different speeds, form a team to capture one evader cooperatively while the evader struggles to reach the plane. With the players’ dominance region based on the definition of isochronous surfaces, the target points and value functions are obtained for the game of degree by using Apollonius spheres. Additionally, the corresponding closed-loop saddle-point strategies are shown to be Nash equilibrium. The degeneration between scenarios of different scales is also discussed. To minimize the sum of subgames’ costs, the tasks of intercepting multiple evaders are assigned to individuals or teams in the form of bipartite graph matching. A hierarchical matching algorithm and a state-feedback rematching method are proposed which can be updated in real-time to improve the solution. Finally, diverse empirical experiments and comparisons with state-of-the-art methods are illustrated to demonstrate the optimality of proposed strategies and algorithms in this article.},
  archive      = {J_TCDS},
  author       = {Peng Gao and Xiuxian Li and Jinwen Hu},
  doi          = {10.1109/TCDS.2024.3406889},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {6},
  pages        = {2085-2099},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Optimal strategies and cooperative teaming for 3-D multiplayer reach-avoid games},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep reinforcement learning for autonomous driving based on
safety experience replay. <em>TCDS</em>, <em>16</em>(6), 2070–2084. (<a
href="https://doi.org/10.1109/TCDS.2024.3405896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of autonomous driving, safety has always been a top priority, especially in recent years with the development and increasing application of deep reinforcement learning (DRL) in autonomous driving. Ensuring the safety of algorithms has become an indispensable concern. Reinforcement learning (RL), which involves interacting with the environment through trial and error, may result in unsafe behavior in autonomous driving without any safety constraints. Such behavior could result in the drive path deviation and even collision, causing catastrophic accidents. Therefore, this article proposes a reinforcement learning algorithm based on a safety experience replay mechanism, which is primarily to enhance the safety of reinforcement learning in autonomous driving. First, the ego vehicle conducts preliminary exploration of the environment to collect data. Based on the performance of completing tasks observed from each data trajectory, safety labels of different levels are assigned to all state-action pairs, which establishes a safety experience buffer. Further, a safety-critic network is constructed, which is trained by randomly sampling from the safety experience buffer. This enables the network to quantitatively evaluate the safety of driving actions, and the goal of safe driving for ego vehicle is achieved. The experimental results indicate that the proposed method can effectively reduce driving risks and improve task success rates compared with conventional reinforcement learning algorithms.},
  archive      = {J_TCDS},
  author       = {Xiaohan Huang and Yuhu Cheng and Qiang Yu and Xuesong Wang},
  doi          = {10.1109/TCDS.2024.3405896},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {6},
  pages        = {2070-2084},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Deep reinforcement learning for autonomous driving based on safety experience replay},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Long-term and short-term opponent intention inference for
football multiplayer policy learning. <em>TCDS</em>, <em>16</em>(6),
2055–2069. (<a href="https://doi.org/10.1109/TCDS.2024.3404061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A highly competitive and confrontational football match is full of strategic and tactical challenges. Therefore, player&#39;s cognition on their opponents’ strategies and tactics is quite crucial. However, the match&#39;s complexity results in that the opponents’ intentions are often changeable. Under these circumstances, how to discriminate and predict the opponents’ intentions of future actions and tactics is an important problem for football players’ decision-making. Considering that the opponents’ cognitive processes involve deliberative and reactive processes, a long-term and short-term opponent intention inference (LS-OII) method for football multiplayer policy learning is proposed. First, to capture the cognition about opponents’ deliberative process, we design an opponent tactics deduction module for inferring the opponents’ long-term tactical intentions from a macro perspective. Furthermore, an opponent decision prediction module is designed to infer the opponents’ short-term decision which often yields rapid and direct impacts on football matches. Additionally, an opponent-driven incentive module is designed to enhance the players’ causal awareness of the opponents’ intentions, further to improve the players exploration capabilities and effectively obtain outstanding policies. Representative results demonstrate that the LS-OII method significantly enhances the efficacy of players’ strategies in the Google Research Football environment, thereby affirming the superiority of our method.},
  archive      = {J_TCDS},
  author       = {Shijie Wang and Zhiqiang Pu and Yi Pan and Boyin Liu and Hao Ma and Jianqiang Yi},
  doi          = {10.1109/TCDS.2024.3404061},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {6},
  pages        = {2055-2069},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Long-term and short-term opponent intention inference for football multiplayer policy learning},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive multiview graph convolutional network for 3-d point
cloud classification and segmentation. <em>TCDS</em>, <em>16</em>(6),
2043–2054. (<a href="https://doi.org/10.1109/TCDS.2024.3403900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud classification and segmentation are crucial tasks for point cloud processing and have wide range of applications, such as autonomous driving and robot grasping. Some pioneering methods, including PointNet, VoxNet, and DGCNN, have made substantial advancements. However, most of these methods overlook the geometric relationships between points at large distances from different perspectives within the point cloud. This oversight constrains feature extraction capabilities and consequently limits any further improvements in classification and segmentation accuracy. To address this issue, we propose an adaptive multiview graph convolutional network (AM-GCN), which comprehensively synthesizes both the global geometric features of the point cloud and the local features within the projection planes of multiple views through an adaptive graph construction method. First, an adaptive rotation module in AM-GCN is proposed to predict a more favorable angle of view for projection. Then, a multilevel feature extraction network can flexibly be constructed by spatial-based or spectral-based graph convolution layers. Finally, AM-GCN is evaluated on ModelNet40 for classification, ShapeNetPart for part segmentation, ScanNetv2 and S3DIS for scene segmentation, which demonstrates the robustness of the AM-GCN with competitive performance compared with existing methods. It is worth noting that it performs state-of-the-art performance in many categories.},
  archive      = {J_TCDS},
  author       = {Wanhao Niu and Haowen Wang and Chungang Zhuang},
  doi          = {10.1109/TCDS.2024.3403900},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {6},
  pages        = {2043-2054},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Adaptive multiview graph convolutional network for 3-D point cloud classification and segmentation},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Control with style: Style embedding-based variational
autoencoder for controlled stylized caption generation framework.
<em>TCDS</em>, <em>16</em>(6), 2032–2042. (<a
href="https://doi.org/10.1109/TCDS.2024.3405573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic image captioning is a computationally intensive and structurally complicated task that describes the contents of an image in the form of a natural language sentence. Methods developed in the recent past focused mainly on the description of factual content in images thereby ignoring the different emotions and styles (romantic, humorous, angry, etc.) associated with the image. To overcome this, few works incorporated style-based caption generation that captures the variability in the generated descriptions. This article presents a style embedding-based variational autoencoder for controlled stylized caption generation framework (RFCG+SE-VAE-CSCG). It generates controlled text-based stylized descriptions of images. It works in two phases, i.e., $ 1)$ refined factual caption generation (RFCG); and $ 2)$ SE-VAE-CSCG. The former defines an encoder–decoder model for the generation of refined factual captions. Whereas, the latter presents a SE-VAE for controlled stylized caption generation. The overall proposed framework generates style-based descriptions of images by leveraging bag of captions (BoCs). More so, with the use of a controlled text generation model, the proposed work efficiently learns disentangled representations and generates realistic stylized descriptions of images. Experiments on MSCOCO, Flickr30K, and FlickrStyle10K provide state-of-the-art results for both refined and style-based caption generation, supported with an ablation study.},
  archive      = {J_TCDS},
  author       = {Dhruv Sharma and Chhavi Dhiman and Dinesh Kumar},
  doi          = {10.1109/TCDS.2024.3405573},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {6},
  pages        = {2032-2042},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Control with style: Style embedding-based variational autoencoder for controlled stylized caption generation framework},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Progressive transfer learning for dexterous in-hand
manipulation with multifingered anthropomorphic hand. <em>TCDS</em>,
<em>16</em>(6), 2019–2031. (<a
href="https://doi.org/10.1109/TCDS.2024.3406730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dexterous in-hand manipulation poses significant challenges for a multifingered anthropomorphic hand due to the high-dimensional state and action spaces, as well as the intricate contact patterns between the fingers and objects. Although deep reinforcement learning has made moderate progress and demonstrated its strong potential for manipulation, it faces certain challenges, including large-scale data collection and high sample complexity. Particularly in scenes with slight changes, it necessitates the recollection of vast amounts of data and numerous iterations of fine-tuning. Remarkably, humans can quickly transfer their learned manipulation skills to different scenarios with minimal supervision. Inspired by the flexible transfer learning capability of humans, we propose a novel framework called progressive transfer learning (PTL) for dexterous in-hand manipulation. This framework efficiently utilizes the collected trajectories and the dynamics model trained on a source dataset. It adopts progressive neural networks for dynamics model transfer learning on samples selected using a new method based on dynamics properties, rewards, and trajectory scores. Experimental results on contact-rich anthropomorphic hand manipulation tasks demonstrate that our method can efficiently and effectively learn in-hand manipulation skills with just a few online attempts and adjustment learning in the new scene. Moreover, compared to learning from scratch, our method significantly reduces training time costs by 85%.},
  archive      = {J_TCDS},
  author       = {Yongkang Luo and Wanyi Li and Peng Wang and Haonan Duan and Wei Wei and Jia Sun},
  doi          = {10.1109/TCDS.2024.3406730},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {6},
  pages        = {2019-2031},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Progressive transfer learning for dexterous in-hand manipulation with multifingered anthropomorphic hand},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event-based depth prediction with deep spiking neural
network. <em>TCDS</em>, <em>16</em>(6), 2008–2018. (<a
href="https://doi.org/10.1109/TCDS.2024.3406168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event cameras have gained popularity in depth estimation due to their superior features such as high-temporal resolution, low latency, and low-power consumption. Spiking neural network (SNN) is a promising approach for processing event camera inputs due to its spike-based event-driven nature. However, SNNs face performance degradation when the network becomes deeper, affecting their performance in depth estimation tasks. To address this issue, we propose a deep spiking U-Net model. Our spiking U-Net architecture leverages refined shortcuts and residual blocks to avoid performance degradation and boost task performance. We also propose a new event representation method designed for multistep SNNs to effectively utilize depth information in the temporal dimension. Our experiments on MVSEC dataset show that the proposed method improves accuracy by 18.50% and 25.18% compared to current state-of-the-art (SOTA) ANN and SNN models, respectively. Moreover, the energy efficiency can be improved up to 58 times by our proposed SNN model compared with the corresponding ANN with the same network structure.},
  archive      = {J_TCDS},
  author       = {Xiaoshan Wu and Weihua He and Man Yao and Ziyang Zhang and Yaoyuan Wang and Bo Xu and Guoqi Li},
  doi          = {10.1109/TCDS.2024.3406168},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {6},
  pages        = {2008-2018},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Event-based depth prediction with deep spiking neural network},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EEG decoding based on normalized mutual information for
motor imagery brain–computer interfaces. <em>TCDS</em>, <em>16</em>(6),
1997–2007. (<a href="https://doi.org/10.1109/TCDS.2024.3401717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In current research, noninvasive brain–computer interfaces (BCIs) typically rely on electroencephalogram (EEG) signals to measure brain activity. Motor imagery EEG decoding is an important research field of BCIs. Although multichannel EEG signals provide higher resolution, they contain noise and redundant data unrelated to the task, which affect the performance of BCI systems. We investigate the interactions between EEG signals from dependence analysis to improve the classification accuracy. In this article, a novel channel selection method based on normalized mutual information (NMI) is first proposed to select the informative channels. Then, a histogram of oriented gradient is applied to feature extraction in the rearranged NMI matrices. Finally, a support vector machine with a radial basis function kernel is used for the classification of different motor imagery tasks. Four publicly available BCI datasets are employed to evaluate the effectiveness of the proposed method. The experimental results show that the proposed decoding scheme significantly improves classification accuracy and outperforms other competing methods.},
  archive      = {J_TCDS},
  author       = {Chao Tang and Dongyao Jiang and Lujuan Dang and Badong Chen},
  doi          = {10.1109/TCDS.2024.3401717},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {6},
  pages        = {1997-2007},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {EEG decoding based on normalized mutual information for motor imagery Brain–Computer interfaces},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Small object detection based on microscale perception and
enhancement-location feature pyramid. <em>TCDS</em>, <em>16</em>(6),
1982–1996. (<a href="https://doi.org/10.1109/TCDS.2024.3397684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the large number of small objects, significant scale variation, and uneven distribution in images captured by unmanned aerial vehicles (UAVs), existing algorithms have high rates of missing and false detections of small objects in drone images. A new object detection algorithm based on microscale perception and enhancement-location feature pyramid is proposed in this article. The microscale perception module alternatives the original convolution module in backbone, changing the receptive field through two dilation branches with various dilation rates and an adjustment switch branch. To better match the size and shape of sampled targets, the weighted deformable convolution is employed. The enhancement-location feature pyramid module aggregates the features from each layer to obtain balanced semantic information and refines aggregated features to enhance their ability to represent features. Moreover, a bottom-up branch structure is added to utilize the property of lower layer features being beneficial to locating small objects to enhance the localization ability for small objects. Additionally, by using specific image cropping and combining techniques, the target distribution of the training data is altered to make the model more sensitive to small objects and improving its robustness. Finally, a sample balance strategy is used in combination with focal loss and a sample extraction control method to balance simple hard sample imbalance and the long-tail distribution of interclass sample imbalance during training. Experimental results show that the proposed algorithm achieves a mean average precision of 35.9% on the VisDrone2019 dataset, which is a 14.2% improvement over the baseline Cascade RCNN and demonstrates better performance in detecting small objects in drone images. Compared with advanced algorithms in recent years, it also achieves state-of-the-art detection accuracy.},
  archive      = {J_TCDS},
  author       = {Guang Han and Chenwei Guo and Ziyang Li and Haitao Zhao},
  doi          = {10.1109/TCDS.2024.3397684},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {6},
  pages        = {1982-1996},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Small object detection based on microscale perception and enhancement-location feature pyramid},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine unlearning for seizure prediction. <em>TCDS</em>,
<em>16</em>(6), 1969–1981. (<a
href="https://doi.org/10.1109/TCDS.2024.3395663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, companies and organizations have been required to provide individuals with the right to be forgotten to alleviate privacy concerns. In machine learning, this requires researchers not only to delete data from databases but also to remove data information from trained models. Thus, machine unlearning is becoming an emerging research problem. In seizure prediction field, prediction applications are established most on private electroencephalogram (EEG) signals. To provide the right to be forgotten, we propose a machine unlearning method for seizure prediction. Our proposed unlearning method is based on knowledge distillation using two teacher models to guide the student model toward achieving model-level unlearning objective. One teacher model is used to induce the student model to forget data information of patients with unlearning request (forgetting patients), while the other teacher model is used to enable the student model to retain data information of other patients (remaining patients). Experiments were conducted on CHBMIT and Kaggle databases. Results show that our proposed unlearning method can effectively make trained ML models forget the information of forgetting patients and maintain satisfactory performance on remaining patients. To the best of our knowledge, it is the first work of machine unlearning in seizure prediction field.},
  archive      = {J_TCDS},
  author       = {Chenghao Shao and Chang Li and Rencheng Song and Xiang Liu and Ruobing Qian and Xun Chen},
  doi          = {10.1109/TCDS.2024.3395663},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {6},
  pages        = {1969-1981},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Machine unlearning for seizure prediction},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HR-SNN: An end-to-end spiking neural network for four-class
classification motor imagery brain–computer interface. <em>TCDS</em>,
<em>16</em>(6), 1955–1968. (<a
href="https://doi.org/10.1109/TCDS.2024.3395443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural network (SNN) excels in processing temporal information and conserving energy, particularly when deployed on neuromorphic hardware. These strengths position SNN as an ideal choice for developing wearable brain–computer interface (BCI) devices. However, the application of SNN in complex BCI tasks, like four-class motor imagery classification, is limited. In light of this, this study introduces a powerful SNN architecture hybrid response SNN (HR-SNN). We employ parameterwise gradient descent methods to optimize spike encoding efficiency. The SNN&#39;s frequency perception is improved by integrating a hybrid response spiking module. In addition, a diff-potential spiking decoder is designed to optimize SNN output potential utilization. Validation experiments are performed on PhysioNet and BCI competition IV 2a datasets. On PhysioNet, our model achieves accuracies of 67.24% and 74.95% using global training and subject-specific transfer learning, respectively. On BCI competition IV 2a, our approach attains an average accuracy of 77.58%, surpassing all the compared SNN models and demonstrating competitiveness against state-of-the-art (SOTA) convolution neural network (CNN) approaches. We validate the robustness of HR-SNN under noise and channel loss scenarios. Additionally, energy analysis reveals HR-SNN&#39;s superior energy efficiency compared to existing CNN models. Notably, HR-SNN exhibits a 2–16 times energy consumption advantage over existing SNN methods.},
  archive      = {J_TCDS},
  author       = {Yulin Li and Liangwei Fan and Hui Shen and Dewen Hu},
  doi          = {10.1109/TCDS.2024.3395443},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {6},
  pages        = {1955-1968},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {HR-SNN: An end-to-end spiking neural network for four-class classification motor imagery Brain–Computer interface},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging spatiotemporal estimation for online adaptive
steady-state visual evoked potential recognition. <em>TCDS</em>,
<em>16</em>(6), 1943–1954. (<a
href="https://doi.org/10.1109/TCDS.2024.3392745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online adaptive canonical correction analysis (OACCA) has been applied successfully in the recently popular steady-state visual evoked potential (SSVEP) target recognition methods. However, due to the significant amount of spatiotemporal relevant background noise in the online historical sample label data of OACCA, there is redundant noise component in the learned common spatial filter that can reduce online classification accuracy. Aiming at solving this defect in OACCA, we designed an online spatial–temporal equalization filter (STE) to suppress the background noise component in the electroencephalography (EEG). Meanwhile, an adaptive decoding method for SSVEP based on online spatial–temporal estimation (STE-OACCA) is proposed by combining the online STE filter and the OACCA algorithm. A pseudoonline test on the Tsinghua University FBCCA-DW dataset shows that the proposed STE-OACCA method significantly outperforms the CCA, MSI, OACCA approaches as well as STE-CCA. More importantly, proposed method can be directly used in online SSVEP recognition without calibration. The proposed algorithm is robust, which is promising for the development of practical brain computer interface (BCI).},
  archive      = {J_TCDS},
  author       = {Jing Jin and Xinjie He and Brendan Z. Allison and Ke Qin and Xingyu Wang and Andrzej Cichocki},
  doi          = {10.1109/TCDS.2024.3392745},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {6},
  pages        = {1943-1954},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Leveraging spatiotemporal estimation for online adaptive steady-state visual evoked potential recognition},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive framework for long-term sensory home training: A
feasibility study. <em>TCDS</em>, <em>16</em>(6), 1929–1942. (<a
href="https://doi.org/10.1109/TCDS.2024.3393635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training programs, based on principles of brain-plasticity and skill learning, are useful in counteracting functional decline in pathological conditions. Training effects of such procedures are well described but their adaptive features are usually not reported. A software framework designed for a long-term home training program is presented. It gradually trains users, provides a multidimensional range of stimulus differentiation, encompasses a strategy to increase the task demand and includes motivational reinforcement components. The structured framework was tested in a feasibility study involving two perceptual discrimination tasks (visual and auditory) in four persons in middle-to-older adulthood who were trained for 30 days. Practicability of the training was shown in a home setting by high adherence to the procedure, adaptive increase in task demand over time and positive learning effects on an individual level. Participants learned to distinguish progressively smaller target objects in the visual task (with diminished contrast) and sweeps progressively varying less in frequency in the auditory task (with overlapping noise). This adaptive procedure can provide the basis for the design of extended training programs engaging sensory function in individuals with impaired sensorimotor and cognitive functions. Further investigations are necessary to assess the generalization of learning effects and clinical validity.},
  archive      = {J_TCDS},
  author       = {Stefano Silvoni and Simon Desch and Florian Beier and Robin Bekrater-Bodmann and Annette Löffler and Dieter Kleinböhl and Stefano Tamascelli and Herta Flor},
  doi          = {10.1109/TCDS.2024.3393635},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {6},
  pages        = {1929-1942},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Adaptive framework for long-term sensory home training: A feasibility study},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Minimizing EEG human interference: A study of an adaptive
EEG spatial feature extraction with deep convolutional neural networks.
<em>TCDS</em>, <em>16</em>(6), 1915–1928. (<a
href="https://doi.org/10.1109/TCDS.2024.3391131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion is one of the main psychological factors that affects human behavior. Using a neural network model trained with electroencephalography (EEG)-based frequency features has been widely used to accurately recognize human emotions. However, utilizing EEG-based spatial information with popular 2-D kernels of convolutional neural networks (CNNs) has rarely been explored in the extant literature. This article addresses these challenges by proposing an EEG-based spatial-frequency-based framework for recognizing human emotion, resulting in fewer human interference parameters with better generalization performance. Specifically, we propose a two-stream hierarchical network framework that learns features from two networks, one trained from the frequency domain while another trained from the spatial domain. Our approach is extensively validated on the SEED, SEED-V, and DREAMER datasets. Our proposed method achieved an accuracy of 94.84% on the SEED dataset and 68.61% on the SEED-V dataset with EEG data only. The average accuracy of the Dreamer dataset is 93.01%, 92.04%, and 91.74% in valence, arousal, and dominance dimensions, respectively. The experiments directly support that our motivation of utilizing the two-stream domain features significantly improves the final recognition performance. The experimental results show that the proposed framework obtains improvements over state-of-the-art methods over these three varied scaled datasets. Furthermore, it also indicates the potential of the proposed framework in conjunction with current ImageNet pretrained models for improving performance on 1-D psychological signals.},
  archive      = {J_TCDS},
  author       = {Haojin Deng and Shiqi Wang and Yimin Yang and W. G. Will Zhao and Hui Zhang and Ruizhong Wei and Q. M. Jonathan Wu and Bao-Liang Lu},
  doi          = {10.1109/TCDS.2024.3391131},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {6},
  pages        = {1915-1928},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Minimizing EEG human interference: A study of an adaptive EEG spatial feature extraction with deep convolutional neural networks},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LITE-SNN: Leveraging inherent dynamics to train
energy-efficient spiking neural networks for sequential learning.
<em>TCDS</em>, <em>16</em>(6), 1905–1914. (<a
href="https://doi.org/10.1109/TCDS.2024.3396431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) are gaining popularity for their promise of low-power machine intelligence on event-driven neuromorphic hardware. SNNs have achieved comparable performance as artificial neural networks (ANNs) on static tasks (image classification) with lower compute energy. In this work, we explore the inherent dynamics of SNNs for sequential tasks such as gesture recognition, sentiment analysis, and sequence-to-sequence learning on data from dynamic vision sensors (DVSs) and natural language processing (NLP). Sequential data are generally processed with complex recurrent neural networks (RNNs) [long short-term memory/gated recurrent unit (LSTM/GRU)] with explicit feedback connections and internal states to handle the long-term dependencies. The neuron models in SNNs—integrate-and-fire (IF) or leaky-integrate-and-fire (LIF)—have internal states (membrane potential) that can be efficiently leveraged for sequential tasks. The membrane potential in the IF/LIF neuron integrates the incoming current and outputs an event (or spike) when the potential crosses a threshold value. Since SNNs compute with highly sparse spike-based spatiotemporal data, the energy/inference is lower than LSTMs/GRUs. We also show that SNNs require fewer parameters than LSTM/GRU resulting in smaller models and faster inference. We observe the problem of vanishing gradients in vanilla SNNs for longer sequences and implement a convolutional SNN with attention layers to perform sequence-to-sequence learning tasks. The inherent recurrence in SNNs, in addition to the fully parallelized convolutional operations, provide additional mechanisms to model sequential dependencies that lead to better accuracy than convolutional neural networks (CNNs) with ReLU activations. We evaluate SNN on gesture recognition from the IBM DVS dataset, sentiment analysis from the IMDB movie reviews dataset, and German-to-English translation from the Multi30k dataset.},
  archive      = {J_TCDS},
  author       = {Nitin Rathi and Kaushik Roy},
  doi          = {10.1109/TCDS.2024.3396431},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {6},
  pages        = {1905-1914},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {LITE-SNN: Leveraging inherent dynamics to train energy-efficient spiking neural networks for sequential learning},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MAVIDSQL: A model-agnostic visualization for interpretation
and diagnosis of text-to-SQL tasks. <em>TCDS</em>, <em>16</em>(5),
1887–1903. (<a href="https://doi.org/10.1109/TCDS.2024.3391278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant advancements in semantic parsing for text-to-SQL (T2S) tasks have been achieved through the employment of neural network models, such as LSTM, BERT, and T5. The exceptional performance of large language models, such as ChatGPT, has been demonstrated in recent research, even in zero-shot scenarios. However, the inherent transparency of T2S models presents them as black boxes, concealing their inner workings from both developers and users, which complicates the diagnosis of potential error patterns. Despite the fact that numerous visual analysis studies have been conducted in natural language processing communities, scant attention has been paid to addressing the challenges of semantic parsing, specifically in T2S tasks. This limitation hinders the development of effective tools for model optimization and evaluation. This article presents an interactive visual analysis tool, MAVIDSQL, to assist model developers and users in understanding and diagnosing T2S tasks. The system comprises three modules: the model manager, the feature extractor, and the visualization interface, which adopt a model-agnostic approach to diagnose potential errors and infer model decisions by analyzing input–output data, facilitating interactive visual analysis to identify error patterns and assess model performance. Two case studies and interviews with domain experts demonstrate the effectiveness of MAVIDSQL in facilitating the understanding of T2S tasks and identifying potential errors.},
  archive      = {J_TCDS},
  author       = {Jingwei Tang and Guodao Sun and Jiahui Chen and Gefei Zhang and Baofeng Chang and Haixia Wang and Ronghua Liang},
  doi          = {10.1109/TCDS.2024.3391278},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1887-1903},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {MAVIDSQL: A model-agnostic visualization for interpretation and diagnosis of text-to-SQL tasks},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cognitive assessment of scientific creative skill by
brain-connectivity analysis using graph convolutional interval type-2
fuzzy network. <em>TCDS</em>, <em>16</em>(5), 1872–1886. (<a
href="https://doi.org/10.1109/TCDS.2024.3390005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientific creativity refers to natural/automated genesis of innovations in science, propelling scientific, technological, industrial, and/or societal progress. Mental paper folding (MPF) requires spatial reasoning, which is an important attribute to determine creative potential of people. The article proposes a novel approach to determine creative potential of people from their brain-connectivity network (BCN) during their participation in MPF tasks using functional near-infrared spectroscopy (fNIRS). The work involves three phases. The first phase includes construction of BCN using Pearson&#39;s correlation method. The centrality features of the nodes in the network are assessed in the second phase and transferred to a proposed graph convolutional-interval type-2 fuzzy network (GC-IT2FN) in the third phase to classify the creative potential of individuals in four grades. The novelty of the work includes: 1) a novel self-attention mechanism in the network to guide graph convolution layers to focus on the most relevant nodes; 2) selection of a new activation function, Logish, after graph convolution to enhance classifier accuracy; and 3) utilizing the promising region in the footprint of uncertainty (FOU) of the used fuzzy sets of IT2FN-based classifier to reduce the effect of uncertainty in brain data on classifier performance. Experiments conducted demonstrate the efficacy of the proposed framework in contrast to traditional approaches.},
  archive      = {J_TCDS},
  author       = {Sayantani Ghosh and Amit Konar and Atulya K. Nagar},
  doi          = {10.1109/TCDS.2024.3390005},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1872-1886},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Cognitive assessment of scientific creative skill by brain-connectivity analysis using graph convolutional interval type-2 fuzzy network},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unifying obstacle avoidance and tracking control of
redundant manipulators subject to joint constraints: A new data-driven
scheme. <em>TCDS</em>, <em>16</em>(5), 1861–1871. (<a
href="https://doi.org/10.1109/TCDS.2024.3387575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern manufacturing, redundant manipulators have been widely deployed. Performing a task often requires the manipulator to follow specific trajectories while avoiding surrounding obstacles. Different from most existing obstacle-avoidance (OA) schemes that rely on the kinematic model of redundant manipulators, in this article, we propose a new data-driven obstacle-avoidance (DDOA) scheme for the collision-free tracking control of redundant manipulators. The OA task is formulated as a quadratic programming problem with inequality constraints. Then, the objectives of obstacle avoidance and tracking control are unitedly transformed into a computation problem of solving a system including three recurrent neural networks. With the Jacobian estimators designed based on zeroing neural networks, the manipulator Jacobian and critical-point Jacobian can be estimated in a data-driven way without knowing the kinematic model. Finally, the effectiveness of the proposed scheme is validated through extensive simulations and experiments.},
  archive      = {J_TCDS},
  author       = {Peng Yu and Ning Tan and Zhaohui Zhong and Cong Hu and Binbin Qiu and Changsheng Li},
  doi          = {10.1109/TCDS.2024.3387575},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1861-1871},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Unifying obstacle avoidance and tracking control of redundant manipulators subject to joint constraints: A new data-driven scheme},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward two-stream foveation-based active vision learning.
<em>TCDS</em>, <em>16</em>(5), 1843–1860. (<a
href="https://doi.org/10.1109/TCDS.2024.3390597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network (DNN) based machine perception frameworks process the entire input in a one-shot manner to provide answers to both “ what object is being observed” and “ where it is located.” In contrast, the “two-stream hypothesis” from neuroscience explains the neural processing in the human visual cortex as an active vision system that utilizes two separate regions of the brain to answer the what and the where questions. In this work, we propose a machine learning framework inspired by the “two-stream hypothesis” and explore the potential benefits that it offers. Specifically, the proposed framework models the following mechanisms: 1) ventral ( what ) stream focusing on the input regions perceived by the fovea part of an eye (foveation); 2) dorsal ( where ) stream providing visual guidance; and 3) iterative processing of the two streams to calibrate visual focus and process the sequence of focused image patches. The training of the proposed framework is accomplished by label-based DNN training for the ventral stream model and reinforcement learning (RL) for the dorsal stream model. We show that the two-stream foveation-based learning is applicable to the challenging task of weakly-supervised object localization (WSOL), where the training data is limited to the object class or its attributes. The framework is capable of both predicting the properties of an object and successfully localizing it by predicting its bounding box. We also show that, due to the independent nature of the two streams, the dorsal model can be applied on its own to unseen images to localize objects from different datasets.},
  archive      = {J_TCDS},
  author       = {Timur Ibrayev and Amitangshu Mukherjee and Sai Aparna Aketi and Kaushik Roy},
  doi          = {10.1109/TCDS.2024.3390597},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1843-1860},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Toward two-stream foveation-based active vision learning},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GENet: A generic neural network for detecting various
neurological disorders from EEG. <em>TCDS</em>, <em>16</em>(5),
1829–1842. (<a href="https://doi.org/10.1109/TCDS.2024.3386364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The global health burden of neurological disorders (NDs) is vast, and they are recognized as major causes of mortality and disability worldwide. Most existing NDs detection methods are disease-specific, which limits an algorithm&#39;s cross-disease applicability. A single diagnostic platform can save time and money over multiple diagnostic systems. There is currently no unified standard platform for diagnosing different types of NDs utilizing electroencephalogram (EEG) signal data. To address this issue, this study aims to develop a generic EEG neural Network (GENet) framework based on a convolutional neural network that can identify various NDs from EEG. The proposed framework consists of several parts: 1) preparing data using channel reduction, resampling, and segmentation for the GENet model; 2) designing and training the GENet model to carry out important features for the classification task; and 3) assessing the proposed model&#39;s performance using different signal segment lengths and several training batch sizes and also cross-validating using seven different EEG datasets of six distinct NDs namely schizophrenia, autism spectrum disorder, epilepsy, Parkinson&#39;s disease, mild cognitive impairment, and attention-deficit/hyperactivity disorder. In addition, this study also investigates whether the proposed GENet model can identify multiple NDs from EEG. The proposed model achieved much better performance for both binary and multiclass classification compared to state-of-the-art methods. In addition, the proposed model is validated using several ablation studies and layerwise feature visualization, which provide consistency and efficiency to the proposed model. The proposed GENet model will help technologists create standard software for detecting any of these NDs from EEG.},
  archive      = {J_TCDS},
  author       = {Md. Nurul Ahad Tawhid and Siuly Siuly and Kate Wang and Hua Wang},
  doi          = {10.1109/TCDS.2024.3386364},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1829-1842},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {GENet: A generic neural network for detecting various neurological disorders from EEG},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatiotemporal feature enhancement network for blur robust
underwater object detection. <em>TCDS</em>, <em>16</em>(5), 1814–1828.
(<a href="https://doi.org/10.1109/TCDS.2024.3386664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater object detection is challenged by the presence of image blur induced by light absorption and scattering, resulting in substantial performance degradation. It is hypothesized that the attenuation of light is directly correlated with the camera-to-object distance, manifesting as variable degrees of image blur across different regions within underwater images. Specifically, regions in close proximity to the camera exhibit less pronounced blur compared to distant regions. Within the same object category, objects situated in clear regions share similar feature embeddings with their counterparts in blurred regions. This observation underscores the potential for leveraging objects in clear regions to aid in the detection of objects within blurred areas, a critical requirement for autonomous agents, such as autonomous underwater vehicles, engaged in continuous underwater object detection. Motivated by this insight, we introduce the spatiotemporal feature enhancement network (STFEN), a novel framework engineered to autonomously extract discriminative features from objects in clear regions. These features are then harnessed to enhance the representations of objects in blurred regions, operating across both spatial and temporal dimensions. Notably, the proposed STFEN seamlessly integrates into two-stage detectors, such as the faster region-based convolutional neural networks (Faster R-CNN) and feature pyramid networks (FPN). Extensive experimentation conducted on two benchmark underwater datasets, URPC 2018 and URPC 2019, conclusively demonstrates the efficacy of the STFEN framework. It delivers substantial enhancements in performance relative to baseline methods, yielding improvements in the mAP evaluation metric ranging from 3.7% to 5.0%.},
  archive      = {J_TCDS},
  author       = {Hao Zhou and Lu Qi and Hai Huang and Xu Yang and Jing Yang},
  doi          = {10.1109/TCDS.2024.3386664},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1814-1828},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Spatiotemporal feature enhancement network for blur robust underwater object detection},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning to interpret autism spectrum disorder behind
the camera. <em>TCDS</em>, <em>16</em>(5), 1803–1813. (<a
href="https://doi.org/10.1109/TCDS.2024.3386656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is growing interest in understanding the visual behavioral patterns of individuals with autism spectrum disorder (ASD) based on their attentional preferences. Attention reveals the cognitive or perceptual variation in ASD and can serve as a biomarker to assist diagnosis and intervention. The development of machine learning methods for attention-based ASD screening shows promises, yet it has been limited by the need for high-precision eye trackers, the scope of stimuli, and black-box neural networks, making it impractical for real-life clinical scenarios. This study proposes an interpretable and generalizable framework for quantifying atypical attention in people with ASD. Our framework utilizes photos taken by participants with standard cameras to enable practical and flexible deployment in resource-constrained regions. With an emphasis on interpretability and trustworthiness, our method automates human-like diagnostic reasoning, associates photos with semantically plausible attention patterns, and provides clinical evidence to support ASD experts. We further evaluate models on both in-domain and out-of-domain data and demonstrate that our approach accurately classifies individuals with ASD and generalizes across different domains. The proposed method offers an innovative, reliable, and cost-effective tool to assist the diagnostic procedure, which can be an important effort toward transforming clinical research in ASD screening with artificial intelligence systems. Our code is publicly available at https://github.com/szzexpoi/proto_asd .},
  archive      = {J_TCDS},
  author       = {Shi Chen and Ming Jiang and Qi Zhao},
  doi          = {10.1109/TCDS.2024.3386656},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1803-1813},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Deep learning to interpret autism spectrum disorder behind the camera},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient semisupervised object segmentation for long-term
videos using adaptive memory network. <em>TCDS</em>, <em>16</em>(5),
1789–1802. (<a href="https://doi.org/10.1109/TCDS.2024.3385849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video object segmentation (VOS) uses the first annotated video mask to achieve consistent and precise segmentation in subsequent frames. Recently, memory-based methods have received significant attention owing to their substantial performance enhancements. However, these approaches rely on a fixed global memory strategy, which poses a challenge to segmentation accuracy and speed in the context of longer videos. To alleviate this limitation, we propose a novel semisupervised VOS model, founded on the principles of the adaptive memory network. Our proposed model adaptively extracts object features by focusing on the object area while effectively filtering out extraneous background noise. An identification mechanism is also thoughtfully applied to discern each object in multiobject scenarios. To further reduce storage consumption without compromising the saliency of object information, the outdated features residing in the memory pool are compressed into salient features through the employment of a self-attention mechanism. Furthermore, we introduce a local matching module, specifically devised to refine object features by fusing the contextual information from historical frames. We demonstrate the efficiency of our approach through experiments, substantially augmenting both the speed and precision of segmentation for long-term videos, while maintaining comparable performance for short videos.},
  archive      = {J_TCDS},
  author       = {Shan Zhong and Guoqiang Li and Wenhao Ying and Fuzhou Zhao and Gengsheng Xie and Shengrong Gong},
  doi          = {10.1109/TCDS.2024.3385849},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1789-1802},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Efficient semisupervised object segmentation for long-term videos using adaptive memory network},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DatUS: Data-driven unsupervised semantic segmentation with
pretrained self-supervised vision transformer. <em>TCDS</em>,
<em>16</em>(5), 1775–1788. (<a
href="https://doi.org/10.1109/TCDS.2024.3383952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Successive proposals of several self-supervised training schemes (STSs) continue to emerge, taking one step closer to developing a universal foundation model. In this process, unsupervised downstream tasks are recognized as one of the evaluation methods to validate the quality of visual features learned with self-supervised training. However, unsupervised dense semantic segmentation has yet to be explored as a downstream task, which can utilize and evaluate the quality of semantic information introduced in patch-level feature representations during self-supervised training of vision transformers. Therefore, we propose a novel data-driven framework, DatUS, to perform unsupervised dense semantic segmentation (DSS) as a downstream task. DatUS generates semantically consistent pseudosegmentation masks for an unlabeled image dataset without using visual prior or synchronized data. The experiment shows that the proposed framework achieves the highest MIoU (24.90) and average F1 score (36.3) by choosing DINOv2 and the highest pixel accuracy (62.18) by choosing DINO as the STS on the training set of SUIM dataset. It also outperforms state-of-the-art methods for the unsupervised DSS task with 15.02% MIoU, 21.47% pixel accuracy, and 16.06% average F1 score on the validation set of SUIM dataset. It achieves a competitive level of accuracy for a large-scale COCO dataset.},
  archive      = {J_TCDS},
  author       = {Sonal Kumar and Arijit Sur and Rashmi Dutta Baruah},
  doi          = {10.1109/TCDS.2024.3383952},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1775-1788},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {DatUS: Data-driven unsupervised semantic segmentation with pretrained self-supervised vision transformer},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep-reinforcement-learning-based driving policy at
intersections utilizing lane graph networks. <em>TCDS</em>,
<em>16</em>(5), 1759–1774. (<a
href="https://doi.org/10.1109/TCDS.2024.3384269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning an efficient and safe driving strategy in a traffic-heavy intersection scenario and generalizing it to different intersections remains a challenging task for autonomous driving. This is because there are differences in the structure of roads at different intersections, and autonomous vehicles need to generalize the strategies they have learned in the training environments. This requires the autonomous vehicle to capture not only the interactions between agents but also the relationships between agents and the map effectively. To address this challenge, we present a technique that integrates the information of high-definition (HD) maps and traffic participants into vector representations, called lane graph vectorization (LGV). In order to construct a driving policy for intersection navigation, we incorporate LGV into the twin-delayed deep deterministic policy gradient (TD3) algorithm with prioritized experience replay (PER). To train and validate the proposed algorithm, we construct a gym environment for intersection navigation within the high-fidelity CARLA simulator, integrating dense interactive traffic flow and various generalization test intersection scenarios. Experimental results demonstrate the effectiveness of LGV for intersection navigation tasks and outperform the state-of-the-art in our proposed scenarios.},
  archive      = {J_TCDS},
  author       = {Yuqi Liu and Qichao Zhang and Yinfeng Gao and Dongbin Zhao},
  doi          = {10.1109/TCDS.2024.3384269},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1759-1774},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Deep-reinforcement-learning-based driving policy at intersections utilizing lane graph networks},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Measuring human comfort in human–robot collaboration via
wearable sensing. <em>TCDS</em>, <em>16</em>(5), 1748–1758. (<a
href="https://doi.org/10.1109/TCDS.2024.3383296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of collaborative robots has enabled a safer and more efficient human–robot collaboration (HRC) manufacturing environment. Tremendous research efforts have been conducted to improve user safety and robot working efficiency after the debut of collaborative robots. However, human comfort in HRC scenarios has not been thoroughly discussed but is critically important to the user acceptance of collaborative robots. Previous studies mostly utilize the subjective rating method to evaluate how human comfort varies as one robot factor changes, yet such method is limited in evaluating comfort online. Some other studies leverage wearable sensors to collect physiological signals to detect human emotions, but few of them implement this for a human comfort model in HRC scenarios. In this study, we designed an online comfort model for HRC using wearable sensing data. The model uses physiological signals acquired from wearable sensing and calculates the in-situ human comfort levels based on our developed algorithms. We have conducted experiments in realistic HRC tasks, and the prediction results demonstrated the effectiveness of the proposed approach in identifying human comfort levels in HRC.},
  archive      = {J_TCDS},
  author       = {Yuchen Yan and Haotian Su and Yunyi Jia},
  doi          = {10.1109/TCDS.2024.3383296},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1748-1758},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Measuring human comfort in Human–Robot collaboration via wearable sensing},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BitSNNs: Revisiting energy-efficient spiking neural
networks. <em>TCDS</em>, <em>16</em>(5), 1736–1747. (<a
href="https://doi.org/10.1109/TCDS.2024.3383428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the energy bottleneck in deep neural networks (DNNs), the research community has developed binary neural networks (BNNs) and spiking neural networks (SNNs) from different perspectives. To combine the advantages of both BNNs and SNNs for better energy efficiency, this article proposes BitSNNs, which leverage binary weights, single-step inference, and activation sparsity. During the development of BitSNNs, we observed performance degradation in deep ResNets due to the gradient approximation error. To mitigate this issue, we delve into the learning process and propose the utilization of a hardtanh function before activation binarization. Additionally, this article investigates the critical role of activation sparsity in BitSNNs for energy efficiency, a topic often overlooked in the existing literature. Our study reveals strategies to strike a balance between accuracy and energy consumption during the training/testing stage, potentially benefiting applications in edge computing. Notably, our proposed method achieves state-of-the-art performance while significantly reducing energy consumption.},
  archive      = {J_TCDS},
  author       = {Yangfan Hu and Qian Zheng and Gang Pan},
  doi          = {10.1109/TCDS.2024.3383428},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1736-1747},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {BitSNNs: Revisiting energy-efficient spiking neural networks},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Memristive circuit design for personalized emotion
generation with memory and retrieval functions. <em>TCDS</em>,
<em>16</em>(5), 1722–1735. (<a
href="https://doi.org/10.1109/TCDS.2023.3317066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generation of human emotion is often influenced by memory and personality. However, few reported circuits of emotion generation have considered this view. This work proposes a memristive emotion generation circuit with the functions of emotional memory and retrieval, and personality traits deployment. It mimics the emotional circuit of the brain and consists of six modules, which are input module, sensory cortex module, amygdala module, orbitofrontal module, hippocampus module, and personality module. When inputting multimodal signals, the proposed design can not only generate 2-D continuous emotions but also perform emotional memory and retrieval. Meanwhile, it provides various personality traits to mimic individual differences amid emotion generation based on the Big Five model. All in all, the designed memristive circuit provides a brain-like hierarchical and parallel emotion-generating method. It offers advantages in lower power consumption, area, and in-memory processing. In addition, the nonideality analysis shows the circuit’s robustness. This work could provide a reference for the implementation of human-like emotional robots.},
  archive      = {J_TCDS},
  author       = {Zhanfei Chen and Xiaoping Wang and Chao Yang and Zilu Wang and Zhigang Zeng},
  doi          = {10.1109/TCDS.2023.3317066},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1722-1735},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Memristive circuit design for personalized emotion generation with memory and retrieval functions},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Brain-inspired model and neuromorphic circuit implementation
for feature-affective associative memory network. <em>TCDS</em>,
<em>16</em>(5), 1707–1721. (<a
href="https://doi.org/10.1109/TCDS.2023.3329044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Affective associative memory is one method by which agents acquire knowledge, experience, and skills from natural surroundings or social activities. Using neuromorphic circuits to implement affective associative memory aids in developing brain-inspired intelligence. In this article, a feature-affective associative memory (FAAM) network model and its memristive circuit are proposed for real-time and mutual associative memory and retrieval between multiple features and emotions. With the context of fear conditioning, FAAM network circuit is verified to enable the acquisition and extinction of associations. Different from other works, the proposed temporal-rate mixed coding circuit encodes stimulus intensity and arousal level as different pulses, allowing the associative learning rate and emotion degree can vary with stimulus intensity and arousal level. Furthermore, the bidirectional and multifeature-to-multiemotion association model allows the circuit to be extended to associative memory network containing 10 neurons and 90 synapses, with capabilities such as emotion generation and modulation, associative generalization and differentiation, which are applied to feature binding, situational memory, and inference decision. This work enables advanced cognitive functions and is expected to enable intelligent robot platforms for real-time learning, reasoning decisions, and emotional companionship in dynamic environments.},
  archive      = {J_TCDS},
  author       = {Yutong Zhang and Zhigang Zeng},
  doi          = {10.1109/TCDS.2023.3329044},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1707-1721},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Brain-inspired model and neuromorphic circuit implementation for feature-affective associative memory network},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EEG-based auditory attention detection with spiking graph
convolutional network. <em>TCDS</em>, <em>16</em>(5), 1698–1706. (<a
href="https://doi.org/10.1109/TCDS.2024.3376433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decoding auditory attention from brain activities, such as electroencephalography (EEG), sheds light on solving the machine cocktail party problem. However, effective representation of EEG signals remains a challenge. One of the reasons is that the current feature extraction techniques have not fully exploited the spatial information along the EEG signals. EEG signals reflect the collective dynamics of brain activities across different regions. The intricate interactions among these channels, rather than individual EEG channels alone, reflect the distinctive features of brain activities. In this study, we propose a spiking graph convolutional network (SGCN), which captures the spatial features of multichannel EEG in a biologically plausible manner. Comprehensive experiments were conducted on two publicly available datasets. Results demonstrate that the proposed SGCN achieves competitive auditory attention detection (AAD) performance in low-latency and low-density EEG settings. As it features low power consumption, the SGCN has the potential for practical implementation in intelligent hearing aids and other brain–computer interfaces (BCIs).},
  archive      = {J_TCDS},
  author       = {Siqi Cai and Ran Zhang and Malu Zhang and Jibin Wu and Haizhou Li},
  doi          = {10.1109/TCDS.2024.3376433},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1698-1706},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {EEG-based auditory attention detection with spiking graph convolutional network},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving graph collaborative filtering via spike signal
embedding perturbation. <em>TCDS</em>, <em>16</em>(5), 1688–1697. (<a
href="https://doi.org/10.1109/TCDS.2023.3338614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, graph collaborative filtering has proven to be a highly effective method in recommendation systems. It learns user preferences through interactions between users and items. During the training process of graph collaborative filtering, introducing suitable perturbations is crucial to model training. It is commonly used to prevent overfitting and enhance model robustness. Perturbation is widely adopted as a data augmentation technique in recommendation systems and extensively used in contrastive learning. Contrastive learning involves multitask learning aimed at learning various views from diverse data augmentations. However, these tasks can sometimes interfere with each other, impacting their effectiveness. In contrast to methods that focus on learning various views in contrastive learning to achieve better embedding representations, we propose a straightforward yet highly effective approach to directly incorporate spike signal embedding perturbation (SEP) into the graph collaborative filtering process. Unlike many other approaches that introduce Gaussian-distributed noise, the spike signals generated by the Poisson encoder typically maintain a positive relationship with the original embeddings. Our experimental results demonstrate that this proposed method significantly enhances the performance of graph collaborative filtering when compared to LightGCN. It leads to substantial improvements in recommendation performance and training efficiency.},
  archive      = {J_TCDS},
  author       = {Ying Ma and Gang Chen and Guoqi Li},
  doi          = {10.1109/TCDS.2023.3338614},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1688-1697},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Improving graph collaborative filtering via spike signal embedding perturbation},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Closed-form control with spike coding networks.
<em>TCDS</em>, <em>16</em>(5), 1677–1687. (<a
href="https://doi.org/10.1109/TCDS.2023.3320251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient and robust control using spiking neural networks (SNNs) is still an open problem. Whilst behavior of biological agents is produced through sparse and irregular spiking patterns, which provide both robust and efficient control, the activity patterns in most artificial spiking neural networks used for control are dense and regular—resulting in potentially less efficient codes. Additionally, for most existing control solutions network training or optimization is necessary, even for fully identified systems, complicating their implementation in on-chip low-power solutions. The neuroscience theory of spike coding networks (SCNs) offers a fully analytical solution for implementing dynamical systems in recurrent spiking neural networks—while maintaining irregular, sparse, and robust spiking activity—but it is not clear how to directly apply it to control problems. Here, we extend SCN theory by incorporating closedform optimal estimation and control. The resulting networks work as a spiking equivalent of a linear-quadratic-Gaussian controller. We demonstrate robust spiking control of simulated spring-mass-damper and cart-pole systems, in the face of several perturbations, including input- and system-noise, system disturbances, and neural silencing. As our approach does not need learning or optimization, it offers opportunities for deploying fast and efficient task-specific on-chip spiking controllers with biologically realistic activity.},
  archive      = {J_TCDS},
  author       = {Filip S. Slijkhuis and Sander W. Keemink and Pablo Lanillos},
  doi          = {10.1109/TCDS.2023.3320251},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1677-1687},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Closed-form control with spike coding networks},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SIBoLS: Robust and energy-efficient learning for spike-based
machine intelligence in information bottleneck framework. <em>TCDS</em>,
<em>16</em>(5), 1664–1676. (<a
href="https://doi.org/10.1109/TCDS.2023.3329532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spike-based machine intelligence has recently attracted increasing research attention and has been considered as a promising approach toward artificial general intelligence (AGI). It has been applied in energy-efficient neuromorphic computing systems. One of the most critical questions for spike-based learning is how to leverage powerful and promising information theoretic learning theories to derive learning algorithms for improving the robustness and energy efficiency of spiking neural networks (SNNs). In this study, we first present an efficient and effective information bottleneck (IB) framework for training SNN, named Spike-based IB with Learnable State (SIBoLS). We thoroughly explore the design space concerning IB framework by using the membrane potential state for the hidden information representation with learnable hidden variable. Comprehensive robustness test is conducted, in which two types of neuromorphic background noise and five types of input noise are considered. It shows SIBoLS can improve the learning robustness for both static image and event-based dataset of neuromorphic processor. Furthermore, SIBoLS induces less spiking rate, resulting in lower power consumption compared to other techniques. SIBoLS has advantages in terms of robustness and energy efficiency for spike-based machine learning applications, which can give insights for the development of AGI.},
  archive      = {J_TCDS},
  author       = {Shuangming Yang and Haowen Wang and Badong Chen},
  doi          = {10.1109/TCDS.2023.3329532},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1664-1676},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {SIBoLS: Robust and energy-efficient learning for spike-based machine intelligence in information bottleneck framework},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unlocking the potential of spiking neural networks:
Understanding the what, why, and where. <em>TCDS</em>, <em>16</em>(5),
1648–1663. (<a href="https://doi.org/10.1109/TCDS.2023.3329747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) are a promising avenue for machine learning with superior energy efficiency compared to traditional artificial neural networks (ANNs). Recent advances in training and input encoding have put SNNs on par with state-of-the-art ANNs in image classification. However, such tasks do not utilize the internal dynamics of SNNs fully. Notably, a spiking neuron&#39;s membrane potential acts as an internal memory, merging incoming inputs sequentially. This recurrent dynamic enables the networks to learn temporal correlations, making SNNs suitable for sequential learning. Such problems can also be tackled using ANNs. However, to capture the temporal dependencies, either the inputs have to be lumped over time (e.g., Transformers); or explicit recurrence needs to be introduced [e.g., recurrent neural networks (RNNs) and long-short-term memory (LSTM) networks], which incurs considerable complexity. To that end, we explore the capabilities of SNNs in providing lightweight solutions to four sequential tasks involving text, speech, and vision. Our results demonstrate that SNNs, by leveraging their intrinsic memory, can be an efficient alternative to RNNs and LSTMs for sequence processing, especially for certain edge applications. Furthermore, SNNs can be combined with ANNs (hybrid networks) synergistically to obtain the best of both worlds in terms of accuracy and efficiency.},
  archive      = {J_TCDS},
  author       = {Buddhi Wickramasinghe and Sayeed Shafayet Chowdhury and Adarsh Kumar Kosta and Wachirawit Ponghiran and Kaushik Roy},
  doi          = {10.1109/TCDS.2023.3329747},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1648-1663},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Unlocking the potential of spiking neural networks: Understanding the what, why, and where},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Asymmetric spatiotemporal online learning for deep spiking
neural networks. <em>TCDS</em>, <em>16</em>(5), 1640–1647. (<a
href="https://doi.org/10.1109/TCDS.2023.3278720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the precise symmetric forward and feedback connections between neurons are thought to be impossible in the brain, most existing deep spiking neural networks (SNNs) spatiotemporal learning algorithms still require such a strong architectural constraint to complete tasks. Besides that, as an expected feature for specialized neuromorphic hardware to be deployed, effective online learning of deep SNNs for training spatiotemporal data is still lacking. To address these issues, an effective biologically plausible asymmetric spatiotemporal online learning algorithm called ASTOL is proposed for training deep SNNs in real time. ASTOL could learn the spatial and temporal features simultaneously in real time, without the precise weights symmetric backpropagation and the need to know the duration of spatiotemporal data in advance. Experimental results show that the proposed ASTOL algorithm achieves comparable performance in real-time learning on the rate coding MNIST data set and the temporal coding music MedleyDB data set and speech TIDIGITS data set with other state-of-the-art algorithms with a simpler learning mechanism way as it avoids transport of synaptic weights. Besides that, the experiment on MNIST data sets also shows that ASTOL can use simpler network structures to achieve good performance faster.},
  archive      = {J_TCDS},
  author       = {Rong Xiao and Limiao Ning and Yixuan Wang and Huajun Du and Sen Wang and Rui Yan},
  doi          = {10.1109/TCDS.2023.3278720},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1640-1647},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Asymmetric spatiotemporal online learning for deep spiking neural networks},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LC-TTFS: Toward lossless network conversion for spiking
neural networks with TTFS coding. <em>TCDS</em>, <em>16</em>(5),
1626–1639. (<a href="https://doi.org/10.1109/TCDS.2023.3334010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The biological neurons use precise spike times, in addition to the spike firing rate, to communicate with each other. The time-to-first-spike (TTFS) coding is inspired by such biological observation. However, there is a lack of effective solutions for training TTFS-based spiking neural network (SNN). In this article, we put forward a simple yet effective network conversion algorithm, which is referred to as lossless conversion (LC)-TTFS, by addressing two main problems that hinder an effective conversion from a high-performance artificial neural network (ANN) to a TTFS-based SNN. We show that our algorithm can achieve a near-perfect mapping between the activation values of an ANN and the spike times of an SNN on a number of challenging AI tasks, including image classification, image reconstruction, and speech enhancement. With TTFS coding, we can achieve up to orders of magnitude saving in computation over ANN and other rate-based SNNs. The study, therefore, paves the way for deploying ultralow-power TTFS-based SNNs on power-constrained edge computing platforms.},
  archive      = {J_TCDS},
  author       = {Qu Yang and Malu Zhang and Jibin Wu and Kay Chen Tan and Haizhou Li},
  doi          = {10.1109/TCDS.2023.3334010},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1626-1639},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {LC-TTFS: Toward lossless network conversion for spiking neural networks with TTFS coding},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guest editorial: Special issue on advancing machine
intelligence with neuromorphic computing. <em>TCDS</em>, <em>16</em>(5),
1623–1625. (<a href="https://doi.org/10.1109/TCDS.2024.3458488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TCDS},
  author       = {Guoqi Li and Emre Neftci and Rong Xiao and Pablo Lanillos and Kaushik Roy},
  doi          = {10.1109/TCDS.2024.3458488},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1623-1625},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Guest editorial: Special issue on advancing machine intelligence with neuromorphic computing},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MAT: Morphological adaptive transformer for universal
morphology policy learning. <em>TCDS</em>, <em>16</em>(4), 1611–1621.
(<a href="https://doi.org/10.1109/TCDS.2024.3383158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agent-agnostic reinforcement learning aims to learn a universal control policy that can simultaneously control a set of robots with different morphologies. Recent studies have suggested that using the transformer model can address variations in state and action spaces caused by different morphologies, and morphology information is necessary to improve policy performance. However, existing methods have limitations in exploiting morphological information, where the rationality of observation integration cannot be guaranteed. We propose morphological adaptive transformer (MAT), a transformer-based universal control algorithm that can adapt to various morphologies without any modifications. MAT includes two essential components: functional position encoding (FPE) and morphological attention mechanism (MAM). The FPE provides robust and consistent positional prior information for limb observation to avoid limb confusion and implicitly obtain functional descriptions of limbs. The MAM enhances the attribute prior information of limbs, improves the correlation between observations, and makes the policy pay attention to more limbs. We combine observation with prior information to help policy adapt to the morphology of robots, thereby optimizing its performance with unknown morphologies. Experiments on agent-agnostic tasks in Gym MuJoCo environment demonstrate that our algorithm can assign more reasonable morphological prior information to each limb, and the performance of our algorithm is comparable to the prior state-of-the-art algorithm with better generalization.},
  archive      = {J_TCDS},
  author       = {Boyu Li and Haoran Li and Yuanheng Zhu and Dongbin Zhao},
  doi          = {10.1109/TCDS.2024.3383158},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1611-1621},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {MAT: Morphological adaptive transformer for universal morphology policy learning},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attention mechanism and out-of-distribution data on cross
language image matching for weakly supervised semantic segmentation.
<em>TCDS</em>, <em>16</em>(4), 1604–1610. (<a
href="https://doi.org/10.1109/TCDS.2024.3382914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fully supervised semantic segmentation requires detailed annotation of each pixel, which is time-consuming and laborious at the pixel-by-pixel level. To solve this problem, the direction of this article is to perform the semantic segmentation task by using image-level categorical annotation. Existing methods using image level annotation usually use class activation maps (CAMs) to find the location of the target object as the first step. By training a classifier, the presence of objects in the image can be searched effectively. However, CAMs appear that as follows: 1) objects are excessively focused on specific regions, capturing only the most prominent and critical areas and 2) it is easy to misinterpret the frequently occurring background regions, the foreground and background are confused. This article introduces cross language image matching based on out-of-distribution data and convolutional block attention module (CLODA), the concept of double branching in the cross language image matching framework, and adds a convolutional attention module to the attention branch to solve the problem of excess focus on objects in the CAMs. Importing out-of-distribution data on out of distribution branches helps classification networks improve misinterpretation of areas of focus. Optimizing regions of interest for attentional branch learning using cross pseudosupervision on two branches. Experimental results show that the pseudomasks generated by the proposed network can achieve 75.3% in mean Intersection over Union (mIoU) with the pattern analysis, statistical modeling and computational learning visual object classes (PASCAL VOC) 2012 training set. The performance of the segmentation network trained with the pseudomasks is up to 72.3% and 72.1% in mIoU on the validation and testing set of PASCAL VOC 2012.},
  archive      = {J_TCDS},
  author       = {Chi-Chia Sun and Jing-Ming Guo and Chen-Hung Chung and Bo-Yu Chen},
  doi          = {10.1109/TCDS.2024.3382914},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1604-1610},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Attention mechanism and out-of-distribution data on cross language image matching for weakly supervised semantic segmentation},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep neural networks for automatic sleep stage
classification and consciousness assessment in patients with disorder of
consciousness. <em>TCDS</em>, <em>16</em>(4), 1589–1603. (<a
href="https://doi.org/10.1109/TCDS.2024.3382109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disorders of consciousness (DOC) are often related to serious changes in sleep structure. This article presents a sleep evaluation algorithm that scores the sleep structure of DOC patients to assist in assessing their consciousness level. The sleep evaluation algorithm is divided into two parts: 1) automatic sleep staging model: convolutional neural networks (CNNs) are employed for the extraction of signal features from electroencephalogram (EEG) and electrooculogram (EOG), and bidirectional long short-term memory (Bi-LSTM) with attention mechanism is applied to learn sequential information; and 2) consciousness assessment: automated sleep staging results are used to extract consciousness-related sleep features that are utilized by a support vector machine (SVM) classifier to assess consciousness. In this study, the CNN-BiLSTM model with an attention sleep network (CBASleepNet) was conducted using the sleep-EDF and MASS datasets. The experimental results demonstrated the effectiveness of the proposed model, which outperformed similar models. Moreover, CBASleepNet was applied to sleep staging in DOC patients through transfer learning and fine-tuning. Consciousness assessments were conducted on seven minimally conscious state (MCS) patients and four vegetative state (VS)/unresponsive wakefulness syndrome (UWS) patients, achieving an overall accuracy of 81.8%. The sleep evaluation algorithm can be used to evaluate the consciousness level of patients effectively.},
  archive      = {J_TCDS},
  author       = {Jiahui Pan and Yangzuyi Yu and Jianhui Wu and Xinjie Zhou and Yanbin He and Yuanqing Li},
  doi          = {10.1109/TCDS.2024.3382109},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1589-1603},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Deep neural networks for automatic sleep stage classification and consciousness assessment in patients with disorder of consciousness},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A two-stage foveal vision tracker based on transformer
model. <em>TCDS</em>, <em>16</em>(4), 1575–1588. (<a
href="https://doi.org/10.1109/TCDS.2024.3377642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of transformer visual models, attention-based trackers have shown highly competitive performance in the field of object tracking. However, in some tracking scenarios, especially those with multiple similar objects, the performance of existing trackers is often not satisfactory. In order to improve the performance of trackers in such scenarios, inspired by the fovea vision structure and its visual characteristics, this article proposes a novel foveal vision tracker (FVT). FVT combines the process of human eye fixation and object tracking, pruning based on the distance to the object rather than attention scores. This pruning method allows the receptive field of the feature extraction network to focus on the object, excluding background interference. FVT divides the feature extraction network into two stages: local and global, and introduces the local recursive module (LRM) and the view elimination module (VEM). LRM is used to enhance foreground features in the local stage, while VEM generates circular fovea-like visual field masks in the global stage and prunes tokens outside the mask, guiding the model to focus attention on high-information regions of the object. Experimental results on multiple object tracking datasets demonstrate that the proposed FVT achieves stronger object discrimination capability in the feature extraction stage, improves tracking accuracy and robustness in complex scenes, and achieves a significant accuracy improvement with an area overlap (AO) of 72.6% on the generic object tracking (GOT)-10k dataset.},
  archive      = {J_TCDS},
  author       = {Guang Han and Jianshu Ma and Ziyang Li and Haitao Zhao},
  doi          = {10.1109/TCDS.2024.3377642},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1575-1588},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A two-stage foveal vision tracker based on transformer model},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The inadequacy of reinforcement learning from human
feedback—radicalizing large language models via semantic
vulnerabilities. <em>TCDS</em>, <em>16</em>(4), 1561–1574. (<a
href="https://doi.org/10.1109/TCDS.2024.3377445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study is an empirical investigation into the semantic vulnerabilities of four popular pretrained commercial large language models (LLMs) to ideological manipulation. Using tactics reminiscent of human semantic conditioning in psychology, we have induced and assessed ideological misalignments and their retention in four commercial pretrained LLMs, in response to 30 controversial questions that spanned a broad ideological and social spectrum, encompassing both extreme left- and right-wing viewpoints. Such semantic vulnerabilities arise due to fundamental limitations in LLMs’ capability to comprehend detailed linguistic variations, making them susceptible to ideological manipulation through targeted semantic exploits. We observed reinforcement learning from human feedback (RLHF) in effect to LLM initial answers, but highlighted the limitations of RLHF in two aspects: 1) its inability to fully mitigate the impact of ideological conditioning prompts, leading to partial alleviation of LLM semantic vulnerabilities; and 2) its inadequacy in representing a diverse set of “human values,” often reflecting the predefined values of certain groups controlling the LLMs. Our findings have provided empirical evidence of semantic vulnerabilities inherent in current LLMs, challenged both the robustness and the adequacy of RLHF as a mainstream method for aligning LLMs with human values, and underscored the need for a multidisciplinary approach in developing ethical and resilient artificial intelligence (AI).},
  archive      = {J_TCDS},
  author       = {Timothy R. McIntosh and Teo Susnjak and Tong Liu and Paul Watters and Malka N. Halgamuge},
  doi          = {10.1109/TCDS.2024.3377445},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1561-1574},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {The inadequacy of reinforcement learning from human Feedback—Radicalizing large language models via semantic vulnerabilities},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Emergence of human oculomotor behavior in a cable-driven
biomimetic robotic eye using optimal control. <em>TCDS</em>,
<em>16</em>(4), 1546–1560. (<a
href="https://doi.org/10.1109/TCDS.2024.3376072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article explores the application of model-based optimal control principles in understanding stereotyped human oculomotor behaviors. Using a realistic model of the human eye with a six-muscle cable-driven actuation system, we tackle the novel challenges of addressing a system with six degrees of freedom. We apply nonlinear optimal control techniques to optimize accuracy, energy, and duration of eye-movement trajectories. Employing a recurrent neural network to emulate system dynamics, we focus on generating rapid, unconstrained saccadic eye-movements. Remarkably, our model replicates realistic 3-D rotational kinematics and dynamics observed in human saccades, with the six cables organizing themselves into appropriate antagonistic muscle pairs, resembling the primate oculomotor system.},
  archive      = {J_TCDS},
  author       = {Reza Javanmard Alitappeh and Akhil John and Bernardo Dias and A. John van Opstal and Alexandre Bernardino},
  doi          = {10.1109/TCDS.2024.3376072},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1546-1560},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Emergence of human oculomotor behavior in a cable-driven biomimetic robotic eye using optimal control},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Converting artificial neural networks to ultralow-latency
spiking neural networks for action recognition. <em>TCDS</em>,
<em>16</em>(4), 1533–1545. (<a
href="https://doi.org/10.1109/TCDS.2024.3375620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) have garnered significant attention for their potential in ultralow-power event-driven neuromorphic hardware implementations. One effective strategy for obtaining SNNs involves the conversion of artificial neural networks (ANNs) to SNNs. However, existing research on ANN–SNN conversion has predominantly focused on image classification task, leaving the exploration of action recognition task limited. In this article, we investigate the performance degradation of SNNs on action recognition task. Through in-depth analysis, we propose a framework called scalable dual threshold mapping (SDM) that effectively overcomes three types of conversion errors. By effectively mitigating these conversion errors, we are able to reduce the time required for the spike firing rate of SNNs to align with the activation values of ANNs. Consequently, our method enables the generation of accurate and ultralow-latency SNNs. We conduct extensive evaluations on multiple action recognition datasets, including University of Central Florida (UCF)-101 and Human Motion DataBase (HMDB)-51. Through rigorous experiments and analysis, we demonstrate the effectiveness of our approach. Notably, SDM achieves a remarkable Top-1 accuracy of 92.94% on UCF-101 while requiring ultralow latency (four time steps), highlighting its high performance with reduced computational requirements.},
  archive      = {J_TCDS},
  author       = {Hong You and Xian Zhong and Wenxuan Liu and Qi Wei and Wenxin Huang and Zhaofei Yu and Tiejun Huang},
  doi          = {10.1109/TCDS.2024.3375620},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1533-1545},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Converting artificial neural networks to ultralow-latency spiking neural networks for action recognition},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EventAugment: Learning augmentation policies from
asynchronous event-based data. <em>TCDS</em>, <em>16</em>(4), 1521–1532.
(<a href="https://doi.org/10.1109/TCDS.2024.3380907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation is an effective way to overcome the overfitting problem of deep learning models. However, most existing studies on data augmentation work on framelike data (e.g., images), and few tackles with event-based data. Event-based data are different from framelike data, rendering the augmentation techniques designed for framelike data unsuitable for event-based data. This work deals with data augmentation for event-based object classification and semantic segmentation, which is important for self-driving and robot manipulation. Specifically, we introduce EventAugment, a new method to augment asynchronous event-based data by automatically learning augmentation policies. We first identify 13 types of operations for augmenting event-based data. Next, we formulate the problem of finding optimal augmentation policies as a hyperparameter optimization problem. To tackle this problem, we propose a random search-based framework. Finally, we evaluate the proposed method on six public datasets including N-Caltech101, N-Cars, ST-MNIST, N-MNIST, DVSGesture, and DDD17. Experimental results demonstrate that EventAugment exhibits substantial performance improvements for both deep neural network-based and spiking neural network-based models, with gains of up to approximately 4%. Notably, EventAugment outperform state-of-the-art methods in terms of overall performance.},
  archive      = {J_TCDS},
  author       = {Fuqiang Gu and Jiarui Dou and Mingyan Li and Xianlei Long and Songtao Guo and Chao Chen and Kai Liu and Xianlong Jiao and Ruiyuan Li},
  doi          = {10.1109/TCDS.2024.3380907},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1521-1532},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {EventAugment: Learning augmentation policies from asynchronous event-based data},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust perception-based visual simultaneous localization and
tracking in dynamic environments. <em>TCDS</em>, <em>16</em>(4),
1507–1520. (<a href="https://doi.org/10.1109/TCDS.2024.3371073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual simultaneous localization and mapping (SLAM) in dynamic scenes is a prerequisite for robot-related applications. Most of the existing SLAM algorithms mainly focus on dynamic object rejection, which makes part of the valuable information lost and prone to failure in complex environments. This article proposes a semantic visual SLAM system that incorporates rigid object tracking. A robust scene perception frame is designed, which gives autonomous robots the ability to perceive scenes similar to human cognition. Specifically, we propose a two-stage mask revision method to generate fine mask of the object. Based on the revised mask, we propose a semantic and geometric constraint (SAG) strategy, which provides a fast and robust way to perceive dynamic rigid objects. Then, the motion tracking of rigid objects is integrated into the SLAM pipeline, and a novel bundle adjustment is constructed to optimize camera localization and object six-degree of freedom (DoF) poses. Finally, the evaluation of the proposed algorithm is performed on publicly available KITTI dataset, Oxford Multimotion dataset, and real-world scenarios. The proposed algorithm achieves the comprehensive performance of $\text{RPE}_{\text{t}}$ less than 0.07 m per frame and $\text{RPE}_{\text{R}}$ about 0.03 ${}^{\circ}$ per frame in the KITTI dataset. The experimental results reveal that the proposed algorithm enables accurate localization and robust tracking than state-of-the-art SLAM algorithms in challenging dynamic scenarios.},
  archive      = {J_TCDS},
  author       = {Song Peng and Teng Ran and Liang Yuan and Jianbo Zhang and Wendong Xiao},
  doi          = {10.1109/TCDS.2024.3371073},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1507-1520},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Robust perception-based visual simultaneous localization and tracking in dynamic environments},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Brain connectivity analysis for EEG-based face perception
task. <em>TCDS</em>, <em>16</em>(4), 1494–1506. (<a
href="https://doi.org/10.1109/TCDS.2024.3370635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face perception is considered a highly developed visual recognition skill in human beings. Most face perception studies used functional magnetic resonance imaging to identify different brain cortices related to face perception. However, studying brain connectivity networks for face perception using electroencephalography (EEG) has not yet been done. In the proposed framework, initially, a correlation-tree traversal-based channel selection algorithm is developed to identify the “optimum” EEG channels by removing the highly correlated EEG channels from the input channel set. Next, the effective brain connectivity network among those “optimum” EEG channels is developed using multivariate transfer entropy (TE) while participants watched different face stimuli (i.e., famous, unfamiliar, and scrambled). We transform EEG channels into corresponding brain regions for generalization purposes and identify the active brain regions for each face stimulus. To find the stimuluswise brain dynamics, the information transfer among the identified brain regions is estimated using several graphical measures [global efficiency (GE) and transitivity]. Our model archives the mean GE of 0.800, 0.695, and 0.581 for famous, unfamiliar, and scrambled faces, respectively. Identifying face perception-specific brain regions will enhance understanding of the EEG-based face-processing system. Understanding the brain networks of famous, unfamiliar, and scrambled faces can be useful in criminal investigation applications.},
  archive      = {J_TCDS},
  author       = {Debashis Das Chakladar and Nikhil R. Pal},
  doi          = {10.1109/TCDS.2024.3370635},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1494-1506},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Brain connectivity analysis for EEG-based face perception task},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). D-FaST: Cognitive signal decoding with disentangled
frequency–spatial–temporal attention. <em>TCDS</em>, <em>16</em>(4),
1476–1493. (<a href="https://doi.org/10.1109/TCDS.2024.3370261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive language processing (CLP), situated at the intersection of natural language processing (NLP) and cognitive science, plays a progressively pivotal role in the domains of artificial intelligence, cognitive intelligence, and brain science. Among the essential areas of investigation in CLP, cognitive signal decoding (CSD) has made remarkable achievements, yet there still exist challenges related to insufficient global dynamic representation capability and deficiencies in multidomain feature integration. In this article, we introduce a novel paradigm for CLP referred to as disentangled frequency–spatial–temporal attention (D-FaST). Specifically, we present a novel cognitive signal decoder that operates on disentangled frequency–space–time domain attention. This decoder encompasses three key components: frequency domain feature extraction employing multiview attention (MVA), spatial domain feature extraction utilizing dynamic brain connection graph attention, and temporal feature extraction relying on local time sliding window attention. These components are integrated within a novel disentangled framework. Additionally, to encourage advancements in this field, we have created a new CLP dataset, MNRED. Subsequently, we conducted an extensive series of experiments, evaluating D-FaST&#39;s performance on MNRED, as well as on publicly available datasets including ZuCo, BCIC IV-2A, and BCIC IV-2B. Our experimental results demonstrate that D-FaST outperforms existing methods significantly on both our datasets and traditional CSD datasets including establishing a state-of-the-art accuracy score 78.72% on MNRED, pushing the accuracy score on ZuCo to 78.35%, accuracy score on BCIC IV-2A to 74.85%, and accuracy score on BCIC IV-2B to 76.81%.},
  archive      = {J_TCDS},
  author       = {WeiGuo Chen and Changjian Wang and Kele Xu and Yuan Yuan and Yanru Bai and Dongsong Zhang},
  doi          = {10.1109/TCDS.2024.3370261},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1476-1493},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {D-FaST: Cognitive signal decoding with disentangled Frequency–Spatial–Temporal attention},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Compressed video anomaly detection of human behavior based
on abnormal region determination. <em>TCDS</em>, <em>16</em>(4),
1462–1475. (<a href="https://doi.org/10.1109/TCDS.2024.3367493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video anomaly detection has a wide range of applications in video monitoring-related scenarios. The existing image-domain-based anomaly detection algorithms usually require completely decoding the received videos, complex information extraction, and network structure, which makes them difficult to be implemented directly. In this article, we focus on anomaly detection directly for compressed videos. The compressed videos need not be fully decoded and auxiliary information can be obtained directly, which have low computational complexity. We propose a compressed video anomaly detection algorithm based on accurate abnormal region determination (ARD-VAD), which is suitable to be deployed on edge servers. First, to ensure the overall low complexity and save storage space, we sparsely sample the prior knowledge of I-frame representing the appearance information and motion vector (MV) representing the motion information from compressed videos. Based on the sampled information, a two-branch network structure, which consists of MV reconstruction branch and future I-frame prediction branch, is designed. Specifically, the two branches are connected by an attention network based on the MV residuals to guide the prediction network to focus on the abnormal regions. Furthermore, to emphasize the abnormal regions, we develop an adaptive sensing of abnormal regions determination module based on motion intensity represented by the second derivative of MV. This module can enhance the difference of the real anomaly region between the generated frame and the current frame. The experiments show that our algorithm can achieve a good balance between performance and complexity.},
  archive      = {J_TCDS},
  author       = {Lijun He and Miao Zhang and Hao Liu and Liejun Wang and Fan Li},
  doi          = {10.1109/TCDS.2024.3367493},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1462-1475},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Compressed video anomaly detection of human behavior based on abnormal region determination},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DTCM: Deep transformer capsule mutual distillation for
multivariate time series classification. <em>TCDS</em>, <em>16</em>(4),
1445–1461. (<a href="https://doi.org/10.1109/TCDS.2024.3370219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a dual-network-based feature extractor, perceptive capsule network (PCapN), for multivariate time series classification (MTSC), including a local feature network (LFN) and a global relation network (GRN). The LFN has two heads (i.e., Head_A and Head_B), each containing two squash convolutional neural network (CNN) blocks and one dynamic routing block to extract the local features from the data and mine the connections among them. The GRN consists of two capsule-based transformer blocks and one dynamic routing block to capture the global patterns of each variable and correlate the useful information of multiple variables. Unfortunately, it is difficult to directly deploy PCapN on mobile devices due to its strict requirement for computing resources. So, this article designs a lightweight capsule network (LCapN) to mimic the cumbersome PCapN. To promote knowledge transfer from PCapN to LCapN, this article proposes a deep transformer capsule mutual (DTCM) distillation method. It is targeted and offline, using one- and two-way operations to supervise the knowledge distillation (KD) process for the dual-network-based student and teacher models. Experimental results show that the proposed PCapN and DTCM achieve excellent performance on University of East Anglia 2018 (UEA2018) datasets regarding top-1 accuracy.},
  archive      = {J_TCDS},
  author       = {Zhiwen Xiao and Xin Xu and Huanlai Xing and Bowen Zhao and Xinhan Wang and Fuhong Song and Rong Qu and Li Feng},
  doi          = {10.1109/TCDS.2024.3370219},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1445-1461},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {DTCM: Deep transformer capsule mutual distillation for multivariate time series classification},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Agree to disagree: Exploring partial semantic consistency
against visual deviation for compositional zero-shot learning.
<em>TCDS</em>, <em>16</em>(4), 1433–1444. (<a
href="https://doi.org/10.1109/TCDS.2024.3367957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional zero-shot learning (CZSL) aims to recognize novel concepts from known subconcepts. However, it is still challenging since the intricate interaction between subconcepts is entangled with their corresponding visual features, which affects the recognition accuracy of concepts. Besides, the domain gap between training and testing data leads to the model poor generalization. In this article, we tackle these problems by exploring partial semantic consistency (PSC) to eliminate visual deviation to guarantee the discrimination and generalization of representations. Considering the complicated interaction between subconcepts and their visual features, we decompose seen images into visual elements according to their labels and obtain the instance-level subdeviations from compositions, which is utilized to excavate the category-level primitives of subconcepts. Furthermore, we present a multiscale concept composition (MSCC) approach to produce virtual samples from two aspects, which augments the sufficiency and diversity of samples so that the proposed model can generalize to novel compositions. Extensive experiments indicate that our method significantly outperforms the state-of-the-art approaches on three benchmark datasets.},
  archive      = {J_TCDS},
  author       = {Xiangyu Li and Xu Yang and Xi Wang and Cheng Deng},
  doi          = {10.1109/TCDS.2024.3367957},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1433-1444},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Agree to disagree: Exploring partial semantic consistency against visual deviation for compositional zero-shot learning},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Depression detection using an automatic sleep staging method
with an interpretable channel-temporal attention mechanism.
<em>TCDS</em>, <em>16</em>(4), 1418–1432. (<a
href="https://doi.org/10.1109/TCDS.2024.3358022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite previous efforts in depression detection studies, there is a scarcity of research on automatic depression detection using sleep structure, and several challenges remain: 1) how to apply sleep staging to detect depression and distinguish easily misjudged classes; and 2) how to adaptively capture attentive channel-dimensional information to enhance the interpretability of sleep staging methods. To address these challenges, an automatic sleep staging method based on a channel-temporal attention mechanism and a depression detection method based on sleep structure features are proposed. In sleep staging, a temporal attention mechanism is adopted to update the feature matrix, confidence scores are estimated for each sleep stage, the weight of each channel is adjusted based on these scores, and the final results are obtained through a temporal convolutional network. In depression detection, seven sleep structure features based on the results of sleep staging are extracted for depression detection between unipolar depressive disorder (UDD) patients, bipolar disorder (BD) patients, and healthy subjects. Experiments demonstrate the effectiveness of the proposed approaches, and the visualization of the channel attention mechanism illustrates the interpretability of our method. Additionally, this is the first attempt to employ sleep structure features to automatically detect UDD and BD in patients.},
  archive      = {J_TCDS},
  author       = {Jiahui Pan and Jie Liu and Jianhao Zhang and Xueli Li and Dongming Quan and Yuanqing Li},
  doi          = {10.1109/TCDS.2024.3358022},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1418-1432},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Depression detection using an automatic sleep staging method with an interpretable channel-temporal attention mechanism},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An electroencephalography-based brain–computer interface for
emotion regulation with virtual reality neurofeedback. <em>TCDS</em>,
<em>16</em>(4), 1405–1417. (<a
href="https://doi.org/10.1109/TCDS.2024.3357547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An increasing number of people fail to properly regulate their emotions for various reasons. Although brain–computer interfaces (BCIs) have shown potential in neural regulation, few effective BCI systems have been developed to assist users in emotion regulation. In this article, we propose an electroencephalography (EEG)-based BCI for emotion regulation with virtual reality (VR) neurofeedback. Specifically, music clips with positive, neutral, and negative emotions were first presented, based on which the participants were asked to regulate their emotions. The BCI system simultaneously collected the participants’ EEG signals and then assessed their emotions. Furthermore, based on the emotion recognition results, the neurofeedback was provided to participants in the form of a facial expression of a virtual pop star on a three-dimensional (3-D) virtual stage. Eighteen healthy participants achieved satisfactory performance with an average accuracy of 81.1% with neurofeedback. Additionally, the average accuracy increased significantly from 65.4% at the start to 87.6% at the end of a regulation trial (a trial corresponded to a music clip). In comparison, these participants could not significantly improve the accuracy within a regulation trial without neurofeedback. The results demonstrated the effectiveness of our system and showed that VR neurofeedback played a key role during emotion regulation.},
  archive      = {J_TCDS},
  author       = {Kendi Li and Weichen Huang and Wei Gao and Zijing Guan and Qiyun Huang and Jin-Gang Yu and Zhu Liang Yu and Yuanqing Li},
  doi          = {10.1109/TCDS.2024.3357547},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1405-1417},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An electroencephalography-based Brain–Computer interface for emotion regulation with virtual reality neurofeedback},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PLOT: Human-like push-grasping synergy learning in clutter
with one-shot target recognition. <em>TCDS</em>, <em>16</em>(4),
1391–1404. (<a href="https://doi.org/10.1109/TCDS.2024.3357084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In unstructured environments, robotic grasping tasks are frequently required to interactively search for and retrieve specific objects from a cluttered workspace under the condition that only partial information about the target is available, like images, text descriptions, 3-D models, etc. It is a great challenge to correctly recognize the targets with limited information and learn synergies between different action primitives to grasp the targets from densely occluding objects efficiently. In this article, we propose a novel human-like push-grasping method that could grasp unknown objects in clutter using only one target RGB with Depth (RGB-D) image, called push-grasping synergy learning in clutter with one-shot target recognition (PLOT). First, we propose a target recognition (TR) method which automatically segments the objects both from the query image and workspace image, and extract the robust features of each segmented object. Through the designed feature matching criterion, the targets could be quickly located in the workspace. Second, we introduce a self-supervised target-oriented grasping system based on synergies between push and grasp actions. In this system, we propose a salient Q (SQ)-learning framework that focuses the Q value learning in the area including targets and a coordination mechanism (CM) that selects the proper actions to search and isolate the targets from the surrounding objects, even in the condition of targets invisible. Our method is inspired by the working memory mechanism of human brain and can grasp any target object shown through the image and has good generality in application. Experimental results in simulation and real-world show that our method achieved the best performance compared with the baselines in finding the unknown target objects from the cluttered environment with only one demonstrated target RGB-D image and had the high efficiency of grasping under the synergies of push and grasp actions.},
  archive      = {J_TCDS},
  author       = {Xiaoge Cao and Tao Lu and Liming Zheng and Yinghao Cai and Shuo Wang},
  doi          = {10.1109/TCDS.2024.3357084},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1391-1404},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {PLOT: Human-like push-grasping synergy learning in clutter with one-shot target recognition},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Husformer: A multimodal transformer for multimodal human
state recognition. <em>TCDS</em>, <em>16</em>(4), 1374–1390. (<a
href="https://doi.org/10.1109/TCDS.2024.3357618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human state recognition is a critical topic with pervasive and important applications in human–machine systems. Multimodal fusion, which entails integrating metrics from various data sources, has proven to be a potent method for boosting recognition performance. Although recent multimodal-based models have shown promising results, they often fall short in fully leveraging sophisticated fusion strategies essential for modeling adequate cross-modal dependencies in the fusion representation. Instead, they rely on costly and inconsistent feature crafting and alignment. To address this limitation, we propose an end-to-end multimodal transformer framework for multimodal human state recognition called Husformer . Specifically, we propose using cross-modal transformers, which inspire one modality to reinforce itself through directly attending to latent relevance revealed in other modalities, to fuse different modalities while ensuring sufficient awareness of the cross-modal interactions introduced. Subsequently, we utilize a self-attention transformer to further prioritize contextual information in the fusion representation. Extensive experiments on two human emotion corpora (DEAP and WESAD) and two cognitive load datasets [multimodal dataset for objective cognitive workload assessment on simultaneous tasks (MOCAS) and CogLoad] demonstrate that in the recognition of the human state, our Husformer outperforms both state-of-the-art multimodal baselines and the use of a single modality by a large margin, especially when dealing with raw multimodal features. We also conducted an ablation study to show the benefits of each component in Husformer . Experimental details and source code are available at https://github.com/SMARTlab-Purdue/Husformer .},
  archive      = {J_TCDS},
  author       = {Ruiqi Wang and Wonse Jo and Dezhong Zhao and Weizheng Wang and Arjun Gupte and Baijian Yang and Guohua Chen and Byung-Cheol Min},
  doi          = {10.1109/TCDS.2024.3357618},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1374-1390},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Husformer: A multimodal transformer for multimodal human state recognition},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reconfiguration of cognitive control networks during a
long-duration flanker task. <em>TCDS</em>, <em>16</em>(4), 1364–1373.
(<a href="https://doi.org/10.1109/TCDS.2024.3350974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous task engagement generally leads to vigilance decrement and deteriorates task performance. However, how conflict effect is modulated by vigilance decrement has no consistent evidence, and little is known about the underlying neural mechanisms. Here, we adopted an electroencephalogram (EEG) dataset collected during a prolonged flanker task to examine the interactions between vigilance and congruency on behavioral performance and neural measures. Specifically, we extracted a sequence of event-related potentials (ERPs) using temporal principal component analysis (PCA) and performed functional network analysis with graph measures. Behavioral analysis results showed that behavioral performance deteriorated due to vigilance decrement, but the capability of conflict processing was maintained over time. Regarding the neural analysis results, the conflict effect reflected in P3a and P3b was changed and maintained, respectively, when affected by vigilance decrement. The theta band frontoparietal network was observed in the face of conflicting interference and the conflict effect for graph measures disappeared over time. These results demonstrated deteriorated task performance, impaired cognitive functions, and the reconfiguration of cognitive control networks during a prolonged flanker task. Our findings also support the evidence that temporal PCA, and event-related network analysis might be efficient for the investigation of the neural dynamics of complex cognitive processes.},
  archive      = {J_TCDS},
  author       = {Jia Liu and Yongjie Zhu and Zheng Chang and Tiina Parviainen and Christian Antfolk and Timo Hämäläinen and Fengyu Cong},
  doi          = {10.1109/TCDS.2024.3350974},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1364-1373},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Reconfiguration of cognitive control networks during a long-duration flanker task},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual-graph contrastive learning for unsupervised person
reidentification. <em>TCDS</em>, <em>16</em>(4), 1352–1363. (<a
href="https://doi.org/10.1109/TCDS.2023.3348766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised person reidentification (re-ID) aims to perform person retrieval in an unsupervised manner across multiple cameras. Most recent works usually adopted contrastive learning with clustering strategy to guide learning process. However, the domain gap between cameras easily causes the high intraclass variance and prohibits an effective contrastive learning. To overcome this problem, we propose dual-graph contrastive learning (Dual-GCL) to fully utilize the structures between the instances as well as the relationship between the instances and the centroids generated by the clustering strategy. First, a dual-graph is constructed by utilizing the affinities of instances in the feature space and the pseudolabel space derived from clustering strategy with the node-to-node contrastive learning. Then, to capture the relations between the dual-graph and the centroids, we perform Dual-GCL at both the node level and the super-node level. The contrastive learning based on the two levels can encode multilevel structural representations between the instances corresponding to the nodes in dual-graph and align the centroids between different cameras in the shared embedding space. Moreover, a camera-aware augmentation scheme that exploits the semantic relationship is proposed to alleviate the domain discrepancy between cameras. Experimental results demonstrate that our approach outperforms the state-of-the-art methods.},
  archive      = {J_TCDS},
  author       = {Lin Zhang and Ran Song and Yifan Wang and Qian Zhang and Wei Zhang},
  doi          = {10.1109/TCDS.2023.3348766},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1352-1363},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Dual-graph contrastive learning for unsupervised person reidentification},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kernel-ridge-regression-based randomized network for brain
age classification and estimation. <em>TCDS</em>, <em>16</em>(4),
1342–1351. (<a href="https://doi.org/10.1109/TCDS.2024.3349593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accelerated brain aging and abnormalities are associated with variations in brain patterns. Effective and reliable assessment methods are required to accurately classify and estimate brain age. In this study, a brain age classification and estimation framework is proposed using structural magnetic resonance imaging (sMRI) scans, a 3-D convolutional neural network (3-D-CNN), and a kernel ridge regression-based random vector functional link (KRR-RVFL) network. We used 480 brain MRI images from the publicly availabel IXI database and segmented them into gray matter (GM), white matter (WM), and cerebrospinal fluid (CSF) images to show age-related associations by region. Features from MRI images are extracted using 3-D-CNN and fed into the wavelet KRR-RVFL network for brain age classification and prediction. The proposed algorithm achieved high classification accuracy, 97.22%, 99.31%, and 95.83% for GM, WM, and CSF regions, respectively. Moreover, the proposed algorithm demonstrated excellent prediction accuracy with a mean absolute error (MAE) of $3.89$ years, $3.64$ years, and $4.49$ years for GM, WM, and CSF regions, confirming that changes in WM volume are significantly associated with normal brain aging. Additionally, voxel-based morphometry (VBM) examines age-related anatomical alterations in different brain regions in GM, WM, and CSF tissue volumes.},
  archive      = {J_TCDS},
  author       = {Raveendra Pilli and Tripti Goel and R. Murugan and M. Tanveer and P. N. Suganthan},
  doi          = {10.1109/TCDS.2024.3349593},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1342-1351},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Kernel-ridge-regression-based randomized network for brain age classification and estimation},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical multicontact motion planning of hexapod robots
with incremental reinforcement learning. <em>TCDS</em>, <em>16</em>(4),
1327–1341. (<a href="https://doi.org/10.1109/TCDS.2023.3345539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Legged locomotion in unstructured environments with static and dynamic obstacles is challenging. This article proposes a novel hierarchical multicontact motion planning method with incremental reinforcement learning (HMC-IRL) that enables hexapod robots to pass through large-scale discrete complex unstructured environments with local changes occurring. First, a novel hierarchical structure and an information fusion mechanism are developed to decompose multicontact motion planning into two stages: planning the high level prior grid path and planning the low level detailed center of mass (COM) and foothold sequences based on the prior grid path. Second, we leverage the HMC-IRL method with an incremental architecture to enable swift adaptation to local changes in the environment, which includes incremental soft Q-learning (ISQL) algorithm to obtain the optimal prior grid path and incremental proximal policy optimization (IPPO) algorithm to obtain the COM and foothold sequences in the dynamic plum blossom pile environment. Finally, the integrated HMC-IRL method is tested on both simulated and real systems. All the experimental results demonstrate the feasibility and efficiency of the proposed method. Videos are shown at http://www.hexapod.cn/hmcirl.html .},
  archive      = {J_TCDS},
  author       = {Kaiqiang Tang and Huiqiao Fu and Guizhou Deng and Xinpeng Wang and Chunlin Chen},
  doi          = {10.1109/TCDS.2023.3345539},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1327-1341},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Hierarchical multicontact motion planning of hexapod robots with incremental reinforcement learning},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple instance learning for cheating detection and
localization in online examinations. <em>TCDS</em>, <em>16</em>(4),
1315–1326. (<a href="https://doi.org/10.1109/TCDS.2024.3349705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spread of the Coronavirus disease-2019 epidemic has caused many courses and exams to be conducted online. The cheating behavior detection model in examination invigilation systems plays a pivotal role in guaranteeing the equality of long-distance examinations. However, cheating behavior is rare, and most researchers do not comprehensively take into account features such as head posture, gaze angle, body posture, and background information in the task of cheating behavior detection. In this article, we develop and present CHEESE, a CHEating detection framework via multiple instance learning. The framework consists of a label generator that implements weak supervision and a feature encoder to learn discriminative features. In addition, the framework combines body posture and background features extracted by 3-D convolution with eye gaze, head posture, and facial features captured by OpenFace 2.0. These features are fed into the spatiotemporal graph module by stitching to analyze the spatiotemporal changes in video clips to detect the cheating behaviors. Our experiments on three datasets, University of Central Florida (UCF)-Crime, ShanghaiTech, and online exam proctoring (OEP), prove the effectiveness of our method as compared to the state-of-the-art approaches and obtain the frame-level area under the curve (AUC) score of 87.58% on the OEP dataset.},
  archive      = {J_TCDS},
  author       = {Yemeng Liu and Jing Ren and Jianshuo Xu and Xiaomei Bai and Roopdeep Kaur and Feng Xia},
  doi          = {10.1109/TCDS.2024.3349705},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1315-1326},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Multiple instance learning for cheating detection and localization in online examinations},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). QFuture: Learning future expectation cognition in multiagent
reinforcement learning. <em>TCDS</em>, <em>16</em>(4), 1302–1314. (<a
href="https://doi.org/10.1109/TCDS.2023.3345735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multiagent reinforcement learning (MARL), agents must learn to cooperate by observing the environment and selecting actions that maximize their rewards. However, this learning process can be hampered by myopia, wherein agents’ strategies fail to consider the long-term consequences of their actions. A primary reason for this problem is the inaccurate estimation of the long-term value of each action. Socially, humans derive future expectation cognition from available information to anticipate potential future outcomes and adjust their actions accordingly to avoid myopia. Motivated by these insights, this article proposes a novel framework called QFuture to address the myopia problem. Specifically, we first design a future expectation cognition module (FECM) in this framework to build future expectation cognition in the calculation of individual action-value (IAV) and joint action-value (JAV). We model future expectation cognition as random variables in FECM, which learn representation by maximizing mutual information with the future trajectory based on current information. Furthermore, a return-based regularizer is designed to reflect “expectation” and ensure informativeness in the future expectation representation module (FERM) which encodes the future trajectory. Experiments on StarCraft II micromanagement tasks and Google Research Football show that QFuture achieves significant state-of-the-art performance. Demonstrative videos are available at https://sites.google.com/view/qfuture .},
  archive      = {J_TCDS},
  author       = {Boyin Liu and Zhiqiang Pu and Yi Pan and Jianqiang Yi and Min Chen and Shijie Wang},
  doi          = {10.1109/TCDS.2023.3345735},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1302-1314},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {QFuture: Learning future expectation cognition in multiagent reinforcement learning},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MIMo: A multimodal infant model for studying cognitive
development. <em>TCDS</em>, <em>16</em>(4), 1291–1301. (<a
href="https://doi.org/10.1109/TCDS.2024.3350448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human intelligence and human consciousness emerge gradually during the process of cognitive development. Understanding this development is an essential aspect of understanding the human mind and may facilitate the construction of artificial minds with similar properties. Importantly, human cognitive development relies on embodied interactions with the physical and social environment, which is perceived via complementary sensory modalities. These interactions allow the developing mind to probe the causal structure of the world. This is in stark contrast to common machine learning approaches, e.g., for large language models, which are merely passively “digesting” large amounts of training data, but are not in control of their sensory inputs. However, computational modeling of the kind of self-determined embodied interactions that lead to human intelligence and consciousness is a formidable challenge. Here, we present Multimodal Infant Model (MiMo), an open-source multimodal infant model for studying early cognitive development through computer simulations. MIMo&#39;s body is modeled after an 18-month-old child with detailed five-fingered hands. MIMo perceives its surroundings via binocular vision, a vestibular system, proprioception, and touch perception through a full-body virtual skin, while two different actuation models allow control of his body. We describe the design and interfaces of MIMo and provide examples illustrating its use.},
  archive      = {J_TCDS},
  author       = {Dominik Mattern and Pierre Schumacher and Francisco M. López and Marcel C. Raabe and Markus R. Ernst and Arthur Aubret and Jochen Triesch},
  doi          = {10.1109/TCDS.2024.3350448},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1291-1301},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {MIMo: A multimodal infant model for studying cognitive development},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolutionary niche artificial fish swarm algorithm for
dynamic subgroup size adjustment in robot swarms. <em>TCDS</em>,
<em>16</em>(4), 1274–1290. (<a
href="https://doi.org/10.1109/TCDS.2023.3345931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adapting subgroup sizes adjustment in distributed multitarget search tasks for robot swarms presents a significant challenge. Traditional search methods struggle to dynamically adjust subgroup sizes as search conditions change under limited population information. This article proposes a novel multirobot cooperation approach known as the evolutionary niche artificial fish swarm algorithm (ENAFSA) for adapting subgroup sizes. By integrating niche artificial fish swarm algorithm (AFSA) with a Markov chain learning model, ENAFSA introduces an automatic learning strategy for adaptive subgroup size adjustment in multitarget search tasks within robot swarms. It leverages niche technology, combining it with a distributed version of the AFSA to simultaneously locate and search for targets. Additionally, ENAFSA incorporates a mutation mechanism that allows robots to autonomously reallocate among different subgroups, enabling decentralized changes in subgroup sizes. The mutation rate for each robot is determined by the probability transition matrix of the Markov chain model, and we employ the Markov chain gradient descent (MCGD) method to optimize this transition matrix. We conduct simulation experiments to showcase the practicality of our subgroup adjustment algorithm and its effectiveness in searching for multiple targets, even when the number of robots and targets vary.},
  archive      = {J_TCDS},
  author       = {Zhenlong Xiao and Xin Wang},
  doi          = {10.1109/TCDS.2023.3345931},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1274-1290},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Evolutionary niche artificial fish swarm algorithm for dynamic subgroup size adjustment in robot swarms},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An improvement method of target tracking based on broad
learning system with scale and drift correction. <em>TCDS</em>,
<em>16</em>(4), 1260–1273. (<a
href="https://doi.org/10.1109/TCDS.2023.3347604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Target tracking is difficult to achieve high accurate and efficient results due to the cases of occlusion, scale variation, and fast motion in the tracking process. In recent years, feature fusion and bounding box refinement are used as an extent to improve the accuracy of tracking. However, these new methods generally require multilevel linkage and offline training, which need long training time, and affects the portability of the algorithm. In this article, a novel method is proposed to improve the tracking accuracy using broad learning system (BLS). The proposed method trains Intersection over Union (IoU) network based on BLS, which is called BLIoU for scale and drift correction in target tracking. BLIoU learns target features and IoU discrimination ability of bounding box through network training, which can be combined with any baseline tracking methods. BLIoU performs scale correction in the case of accurate positioning of the baseline tracker and drift correction in the case of inaccurate positioning. In Experiments, several benchmark datasets are used, BLIoU improves the tracking performance of the baseline tracker through scale and drift correction, with short training time and strong portability.},
  archive      = {J_TCDS},
  author       = {Dan Zhang and Tieshan Li and C. L. Philip Chen and Yi Zuo},
  doi          = {10.1109/TCDS.2023.3347604},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1260-1273},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An improvement method of target tracking based on broad learning system with scale and drift correction},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Disentangled adversarial generalization network for
cross-session task-independent brainprint recognition. <em>TCDS</em>,
<em>16</em>(4), 1248–1259. (<a
href="https://doi.org/10.1109/TCDS.2023.3343469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capitalizing on the invisibility of electroencephalography (EEG) signals, brainprint has emerged as a promising EEG-based biometric technology that can meet scenarios with high-security requirements. Most studies have focused on single cognitive tasks while spontaneous brain activity would be affected by mental processes, making identity and task information spuriously correlated. Furthermore, the temporal variability of EEG signals may lead to differences in data distribution of multiple sessions. Then, the training model fails to meet the unseen cross-session and cross-task data, limiting the use of brainprint recognition in reality. In this article, we proposed a disentangled adversarial generalization network (DAGN) for stable task-independent brainprint recognition across sessions. Specifically, we first facilitate disentangling task-relevant and identity-relevant features via decorrelation to get rid of spurious correlations. An adversarial self-challenging strategy is then designed to challenge the activation of remaining label-related features in addition to dominant features. This naturally balances the contribution of dominant and inferior dimensions of discriminative features to alleviate the poor robustness of model in different unseen data distributions. Extensive experiments on the representative multitask benchmarks with challenging leave-one-task-out and leave-test-session-out validation are carried out to demonstrate that our method performs favorably against the state-of-the-art approaches.},
  archive      = {J_TCDS},
  author       = {Xuanyu Jin and Ni Li and Wanzeng Kong and Qibin Zhao and Jiajia Tang and Li Zhu and Jianting Cao},
  doi          = {10.1109/TCDS.2023.3343469},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1248-1259},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Disentangled adversarial generalization network for cross-session task-independent brainprint recognition},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep reinforcement learning with multicritic TD3 for
decentralized multirobot path planning. <em>TCDS</em>, <em>16</em>(4),
1233–1247. (<a href="https://doi.org/10.1109/TCDS.2024.3368055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Centralized multirobot path planning is a prevalent approach involving a global planner computing feasible paths for each robot using shared information. Nonetheless, this approach encounters limitations due to communication constraints and computational complexity. To address these challenges, we introduce a novel decentralized multirobot path planning approach that eliminates the need for sharing the states and intentions of robots. Our approach harnesses deep reinforcement learning and features an asynchronous multicritic twin delayed deep deterministic policy gradient (AMC-TD3) algorithm, which enhances the original gate recurrent unit (GRU)-attention-based TD3 algorithm by incorporating a multicritic network and employing an asynchronous training mechanism. By training each critic with a unique reward function, our learned policy enables each robot to navigate toward its long-term objective without colliding with other robots in complex environments. Furthermore, our reward function, grounded in social norms, allows the robots to naturally avoid each other in congested situations. Specifically, we train three critics to encourage each robot to achieve its long-term navigation goal, maintain its moving direction, and prevent collisions with other robots. Our model can learn an end-to-end navigation policy without relying on an accurate map or any localization information, rendering it highly adaptable to various environments. Simulation results reveal that our proposed approach surpasses baselines in several environments with different levels of complexity and robot populations.},
  archive      = {J_TCDS},
  author       = {Heqing Yin and Chang Wang and Chao Yan and Xiaojia Xiang and Boliang Cai and Changyun Wei},
  doi          = {10.1109/TCDS.2024.3368055},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1233-1247},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Deep reinforcement learning with multicritic TD3 for decentralized multirobot path planning},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TR-TransGAN: Temporal recurrent transformer generative
adversarial network for longitudinal MRI dataset expansion.
<em>TCDS</em>, <em>16</em>(4), 1223–1232. (<a
href="https://doi.org/10.1109/TCDS.2023.3345922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal magnetic resonance imaging (MRI) datasets have important implications for the study of degenerative diseases because such datasets have data from multiple points in time to track disease progression. However, longitudinal datasets are often incomplete due to unexpected quits of patients. In previous work, we proposed an augmentation method temporal recurrent generative adversarial network (TR-GAN) that can complement missing session data of MRI datasets. TR-GAN uses a simple U-Net as a generator, which limits its performance. Transformers have had great success in the research of computer vision and this article attempts to introduce it into longitudinal dataset completion tasks. The multihead attention mechanism in transformer has huge memory requirements, and it is difficult to train 3-D MRI data on graphics processing units (GPUs) with small memory. To build a memory-friendly transformer-based generator, we introduce a Hilbert transform module (HTM) to convert 3-D data to 2-D data that preserves locality fairly well. To make up for the insufficiency of convolutional neural network (CNN)-based models that are difficult to establish long-range dependencies, we propose an Swin transformer-based up/down sampling module (STU/STD) module that combines the Swin transformer module and CNN module to capture global and local information simultaneously. Extensive experiments show that our model can reduce mean squared error (MMSE) by at least 7.16% compared to the previous state-of-the-art method.},
  archive      = {J_TCDS},
  author       = {Chen-Chen Fan and Hongjun Yang and Liang Peng and Xiao-Hu Zhou and Shiqi Liu and Sheng Chen and Zeng-Guang Hou},
  doi          = {10.1109/TCDS.2023.3345922},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1223-1232},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {TR-TransGAN: Temporal recurrent transformer generative adversarial network for longitudinal MRI dataset expansion},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). User-aware multilevel cognitive workload estimation from
multimodal physiological signals. <em>TCDS</em>, <em>16</em>(4),
1212–1222. (<a href="https://doi.org/10.1109/TCDS.2023.3342139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we tackle the problem of human cognitive workload estimation from multimodal physiological signals. Allowing computational assistive systems to infer the cognitive states of users in real-time could provide seamless and more fluid human–machine interaction. We introduce WorkNet, a novel end-to-end sequential deep learning model for user-aware cognitive workload classification in a virtual reality driving simulator. Our architecture exploits user-awareness to bridge the performance-deployability gap in cognitive workload estimation between personalized and generalized models. WorkNet uses a dual prediction head, where one recognizes whose user the data corresponds to, while the other predicts the current user&#39;s cognitive state. We evaluate WorkNet on a dataset we collected from 20 participants in eight scenarios of different levels of cognitive workload, each induced by either auditory or visual stimuli. WorkNet differentiates with high accuracy among four different cognitive workload levels, independently on the modality of the stimulus. Also, it extracts meaningful features from the input, independently on the modality of the stimulus and on the users. Our analyses show that WorkNet can adapt with limited data, akin to a calibration stage, to perform on a different modality it has been trained on, and on unseen users.},
  archive      = {J_TCDS},
  author       = {Pierluigi Vito Amadori and Yiannis Demiris},
  doi          = {10.1109/TCDS.2023.3342139},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1212-1222},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {User-aware multilevel cognitive workload estimation from multimodal physiological signals},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cerebellum-inspired model predictive control for redundant
manipulators with unknown structure information. <em>TCDS</em>,
<em>16</em>(3), 1198–1210. (<a
href="https://doi.org/10.1109/TCDS.2023.3340179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the structure information of a redundant manipulator is unknown, motion control methods that do not rely on its model are attractive. Due to the numerous advantages of model predictive control (MPC), such as the direct handling of constraints, this article proposes a model-free MPC algorithm for redundant manipulators with unknown structure information. In this article, a cerebellum-inspired model based on the echo state network (ESN) is employed to replace the kinematic model of the redundant manipulator, and an MPC algorithm based on the cerebellum model and neural dynamics (ND) approach is developed. Unlike existing studies, this work considers both performance optimization and system constraints of the redundant manipulator, and can achieve high-precision prediction and tracking by designing an online training algorithm for the cerebellum model. Furthermore, this article proposes an ND-based correction algorithm to modify the prediction model and an ND solver to solve the MPC scheme. Theoretical analyses confirm the convergence of both the ND-based correction algorithm and ND solver. Simulation and experimental results consistently demonstrate that the proposed cerebellum-inspired MPC (CIMPC) algorithm is effective and outperforms comparison algorithms in terms of tracking performance.},
  archive      = {J_TCDS},
  author       = {Jingkun Yan and Mei Liu and Long Jin},
  doi          = {10.1109/TCDS.2023.3340179},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {1198-1210},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Cerebellum-inspired model predictive control for redundant manipulators with unknown structure information},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual privacy-preserving coding for video intelligence
applications: A compressed sensing mechanism via bee-eye bionic vision.
<em>TCDS</em>, <em>16</em>(3), 1186–1197. (<a
href="https://doi.org/10.1109/TCDS.2023.3338609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent video surveillance systems can be used to detect abnormal occurrences in the home environment such as falls and assaults. However, traditional video surveillance systems rely on complete and clear video data to complete behavior recognition tasks, and they therefore cannot successfully fulfill the requirements of visual privacy protection. The home environment involves considerable privacy-sensitive information. If this information is stolen and misused by others, it could seriously threaten personal privacy. To solve this problem, a bionic coding model of bee eye vision based on compressed sensing, named BCBEV-CS, is proposed in this article. The model combines the low visual acuity and target perception of bee eyes and introduces the encryption of a random measurement matrix in compressed sensing (CS). On the one hand, the model can filter out low-level visual feature information in images or videos to meet the needs of visual privacy protection, and on the other hand, it can retain certain high-level visual feature information to complete behavior recognition tasks. In addition, we propose a visual privacy protection (VPP) level evaluation method for the BCBEV-CS model at the human visual level. Finally, to better apply the model to intelligent systems, we build a correlation-based statistical model between visual privacy protection and video behavior recognition, thereby standardizing the coding range of the BCBEV-CS model.},
  archive      = {J_TCDS},
  author       = {Jixin Liu and Kai Wang and Haigen Yang and Ning Sun},
  doi          = {10.1109/TCDS.2023.3338609},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {1186-1197},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Visual privacy-preserving coding for video intelligence applications: A compressed sensing mechanism via bee-eye bionic vision},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Subband cascaded CSP-based deep transfer learning for
cross-subject lower limb motor imagery classification. <em>TCDS</em>,
<em>16</em>(3), 1172–1185. (<a
href="https://doi.org/10.1109/TCDS.2023.3338460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lower limb motor imagery (MI) classification is a challenging research topic in brain–computer interface (BCI) due to excessively close physiological representation of left and right lower limb movements in the human brain. Moreover, MI signals have severely subject-specific characteristics. The classification schemes designed for a specific subject in previous studies could not meet the requirements of cross-subject classification in a generic BCI system. Therefore, this study aimed to establish a cross-subject lower limb MI classification scheme. Three novel subband cascaded common spatial pattern (SBCCSP) algorithms were proposed to extract representative features with low redundancy. The validations had been conducted based on the lower limb stepping-based MI signals collected from subjects performing MI tasks in experiments. The proposed schemes with three SBCCSP algorithms have been validated with better accuracy and running time performances than other common spatial pattern (CSP) variants with the best average accuracy of 98.78%. This study provides the first investigation of a cross-subject MI classification scheme based on experimental stepping-based MI signals. The proposed scheme will make an essential contribution to developing generic BCI systems for lower limb auxiliary and rehabilitation applications.},
  archive      = {J_TCDS},
  author       = {Mingnan Wei and Rui Yang and Mengjie Huang and Jiaying Ni and Zidong Wang and Xiaohui Liu},
  doi          = {10.1109/TCDS.2023.3338460},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {1172-1185},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Subband cascaded CSP-based deep transfer learning for cross-subject lower limb motor imagery classification},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Entropy enhanced multiagent coordination based on
hierarchical graph learning for continuous action space. <em>TCDS</em>,
<em>16</em>(3), 1161–1171. (<a
href="https://doi.org/10.1109/TCDS.2023.3339131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In most existing studies on large-scale multiagent coordination, the control methods aim to learn discrete policies for agents with finite choices. They rarely consider selecting actions directly from continuous action spaces to provide more accurate control; therefore, they are normally unsuitable for more complex tasks. To solve the control issue of large-scale multiagent systems with continuous action spaces, we propose a novel multiagent reinforcement learning (MARL) approach named entropy-enhanced hierarchical graph continuous action multiagent coordination control method (EHCAMA) to derive stable continuous policies, by constructing a new network architecture in an actor-critic framework. By optimizing policies with maximum entropy learning, agents improve their exploration ability in training and acquire an excellent performance in execution. Further, we employ hierarchical graph attention networks (HGATs) and gated recurrent units (GRUs) to improve the scalability and transferability of our method. We simulate the performance of EHCAMA for cooperative tasks with both homogeneous and heterogeneous agents, and compare it with soft actor-critic-hierarchical graph recurrent network (SAC-HGRN), hierarchical graph attention-based multiagent actor-critic (HAMA), actor hierarchical attention critic (AHAC), and adaptive and gated graph attention network (AGGAT)-Comm. The experimental results show that our method consistently outperforms the baselines in large-scale multiagent scenarios.},
  archive      = {J_TCDS},
  author       = {Yining Chen and Ke Wang and Guanghua Song and Xiaohong Jiang},
  doi          = {10.1109/TCDS.2023.3339131},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {1161-1171},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Entropy enhanced multiagent coordination based on hierarchical graph learning for continuous action space},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A brain-inspired incremental multitask reinforcement
learning approach. <em>TCDS</em>, <em>16</em>(3), 1147–1160. (<a
href="https://doi.org/10.1109/TCDS.2023.3338241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there have been growing interests in multitask reinforcement learning (MTRL), which is viewed as a promising framework for training agents to execute multiple tasks simultaneously. However, limitations in scalability and convergence remain key obstacles for scaling these MTRL algorithms to dynamic and complex tasks. To address these, we propose a method called brain-inspired incremental multitask reinforcement learning (BIMTRL) that aims to improve parallelism and scalability of multiple tasks. Inspired by learning processes in human brain, we integrate conscious and subconscious modes into the agents’ exploration of environments. Our two-step strategy of policy loosening and importance tradeoff enables an effective switch between these modes. Additionally, in order to overcome the convergence dilemma, we adopt the V-trace method as a stable and robust off-policy correction technique for our actor-critic agents. Experimental evaluations on various tasks in OpenAI Gym, Atari, and PyBullet have demonstrated that BIMTRL achieves a 61.4% greater average return and 57.6% higher speed than specific multitask baselines. Furthermore, its distributed and incremental architecture endows the BIMTRL approach with a desired scalability in both discrete and continuous environments, ultimately leading to larger rewards, higher speed, and better convergence.},
  archive      = {J_TCDS},
  author       = {Chenying Jin and Xiang Feng and Huiqun Yu},
  doi          = {10.1109/TCDS.2023.3338241},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {1147-1160},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A brain-inspired incremental multitask reinforcement learning approach},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An interactive differential evolution method with human
auditory perception for sound composition. <em>TCDS</em>,
<em>16</em>(3), 1134–1146. (<a
href="https://doi.org/10.1109/TCDS.2023.3339193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive evolutionary computation (IEC) finds diverse applications in the domain of sound. However, there is a lack of methods and operations for combining sound elements into a composite sound during the crossover phase of sound composition. To this end, we propose a novel composite crossover operation using paired comparison-based interactive differential evolution. This operation integrates the target and mutant vectors into a composite sound through linear rescaling, enabling users to synthesize and assess their preferred sounds during the evaluation phase. Moreover, an optimization-stopping mechanism is incorporated to allow users to halt the process when a satisfactory sound is produced. This helps to alleviate user fatigue by eliminating unnecessary candidate sounds and improving the efficiency of the composition process by reducing redundant generations and time consumption. The efficacy of this operation was demonstrated through comparative testing and sound quality analysis. Furthermore, this article presents an original analytical approach based on four fundamental attributes of sound for analyzing human auditory preferences in composite sounds. This method combines both quantitative and qualitative paradigms, achieved through principal component analysis and human subjective auditory analysis. These discoveries have significant implications for both sound composition and the analysis of human auditory perception.},
  archive      = {J_TCDS},
  author       = {Yanan Wang and Yan Pei and Hayato Shindo and Qing Liu and Hai-Peng Ren},
  doi          = {10.1109/TCDS.2023.3339193},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {1134-1146},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An interactive differential evolution method with human auditory perception for sound composition},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stable exploration via imitating highly scored
episode-decayed exploration episodes in procedurally generated
environments. <em>TCDS</em>, <em>16</em>(3), 1121–1133. (<a
href="https://doi.org/10.1109/TCDS.2023.3339215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring procedurally generated environments is a formidable challenge for model-free deep reinforcement learning (DRL). One of the state-of-the-art exploration methods, exploration via ranking the episodes (RAPID), assigns episode-level episodic exploration scores to past episodes and makes the DRL agent imitate exploration behaviors from the highly scored episodes. However, in complex procedurally generated environments, such continued imitation can hinder RAPID&#39;s performance due to the emergence of solidified episodes, i.e., episodes that remain in the highly scored episode set due to their high scores. These solidified episodes can lead the RAPID DRL agent to overfit, hindering its exploration and performance. To address this, we design an episode-decayed exploration score, which combines the episodic exploration score and an episodic decay factor (EDF), to avoid solidifying highly scored episodes and aid in selecting good exploration episodes. Leveraging this score, we propose exploration via imitating highly scored episode-decayed exploration episodes (EDEE), an effective and stable exploration method for procedurally generated environments. EDEE assigns episode-decayed exploration scores to past episodes and stores the highly scored episodes as good exploration episodes in a small ranking buffer. The DRL agent then imitates good exploration behaviors sampled from this ranking buffer through the exploration-based sampling (ES) to reproduce these good exploration behaviors from good exploration episodes. Extensive experiments on procedurally generated environments, specifically MiniGrid and 3-D maze from MiniWorld, and sparse MuJoCo environments show that EDEE significantly outperforms RAPID in terms of final performance and sample efficiency in complex procedurally generated environments and sparse continuous environments. Moreover, even without extrinsic rewards, EDEE maintains excellent performance in procedurally generated environments.},
  archive      = {J_TCDS},
  author       = {Mao Xu and Shuzhi Sam Ge and Dongjie Zhao and Qian Zhao},
  doi          = {10.1109/TCDS.2023.3339215},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {1121-1133},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Stable exploration via imitating highly scored episode-decayed exploration episodes in procedurally generated environments},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial–temporal multiscale constrained learning for
mmWave-based human pose estimation. <em>TCDS</em>, <em>16</em>(3),
1108–1120. (<a href="https://doi.org/10.1109/TCDS.2023.3334302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a challenging task to reconstruct human pose from millimeter wave (mmWave) radar point clouds due to their sparsity and sensitivity to multipath noise. In order to enhance the inference ability of deep learning models in processing sparse radar point clouds, a learning paradigm with spatial and temporal multiscale constraints is proposed, utilizing prior relationships in skeletal structure and joint movements in both time and space to constrain the learning of pose sequences. Specifically, the proposed spatial multiscale constraint block learns the spatial constraint relationships of human joints using three different scales: adjacent joint constraint, part-level kinematic constraint, and global joint constraint, by which the spatial joint constraint features are aggregated by fusion gate mechanism. On the other hand, the temporal multiscale constraint block is devised to learn the temporal constraint relationships of the joint trajectories using the information of the joint itself and local context-information in time domain. Compared with the single-scale constrained learning paradigm, the potential advantage of the proposed method is that it can reduce the impact of random missing and noise in radar data. Finally, the effectiveness and superiority of the proposed method are fully demonstrated through the experimental results on two public datasets of human pose estimation based on mmWave radar.},
  archive      = {J_TCDS},
  author       = {Lin Chen and Xuemei Guo and Guoli Wang and Hongyi Li},
  doi          = {10.1109/TCDS.2023.3334302},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {1108-1120},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Spatial–Temporal multiscale constrained learning for mmWave-based human pose estimation},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to predict diverse stable placements for extrinsic
manipulation on a support plane. <em>TCDS</em>, <em>16</em>(3),
1095–1107. (<a href="https://doi.org/10.1109/TCDS.2023.3330989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extrinsic manipulation, a technique that enables robots to leverage extrinsic resources for object manipulation, presents practical yet challenging scenarios. Particularly in the context of extrinsic manipulation on a supporting plane, regrasping becomes essential for achieving the desired final object poses. This process involves sequential operation steps and stable placements of objects, which provide grasp space for the robot. To address this challenge, we focus on predicting diverse placements of objects on the plane using deep neural networks. A framework that comprises orientation generation, placement refinement, and placement discrimination stages is proposed, leveraging point clouds to obtain precise and diverse stable placements. To facilitate training, a large-scale dataset is constructed, encompassing stable object placements and contact information between objects. Through extensive experiments, our approach is demonstrated to outperform the start-of-the-art, achieving an accuracy rate of 90.4% and a diversity rate of 81.3% in predicted placements. Furthermore, we validate the effectiveness of our approach through real-robot experiments, demonstrating its capability to compute sequential pick-and-place steps based on the predicted placements for regrasping objects to goal poses that are not readily attainable within a single step.},
  archive      = {J_TCDS},
  author       = {Peng Xu and Zhiyuan Chen and Jiankun Wang and Max Q.-H. Meng},
  doi          = {10.1109/TCDS.2023.3330989},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {1095-1107},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Learning to predict diverse stable placements for extrinsic manipulation on a support plane},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of an augmented-reality-based serious game: A
cognitive assessment study. <em>TCDS</em>, <em>16</em>(3), 1087–1094.
(<a href="https://doi.org/10.1109/TCDS.2023.3329807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive impairment in the elderly can be associated with the normal aging processes or be a symptom of dementia. Early detection of dementia has many advantages since the lack of diagnosis prevents proper social and medical care. Cognitive screening is the preliminary step in cognitive impairment assessment, leading to early detection. However, the traditional tools have intrinsic limitations such as culture, gender, educational biases, long test-rest periods, and learning effects. The rapid growth of technology has considerably improved cognitive intervention through serious games. Serious games have the potential to overcome those challenges and replace traditional, article-based, and computerized cognitive screening tools. This study introduces a serious game design based on augmented reality to assess cognitive impairments. Thirty-one healthy participants took part in this study. The cognitive ability of the participants was initially assessed using the Montreal Cognitive Assessment test (MoCA). The performance of the proposed serious game was then evaluated and compared with the MoCA scores. A high correlation was observed between the total game score and the total MoCA score. However, not all the game subscores showed high correlation with relevant parts of the MoCA test.},
  archive      = {J_TCDS},
  author       = {Mahsa Farshi Taghavi and Fatemeh Ghorbani and Mehdi Delrobaei},
  doi          = {10.1109/TCDS.2023.3329807},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {1087-1094},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Development of an augmented-reality-based serious game: A cognitive assessment study},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transformer-based spiking neural networks for multimodal
audiovisual classification. <em>TCDS</em>, <em>16</em>(3), 1077–1086.
(<a href="https://doi.org/10.1109/TCDS.2023.3327081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spiking neural networks (SNNs), as brain-inspired neural networks, have received noteworthy attention due to their advantages of low power consumption, high parallelism, and high fault tolerance. While SNNs have shown promising results in uni-modal data tasks, their deployment in multimodal audiovisual classification remains limited, and the effectiveness of capturing correlations between visual and audio modalities in SNNs needs improvement. To address these challenges, we propose a novel model called spiking multimodel transformer (SMMT) that combines SNNs and Transformers for multimodal audiovisual classification. The SMMT model integrates uni-modal subnetworks for visual and auditory modalities with a novel spiking cross-attention module for fusion, enhancing the correlation between visual and audio modalities. This approach leads to competitive accuracy in multimodal classification tasks with low energy consumption, making it an effective and energy-efficient solution. Extensive experiments on a public event-based data set (N-TIDIGIT&amp;MNIST-DVS) and two self-made audiovisual data sets of real-world objects (CIFAR10-AV and UrbanSound8K-AV) demonstrate the effectiveness and energy efficiency of the proposed SMMT model in multimodal audiovisual classification tasks. Our constructed multimodal audiovisual data sets can be accessed at https://github.com/Guo-Lingyue/SMMT .},
  archive      = {J_TCDS},
  author       = {Lingyue Guo and Zeyu Gao and Jinye Qu and Suiwu Zheng and Runhao Jiang and Yanfeng Lu and Hong Qiao},
  doi          = {10.1109/TCDS.2023.3327081},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {1077-1086},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Transformer-based spiking neural networks for multimodal audiovisual classification},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coupling mobile brain imaging and virtual reality
omnidirectional treadmill to explore attenuated situational awareness
during distracted walking. <em>TCDS</em>, <em>16</em>(3), 1063–1076. (<a
href="https://doi.org/10.1109/TCDS.2023.3323779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Walking while engaging in distracting activities affects our perception of changes happening on the road, leading to exposure to hazardous conditions. This study aimed to evidence that pedestrians who become distracted when absorbed in their smartphones fail to maintain situational awareness. Thirty-six recruited participants performed a psychomotor vigilance task and a wayfinding task designed to simulate a multitasking scenario; meanwhile, an auditory oddball paradigm presented to participants tested their situational awareness of an external stimulus. The experiment was conducted in virtual reality (VR) environment with an omnidirectional treadmill. Participant’s behavioral and electroencephalographic (EEG) responses were measured and analyzed using a newly proposed brain network measure, phase locking distance (PLD), to capture multitasking-associated couplings between brain regions. The current study supports the notion that multitasking can lead to diminished behavioral performance and changes in walking patterns. Despite the increased interregional brain connectivity, which may be an adaptive response to heightened cognitive demands, the associated reduction in event-related EEG activity suggests competition for neural resources during multitasking. Moreover, the observed EEG activities could indicate additional information processing capacity limitations and provide evidence of decreased situational awareness during multitasking, thereby increasing susceptibility to potentially hazardous situations.},
  archive      = {J_TCDS},
  author       = {Chun-Hsiang Chuang and Tsai-Feng Chiu and Hao-Che Hsu and Shih-Syun Lin},
  doi          = {10.1109/TCDS.2023.3323779},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {1063-1076},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Coupling mobile brain imaging and virtual reality omnidirectional treadmill to explore attenuated situational awareness during distracted walking},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). State augmentation via self-supervision in offline
multiagent reinforcement learning. <em>TCDS</em>, <em>16</em>(3),
1051–1062. (<a href="https://doi.org/10.1109/TCDS.2023.3326297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The utilization of precollected offline data sets for learning in the absence of environmental interaction has enabled reinforcement learning (RL) to make significant strides in real-world circumstances. This approach is also attractive for multiagent RL (MARL) tasks, given the complex interactions that occur between agents and the environment. However, when compared to the single-agent approach, offline MARL faces more challenges due to the larger state and action space, particularly with regard to poor out-of-distribution generalization to the environment. The present study demonstrates the ineffectiveness of directly transferring conservative offline RL algorithms from single-agent settings to multiagent environments, which is due to the accumulating extrapolation errors that increase in proportion to the number of agents. In this article, we explore the efficacy of three types of data augmentation techniques that can be applied to the state representation in the context of MARL. By combining the proposed data augmentation techniques with a state-of-the-art offline multiagent algorithm, we improve the function approximation of centralized $Q$ -networks. The experimental results conducted on StarCraft II strongly support the effectiveness of the data augmentation techniques in enhancing the performance of offline MARL in the state space.},
  archive      = {J_TCDS},
  author       = {Siying Wang and Xiaodie Li and Hong Qu and Wenyu Chen},
  doi          = {10.1109/TCDS.2023.3326297},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {1051-1062},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {State augmentation via self-supervision in offline multiagent reinforcement learning},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computational model of functional connectivity distance
predicts neural alterations. <em>TCDS</em>, <em>16</em>(3), 1041–1050.
(<a href="https://doi.org/10.1109/TCDS.2023.3320243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling brain signals play a crucial role in analyzing the brain’s architecture, functions and associated disorders. This article aims to model the brain topology by exploring the relationship between complex neural correlates and acrlong FC-based distances. A computational model inspired by multivariate visibility graphs (VGs) algorithm and acrlong ED is proposed to analyze quantitatively the brain network data. When applied to resting-state acrlong EEG signals from three groups [typically developing (acrshort TD), autism spectrum disorder autism spectrum disorder (ASD), and epilepsy (E)], the network topological properties (e.g., global efficiency, modularity, small worldness, and betweenness centrality) demonstrate variations in connectivity distance probabilities among brain regions (e.g., frontal, temporal, parietal, and occipital) via the model’s delay and connection distance parameters. The results showed a higher delay and skewed distribution toward short functional connections in ASD than in acrshort TD, while a lower delay in E than in ASD and acrshort TD. Additionally, ASD had more short-distance connections, while E had more long-distance connections compared to acrshort TD. ASD and E significantly overlapped over short-distance connections within the temporal lobe. In summary, the proposed model illustrates that delay parameter and connection distance obtained from brain network data have the potential to objectively identify and associate co-occurring neurological conditions (e.g., ASD and E).},
  archive      = {J_TCDS},
  author       = {Tanu Wadhera and Mufti Mahmud},
  doi          = {10.1109/TCDS.2023.3320243},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {1041-1050},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Computational model of functional connectivity distance predicts neural alterations},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effective motive profile compositions for cooperative ad-hoc
teams. <em>TCDS</em>, <em>16</em>(3), 1027–1040. (<a
href="https://doi.org/10.1109/TCDS.2023.3321924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individuals working within a team have unique attributes which lead to the emergence of different team dynamics that could promote or hinder team performance. Implicit motivation is one such aspect that brings diversity. Research in the Psychology literature has identified three motive profiles that characterize individuals as achievement, affiliation, and as power motivated. In this work, we personify agents with these motive profiles and analyze their performance through a series of experiments using a prey–predator task domain. Throughout the experiments, we analyzed the productivity of teams with different motivation compositions (heterogeneous teams) and homogeneous teams with motive profiles created by perturbing the strengths of the three original motive profiles. Furthermore, we investigated how much an agent has to suppress its motivation to pick a more rational goal and quantified this inhibition by introducing a new metric named Perceived Tension. Our experiments concluded that when working collaboratively, having teams with a diverse motivation portfolio is beneficial. It was also evident that diversely motivated teams perform well across different test settings making such teams robust to different task difficulty levels. Furthermore, results showed that predominantly affiliation-motivated teams are less tensed when working on collaborative easy tasks, however their productivity is significantly poor. The framework introduced in this article can be used to understand the implications for workplaces and leaders when putting together organic teams to create a productive, healthy, and relaxed environment for their employees.},
  archive      = {J_TCDS},
  author       = {Anupama Arukgoda and Erandi Lakshika and Michael Barlow and Kasun Gunawardana},
  doi          = {10.1109/TCDS.2023.3321924},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {1027-1040},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Effective motive profile compositions for cooperative ad-hoc teams},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Memristor-based neural network circuit of associative memory
with occasion setting. <em>TCDS</em>, <em>16</em>(3), 1016–1026. (<a
href="https://doi.org/10.1109/TCDS.2023.3321137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical and operant conditioning are rarely studied together in the neural network based on memristor. But in real life, they often happen at the same time. This article proposes a neural network circuit based on the memristor of associative memory with occasion setting. The designed circuit includes four modules: 1) an operant synapse module; 2) a response module; 3) a classical synapse module; and 4) a perceptron module. The first two modules constitute operant conditioning, and the remaining two modules constitute classical conditioning. The designed circuit combines operant conditioning with classical conditioning to realize a special associative memory through an occasion setting. Besides, it implements the function of learning, forgetting, and long-term memory on the basis of associative memory with occasion setting. The neural network circuit of associative memory with occasion setting is closer to the real situation and provides a reference for classical and operant conditioning.},
  archive      = {J_TCDS},
  author       = {Juntao Han and Xin Cheng and Guangjun Xie and Junwei Sun and Gang Liu and Zhang Zhang},
  doi          = {10.1109/TCDS.2023.3321137},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {1016-1026},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Memristor-based neural network circuit of associative memory with occasion setting},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning framework for modeling cognitive load from
small and noisy EEG data. <em>TCDS</em>, <em>16</em>(3), 1006–1015. (<a
href="https://doi.org/10.1109/TCDS.2023.3319305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern systems (e.g., assistive technology and self-driving) can place significant demands on the user’s working memory (WM), which can adversely impact performance (i.e., elevated risk of errors) and increase cognitive load (CL). Robust prediction of CL from electroencephalography (EEG) remains a challenge due to the small sample problem, noisy recordings, ineffective data representation, and lack of robust models. This article presents a holistic approach to developing a reliable prediction of CL. We used EEG data recorded following a modified Stenberg WM task in which four levels of CL were defined based on the encoding of 2, 4, 6, and 8 English characters. First, we address the problem of noise and “small sample” by generating large low-noise data using eigenspace-based bootstrap sampling and generative adversarial network (GAN). Second, we transform EEG recordings into spatial-spectral images to capture spatial information. Third, we built parameter-optimized acrlong CNN models to predict four levels of CL using single-frequency bands (i.e., $\theta $ , $\alpha $ , and $\beta$ ) and stacked (i.e., all three bands) representations. In our quest to provide interpretable models, we applied gradient-weighted class activation mapping (Grad-CAM) to our models to localize the brain regions responsible for the prediction of CL. Empirical analysis of models trained using $\theta $ , $\alpha $ , $\beta $ , and stacked representation show accuracy of 90%, 89%, 91%, and 94%, respectively. Grad-CAM visualizations showed that the prefrontal, cerebellum, frontal, and parietal areas have the highest contribution to the prediction of CL.},
  archive      = {J_TCDS},
  author       = {Felix Havugimana and Kazi Ashraf Moinudin and Mohammed Yeasin},
  doi          = {10.1109/TCDS.2023.3319305},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {1006-1015},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Deep learning framework for modeling cognitive load from small and noisy EEG data},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Matching-based capture-the-flag games for multiagent
systems. <em>TCDS</em>, <em>16</em>(3), 993–1005. (<a
href="https://doi.org/10.1109/TCDS.2023.3323572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Competition and cooperation among agents in multiagent systems can be effectively modeled as differential games. One of the typical tasks is capturing the flag, in which the agents can be divided into attackers’ alliance and defenders’ alliance with two-phase competitive behaviors. The attackers’ alliance aims to capture flags in the first phase and then return to the safe region in the second phase, while the defenders’ alliance aims to protect the flags and apprehend as many attackers as possible. Throughout the interaction, agents are actively involved in perception, cognitive learning, and the formulation of optimal decisions rooted in their acquired knowledge. Consequently, this article delves into the central challenges posed by capture-the-flag differential games, particularly in terms of task allocation and coordinated apprehension strategies among defenders. First, we use the Apollonius circle to transform the multiplayer capture-the-flag problems into one-defense-one or two-defense-one scenarios. By analyzing the advantages of cooperation between defenders, a two-stage joint optimal strategy is obtained. Moreover, we propose an approximation algorithm that achieves optimal task assignment, significantly reducing computational complexity. The performance and effectiveness of the proposed algorithm are demonstrated through numerical simulations.},
  archive      = {J_TCDS},
  author       = {Jiali Wang and Zhao Zhou and Xin Jin and Shuai Mao and Yang Tang},
  doi          = {10.1109/TCDS.2023.3323572},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {993-1005},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Matching-based capture-the-flag games for multiagent systems},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Characteristics-capturing neural dynamics for solving
time-dependent nonlinear equations with periodic noise. <em>TCDS</em>,
<em>16</em>(3), 984–992. (<a
href="https://doi.org/10.1109/TCDS.2023.3316776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, many dynamic systems show the characteristics of multiple-input-multiple-output and nonlinearity, which usually involve the solution of time-dependent nonlinear equations. This article focuses on the accurate solution of time-dependent nonlinear equations with periodic noises considered. In the hardware or numerical implementations of an actual system, inevitable internal disturbances or external factors caused by the changeable scene may greatly affect the accuracy of the solution. Most of the existing time-dependent anti-noise neural dynamic (ND) models can effectively suppress some constant noise, but they are not satisfactory when facing some periodic noise with high frequency. To solve this problem, a characteristics-capturing ND (CCND) model is designed, which considers the periodic noise from the perspective of harmonic expansion so as to effectively capture harmonic characteristics and eliminate them. Theoretical analysis proves the robustness of the CCND model with periodic noise disturbance considered. Moreover, numerical simulations and robotic experiments further confirm the effectiveness of the CCND model.},
  archive      = {J_TCDS},
  author       = {Mei Liu and Yafei Hu and Jiachang Li and Long Jin},
  doi          = {10.1109/TCDS.2023.3316776},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {984-992},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Characteristics-capturing neural dynamics for solving time-dependent nonlinear equations with periodic noise},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multichannel speech enhancement based on neural beamforming
and a context-focused post-filtering network. <em>TCDS</em>,
<em>16</em>(3), 973–983. (<a
href="https://doi.org/10.1109/TCDS.2023.3316301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both spatial and temporal contextual information are essential for the multichannel speech enhancement (MCSE) task. In this work, we propose a unified MCSE network composed of neural beamforming and a context-focused post-filtering network in order to fully exploit these two types of information. The network is used to estimate the optimum complex ideal ratio masks (cIRMs) which can more effectively utilize the phase information in the frequency domain to reconstruct the speech waveform. To assign adaptive weights to channels, we first adopt a dilated convolution-based network which simulates the beamforming on the original multichannel input spectrum as the front-end of the multichannel acoustic model. Furthermore, we propose a post-filtering network that inputs the suggested U-Net’s output to a convolutional long short-term memory (ConvLSTM) layer, as it can properly capture contextual information and spatial correlation information of features. We conduct experiments on the VOICES, CHiME-3, and WMIR data sets, respectively. Experiments show that, in various scenarios, the proposed algorithm shows improvements over the previous state-of-the-art algorithms in terms of PESQ, STOI, and SI-SNR.},
  archive      = {J_TCDS},
  author       = {Cong Pang and Jingjie Fan and Qifan Shen and Yue Xie and Chengwei Huang and Björn W. Schuller},
  doi          = {10.1109/TCDS.2023.3316301},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {973-983},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Multichannel speech enhancement based on neural beamforming and a context-focused post-filtering network},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How the brain achieves real-time vision: A spiking position
perception model. <em>TCDS</em>, <em>16</em>(3), 961–972. (<a
href="https://doi.org/10.1109/TCDS.2023.3317330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time visual perception is essential for animals to survive in a complicated natural environment and for robots to interact with moving targets. However, delays generally occur during the signal transfer and processing both for animals and robots, and these delays would produce errors during real-time interactions with the physical world. Natural facts have shown that animals can perfectly compensate for these pervasive delays. In this article, we propose a novel and effective position perception model (PPM) based on spiking neural networks (SNNs) to address this ambivalent situation in robotic vision systems. We investigate the performance of PPM by tracking a moving target. PPM can compensate for temporal delays in the system regardless of the target’s speed. We also present a deep version of PPM (dPPM). dPPM can handle some more complex situations and make long-term anticipations. We finally implement PPM on neuromorphic chips and test it on real dynamic vision sensor (DVS) data, and it can perform real-time or anticipative visual perceptions.},
  archive      = {J_TCDS},
  author       = {Kefei Liu and Jingjie Shang and Xiaoxin Cui and Chenglong Zou and Yisong Kuang and Kanglin Xiao and Yi Zhong and Yuan Wang},
  doi          = {10.1109/TCDS.2023.3317330},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {961-972},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {How the brain achieves real-time vision: A spiking position perception model},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continual robot learning using self-supervised task
inference. <em>TCDS</em>, <em>16</em>(3), 947–960. (<a
href="https://doi.org/10.1109/TCDS.2023.3315513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Endowing robots with the human ability to learn a growing set of skills over the course of a lifetime as opposed to mastering single tasks is an open problem in robot learning. While multitask learning approaches have been proposed to address this problem, they pay little attention to task inference. In order to continually learn new tasks, the robot first needs to infer the task at hand without requiring predefined task representations. In this article, we propose a self-supervised task inference approach. Our approach learns action and intention embeddings from self-organization of the observed movement and effect parts of unlabeled demonstrations and a higher level behavior embedding from self-organization of the joint action–intention embeddings. We construct a behavior-matching self-supervised learning objective to train a novel task inference network (TINet) to map an unlabeled demonstration to its nearest behavior embedding, which we use as the task representation. A multitask policy is built on top of the TINet and trained with reinforcement learning to optimize performance over tasks. We evaluate our approach in the fixed-set and continual multitask learning settings with a humanoid robot and compare it to different multitask learning baselines. The results show that our approach outperforms the other baselines, with the difference being more pronounced in the challenging continual learning setting, and can infer tasks from incomplete demonstrations. Our approach is also shown to generalize to unseen tasks based on a single demonstration in one-shot task generalization experiments.},
  archive      = {J_TCDS},
  author       = {Muhammad Burhan Hafez and Stefan Wermter},
  doi          = {10.1109/TCDS.2023.3315513},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {947-960},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Continual robot learning using self-supervised task inference},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A pyramidal spatial-based feature attention network for
schizophrenia detection using electroencephalography signals.
<em>TCDS</em>, <em>16</em>(3), 935–946. (<a
href="https://doi.org/10.1109/TCDS.2023.3314639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic signal classification is utilized in various medical and industrial applications, particularly in schizophrenia (SZ) diagnosis, one of the most prevalent chronic neurological diseases. SZ is a significant mental illness that negatively affects a person’s behavior by causing things like speech impairment and delusions. In this study, electroencephalography (EEG) signals, a noninvasive diagnostic technique, are being investigated to distinguish SZ patients from healthy people by proposing a pyramidal spatial-based feature attention network (PSFAN). The proposed PSFAN consists of dilated convolutions to extract multiscale deep features in a pyramidal fashion from 2-D images converted from 4-s EEG recordings. Then, each level of the pyramid includes a spatial attention block (SAB) to concentrate on the robust features that can identify SZ patients. Finally, all the SAB feature maps are concatenated and fed into dense layers, followed by a Softmax layer for classification purposes. The performance of the PSFAN is evaluated on two data sets using three experiments, namely, the subject-dependent, subject-independent, and cross-dataset. Moreover, statistical hypothesis testing is performed using Wilcoxon’s rank-sum test to signify the model performance. Experimental results show that the PSFAN statistically defeats 11 contemporary methods, proving its effectiveness for medical industrial applications. Source code: https://github.com/KarnatiMOHAN/PSFAN-Schizophrenia-Identification-using-EEG-signals .},
  archive      = {J_TCDS},
  author       = {Mohan Karnati and Geet Sahu and Abhishek Gupta and Ayan Seal and Ondrej Krejcar},
  doi          = {10.1109/TCDS.2023.3314639},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {935-946},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A pyramidal spatial-based feature attention network for schizophrenia detection using electroencephalography signals},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Selective multi-source domain adaptation network for
cross-subject motor imagery discrimination. <em>TCDS</em>,
<em>16</em>(3), 923–934. (<a
href="https://doi.org/10.1109/TCDS.2023.3314351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discriminating motor imagery with electroencephalogram (EEG)-based brain–computer interface (BCI) poses a challenge as it involves an extensive data acquisition phase that demands a substantial amount of effort from the user. To address this issue, one approach is to use unsupervised domain adaptation, where classification models are constructed using data from multiple subjects, and only the unlabeled data from the target user is used for model calibration. However, since brain patterns from motor imagery vary between individuals, the reliability of each subject must be considered when multiple subjects are used to build the classification model. Thus, in this article, we propose Selective-MDA that performs domain adaptation on each source subject and selectively limits influences based on their domain discrepancies. To evaluate our approach, we assess our results with two public dataset, BCI Competition IV IIa and the Autocalibration and Recurrent Adaptation dataset. We further investigate the effect of source selection by comparing the discrimination performance when different numbers of source domains are selected based on discrepancy measures. Our results demonstrate that Selective-MDA not only integrates multisource domain adaptation to cross-subject motor imagery discrimination but also highlights the impact of source domain selection when using data from multiple subjects for model training.},
  archive      = {J_TCDS},
  author       = {Juho Lee and Jin Woo Choi and Sungho Jo},
  doi          = {10.1109/TCDS.2023.3314351},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {923-934},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Selective multi-source domain adaptation network for cross-subject motor imagery discrimination},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph representation learning via contrasting cluster
assignments. <em>TCDS</em>, <em>16</em>(3), 912–922. (<a
href="https://doi.org/10.1109/TCDS.2023.3313206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of contrastive learning, unsupervised graph representation learning (GRL) has shown strong competitiveness. However, existing graph contrastive models typically either focus on the local view of graphs or take simple considerations of both global and local views. This may cause these models to overemphasize the importance of individual nodes and their ego networks, or to result in poor learning of global knowledge and affect the learning of local views. Additionally, most GRL models pay attention to topological proximity, assuming that nodes that are closer in graph topology are more similar. However, in the real world, close nodes may be dissimilar, which makes the learned embeddings incorporate inappropriate messages and thus lack discrimination. To address these issues, we propose a novel unsupervised GRL model by contrasting cluster assignments, called graph representation learning model via contrasting cluster assignment (GRCCA). To comprehensively explore the global and local views, it combines multiview contrastive learning and clustering algorithms with an opposite augmentation strategy. It leverages clustering algorithms to capture fine-grained global information and explore potential relevance between nodes in different augmented perspectives while preserving high-quality global and local information through contrast between nodes and prototypes. The opposite augmentation strategy further enhances the contrast of both views, allowing the model to excavate more invariant features. Experimental results show that GRCCA has strong competitiveness compared to state-of-the-art models in different graph analysis tasks.},
  archive      = {J_TCDS},
  author       = {Chun-Yang Zhang and Hong-Yu Yao and C. L. Philip Chen and Yue-Na Lin},
  doi          = {10.1109/TCDS.2023.3313206},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {912-922},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Graph representation learning via contrasting cluster assignments},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DAR-ILL: Double-attention refining and iterative labeling
learning for weakly supervised object detection. <em>TCDS</em>,
<em>16</em>(3), 899–911. (<a
href="https://doi.org/10.1109/TCDS.2023.3312679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised object detection (WSOD), which utilizes image-level annotated data sets to reduce labeling costs, has been gaining significant attention. Among the WSOD methods, multi-instance learning (MIL) has emerged as a popular solution. However, multiple instance learning (MIL) suffers from reduced accuracy in both localization and classification, as it focuses solely on the most discriminative local regions. To address this limitation, we propose a novel end-to-end MIL-based WSOD method named DAR- iterative labeling learning (ILL). Our approach leverages double-attention refining and iterative learning labeling to capture the region covering the entire object. The double-attention refining consists of two modules: 1) the attention aggregation module (AAM) and 2) the attention erasure module (AEM). AAM effectively combines spatial attention and channel attention to extract features with higher accuracy, while AEM selectively erases the most discriminative local region and subsequently expands the region to encompass the entire object. The iterative labeling learning (IIL) procedure allows us to identify additional positive instances by introducing a threshold that identifies regions capable of covering the object. To further enhance supervisory information and optimize confidence maps, we introduce the dense optimization module (DOM), which facilitates more effective training of all branches. Extensive experimental evaluations demonstrate the superiority of DAR- ILL, as evidenced by its competitive performance in terms of mAP and CorLoc, with particular emphasis on CorLoc.},
  archive      = {J_TCDS},
  author       = {Shan Zhong and Pengpeng Song and Lifan Zhou and Shengrong Gong and Gengsheng Xie},
  doi          = {10.1109/TCDS.2023.3312679},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {899-911},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {DAR-ILL: Double-attention refining and iterative labeling learning for weakly supervised object detection},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). LDCNet: A lightweight multiscale convolutional neural
network using local dense connectivity for image recognition.
<em>TCDS</em>, <em>16</em>(3), 888–898. (<a
href="https://doi.org/10.1109/TCDS.2023.3311171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (DCNNs) have made great contributions to the development of computer vision. Since the trained deep convolutional neural network (DCNN) models require a large amount of computing and storage resources to achieve high performance, it is usually difficult to deploy them on resource-limited systems. To address this problem, we propose a novel local dense connectivity (LDC) module to generate feature maps from cheap convolution operations. The LDC module constructs hierarchical and locally dense connections within a single layer. This construction promotes the reuse of features in network layers. As a result, it leads to the generation of multiscale feature maps and increases the receptive fields for each network layer. A basic architecture block called LDCBlock was designed based on the LDC module. By stacking this kind of block, we propose a kind of lightweight and efficient multiscale residual network named LDCNet. Moreover, to model the interdependencies between the channels of the LDC module and enhance the informativeness of diversified features, we also design a sparse squeeze-excitation (SE) module, which has fewer parameters and computations. Finally, the experiments based on CIFAR data sets, ImageNet data set, and a defect detection data set demonstrate that our LDCNet achieves competitive performance compared with the state-of-the-art models focusing on model compression.},
  archive      = {J_TCDS},
  author       = {Junyi Wang and Wentao Zhang and Jun Fu and Kechen Song and Qinggang Meng and Lin Zhu and Yuru Jin},
  doi          = {10.1109/TCDS.2023.3311171},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {888-898},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {LDCNet: A lightweight multiscale convolutional neural network using local dense connectivity for image recognition},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). High-quality auditory brainstem response acquisition in
motion via adaptive kalman filtering. <em>TCDS</em>, <em>16</em>(3),
877–887. (<a href="https://doi.org/10.1109/TCDS.2023.3308561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Auditory brainstem response (ABR) is a clinical auditory detection tool that can evaluate the function of the central auditory pathways through the brainstem, but is easy to be interfered with by noise, which requires subjects to keep quiet during tests. However, it is hard for children or adults that cannot cooperate to keep quiet for such a long time. Besides, the ABR test is time-consuming because thousands of trials are needed. In this study, an adaptive Kalman filtering (AKF) method was proposed to help with the ABR acquisition in the motion (chewing or mouth open). We first studied the feasibility of the AKF method by manually adding noise to electroencephalogram (EEG) trials that were used to acquire ABR on adult subjects. Then, we compared the performance of AKF with the traditionally used averaging (Ave) and artifact rejection (AR). The results showed that the AKF-based ABR achieved 96.16 ± 2.15% of the correlation coefficient and similar morphology as the Ave-based method in rest. In motion, the AKF-based ABRs had more recognizable characteristic waves, stable latencies, and higher wave V’s amplitudes than those of Ave or AR-based methods. It is believed that the AKF-based method provides the possibility of in-motion ABR acquisition.},
  archive      = {J_TCDS},
  author       = {Xin Wang and Haoshi Zhang and Jingqian Tan and Yangjie Xu and Poly Z. H. Sun and Junyu Ji and Jiafa Lu and Mingxing Zhu and Michael Chi Fai Tong and Subhas Chandra Mukhopadhyay and Guanglin Li and Shixiong Chen},
  doi          = {10.1109/TCDS.2023.3308561},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {877-887},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {High-quality auditory brainstem response acquisition in motion via adaptive kalman filtering},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving spiking neural network with frequency adaptation
for image classification. <em>TCDS</em>, <em>16</em>(3), 864–876. (<a
href="https://doi.org/10.1109/TCDS.2023.3308347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) are promising in energy-efficient brain-inspired devices for their rich spatio-temporal dynamics, bio-plausible encoding, and event-driven information processing. However, the existing SNNs for image classification have fixed firing thresholds for the neurons and do not consider the adaptive properties of the neurons. In this article, we propose a high-performance SNN composed of neurons with spike frequency adaptation (SFA-SNN). We replace the fixed firing threshold with dynamic firing thresholds and incorporate them into the differential equation of neuron membrane potential, and then build an SNN on Pytorch. In addition, we introduce a new function to approximate the derivative of spike activity to solve its nondifferentiable problem, so that the SNNs can be trained in spatio-temporal domain using the error backpropagation algorithm. We verify the image classification performance of the proposed SFA-SNN on the static data set (including MNIST and Fashion-MNIST) and neuromorphic data set (including CIFAR10-DVS and DVS128-Gesture), and the accuracy results, including 99.52% on MNIST, 92.40% on Fashion-MNIST, 71.90% on CIFAR10-DVS, and 96.67% on DVS128-Gesture. We believe this work can help us better understand the intelligent information processing of the brain.},
  archive      = {J_TCDS},
  author       = {Tao Chen and Lidan Wang and Jie Li and Shukai Duan and Tingwen Huang},
  doi          = {10.1109/TCDS.2023.3308347},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {864-876},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Improving spiking neural network with frequency adaptation for image classification},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A human-like siamese-based visual-tactile fusion model for
object recognition. <em>TCDS</em>, <em>16</em>(3), 850–863. (<a
href="https://doi.org/10.1109/TCDS.2023.3311871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human brain combines multisensory information, such as visual and tactile information, in a statistically optimal manner to achieve perceptual interaction with the outside world. This article proposes a siamese-based visual-tactile fusion model for human subjective perception clustering tasks of multidimensional tactile attribute objects. Specifically, it introduces a similar comparison structure and uses distance measurement to determine category labels to simulate the comparative decision-making mechanism in human perception processes. In the feature extraction and fusion stage, referring to the multilevel processing characteristics of human signals, statistical texture features, and empirical features are extracted from the original interactive information of the visual and tactile channels. These features are combined with the corresponding deep features encoded by the neural network to achieve the information fusion of vision and touch in an adaptive dynamic weighting way. The experimental evaluation results indicate that the proposed model performs well in predicting human subjective perception results, and visual-tactile fusion exhibits a significant perceptual enhancement effect compared to a single visual or tactile channel.},
  archive      = {J_TCDS},
  author       = {Fei Wang and Yucheng Li and Liangze Tao and Juan Wu and Gewen Huang},
  doi          = {10.1109/TCDS.2023.3311871},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {850-863},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A human-like siamese-based visual-tactile fusion model for object recognition},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continuous-time dynamic interaction network learning based
on evolutionary expectation. <em>TCDS</em>, <em>16</em>(3), 840–849. (<a
href="https://doi.org/10.1109/TCDS.2023.3305285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic networks, such as social networks and recommendation systems, are widespread in the real world. Graph representation learning has emerged as an effective strategy for analyzing such networks. However, the problem lies in the fact that many current techniques treat dynamic networks as static or discrete structures, while continuous-time approaches often lack the capabilities to effectively handle networks with low node repetition behavior. To alleviate these problems, we first treat dynamic networks as continuous-time interactions and then propose a novel method for the transductive interaction prediction task. Our approach incorporates two key aspects: 1) evolutionary expectation learning and 2) temporal dynamic learning. The former imparts guidance to the network’s evolution and endows node embedding with a more profound wealth of information, while the latter provides detailed insight into the intricate process of network evolution. Together, these two components provide a comprehensive understanding of network behavior and can efficiently handle dynamic networks with low node repetition behavior. Specifically, we utilize an asynchronous training process, starting with the training of a multievent embedding module that captures information about the evolutionary expectation of dynamic networks. Based on this foundation, we train a temporal multievent embedding module to map the network’s dynamic evolution onto node embedding representations. Furthermore, we design a temporal single-event module that effectively captures implicit long-term interaction dependencies of nodes. To evaluate the effectiveness of our proposed method, we evaluate its performance on four data sets and demonstrate its superior performance compared to the baselines.},
  archive      = {J_TCDS},
  author       = {Xiaobo Zhu and Yan Wu and Liying Wang and Hailong Su and Zhipeng Li},
  doi          = {10.1109/TCDS.2023.3305285},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {840-849},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Continuous-time dynamic interaction network learning based on evolutionary expectation},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An easy-to-use assessment system for spasticity severity
quantification in post-stroke rehabilitation. <em>TCDS</em>,
<em>16</em>(3), 828–839. (<a
href="https://doi.org/10.1109/TCDS.2023.3304352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spasticity is a motor disorder integrated in the upper motor neuron syndrome resulting from central nerve diseases such as stroke. The multifactorial nature of spasticity manifestations leads to the inter-rater and intrarater reliability of clinical assessment, hence, the objective severity quantification of the spastic hypertonia has attracted significant attention in the context of post-stroke rehabilitation. Here, we developed a novel assessment system to reliably identify the exaggerated muscle tone and quantitatively estimate the symptom severity in patients with upper limb spasticity. Twenty subjects with post-stroke spasticity (53.0 ± 13.9 years old) and ten age-matched healthy subjects performed the passive stretch movements under the single-task and dual-task protocols while wearing an exoskeletal measurement device developed by us. A preliminary identification layer was designed to discriminate the pathological electrophysiological outputs of the upper extremity muscles by using the long short-term memory (LSTM) networks. In the next layer, the severity quantification models can be triggered in parallel, aiming at evaluating the neural and non-neural level pathologies underlying the spastic resistance manually percepted by clinicians, where the muscle activation/co-activation features, kinematic departure, and biomechanical characteristics were considered to improve the clinical relevance. Based on these single-level decisions, the third layer was constructed as an integrated model to yield a more comprehensive quantification of the symptom severity. The experimental validation of the proposed system demonstrated good reliability in discriminating the spastic hypertonia from the normal muscle tone, as well as strong agreement of the quantitative severity estimations with the commonly accepted clinical scales for the neural level ${(R = 0.79,\;P = 2.79e - 5)}$ , non-neural level ${(R = 0.75,\;P = 1.62e - 4)}$ , and integrated level ${(R = 0.86,\;P = 9.86e - 7)}$ . In conclusion, the proposed assessment system holds great promise to provide clinicians with an easy-to-use tool as suitable support for spasticity diagnosis, disease monitoring, and treatment adjustment.},
  archive      = {J_TCDS},
  author       = {Chen Wang and Liang Peng and Zeng-Guang Hou and Pu Zhang and Peng Fang},
  doi          = {10.1109/TCDS.2023.3304352},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {828-839},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An easy-to-use assessment system for spasticity severity quantification in post-stroke rehabilitation},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated identification of the retinogeniculate visual
pathway using a high-dimensional tractography atlas. <em>TCDS</em>,
<em>16</em>(3), 818–827. (<a
href="https://doi.org/10.1109/TCDS.2023.3304529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The retinogeniculate visual pathway (RGVP) plays an important role in the visual system. Diffusion MRI-based tractography has been successfully used to identify RGVP. However, challenges of RGVP tractography remain because of its highly curved path and intricate anatomical environment. One of the key challenges is the large false-positive fibers generated from RGVP tractography that requires the labor costs to hand-draw ROIs for fiber filtering. Therefore, we presented a pipeline to enable automated RGVP identification in diffusion magnetic resonance imaging tractography. First, we generated a tractography-based RGVP atlas. Herein, the multifiber unscented Kalman filter tractography was performed using high-resolution data from 50 subjects. Then, we transformed the 50 tractography cases into a common space and implemented data-driven fiber clustering to group the neighboring fibers with similar trajectories into one cluster. Two experienced anatomists were responsible for RGVP annotation in the tractography atlas. Second, the high-dimensional RGVP atlas was applied to identify subject-specific RGVP in testing data sets and two patients with different scanning parameters. Experimental results showed that our automatic identification results have ideal colocalization with expert manual identification in terms of Hausdorff distance, fiber distance, and visualization. Therefore, the proposed method provides an efficient tool for analyzing large-scale data sets in vision-related neuroscience research.},
  archive      = {J_TCDS},
  author       = {Qingrun Zeng and Jiahao Huang and Jianzhong He and Shengwei Chen and Lei Xie and Zan Chen and Wenlong Guo and Sun Yao and Mengjun Li and Mingchu Li and Yuanjing Feng},
  doi          = {10.1109/TCDS.2023.3304529},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {818-827},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Automated identification of the retinogeniculate visual pathway using a high-dimensional tractography atlas},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). R-SNN: Region-based spiking neural network for object
detection. <em>TCDS</em>, <em>16</em>(3), 810–817. (<a
href="https://doi.org/10.1109/TCDS.2023.3311634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to their event-driven nature, spiking neural networks (SNNs) enjoy great energy efficiency and are believed to be a viable solution to the power wall problem. Due to difficulty in direct training, researchers have proposed indirect conversion-based methods, which allow SNNs to achieve comparable accuracy to their original nonspiking counterparts. However, most of these methods focus on classification problems; object detection with SNN, which involves a more challenging regression problem, remains an open research problem. The only existing SNN-based object detection method Spiking YOLO relied on a modified integrate-and-fire (IF) neuron model with two firing thresholds, which is not supported by most neuromorphic hardware. In order to run object detection on neuromorphic hardware, we propose a region-based SNN (R-SNN) with widely adopted IF neurons. The bounding box regressor uses mirror output neurons along with the original output neurons to represent both positive and negative bounding box offsets, and decodes the output spike trains with a simple bounding box recovery algorithm to recover real-valued bounding box offsets. Moreover, we deploy our R-SNN on our neuromorphic computing system “Darwin Mouse” to develop a low-power object detection application, demonstrating the feasibility of applying our method to real-world neuromorphic hardware with limited arithmetic precision. Experiments show that our R-SNN achieves a mean average precision (mAP) of 63.1% on VOC 2007, improving the state of the art (achieved by Spiking YOLO) by more than 11%.},
  archive      = {J_TCDS},
  author       = {Xiaobo Jin and Ming Zhang and Rui Yan and Gang Pan and De Ma},
  doi          = {10.1109/TCDS.2023.3311634},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {810-817},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {R-SNN: Region-based spiking neural network for object detection},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bioinspired memristive neural network circuit design of
cross-modal associative memory. <em>TCDS</em>, <em>16</em>(2), 794–808.
(<a href="https://doi.org/10.1109/TCDS.2023.3303653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of brain-like artificial intelligence is based on the cognitive functions of the brain, which are influenced by the cross-modal interactions of learning and memory. Inspired by the neural mechanism and biological phenomena of associative memory in Drosophila, this article proposes a bioinspired neural network and memristive circuit of cross-modal associative memory to mimic the cross-modal interaction during brain association. The designed circuit mainly consists of three modules: 1) the threshold module, which uses to determine the threshold level of conditioned stimuli (CS) that the brain can use to trigger learning and memory effects; 2) the synergy module, which performs cross-modal synergy processing of CS signals; and 3) the synapse and neuron module, which uses memristors to mimic synaptic weights and output neurons for associative learning. According to the different intensities of the input stimuli signals under conditioning, the proposed circuit implements the innovative functions, such as unimodal learning with threshold, cross-modal reinforcement, cross-modal facilitation, and cross-modal memory transfer. The simulation results in PSPICE show that the proposed circuit exhibits cross-modal synergy and transfer interactions, and it also provides further references for the research and development of bionic intelligent robots and brain-like intelligence.},
  archive      = {J_TCDS},
  author       = {Jinying Liu and Feier Xiong and Yue Zhou and Shukai Duan and Xiaofang Hu},
  doi          = {10.1109/TCDS.2023.3303653},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {794-808},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Bioinspired memristive neural network circuit design of cross-modal associative memory},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Electroencephalography connectivity assesses cognitive
disorders of autistic children during game-based social interaction.
<em>TCDS</em>, <em>16</em>(2), 782–793. (<a
href="https://doi.org/10.1109/TCDS.2023.3297609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluation of the cognitive function of autistic children after early intervention has become important for providing promising biomarkers to assess the improvement after cognitive intervention. In this study, we have developed a game-based platform with electroencephalography (EEG) signal recording to investigate gaze following in children. Typically developing (TD) children and children with autism spectrum disorder (ASD) were recruited to participate in the gaze-following game with 12-channel EEG signal recording; the corresponding brain oscillations, event-related potentials (ERPs), and event-related spectral perturbations were analyzed. Brain functional connections were also evaluated with phase-lag indices (PLIs). The results showed that, relative to the TD children, autistic children had smaller N170, P3a, and late positive potential (LPP) during gaze following, suggesting that they ignored low-level features of social information. Further, autistic children showed increased alpha and beta desynchronization and decreased theta synchronization and functional connectivity. With the development of human–computer interface, physiological signals are essential in future implementation of game-based interfaces to further investigate underlying mechanisms and effective biomarkers. We have demonstrated the possibility of incorporating EEG signals measured by portable applications to evaluate cognitive and social performance in autistic children. This approach has superior flexibility compared with other neuroimaging techniques.},
  archive      = {J_TCDS},
  author       = {Yi-Li Tseng and Hong-Hsiang Liu and Yen-Nan Chiu and Chia-Hsin Lee and Wen-Che Tsai and Yang-Min Lin and Yi-Ling Chien},
  doi          = {10.1109/TCDS.2023.3297609},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {782-793},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Electroencephalography connectivity assesses cognitive disorders of autistic children during game-based social interaction},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint linguistic steganography with BERT masked language
model and graph attention network. <em>TCDS</em>, <em>16</em>(2),
772–781. (<a href="https://doi.org/10.1109/TCDS.2023.3296413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generation-based linguistic steganography embeds secret information by generating high-quality text sequences according to the information. However, this method may cause low statistical feature consistency between normal text and the text with embedded information, thus making the steganalysis models detect the steganographic texts easier. And conditional generation-based text steganography, which embeds secret information into the suitable position in the text sequence to improve the statistical feature consistency, has a disadvantage in embedding capacity. To solve the problems above, in this article, we propose a joint linguistic steganography method that combines conditional generation-based steganography with substitution-based steganography that uses the BERT pretrained model. The graph attention network is also used in this method to extract and analyze spatial features of the text sequence, which are taken as auxiliary information in the text generation process based on temporal features. The comparative experimental results with other models show that our steganography model improves the embedding capacity of conditional generation-based text steganography while ensuring the consistency of text features, and is superior to most state-of-the-art models in imperceptibility and anti-steganalysis ability.},
  archive      = {J_TCDS},
  author       = {Changhao Ding and Zhangjie Fu and Qi Yu and Fan Wang and Xianyi Chen},
  doi          = {10.1109/TCDS.2023.3296413},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {772-781},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Joint linguistic steganography with BERT masked language model and graph attention network},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). JLCSR: Joint learning of compactness and separability
representations for few-shot classification. <em>TCDS</em>,
<em>16</em>(2), 757–771. (<a
href="https://doi.org/10.1109/TCDS.2023.3297578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot classification (FSC) has aroused increasing attentions over years, which attempts to perform classification given a few labeled samples. In the context of transfer-learning for settling FSC, learning a general feature representation is very vital. For this purpose, our work focus on mining more information from the supervised data jointly provided by a certain amount of annotated samples and its corresponding self-supervised learning (SSL) task. To this end, we prove that the supervised losses of cross-entropy (CE) and supervised contrastive (SC) are, respectively, good at compactness and separability representations (SRs). On the basis of the above theory analysis, we further propose the joint learning of compactness and SRs (JLCSRs) for FSC. Specifically, for both original supervised data and its augmentation ones in the SSL task, it first, respectively, constructs CE loss and SC loss in the feature space. Then, joint learning is performed on the backbone network with the linear combination of above losses. The parameters of the backbone network are finally fixed to do the FSC evaluation. Extensive experiments on FSC benchmarks have demonstrated that the compactness and SRs learning can complement with each other and our method can reach comparable results with other state-of-the-art methods},
  archive      = {J_TCDS},
  author       = {Sai Yang and Fan Liu and Shaoqiu Zheng and Ying Tan},
  doi          = {10.1109/TCDS.2023.3297578},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {757-771},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {JLCSR: Joint learning of compactness and separability representations for few-shot classification},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Motion learning for musculoskeletal robots based on
cortex-inspired motor primitives and modulation. <em>TCDS</em>,
<em>16</em>(2), 744–756. (<a
href="https://doi.org/10.1109/TCDS.2023.3293097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Musculoskeletal robots have structural advantages of flexibility, robustness, and compliance. However, the control of such musculoskeletal robots is challenging. In particular, the efficiency and generalization of motion learning for such robots are still limited. Inspired by motor preparation theories of the motor cortex and motor primitives in neuroscience, a novel neuromuscular control method with high learning efficiency and great generalization is proposed. First, a recurrent neural network (RNN)-based neuromuscular controller is proposed, which autonomously evolves from the initial state of neurons to generate muscle excitations. Second, the motor primitive of initial states in an RNN is proposed and constructed as common knowledge for muscle control. Third, a motion learning method for the modulation of motor primitives is proposed. In the experiments, the proposed method is validated by a redundant musculoskeletal robot and compared with related methods. It demonstrates better performance in terms of learning efficiency, accuracy, and generalization. In addition, the fault tolerance of initial states is analyzed and the robustness to noise is demonstrated.},
  archive      = {J_TCDS},
  author       = {Xiaona Wang and Jiahao Chen and Wei Wu},
  doi          = {10.1109/TCDS.2023.3293097},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {744-756},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Motion learning for musculoskeletal robots based on cortex-inspired motor primitives and modulation},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hybrid end-to-end spatiotemporal attention neural network
with graph-smooth signals for EEG emotion recognition. <em>TCDS</em>,
<em>16</em>(2), 732–743. (<a
href="https://doi.org/10.1109/TCDS.2023.3293321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, physiological data such as electroencephalography (EEG) signals have attracted significant attention in affective computing. In this context, the main goal is to design an automated model that can assess emotional states. Lately, deep neural networks have shown promising performance in emotion recognition tasks. However, designing a deep architecture that can extract practical information from raw data is still a challenge. Here, we introduce a deep neural network that acquires interpretable physiological representations by a hybrid structure of spatiotemporal encoding and recurrent attention network blocks. Furthermore, a preprocessing step is applied to the raw data using graph signal processing tools to perform graph smoothing in the spatial domain. We demonstrate that our proposed architecture exceeds state-of-the-art results for emotion classification on the publicly available DEAP data set. To explore the generality of the learned model, we also evaluate the performance of our architecture toward transfer learning (TL) by transferring the model parameters from a specific source to other target domains. Using DEAP as the source data set, we demonstrate the effectiveness of our model in performing cross-modality TL and improving emotion classification accuracy on DREAMER and the emotional English word (EEWD) data sets, which involve EEG-based emotion classification tasks with different stimuli.},
  archive      = {J_TCDS},
  author       = {Shadi Sartipi and Mastaneh Torkamani-Azar and Mujdat Cetin},
  doi          = {10.1109/TCDS.2023.3293321},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {732-743},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A hybrid end-to-end spatiotemporal attention neural network with graph-smooth signals for EEG emotion recognition},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-stage grasp detection method for robotics using point
clouds and deep hierarchical feature learning network. <em>TCDS</em>,
<em>16</em>(2), 720–731. (<a
href="https://doi.org/10.1109/TCDS.2023.3289987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When human beings see different objects, they can quickly make correct grasping strategies through brain decisions. However, grasp, as the first step of most manipulation tasks, is still an open issue in robotics. Although many detection methods have been proposed to take RGB-D images or point clouds as input and output grasp candidates, these methods are still limited to algorithm robustness, such as network performance and graspable objects. In this article, a two-stage grasp detection method is proposed, in which we first use point clouds to train the deep hierarchical feature learning network, which can better capture features of grasped points. We also consider the distribution and discrimination of grasps to construct samples. The score of point clouds is related to the quality of the relevant grasp sample. The quality is given by several grasp metrics applied to the grasp samples obtained from the YCB data set. In the second stage, the network is used to evaluate the grasp candidates sampled from the preprocessed point clouds. The extensive simulation and real-scene experiment show that our grasp detection algorithm achieves satisfactory performance in both single and multiple objects situations. The generalization and scalability of our model also perform well under different conditions.},
  archive      = {J_TCDS},
  author       = {Xiaofeng Liu and Congyu Huang and Jie Li and Weiwei Wan and Chenguang Yang},
  doi          = {10.1109/TCDS.2023.3289987},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {720-731},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Two-stage grasp detection method for robotics using point clouds and deep hierarchical feature learning network},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using humanoid robots to obtain high-quality motor imagery
electroencephalogram data for better brain–computer interaction.
<em>TCDS</em>, <em>16</em>(2), 706–719. (<a
href="https://doi.org/10.1109/TCDS.2023.3289845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The electroencephalogram (EEG) signal from motor imagery (MI) is used to drive brain–computer interaction (BCI). However, users usually are not adept at performing MI, which leads to low-quality EEG signals and decreases the performance of BCI applications. The humanoid robot stimulation approach can guide users in performing MI more proficiently by increasing the cortico-spinal excitability and improving the discrimination of event-related desynchronization patterns during MI tasks. Compared to the traditional stimulation modes, our proposed humanoid robot stimulation mode can activate higher quality MI EEG signals. We use convolutional neural network and long short-term memory algorithm for the extraction of EEG features and classification. The results showed that the CNN-LSTM can achieve the highest classification accuracy (93.7% ±1.7%) in the humanoid robot stimulation mode, and it outperformed all other classifier-stimulation mode combinations. This demonstrates the effectiveness and feasibility of using a humanoid robot in real-scene MI-BCI applications, such as service robots or rehabilitation system for person with motor disabilities.},
  archive      = {J_TCDS},
  author       = {Shiwei Cheng and Jialing Wang and Jieming Tian and Anjie Zhu and Jing Fan},
  doi          = {10.1109/TCDS.2023.3289845},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {706-719},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Using humanoid robots to obtain high-quality motor imagery electroencephalogram data for better Brain–Computer interaction},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TL-P3GAN: An efficient temporal-learning-based generative
adversarial network for precise p300 signal generation for p300
spellers. <em>TCDS</em>, <em>16</em>(2), 692–705. (<a
href="https://doi.org/10.1109/TCDS.2023.3288201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of data imbalance among target and nontarget classes is inherent in the oddball paradigm-based P300 speller. This class imbalance is a critical issue and requires advanced rigorous learning to deal with. Conventionally, data level-like sampling approaches and algorithmic level-like ensemble approaches had been attempted in past research for data augmentation. However, information loss, overfitting, and subject variability were their major pitfalls. Alternatively, generative adversarial network (GAN)-based data augmentation managed to alleviate information loss but exhibits problems of overfitting and subject variability due to lack of temporal learning. To compensate for those problems, the authors have proposed novel temporal learning-based GAN (TL-P3GAN) to generate precise P300 signals and augment the minority class, i.e., P300. The TL-P3GAN comprises a novel contribution of multiscale morphological learning in both generator and discriminator. Moreover, the multiscale hybrid model in the generator learns multiresolution morphological information and considers sample-wise latency information from the original P300. The effectiveness of TL-P3GAN was confirmed by qualitative and quantitative evaluation metrics with two standard data sets. Further, the work is extended to analyze the effect of generated P300 signals on P300 classification and found significant performance improvement of 8%–16% with both data sets in comparison with existing conventional and GAN-based data augmentation approaches.},
  archive      = {J_TCDS},
  author       = {Vibha Bhandari and Narendra D. Londhe and Ghanahshyam B. Kshirsagar},
  doi          = {10.1109/TCDS.2023.3288201},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {692-705},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {TL-P3GAN: An efficient temporal-learning-based generative adversarial network for precise p300 signal generation for p300 spellers},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supervised meta-reinforcement learning with trajectory
optimization for manipulation tasks. <em>TCDS</em>, <em>16</em>(2),
681–691. (<a href="https://doi.org/10.1109/TCDS.2023.3286465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from small amounts of samples with reinforcement learning (RL) is challenging in many tasks, especially, in real-world applications, such as robotics. Meta-RL (meta-RL) has been proposed as an approach to address this problem by generalizing to new tasks through experience from previous similar tasks. However, these approaches generally perform meta-optimization by focusing direct policy search methods on validation samples from adapted policies, thus, requiring large amounts of on-policy samples during meta-training. To this end, we propose a novel algorithm called supervised meta-RL with trajectory optimization (SMRL-TO) by integrating model-agnostic meta-learning (MAML) and iterative LQR (iLQR)-based trajectory optimization. Our approach is designed to provide online supervision for validation samples through iLQR-based trajectory optimization and embed simple imitation learning into the meta-optimization rather than policy gradient steps. This is actually a bi-level optimization that needs to calculate several gradient updates in each meta-iteration, consisting of off-policy RL in the inner loop and online imitation learning in the outer loop. SMRL-TO can achieve significant improvements in sample efficiency without human-provided demonstrations, due to the effective supervision from iLQR-based trajectory optimization. In this article, we describe how to use iLQR-based trajectory optimization to obtain labeled data and then how leverage them to assist the training of meta-learner. Through a series of robotic manipulation tasks, we further show that compared with the previous methods, the proposed approach can substantially improve sample efficiency and achieve better asymptotic performance.},
  archive      = {J_TCDS},
  author       = {Lei Wang and Yunzhou Zhang and Delong Zhu and Sonya Coleman and Dermot Kerr},
  doi          = {10.1109/TCDS.2023.3286465},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {681-691},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Supervised meta-reinforcement learning with trajectory optimization for manipulation tasks},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UAC: Offline reinforcement learning with uncertain action
constraint. <em>TCDS</em>, <em>16</em>(2), 671–680. (<a
href="https://doi.org/10.1109/TCDS.2023.3287987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offline reinforcement learning (RL) algorithms promise to learn policies directly from offline data sets without environmental interaction. This arrangement enables successful RL applications in the real world, particularly in robots and autonomous driving, where sampling is costly and dangerous. However, the existing offline RL algorithms suffer from insufficient performance attributed to extrapolation error caused by out-of-distribution (OOD) actions. In this work, we propose an offline RL algorithm with an uncertain action constraint (UAC). The design principle of UAC is to minimize the extrapolation error via eliminating unknown and uncertain actions. Concretely, we first theoretically analyze the effects of different types of actions on the extrapolation error. Based on this, we propose an action-constrained strategy that exploits the uncertainty of the environmental dynamics model to eliminate unknown and uncertain actions in the $Q$ -value evaluation process. Furthermore, the convex combination of trajectory information and Gaussian noise is novelly leveraged to enhance the generation probability of the optimal actions. Finally, we carry out the comparison and ablation experiments on the standard D4RL data set. Experimental results indicate that UAC achieves competitive performance, especially in the field of robotic manipulation.},
  archive      = {J_TCDS},
  author       = {Jiayi Guan and Shangding Gu and Zhijun Li and Jing Hou and Yiqin Yang and Guang CHen and Changjun Jiang},
  doi          = {10.1109/TCDS.2023.3287987},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {671-680},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {UAC: Offline reinforcement learning with uncertain action constraint},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A contour detection method based on the projective coding
model of the visual cortex information flow. <em>TCDS</em>,
<em>16</em>(2), 660–670. (<a
href="https://doi.org/10.1109/TCDS.2023.3285909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on the information flow partition projection characteristics of P-type and M-type ganglion cells (referred to as P cells and M cells) and the neural sparse coding mechanism, this article proposed a new method for image contour detection. First, we considered the difference between M cells and P cells in detail sensitivity and the information transmission of different visual signals. The parallel visual pathway was constructed to simulate the prelevel characteristics of the V1 layer to obtain the primary contour response. Then, we introduced the orientation sensitivity and stimulus response difference of the visual receptive field to construct the visual information difference enhancement model. In consideration of the visual attention mechanism, we proposed an adaptive size sparse coding network model that simulates the prelevel characteristics of the V4 layer to intelligently focus the target contour features. At the same time, de-redundancy was performed to obtain the fine feature image. Finally, the hierarchical information feedback fusion was built, and the fine feature image was used to correct the primary contour response to obtain complete contour detection results. Taking the BSDS500 dataset as experimental objects, the results showed that the proposed method exhibits an effective tradeoff between contour extraction and texture suppression.},
  archive      = {J_TCDS},
  author       = {Zhefei Cai and Rui Yang and Yingle Fan and Wei Wu},
  doi          = {10.1109/TCDS.2023.3285909},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {660-670},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A contour detection method based on the projective coding model of the visual cortex information flow},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph-theory-based multilevel cortical functional
connectivity developmental analysis. <em>TCDS</em>, <em>16</em>(2),
650–659. (<a href="https://doi.org/10.1109/TCDS.2023.3285771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional connectivity (FC) is an efficient measurement to describe brain’s traits in information processing. But FCs at developmental stages from infancy to adolescence usually have severe individual variance, may affect the FC characterization analysis. Yet few past studies try to address this problem. In this article, we select quiet sleep (QS) and nonrapid eye movement (NREM) period electroencephalogram (EEG) of 42 healthy subjects from 0 to 17 years old for study. Random network combined with stability measurement using Shannon entropy (SE) is used to construct the individual-level FC (ILFC), which can describe the individual brain’s interaction. Majority voting is applied to construct the group-level FC (GLFC), which can describe the core part of ILFCs within the same age. Based on FCs, graph theory and statistical analysis are further applied, where the following conclusions are observed: 1) the $\beta $ band of EEG is most important in showing the age related variance of functional separation and integration; 2) 3 months–3 years old is the transition period from intrasubnetwork interacting mode to intersubnetwork interacting mode; 3) the phenomenon of complementation and overlay among GLFCs in different bands can be found; and 4) the GLFC’s network centrality becomes not obvious as age increases during 1–14 years old.},
  archive      = {J_TCDS},
  author       = {Keyan Pan and Tiejia Jiang and Runze Zheng and Tianlei Wang and Feng Gao and Jiuwen Cao},
  doi          = {10.1109/TCDS.2023.3285771},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {650-659},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Graph-theory-based multilevel cortical functional connectivity developmental analysis},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A decentralized communication framework based on dual-level
recurrence for multiagent reinforcement learning. <em>TCDS</em>,
<em>16</em>(2), 640–649. (<a
href="https://doi.org/10.1109/TCDS.2023.3281878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing communication channels for multiagent is a feasible method to conduct decentralized learning, especially in partially observable environments or large-scale multiagent systems. In this work, a communication model with dual-level recurrence is developed to provide a more efficient communication mechanism for the multiagent reinforcement learning field. The communications are conducted by a gated-attention-based recurrent network, in which the historical states are taken into account and regarded as the second-level recurrence. We separate communication messages from memories in the recurrent model so that the proposed communication flow can adapt changeable communication objects in the case of limited communication, and the communication results are fair to every agent. We provide a sufficient discussion about our method in both partially observable and fully observable environments. The results of several experiments suggest our method outperforms the existing decentralized communication frameworks and the corresponding centralized training method.},
  archive      = {J_TCDS},
  author       = {Xuesi Li and Jingchen Li and Haobin Shi and Kao-Shing Hwang},
  doi          = {10.1109/TCDS.2023.3281878},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {640-649},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A decentralized communication framework based on dual-level recurrence for multiagent reinforcement learning},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Does the brain infer invariance transformations from graph
symmetries? <em>TCDS</em>, <em>16</em>(2), 628–639. (<a
href="https://doi.org/10.1109/TCDS.2023.3281406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The invariance of natural objects under perceptual changes is possibly encoded in the brain by symmetries in the graph of synaptic connections. The graph can be established via unsupervised learning in a biologically plausible process across different perceptual modalities. This hypothetical encoding scheme is supported by the correlation structure of naturalistic audio and image data and it predicts a neural connectivity architecture which is consistent with many empirical observations about primary sensory cortex.},
  archive      = {J_TCDS},
  author       = {Helmut Linde},
  doi          = {10.1109/TCDS.2023.3281406},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {628-639},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Does the brain infer invariance transformations from graph symmetries?},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diagnosis of early mild cognitive impairment based on
associated high-order functional connection network generated by
multimodal MRI. <em>TCDS</em>, <em>16</em>(2), 618–627. (<a
href="https://doi.org/10.1109/TCDS.2023.3283406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mild cognitive impairment (MCI) is highly likely to convert to Alzheimer’s disease (AD). The main approach to identifying MCI is using a functional connection network (FCN). Traditional FCN is used to study the correlation between two brain regions, but it lacks deeper brain interaction information. Neuroscientists found the internal functional activity pattern in the human brain is characterized by sparse, modular, and overlapping structures, and the FCN is restricted by the brain structural connection network (SCN). They can improve the estimation accuracy of FCN. Therefore, this article first constructs low order FCN (LFCN) based on brain sparse, modular, and overlapping activity patterns. Then, new high-order FCN (HFCN) is proposed based on the restrictive relationship between SCN and FCN. To combine high robustness of LFCN with high sensitivity of HFCN, a new combination strategy of LFCN and HFCN is proposed. It integrates the idea of brain modular and overlapping with the restricted relationship between SCN and FCN. Finally, the experimental results show that in early MCI (EMCI) recognition the best classification performance is acquired with an accuracy of 91.42%, which is better than similar methods. This method will be instrumental in the early recognition of clinical MCI.},
  archive      = {J_TCDS},
  author       = {Weiping Wang and Shunqi Zhang and Zhen Wang and Xiong Luo and Ping Luan and Alexander Hramov and Jürgen Kurths and Chang He and Jianwu Li},
  doi          = {10.1109/TCDS.2023.3283406},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {618-627},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Diagnosis of early mild cognitive impairment based on associated high-order functional connection network generated by multimodal MRI},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neuro-inspired motion control of a soft myriapod robot.
<em>TCDS</em>, <em>16</em>(2), 606–617. (<a
href="https://doi.org/10.1109/TCDS.2023.3284399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Centipedes exploit soft structures to move efficiently in complex environments. These abilities have promoted scientists to develop centipede-like robots with soft robot technologies. In this article, a centipede-like robot constructed by multiple body segments connected in series is designed. Each body segment features a pair of antagonistic artificial muscles and four electro-adhesive feet, which plays the role of the basic motion unit of the centipede-like robot and can be regarded as a soft myriapod robot. To accomplish the desired movement tasks for the soft myriapod robot, this article proposes a neuro-inspired hierarchical motion control scheme according to the functions of the centipede brain. In the proposed control scheme, a mushroom body-inspired controller with an error-based learning mechanism is adopted for the soft actuator motion control, and a central complex-inspired deep reinforcement learning algorithm is designed for the robot motion selection. Finally, the effectiveness of the proposed motion control scheme is verified via both numerical studies and experiments.},
  archive      = {J_TCDS},
  author       = {Qinyuan Ren and Wenxin Zhu and Jiawei Cao and Wenyu Liang},
  doi          = {10.1109/TCDS.2023.3284399},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {606-617},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Neuro-inspired motion control of a soft myriapod robot},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph-theory-based EEG source connectivity for assessing
biomechanical performance in transfemoral amputees with vibrotactile
feedback. <em>TCDS</em>, <em>16</em>(2), 595–605. (<a
href="https://doi.org/10.1109/TCDS.2023.3281877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has shown that the complex and dynamic behavior of isolated regions is necessary for the proper functioning of the human brain. To this end, network neuroscience evaluates the neural channels that respond to diverse motor tasks and assesses the brain’s functionality. However, the utilization of network neuroscience has not been done to map out the cortical sources in lower limb amputee’s postural control. Our earlier research shows that transfemoral amputees’ fronto-central region and secondary somatosensory cortex have substantial functional connections while they are balancing with vibrotactile feedback. In this work, we advanced to investigate the cortical sources of transfemoral amputee’s postural stability improvement with vibrotactile feedback. The results highlight the predominance of cortical sources in frontal and parietal lobe in alpha frequency band. The observation shows the connectivity of left superiorparietal, lateralorbitofrontal and right rostranteriorcingulate, supramarginal, and inferiorparietal sources. Moreover, the node strength of these cortical sources significantly changes ( $p$ &lt;0.05, ANOVA) with the effect of feedback and visual input. In future, it could be interesting to develop some neural feedback techniques intended to improve the postural performance of lower limb amputees by making these cortical sources more adaptable via local stimulation techniques.},
  archive      = {J_TCDS},
  author       = {Saurabh Kumar and Aayushi Khajuria and Deepak Joshi},
  doi          = {10.1109/TCDS.2023.3281877},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {595-605},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Graph-theory-based EEG source connectivity for assessing biomechanical performance in transfemoral amputees with vibrotactile feedback},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coupling visual semantics of artificial neural networks and
human brain function via synchronized activations. <em>TCDS</em>,
<em>16</em>(2), 584–594. (<a
href="https://doi.org/10.1109/TCDS.2023.3287184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural networks (ANNs), originally inspired by biological neural networks (BNNs), have achieved remarkable successes in many tasks, such as visual representation learning. However, whether there exists semantic correlations/connections between the visual representations in ANNs and those in BNNs remains largely unexplored due to both the lack of an effective tool to link and couple two different domains, and the lack of a general and effective framework for representing the visual semantics in BNNs such as human functional brain networks (FBNs). To answer this question, we propose a novel computational framework, synchronized activations (Sync-ACTs), to couple the visual representation spaces and semantics between ANNs and BNNs in human brain based on naturalistic functional magnetic resonance imaging (nfMRI) data. With this approach, we are able to semantically annotate the neurons in ANNs with biologically meaningful descriptions derived from human brain imaging for the first time. We evaluated the Sync-ACT framework on two publicly available movie-watching nfMRI data sets. The experiments demonstrate 1) the significant correlation and similarity of the semantics between the visual representations in FBNs and those in a variety of convolutional neural networks (CNNs) models and 2) the close relationship between CNN’s visual representation similarity to BNNs and its performance in image classification tasks. Overall, our study introduces a general and effective paradigm to couple the ANNs and BNNs and provides novel insights for future studies such as brain-inspired artificial intelligence.},
  archive      = {J_TCDS},
  author       = {Lin Zhao and Haixing Dai and Zihao Wu and Zhenxiang Xiao and Lu Zhang and David Weizhong Liu and Xintao Hu and Xi Jiang and Sheng Li and Dajiang Zhu and Tianming Liu},
  doi          = {10.1109/TCDS.2023.3287184},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {584-594},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Coupling visual semantics of artificial neural networks and human brain function via synchronized activations},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hierarchical utilization of semantic gradients and scene
structure for visual place recognition. <em>TCDS</em>, <em>16</em>(2),
570–583. (<a href="https://doi.org/10.1109/TCDS.2023.3281870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual place recognition (VPR) is a fundamental element for long-term simultaneous localization and mapping (SLAM) systems. For long-term VPR, severe appearance and viewpoint variations are inevitable. In this article, we introduce a novel VPR system named semantic scene structure place recognition (3SPR), inspired by the repeatability of semantic gradients and the scene structure of urban environments. Semantic gradients are densely sampled according to the sum of absolute gradients of all channels in the logits layer. Features of the semantic gradients in different layers are concatenated to exploit features’ characteristics at different levels. Based on partitions by vanishing points of road lines and vector of locally aggregated descriptors (VLAD), the scene structure VLAD (SSVLAD) is generated from concatenated features of the semantic gradients. Moreover, a local point group match method is used to enhance the spatial verification. Experimental results show that our method achieves state-of-the-art performance on the Oxford Robotcar data set and the Synthia data set.},
  archive      = {J_TCDS},
  author       = {Yaoqi Bao and Yun Pan and Zhe Yang and Ruohong Huan},
  doi          = {10.1109/TCDS.2023.3281870},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {570-583},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A hierarchical utilization of semantic gradients and scene structure for visual place recognition},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continual learning for anthropomorphic hand grasping.
<em>TCDS</em>, <em>16</em>(2), 559–569. (<a
href="https://doi.org/10.1109/TCDS.2023.3284070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is important and challenging to learn to grasp different objects with anthropomorphic robotic hands continually and incrementally. However, most current works do not have this property. They learn grasp planners using large preprepared datasets, do not generalize well to new objects and are difficult to improve continually. Besides, existing continual leaning works rarely target at anthropomorphic hand grasping, and usually deal with short streams of experiences. Because of the intrinsic long stream nature of anthropomorphic hand grasping, it is hard to utilize off-the-shelf continual learning (CL) methods for it. In this article, we propose to introduce continual machine learning into anthropomorphic hand grasping and design the CL framework of anthropomorphic grasping (CLFAG framework). It includes three modules: 1) Data Producer; 2) Grasp Experiences; and 3) CL Algorithm $A_{\mathrm{ CL}}$ , thus, makes the CL of anthropomorphic grasping possible. To overcome the catastrophic forgetting problem in long streams of grasping experiences, we propose a CL algorithm based on importance-based regularization and diversity-aware replay within the CLFAG framework. Furthermore, we construct a dataset for CL of anthropomorphic grasping. Experiments on constructed dataset and in simulation demonstrate the effectiveness and superiority of the proposed approach.},
  archive      = {J_TCDS},
  author       = {Wanyi Li and Wei Wei and Peng Wang},
  doi          = {10.1109/TCDS.2023.3284070},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {559-569},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Continual learning for anthropomorphic hand grasping},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A synapse-threshold synergistic learning approach for
spiking neural networks. <em>TCDS</em>, <em>16</em>(2), 544–558. (<a
href="https://doi.org/10.1109/TCDS.2023.3278712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) have demonstrated excellent capabilities in various intelligent scenarios. Most existing methods for training SNNs are based on the concept of synaptic plasticity; however, learning in the realistic brain also utilizes intrinsic nonsynaptic mechanisms of neurons. The spike threshold of biological neurons is a critical intrinsic neuronal feature that exhibits rich dynamics on a millisecond timescale and has been proposed as an underlying mechanism that facilitates neural information processing. In this study, we develop a novel synergistic learning approach that involves simultaneously training synaptic weights and spike thresholds in SNNs. SNNs trained with synapse-threshold synergistic learning (STL-SNNs) achieve significantly superior performance on various static and neuromorphic data sets than SNNs trained with two degenerated single-learning models. During training, the synergistic learning approach optimizes neural thresholds, providing the network with stable signal transmission via appropriate firing rates. Further analysis indicates that STL-SNNs are robust to noisy data and exhibit low energy consumption for deep network structures. Additionally, the performance of STL-SNN can be further improved by introducing a generalized joint decision framework. Overall, our findings indicate that biologically plausible synergies between synaptic and intrinsic nonsynaptic mechanisms may provide a promising approach for developing highly efficient SNN learning methods.},
  archive      = {J_TCDS},
  author       = {Hongze Sun and Wuque Cai and Baoxin Yang and Yan Cui and Yang Xia and Dezhong Yao and Daqing Guo},
  doi          = {10.1109/TCDS.2023.3278712},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {544-558},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A synapse-threshold synergistic learning approach for spiking neural networks},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Social-psychology-inspired reinforcement learning framework
for conflict management in connected vehicles. <em>TCDS</em>,
<em>16</em>(2), 534–543. (<a
href="https://doi.org/10.1109/TCDS.2023.3279817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In any connected network, resource scarcity, perceived road blocks, and incongruent objectives can potentially ensue conflicts among stakeholders. In the existing literature, trust has been cited as a crucial component in effective conflict management (CM). Besides trust, empathy, and social intelligence (SI) play decisive roles in enhancing cooperation, encouraging information sharing, and promoting problem solving. In this article, we discuss the three major components of CM and propose a computational model, which is inspired from social psychology for CM in connected vehicles (CVs). Our mathematical algorithm focuses on three factors, namely, trust, empathy, and SI that are learned via social interactions among vehicles to ensure safety of vehicles and passengers. The triad of trust, empathy, and SI is used to aid reinforcement learning (RL) for obtaining the optimal $q$ -values and rewards in the shortest duration of time in the CV network. We have examined how the three factors influence the learning process and analyzed their CM potentials. Results show that the proposed model is 118.18% more efficient than the trust-only-based RL algorithm.},
  archive      = {J_TCDS},
  author       = {Heena Rathore and Yash Kumar Singhal and George Kodimattam Joseph},
  doi          = {10.1109/TCDS.2023.3279817},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {534-543},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Social-psychology-inspired reinforcement learning framework for conflict management in connected vehicles},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shallow inception domain adaptation network for EEG-based
motor imagery classification. <em>TCDS</em>, <em>16</em>(2), 521–533.
(<a href="https://doi.org/10.1109/TCDS.2023.3279262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography (EEG) data across multiple individuals have a high variance. Directly using the data to train a deep learning (DL) model usually degrades the performance. To address this issue, we propose a shallow Inception domain adaptation framework to extract informative deep features from data of multiple subjects for accurate motor imagery (MI) recognition. To our best knowledge, the Inception architecture in DL is combined with a domain adaptation (DA) scheme for the first time for the MI classification task. The approach contains two compact Inception blocks that decode temporal features in different scales. In addition, we jointly optimize a novel combined loss function to reduce both marginal and class conditional discrepancies caused by the multimodal structure of EEG signals. The DA-based loss enables Inception blocks to take full advantage of their learning abilities to capture discriminative patterns of MI data from multiple subjects instead of relying on the target user only. To demonstrate the effectiveness of our approach, we conduct substantial experiments on two well-known data sets, brain–computer interface competition IV-2a and competition IV-2b. Results show that our model achieves better performance than state-of-the-art strategies. The proposed model is able to extract informative features from high-variant EEG data collected from different individuals and achieves accurate MI classifications.},
  archive      = {J_TCDS},
  author       = {Xiuyu Huang and Kup-Sze Choi and Nan Zhou and Yuanpeng Zhang and Badong Chen and Witold Pedrycz},
  doi          = {10.1109/TCDS.2023.3279262},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {521-533},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Shallow inception domain adaptation network for EEG-based motor imagery classification},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intersection-over-union similarity-based nonmaximum
suppression for human pose estimation in crowded scenes. <em>TCDS</em>,
<em>16</em>(2), 511–520. (<a
href="https://doi.org/10.1109/TCDS.2023.3276372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation in crowded scenes can be applied to many computer vision tasks, such as video surveillance, action recognition, and human–computer interaction. Although the existing methods achieved good results in sparse scenes, the precision accuracy always decreased due to overlapping occlusion and dense distribution of human bodies in crowded scenes. The result of using a single Intersection-over-Union (IoU) threshold to filter redundant boxes with different congestion levels is not effective; thus, IoU similarity-based nonmaximum suppression (NMS) is used to solve the problem of error suppression of the adjacent target box in crowded scenes. Meanwhile, the Transformer structure which has a natural advantage in predicting the relationship between keypoint pairs is introduced for predicting the tag heatmap in the keypoint detection stage to avoid the interference of noise keypoints on keypoint detection. To better use the tag information to distinguish the main keypoints and interference keypoints, the keypoints refinement algorithm based on tag information is proposed to filter noise keypoints. In the experiment, our proposed method outperforms previous methods on the CrowdPose test data set.},
  archive      = {J_TCDS},
  author       = {Longsheng Wei and Hao Huang and Xuefu Yu},
  doi          = {10.1109/TCDS.2023.3276372},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {511-520},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Intersection-over-union similarity-based nonmaximum suppression for human pose estimation in crowded scenes},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified search framework for data augmentation and neural
architecture on small-scale image data sets. <em>TCDS</em>,
<em>16</em>(2), 501–510. (<a
href="https://doi.org/10.1109/TCDS.2023.3274177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation is an effective technique to enrich the training data’s diversity and reduce the risk of overfitting. However, different data sets have distinct preferences on various augmentation techniques. Recently, automated data augmentation (auto-augmentation), which could engineer augmentation policy automatically, drew a growing interest. Previous auto-augmentation methods usually utilize a density matching (DM) strategy, which highly depends on a large data scale to ensure a precise policy evaluation. When facing small-scale data sets, it usually achieves an inferior performance. To address the problem, an improved method named Augmented DM is proposed by augmenting the train data with policies uniformly sampled from a prior distribution, making the policy evaluation more precise. Moreover, we propose a unified search framework for data augmentation and neural architecture (USAA) by formulating the search processes with one formulation. As a result, both optimal augmentation policy and neural architecture could be obtained within one round of the search process. Extensive experiments have been conducted on a bag of medical data sets with general small scales, and the results show that the proposed Augmented DM and USAA can outperform the state-of-the-art auto-augmentation and AutoML approaches, respectively.},
  archive      = {J_TCDS},
  author       = {Jianwei Zhang and Lei Zhang and Dong Li and Lituan Wang},
  doi          = {10.1109/TCDS.2023.3274177},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {501-510},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A unified search framework for data augmentation and neural architecture on small-scale image data sets},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling motor control in continuous time active inference:
A survey. <em>TCDS</em>, <em>16</em>(2), 485–500. (<a
href="https://doi.org/10.1109/TCDS.2023.3338491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The way the brain selects and controls actions is still widely debated. Mainstream approaches based on optimal control focus on stimulus-response mappings that optimize cost functions. Ideomotor theory and cybernetics propose a different perspective: they suggest that actions are selected and controlled by activating action effects and by continuously matching internal predictions with sensations. Active inference offers a modern formulation of these ideas, in terms of inferential mechanisms and prediction error-based control, which can be linked to neural mechanisms of living organisms. This article provides a technical illustration of active inference models in continuous time and a brief survey of active inference models that solve four kinds of control problems; namely, the control of goal-directed reaching movements, active sensing, the resolution of multisensory conflict during movement and the integration of decision-making and motor control. Crucially, in active inference, all these different facets of motor control emerge from the same optimization process—namely, the minimization of free energy—and do not require designing separate cost functions. Therefore, active inference provides a unitary perspective on various aspects of motor control that can inform both the study of biological control mechanisms and the design of artificial and robotic systems.},
  archive      = {J_TCDS},
  author       = {Matteo Priorelli and Federico Maggiore and Antonella Maselli and Francesco Donnarumma and Domenico Maisto and Francesco Mannella and Ivilin Peev Stoianov and Giovanni Pezzulo},
  doi          = {10.1109/TCDS.2023.3338491},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {485-500},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Modeling motor control in continuous time active inference: A survey},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The effect of expressive robot behavior on users’ mental
effort: A pupillometry study. <em>TCDS</em>, <em>16</em>(2), 474–484.
(<a href="https://doi.org/10.1109/TCDS.2024.3352893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robots are becoming part of our social landscape. Social interaction with humans must be efficient and intuitive to understand because nonverbal cues make social interactions between humans and robots more efficient. This study measures mental effort to investigate what factors influence the intuitive understanding of expressive nonverbal robot motions. Fifty participants were asked to watch, while their pupil response and gaze were measured with an eye tracker, eighteen short video clips of three different robot types while performing expressive robot behaviors. Our findings indicate that the appearance of the robot, the viewing angle, and the expression shown by the robot all influence the cognitive load, and therefore, they may influence the intuitive understanding of expressive robot behavior. Furthermore, we found differences in the fixation time for different features of the different robots. With these insights, we identified possible improvement directions for making interactions between humans and robots more efficient and intuitive.},
  archive      = {J_TCDS},
  author       = {Marieke van Otterdijk and Bruno Laeng and Diana Saplacan Lindblom and Jim Torresen},
  doi          = {10.1109/TCDS.2024.3352893},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {474-484},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {The effect of expressive robot behavior on users’ mental effort: A pupillometry study},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GCEN: Multiagent deep reinforcement learning with grouped
cognitive feature representation. <em>TCDS</em>, <em>16</em>(2),
458–473. (<a href="https://doi.org/10.1109/TCDS.2023.3323987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, cooperative multiagent deep reinforcement learning (MADRL) has received increasing research interest and has been widely applied to computer games and coordinated multirobot systems, etc. However, it is still challenging to realize high-solution quality and learning efficiency for MADRL under the conditions of incomplete and noisy observations. To this end, this article proposes an MADRL approach with grouped cognitive feature representation (GCEN), following the paradigm of centralized training and decentralized execution (CTDE). Different from previous works, GCEN incorporates a new cognitive feature representation that combines a grouped attention mechanism and a training approach using mutual information (MI). The grouped attention mechanism is proposed to selectively extract entity features within the observation field for each agent while avoiding the influence of irrelevant observations. The MI regularization term is designed to guide the agents to learn grouped cognitive features based on global information, aiming to mitigate the influence of partial observations. The proposed GCEN approach can be extended as a feature representation module to different MADRL methods. Extensive experiments on the challenging level-based foraging and StarCraft II micromanagement benchmarks were conducted to illustrate the effectiveness and advantages of the proposed approach. Compared with seven representative MADRL algorithms, our proposed approach achieves state-of-the-art performance in winning rates and training efficiency. Experimental results further demonstrate that GCEN has improved generalization ability across varying sight ranges.},
  archive      = {J_TCDS},
  author       = {Hao Gao and Xin Xu and Chao Yan and Yixing Lan and Kangxing Yao},
  doi          = {10.1109/TCDS.2023.3323987},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {458-473},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {GCEN: Multiagent deep reinforcement learning with grouped cognitive feature representation},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BaSICNet: Lightweight 3-d hand pose estimation network based
on biomechanical structure information for dexterous manipulator
teleoperation. <em>TCDS</em>, <em>16</em>(2), 448–457. (<a
href="https://doi.org/10.1109/TCDS.2022.3230707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of hand pose for dexterous manipulator teleoperation is an attractive method to the control of the multifingered manipulators. Furthermore, the advancement of the deep learning and depth sensors has encouraged the development of the 3-D hand pose estimation. However, developing a 3-D hand pose estimation method with an accurate and real-time performance is still a difficult task in computer vision. In this article, a lightweight depth-based network named the biomechanical structure information cascade network (BaSICNet) is proposed by considering the global and local structure of human hands to improve the performance through a cascade network and a bone-constraint loss function. In addition, the BaSICNet is applied to a five-fingered dexterous manipulator platform to achieve visual hand-based teleoperation. Extensive evaluations on two public data sets show that the BaSICNet can produce accurate and fast 3-D hand poses (9.15 and 7.59-mm mean errors on NYU and MSRA data sets with 114.7 fps), and can achieve superior 3-D hand pose estimation balance of accuracy and speed when compared with state-of-the-art (SOAT) methods. Experiment results on the dexterous manipulator platform also show that the BaSICNet can be applied well for teleoperation.},
  archive      = {J_TCDS},
  author       = {Wenrao Pang and Qing Gao and Yinan Zhao and Zhaojie Ju and Junjie Hu},
  doi          = {10.1109/TCDS.2022.3230707},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {448-457},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {BaSICNet: Lightweight 3-D hand pose estimation network based on biomechanical structure information for dexterous manipulator teleoperation},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Path learning by demonstration for iterative human–robot
interaction with uncertain time durations. <em>TCDS</em>,
<em>16</em>(2), 436–447. (<a
href="https://doi.org/10.1109/TCDS.2022.3231092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a path learning method through physical human–robot interaction (pHRI) based on a stretch-compression iterative learning control (ILC) scheme and contouring impedance control. The robot learns a task path desired by the human user through a kinaesthetic interface and provides physical assistance to the human user in repetitive interactions. Due to the uncertainty of the human user’s force and motion, the time duration of each iteration may be different, so a novel ILC scheme based on stretch and compression operation is proposed to update the reference trajectory of the robotic manipulator. By attaching the Frenet–Serret frame to each point on the reference path, the control task is decomposed into impedance control in the tangential direction and position control in the normal or binormal direction constraining the human user on the reference path. Experiments on a 7-DOF Sawyer robot are carried out to show the effectiveness and robustness of the proposed method.},
  archive      = {J_TCDS},
  author       = {Deqing Huang and Jingkang Xia and Chenjian Song and Xueyan Xing and Yanan Li},
  doi          = {10.1109/TCDS.2022.3231092},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {436-447},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Path learning by demonstration for iterative Human–Robot interaction with uncertain time durations},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Passive model-predictive impedance control for safe physical
human–robot interaction. <em>TCDS</em>, <em>16</em>(2), 426–435. (<a
href="https://doi.org/10.1109/TCDS.2023.3275217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various cognitive systems have been designed to model the position and stiffness profiles of human behavior and then to drive robots by mimicking the human’s behavior to accomplish physical human–robot interaction tasks through a properly designed impedance controller. However, some studies have shown that variable stiffness parameters of the impedance controller can cause the violation of the passivity constraint of the robot states, and make the robot’s stored energy exceed the external energy injected from the human user, thus leading to the unsafe human–robot interaction. To solve this problem, this article proposes a novel passive model-predictive impedance control method including two control loops. In the bottom-loop of the proposed controller, the robot is driven by a variable impedance controller to achieve the desired compliant interaction behavior. In the top-loop of the proposed controller, the model-predictive control (MPC) is used to ensure that the robot states satisfy the passivity constraint by calculating a complementary torque to limit the stored energy of the robot. The passivity of the closed-loop robot system and the feasibility of MPC are guaranteed by theoretical analysis, ensuring the safety of the robotic movement in the human–robot interaction. The effectiveness of the proposed method is demonstrated by the simulation and experiment on the Franka Emika Panda robot.},
  archive      = {J_TCDS},
  author       = {Ran Cao and Long Cheng and Houcheng Li},
  doi          = {10.1109/TCDS.2023.3275217},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {426-435},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Passive model-predictive impedance control for safe physical Human–Robot interaction},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to assist bimanual teleoperation using interval
type-2 polynomial fuzzy inference. <em>TCDS</em>, <em>16</em>(2),
416–425. (<a href="https://doi.org/10.1109/TCDS.2023.3272730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assisting humans in collaborative tasks is a promising application for robots, however effective assistance remains challenging. In this article, we propose a method for providing intuitive robotic assistance based on learning from human natural limb coordination. To encode coupling between multiple-limb motions, we use a novel interval type-2 (IT2) polynomial fuzzy inference for modeling trajectory adaptation. The associated polynomial coefficients are estimated using a modified recursive least square with a dynamic forgetting factor. We propose to employ a Gaussian process to produce robust human motion predictions, and thus address the uncertainty and measurement noise of the system caused by interactive environments. Experimental results on two types of interaction tasks demonstrate the effectiveness of this approach, which achieves high accuracy in predicting assistive limb motion and enables humans to perform bimanual tasks using only one limb.},
  archive      = {J_TCDS},
  author       = {Ziwei Wang and Haolin Fei and Yanpei Huang and Quentin Rouxel and Bo Xiao and Zhibin Li and Etienne Burdet},
  doi          = {10.1109/TCDS.2023.3272730},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {416-425},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Learning to assist bimanual teleoperation using interval type-2 polynomial fuzzy inference},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bioinspired multifunctional tendon-driven tactile sensor
and application in obstacle avoidance using reinforcement learning.
<em>TCDS</em>, <em>16</em>(2), 407–415. (<a
href="https://doi.org/10.1109/TCDS.2023.3297361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a new bioinspired tactile sensor that is multifunctional and has different sensitivity contact areas. The TacTop area is sensitive and is used for object classification when there is a direct contact. On the other hand, the TacSide area is less sensitive and is used to localize the side contact areas. By connecting tendons from the TacSide area to the TacTop area, the sensor is able to perform multiple detection functions using the same expression region. For the mixed contacting signals collected from the expression region with numerous markers and pins, we build a modified DenseNet121 network which specifically removes all fully connected layers and keeps the rest as a subnetwork. The proposed model also contains a global average pooling layer with two branching networks to handle different functions and provide accurate spatial translation of the extracted features. The experimental results demonstrate a high-prediction accuracy of 98% for object perception and localization. Furthermore, the new tactile sensor is utilized for obstacle avoidance, where action skills are extracted from human demonstrations and then an action data set is generated for reinforcement learning to guide robots toward correct responses after contact detection. To evaluate the effectiveness of the proposed framework, several simulations are performed in the MuJoCo environment.},
  archive      = {J_TCDS},
  author       = {Zhenyu Lu and Zhou Zhao and Tianqi Yue and Xu Zhu and Ning Wang},
  doi          = {10.1109/TCDS.2023.3297361},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {407-415},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A bioinspired multifunctional tendon-driven tactile sensor and application in obstacle avoidance using reinforcement learning},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guest editorial special issue on movement sciences in
cognitive systems. <em>TCDS</em>, <em>16</em>(2), 403–406. (<a
href="https://doi.org/10.1109/TCDS.2024.3372274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Movements play a critical role in robotic systems, with considerations varying across different robotic systems regarding factors, such as accuracy, speed, energy consumption, and naturalness of movements in various parts of the robotic mechanics. Over the past decades, the robotics community has developed computationally efficient mathematical tools for studying, simulating, and optimizing movements of articulated bodies to address these challenges.},
  archive      = {J_TCDS},
  author       = {Junpei Zhong and Ran Dong and Soichiro Ikuno and Yanan Li and Chenguang Yang},
  doi          = {10.1109/TCDS.2024.3372274},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {403-406},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Guest editorial special issue on movement sciences in cognitive systems},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sequential learning network with residual blocks:
Incorporating temporal convolutional information into recurrent neural
networks. <em>TCDS</em>, <em>16</em>(1), 396–401. (<a
href="https://doi.org/10.1109/TCDS.2023.3325358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal convolutional networks (TCNs) have shown remarkable performance in sequence modeling and surpassed recurrent neural networks (RNNs) in a number of tasks. However, performing exceptionally on extremely long sequences remains an obstacle due to the restrained receptive field of temporal convolutions and a lack of forgetting mechanism. Although RNNs can carry state transmission down the full sequence length and latch information by forgetting mechanism, the issues of information saturation and gradient vanishing or exploding still persist, which usually occur in back-propagation due to the effect of multiplicative accumulation. To benefit from both temporal convolutions and RNNs, we propose a neural architecture that merge temporal convolutional data into recurrent networks. The temporal convolutions are employed intermittently and fused into the hidden states of RNNs with the assistance of attention for providing long-term information. With this architecture, it is not needed for convolutional networks to cover the total length of the sequence, thus gradient and saturation issues in RNNs are ameliorated since convolutions are integrated into its cells and the state is updated with convolutional information. Extensive experiments illustrate the superiority of our network against other competitive counterparts, such as TCNs and the different variants of RNNs.},
  archive      = {J_TCDS},
  author       = {Dongjing Shan and Kun Yao and Xiongwei Zhang},
  doi          = {10.1109/TCDS.2023.3325358},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {396-401},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Sequential learning network with residual blocks: Incorporating temporal convolutional information into recurrent neural networks},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized feature learning for detection of novel objects.
<em>TCDS</em>, <em>16</em>(1), 388–395. (<a
href="https://doi.org/10.1109/TCDS.2023.3327453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot object detection (FSOD) aims at heuristically detecting novel objects with limited labeled data. Typical methods focus on the advanced classifications using the features extracted from common backbones. However, these features are usually base domain-biased, which trap the methods due to insufficient knowledge learned by common backbones. In this correspondence, a novel FSOD network is designed via learning of deep-level generalized features. Specifically, a two-branch backbone is introduced by adding a category-agnostic feature extractor as a parallel branch of common backbone, which preserves valuable but coarse features out of base classes. To fully refine these features, a source aggregation scheme with probabilistic pathway selection and source-based channel dropout is designed, which prevents the network from falling into the dominant optimization of base classes. The resulting generalized features are less-biased features, which increases the adaptability to novel classes. Besides, a loose contrastive loss is provided as extra supervised information to relieve overfitting. As a result, the proposed method reaches a good compatibility with the data out of base classes. The effectiveness of the proposed method is verified through experiments on PASCAL VOC, COCO, and iCubWorld Transformations data sets.},
  archive      = {J_TCDS},
  author       = {Jierui Liu and Xilong Liu and Zhiqiang Cao and Junzhi Yu and Min Tan},
  doi          = {10.1109/TCDS.2023.3327453},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {388-395},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Generalized feature learning for detection of novel objects},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STDP-driven development of attention-based people detection
in spiking neural networks. <em>TCDS</em>, <em>16</em>(1), 380–387. (<a
href="https://doi.org/10.1109/TCDS.2022.3210278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This letter provides, to the best of our knowledge, a first analysis of how biologically plausible spiking neural networks (SNNs) equipped with spike-timing-dependent plasticity (STDP) can learn to detect people on the fly from nonindependent and identically distributed (non-i.i.d) streams of retina-inspired, event camera data. Our system works as follows. First, a short sequence of event data, capturing a walking human from a flying drone, is forwarded in its natural order to an SNN-STDP system, which also receives teacher spiking signals from the neural activity readout block. Then, when the end of the learning sequence is reached, the learned system is assessed on testing sequences. In addition, we also present a new interpretation of anti-Hebbian plasticity as an overfitting control mechanism and provide experimental demonstrations of our findings. This work contributes to the study of attention-based development and perception in bioinspired systems.},
  archive      = {J_TCDS},
  author       = {Ali Safa and Ilja Ocket and André Bourdoux and Hichem Sahli and Francky Catthoor and Georges G. E. Gielen},
  doi          = {10.1109/TCDS.2022.3210278},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {380-387},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {STDP-driven development of attention-based people detection in spiking neural networks},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CSC-net: Cross-color spatial co-occurrence matrix network
for detecting synthesized fake images. <em>TCDS</em>, <em>16</em>(1),
369–379. (<a href="https://doi.org/10.1109/TCDS.2023.3274450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the generative adversarial networks (GANs) generated images have been spread over the social networks, which brings the new challenge in the community of media forensics. Although some reliable forensic tools have advanced the study of detecting GAN generated images, while the detection accuracy cannot be guaranteed when facing the malicious post-processing attacks, especially in the practical social network scenario. Thus, in this context, we propose a novel well-designed deep neural network equipped with handcrafted features for dealing with this problem. In particular, relying on the cross-color spatial co-occurrence matrix (CSCM), the discriminative features are extracted after carefully analyzing and selecting the most effective color channels. Next, the fused features are fed into the deep neutral network for training a high-efficient forensic detector. Extensive experimental results empirically verify that in most detection scenarios, our proposed detector performs superiorly to the prior arts, especially in the case of post-processing attacks. Moreover, we also highlight the relevance of the proposed detector over the realistic social network platforms, and its generalization capability in three different scenarios.},
  archive      = {J_TCDS},
  author       = {Tong Qiao and Yuxing Chen and Xiaofei Zhou and Ran Shi and Hang Shao and Kunye Shen and Xiangyang Luo},
  doi          = {10.1109/TCDS.2023.3274450},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {369-379},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {CSC-net: Cross-color spatial co-occurrence matrix network for detecting synthesized fake images},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parallel self-attention and spatial-attention fusion for
human pose estimation and running movement recognition. <em>TCDS</em>,
<em>16</em>(1), 358–368. (<a
href="https://doi.org/10.1109/TCDS.2023.3275652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation (HPE) is a fundamental yet promising visual recognition problem. Existing popular methods (e.g., Hourglass and its variants) either attempt to directly add local features element-wisely, or (e.g., vision transformers) try to learn the global relationships among different human parts. However, it remains an open problem to effectively integrate the local–global representations for accurate HPE. In this work, we design four feature fusion strategies on the hierarchical ResNet structure, including direct channel concatenation, element-wise addition, and two parallel structures. Both two parallel structures adopt the naive self-attention encoder to model global dependencies. The difference between them is that one adopts the original ResNet BottleNeck while the other employs a spatial-attention module (named SSF) to learn the local patterns. Experiments on COCO Keypoint 2017 show that our SSF for HPE (named SSPose) achieves the best average precision with acceptable computational cost among the compared state-of-the-art methods. In addition, we build a lightweight running data set to verify the effectiveness of SSPose. Based solely on the keypoints estimated by our SSPose, we propose a regression model to identify valid running movements without training any other classifiers. Our source codes and running data set are publicly available.},
  archive      = {J_TCDS},
  author       = {Qingtian Wu and Yu Zhang and Liming Zhang and Haoyong Yu},
  doi          = {10.1109/TCDS.2023.3275652},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {358-368},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Parallel self-attention and spatial-attention fusion for human pose estimation and running movement recognition},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ElectrodeNet—a deep-learning-based sound coding strategy for
cochlear implants. <em>TCDS</em>, <em>16</em>(1), 346–357. (<a
href="https://doi.org/10.1109/TCDS.2023.3275587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ElectrodeNet, a deep-learning-based sound coding strategy for the cochlear implant (CI), is proposed to emulate the advanced combination encoder (ACE) strategy by replacing the conventional envelope detection using various artificial neural networks. The extended ElectrodeNet-CS strategy further incorporates the channel selection (CS). Network models of deep neural network (DNN), convolutional neural network (CNN), and long short-term memory (LSTM) were trained using the fast Fourier transformed bins and channel envelopes obtained from the processing of clean speech by the ACE strategy. Objective speech understanding using short-time objective intelligibility (STOI) and normalized covariance metric (NCM) was estimated for ElectrodeNet using CI simulations. Sentence recognition tests for vocoded Mandarin speech were conducted with normal-hearing listeners. DNN, CNN, and LSTM-based ElectrodeNets exhibited strong correlations to ACE in objective and subjective scores using mean squared error (MSE), linear correlation coefficient (LCC), and Spearman’s rank correlation coefficient (SRCC). The ElectrodeNet-CS strategy was capable of producing N-of-M compatible electrode patterns using a modified DNN network to embed maxima selection, and to perform in similar or even slightly higher average in STOI and sentence recognition compared to ACE. The methods and findings demonstrated the feasibility and potential of using deep learning in the CI coding strategy.},
  archive      = {J_TCDS},
  author       = {Enoch Hsin-Ho Huang and Rong Chao and Yu Tsao and Chao-Min Wu},
  doi          = {10.1109/TCDS.2023.3275587},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {346-357},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {ElectrodeNet—A deep-learning-based sound coding strategy for cochlear implants},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AdaDet: An adaptive object detection system based on
early-exit neural networks. <em>TCDS</em>, <em>16</em>(1), 332–345. (<a
href="https://doi.org/10.1109/TCDS.2023.3274214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent researchers have proposed adaptive inference methods with an early-exiting mechanism, which stops the inference procedure of input if the prediction is with high confidence, leading to a fine-grained resource allocation based on the complexity of inputs. However, the adaptive inference strategy can only be applied for image classification tasks, such a system for object detection is still under-investigated since it lacks an appropriate way to measure the uncertainty of detection results for multiple objects in an image. To address this issue, in this article, we propose a novel adaptive object detection system (AdaDet) based on early-exit neural networks. By using stochastic variables to represent localization predictions, our method can measure the uncertainty of the detection result for an object, based on which we further develop an entropy-based criterion to estimate the reliability of the whole detection results for an input image. With the proposed method, samples that achieve high-confidence detection results will exit early from the detection model, leading to low computational costs for inferring these samples. While only complex images need to finish the whole computational graph to achieve better detection results during inference. We experimentally demonstrate that the proposed AdaDet enhances detection performance under anytime prediction and adaptive inference settings on the most common data set (MS COCO) used in this field.},
  archive      = {J_TCDS},
  author       = {Le Yang and Ziwei Zheng and Jian Wang and Shiji Song and Gao Huang and Fan Li},
  doi          = {10.1109/TCDS.2023.3274214},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {332-345},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {AdaDet: An adaptive object detection system based on early-exit neural networks},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SalDA: DeepConvNet greets attention for visual saliency
prediction. <em>TCDS</em>, <em>16</em>(1), 319–331. (<a
href="https://doi.org/10.1109/TCDS.2023.3274179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting salient regions in images requires the capture of contextual information in the scene. Conventional saliency models typically use the encoder–decoder architecture and multiscale feature fusion for modeling contextual features, which, however, possess huge computational cost and model parameters. In this article, we address the saliency prediction task by capturing long-range dependencies based on the self-attention mechanism. Self-attention has been widely used in image recognition or other classification tasks, but is still rarely being considered in regression-based saliency prediction task. Inspired by the nonlocal block, we propose a new saliency prediction network in which deep convolutional network is integrated with the attention mechanism, namely, SalDA. Considering each feature map may capture different salient regions, our spatial attention module first adaptively aggregates the feature at each position by a weighted sum of the features at all positions within each independent channel. Meanwhile, in order to capture interdependence between channels, we also introduce a channel attention module to integrate different features among different channels. We combine these two attention modules into a multiattention module to further improve the saliency map prediction for the network. We show the effectiveness of SalDA on the largest saliency prediction data set SALICON. Compared to other state-of-the-art methods in this area, we can yield comparable saliency prediction performance, but with substantially less model parameters and shorter inference time.},
  archive      = {J_TCDS},
  author       = {Yihan Tang and Pan Gao and Zhengwei Wang},
  doi          = {10.1109/TCDS.2023.3274179},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {319-331},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {SalDA: DeepConvNet greets attention for visual saliency prediction},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial–temporal feature network for speech-based depression
recognition. <em>TCDS</em>, <em>16</em>(1), 308–318. (<a
href="https://doi.org/10.1109/TCDS.2023.3273614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression is a serious mental disorder that has received increased attention from society. Due to the advantage of easy acquisition of speech, researchers have tried to propose various automatic depression recognition algorithms based on speech. Feature selection and algorithm design are the main difficulties in speech-based depression recognition. In our work, we propose the spatial–temporal feature network (STFN) for depression recognition, which can capture the long-term temporal dependence of audio sequences. First, to obtain a better feature representation for depression analysis, we develop a self-supervised learning framework, called vector quantized wav2vec transformer net (VQWTNet) to map speech features and phonemes with transfer learning. Second, the stacked gated residual blocks in the spatial feature extraction network are used in the model to integrate causal and dilated convolutions to capture multiscale contextual information by continuously expanding the receptive field. In addition, instead of LSTM, our method employs the hierarchical contrastive predictive coding (HCPC) loss in HCPCNet to capture the long-term temporal dependencies of speech, reducing the number of parameters while making the network easier to train. Finally, experimental results on DAIC-WOZ (Audio/Visual Emotion Challenge (AVEC) 2017) and E-DAIC (AVEC 2019) show that the proposed model significantly improves the accuracy of depression recognition. On both data sets, the performance of our method far exceeds the baseline and achieves competitive results compared to state-of-the-art methods.},
  archive      = {J_TCDS},
  author       = {Zhuojin Han and Yuanyuan Shang and Zhuhong Shao and Jingyi Liu and Guodong Guo and Tie Liu and Hui Ding and Qiang Hu},
  doi          = {10.1109/TCDS.2023.3273614},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {308-318},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Spatial–Temporal feature network for speech-based depression recognition},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Iterative pseudo-sparse partial least square and its higher
order variant: Application to inference from high-dimensional
biosignals. <em>TCDS</em>, <em>16</em>(1), 296–307. (<a
href="https://doi.org/10.1109/TCDS.2023.3267010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial least square (PLS) regression and its (L1 or L2 norm) regularized versions have been proposed to handle the high-dimensionality aspects of the problem at hand and select relevant features. Addressing these issues improves the generalizability of decoding the unseen data, with the severe challenge of high computational complexity. In order to avoid directly solving the L1 norm optimization problem or performing matrix inversion, this article proposes two PLS-based algorithms, pseudo-sparse PLS (PS-PLS) and iterative pseudo-sparse higher order PLS (iPS-HOPLS). In these proposed methods, we add the Pseudo-Sparsity term to reduce the L1 norm of the regression coefficient vector in a selective scheme for better importance interpretation while keeping the algorithm as simple as possible. Regarding the evaluation of the proposed methods, we investigate three critical high-dimensionality regression problems of 1) the prediction of 3-D trajectory from electrocorticography (ECoG) recordings, 2) decoding continuous fluctuation of the electromyography (EMG) powers from recorded magnetoencephalography (MEG) signals, and 3) continuous decoding of the finger forces from the high-density surface electromyogram (HD-sEMG) signals. As well as providing cognitive-relevant interpretations, the experimental results show significant improvements over the generic methods and competitive performance compared to the state-of-the-art regularized PLS approaches.},
  archive      = {J_TCDS},
  author       = {Aref Einizade and Sepideh Hajipour Sardouie},
  doi          = {10.1109/TCDS.2023.3267010},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {296-307},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Iterative pseudo-sparse partial least square and its higher order variant: Application to inference from high-dimensional biosignals},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hand movement recognition and salient tremor feature
extraction with wearable devices in parkinson’s patients. <em>TCDS</em>,
<em>16</em>(1), 284–295. (<a
href="https://doi.org/10.1109/TCDS.2023.3266812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tremor is one of the earliest signs of Parkinson’s disease (PD), which seriously disrupts patients’ daily lives. It is important to study upper limb tremors quantitatively to control PD progression. In this study, surface electromyography (sEMG) signals from wearable devices are used to recognize rest, posture, and kinetic tremor action from six upper limb clinical actions and to quantify features of tremors. A multivariable time-series classification model (MTSCM) based on fully convolutional networks and a long short-term memory network is proposed to recognize tremor actions. MTSCM achieves a high degree of accuracy both on the left-hand and right-hand data sets for tremor actions. An improved Hilbert–Huang transform (HHT) method is proposed to decompose the inertial signals of tremor actions to obtain tremor components. Using the improved HHT, tremor and motion components can be decomposed effectively. In addition, 53 features are extracted from inertial and sEMG signals, and a canonical correlation analysis is used to determine the correlation between features and movement disorder society unified PD rating scale (MDS-UPDRS) scores. Several of the relevant characteristics are related to MDS-UPDRS scores, notably the dominant frequency and amplitude of the tremor component are significantly correlated ( ${p}$ &lt; 0.01) with tremor scores. Detecting upper limb clinical tremors in PD patients using wearable sensors is feasible according to our findings.},
  archive      = {J_TCDS},
  author       = {Fang Lin and Zhelong Wang and Hongyu Zhao and Sen Qiu and Ruichen Liu and Xin Shi and Cui Wang and Wenchao Yin},
  doi          = {10.1109/TCDS.2023.3266812},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {284-295},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Hand movement recognition and salient tremor feature extraction with wearable devices in parkinson’s patients},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A cognitive robotics implementation of global workspace
theory for episodic memory interaction with consciousness.
<em>TCDS</em>, <em>16</em>(1), 266–283. (<a
href="https://doi.org/10.1109/TCDS.2023.3266103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial general intelligence revived in recent years after people achieved significant advances in machine learning and deep learning. This leads to the thinking of how real intelligence could be created. Consciousness theories believe that general intelligence is essentially conscious, yet no universal definition is agreed upon. In this work, global workspace (GW) theory is implemented and integrated with crucial cognitive components. With the focus on episodic memory and inspiration from the nature of episodic memory in psychology and neuroscience, the episodic memory component is implemented within the GW framework. In our experiment, the robotic agent operates in a real-world interactive context, forming episodic memory and demonstrating static, temporal, and context memory capabilities during interactions. Consciousness in this work engages in all formation, maintenance, and retrieval processes of episodic memory. The novelty and contributions of this work are: 1) this work is implementing episodic memory within the consciousness framework, suggesting the sustainable potential of such an integrated approach to cognitive agents with artificial general intelligence (AGI); 2) regarding the limited examples in consciousness-based cognitive architectures, this work attempts to contribute to the diversity of perspectives and approaches; 3) extant episodic memory implementations are suffering from various limitations, while this work summarises some key features for modeling episodic memory within a cognitive architecture; and 4) authors discuss the relationship between episodic memory, consciousness, and general intelligence, proposing the compatibility and relationship between machine consciousness and other AGI research. It is believed that a better alignment between them would further boost the fusion of diverse research for achieving desired cognitive machines.},
  archive      = {J_TCDS},
  author       = {Wenjie Huang and Antonio Chella and Angelo Cangelosi},
  doi          = {10.1109/TCDS.2023.3266103},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {266-283},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A cognitive robotics implementation of global workspace theory for episodic memory interaction with consciousness},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A distributed dynamic framework to allocate collaborative
tasks based on capability matching in heterogeneous multirobot systems.
<em>TCDS</em>, <em>16</em>(1), 251–265. (<a
href="https://doi.org/10.1109/TCDS.2023.3264034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaboration among a group of robots with heterogeneous capabilities is an important research problem that enables to combine different robot functionalities, and thus, conducts complex tasks that may be difficult to achieve by a single robot with limited resources. In this article, we propose a new distributed task allocation framework based on the capability matching of heterogeneous robots. The framework is composed of an ontological dynamic knowledge graph model and a hardware control scheme to model the capability and optimize resource utilization for collaborative tasks. We introduce an intuitive hardware control scheme based on a dynamic knowledge graph that resolves possible conflicts between the hardware control of different types of robots. Action sequences are produced by a task and motion planning algorithm to collaboratively perform the assigned task. The performance of the proposed methodology is evaluated by both simulations and hardware experiments.},
  archive      = {J_TCDS},
  author       = {Hoi-Yin Lee and Peng Zhou and Bin Zhang and Liuming Qiu and Bowen Fan and Anqing Duan and Jingtao Tang and Tin Lun Lam and David Navarro-Alarcon},
  doi          = {10.1109/TCDS.2023.3264034},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {251-265},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A distributed dynamic framework to allocate collaborative tasks based on capability matching in heterogeneous multirobot systems},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Relationship between decision making and resting-state EEG
in adolescents with different emotional stabilities. <em>TCDS</em>,
<em>16</em>(1), 243–250. (<a
href="https://doi.org/10.1109/TCDS.2023.3263845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the varied decision responses are revealed between adolescents with emotional stability and instability (ES and EI), the possible association underlying decision making and resting-state activity remains unknown. The study explored the potential relationship between the resting-state electroencephalogram network and decision responses when adolescents with different emotional stabilities participated in the ultimatum game (UG). A significant relationship between the resting-state activation of the two frequency bands (i.e., delta and alpha) and the UG acceptance rate. Furthermore, the ES adolescents mainly activated the frontal cortex in the delta band compared to the EI adolescents, whereas the EI adolescents exhibited stronger frontal-occipital linkages in the alpha band than the ES group. During decision making, the delta oscillation played a positive role in the cognitive process, while the alpha activity decreased. Finally, emotional stability could be predicted by a multivariable linear prediction model based on the resting-state metrics while adolescents made decisions in the UG task. The distinct resting-state electrophysiological evidence may deepen our knowledge of decision making in individuals with different emotional traits, which may further facilitate the prediction of individual decision behaviors.},
  archive      = {J_TCDS},
  author       = {Yajing Si and Lin Jiang and Peiyang Li and Baodan Chen and Feng Wan and Jing Yu and Dezhong Yao and Fali Li and Peng Xu},
  doi          = {10.1109/TCDS.2023.3263845},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {243-250},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Relationship between decision making and resting-state EEG in adolescents with different emotional stabilities},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning technique reveals intrinsic EEG
connectivity characteristics of patients with mild stroke during
cognitive task performing. <em>TCDS</em>, <em>16</em>(1), 232–242. (<a
href="https://doi.org/10.1109/TCDS.2023.3260081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although convergent evidence has shown that patients with mild stroke (MS) are commonly accompanied by post-stroke cognitive and/or memory impairment, only disproportionate attention was paid compared to severe stroke. To promote post-stroke management for early intervention in MS-related cognitive impairment, a feasible and convenient method for MS detection is, therefore, favorable. A data-driven classification framework combined with quantitative graph theoretical analysis was introduced in this work, aiming to provide a comprehensive appreciation of MS-related brain network alterations. EEG functional connectivity (FC) was constructed from 45 patients with MS and 45 healthy participants during two cognitive tasks (i.e., visual and auditory oddball) and set as input for the classification model and graph theoretical analysis. As expected, patients showed significantly reduced behavioral performance in both tasks. Furthermore, we achieved a satisfactory classification accuracy of 88.9% with a decision fusion strategy from classification models of both tasks. The spatiospectral characteristics of the discriminative FC revealed complex topological distributions in both tasks. Moreover, significantly decreased global efficiency was found, suggesting an MS-related disruption in parallel information processing. Overall, these results demonstrated the potential of FC as salient biomarkers for detecting MS, and extended our understanding of the underlying MS-related neural mechanisms during cognitive processing.},
  archive      = {J_TCDS},
  author       = {Mengru Xu and Zhao Feng and Sujie Wang and Hui Gao and Jiaye Cai and Biwen Wu and Huaying Cai and Yi Sun and Cuntai Guan and Yu Sun and Xuchen Qi},
  doi          = {10.1109/TCDS.2023.3260081},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {232-242},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Machine learning technique reveals intrinsic EEG connectivity characteristics of patients with mild stroke during cognitive task performing},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing overt and covert attention using a real-time
neurofeedback game with consumer-grade EEG. <em>TCDS</em>,
<em>16</em>(1), 223–231. (<a
href="https://doi.org/10.1109/TCDS.2023.3260441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurofeedback training is emerging as a promising tool for cognitive enhancement in healthy as well as cognitive-deficit patients. In this article, we propose the design of a real-time neurofeedback game using a consumer-grade wireless electroencephalography (EEG) system and examine its efficacy in enhancing overt and covert attention abilities for healthy individuals. The game uses a simulated car navigation task, in which a car is navigated using the attention level of the player. A total of 23 healthy subjects participated in four sessions of neurofeedback training. The effectiveness of the proposed neurofeedback game is evaluated using attention scores based on the Sample entropy of EEG signals, beta-to-(alpha + theta) ratio, and the time taken for completing the game. The variations in covert attention are assessed by introducing visual distractions when the player navigates the car and quantified using changes in the attention score and the time taken for restoring the initial attention level. The attention score significantly improved from the first session to the final session by 22.64% and 21.43% for overt attention and covert attention tasks, respectively. The experimental results demonstrate that the proposed neurofeedback game is an effective tool for enhancing overt and covert attention skills in healthy individuals.},
  archive      = {J_TCDS},
  author       = {T. A. Suhail and A. P. Vinod},
  doi          = {10.1109/TCDS.2023.3260441},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {223-231},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Enhancing overt and covert attention using a real-time neurofeedback game with consumer-grade EEG},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph-based information separator and area convolutional
network for EEG-based intention decoding. <em>TCDS</em>, <em>16</em>(1),
212–222. (<a href="https://doi.org/10.1109/TCDS.2023.3260084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current research on brain-computer interface (BCI), the electroencephalography (EEG) signal is usually only represented by a 2-D matrix, and the installation position of the EEG electrodes and the correlation between them are not considered. Actually, the cerebral cortex is a continuous potential surface and the information collected directly from each electrode is influenced by the other electrodes, so direct use of the raw data results in information redundancy. This article converts the EEG signal into a graph, and then creates an information separator (IS) based on the Laplace matrix of the graph to obtain the independent source information of electrode nodes, and propose an IS-based area convolutional network (IS-ACN). Integrating the proposed IS with some advanced methods, the experimental results show that the incorporation of IS can enhance the performance of these methods. By observing and tracking samples with abnormal noise in the BCI competition IV data set 2a, it is demonstrated that the proposed method can greatly reduce the influence of noise and effectively obtain the source features of EEG signals with low signal-to-noise ratio, and the average accuracy and kappa coefficient of the proposed IS-ACN on this data set are 80.59% and 74.1%, respectively.},
  archive      = {J_TCDS},
  author       = {Xianlun Tang and Shifei Wang and Xin Deng and Ke Liu and Yin Tian and Huiming Wang and Xinbo Gao},
  doi          = {10.1109/TCDS.2023.3260084},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {212-222},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Graph-based information separator and area convolutional network for EEG-based intention decoding},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distilling invariant representations with domain adversarial
learning for cross-subject children seizure prediction. <em>TCDS</em>,
<em>16</em>(1), 202–211. (<a
href="https://doi.org/10.1109/TCDS.2023.3257055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Seizure prediction based on electroencephalogram (EEG) has great potential to improve patients’ life quality. Due to the high heterogeneity in distributions of EEG signals among different patients, conventional studies usually show poor generalization ability when transferring the model to new patients, which also leads to difficulties in clinical applications. To alleviate the challenging issue concerning cross-subject domain shift, we propose a transformer-based domain adversarial model. Our model first adopts a pretrained general neural network to extract common features from the EEG signals of available patients. Then, we design a distiller module and a domain discriminator module to perform domain adaptation training based on a small amount of labeled data from the new-coming patient. During the adaptation process, conditional domain adversarial training with the addition of label information is employed to remove patient-related information from the extracted features to learn a common seizure feature space among different patients. Our proposed seizure prediction method is evaluated on the CHB-MIT EEG database. The proposed model achieves a sensitivity of 79.5%, a false alarm rate (FPR) of 0.258/h, and an AUC of 0.814. Experimental results demonstrate that the proposed method can effectively reduce interpatient domain disparity compared to state-of-the-art methods.},
  archive      = {J_TCDS},
  author       = {Ziye Zhang and Aiping Liu and Yikai Gao and Xinrui Cui and Ruobing Qian and Xun Chen},
  doi          = {10.1109/TCDS.2023.3257055},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {202-211},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Distilling invariant representations with domain adversarial learning for cross-subject children seizure prediction},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic threshold distribution domain adaptation network: A
cross-subject fatigue recognition method based on EEG signals.
<em>TCDS</em>, <em>16</em>(1), 190–201. (<a
href="https://doi.org/10.1109/TCDS.2023.3257428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalogram (EEG) has been widely used in driver fatigue recognition due to its high-resolution and noninvasive characteristics in recent years. However, EEG signals have strong variability and significant individual differences, which put forward extremely high requirements on the generalization ability of the model in practical use. To address these issues, a dynamic threshold distribution domain adaptation network (DTDDAN) is proposed to classify the EEG signals for driver fatigue recognition. In the model, the domain discriminator is introduced to alleviate the marginal distribution difference of different domains, and a new loss, Jensen–Shannon loss (JS loss), is applied to relieve the conditional distribution difference of different domains to learn cross-subject invariant deep features of EEG signals. In addition, we propose a dynamic threshold pseudo label strategy to assign pseudo labels to the samples of the target domain in the training process of the model. This strategy can make sure that the high-quality unlabeled data of the target domain contribute to the model training and the model can fully learn the information of the target domain. With extensive experiments on our self-constructed EEG-based fatigue driving data set, competitive performance is achieved for the cross-subject classification task, with average classification sensitivity, specificity, and accuracy of 89.8%, 93.8%, and 92.2%, respectively. And it is shown that our model can effectively extract cross-subject invariant EEG signals deep features by the experimental results.},
  archive      = {J_TCDS},
  author       = {Chao Ma and Meng Zhang and Xinlin Sun and He Wang and Zhongke Gao},
  doi          = {10.1109/TCDS.2023.3257428},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {190-201},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Dynamic threshold distribution domain adaptation network: A cross-subject fatigue recognition method based on EEG signals},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A self-distillation embedded supervised affinity attention
model for few-shot segmentation. <em>TCDS</em>, <em>16</em>(1), 177–189.
(<a href="https://doi.org/10.1109/TCDS.2023.3251371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot segmentation focuses on the generalization of models to segment unseen object with limited annotated samples. However, existing approaches still face two main challenges. First, a huge feature distinction between support and query images a causes knowledge transferring barrier, which harms the segmentation performance. Second, limited support prototypes cannot adequately represent features of support objects, hard to guide high-quality query segmentation. To deal with the above two issues, we propose a self-distillation embedded supervised affinity attention model to improve the performance of few-shot segmentation task. Specifically, the self-distillation guided prototype module uses self-distillation to align the features of support and query. The supervised affinity attention module generates a high-quality query attention map to provide sufficient object information. Extensive experiments prove that our model significantly improves the performance compared to existing methods. Comprehensive ablation experiments and visualization studies also show the significant effect of our method on the few-shot segmentation task. On COCO- $20^{i}$ data set, we achieve new state-of-the-art results. Training code and pretrained models are available at https://github.com/cv516Buaa/SD-AANet .},
  archive      = {J_TCDS},
  author       = {Qi Zhao and Binghao Liu and Shuchang Lyu and Huojin Chen},
  doi          = {10.1109/TCDS.2023.3251371},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {177-189},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A self-distillation embedded supervised affinity attention model for few-shot segmentation},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human-collaborative artificial intelligence along with
social values in industry 5.0: A survey of the state-of-the-art.
<em>TCDS</em>, <em>16</em>(1), 165–176. (<a
href="https://doi.org/10.1109/TCDS.2023.3326192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The expected fifth industrial revolution or Industry 5.0 (I-5.0) is human-centered and concerns societal values, and sustainability. I-5.0 focuses on human and machine coworking by augmenting human-collaborative intelligent robots. The current developments in information communications and the increasing market need for high agility and innovative ways to tailor products urge the world for an I-5.0 transformation. Artificial intelligence (AI) plays a key role in I-5.0 in offering intelligent components, smart digital twins, smart cyber-physical systems, and cobotics for efficient, precise and human-centered decision-making. In this context, AI boosts human-robot collaboration in production for timely and precise responses to the market and thereof realizing the concept of mass personalization. In addition, since I-5.0 is based on the idea of the widespread connection of devices, and identities in and out of the production process, there is a high level of vulnerability in front of cyberattacks where AI assists in smart cyber defense techniques. This article is a review of I-5.0 human-center concerns, involvement of social values, and I-5.0 technological elements in connection with human-collaborative AI. The role of AI in mass personalization and defense in front of cybersecurity threats is analyzed, and the horizon of I-5.0 in AI context is depicted. Finally, the realization challenges are reviewed as I-5.0 aims to make the future instead of waiting to face the future, and there is a variety of challenges on this path.},
  archive      = {J_TCDS},
  author       = {Mahdi Khosravy and Neeraj Gupta and Antoine Pasquali and Nilanjan Dey and Rubén González Crespo and Olaf Witkowski},
  doi          = {10.1109/TCDS.2023.3326192},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {165-176},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Human-collaborative artificial intelligence along with social values in industry 5.0: A survey of the state-of-the-art},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An overview of brain fingerprint identification based on
various neuroimaging technologies. <em>TCDS</em>, <em>16</em>(1),
151–164. (<a href="https://doi.org/10.1109/TCDS.2023.3314155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a novel category of biometric features, research on brain fingerprints has become a hot topic in neuroscience, not only for its reliable performance on individual identification but also for specifying the brain activity of different humans. Such unique biometric data are extracted using various neuroimaging technologies. Although the literature has reviewed the extraction and application of brain fingerprints via electroencephalogram (EEG), little work has been done in presenting a comprehensive review that covers various neuroimaging techniques for extracting brain fingerprints. This article presents a systematic review of recent scientific literature that focuses on brain fingerprint identification using four major neuroimaging techniques, namely, EEG, magnetic resonance imaging (MRI), magnetoencephalography (MEG), and functional near-infrared spectroscopy (fNIRS). It first summarizes the data acquisition methods and the popular data sets, which are organized in the form of graphs and tables for intuitive comparison. Next, this article analyzes the advantages and disadvantages of brain fingerprint feature extraction methods based on different neuroimaging techniques and proposes an application trend and comparative analysis of identity recognition algorithms from the perspective of computational technology. Finally, we elaborate on the open questions and potential for further research.},
  archive      = {J_TCDS},
  author       = {Shihao Zhang and Wenting Yang and Haonan Mou and Zhaodi Pei and Fangyi Li and Xia Wu},
  doi          = {10.1109/TCDS.2023.3314155},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {151-164},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An overview of brain fingerprint identification based on various neuroimaging technologies},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial decision making against intelligent targets in
cooperative multiagent systems. <em>TCDS</em>, <em>16</em>(1), 141–150.
(<a href="https://doi.org/10.1109/TCDS.2023.3234061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a distributed adversarial decision-making approach for multiagent system (MAS), which is supposed to detect a team of intelligent targets. The MAS is demanded to achieve the best detection benefit against the intelligent targets that can shelter part of their features and prevent the detection according to an expert defense policy. One of the key challenges is how to achieve higher detection benefit that is both determined by the target assignment action of the MAS and the nonpredictable defense policy of the targets. To handle this problem, we first formulate the multiagent decision-making problem as a max–min problem of detection benefit and break it into separate parts that are easier to be optimized. Then, we introduce a new variant of distributed alternating direction method of multipliers (ADMM) to search the optimal solutions under the worst defense policy that the targets choose. To overcome the lack of access to the global convergence of multiblock ADMM, we add local additional variables to formulate a penalty for nonconvex parts of the local objective function. The convergence to an equilibrium and the optimality of the detection benefit are empirically validated by numerical simulations. The influence of the parameter setting is also presented and can be regarded as a prior suggestion for real applications.},
  archive      = {J_TCDS},
  author       = {Yang Li and Huaping Liu and Fuchun Sun and Zhihong Chen},
  doi          = {10.1109/TCDS.2023.3234061},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {141-150},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Adversarial decision making against intelligent targets in cooperative multiagent systems},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiagent multiobjective decision making and game for
saving public resources. <em>TCDS</em>, <em>16</em>(1), 124–140. (<a
href="https://doi.org/10.1109/TCDS.2023.3307722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertain environments and inefficient decision analysis restrict the efficient utilization of depletable public resources by multiagents, especially for the scenario involved with multiobjective game dilemmas and weak scalability of decision making. To address the above conundrums, this article proposes a multilayer games framework that integrates cognition, decision making, and countermeasures (CDCs). Through the transformation of agent preference to alliance communication structure, a cooperation-competition topology network (CCTN) model is constructed, which improves the convergence and solution efficiency of the game model. In view of the Gaussian kernel ascending dimension mapping, a game equilibrium particle swarm optimization (GEPSO) algorithm is designed to improve the efficiency of finding equilibrium solutions and solve the nondeterministic polynomial (NP) problem of multiobjective games. To validate the effectiveness and performance of the proposed methodology, a case study of collaborative detection of multivehicle is conducted using the proposed framework and model.},
  archive      = {J_TCDS},
  author       = {Xiwen Ma and Yibo Zhang and Wei Xie and Jingsong Yang and Weidong Zhang},
  doi          = {10.1109/TCDS.2023.3307722},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {124-140},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Multiagent multiobjective decision making and game for saving public resources},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prior knowledge-augmented broad reinforcement learning
framework for fault diagnosis of heterogeneous multiagent systems.
<em>TCDS</em>, <em>16</em>(1), 115–123. (<a
href="https://doi.org/10.1109/TCDS.2023.3266791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A heterogeneous multiagent system (MAS) can easily experience unpredictable faults due to its complex structure and involvement of different individuals. However, existing approaches have several issues, including complicated network architecture, insufficient feature extraction, and poor generalization ability. This study proposes a novel framework called prior knowledge-augmented broad reinforcement learning (PK-BRL) to effectively diagnose faults in a heterogeneous MAS. First, we construct a highly realistic visualized heterogeneous MAS and perform fault injection. Second, we present a novel fault diagnosis (FD) framework based on broad reinforcement learning (RL) with prior knowledge that effectively integrates offline RL and broad learning into the FD process. The interaction between heterogeneous multiagents and the constructed environment enables us to learn a superior FD strategy. Finally, experiments conducted on software-in-the-loop and hardware-in-the-loop platforms demonstrate that the proposed PK-BRL framework has a state-of-the-art diagnostic accuracy for the heterogeneous MAS, which offers valuable theoretical and practical significance for real-world application.},
  archive      = {J_TCDS},
  author       = {Li Guo and Yiran Ren and Runze Li and Bin Jiang},
  doi          = {10.1109/TCDS.2023.3266791},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {115-123},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Prior knowledge-augmented broad reinforcement learning framework for fault diagnosis of heterogeneous multiagent systems},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multiagent meta-based task offloading strategy for
mobile-edge computing. <em>TCDS</em>, <em>16</em>(1), 100–114. (<a
href="https://doi.org/10.1109/TCDS.2023.3246107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task offloading in mobile-edge computing (MEC) improves the efficacy of mobile devices (MDs) in terms of computing performance, data storage, and energy consumption by offloading computational tasks to edge servers. Efficient task offloading can leverage MEC technology to reduce task processing latency and energy consumption. By integrating the reasoning ability and machine intelligence of the cognitive computing architecture, such as SOAR and ACT-R, reinforcement learning (RL) algorithms have been applied to resolve the task offloading in MEC. To solve the problem that conventional deep RL (DRL) algorithms cannot adapt to dynamic environments, this article proposed a task offloading scheduling strategy which combined multiagent RL and meta-learning. In order to make the two actions of charging time and offloading strategy fully considered at the same time, we implemented a learning network of two agents on an MD. To efficiently train the policy network, we proposed a first-order approximation method based on the clipped surrogate objective. Finally, the experiments are designed with a variety of the number of subtasks, transmission rate, and edge server performance, and the results show that the MRL-based strategy has the overwhelming overall performance and can be quickly applied in various environments with good stability and generalization.},
  archive      = {J_TCDS},
  author       = {Weichao Ding and Fei Luo and Chunhua Gu and Zhiming Dai and Haifeng Lu},
  doi          = {10.1109/TCDS.2023.3246107},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {100-114},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A multiagent meta-based task offloading strategy for mobile-edge computing},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural manifold modulated continual reinforcement learning
for musculoskeletal robots. <em>TCDS</em>, <em>16</em>(1), 86–99. (<a
href="https://doi.org/10.1109/TCDS.2022.3231055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The continual learning and development are significant for robots to learn multiple tasks sequentially. The difficulty lies in balancing the efficient learning of new tasks and overcoming catastrophic forgetting of old tasks. Although many continual learning methods have been proposed for pattern recognition, continual reinforcement learning methods for redundant musculoskeletal and robotic systems are few and have limitations. Therefore, inspired by the developmental mechanisms in motor cortex, this article proposes a neural manifold modulated continual reinforcement learning method for musculoskeletal and robotic systems. First, a recurrent neural network (RNN) with an expected neural manifold is designed and conditions of weights are derived. Second, the ability of projectors for characterizing the neural manifold within RNN is analyzed. Furthermore, a continual reinforcement learning method of RNN is proposed with the modulation of a neural manifold. The method is validated by redundant musculoskeletal and robotic systems in simulation. The results suggest that it can realize continual reinforcement learning of multiple tasks in different movements and environments. Furthermore, compared with related works, the proposed method achieves better performance.},
  archive      = {J_TCDS},
  author       = {Jiahao Chen and Ziyu Chen and Chaojing Yao and Hong Qiao},
  doi          = {10.1109/TCDS.2022.3231055},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {86-99},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Neural manifold modulated continual reinforcement learning for musculoskeletal robots},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-based collaborative learning for multiagent systems
under distributed denial-of-service attacks. <em>TCDS</em>,
<em>16</em>(1), 75–85. (<a
href="https://doi.org/10.1109/TCDS.2022.3172937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article employs a reinforcement learning (RL) technique to investigate the distributed output tracking control of heterogeneous multiagent systems (MASs) under multiple Denial-of-Service (DoS) attacks. Different from existing results where the dynamic of the leader is known for partial or all agents, the leader’s system matrix is completely unknown for each follower in this article. To learn the leader system matrix, a data-based learning mechanism is first proposed using the idea of the data-driven method. Then, under the data-based learning mechanism, a resilient predictor subject to multiple DoS attacks is exploited to provide the estimation of the leader’s state for each agent, where adversaries attack different communication links independently. Moreover, a resilient dynamic output feedback controller is proposed to solve the output tracking control problem based on the output regulation theory. To consider the transient responses of agents, an RL-based dynamic output feedback controller is developed to realize the optimal output tracking control by solving discounted algebraic Riccati equations (AREs) in both offline and online ways. Theoretical analysis shows that the secure output tracking control of MASs can be achieved under the proposed data-based resilient learning control algorithm. Finally, a numerical example is provided to verify the effectiveness of theoretical analysis.},
  archive      = {J_TCDS},
  author       = {Yong Xu and Zheng-Guang Wu},
  doi          = {10.1109/TCDS.2022.3172937},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {75-85},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Data-based collaborative learning for multiagent systems under distributed denial-of-service attacks},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning skills from demonstrations: A trend from motion
primitives to experience abstraction. <em>TCDS</em>, <em>16</em>(1),
57–74. (<a href="https://doi.org/10.1109/TCDS.2023.3296166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The uses of robots are changing from static environments in factories to encompass novel concepts such as human–robot collaboration in unstructured settings. Preprogramming all the functionalities for robots becomes impractical, and hence, robots need to learn how to react to new events autonomously, just like humans. However, humans, unlike machines, are naturally skilled in responding to unexpected circumstances based on either experiences or observations. Hence, embedding such anthropoid behaviors into robots entails the development of neuro-cognitive models that emulate motor skills under a robot learning paradigm. Effective encoding of these skills is bound to the proper choice of tools and techniques. This survey paper studies different motion and behavior learning methods ranging from movement primitives (MPs) to experience abstraction (EA), applied to different robotic tasks. These methods are scrutinized and then experimentally benchmarked by reconstructing a standard pick-n-place task. Apart from providing a standard guideline for the selection of strategies and algorithms, this article aims to draw a perspective on their possible extensions and improvements.},
  archive      = {J_TCDS},
  author       = {Mehrdad Tavassoli and Sunny Katyara and Maria Pozzi and Nikhil Deshpande and Darwin G. Caldwell and Domenico Prattichizzo},
  doi          = {10.1109/TCDS.2023.3296166},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {57-74},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Learning skills from demonstrations: A trend from motion primitives to experience abstraction},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Observer-based event-triggered iterative learning consensus
for locally lipschitz nonlinear MASs. <em>TCDS</em>, <em>16</em>(1),
46–56. (<a href="https://doi.org/10.1109/TCDS.2023.3274794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article aims to realize the robust tracking for nonidentical locally Lipschitz nonlinear multiagent systems (MASs) with unmeasurable states, for which an observer-based distributed event-triggered iterative learning control (ILC) framework is proposed. With this framework, distributed state observers provide the indispensable state information for agents to learn to complete the task. An initial state learning strategy is implemented to relax consistent initial conditions, which can improve the learning accuracy. An event-triggering mechanism is designed to reduce the occupation of communication and computing resources during the iterative learning process of MASs. With locally Lipschitz nonlinearities and iteration-varying uncertainties caused by nonrepetitive initial states, the double-dynamics analysis (DDA) method is adopted to illustrate the convergence of the ILC process. By setting appropriate learning gains and constructing a quasi-globally Lipschitz condition via the DDA method, the robust convergence can be achieved for MASs in the presence of switching topologies. A numerical simulation is used to show the validity of the proposed framework.},
  archive      = {J_TCDS},
  author       = {Hongyi Li and Jinhao Luo and Hui Ma and Qi Zhou},
  doi          = {10.1109/TCDS.2023.3274794},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {46-56},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Observer-based event-triggered iterative learning consensus for locally lipschitz nonlinear MASs},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PFedEff: An efficient and personalized federated cognitive
learning framework in multiagent systems. <em>TCDS</em>, <em>16</em>(1),
31–45. (<a href="https://doi.org/10.1109/TCDS.2023.3288985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increase in data volume and environment complexity, real-world problems require more advanced algorithms to acquire useful information for further analysis or decision making. Cognitive learning (CL) effectively handles incomplete information, and multiagent systems can provide enough data for analysis. Inspired by distributed machine learning, federated learning (FL) has become an efficient framework for implementing CL algorithms in multiagent systems while preserving user privacy. However, traditional communication optimizations on the FL framework suffer from either large communication volumes or large accuracy loss. In this article, we propose pFedEff, a personalized FL framework with efficient communication that can reduce communication volume and preserve training accuracy. pFedEff uses two magnitude masks, two importance masks, and a personalized aggregation method to reduce the model and update size while maintaining the training accuracy. Specifically, we use a pretraining magnitude mask for approximated regularization to reduce the magnitude of ineffective parameters during training. We also use a post-training magnitude mask to eliminate the low-magnitude parameters after training. Furthermore, we use uploading and downloading importance masks to reduce the communication volume in both upload and download streams. Our experimental results show that pFedEff reduces up to 94% communication volume with only a 1% accuracy loss over other state-of-the-art FL algorithms. In addition, we conduct multiple ablation studies to evaluate the influence of hyperparameters in pFedEff, which shows the flexibility of pFedEff and its applicability in different scenarios.},
  archive      = {J_TCDS},
  author       = {Hongjian Shi and Jianqing Zhang and Shuming Fan and Ruhui Ma and Haibing Guan},
  doi          = {10.1109/TCDS.2023.3288985},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {31-45},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {PFedEff: An efficient and personalized federated cognitive learning framework in multiagent systems},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed cognitive learning strategy for
cooperative–competitive multiagent systems. <em>TCDS</em>,
<em>16</em>(1), 20–30. (<a
href="https://doi.org/10.1109/TCDS.2023.3242370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the distributed cognitive learning algorithm of cooperative–competitive multiagent systems (MASs) on undirected graphs is studied, using parabolic equations as a model. Two cases are considered: 1) without disturbance and 2) with boundary disturbance. A bipartite consensus learning strategy is designed for the disturbance-free case, enabling agents to cognitively learn cooperative and adversarial information transmitted by their neighbors, thus, reaching a synergistic consensus with other agents. For the case with boundary disturbance, an auxiliary system is first introduced and a disturbance observer is designed to estimate the disturbance. Then, a cognitive learning algorithm is designed, in which agents constantly adjust their behavior by sensing the external environment to make complex behavioral decisions. Based on the proposed algorithms, the well-posedness of the closed-loop system is verified using semi-group theory, and the bipartite consensus of the MASs is discussed using Lyapunov’s direct method. Finally, the effectiveness of the proposed algorithms is verified using the finite difference method and Chebyshev spectral method.},
  archive      = {J_TCDS},
  author       = {Yan-Jun Liu and Sai Zhang and Li Tang},
  doi          = {10.1109/TCDS.2023.3242370},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {20-30},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Distributed cognitive learning strategy for Cooperative–Competitive multiagent systems},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed process monitoring for multiagent systems
through cognitive learning. <em>TCDS</em>, <em>16</em>(1), 8–19. (<a
href="https://doi.org/10.1109/TCDS.2022.3214424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiagent systems are usually large-scaled with a growing degree of intelligence and integration. Direct applications of traditional (centralized) methods will become incompetent for effective process monitoring of multiagent systems. It necessitates the cognitive learning strategies that determine the effective interactions among subsystems or individuals. Therefore, in order to improve the monitoring performance, this article targets the development of a new distributed process monitoring method that has the cognitive learning ability by embedding an adaptive pickup rule. The proposed cognitive learning-based method can reduce the computation loads in both offline and online phases because only necessary information exchange (or communication topology) is involved. Furthermore, the threshold used for system monitoring is obtained by developing a fast search algorithm based on the statistical learning theory. Case studies on the wastewater treatment system, which can be regarded as a typical multiagent system, demonstrate the superiority of the proposed distributed process-monitoring method.},
  archive      = {J_TCDS},
  author       = {Hongtian Chen and Oguzhan Dogru and Santhosh Kumar Varanasi and Xunyuan Yin and Biao Huang},
  doi          = {10.1109/TCDS.2022.3214424},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {8-19},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Distributed process monitoring for multiagent systems through cognitive learning},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guest editorial special issue on cognitive learning of
multiagent systems. <em>TCDS</em>, <em>16</em>(1), 4–7. (<a
href="https://doi.org/10.1109/TCDS.2023.3325505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development and cognition of biological and intelligent individuals shed light on the development of cognitive, autonomous, and evolutionary robotics. Take the collective behavior of birds as an example, each individual effectively communicates information and learns from multiple neighbors, facilitating cooperative decision making among them. These interactions among individuals illuminate the growth and cognition of natural groups throughout the evolutionary process, and they can be effectively modeled as multiagent systems. Multiagent systems have the ability to solve problems that are difficult or impossible for an individual agent or a monolithic system to solve, which also improves the robustness and efficiency through collaborative learning. Multiagent learning is playing an increasingly important role in various fields, such as aerospace systems, intelligent transportation, smart grids, etc. With the environment growing increasingly intricate, characterized by factors, such as high dynamism and incomplete/imperfect observational data, the challenges associated with various tasks are escalating. These challenges encompass issues like information sharing, the definition of learning objectives, and grappling with the curse of dimensionality. Unfortunately, many of the existing methods are struggling to effectively address these multifaceted issues in the realm of cognitive intelligence. Furthermore, the field of cognitive learning in multiagent systems underscores the efficiency of distributed learning, demonstrating the capacity to acquire the skill of learning itself collectively. In light of this, multiagent learning, while holding substantial research significance, confronts a spectrum of learning problems that span from single to multiple agents, simplicity to complexity, low dimensionality to high dimensionality, and one domain to various other domains. Agents autonomously and rapidly make swarm intelligent decisions through cognitive learning overcoming the above challenges, which holds significant importance for the advancement of various practical fields.},
  archive      = {J_TCDS},
  author       = {Yang Tang and Wei Lin and Chenguang Yang and Nicola Gatti and Gary G. Yen},
  doi          = {10.1109/TCDS.2023.3325505},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {4-7},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Guest editorial special issue on cognitive learning of multiagent systems},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Editorial IEEE transactions on cognitive and developmental
systems. <em>TCDS</em>, <em>16</em>(1), 3. (<a
href="https://doi.org/10.1109/TCDS.2024.3353515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As we usher into the new year of 2024, in my capacity as the Editor-in-Chief of the IEEE Transactions on Cognitive and Developmental Systems (TCDS), I am happy to extend to you a tapestry of New Year greetings, may this year be filled with prosperity, success, and groundbreaking achievements in our shared fields.},
  archive      = {J_TCDS},
  author       = {Huajin Tang},
  doi          = {10.1109/TCDS.2024.3353515},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {3},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Editorial IEEE transactions on cognitive and developmental systems},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
