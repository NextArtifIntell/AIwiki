<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TVCG_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tvcg---609">TVCG - 609</h2>
<ul>
<li><details>
<summary>
(2024). A testbed for studying cybersickness and its mitigation in
immersive virtual reality. <em>TVCG</em>, <em>30</em>(12), 7788–7805.
(<a href="https://doi.org/10.1109/TVCG.2024.3448203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cybersickness (CS) represents one of the oldest problems affecting Virtual Reality (VR) technology. In an attempt to resolve or at least limit this form of discomfort, an increasing number of mitigation techniques have been proposed by academic and industrial researchers. However, the validation of such techniques is often carried out without grounding on a common methodology, making the comparison between the various works in the state of the art difficult. To address this issue, the present article proposes a novel testbed for studying CS in immersive VR and, in particular, methods to mitigate it. The testbed consists of four virtual scenarios, which have been designed to elicit CS in a targeted and predictable manner. The scenarios, grounded on available literature, support the extraction of objective metrics about user&#39;s performance. The testbed additionally integrates an experimental protocol that employs standard questionnaires as well as measurements typically adopted in state-of-the-art practice to assess levels of CS and other subjective aspects regarding User Experience. The article shows a possible use case of the testbed, concerning the evaluation of a CS mitigation technique that is compared with the absence of mitigation as baseline condition.},
  archive      = {J_TVCG},
  author       = {Davide Calandra and Fabrizio Lamberti},
  doi          = {10.1109/TVCG.2024.3448203},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {12},
  number       = {12},
  pages        = {7788-7805},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A testbed for studying cybersickness and its mitigation in immersive virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Implicit frictional dynamics with soft constraints.
<em>TVCG</em>, <em>30</em>(12), 7776–7787. (<a
href="https://doi.org/10.1109/TVCG.2024.3437417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamics simulation with frictional contacts is important for a wide range of applications, from cloth simulation to object manipulation. Recent methods using smoothed lagged friction forces have enabled robust and differentiable simulation of elastodynamics with friction. However, the resulting frictional behavior can be inaccurate and may not converge to analytic solutions. Here we evaluate the accuracy of lagged friction models in comparison with implicit frictional contact systems. We show that major inaccuracies near the stick-slip threshold in such systems are caused by lagging of friction forces rather than by smoothing the Coulomb friction curve. Furthermore, we demonstrate how systems involving implicit or lagged friction can be correctly used with higher-order time integration and highlight limitations in earlier attempts. We demonstrate how to exploit forward-mode automatic differentiation to simplify and, in some cases, improve the performance of the inexact Newton method. Finally, we show that other complex phenomena can also be simulated effectively while maintaining smoothness of the entire system. We extend our method to exhibit stick-slip frictional behavior and preserve volume on compressible and nearly-incompressible media using soft constraints.},
  archive      = {J_TVCG},
  author       = {Egor Larionov and Andreas Longva and Uri M. Ascher and Jan Bender and Dinesh K. Pai},
  doi          = {10.1109/TVCG.2024.3437417},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {12},
  number       = {12},
  pages        = {7776-7787},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Implicit frictional dynamics with soft constraints},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mountaineer: Topology-driven visual analytics for comparing
local explanations. <em>TVCG</em>, <em>30</em>(12), 7763–7775. (<a
href="https://doi.org/10.1109/TVCG.2024.3418653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing use of black-box Machine Learning (ML) techniques in critical applications, there is a growing demand for methods that can provide transparency and accountability for model predictions. As a result, a large number of local explainability methods for black-box models have been developed and popularized. However, machine learning explanations are still hard to evaluate and compare due to the high dimensionality, heterogeneous representations, varying scales, and stochastic nature of some of these methods. Topological Data Analysis (TDA) can be an effective method in this domain since it can be used to transform attributions into uniform graph representations, providing a common ground for comparison across different explanation methods. We present a novel topology-driven visual analytics tool, Mountaineer, that allows ML practitioners to interactively analyze and compare these representations by linking the topological graphs back to the original data distribution, model predictions, and feature attributions. Mountaineer facilitates rapid and iterative exploration of ML explanations, enabling experts to gain deeper insights into the explanation techniques, understand the underlying data distributions, and thus reach well-founded conclusions about model behavior. Furthermore, we demonstrate the utility of Mountaineer through two case studies using real-world data. In the first, we show how Mountaineer enabled us to compare black-box ML explanations and discern regions of and causes of disagreements between different explanations. In the second, we demonstrate how the tool can be used to compare and understand ML models themselves. Finally, we conducted interviews with three industry experts to help us evaluate our work.},
  archive      = {J_TVCG},
  author       = {Parikshit Solunke and Vitoria Guardieiro and João Rulff and Peter Xenopoulos and Gromit Yeuk-Yin Chan and Brian Barr and Luis Gustavo Nonato and Claudio Silva},
  doi          = {10.1109/TVCG.2024.3418653},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {12},
  number       = {12},
  pages        = {7763-7775},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Mountaineer: Topology-driven visual analytics for comparing local explanations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Text2NeRF: Text-driven 3D scene generation with neural
radiance fields. <em>TVCG</em>, <em>30</em>(12), 7749–7762. (<a
href="https://doi.org/10.1109/TVCG.2024.3361502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-driven 3D scene generation is widely applicable to video gaming, film industry, and metaverse applications that have a large demand for 3D scenes. However, existing text-to-3D generation methods are limited to producing 3D objects with simple geometries and dreamlike styles that lack realism. In this work, we present Text2NeRF, which is able to generate a wide range of 3D scenes with complicated geometric structures and high-fidelity textures purely from a text prompt. To this end, we adopt NeRF as the 3D representation and leverage a pre-trained text-to-image diffusion model to constrain the 3D reconstruction of the NeRF to reflect the scene description. Specifically, we employ the diffusion model to infer the text-related image as the content prior and use a monocular depth estimation method to offer the geometric prior. Both content and geometric priors are utilized to update the NeRF model. To guarantee textured and geometric consistency between different views, we introduce a progressive scene inpainting and updating strategy for novel view synthesis of the scene. Our method requires no additional training data but only a natural language description of the scene as the input. Extensive experiments demonstrate that our Text2NeRF outperforms existing methods in producing photo-realistic, multi-view consistent, and diverse 3D scenes from a variety of natural language prompts. Our code and model are available at https://github.com/eckertzhang/Text2NeRF .},
  archive      = {J_TVCG},
  author       = {Jingbo Zhang and Xiaoyu Li and Ziyu Wan and Can Wang and Jing Liao},
  doi          = {10.1109/TVCG.2024.3361502},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {12},
  number       = {12},
  pages        = {7749-7762},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Text2NeRF: Text-driven 3D scene generation with neural radiance fields},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive spline surface fitting with arbitrary topological
control mesh. <em>TVCG</em>, <em>30</em>(12), 7736–7748. (<a
href="https://doi.org/10.1109/TVCG.2024.3361488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing a spline surface from a given arbitrary topological triangle mesh is a fundamental and challenging problem in computer-aided design and engineering. This article introduces a novel surface fitting method utilizing G-NURBS capable of handling control meshes with arbitrary topologies. This method employs adaptive control point adjustment, guided by the geometric attributes of the input model, ensuring precise representation of sharp features such as edges and corners. Two primary strategies are employed: A parameter correspondence approach designed for sharp features and a control mesh iterative refinement technique that incorporates geometrical feature information. The proposed method has been tested and evaluated on various CAD models to demonstrate its effectiveness. This method can achieve higher fitting accuracy while faithfully preserving the geometrical features with fewer control points.},
  archive      = {J_TVCG},
  author       = {Yi-Bo Kou and Yi-Fei Feng and Li-Yong Shen and Xin Li and Chun-Ming Yuan},
  doi          = {10.1109/TVCG.2024.3361488},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {12},
  number       = {12},
  pages        = {7736-7748},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Adaptive spline surface fitting with arbitrary topological control mesh},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Video2Haptics: Converting video motion to dynamic haptic
feedback with bio-inspired event processing. <em>TVCG</em>,
<em>30</em>(12), 7717–7735. (<a
href="https://doi.org/10.1109/TVCG.2024.3360468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cinematic VR applications, haptic feedback can significantly enhance the sense of reality and immersion for users. The increasing availability of emerging haptic devices opens up possibilities for future cinematic VR applications that allow users to receive haptic feedback while they are watching videos. However, automatically rendering haptic cues from real-time video content, particularly from video motion, is a technically challenging task. In this article, we propose a novel framework called “Video2Haptics” that leverages the emerging bio-inspired event camera to capture event signals as a lightweight representation of video motion. We then propose efficient event-based visual processing methods to estimate force or intensity from video motion in the event domain, rather than the pixel domain. To demonstrate the application of Video2Haptics, we convert the estimated force or intensity to dynamic vibrotactile feedback on emerging haptic gloves, synchronized with the corresponding video motion. As a result, Video2Haptics allows users not only to view the video but also to perceive the video motion concurrently. Our experimental results show that the proposed event-based processing methods for force and intensity estimation are one to two orders of magnitude faster than conventional methods. Our user study results confirm that the proposed Video2Haptics framework can considerably enhance the users’ video experience.},
  archive      = {J_TVCG},
  author       = {Xiaoming Chen and Zeke Zexi Hu and Guangxin Zhao and Haisheng Li and Vera Chung and Aaron Quigley},
  doi          = {10.1109/TVCG.2024.3360468},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {12},
  number       = {12},
  pages        = {7717-7735},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Video2Haptics: Converting video motion to dynamic haptic feedback with bio-inspired event processing},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). KGScope: Interactive visual exploration of knowledge graphs
with embedding-based guidance. <em>TVCG</em>, <em>30</em>(12),
7702–7716. (<a href="https://doi.org/10.1109/TVCG.2024.3360690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs have been commonly used to represent relationships between entities and are utilized in the industry to enhance service qualities. As knowledge graphs integrate data from a variety of sources, they can also be useful references for data analysts. However, there is a lack of effective tools to make the most of the rich information in knowledge graphs. Existing knowledge graph exploration systems are ineffective because they did not consider various user needs and characteristics of knowledge graphs. Exploratory approaches specifically designed to uncover and summarize insights in knowledge graphs have not been well studied yet. In this article, we propose KGScope that supports interactive visual explorations and provides embedding-based guidance to derive insights from knowledge graphs. We demonstrate KGScope with usage scenarios and assess its efficacy in supporting the exploration of knowledge graphs with a user study. The results show that KGScope supports knowledge graph exploration effectively by providing useful information and helping explore the entire network.},
  archive      = {J_TVCG},
  author       = {Chao-Wen Hsuan Yuan and Tzu-Wei Yu and Jia-Yu Pan and Wen-Chieh Lin},
  doi          = {10.1109/TVCG.2024.3360690},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {12},
  number       = {12},
  pages        = {7702-7716},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {KGScope: Interactive visual exploration of knowledge graphs with embedding-based guidance},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comparative study on fixed-order event sequence
visualizations: Gantt, extended gantt, and stringline charts.
<em>TVCG</em>, <em>30</em>(12), 7687–7701. (<a
href="https://doi.org/10.1109/TVCG.2024.3358919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We conduct two in-lab experiments (N = 93) to evaluate the effectiveness of Gantt charts, extended Gantt charts, and stringline charts for visualizing fixed-order event sequence data. We first formulate five types of event sequences and define three types of sequence elements: point events, interval events, and the temporal gaps between them. Our two experiments focus on event sequences with a pre-defined, fixed order and measure task error rates and completion time. The first experiment shows single sequences and assesses the three charts’ performance in comparing event duration or gap. The second experiment shows multiple sequences and evaluates how well the charts reveal temporal patterns. The results suggest that when visualizing single fixed-order event sequences, 1) Gantt and extended Gantt charts lead to comparable error rates in the duration-comparing task; 2) Gantt charts exhibit either shorter or equal completion time than extended Gantt charts; 3) both Gantt and extended Gantt charts demonstrate shorter completion times than stringline charts; 4) however, stringline charts outperform the other two charts with fewer errors in the comparing task when event type counts are high. Additionally, when visualizing multiple point-based fixed-order event sequences, stringline charts require less time than Gantt charts for people to find temporal patterns. Based on these findings, we discuss design opportunities for visualizing fixed-order event sequences and discuss future avenues for optimizing these charts.},
  archive      = {J_TVCG},
  author       = {Junxiu Tang and Fumeng Yang and Jiang Wu and Yifang Wang and Jiayi Zhou and Xiwen Cai and Lingyun Yu and Yingcai Wu},
  doi          = {10.1109/TVCG.2024.3358919},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {12},
  number       = {12},
  pages        = {7687-7701},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A comparative study on fixed-order event sequence visualizations: Gantt, extended gantt, and stringline charts},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DRCmpVis: Visual comparison of physical targets in mobile
diminished and mixed reality. <em>TVCG</em>, <em>30</em>(12), 7672–7686.
(<a href="https://doi.org/10.1109/TVCG.2024.3358419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous physical objects in our daily lives are grouped or ranked according to a stereotyped presentation style. For example, in a library, books are typically grouped and ranked based on classification numbers. However, for better comparison, we often need to re-group or re-rank the books using additional attributes such as ratings, publishers, comments, publication years, keywords, prices, etc., or a combination of these factors. In this article, we propose a novel mobile DR/MR-based application framework named DRCmpVis to achieve in-context multi-attribute comparisons of physical objects with text labels or textual information. The physical objects are scanned in the real world using mobile cameras. All scanned objects are then segmented and labeled by a convolutional neural network and replaced (diminished) by their virtual avatars in a DR environment. We formulate three visual comparison strategies, including filtering, re-grouping, and re-ranking, which can be intuitively, flexibly, and seamlessly performed on their avatars. This approach avoids breaking the original layouts of the physical objects. The computation resources in virtual space can be fully utilized to support efficient object searching and multi-attribute visual comparisons. We demonstrate the usability, expressiveness, and efficiency of DRCmpVis through a user study, NASA TLX assessment, quantitative evaluation, and case studies involving different scenarios.},
  archive      = {J_TVCG},
  author       = {Richen Liu and Shunlong Ye and Zhifei Ding and Guang Yang and Shenghui Cheng and Klaus Mueller},
  doi          = {10.1109/TVCG.2024.3358419},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {12},
  number       = {12},
  pages        = {7672-7686},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DRCmpVis: Visual comparison of physical targets in mobile diminished and mixed reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Laplacian projection based global physical prior smoke
reconstruction. <em>TVCG</em>, <em>30</em>(12), 7657–7671. (<a
href="https://doi.org/10.1109/TVCG.2024.3358636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel framework for reconstructing fluid dynamics in real-life scenarios. Our approach leverages sparse view images and incorporates physical priors across long series of frames, resulting in reconstructed fluids with enhanced physical consistency. Unlike previous methods, we utilize a differentiable fluid simulator (DFS) and a differentiable renderer (DR) to exploit global physical priors, reducing reconstruction errors without the need for manual regularization coefficients. We introduce divergence-free Laplacian eigenfunctions (div-free LE) as velocity bases, improving computational efficiency and memory usage. By employing gradient-related strategies, we achieve better convergence and superior results. Extensive experiments demonstrate the effectiveness of our method, showcasing improved reconstruction quality and computational efficiency compared to existing approaches. We validate our approach using both synthetic and real data, highlighting its practical potential.},
  archive      = {J_TVCG},
  author       = {Shibang Xiao and Chao Tong and Qifan Zhang and Yunchi Cen and Frederick W. B. Li and Xiaohui Liang},
  doi          = {10.1109/TVCG.2024.3358636},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {12},
  number       = {12},
  pages        = {7657-7671},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Laplacian projection based global physical prior smoke reconstruction},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual analytics for machine learning: A data perspective
survey. <em>TVCG</em>, <em>30</em>(12), 7637–7656. (<a
href="https://doi.org/10.1109/TVCG.2024.3357065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The past decade has witnessed a plethora of works that leverage the power of visualization (VIS) to interpret machine learning (ML) models. The corresponding research topic, VIS4ML, keeps growing at a fast pace. To better organize the enormous works and shed light on the developing trend of VIS4ML, we provide a systematic review of these works through this survey. Since data quality greatly impacts the performance of ML models, our survey focuses specifically on summarizing VIS4ML works from the data perspective . First, we categorize the common data handled by ML models into five types, explain the unique features of each type, and highlight the corresponding ML models that are good at learning from them. Second, from the large number of VIS4ML works, we tease out six tasks that operate on these types of data (i.e., data-centric tasks) at different stages of the ML pipeline to understand, diagnose, and refine ML models. Lastly, by studying the distribution of 143 surveyed papers across the five data types, six data-centric tasks, and their intersections, we analyze the prospective research directions and envision future research trends.},
  archive      = {J_TVCG},
  author       = {Junpeng Wang and Shixia Liu and Wei Zhang},
  doi          = {10.1109/TVCG.2024.3357065},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {12},
  number       = {12},
  pages        = {7637-7656},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual analytics for machine learning: A data perspective survey},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beyond vision impairments: Redefining the scope of
accessible data representations. <em>TVCG</em>, <em>30</em>(12),
7619–7636. (<a href="https://doi.org/10.1109/TVCG.2024.3356566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing ubiquity of data in everyday life has elevated the importance of data literacy and accessible data representations, particularly for individuals with disabilities. While prior research predominantly focuses on the needs of the visually impaired, our survey aims to broaden this scope by investigating accessible data representations across a more inclusive spectrum of disabilities. After conducting a systematic review of 152 accessible data representation papers from ACM and IEEE databases, we found that roughly 78% of existing articles center on vision impairments. In this article, we conduct a comprehensive review of the remaining 22% of papers focused on underrepresented disability communities. We developed categorical dimensions based on accessibility, visualization, and human-computer interaction to classify the papers. These dimensions include the community of focus, issues addressed, contribution type, study methods, participants involved, data type, visualization type, and data domain. Our work redefines accessible data representations by illustrating their application for disabilities beyond those related to vision. Building on our literature review, we identify and discuss opportunities for future research in accessible data representations.},
  archive      = {J_TVCG},
  author       = {Brianna Lynn Wimer and Laura South and Keke Wu and Danielle Albers Szafir and Michelle A. Borkin and Ronald A. Metoyer},
  doi          = {10.1109/TVCG.2024.3356566},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {12},
  number       = {12},
  pages        = {7619-7636},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Beyond vision impairments: Redefining the scope of accessible data representations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simple and efficient? Evaluation of transitions for
task-driven cross-reality experiences. <em>TVCG</em>, <em>30</em>(12),
7601–7618. (<a href="https://doi.org/10.1109/TVCG.2024.3356949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inquiry into the impact of diverse transitions between cross-reality environments on user experience remains a compelling research endeavor. Existing work often offers fragmented perspectives on various techniques or confines itself to a singular segment of the reality-virtuality spectrum, be it virtual reality or augmented reality. This study embarks on bridging this knowledge gap by systematically assessing the effects of six prevalent transitions while users remain immersed in tasks spanning both virtual and physical domains. In particular, we investigate the effect of different transitions while the user is continuously engaged in a demanding task instead of purely focusing on a given transition. As a preliminary step, we evaluate these six transitions within the realm of pure virtual reality to establish a baseline. Our findings reveal a clear preference among participants for brief and efficient transitions in a task-driven experience, instead of transitions that prioritize interactivity and continuity. Subsequently, we extend our investigation into a cross-reality context, encompassing transitions between virtual and physical environments. Once again, our results underscore the prevailing preference for concise and effective transitions. Furthermore, our research offers intriguing insights about the potential mitigation of visual incoherence between virtual and augmented reality environments by utilizing different transitions.},
  archive      = {J_TVCG},
  author       = {Nico Feld and Pauline Bimberg and Benjamin Weyers and Daniel Zielasko},
  doi          = {10.1109/TVCG.2024.3356949},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {12},
  number       = {12},
  pages        = {7601-7618},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Simple and efficient? evaluation of transitions for task-driven cross-reality experiences},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Elucidating diurnal patterns in touch desire using social
media data toward design of haptic applications and displays.
<em>TVCG</em>, <em>30</em>(12), 7592–7600. (<a
href="https://doi.org/10.1109/TVCG.2024.3355413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in haptic technology have led researchers and engineers to seek out killer applications in which users can enjoy an experience of touch in AR/VR spaces. Such applications will respond appropriately to human desire for haptic experiences (i.e., touch desire) and thus it is essential for researchers and engineers to understand the nature of people&#39;s touch desires as they arise in the course of daily life. In this study, we employed Twitter data analysis to investigate a diurnal pattern in touch desire. Our results showed that touch desire identified in and extracted from Twitter texts did reveal a diurnal pattern. Touch desire tended to be at its lowest in the morning and increased as the day progressed. The time at which it peaked varied with the specific target of touch desire. Touch desire in relation to other people and objects reached its peak at night, but touch desire in relation to animals reached its peak at noon. These results were confirmed not only by our Twitter text analysis but also by data from other social media and an online survey. In addition, we found that the diurnal pattern of touch desire for each target shows a strong correlation with that of visual desire for the same target. This suggests that the diurnal pattern of touch desire is not limited to the sense of touch but is common to other sensory desires for each target. Our findings suggest that researchers need to take the time of day into account when investigating touch desire. Our findings also offer valuable insights for developers into the design of haptic applications and displays that takes into account the timing of daily peaks in touch desire.},
  archive      = {J_TVCG},
  author       = {Yusuke Ujitoko and Yuki Ban and Takumi Yokosaka},
  doi          = {10.1109/TVCG.2024.3355413},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {12},
  number       = {12},
  pages        = {7592-7600},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Elucidating diurnal patterns in touch desire using social media data toward design of haptic applications and displays},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolution-based shape and behavior co-design of virtual
agents. <em>TVCG</em>, <em>30</em>(12), 7579–7591. (<a
href="https://doi.org/10.1109/TVCG.2024.3355745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel co-design method for autonomous moving agents’ shape attributes and locomotion by combining deep reinforcement learning and evolution with user control. Our main inspiration comes from evolution, which has led to wide variability and adaptation in Nature and has significantly improved design and behavior simultaneously. Our method takes an input agent with optional user-defined constraints, such as leg parts that should not evolve or are only within the allowed ranges of changes. It uses physics-based simulation to determine its locomotion and finds a behavior policy for the input design that is used as a baseline for comparison. The agent is randomly modified within the allowed ranges, creating a new generation of several hundred agents. The generation is trained by transferring the previous policy, which significantly speeds up the training. The best-performing agents are selected, and a new generation is formed using their crossover and mutations. The next generations are then trained until satisfactory results are reached. We show a wide variety of evolved agents, and our results show that even with only 10% of allowed changes, the overall performance of the evolved agents improves by 50%. If more significant changes to the initial design are allowed, our experiments’ performance will improve even more to 150%. Our method significantly improved motion tasks without changing body structures, and it does not require considerable computation resources as it works on a single GPU and provides results by training thousands of agents within 30 minutes.},
  archive      = {J_TVCG},
  author       = {Zhiquan Wang and Bedrich Benes and Ahmed H. Qureshi and Christos Mousas},
  doi          = {10.1109/TVCG.2024.3355745},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {12},
  number       = {12},
  pages        = {7579-7591},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Evolution-based shape and behavior co-design of virtual agents},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A clinical user study investigating the benefits of adaptive
volumetric illumination sampling. <em>TVCG</em>, <em>30</em>(12),
7571–7578. (<a href="https://doi.org/10.1109/TVCG.2024.3353926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and fast understanding of the patient&#39;s anatomy is crucial in surgical decision making and particularly important in visceral surgery. Sophisticated visualization techniques such as 3D Volume Rendering can aid the surgeon and potentially lead to a benefit for the patient. Recently, we proposed a novel volume rendering technique called Adaptive Volumetric Illumination Sampling (AVIS) that can generate realistic lighting in real-time, even for high resolution images and volumes but without introducing additional image noise. In order to evaluate this new technique, we conducted a randomized, three-period crossover study comparing AVIS to conventional Direct Volume Rendering (DVR) and Path Tracing (PT). CT datasets from 12 patients were evaluated by 10 visceral surgeons who were either senior physicians or experienced specialists. The time needed for answering clinically relevant questions as well as the correctness of the answers were analyzed for each visualization technique. In addition to that, the perceived workload during these tasks was assessed for each technique, respectively. The results of the study indicate that AVIS has an advantage in terms of both time efficiency and most aspects of the perceived workload, while the average correctness of the given answers was very similar for all three methods. In contrast to that, Path Tracing seems to show particularly high values for mental demand and frustration. We plan to repeat a similar study with a larger participant group to consolidate the results.},
  archive      = {J_TVCG},
  author       = {Valentin Kraft and Christian Schumann and Daniela Salzmann and Dirk Weyhe and Gabriel Zachmann and Andrea Schenk},
  doi          = {10.1109/TVCG.2024.3353926},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {12},
  number       = {12},
  pages        = {7571-7578},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A clinical user study investigating the benefits of adaptive volumetric illumination sampling},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual guidance for user placement in avatar-mediated
telepresence between dissimilar spaces. <em>TVCG</em>, <em>30</em>(12),
7558–7570. (<a href="https://doi.org/10.1109/TVCG.2024.3354256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid advances in technology gradually realize immersive mixed-reality (MR) telepresence between distant spaces. This paper presents a novel visual guidance system for avatar-mediated telepresence, directing users to optimal placements that facilitate the clear transfer of gaze and pointing contexts through remote avatars in dissimilar spaces, where the spatial relationship between the remote avatar and the interaction targets may differ from that of the local user. Representing the spatial relationship between the user/avatar and interaction targets with angle-based interaction features, we assign recommendation scores of sampled local placements as their maximum feature similarity with remote placements. These scores are visualized as color-coded 2D sectors to inform the users of better placements for interaction with selected targets. In addition, virtual objects of the remote space are overlapped with the local space for the user to better understand the recommendations. We examine whether the proposed score measure agrees with the actual user perception of the partner&#39;s interaction context and find a score threshold for recommendation through user experiments in virtual reality (VR). A subsequent user study in VR investigates the effectiveness and perceptual overload of different combinations of visualizations. Finally, we conduct a user study in an MR telepresence scenario to evaluate the effectiveness of our method in real-world applications.},
  archive      = {J_TVCG},
  author       = {Dongseok Yang and Jiho Kang and Taehei Kim and Sung-Hee Lee},
  doi          = {10.1109/TVCG.2024.3354256},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {12},
  number       = {12},
  pages        = {7558-7570},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual guidance for user placement in avatar-mediated telepresence between dissimilar spaces},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient distortion-free neural projector deblurring in
dynamic projection mapping. <em>TVCG</em>, <em>30</em>(12), 7544–7557.
(<a href="https://doi.org/10.1109/TVCG.2024.3354957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic Projection Mapping (DPM) necessitates geometric compensation of the projection image based on the position and orientation of moving objects. Additionally, the projector&#39;s shallow depth of field results in pronounced defocus blur even with minimal object movement. Achieving delay-free DPM with high image quality requires real-time implementation of geometric compensation and projector deblurring. To meet this demand, we propose a framework comprising two neural components: one for geometric compensation and another for projector deblurring. The former component warps the image by detecting the optical flow of each pixel in both the projection and captured images. The latter component performs real-time sharpening as needed. Ideally, our network&#39;s parameters should be trained on data acquired in an actual environment. However, training the network from scratch while executing DPM, which demands real-time image generation, is impractical. Therefore, the network must undergo pre-training. Unfortunately, there are no publicly available large real datasets for DPM due to the diverse image quality degradation patterns. To address this challenge, we propose a realistic synthetic data generation method that numerically models geometric distortion and defocus blur in real-world DPM. Through exhaustive experiments, we have confirmed that the model trained on the proposed dataset achieves projector deblurring in the presence of geometric distortions with a quality comparable to state-of-the-art methods.},
  archive      = {J_TVCG},
  author       = {Yuta Kageyama and Daisuke Iwai and Kosuke Sato},
  doi          = {10.1109/TVCG.2024.3354957},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {12},
  number       = {12},
  pages        = {7544-7557},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Efficient distortion-free neural projector deblurring in dynamic projection mapping},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pattern guided UV recovery for realistic video garment
texturing. <em>TVCG</em>, <em>30</em>(12), 7531–7543. (<a
href="https://doi.org/10.1109/TVCG.2024.3354727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fast growth of E-Commerce creates a global market worth USD 821 billion for online fashion shopping. What unique about fashion presentation is that, the same design can usually be offered with different cloths textures. However, only real video capturing or manual per-frame editing can be used for virtual showcase on the same design with different textures, both of which are heavily labor intensive. In this paper, we present a pattern-based approach for UV and shading recovery from a captured real video so that the garment&#39;s texture can be replaced automatically. The core of our approach is a per-pixel UV regression module via blended-weight multilayer perceptrons (MLPs) driven by the detected discrete correspondences from the cloth pattern. We propose a novel loss on the Jacobian of the UV mapping to create pleasant seams around the folding areas and the boundary of occluded regions while avoiding UV distortion. We also adopts the temporal constraint to ensure consistency and accuracy in UV prediction across adjacent frames. We show that our approach is robust to a variety type of clothes, in the wild illuminations and with challenging motions. We show plausible texture replacement results in our experiment, in which the folding and overlapping of the garment can be greatly preserved. We also show clear qualitative and quantitative improvement compared to the baselines as well. With the one-click setup, we look forward to our approach contributing to the growth of fashion E-commerce.},
  archive      = {J_TVCG},
  author       = {Youyi Zhan and Tuanfeng Y. Wang and Tianjia Shao and Kun Zhou},
  doi          = {10.1109/TVCG.2024.3354727},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {12},
  number       = {12},
  pages        = {7531-7543},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Pattern guided UV recovery for realistic video garment texturing},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GUESS: GradUally enriching SyntheSis for text-driven human
motion generation. <em>TVCG</em>, <em>30</em>(12), 7518–7530. (<a
href="https://doi.org/10.1109/TVCG.2024.3352002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel cascaded diffusion-based generative framework for text-driven human motion synthesis, which exploits a strategy named G rad U ally E nriching S ynthe S is ( GUESS as its abbreviation). The strategy sets up generation objectives by grouping body joints of detailed skeletons in close semantic proximity together and then replacing each of such joint group with a single body-part node. Such an operation recursively abstracts a human pose to coarser and coarser skeletons at multiple granularity levels. With gradually increasing the abstraction level, human motion becomes more and more concise and stable, significantly benefiting the cross-modal motion synthesis task. The whole text-driven human motion synthesis problem is then divided into multiple abstraction levels and solved with a multi-stage generation framework with a cascaded latent diffusion model: an initial generator first generates the coarsest human motion guess from a given text description; then, a series of successive generators gradually enrich the motion details based on the textual description and the previous synthesized results. Notably, we further integrate GUESS with the proposed dynamic multi-condition fusion mechanism to dynamically balance the cooperative effects of the given textual condition and synthesized coarse motion prompt in different generation stages. Extensive experiments on large-scale datasets verify that GUESS outperforms existing state-of-the-art methods by large margins in terms of accuracy, realisticness, and diversity.},
  archive      = {J_TVCG},
  author       = {Xuehao Gao and Yang Yang and Zhenyu Xie and Shaoyi Du and Zhongqian Sun and Yang Wu},
  doi          = {10.1109/TVCG.2024.3352002},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {12},
  number       = {12},
  pages        = {7518-7530},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GUESS: GradUally enriching SyntheSis for text-driven human motion generation},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Overlap removal by stochastic gradient descent with(out)
shape awareness. <em>TVCG</em>, <em>30</em>(12), 7500–7517. (<a
href="https://doi.org/10.1109/TVCG.2024.3351479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many 2D visualizations, data points are projected without considering their surface area, although they are often represented as shapes in visualization tools. These shapes support the display of information such as labels or encode data with size or color. However, inappropriate shape and size selections can lead to overlaps that obscure information and hinder the visualization&#39;s exploration. Overlap Removal (OR) algorithms have been developed as a layout post-processing solution to ensure that the visible graphical elements accurately represent the underlying data. As the original data layout contains vital information about its topology, it is essential for OR algorithms to preserve it as much as possible. This article presents an extension of the previously published FORBID algorithm by introducing a new approach that models OR as a joint stress and scaling optimization problem, utilizing efficient stochastic gradient descent. The goal is to produce an overlap-free layout that proposes a compromise between compactness (to ensure the encoded data is still readable) and preservation of the original layout (to preserve the structures that convey information about the data). Additionally, this article proposes SORDID, a shape-aware adaptation of FORBID that can handle the OR task on data points having any polygonal shape. Our approaches are compared against state-of-the-art algorithms, and several quality metrics demonstrate their effectiveness in removing overlaps while retaining the compactness and structures of the input layouts.},
  archive      = {J_TVCG},
  author       = {Loann Giovannangeli and Frederic Lalanne and Romain Giot and Romain Bourqui},
  doi          = {10.1109/TVCG.2024.3351479},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {12},
  number       = {12},
  pages        = {7500-7517},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Overlap removal by stochastic gradient descent with(out) shape awareness},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CylinderTag: An accurate and flexible marker for
cylinder-shape objects pose estimation based on projective invariants.
<em>TVCG</em>, <em>30</em>(12), 7486–7499. (<a
href="https://doi.org/10.1109/TVCG.2024.3350901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-precision pose estimation based on visual markers has been a thriving research topic in the field of computer vision. However, the suitability of traditional flat markers on curved objects is limited due to the diverse shapes of curved surfaces, which hinders the development of high-precision pose estimation for curved objects. Therefore, this paper proposes a novel visual marker called CylinderTag, which is designed for developable curved surfaces such as cylindrical surfaces. CylinderTag is a cyclic marker that can be firmly attached to objects with a cylindrical shape. Leveraging the manifold assumption, the cross-ratio in projective invariance is utilized for encoding in the direction of zero curvature on the surface. Additionally, to facilitate the usage of CylinderTag, we propose a heuristic search-based marker generator and a high-performance recognizer as well. Moreover, an all-encompassing evaluation of CylinderTag properties is conducted by means of extensive experimentation, covering detection rate, detection speed, dictionary size, localization jitter, and pose estimation accuracy. CylinderTag showcases superior detection performance from varying view angles in comparison to traditional visual markers, accompanied by higher localization accuracy. Furthermore, CylinderTag boasts real-time detection capability and an extensive marker dictionary, offering enhanced versatility and practicality in a wide range of applications. Experimental results demonstrate that the CylinderTag is a highly promising visual marker for use on cylindrical-like surfaces, thus offering important guidance for future research on high-precision visual localization of cylinder-shaped objects.},
  archive      = {J_TVCG},
  author       = {Shaoan Wang and Mingzhu Zhu and Yaoqing Hu and Dongyue Li and Fusong Yuan and Junzhi Yu},
  doi          = {10.1109/TVCG.2024.3350901},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {12},
  number       = {12},
  pages        = {7486-7499},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CylinderTag: An accurate and flexible marker for cylinder-shape objects pose estimation based on projective invariants},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comprehensive visual question answering on point clouds
through compositional scene manipulation. <em>TVCG</em>,
<em>30</em>(12), 7473–7485. (<a
href="https://doi.org/10.1109/TVCG.2023.3340679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Question Answering on 3D Point Cloud (VQA-3D) is an emerging yet challenging field that aims at answering various types of textual questions given an entire point cloud scene. To tackle this problem, we propose the CLEVR3D , a large-scale VQA-3D dataset consisting of 171K questions from 8,771 3D scenes. Specifically, we develop a question engine leveraging 3D scene graph structures to generate diverse reasoning questions, covering the questions of objects’ attributes (i.e., size, color, and material) and their spatial relationships. Through such a manner, we initially generated 44K questions from 1,333 real-world scenes. Moreover, a more challenging setup is proposed to remove the confounding bias and adjust the context from a common-sense layout. Such a setup requires the network to achieve comprehensive visual understanding when the 3D scene is different from the general co-occurrence context (e.g., chairs always exist with tables). To this end, we further introduce the compositional scene manipulation strategy and generate 127K questions from 7,438 augmented 3D scenes, which can improve VQA-3D models for real-world comprehension. Built upon the proposed dataset, we baseline several VQA-3D models, where experimental results verify that the CLEVR3D can significantly boost other 3D scene understanding tasks.},
  archive      = {J_TVCG},
  author       = {Xu Yan and Zhihao Yuan and Yuhao Du and Yinghong Liao and Yao Guo and Shuguang Cui and Zhen Li},
  doi          = {10.1109/TVCG.2023.3340679},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {12},
  number       = {12},
  pages        = {7473-7485},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Comprehensive visual question answering on point clouds through compositional scene manipulation},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Message from the ISMAR 2024 science and technology program
chairs and TVCG guest editors. <em>TVCG</em>, <em>30</em>(11), vii. (<a
href="https://doi.org/10.1109/TVCG.2024.3453128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this special issue of IEEE Transactions on Visualization and Computer Graphics (TVCG), we are pleased to present the journal papers from the 23rd IEEE International Symposium on Mixed and Augmented Reality (ISMAR 2024), which will be held as a hybrid conference between October 21 and 25, 2024 in the Greater Seattle Area, USA. ISMAR continues the over twenty-year long tradition of IWAR, ISMR, and ISAR, and is the premier conference for Mixed and Augmented Reality in the world.},
  archive      = {J_TVCG},
  author       = {Ulrich Eck and Maki Sugimoto and Misha Sra and Markus Tatzgern and Jeanine Stefanucci and Ian Williams},
  doi          = {10.1109/TVCG.2024.3453128},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {vii},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Message from the ISMAR 2024 science and technology program chairs and TVCG guest editors},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Message from the editor-in-chief and from the associate
editor-in-chief. <em>TVCG</em>, <em>30</em>(11), v–vi. (<a
href="https://doi.org/10.1109/TVCG.2024.3453148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Welcome to the 10th IEEE Transactions on Visualization and Computer Graphics (TVCG) special issue on IEEE International Symposium on Mixed and Augmented Reality (ISMAR). This volume contains a total of 44 full papers selected for and presented at ISMAR 2024, held from October 21 to 25, 2024 in the Greater Seattle Area, USA, in a hybrid mode.},
  archive      = {J_TVCG},
  author       = {Han-Wei Shen and Kiyoshi Kiyokawa},
  doi          = {10.1109/TVCG.2024.3453148},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {v-vi},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Message from the editor-in-chief and from the associate editor-in-chief},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HaptoFloater: Visuo-haptic augmented reality by embedding
imperceptible color vibration signals for tactile display control in a
mid-air image. <em>TVCG</em>, <em>30</em>(11), 7463–7472. (<a
href="https://doi.org/10.1109/TVCG.2024.3456175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose HaptoFloater, a low-latency mid-air visuo-haptic augmented reality (VHAR) system that utilizes imperceptible color vibrations. When adding tactile stimuli to the visual information of a mid-air image, the user should not perceive the latency between the tactile and visual information. However, conventional tactile presentation methods for mid-air images, based on camera-detected fingertip positioning, introduce latency due to image processing and communication. To mitigate this latency, we use a color vibration technique; humans cannot perceive the vibration when the display alternates between two different color stimuli at a frequency of 25 Hz or higher. In our system, we embed this imperceptible color vibration into the mid-air image formed by a micromirror array plate, and a photodiode on the fingertip device directly detects this color vibration to provide tactile stimulation. Thus, our system allows for the tactile perception of multiple patterns on a mid-air image in 59.5 ms. In addition, we evaluate the visual-haptic delay tolerance on a mid-air display using our VHAR system and a tactile actuator with a single pattern and faster response time. The results of our user study indicate a visual-haptic delay tolerance of 110.6 ms, which is considerably larger than the latency associated with systems using multiple tactile patterns.},
  archive      = {J_TVCG},
  author       = {Rina Nagano and Takahiro Kinoshita and Shingo Hattori and Yuichi Hiroi and Yuta Itoh and Takefumi Hiraki},
  doi          = {10.1109/TVCG.2024.3456175},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7463-7472},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HaptoFloater: Visuo-haptic augmented reality by embedding imperceptible color vibration signals for tactile display control in a mid-air image},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collaborative forensic autopsy documentation and supervised
report generation using a hybrid mixed-reality environment and
generative AI. <em>TVCG</em>, <em>30</em>(11), 7452–7462. (<a
href="https://doi.org/10.1109/TVCG.2024.3456212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forensic investigation is a complex procedure involving experts working together to establish cause of death and report findings to legal authorities. While new technologies are being developed to provide better post-mortem imaging capabilities—including mixed-reality (MR) tools to support 3D visualisation of such data—these tools do not integrate seamlessly into their existing collaborative workflow and report authoring process, requiring extra steps, e.g. to extract imagery from the MR tool and combine with physical autopsy findings for inclusion in the report. Therefore, in this work we design and evaluate a new forensic autopsy report generation workflow and present a novel documentation system using hybrid mixed-reality approaches to integrate visualisation, voice and hand interaction, as well as collaboration and procedure recording. Our preliminary findings indicate that this approach has the potential to improve data management, aid reviewability, and thus, achieve more robust standards. Further, it potentially streamlines report generation and minimise dependency on external tools and assistance, reducing autopsy time and related costs. This system also offers significant potential for education. A free copy of this paper and all supplemental materials are available at https://osf.io/ygfzx.},
  archive      = {J_TVCG},
  author       = {Vahid Pooryousef and Maxime Cordeil and Lonni Besançon and Richard Bassed and Tim Dwyer},
  doi          = {10.1109/TVCG.2024.3456212},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7452-7462},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Collaborative forensic autopsy documentation and supervised report generation using a hybrid mixed-reality environment and generative AI},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RingGesture: A ring-based mid-air gesture typing system
powered by a deep-learning word prediction framework. <em>TVCG</em>,
<em>30</em>(11), 7441–7451. (<a
href="https://doi.org/10.1109/TVCG.2024.3456179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text entry is a critical capability for any modern computing experience, with lightweight augmented reality (AR) glasses being no exception. Designed for all-day wearability, a limitation of lightweight AR glass is the restriction to the inclusion of multiple cameras for extensive field of view in hand tracking. This constraint underscores the need for an additional input device. We propose a system to address this gap: a ring-based mid-air gesture typing technique, RingGesture, utilizing electrodes to mark the start and end of gesture trajectories and inertial measurement units (IMU) sensors for hand tracking. This method offers an intuitive experience similar to raycast-based mid-air gesture typing found in VR headsets, allowing for a seamless translation of hand movements into cursor navigation. To enhance both accuracy and input speed, we propose a novel deep-learning word prediction framework, Score Fusion, comprised of three key components: a) a word-gesture decoding model, b) a spatial spelling correction model, and c) a lightweight contextual language model. In contrast, this framework fuses the scores from the three models to predict the most likely words with higher precision. We conduct comparative and longitudinal studies to demonstrate two key findings: firstly, the overall effectiveness of RingGesture, which achieves an average text entry speed of 27.3 words per minute (WPM) and a peak performance of 47.9 WPM. Secondly, we highlight the superior performance of the Score Fusion framework, which offers a 28.2% improvement in uncorrected Character Error Rate over a conventional word prediction framework, Naive Correction, leading to a 55.2% improvement in text entry speed for RingGesture. Additionally, RingGesture received a System Usability Score of 83 signifying its excellent usability.},
  archive      = {J_TVCG},
  author       = {Junxiao Shen and Roger Boldu and Arpit Kalla and Michael Glueck and Hemant Bhaskar Surale and Amy Karlson},
  doi          = {10.1109/TVCG.2024.3456179},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7441-7451},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {RingGesture: A ring-based mid-air gesture typing system powered by a deep-learning word prediction framework},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TouchMark: Partial tactile feedback design for upper limb
rehabilitation in virtual reality. <em>TVCG</em>, <em>30</em>(11),
7430–7440. (<a href="https://doi.org/10.1109/TVCG.2024.3456173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of Virtual Reality (VR) technology, especially in medical rehabilitation, has expanded to include tactile cues along with visual stimuli. For patients with upper limb hemiplegia, tangible handles with haptic stimuli could improve their ability to perform daily activities. Traditional VR controllers are unsuitable for patient rehabilitation in VR, necessitating the design of specialized tangible handles with integrated tracking devices. Besides, matching tactile stimulation with corresponding virtual visuals could strengthen users&#39; embodiment (i.e., owning and controlling virtual bodies) in VR, which is crucial for patients&#39; training with virtual hands. Haptic stimuli have been shown to amplify the embodiment in VR, whereas the effect of partial tactile stimulation from tangible handles on embodiment remains to be clarified. This research, including three experiments, aims to investigate how partial tactile feedback of tangible handles impacts users&#39; embodiment, and we proposed a design concept called TouchMark for partial tactile stimuli that could help users quickly connect the physical and virtual worlds. To evaluate users&#39; tactile and comfort perceptions when grasping tangible handles in a non-VR setting, various handles with three partial tactile factors were manipulated in Study 1. In Study 2, we explored the effects of partial feedback using three forms of TouchMark on the embodiment of healthy users in VR, with various tangible handles, while Study 3 focused on similar investigations with patients. These handles were utilized to complete virtual food preparation tasks. The tactile and comfort perceptions of tangible handles and users&#39; embodiment were evaluated in this research using questionnaires and interviews. The results indicate that TouchMark with haptic line and ring forms over no stimulation would significantly enhance users&#39; embodiment, especially for patients. The low-cost and innovative TouchMark approach may assist users, particularly those with limited VR experience, in achieving the embodiment and enhancing their virtual interactive experience.},
  archive      = {J_TVCG},
  author       = {Jingjing Zhang and Mengjie Huang and Yonglin Chen and Kai-Lun Liao and Jiajia Shi and Hai-Ning Liang and Rui Yang},
  doi          = {10.1109/TVCG.2024.3456173},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7430-7440},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TouchMark: Partial tactile feedback design for upper limb rehabilitation in virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A framework for multimodal medical image interaction.
<em>TVCG</em>, <em>30</em>(11), 7419–7429. (<a
href="https://doi.org/10.1109/TVCG.2024.3456163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical doctors rely on images of the human anatomy, such as magnetic resonance imaging (MRI), to localize regions of interest in the patient during diagnosis and treatment. Despite advances in medical imaging technology, the information conveyance remains unimodal. This visual representation fails to capture the complexity of the real, multisensory interaction with human tissue. However, perceiving multimodal information about the patient&#39;s anatomy and disease in real-time is critical for the success of medical procedures and patient outcome. We introduce a Multimodal Medical Image Interaction (MMII) framework to allow medical experts a dynamic, audiovisual interaction with human tissue in three-dimensional space. In a virtual reality environment, the user receives physically informed audiovisual feedback to improve the spatial perception of anatomical structures. MMII uses a model-based sonification approach to generate sounds derived from the geometry and physical properties of tissue, thereby eliminating the need for hand-crafted sound design. Two user studies involving 34 general and nine clinical experts were conducted to evaluate the proposed interaction framework&#39;s learnability, usability, and accuracy. Our results showed excellent learnability of audiovisual correspondence as the rate of correct associations significantly improved ($p &lt; 0.001$) over the course of the study. MMII resulted in superior brain tumor localization accuracy ($p &lt; 0.05$) compared to conventional medical image interaction. Our findings substantiate the potential of this novel framework to enhance interaction with medical images, for example, during surgical procedures where immediate and precise feedback is needed.},
  archive      = {J_TVCG},
  author       = {Laura Schütz and Sasan Matinfar and Gideon Schafroth and Navid Navab and Merle Fairhurst and Arthur Wagner and Benedikt Wiestler and Ulrich Eck and Nassir Navab},
  doi          = {10.1109/TVCG.2024.3456163},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7419-7429},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A framework for multimodal medical image interaction},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cultural reflections in virtual reality: The effects of user
ethnicity in avatar matching experiences on sense of embodiment.
<em>TVCG</em>, <em>30</em>(11), 7408–7418. (<a
href="https://doi.org/10.1109/TVCG.2024.3456196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matching avatar characteristics to a user can impact sense of embodiment (SoE) in YR. However, few studies have examined how participant demographics may interact with these matching effects. We recruited a diverse and racially balanced sample of 78 participants to investigate the differences among participant groups when embodying both demographically matched and unmatched avatars. We found that participant ethnicity emerged as a significant factor, with Asian and Black participants reporting lower total SoE compared to Hispanic participants. Furthermore, we found that user ethnicity significantly influences ownership (a subscale of SoE), with Asian and Black participants exhibiting stronger effects of matched avatar ethnicity compared to White participants. Additionally, Hispanic participants showed no significant differences, suggesting complex dynamics in ethnic-racial identity. Our results also reveal significant main effects of matched avatar ethnicity and gender on SoE, indicating the importance of considering these factors in VR experiences. These findings contribute valuable insights into understanding the complex dynamics shaping VR experiences across different demographic groups.},
  archive      = {J_TVCG},
  author       = {Tiffany D. Do and Juanita Benjamin and Camille Isabella Protko and Ryan P. McMahan},
  doi          = {10.1109/TVCG.2024.3456196},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7408-7418},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Cultural reflections in virtual reality: The effects of user ethnicity in avatar matching experiences on sense of embodiment},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Expressive 3D facial animation generation based on
local-to-global latent diffusion. <em>TVCG</em>, <em>30</em>(11),
7397–7407. (<a href="https://doi.org/10.1109/TVCG.2024.3456213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Facial animations, crucial to augmented and mixed reality digital media, have evolved from mere aesthetic elements to potent storytelling media. Despite considerable progress in facial animation of neutral emotions, existing methods still struggle to capture the authenticity of emotions. This paper introduces a novel approach to capture fine facial expressions and generate facial animations using audio synchronization. Our method consists of two key components: First, the Local-to-global Latent Diffusion Model (LG-LDM) tailored for authentic facial expressions, which can integrate audio, time step, facial expressions, and other conditions towards possible encoding of emotionally rich yet latent features in response to possibly noisy raw audio signals. The core of LG-LDM is our carefully designed Facial Denoiser Model (FDM) for aligning the local-to-global animation feature with audio. Second, we redesign an Emotion-centric Vector Quantized-Variational AutoEncoder framework (EVQ-VAE) to finely decode the subtle differences under different emotions and reconstruct the final 3D facial geometry. Our work significantly contributes to the key challenges of emotionally realistic 3D facial animation for audio synchronization and enhances the immersive experience and emotional depth in augmented and mixed reality applications. We provide a reproducibility kit including our code, dataset, and detailed instructions for running the experiments. This kit is available at https://github.com/wangxuanx/Face-Diffusion-Model.},
  archive      = {J_TVCG},
  author       = {Wenfeng Song and Xuan Wang and Yiming Jiang and Shuai Li and Aimin Hao and Xia Hou and Hong Qin},
  doi          = {10.1109/TVCG.2024.3456213},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7397-7407},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Expressive 3D facial animation generation based on local-to-global latent diffusion},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From avatars to agents: Self-related cues through embodiment
and personalization affect body perception in virtual reality.
<em>TVCG</em>, <em>30</em>(11), 7386–7396. (<a
href="https://doi.org/10.1109/TVCG.2024.3456211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our work investigates the influence of self-related cues in the design of virtual humans on body perception in virtual reality. In a $2\times 2$ mixed design, 64 participants faced photorealistic virtual humans either as a motion-synchronized embodied avatar or as an autonomous moving agent, appearing subsequently with a personalized and generic texture. Our results unveil that self-related cues through embodiment and personalization yield an individual and complemented increase in participants&#39; sense of embodiment and self-identification towards the virtual human. Different body weight modification and estimation tasks further showed an impact of both factors on participants&#39; body weight perception. Additional analyses revealed that the participant&#39;s body mass index predicted body weight estimations in all conditions and that participants&#39; self-esteem and body shape concerns correlated with different body weight perception results. Hence, we have demonstrated the occurrence of double standards through induced self-related cues in virtual human perception, especially through embodiment.},
  archive      = {J_TVCG},
  author       = {Marie Luisa Fielder and Erik Wolf and Nina Döllinger and David Mal and Mario Botsch and Marc Erich Latoschik and Carolin Wienrich},
  doi          = {10.1109/TVCG.2024.3456211},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7386-7396},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {From avatars to agents: Self-related cues through embodiment and personalization affect body perception in virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HOIMotion: Forecasting human motion during human-object
interactions using egocentric 3D object bounding boxes. <em>TVCG</em>,
<em>30</em>(11), 7375–7385. (<a
href="https://doi.org/10.1109/TVCG.2024.3456161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present HOIMotion - a novel approach for human motion forecasting during human-object interactions that integrates information about past body poses and egocentric 3D object bounding boxes. Human motion forecasting is important in many augmented reality applications but most existing methods have only used past body poses to predict future motion. HOIMotion first uses an encoder-residual graph convolutional network (GCN) and multi-layer perceptrons to extract features from body poses and egocentric 3D object bounding boxes, respectively. Our method then fuses pose and object features into a novel pose-object graph and uses a residual-decoder GCN to forecast future body motion. We extensively evaluate our method on the Aria digital twin (ADT) and MoGaze datasets and show that HOIMotion consistently outperforms state-of-the-art methods by a large margin of up to 8.7% on ADT and 7.2% on MoGaze in terms of mean per joint position error. Complementing these evaluations, we report a human study (N=20) that shows that the improvements achieved by our method result in forecasted poses being perceived as both more precise and more realistic than those of existing methods. Taken together, these results reveal the significant information content available in egocentric 3D object bounding boxes for human motion forecasting and the effectiveness of our method in exploiting this information.},
  archive      = {J_TVCG},
  author       = {Zhiming Hu and Zheming Yin and Daniel Haeufle and Syn Schmitt and Andreas Bulling},
  doi          = {10.1109/TVCG.2024.3456161},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7375-7385},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HOIMotion: Forecasting human motion during human-object interactions using egocentric 3D object bounding boxes},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Measuring and predicting multisensory reaction latency: A
probabilistic model for visual-auditory integration. <em>TVCG</em>,
<em>30</em>(11), 7364–7374. (<a
href="https://doi.org/10.1109/TVCG.2024.3456185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual/augmented reality (VR/AR) devices offer both immersive imagery and sound. With those wide-field cues, we can simultaneously acquire and process visual and auditory signals to quickly identify objects, make decisions, and take action. While vision often takes precedence in perception, our visual sensitivity degrades in the periphery. In contrast, auditory sensitivity can exhibit an opposite trend due to the elevated interaural time difference. What occurs when these senses are simultaneously integrated, as is common in VR applications such as 360° video watching and immersive gaming? We present a computational and probabilistic model to predict VR users&#39; reaction latency to visual-auditory multisensory targets. To this aim, we first conducted a psychophysical experiment in VR to measure the reaction latency by tracking the onset of eye movements. Experiments with numerical metrics and user studies with naturalistic scenarios showcase the model&#39;s accuracy and generalizability. Lastly, we discuss the potential applications, such as measuring the sufficiency of target appearance duration in immersive video playback, and suggesting the optimal spatial layouts for AR interface design.},
  archive      = {J_TVCG},
  author       = {Xi Peng and Yunxiang Zhang and Daniel Jiménez-Navarro and Ana Serrano and Karol Myszkowski and Qi Sun},
  doi          = {10.1109/TVCG.2024.3456185},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7364-7374},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Measuring and predicting multisensory reaction latency: A probabilistic model for visual-auditory integration},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust collaborative visual-inertial SLAM for mobile
augmented reality. <em>TVCG</em>, <em>30</em>(11), 7354–7363. (<a
href="https://doi.org/10.1109/TVCG.2024.3456152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving precise real-time localization and ensuring robustness are critical challenges in multi-user mobile AR applications. Leveraging collaborative information to augment tracking accuracy on lightweight devices and fortify overall system robustness emerges as a crucial necessity. In this paper, we propose a robust centralized collaborative rnulti-agent VI-SLAM system for mobile AR interaction and server-side efficient consistent mapping. The system deploys a lightweight VIO frontend on mobile devices for real-time tracking, and a backend running on a remote server to update multiple submaps. When overlapping areas between submaps across agents are detected, the system performs submap fusion to establish a globally consistent map. Additionally, we propose a map registration and fusion strategy based on covisibility areas for online registration and fusion in multi-agent scenarios. To improve the tracking accuracy of the frontend on agent, we introduce a strategy for updating the global map to the local map at a moderate frequency between the camera-rate pose estimation of the frontend VIO and the low-frequency global map optimization, using a tightly coupled strategy to achieve consistency of the multi-agent frontend poses estimation in the global map. The effectiveness of the proposed method is further confirmed by executing backend mapping on the server and deploying VIO frontends on multiple mobile devices for AR demostration. Additionally, we discuss the scalability of the proposed system by analyzing network traffic, synchronization frequency, and other factors at both the agent and server ends.},
  archive      = {J_TVCG},
  author       = {Xiaokun Pan and Gan Huang and Ziyang Zhang and Jinyu Li and Hujun Bao and Guofeng Zhang},
  doi          = {10.1109/TVCG.2024.3456152},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7354-7363},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Robust collaborative visual-inertial SLAM for mobile augmented reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An immersive and interactive VR dataset to elicit emotions.
<em>TVCG</em>, <em>30</em>(11), 7343–7353. (<a
href="https://doi.org/10.1109/TVCG.2024.3456202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images and videos are widely used to elicit emotions; however, their visual appeal differs from real-world experiences. With virtual reality becoming more realistic, immersive, and interactive, we envision virtual environments to elicit emotions effectively, rapidly, and with high ecological validity. This work presents the first interactive virtual reality dataset to elicit emotions. We created five interactive virtual environments based on corresponding validated 360° videos and validated their effectiveness with 160 participants. Our results show that our virtual environments successfully elicit targeted emotions. Compared with the existing methods using images or videos, our dataset allows virtual reality researchers and practitioners to integrate their designs effectively with emotion elicitation settings in an immersive and interactive way.},
  archive      = {J_TVCG},
  author       = {Weiwei Jiang and Maximiliane Windl and Benjamin Tag and Zhanna Sarsenbayeva and Sven Mayer},
  doi          = {10.1109/TVCG.2024.3456202},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7343-7353},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {An immersive and interactive VR dataset to elicit emotions},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Classification of internal and external distractions in an
educational VR environment using multimodal features. <em>TVCG</em>,
<em>30</em>(11), 7332–7342. (<a
href="https://doi.org/10.1109/TVCG.2024.3456207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) can potentially enhance student engagement and memory retention in the classroom. However, distraction among participants in a VR-based classroom is a significant concern. Several factors, including mind wandering, external noise, stress, etc., can cause students to become internally and/or externally distracted while learning. To detect distractions, single or multi-modal features can be used. A single modality is found to be insufficient to detect both internal and external distractions, mainly because of individual variability. In this work, we investigated multi-modal features: eye tracking and EEG data, to classify the internal and external distractions in an educational VR environment. We set up our educational VR environment and equipped it for multi-modal data collection. We implemented different machine learning (ML) methods, including k-nearest-neighbors (kNN), Random Forest (RF), one-dimensional convolutional neural network - long short-term memory (1 D-CNN-LSTM), and two-dimensional convolutional neural networks (2D-CNN) to classify participants&#39; internal and external distraction states using the multi-modal features. We performed cross-subject, cross-session, and gender-based grouping tests to evaluate our models. We found that the RF classifier achieves the highest accuracy over 83% in the cross-subject test, around 68% to 78% in the cross-session test, and around 90% in the gender-based grouping test compared to other models. SHAP analysis of the extracted features illustrated greater contributions from the occipital and prefrontal regions of the brain, as well as gaze angle, gaze origin, and head rotation features from the eye tracking data.},
  archive      = {J_TVCG},
  author       = {Sarker M Asish and Arun K Kulshreshth and Christoph W Borst and Shaon Sutradhar},
  doi          = {10.1109/TVCG.2024.3456207},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7332-7342},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Classification of internal and external distractions in an educational VR environment using multimodal features},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MobiTangibles: Enabling physical manipulation experiences of
virtual precision hand-held tools’ miniature control in VR.
<em>TVCG</em>, <em>30</em>(11), 7321–7331. (<a
href="https://doi.org/10.1109/TVCG.2024.3456191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Realistic simulation for miniature control interactions, typically identified by precise and confined motions, commonly found in precision hand-held tools, like calipers, powered engravers, retractable knives, etc., are beneficial for skill training associated with these kinds of tools in virtual reality (VR) environments. However, existing approaches aiming to simulate hand-held tools&#39; miniature control manipulation experiences in VR entail prototyping complexity and require expertise, posing challenges for novice users and individuals with limited resources. Addressing this challenge, we introduce MobiTangibles—proxies for precision hand-held tools&#39; miniature control interactions utilizing smartphone-based magnetic field sensing. MobiTangibles passively replicate fundamental miniature control experiences associated with hand-held tools, such as single-axis translation and rotation, enabling quick and easy use for diverse VR scenarios without requiring extensive technical knowledge. We conducted a comprehensive technical evaluation to validate the functionality of MobiTangibles across diverse settings, including evaluations for electromagnetic interference within indoor environments. In a user-centric evaluation involving 15 participants across bare hands, VR controllers, and MobiTangibles conditions, we further assessed the quality of miniaturized manipulation experiences in VR. Our findings indicate that MobiTangibles outperformed conventional methods in realism and fatigue, receiving positive feedback.},
  archive      = {J_TVCG},
  author       = {Abhijeet Mishra and Harshvardhan Singh and Aman Parnami and Jainendra Shukla},
  doi          = {10.1109/TVCG.2024.3456191},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7321-7331},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MobiTangibles: Enabling physical manipulation experiences of virtual precision hand-held tools&#39; miniature control in VR},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A real-time and interactive fluid modeling system for mixed
reality. <em>TVCG</em>, <em>30</em>(11), 7310–7320. (<a
href="https://doi.org/10.1109/TVCG.2024.3456140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the realm of mixed reality, the capability to dynamically render environmental effects with high realism plays a crucial role in amplifying user engagement and interaction. Fluid dynamics, in particular, stand out as essential elements for crafting immersive virtual settings. This includes the simulation of phenomena like smoke, fire, and clouds, which are instrumental in enriching the virtual experience. This work showcases a cutting-edge system developed to produce dynamic and interactive fluid effects that mirror real captured data in real-time for mixed reality applications. This innovative system seamlessly incorporates fluid reconstruction alongside velocity estimation processes within the Unity engine environment. Our approach leverages a novel physics-based differentiable rendering technique, grounded in the principles of light transport in participating media, to simulate the intricate behaviors of fluid while ensuring high fidelity in visual appearance. To further enhance realism, we have expanded our framework to include the estimation of velocity fields, addressing the critical need for fluid motion simulation. The practical application of these techniques demonstrates the system&#39;s capacity to offer a robust platform for fluid modeling in mixed reality environments. Through extensive evaluations, we illustrate the effectiveness of our approach in various scenes, underscoring its potential to transform mixed reality content creation by providing developers with the tools to incorporate highly realistic and interactive fluid seamlessly.},
  archive      = {J_TVCG},
  author       = {Yunchi Cen and Hanchen Deng and Yue Ma and Xiaohui Liang},
  doi          = {10.1109/TVCG.2024.3456140},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7310-7320},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A real-time and interactive fluid modeling system for mixed reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient and accurate semi-automatic neuron tracing with
extended reality. <em>TVCG</em>, <em>30</em>(11), 7299–7309. (<a
href="https://doi.org/10.1109/TVCG.2024.3456197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuron tracing, alternately referred to as neuron reconstruction, is the procedure for extracting the digital representation of the three-dimensional neuronal morphology from stacks of microscopic images. Achieving accurate neuron tracing is critical for profiling the neuroanatomical structure at single-cell level and analyzing the neuronal circuits and projections at whole-brain scale. However, the process often demands substantial human involvement and represents a nontrivial task. Conventional solutions towards neuron tracing often contend with challenges such as non-intuitive user interactions, suboptimal data generation throughput, and ambiguous visualization. In this paper, we introduce a novel method that leverages the power of extended reality (XR) for intuitive and progressive semi-automatic neuron tracing in real time. In our method, we have defined a set of interactors for controllable and efficient interactions for neuron tracing in an immersive environment. We have also developed a GPU-accelerated automatic tracing algorithm that can generate updated neuron reconstruction in real time. In addition, we have built a visualizer for fast and improved visual experience, particularly when working with both volumetric images and 3D objects. Our method has been successfully implemented with one virtual reality (VR) headset and one augmented reality (AR) headset with satisfying results achieved. We also conducted two user studies and proved the effectiveness of the interactors and the efficiency of our method in comparison with other approaches for neuron tracing.},
  archive      = {J_TVCG},
  author       = {Jie Chen and Zexin Yuan and Jiaqi Xi and Ziqin Gao and Ying Li and Xiaoqiang Zhu and Yun Stone Shi and Frank Guan and Yimin Wang},
  doi          = {10.1109/TVCG.2024.3456197},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7299-7309},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Efficient and accurate semi-automatic neuron tracing with extended reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring the effect of viewing attributes of mobile AR
interfaces on remote collaborative and competitive tasks. <em>TVCG</em>,
<em>30</em>(11), 7288–7298. (<a
href="https://doi.org/10.1109/TVCG.2024.3456177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile devices have the potential to facilitate remote tasks through Augmented Reality (AR) solutions by integrating digital information into the real world. Although prior studies have explored Mobile Augmented Reality (MAR) for co-located collaboration, none have investigated the impact of various viewing attributes that can influence remote task performance, such as target object viewing angles, synchronization styles, or having a secondary small screen showing other users current view in the MAR environment. In this paper, we explore five techniques considering these attributes, specifically designed for two modes of remote tasks: collaborative and competitive. We conducted a user study employing various combinations of those attributes for both tasks. In both instances, results indicate users&#39; optimal performance and preference for the technique that allows asynchronous viewing of object manipulations on the small screen. Overall, this paper contributes novel techniques for remote tasks in MAR, addressing aspects such as viewing angle and synchronization in object manipulation alongside secondary small-screen interfaces. Additionally, it presents the results of a user study evaluating the effectiveness, usability, and user preference of these techniques in remote settings and offers a set of recommendations for designing and implementing MAR solutions to enhance remote activities.},
  archive      = {J_TVCG},
  author       = {Nelusha Nugegoda and Marium-E Jannat and Khalad Hasan and Patricia Lasserre},
  doi          = {10.1109/TVCG.2024.3456177},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7288-7298},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring the effect of viewing attributes of mobile AR interfaces on remote collaborative and competitive tasks},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tasks reflected in the eyes: Egocentric gaze-aware visual
task type recognition in virtual reality. <em>TVCG</em>,
<em>30</em>(11), 7277–7287. (<a
href="https://doi.org/10.1109/TVCG.2024.3456164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With eye tracking finding widespread utility in augmented reality and virtual reality headsets, eye gaze has the potential to recognize users&#39; visual tasks and adaptively adjust virtual content displays, thereby enhancing the intelligence of these headsets. However, current studies on visual task recognition often focus on scene-specific tasks, like copying tasks for office environments, which lack applicability to new scenarios, e.g., museums. In this paper, we propose four scene-agnostic task types for facilitating task type recognition across a broader range of scenarios. We present a new dataset that includes eye and head movement data recorded from 20 participants while they engaged in four task types across 15 360-degree VR videos. Using this dataset, we propose an egocentric gaze-aware task type recognition method, TRCLP, which achieves promising results. Additionally, we illustrate the practical applications of task type recognition with three examples. Our work offers valuable insights for content developers in designing task-aware intelligent applications. Our dataset and source code are available at zhimin-wang.github.io/TaskTypeRecognition.html.},
  archive      = {J_TVCG},
  author       = {Zhimin Wang and Feng Lu},
  doi          = {10.1109/TVCG.2024.3456164},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7277-7287},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Tasks reflected in the eyes: Egocentric gaze-aware visual task type recognition in virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Depth perception in optical see-through augmented reality:
Investigating the impact of texture density, luminance contrast, and
color contrast. <em>TVCG</em>, <em>30</em>(11), 7266–7276. (<a
href="https://doi.org/10.1109/TVCG.2024.3456162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The immersive augmented reality (AR) system necessitates precise depth registration between virtual objects and the real scene. Prior studies have emphasized the efficacy of surface texture in providing depth cues to enhance depth perception across various media, including the real scene, virtual reality, and AR. However, these studies predominantly focus on black-and-white textures, leaving a gap in understanding the effectiveness of colored textures. To address this gap and further explore texture-related factors in AR, a series of experiments were conducted to investigate the effects of different texture cues on depth perception using the perceptual matching method. Findings indicate that the absolute depth error increases with decreasing contrast under black-and-white texture. Moreover, textures with higher color contrast also contribute to enhanced accuracy of depth judgments in AR. However, no significant effect of texture density on depth perception was observed. The findings serve as a theoretical reference for texture design in AR, aiding in the optimization of virtual-real registration processes.},
  archive      = {J_TVCG},
  author       = {Chaochao Liu and Shining Ma and Yue Liu and Yongtian Wang and Weitao Song},
  doi          = {10.1109/TVCG.2024.3456162},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7266-7276},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Depth perception in optical see-through augmented reality: Investigating the impact of texture density, luminance contrast, and color contrast},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An exploratory expert-study for multi-type haptic feedback
for automotive virtual reality tasks. <em>TVCG</em>, <em>30</em>(11),
7255–7265. (<a href="https://doi.org/10.1109/TVCG.2024.3456203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous research has shown that integrating haptic feedback can improve immersion and realism in automotive VR applications. However, current haptic feedback approaches primarily focus on a single feedback type. This means users must switch between devices to experience haptic stimuli for different feedback types, such as grabbing, collision, or weight simulation. This restriction limits the ability to simulate haptics realistically for complex tasks such as maintenance. To address this issue, we evaluated existing feedback devices based on our requirements analysis to determine which devices are most suitable for simulating these three feedback types. Since no suitable haptic feedback system can simulate all three feedback types simultaneously, we evaluated which devices can be combined. Based on that, we devised a new multi-type haptic feedback system combining three haptic feedback devices. We evaluated the system with different feedback-type combinations through a qualitative expert study involving twelve automotive VR experts. The results showed that combining weight and collision feedback yielded the best and most realistic experience. The study also highlighted technical limitations in current grabbing devices. Our findings provide insights into the effectiveness of haptic device combinations and practical boundaries for automotive virtual reality tasks.},
  archive      = {J_TVCG},
  author       = {Alexander Achberger and Patrick Gebhardt and Michael Sedlmair},
  doi          = {10.1109/TVCG.2024.3456203},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7255-7265},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {An exploratory expert-study for multi-type haptic feedback for automotive virtual reality tasks},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual perceptual confidence: Exploring discrepancies
between self-reported and actual distance perception in virtual reality.
<em>TVCG</em>, <em>30</em>(11), 7245–7254. (<a
href="https://doi.org/10.1109/TVCG.2024.3456165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality (VR) systems are widely used, and it is essential to know if spatial perception in virtual environments (VEs) is similar to reality. Research indicates that users tend to underestimate distances in VR. Prior work suggests that actual distance judgments in VR may not always match the users self-reported preference of where they think they most accurately estimated distances. However, no explicit investigation evaluated whether user preferences match actual performance in a spatial judgment task. We used blind walking to explore potential dissimilarities between actual distance estimates and user-selected preferences of visual complexities, VE conditions, and targets. Our findings show a gap between user preferences and actual performance when visual complexities were varied, which has implications for better visual perception understanding, VR applications design, and research in spatial perception, indicating the need to calibrate and align user preferences and true spatial perception abilities in VR.},
  archive      = {J_TVCG},
  author       = {Yahya Hmaiti and Mykola Maslych and Amirpouya Ghasemaghaei and Ryan K Ghamandi and Joseph J. LaViola},
  doi          = {10.1109/TVCG.2024.3456165},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7245-7254},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual perceptual confidence: Exploring discrepancies between self-reported and actual distance perception in virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Filtering on the go: Effect of filters on gaze pointing
accuracy during physical locomotion in extended reality. <em>TVCG</em>,
<em>30</em>(11), 7234–7244. (<a
href="https://doi.org/10.1109/TVCG.2024.3456153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eye tracking filters have been shown to improve accuracy of gaze estimation and input for stationary settings. However, their effectiveness during physical movement remains underexplored. In this work, we compare common online filters in the context of physical locomotion in extended reality and propose alterations to improve them for on-the-go settings. We conducted a computational experiment where we simulate performance of the online filters using data on participants attending visual targets located in world-, path-, and two head-based reference frames while standing, walking, and jogging. Our results provide insights into the filters&#39; effectiveness and factors that affect it, such as the amount of noise caused by locomotion and differences in compensatory eye movements, and demonstrate that filters with saccade detection prove most useful for on-the-go settings. We discuss the implications of our findings and conclude with guidance on gaze data filtering for interaction in extended reality.},
  archive      = {J_TVCG},
  author       = {Pavel Manakhov and Ludwig Sidenmark and Ken Pfeuffer and Hans Gellersen},
  doi          = {10.1109/TVCG.2024.3456153},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7234-7244},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Filtering on the go: Effect of filters on gaze pointing accuracy during physical locomotion in extended reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Is video gaming a cure for cybersickness? Gamers experience
less cybersickness than non-gamers in a VR self-motion task.
<em>TVCG</em>, <em>30</em>(11), 7225–7233. (<a
href="https://doi.org/10.1109/TVCG.2024.3456176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cybersickness remains a major drawback of Virtual Reality (VR) headsets, as a breadth of stationary experiences with visual self-motion can result in visually-induced motion sickness. However, not everybody experiences the same intensity or type of adverse symptoms. Here we propose that prior experience with virtual environments can predict ones degree of cybersickness. Video gaming can enhance visuospatial abilities, which in-turn relate negatively to cybersickness - meaning that consistently engaging in virtual environments can result in protective habituation effects. In a controlled stationary VR experiment, we found that ‘VR-naive’ video gamers experienced significantly less cybersickness in a virtual tunnel-travel task and outperformed ‘VR-naive’ non-video gamers on a visual attention task. These findings strongly motivate the use of non-VR games for training VR cybersickness resilience, with future research needed to further understand the mechanism(s) by which gamers become cybersickness resilient - potentially expanding access to VR for even the most susceptible participants.},
  archive      = {J_TVCG},
  author       = {Katharina M. T. Pöhlmann and Gang Li and Graham Wilson and Mark McGill and Frank Pollick and Stephen Brewster},
  doi          = {10.1109/TVCG.2024.3456176},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7225-7233},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Is video gaming a cure for cybersickness? gamers experience less cybersickness than non-gamers in a VR self-motion task},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Immersive study analyzer: Collaborative immersive analysis
of recorded social VR studies. <em>TVCG</em>, <em>30</em>(11),
7214–7224. (<a href="https://doi.org/10.1109/TVCG.2024.3456146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality (VR) has become an important tool for conducting behavioral studies in realistic, reproducible environments. In this paper, we present ISA, an Immersive Study Analyzer system designed for the comprehensive analysis of social VR studies. For in-depth analysis of participant behavior, ISA records all user actions, speech, and the contextual environment of social VR studies. A key feature is the ability to review and analyze such immersive recordings collaboratively in VR, through support of behavioral coding and user-defined analysis queries for efficient identification of complex behavior. Respatialization of the recorded audio streams enables analysts to follow study participants&#39; conversations in a natural and intuitive way. To support phases of close and loosely coupled collaboration, ISA allows joint and individual temporal navigation, and provides tools to facilitate collaboration among users at different temporal positions. An expert review confirms that ISA effectively supports collaborative immersive analysis, providing a novel and effective tool for nuanced understanding of user behavior in social VR studies.},
  archive      = {J_TVCG},
  author       = {Anton Lammert and Gareth Rendle and Felix Immohr and Annika Neidhardt and Karlheinz Brandenburg and Alexander Raake and Bernd Froehlich},
  doi          = {10.1109/TVCG.2024.3456146},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7214-7224},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Immersive study analyzer: Collaborative immersive analysis of recorded social VR studies},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gaze-contingent layered optical see-through displays with a
confidence-driven view volume. <em>TVCG</em>, <em>30</em>(11),
7203–7213. (<a href="https://doi.org/10.1109/TVCG.2024.3456204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vergence-accommodation conflict (VAC) presents a major perceptual challenge for head-mounted displays with a fixed image plane. Varifocal and layered display designs can mitigate the VAC. However, the image quality of varifocal displays is affected by imprecise eye tracking, whereas layered displays suffer from reduced image contrast as the distance between layers increases. Combined designs support a larger workspace and tolerate some eye-tracking error. However, any layered design with a fixed layer spacing restricts the amount of error compensation and limits the in-focus contrast. We extend previous hybrid designs by introducing confidence-driven volume control, which adjusts the size of the view volume at runtime. We use the eye tracker&#39;s confidence to control the spacing of display layers and optimize the trade-off between the display&#39;s view volume and the amount of eye tracking error the display can compensate. In the case of high-quality focus point estimation, our approach provides high in-focus contrast, whereas low-quality eye tracking increases the view volume to tolerate the error. We describe our design, present its implementation as an optical-see head-mounted display using a multiplicative layer combination, and present an evaluation comparing our design with previous approaches.},
  archive      = {J_TVCG},
  author       = {Christoph Ebner and Alexander Plopski and Dieter Schmalstieg and Denis Kalkofen},
  doi          = {10.1109/TVCG.2024.3456204},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7203-7213},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Gaze-contingent layered optical see-through displays with a confidence-driven view volume},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal feedback methods for advancing the accessibility
of immersive virtual reality for people with balance impairments due to
multiple sclerosis. <em>TVCG</em>, <em>30</em>(11), 7193–7202. (<a
href="https://doi.org/10.1109/TVCG.2024.3456189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintaining balance in immersive virtual reality (VR) environments poses a significant challenge for users, particularly affecting those with pre-existing balance disorders. This study investigates the efficacy of multimodal feedback-comprising auditory, vibrotactile, and visual stimuli-in mitigating balance issues within VR. A sample of 68 participants, divided equally between individuals with balance deficits related to multiple sclerosis and those without, was evaluated. The research explored the impact of various feedback conditions on balance performance. The results demonstrated that the multimodal feedback condition significantly enhanced balance control compared to other conditions, with statistical analysis confirming this improvement (p &lt;. 001). These findings underscore the potential of integrated sensory feedback in addressing balance-related difficulties in VR, thereby improving the overall accessibility and user experience for individuals affected by balance impairments. This research contributes valuable insights into optimizing VR environments for enhanced stability and user comfort.},
  archive      = {J_TVCG},
  author       = {M Rasel Mahmud and Alberto Cordova and John Quarles},
  doi          = {10.1109/TVCG.2024.3456189},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7193-7202},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multimodal feedback methods for advancing the accessibility of immersive virtual reality for people with balance impairments due to multiple sclerosis},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VPRF: Visual perceptual radiance fields for foveated image
synthesis. <em>TVCG</em>, <em>30</em>(11), 7183–7192. (<a
href="https://doi.org/10.1109/TVCG.2024.3456184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural radiance fields (NeRF) has achieved revolutionary breakthrough in the novel view synthesis task for complex 3D scenes. However, this new paradigm struggles to meet the requirements for real-time rendering and high perceptual quality in virtual reality. In this paper, we propose VPRF, a novel visual perceptual based radiance fields representation method, which for the first time integrates the visual acuity and contrast sensitivity models of human visual system (HVS) into the radiance field rendering framework. Initially, we encode both the appearance and visual sensitivity information of the scene into our radiance field representation. Then, we propose a visual perceptual sampling strategy, allocating computational resources according to the HVS sensitivity of different regions. Finally, we propose a sampling weight-constrained training scheme to ensure the effectiveness of our sampling strategy and improve the representation of the radiance field based on the scene content. Experimental results demonstrate that our method renders more efficiently, with higher PSNR and SSIM in the foveal and salient regions compared to the state-of-the-art FoV-NeRF. The results of the user study confirm that our rendering results exhibit high-fidelity visual perception.},
  archive      = {J_TVCG},
  author       = {Zijun Wang and Jian Wu and Runze Fan and Wei Ke and Lili Wang},
  doi          = {10.1109/TVCG.2024.3456184},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7183-7192},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VPRF: Visual perceptual radiance fields for foveated image synthesis},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A sense of urgency on the sense of agency: Challenges in
evaluating agency and embodiment in virtual reality. <em>TVCG</em>,
<em>30</em>(11), 7172–7182. (<a
href="https://doi.org/10.1109/TVCG.2024.3456139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Control over an avatar in virtual reality can improve one&#39;s perceived sense of agency and embodiment towards their avatar. Yet, the relationship between control on agency and embodiment remains unclear. This work aims to investigate two main ideas: (1) the effectiveness of currently used metrics in measuring agency and embodiment and (2) the relationship between different levels of control on agency, embodiment, and cognitive performance. To do this, we conducted a between-participants user study with three conditions on agency ($\mathrm{n}=57$). Participants embodied an avatar with one of three types of control (i.e., Low - control over head only, Medium - control over head and torso, or High - control over head, torso, and arms) and completed a Stroop test. Our results indicate that the degree of control afforded to participants impacted their embodiment and cognitive performance but, as expected, could not be detected in the self-reported agency scores. Furthermore, our results elucidated further insights into the relationship between control and embodiment, suggesting potential uncanny valley-like effects. Future work should aim to refine agency measures to better capture the effect of differing levels of control and consider other methodologies to measure agency.},
  archive      = {J_TVCG},
  author       = {Christopher You and Roshan Venkatakrishnan and Rohith Venkatakrishnan and Zhuoming Han and Benjamin Lok and Tabitha Peck},
  doi          = {10.1109/TVCG.2024.3456139},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7172-7182},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A sense of urgency on the sense of agency: Challenges in evaluating agency and embodiment in virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Frankenstein’s monster in the metaverse: User interaction
with customized virtual agents. <em>TVCG</em>, <em>30</em>(11),
7162–7171. (<a href="https://doi.org/10.1109/TVCG.2024.3456205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enabled by the latest achievements in artificial intelligence (AI), computer graphics as well as virtual, augmented, and mixed reality (VR/AR/MR), virtual agents are increasingly resembling humans in both their appearance and intelligent behavior. This results in enormous potential for agents to support users in their daily lives, for example in customer service, healthcare, education or the envisioned all-encompassing metaverse. Today&#39;s technology would allow users to customize their conversation partners in the metaverse - as opposed to reality - according to their preferences, potentially improving the user experience. On the other hand, there is little research on how reshaping the head of a communication partner might affect the immediate interaction with them. In this paper, we investigate the user requirements for and the effects of agent customization. In a two-stage user study ($N=30$), we collected both self-reported evaluations (e.g., intrinsic motivation) and interaction metrics (e.g., interaction duration and number of tried out items) for the process of agent customization itself as well as data on how users perceived the subsequent human-agent interaction in VR. Our results indicate that users only wish to have full customization for agents in their personal social circle, while for general services, a selection or even a definite assignment of pre-configured agents is sufficient. When customization is offered, attributes such as gender, clothing or hair are subjectively more relevant to users than facial features such as skin or eye color. Although the customization of human interaction partners is beyond our control, customization of virtual agents significantly increases perceived social presence as well as rapport and trust. Further findings on user motivation and agent diversity are discussed in the paper.},
  archive      = {J_TVCG},
  author       = {Susanne Schmidt and Ipek Köysürenbars and Frank Steinicke},
  doi          = {10.1109/TVCG.2024.3456205},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7162-7171},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Frankenstein&#39;s monster in the metaverse: User interaction with customized virtual agents},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SmoothRide: A versatile solution to combat cybersickness in
elevation-altering environments. <em>TVCG</em>, <em>30</em>(11),
7152–7161. (<a href="https://doi.org/10.1109/TVCG.2024.3456194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cybersickness continues to bar many individuals from taking full advantage of virtual reality (VR) technology. Previous work has established that navigating virtual terrain with elevation changes poses a significant risk in this regard. In this paper, we investigate the effectiveness of three cybersickness reduction strategies on users performing a navigation task across virtual elevation-altering terrain. These strategies include static field of view (FOV) reduction, a flat surface approach that disables terrain collision and maintains constant elevation for users, and SmoothRide, a novel technique designed to dampen a user&#39;s perception of vertical motion as they travel. To assess the impact of these strategies, we conducted a within-subjects study involving 61 participants. Each strategy was compared against a control condition, where users navigated across terrain without any cybersickness reduction measures in place. Cybersickness data were collected using the Fast Motion Sickness Scale (FMS) and Simulator Sickness Questionnaire (SSQ), along with galvanic skin response (GSR) data. We measured user presence using the IGroup Presence questionnaire (IPQ) and a Single-Item Presence Scale (SIP). Our findings reveal that users experienced significantly lower levels of cybersickness using SmoothRide or FOV reduction. Presence scores reported on the IPQ were statistically similar between SmoothRide and the control condition. Conversely, terrain flattening had adverse effects on user presence scores, and we could not identify a significant effect on cybersickness compared to the control. We demonstrate that SmoothRide is an effective, lightweight, configurable, and easy-to-integrate tool for reducing cybersickness in simulations featuring elevation-altering terrain.},
  archive      = {J_TVCG},
  author       = {Samuel Ang and John Quarles},
  doi          = {10.1109/TVCG.2024.3456194},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7152-7161},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SmoothRide: A versatile solution to combat cybersickness in elevation-altering environments},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The effects of depth of knowledge of a virtual agent.
<em>TVCG</em>, <em>30</em>(11), 7140–7151. (<a
href="https://doi.org/10.1109/TVCG.2024.3456148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explored the impact of depth of knowledge on conversational agents and human perceptions in a virtual reality (VR) environment. We designed experimental conditions with low, medium, and high depths of knowledge in the domain of game development and tested them among 27 game development students. We aimed to understand how the agent&#39;s predefined knowledge levels affected the participants&#39; perceptions of the agent and its knowledge. Our findings showed that participants could distinguish between different knowledge levels of the virtual agent. Moreover, the agent&#39;s depth of knowledge significantly impacted participants&#39; perceptions of intelligence, rapport, factuality, the uncanny valley effect, anthropomorphism, and willingness for future interaction. We also found strong correlations between perceived knowledge, perceived intelligence, factuality, and willingness for future interactions. We developed design guidelines for creating conversational agents from our data and observations. This study contributes to the human-agent interaction field in VR settings by providing empirical evidence on the importance of tailoring virtual agents&#39; depth of knowledge to improve user experience, offering insights into designing more engaging and effective conversational agents.},
  archive      = {J_TVCG},
  author       = {Fu-Chia Yang and Kevin Duque and Christos Mousas},
  doi          = {10.1109/TVCG.2024.3456148},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7140-7151},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The effects of depth of knowledge of a virtual agent},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NIS-SLAM: Neural implicit semantic RGB-d SLAM for 3D
consistent scene understanding. <em>TVCG</em>, <em>30</em>(11),
7129–7139. (<a href="https://doi.org/10.1109/TVCG.2024.3456201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the paradigm of neural implicit representations has gained substantial attention in the field of Simultaneous Localization and Mapping (SLAM). However, a notable gap exists in the existing approaches when it comes to scene understanding. In this paper, we introduce NIS-SLAM, an efficient neural implicit semantic RGB-D SLAM system, that leverages a pre-trained 2D segmentation network to learn consistent semantic representations. Specifically, for high-fidelity surface reconstruction and spatial consistent scene understanding, we combine high-frequency multi-resolution tetrahedron-based features and low-frequency positional encoding as the implicit scene representations. Besides, to address the inconsistency of 2D segmentation results from multiple views, we propose a fusion strategy that integrates the semantic probabilities from previous non-keyframes into keyframes to achieve consistent semantic learning. Furthermore, we implement a confidence-based pixel sampling and progressive optimization weight function for robust camera tracking. Extensive experimental results on various datasets show the better or more competitive performance of our system when compared to other existing neural dense implicit RGB-D SLAM approaches. Finally, we also show that our approach can be used in augmented reality applications. Project page: https://zju3dv.github.io/nis_slam.},
  archive      = {J_TVCG},
  author       = {Hongjia Zhai and Gan Huang and Qirui Hu and Guanglin Li and Hujun Bao and Guofeng Zhang},
  doi          = {10.1109/TVCG.2024.3456201},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7129-7139},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {NIS-SLAM: Neural implicit semantic RGB-D SLAM for 3D consistent scene understanding},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gesture2Text: A generalizable decoder for word-gesture
keyboards in XR through trajectory coarse discretization and
pre-training. <em>TVCG</em>, <em>30</em>(11), 7118–7128. (<a
href="https://doi.org/10.1109/TVCG.2024.3456198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text entry with word-gesture keyboards (WGK) is emerging as a popular method and becoming a key interaction for Extended Reality (XR). However, the diversity of interaction modes, keyboard sizes, and visual feedback in these environments introduces divergent word-gesture trajectory data patterns, thus leading to complexity in decoding trajectories into text. Template-matching decoding methods, such as SHARK2 [32], are commonly used for these WGK systems because they are easy to implement and configure. However, these methods are susceptible to decoding inaccuracies for noisy trajectories. While conventional neural-network-based decoders (neural decoders) trained on word-gesture trajectory data have been proposed to improve accuracy, they have their own limitations: they require extensive data for training and deep-learning expertise for implementation. To address these challenges, we propose a novel solution that combines ease of implementation with high decoding accuracy: a generalizable neural decoder enabled by pre-training on large-scale coarsely discretized word-gesture trajectories. This approach produces a ready-to-use WGK decoder that is generalizable across mid-air and on-surface WGK systems in augmented reality (AR) and virtual reality (VR), which is evident by a robust average Top-4 accuracy of 90.4% on four diverse datasets. It significantly outperforms SHARK2 with a 37.2% enhancement and surpasses the conventional neural decoder by 7.4%. Moreover, the Pre-trained Neural Decoder&#39;s size is only 4 MB after quantization, without sacrificing accuracy, and it can operate in real-time, executing in just 97 milliseconds on Quest 3.},
  archive      = {J_TVCG},
  author       = {Junxiao Shen and Khadija Khaldi and Enmin Zhou and Hemant Bhaskar Surale and Amy Karlson},
  doi          = {10.1109/TVCG.2024.3456198},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7118-7128},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Gesture2Text: A generalizable decoder for word-gesture keyboards in XR through trajectory coarse discretization and pre-training},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring and modeling directional effects on steering
behavior in virtual reality. <em>TVCG</em>, <em>30</em>(11), 7107–7117.
(<a href="https://doi.org/10.1109/TVCG.2024.3456166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Steering is a fundamental task in interactive Virtual Reality (VR) systems. Prior work has demonstrated that movement direction can significantly influence user behavior in the steering task, and different interactive environments (VEs) can lead to various behavioral patterns, such as tablets and PCs. However, its impact on VR environments remains unexplored. Given the widespread use of steering tasks in VEs, including menu adjustment and object manipulation, this work seeks to understand and model the directional effect with a focus on barehand interaction, which is typical in VEs. This paper presents the results of two studies. The first study was conducted to collect behavioral data with four categories: movement time, average movement speed, success rate, and reenter times. According to the results, we examined the effect of movement direction and built the SθModel. We then empirically evaluated the model through the data collected from the first study. The results proved that our proposed model achieved the best performance across all the metrics (r2 &gt; 0.95), with more than 15% improvement over the original Steering Law in terms of prediction accuracy. Next, we further validated the SθModel by another study with the change of device and steering direction. Consistent with previous assessments, the model continues to exhibit optimal performance in both predicting movement time and speed. Finally, based on the results, we formulated design recommendations for steering tasks in VEs to enhance user experience and interaction efficiency.},
  archive      = {J_TVCG},
  author       = {Yushi Wei and Kemu Xu and Yue Li and Lingyun Yu and Hai-Ning Liang},
  doi          = {10.1109/TVCG.2024.3456166},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7107-7117},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring and modeling directional effects on steering behavior in virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scene-aware foveated rendering. <em>TVCG</em>,
<em>30</em>(11), 7097–7106. (<a
href="https://doi.org/10.1109/TVCG.2024.3456157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new scene-aware foveated rendering method, which incorporates the scene awareness and characteristics of the human visual system into the mapping-based foveated rendering framework. First, we generate the conservative visual importance map that encodes the visual features of the scene, visual acuity, and gaze motion. Second, we construct the pixel size control map using a convolution kernel method. Third, we utilize the pixel size control map to guide the foveated rendering. At last, a temporal coherent refinement strategy is used to maintain the smooth foveated rendering for the adjacent frames. Compared to the state-of-the-art mapping-based foveated rendering methods using the same compression ratio, our method achieves smaller MSE, higher PSNR, and SSIM in the fovea, periphery, salient regions, and the whole image. We also conducted user studies, and the results proved that the perceptual quality of our method has a high visual similarity with the around truth rendered with the full resolution.},
  archive      = {J_TVCG},
  author       = {Runze Fan and Xuehuai Shi and Kangyu Wang and Qixiang Ma and Lili Wang},
  doi          = {10.1109/TVCG.2024.3456157},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7097-7106},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Scene-aware foveated rendering},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). “As if it were my own hand”: Inducing the rubber hand
illusion through virtual reality for motor imagery enhancement.
<em>TVCG</em>, <em>30</em>(11), 7086–7096. (<a
href="https://doi.org/10.1109/TVCG.2024.3456147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain-computer interfaces (BCI) are widely used in the field of disability assistance and rehabilitation, and virtual reality (VR) is increasingly used for visual guidance of BCI-MI (motor imagery). Therefore, how to improve the quality of electroencephalogram (EEG) signals for MI in VR has emerged as a critical issue. People can perform MI more easily when they visualize the hand used for visual guidance as their own, and the Rubber Hand Illusion (RHI) can increase people&#39;s ownership of the prosthetic hand. We proposed to induce RHI in VR to enhance participants&#39; MI ability and designed five methods of inducing RHI, namely active movement, haptic stimulation, passive movement, active movement mixed with haptic stimulation, and passive movement mixed with haptic stimulation, respectively. We constructed a first-person training scenario to train participants&#39; MI ability through the five induction methods. The experimental results showed that through the training, the participants&#39; feeling of ownership of the virtual hand in VR was enhanced, and the MI ability was improved. Among them, the method of mixing active movement and tactile stimulation proved to have a good effect on enhancing MI. Finally, we developed a BCI system in VR utilizing the above training method, and the performance of the participants improved after the training. This also suggests that our proposed method is promising for future application in BCI rehabilitation systems.},
  archive      = {J_TVCG},
  author       = {Shiwei Cheng and Yang Liu and Yuefan Gao and Zhanxun Dong},
  doi          = {10.1109/TVCG.2024.3456147},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7086-7096},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {“As if it were my own hand”: Inducing the rubber hand illusion through virtual reality for motor imagery enhancement},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analysis and design of efficient authentication techniques
for password entry with the qwerty keyboard for VR environments.
<em>TVCG</em>, <em>30</em>(11), 7075–7085. (<a
href="https://doi.org/10.1109/TVCG.2024.3456195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Authentication in digital security relies heavily on text-based passwords, even with other available methods like biometrics and graphical passwords. While virtual reality (VR) keyboards are typically invisible to onlookers, the presence of inconspicuous sensors, including accelerometers, gyroscopes, and barometers, poses a potential risk of unauthorized observation and recording. Traditional defense shoulder-surfing attack methods typically involve breaking apart the Qwerty layout, which destroys the user&#39;s inherent familiarity with the layout. This research addresses the need for secure password entry in VR environments while retaining the Qwerty layout. We explore three keyboard-related position alteration strategies to ensure security while mitigating the decline in user experience. These strategies involve moving the entire keyboard, cursor, and keys. Our theoretical study assesses the effectiveness of these strategies against shoulder-surfing attacks. Two user studies, employing ray-based and position-based text entry methods, respectively, evaluate the practical effectiveness of the three strategies in resisting shoulder-surfing attacks, as well as their impact on typing performance and user experience. Our findings demonstrate that the three strategies achieve shoulder-surfing attack resistance comparable to a random layout keyboard. Moreover, compared to a random layout, the two strategies involving the movement of the entire keyboard and the repositioning of keys support faster entry rates and enhanced user experience.},
  archive      = {J_TVCG},
  author       = {Tingjie Wan and Liangyuting Zhang and Yunxin Xu and Zixuan Guo and Boyu Gao and Hai-Ning Liang},
  doi          = {10.1109/TVCG.2024.3456195},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7075-7085},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Analysis and design of efficient authentication techniques for password entry with the qwerty keyboard for VR environments},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Investigating object translation in room-scale, handheld
virtual reality. <em>TVCG</em>, <em>30</em>(11), 7064–7074. (<a
href="https://doi.org/10.1109/TVCG.2024.3456154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handheld devices have become an inclusive alternative to head-mounted displays in virtual reality (VR) environments, enhancing accessibility and allowing cross-device collaboration. Object manipulation techniques in 3D space with handheld devices, such as those in handheld augmented reality (AR), have been typically evaluated in table-top scale and we currently lack an understanding of how these techniques perform in larger scale environments. We conducted two studies, each with 30 participants, to investigate how different techniques impact usability and performance for room-scale handheld VR object translations. We compared three translation techniques that are similar to commonly studied techniques in handheld AR: 3DSlide, VirtualGrasp, and Joystick. We also examined the effects of target size, target distance, and user mobility conditions (stationary vs. moving). Results indicated that the Joystick technique, which allowed translation in relation to the user&#39;s perspective, was the fastest and most preferred, without difference in precision. Our findings provide insights for designing room-scale handheld VR systems, with potential implications for mixed reality systems involving handheld devices.},
  archive      = {J_TVCG},
  author       = {Daniel Enriquez and Hayoun Moon and Doug A. Bowman and Myounghoon Jeon and Sang Won Lee},
  doi          = {10.1109/TVCG.2024.3456154},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7064-7074},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Investigating object translation in room-scale, handheld virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Co-designing dynamic mixed reality drill positioning
widgets: A collaborative approach with dentists in a realistic setup.
<em>TVCG</em>, <em>30</em>(11), 7053–7063. (<a
href="https://doi.org/10.1109/TVCG.2024.3456149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed Reality (MR) is proven in the literature to support precise spatial dental drill positioning by superimposing 3D widgets. Despite this, the related knowledge about widget&#39;s visual design and interactive user feedback is still limited. Therefore, this study is contributed to by co-designed MR drill tool positioning widgets with two expert dentists and three MR experts. The results of co-design are two static widgets (SWs): a simple entry point, a target axis, and two dynamic widgets (DWs), variants of dynamic error visualization with and without a target axis (DWTA and DWEP). We evaluated the co-designed widgets in a virtual reality simulation supported by a realistic setup with a tracked phantom patient, a virtual magnifying loupe, and a dentist&#39;s foot pedal. The user study involved 35 dentists with various backgrounds and years of experience. The findings demonstrated significant results; DWs outperform SWs in positional and rotational precision, especially with younger generations and subjects with gaming experiences. The user preference remains for DWs (19) instead of SWs (16). However, findings indicated that the precision positively correlates with the time trade-off. The post-experience questionnaire (NASA-TLX) showed that DWs increase mental and physical demand, effort, and frustration more than SWs. Comparisons between DWEP and DWTA show that the DW&#39;s complexity level influences time, physical and mental demands. The DWs are extensible to diverse medical and industrial scenarios that demand precision.},
  archive      = {J_TVCG},
  author       = {Mine Dastan and Michele Fiorentino and Elias D. Walter and Christian Diegritz and Antonio E. Uva and Ulrich Eck and Nassir Navab},
  doi          = {10.1109/TVCG.2024.3456149},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7053-7063},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Co-designing dynamic mixed reality drill positioning widgets: A collaborative approach with dentists in a realistic setup},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Should i evaluate my augmented reality system in an
industrial environment? Investigating the effects of classroom and shop
floor settings on guided assembly. <em>TVCG</em>, <em>30</em>(11),
7042–7052. (<a href="https://doi.org/10.1109/TVCG.2024.3456208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous prior studies have investigated real-time assembly instructions using Augmented Reality (AR). However, most such experiments were conducted in laboratory settings with simplistic assembly tasks, failing to represent real-world industrial conditions. To ascertain to what extent results obtained in a laboratory environment may differ from studies in actual industrial environments, we carried out a user study with 32 manufacturing apprentices. We compared assembly task execution results in two settings, a classroom and an industrial workshop environment. To facilitate the experiments, we developed AR-guided manual assembly systems for simple and more complex assets. Our findings reveal a significantly improved task performance in the industrial workshop, reflected in faster task completion times, fewer errors, and subjectively perceived higher flow. This contradicted participants&#39; subjective ratings, as they expected to perform better in the classroom environment. Our results suggest that the actual manufacturing environment is critical in evaluating AR systems for real-world industrial applications.},
  archive      = {J_TVCG},
  author       = {Vicky Zhang and Alexander Albers and Christine Saeedi-Givi and Per Ola Kristensson and Thomas Bohné and Sławomir Tadeja},
  doi          = {10.1109/TVCG.2024.3456208},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7042-7052},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Should i evaluate my augmented reality system in an industrial environment? investigating the effects of classroom and shop floor settings on guided assembly},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-aperture coaxial projector balancing shadow
suppression and deblurring. <em>TVCG</em>, <em>30</em>(11), 7031–7041.
(<a href="https://doi.org/10.1109/TVCG.2024.3456170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a projection system that optically removes the cast shadow in projection mapping. Specifically, we realize the large-aperture (LA) projection using a large-format Fresnel lens to suppress cast shadows by condensing the projection light from a wide viewing angle. However, the resolution and contrast of the projected results are significantly degraded by defocus blur, veiling glare, and stray light caused by the aberration of an LA Fresnel lens. To solve the technical problems, we employ two different approaches: optical and digital image processing methods. First, we introduce a residual projector with a typical aperture lens on the same optical axis as the LA projector, projecting the residual (i.e., high-frequency) components attenuated in the LA projection. These projectors play different roles in shadow suppression and blur compensation, both achieved by projecting simultaneously. Secondly, we optimize the pair of projection images that can balance the shadow suppression and deblurring performance of our projection system. We implemented a proof-of-concept prototype and validated the above-mentioned techniques through projection experiments and a user study.},
  archive      = {J_TVCG},
  author       = {Hiroki Kusuyama and Yuta Kageyama and Daisuke Iwai and Kosuke Sato},
  doi          = {10.1109/TVCG.2024.3456170},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7031-7041},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A multi-aperture coaxial projector balancing shadow suppression and deblurring},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Precise tool to target positioning widgets (TOTTA) in
spatial environments: A systematic review. <em>TVCG</em>,
<em>30</em>(11), 7020–7030. (<a
href="https://doi.org/10.1109/TVCG.2024.3456206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {TOTTA outlines the spatial position and rotation guidance of a real/virtual tool (TO) towards a real/virtual target (TA), which is a key task in Mixed reality applications. The task error can have critical consequences regarding safety, performance, and quality, such as surgical implantology or industrial maintenance scenarios. The TOTTA problem lacks a dedicated study and it is scattered in different domains with isolated designs. This work contributes to a systematic review of the TOTTA visual widgets, studying 70 unique designs from 24 papers. TOTTA is commonly guided by the visual overlap -an intuitive, pre-attentive “collimation” feedback- of simple shaped widgets: Box, 3D Axes, 3D Model, 2D Crosshair, Globe, Tetrahedron, Line, Plane. Our research discovers that TO and TA are often represented with the same shape. They are distinguished by topological elements (e.g. edges/vertices/faces), colors, transparency levels, and added. shapes, widget quantity, and size. Meanwhile some designs provide continuous “during manipulation feedback” relative to the distance between TO and TA by text, dynamic color, sonification, and amplified graphical visualization. Some approaches trigger discrete “TA reached feedback” such as color alteration, added sound, TA shape change, and added text. We found the lack of golden standards, including in testing procedures, as current ones are limited to partial sets with different and incomparable setups (different target configurations, avatar, background, etc.). We also found a bias in participants: right-handed, young male, non-color impaired.},
  archive      = {J_TVCG},
  author       = {Mine Dastan and Michele Fiorentino and Antonio E. Uva},
  doi          = {10.1109/TVCG.2024.3456206},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7020-7030},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Precise tool to target positioning widgets (TOTTA) in spatial environments: A systematic review},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Virtual crowds rheology: Evaluating the effect of character
representation on user locomotion in crowds. <em>TVCG</em>,
<em>30</em>(11), 7008–7019. (<a
href="https://doi.org/10.1109/TVCG.2024.3456183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd data is a crucial element in the modeling of collective behaviors, and opens the way to simulation for their study or prediction. Given the difficulty of acquiring such data, virtual reality is useful for simplifying experimental processes and opening up new experimental opportunities. This comes at the cost of the need to assess the biases introduced by the use of this technology. Our paper is part of this effort, and investigates the effect of the graphical representation of a crowd on the behavior of a user immersed within. More specifically, we inspect the virtual navigation through virtual crowds, in terms of travel speeds and local navigation choices as a function of the visual representation of the virtual agents that make up the crowd (simple geometric model, anthropomorphic model or realistic model). Through an experiment in which we ask a user to navigate virtual crowds of varying densities, we show that the effect of the visual representation is limited, but that an anthropomorphic representation offers the best trade-off between computational complexity and ecological validity, even though a more realistic representation can be preferred when user behaviour is studied in more details. Our work leads to clear recommendations on the design of immersive simulations for the study of crowd behavior.},
  archive      = {J_TVCG},
  author       = {Jordan Martin and Ludovic Hoyet and Etienne Pinsard and Jean-Luc Paillat and Julien Pettré},
  doi          = {10.1109/TVCG.2024.3456183},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {7008-7019},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Virtual crowds rheology: Evaluating the effect of character representation on user locomotion in crowds},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Searching across realities: Investigating ERPs and
eye-tracking correlates of visual search in mixed reality.
<em>TVCG</em>, <em>30</em>(11), 6997–7007. (<a
href="https://doi.org/10.1109/TVCG.2024.3456172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed Reality allows us to integrate virtual and physical content into users&#39; environments seamlessly. Yet, how this fusion affects perceptual and cognitive resources and our ability to find virtual or physical objects remains uncertain. Displaying virtual and physical information simultaneously might lead to divided attention and increased visual complexity, impacting users&#39; visual processing, performance, and workload. In a visual search task, we asked participants to locate virtual and physical objects in Augmented Reality and Augmented Virtuality to understand the effects on performance. We evaluated search efficiency and attention allocation for virtual and physical objects using event-related potentials, fixation and saccade metrics, and behavioral measures. We found that users were more efficient in identifying objects in Augmented Virtuality, while virtual objects gained saliency in Augmented Virtuality. This suggests that visual fidelity might increase the perceptual load of the scene. Reduced amplitude in distractor positivity ERP, and fixation patterns supported improved distractor suppression and search efficiency in Augmented Virtuality. We discuss design implications for mixed reality adaptive systems based on physiological inputs for interaction.},
  archive      = {J_TVCG},
  author       = {Francesco Chiossi and Ines Trautmannsheimer and Changkun Ou and Uwe Gruenefeld and Sven Mayer},
  doi          = {10.1109/TVCG.2024.3456172},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {11},
  number       = {11},
  pages        = {6997-7007},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Searching across realities: Investigating ERPs and eye-tracking correlates of visual search in mixed reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Speech-driven personalized gesture synthetics: Harnessing
automatic fuzzy feature inference. <em>TVCG</em>, <em>30</em>(10),
6984–6996. (<a href="https://doi.org/10.1109/TVCG.2024.3393236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech-driven gesture generation is an emerging field within virtual human creation. However, a significant challenge lies in accurately determining and processing the multitude of input features (such as acoustic, semantic, emotional, personality, and even subtle unknown features). Traditional approaches, reliant on various explicit feature inputs and complex multimodal processing, constrain the expressiveness of resulting gestures and limit their applicability. To address these challenges, we present Persona-Gestor , a novel end-to-end generative model designed to generate highly personalized 3D full-body gestures solely relying on raw speech audio. The model combines a fuzzy feature extractor and a non-autoregressive Adaptive Layer Normalization (AdaLN) transformer diffusion architecture (DiTs-based). The fuzzy feature extractor harnesses a fuzzy inference strategy that automatically infers implicit, continuous fuzzy features. These fuzzy features, represented as a unified latent feature, are fed into the AdaLN transformer. The AdaLN transformer introduces a conditional mechanism that applies a uniform function across all tokens, thereby effectively modeling the correlation between the fuzzy features and the gesture sequence. This module ensures a high level of gesture-speech synchronization while preserving naturalness. Finally, we employ the diffusion model to train and infer various gestures. Extensive subjective and objective evaluations on the Trinity, ZEGGS, and BEAT datasets confirm our model&#39;s superior performance to the current state-of-the-art approaches. Persona-Gestor improves the system&#39;s usability and generalization capabilities, setting a new benchmark in speech-driven gesture synthesis and broadening the horizon for virtual human technology.},
  archive      = {J_TVCG},
  author       = {Fan Zhang and Zhaohan Wang and Xin Lyu and Siyuan Zhao and Mengjian Li and Weidong Geng and Naye Ji and Hui Du and Fuxing Gao and Hao Wu and Shunman Li},
  doi          = {10.1109/TVCG.2024.3393236},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6984-6996},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Speech-driven personalized gesture synthetics: Harnessing automatic fuzzy feature inference},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stress assessment for augmented reality applications based
on head movement features. <em>TVCG</em>, <em>30</em>(10), 6970–6983.
(<a href="https://doi.org/10.1109/TVCG.2024.3385637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented reality is one of the enabling technologies of the upcoming future. Its usage in working and learning scenarios may lead to a better quality of work and training by helping the operators during the most crucial stages of processes. Therefore, the automatic detection of stress during augmented reality experiences can be a valuable support to prevent consequences on people&#39;s health and foster the spreading of this technology. In this work, we present the design of a non-invasive stress assessment approach. The proposed system is based on the analysis of the head movements of people wearing a Head Mounted Display while performing stress-inducing tasks. First, we designed a subjective experiment consisting of two stress-related tests for data acquisition. Then, a statistical analysis of head movements has been performed to determine which features are representative of the presence of stress. Finally, a stress classifier based on a combination of Support Vector Machines has been designed and trained. The proposed approach achieved promising performances thus paving the way for further studies in this research direction.},
  archive      = {J_TVCG},
  author       = {Anna Ferrarotti and Sara Baldoni and Marco Carli and Federica Battisti},
  doi          = {10.1109/TVCG.2024.3385637},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6970-6983},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Stress assessment for augmented reality applications based on head movement features},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AnimeDiffusion: Anime diffusion colorization. <em>TVCG</em>,
<em>30</em>(10), 6956–6969. (<a
href="https://doi.org/10.1109/TVCG.2024.3357568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Being essential in animation creation, colorizing anime line drawings is usually a tedious and time-consuming manual task. Reference-based line drawing colorization provides an intuitive way to automatically colorize target line drawings using reference images. The prevailing approaches are based on generative adversarial networks (GANs), yet these methods still cannot generate high-quality results comparable to manually-colored ones. In this article, a new AnimeDiffusion approach is proposed via hybrid diffusions for the automatic colorization of anime face line drawings. This is the first attempt to utilize the diffusion model for reference-based colorization, which demands a high level of control over the image synthesis process. To do so, a hybrid end-to-end training strategy is designed, including phase 1 for training diffusion model with classifier-free guidance and phase 2 for efficiently updating color tone with a target reference colored image. The model learns denoising and structure-capturing ability in phase 1, and in phase 2, the model learns more accurate color information. Utilizing our hybrid training strategy, the network convergence speed is accelerated, and the colorization performance is improved. Our AnimeDiffusion generates colorization results with semantic correspondence and color consistency. In addition, the model has a certain generalization performance for line drawings of different line styles. To train and evaluate colorization methods, an anime face line drawing colorization benchmark dataset, containing 31,696 training data and 579 testing data, is introduced and shared. Extensive experiments and user studies have demonstrated that our proposed AnimeDiffusion outperforms state-of-the-art GAN-based methods and another diffusion-based model, both quantitatively and qualitatively.},
  archive      = {J_TVCG},
  author       = {Yu Cao and Xiangqiao Meng and P. Y. Mok and Tong-Yee Lee and Xueting Liu and Ping Li},
  doi          = {10.1109/TVCG.2024.3357568},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6956-6969},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {AnimeDiffusion: Anime diffusion colorization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RD-VIO: Robust visual-inertial odometry for mobile augmented
reality in dynamic environments. <em>TVCG</em>, <em>30</em>(10),
6941–6955. (<a href="https://doi.org/10.1109/TVCG.2024.3353263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is typically challenging for visual or visual-inertial odometry systems to handle the problems of dynamic scenes and pure rotation. In this work, we design a novel visual-inertial odometry (VIO) system called RD-VIO to handle both of these two problems. First, we propose an IMU-PARSAC algorithm which can robustly detect and match keypoints in a two-stage process. In the first state, landmarks are matched with new keypoints using visual and IMU measurements. We collect statistical information from the matching and then guide the intra-keypoint matching in the second stage. Second, to handle the problem of pure rotation, we detect the motion type and adapt the deferred-triangulation technique during the data-association process. We make the pure-rotational frames into the special subframes. When solving the visual-inertial bundle adjustment, they provide additional constraints to the pure-rotational motion. We evaluate the proposed VIO system on public datasets and online comparison. Experiments show the proposed RD-VIO has obvious advantages over other methods in dynamic environments.},
  archive      = {J_TVCG},
  author       = {Jinyu Li and Xiaokun Pan and Gan Huang and Ziyang Zhang and Nan Wang and Hujun Bao and Guofeng Zhang},
  doi          = {10.1109/TVCG.2024.3353263},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6941-6955},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {RD-VIO: Robust visual-inertial odometry for mobile augmented reality in dynamic environments},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decoupling judgment and decision making: A tale of two
tails. <em>TVCG</em>, <em>30</em>(10), 6928–6940. (<a
href="https://doi.org/10.1109/TVCG.2023.3346640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Is it true that if citizens understand hurricane probabilities, they will make more rational decisions for evacuation? Finding answers to such questions is not straightforward in the literature because the terms “ judgment ” and “ decision making ” are often used interchangeably. This terminology conflation leads to a lack of clarity on whether people make suboptimal decisions because of inaccurate judgments of information conveyed in visualizations or because they use alternative yet currently unknown heuristics. To decouple judgment from decision making, we review relevant concepts from the literature and present two preregistered experiments (N = 601) to investigate if the task (judgment versus decision making), the scenario (sports versus humanitarian), and the visualization (quantile dotplots, density plots, probability bars) affect accuracy. While experiment 1 was inconclusive, we found evidence for a difference in experiment 2. Contrary to our expectations and previous research, which found decisions less accurate than their direct-equivalent judgments, our results pointed in the opposite direction. Our findings further revealed that decisions were less vulnerable to status-quo bias, suggesting decision makers may disfavor responses associated with inaction. We also found that both scenario and visualization types can influence people&#39;s judgments and decisions. Although effect sizes are not large and results should be interpreted carefully, we conclude that judgments cannot be safely used as proxy tasks for decision making, and discuss implications for visualization research and beyond. Materials and preregistrations are available at https://osf.io/ufzp5/?view_only=adc0f78a23804c31bf7fdd9385cb264f .},
  archive      = {J_TVCG},
  author       = {Başak Oral and Pierre Dragicevic and Alexandru Telea and Evanthia Dimara},
  doi          = {10.1109/TVCG.2023.3346640},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6928-6940},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Decoupling judgment and decision making: A tale of two tails},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SwinGar: Spectrum-inspired neural dynamic deformation for
free-swinging garments. <em>TVCG</em>, <em>30</em>(10), 6913–6927. (<a
href="https://doi.org/10.1109/TVCG.2023.3346055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our work presents a novel spectrum-inspired learning-based approach for generating clothing deformations with dynamic effects and personalized details. Existing methods in the field of clothing animation are limited to either static behavior or specific network models for individual garments, which hinders their applicability in real-world scenarios where diverse animated garments are required. Our proposed method overcomes these limitations by providing a unified framework that predicts dynamic behavior for different garments with arbitrary topology and looseness, resulting in versatile and realistic deformations. First, we observe that the problem of bias towards low frequency always hampers supervised learning and leads to overly smooth deformations. To address this issue, we introduce a frequency-control strategy from a spectral perspective that enhances the generation of high-frequency details of the deformation. In addition, to make the network highly generalizable and able to learn various clothing deformations effectively, we propose a spectral descriptor to achieve a generalized description of the global shape information. Building on the above strategies, we develop a dynamic clothing deformation estimator that integrates graph attention mechanisms with long short-term memory. The estimator takes as input expressive features from garments and human bodies, allowing it to automatically output continuous deformations for diverse clothing types, independent of mesh topology or vertex count. Finally, we present a neural collision handling method to further enhance the realism of garments. Our experimental results demonstrate the effectiveness of our approach on a variety of free-swinging garments and its superiority over state-of-the-art methods.},
  archive      = {J_TVCG},
  author       = {Tianxing Li and Rui Shi and Qing Zhu and Takashi Kanai},
  doi          = {10.1109/TVCG.2023.3346055},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6913-6927},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SwinGar: Spectrum-inspired neural dynamic deformation for free-swinging garments},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PointVST: Self-supervised pre-training for 3D point clouds
via view-specific point-to-image translation. <em>TVCG</em>,
<em>30</em>(10), 6900–6912. (<a
href="https://doi.org/10.1109/TVCG.2023.3345353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The past few years have witnessed the great success and prevalence of self-supervised representation learning within the language and 2D vision communities. However, such advancements have not been fully migrated to the field of 3D point cloud learning. Different from existing pre-training paradigms designed for deep point cloud feature extractors that fall into the scope of generative modeling or contrastive learning, this paper proposes a translative pre-training framework, namely PointVST, driven by a novel self-supervised pretext task of cross-modal translation from 3D point clouds to their corresponding diverse forms of 2D rendered images. More specifically, we begin with deducing view-conditioned point-wise embeddings through the insertion of the viewpoint indicator, and then adaptively aggregate a view-specific global codeword, which can be further fed into subsequent 2D convolutional translation heads for image generation. Extensive experimental evaluations on various downstream task scenarios demonstrate that our PointVST shows consistent and prominent performance superiority over current state-of-the-art approaches as well as satisfactory domain transfer capability.},
  archive      = {J_TVCG},
  author       = {Qijian Zhang and Junhui Hou},
  doi          = {10.1109/TVCG.2023.3345353},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6900-6912},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PointVST: Self-supervised pre-training for 3D point clouds via view-specific point-to-image translation},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Point cloud completion: A survey. <em>TVCG</em>,
<em>30</em>(10), 6880–6899. (<a
href="https://doi.org/10.1109/TVCG.2023.3344935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud completion is the task of producing a complete 3D shape given an input of a partial point cloud. It has become a vital process in 3D computer graphics, vision and applications such as autonomous driving, robotics, and augmented reality. These applications often rely on the presence of a complete 3D representation of the environment. Over the past few years, many completion algorithms have been proposed and a substantial amount of research has been carried out. However, there are not many in-depth surveys that summarise the research progress in such a way that allows users to make an informed choice of what algorithms to employ given the type of data they have, the end result they want, the challenges they may face and the possible strategies they could use. In this study, we present a comprehensive survey and classification of articles on point cloud completion untill August 2023 based on the strategies, techniques, inputs, outputs, and network architectures. We will also cover datasets, evaluation methods, and application areas in point cloud completion. Finally, we discuss challenges faced by the research community and future research directions.},
  archive      = {J_TVCG},
  author       = {Keneni W. Tesema and Lyndon Hill and Mark W. Jones and Muneeb I. Ahmad and Gary K.L. Tam},
  doi          = {10.1109/TVCG.2023.3344935},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6880-6899},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Point cloud completion: A survey},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Light field depth estimation via stitched epipolar plane
images. <em>TVCG</em>, <em>30</em>(10), 6866–6879. (<a
href="https://doi.org/10.1109/TVCG.2023.3344132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth estimation is a fundamental problem in light field processing. Epipolar-plane image (EPI)-based methods often encounter challenges such as low accuracy in slope computation due to discretization errors and limited angular resolution. Besides, existing methods perform well in most regions but struggle to produce sharp edges in occluded regions and resolve ambiguities in texture-less regions. To address these issues, we propose the concept of stitched-EPI (SEPI) to enhance slope computation. SEPI achieves this by shifting and concatenating lines from different EPIs that correspond to the same 3D point. Moreover, we introduce the half-SEPI algorithm, which focuses exclusively on the non-occluded portion of lines to handle occlusion. Additionally, we present a depth propagation strategy aimed at improving depth estimation in texture-less regions. This strategy involves determining the depth of such regions by progressing from the edges towards the interior, prioritizing accurate regions over coarse regions. Through extensive experimental evaluations and ablation studies, we validate the effectiveness of our proposed method. The results demonstrate its superior ability to generate more accurate and robust depth maps across all regions compared to state-of-the-art methods.},
  archive      = {J_TVCG},
  author       = {Ping Zhou and Langqing Shi and Xiaoyang Liu and Jing Jin and Yuting Zhang and Junhui Hou},
  doi          = {10.1109/TVCG.2023.3344132},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6866-6879},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Light field depth estimation via stitched epipolar plane images},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sketch2Stress: Sketching with structural stress awareness.
<em>TVCG</em>, <em>30</em>(10), 6851–6865. (<a
href="https://doi.org/10.1109/TVCG.2023.3342119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the process of product design and digital fabrication, the structural analysis of a designed prototype is a fundamental and essential step. However, such a step is usually invisible or inaccessible to designers at the early sketching phase. This limits the user&#39;s ability to consider a shape&#39;s physical properties and structural soundness. To bridge this gap, we introduce a novel approach Sketch2Stress that allows users to perform structural analysis of desired objects at the sketching stage. This method takes as input a 2D freehand sketch and one or multiple locations of user-assigned external forces. With the specially-designed two-branch generative-adversarial framework, it automatically predicts a normal map and a corresponding structural stress map distributed over the user-sketched underlying object. In this way, our method empowers designers to easily examine the stress sustained everywhere and identify potential problematic regions of their sketched object. Furthermore, combined with the predicted normal map, users are able to conduct a region-wise structural analysis efficiently by aggregating the stress effects of multiple forces in the same direction. Finally, we demonstrate the effectiveness and practicality of our system with extensive experiments and user studies.},
  archive      = {J_TVCG},
  author       = {Deng Yu and Chufeng Xiao and Manfred Lau and Hongbo Fu},
  doi          = {10.1109/TVCG.2023.3342119},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6851-6865},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Sketch2Stress: Sketching with structural stress awareness},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A partition based method for spectrum-preserving mesh
simplification. <em>TVCG</em>, <em>30</em>(10), 6839–6850. (<a
href="https://doi.org/10.1109/TVCG.2023.3341610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of the simplification methods focus on preserving the appearance of the mesh, ignoring the spectral properties of the differential operators derived from the mesh. The spectrum of the Laplace-Beltrami operator is essential for a large subset of applications in geometry processing. Coarsening a mesh without considering its spectral properties might result in incorrect calculations on the simplified mesh. Given a 3D triangular mesh, this article aims to simplify the mesh using edge collapses, while focusing on preserving the spectral properties of the associated cotangent Laplace-Beltrami operator. Unlike the existing spectrum-preserving coarsening methods, we consider solely the eigenvalues of the operator in order to preserve the spectrum. The presented method is partition based, that is the input mesh is divided into smaller patches which are simplified individually. We evaluate our method on a variety of meshes, by using functional maps and quantitative norms, to measure how well the eigenvalues and eigenvectors of the Laplace-Beltrami operator computed on the input mesh are maintained by the output mesh. We demonstrate that the achieved spectrum preservation is at least as effective as the existing spectral coarsening methods.},
  archive      = {J_TVCG},
  author       = {Misranur Yazgan and Yusuf Sahillioğlu},
  doi          = {10.1109/TVCG.2023.3341610},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6839-6850},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A partition based method for spectrum-preserving mesh simplification},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). KD-INR: Time-varying volumetric data compression via
knowledge distillation-based implicit neural representation.
<em>TVCG</em>, <em>30</em>(10), 6826–6838. (<a
href="https://doi.org/10.1109/TVCG.2023.3345373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional deep learning algorithms assume that all data is available during training, which presents challenges when handling large-scale time-varying data. To address this issue, we propose a data reduction pipeline called knowledge distillation-based implicit neural representation (KD-INR) for compressing large-scale time-varying data. The approach consists of two stages: spatial compression and model aggregation. In the first stage, each time step is compressed using an implicit neural representation with bottleneck layers and features of interest preservation-based sampling. In the second stage, we utilize an offline knowledge distillation algorithm to extract knowledge from the trained models and aggregate it into a single model. We evaluated our approach on a variety of time-varying volumetric data sets. Both quantitative and qualitative results, such as PSNR, LPIPS, and rendered images, demonstrate that KD-INR surpasses the state-of-the-art approaches, including learning-based (i.e., CoordNet, NeurComp, and SIREN) and lossy compression (i.e., SZ3, ZFP, and TTHRESH) methods, at various compression ratios ranging from hundreds to ten thousand.},
  archive      = {J_TVCG},
  author       = {Jun Han and Hao Zheng and Chongke Bi},
  doi          = {10.1109/TVCG.2023.3345373},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6826-6838},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {KD-INR: Time-varying volumetric data compression via knowledge distillation-based implicit neural representation},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VisTA-LIVE: A visualization tool for assessment of
laboratories in virtual environments. <em>TVCG</em>, <em>30</em>(10),
6813–6825. (<a href="https://doi.org/10.1109/TVCG.2023.3341079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Virtual Reality Laboratory (VR Lab) experiment refers to an experiment session that is being conducted in the virtual environment through Virtual Reality (VR) and aims to deliver procedural knowledge to students similar to that in a physical lab environment. While VR Lab is becoming more popular among education institutes as a learning tool for students, existing designs are mostly considered from a student&#39;s perspective. Instructors could only receive limited information on how the students are performing and could not provide useful feedback to aid the students’ learning and evaluate their performance. This motivated us to create VisTA-LIVE: a Visualization Tool for Assessment of Laboratories In Virtual Environments. In this article, we present in detail the design thinking approach that was applied to create VisTA-LIVE. The tool is deployed in an Extended Reality (XR) environment, and we report the evaluation results with domain experts and discuss issues related to monitoring and assessing a live VR lab session which lay potential directions for future work. We also describe how the resulting design of the tool could be used as a reference for other education developers who wish to develop similar applications.},
  archive      = {J_TVCG},
  author       = {Pak Ming Fan and Santawat Thanyadit and Ting-Chuen Pong},
  doi          = {10.1109/TVCG.2023.3341079},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6813-6825},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VisTA-LIVE: A visualization tool for assessment of laboratories in virtual environments},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VoxAR: Adaptive visualization of volume rendered objects in
optical see-through augmented reality. <em>TVCG</em>, <em>30</em>(10),
6801–6812. (<a href="https://doi.org/10.1109/TVCG.2023.3340770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present VoxAR, a method to facilitate an effective visualization of volume-rendered objects in optical see-through head-mounted displays (OST-HMDs). The potential of augmented reality (AR) to integrate digital information into the physical world provides new opportunities for visualizing and interpreting scientific data. However, a limitation of OST-HMD technology is that rendered pixels of a virtual object can interfere with the colors of the real-world, making it challenging to perceive the augmented virtual information accurately. We address this challenge in a two-step approach. First, VoxAR determines an appropriate placement of the volume-rendered object in the real-world scene by evaluating a set of spatial and environmental objectives, managed as user-selected preferences and pre-defined constraints. We achieve a real-time solution by implementing the objectives using a GPU shader language. Next, VoxAR adjusts the colors of the input transfer function (TF) based on the real-world placement region. Specifically, we introduce a novel optimization method that adjusts the TF colors such that the resulting volume-rendered pixels are discernible against the background and the TF maintains the perceptual mapping between the colors and data intensity values. Finally, we present an assessment of our approach through objective evaluations and subjective user studies.},
  archive      = {J_TVCG},
  author       = {Saeed Boorboor and Matthew S. Castellana and Yoonsang Kim and Chen Zhu-tian and Johanna Beyer and Hanspeter Pfister and Arie E. Kaufman},
  doi          = {10.1109/TVCG.2023.3340770},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6801-6812},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VoxAR: Adaptive visualization of volume rendered objects in optical see-through augmented reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The role of text in visualizations: How annotations shape
perceptions of bias and influence predictions. <em>TVCG</em>,
<em>30</em>(10), 6787–6800. (<a
href="https://doi.org/10.1109/TVCG.2023.3338451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the role of text in visualizations, specifically the impact of text position, semantic content, and biased wording. Two empirical studies were conducted based on two tasks (predicting data trends and appraising bias) using two visualization types (bar and line charts). While the addition of text had a minimal effect on how people perceive data trends, there was a significant impact on how biased they perceive the authors to be. This finding revealed a relationship between the degree of bias in textual information and the perception of the authors’ bias. Exploratory analyses support an interaction between a person&#39;s prediction and the degree of bias they perceived. This paper also develops a crowdsourced method for creating chart annotations that range from neutral to highly biased. This research highlights the need for designers to mitigate potential polarization of readers’ opinions based on how authors’ ideas are expressed.},
  archive      = {J_TVCG},
  author       = {Chase Stokes and Cindy Xiong Bearfield and Marti A. Hearst},
  doi          = {10.1109/TVCG.2023.3338451},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6787-6800},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The role of text in visualizations: How annotations shape perceptions of bias and influence predictions},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). An in-situ visual analytics framework for deep neural
networks. <em>TVCG</em>, <em>30</em>(10), 6770–6786. (<a
href="https://doi.org/10.1109/TVCG.2023.3339585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The past decade has witnessed the superior power of deep neural networks (DNNs) in applications across various domains. However, training a high-quality DNN remains a non-trivial task due to its massive number of parameters. Visualization has shown great potential in addressing this situation, as evidenced by numerous recent visualization works that aid in DNN training and interpretation. These works commonly employ a strategy of logging training-related data and conducting post-hoc analysis . Based on the results of offline analysis, the model can be further trained or fine-tuned. This strategy, however, does not cope with the increasing complexity of DNNs, because (1) the time-series data collected over the training are usually too large to be stored entirely; (2) the huge I/O overhead significantly impacts the training efficiency; (3) post-hoc analysis does not allow rapid human-interventions (e.g., stop training with improper hyper-parameter settings to save computational resources). To address these challenges, we propose an in-situ visualization and analysis framework for the training of DNNs. Specifically, we employ feature extraction algorithms to reduce the size of training-related data in-situ and use the reduced data for real-time visual analytics. The states of model training are disclosed to model designers in real-time, enabling human interventions on demand to steer the training. Through concrete case studies, we demonstrate how our in-situ framework helps deep learning experts optimize DNNs and improve their analysis efficiency.},
  archive      = {J_TVCG},
  author       = {Guan Li and Junpeng Wang and Yang Wang and Guihua Shan and Ying Zhao},
  doi          = {10.1109/TVCG.2023.3339585},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6770-6786},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {An in-situ visual analytics framework for deep neural networks},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-task spatial-temporal graph auto-encoder for hand
motion denoising. <em>TVCG</em>, <em>30</em>(10), 6754–6769. (<a
href="https://doi.org/10.1109/TVCG.2023.3337868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many human-computer interaction applications, fast and accurate hand tracking is necessary for an immersive experience. However, raw hand motion data can be flawed due to issues such as joint occlusions and high-frequency noise, hindering the interaction. Using only current motion for interaction can lead to lag, so predicting future movement is crucial for a faster response. Our solution is the Multi-task Spatial-Temporal Graph Auto-Encoder (Multi-STGAE), a model that accurately denoises and predicts hand motion by exploiting the inter-dependency of both tasks. The model ensures a stable and accurate prediction through denoising while maintaining motion dynamics to avoid over-smoothed motion and alleviate time delays through prediction. A gate mechanism is integrated to prevent negative transfer between tasks and further boost multi-task performance. Multi-STGAE also includes a spatial-temporal graph autoencoder block, which models hand structures and motion coherence through graph convolutional networks, reducing noise while preserving hand physiology. Additionally, we design a novel hand partition strategy and hand bone loss to improve natural hand motion generation. We validate the effectiveness of our proposed method by contributing two large-scale datasets with a data corruption algorithm based on two benchmark datasets. To evaluate the natural characteristics of the denoised and predicted hand motion, we propose two structural metrics. Experimental results show that our method outperforms the state-of-the-art, showcasing how the multi-task framework enables mutual benefits between denoising and prediction.},
  archive      = {J_TVCG},
  author       = {Kanglei Zhou and Hubert P. H. Shum and Frederick W. B. Li and Xiaohui Liang},
  doi          = {10.1109/TVCG.2023.3337868},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6754-6769},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multi-task spatial-temporal graph auto-encoder for hand motion denoising},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GRNet: Geometry restoration for g-PCC compressed point
clouds using auxiliary density signaling. <em>TVCG</em>,
<em>30</em>(10), 6740–6753. (<a
href="https://doi.org/10.1109/TVCG.2023.3336936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lossy Geometry-based Point Cloud Compression (G-PCC) inevitably impairs the geometry information of point clouds, which deteriorates the quality of experience (QoE) in reconstruction and/or misleads decisions in tasks such as classification. To tackle it, this work proposes GRNet for the geometry restoration of G-PCC compressed large-scale point clouds. By analyzing the content characteristics of original and G-PCC compressed point clouds, we attribute the G-PCC distortion to two key factors: point vanishing and point displacement. Visible impairments on a point cloud are usually dominated by an individual factor or superimposed by both factors, which are determined by the density of the original point cloud. To this end, we employ two different models for coordinate reconstruction, termed Coordinate Expansion and Coordinate Refinement, to attack the point vanishing and displacement, respectively. In addition, 4-byte auxiliary density information is signaled in the bitstream to assist the selection of Coordinate Expansion, Coordinate Refinement, or their combination. Before being fed into the coordinate reconstruction module, the G-PCC compressed point cloud is first processed by a Feature Analysis Module for multiscale information fusion, in which $k$ NN-based Transformer is leveraged at each scale to adaptively characterize neighborhood geometric dynamics for effective restoration. Following the common test conditions recommended in the MPEG standardization committee, GRNet significantly improves the G-PCC anchor and remarkably outperforms state-of-the-art methods on a great variety of point clouds (e.g., solid, dense, and sparse samples) both quantitatively and qualitatively. Meanwhile, GRNet runs fairly fast and uses a smaller-size model when compared with existing learning-based approaches, making it attractive to industry practitioners.},
  archive      = {J_TVCG},
  author       = {Gexin Liu and Ruixiang Xue and Jiaxin Li and Dandan Ding and Zhan Ma},
  doi          = {10.1109/TVCG.2023.3336936},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6740-6753},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GRNet: Geometry restoration for G-PCC compressed point clouds using auxiliary density signaling},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MoNetExplorer: A visual analytics system for analyzing
dynamic networks with temporal network motifs. <em>TVCG</em>,
<em>30</em>(10), 6725–6739. (<a
href="https://doi.org/10.1109/TVCG.2023.3337396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partitioning a dynamic network into subsets (i.e., snapshots) based on disjoint time intervals is a widely used technique for understanding how structural patterns of the network evolve. However, selecting an appropriate time window (i.e., slicing a dynamic network into snapshots) is challenging and time-consuming, often involving a trial-and-error approach to investigating underlying structural patterns. To address this challenge, we present MoNetExplorer, a novel interactive visual analytics system that leverages temporal network motifs to provide recommendations for window sizes and support users in visually comparing different slicing results. MoNetExplorer provides a comprehensive analysis based on window size, including (1) a temporal overview to identify the structural information, (2) temporal network motif composition, and (3) node-link-diagram-based details to enable users to identify and understand structural patterns at various temporal resolutions. To demonstrate the effectiveness of our system, we conducted a case study with network researchers using two real-world dynamic network datasets. Our case studies show that the system effectively supports users to gain valuable insights into the temporal and structural aspects of dynamic networks.},
  archive      = {J_TVCG},
  author       = {Seokweon Jung and DongHwa Shin and Hyeon Jeon and Kiroong Choe and Jinwook Seo},
  doi          = {10.1109/TVCG.2023.3337396},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6725-6739},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MoNetExplorer: A visual analytics system for analyzing dynamic networks with temporal network motifs},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TCDM: Transformational complexity based distortion metric
for perceptual point cloud quality assessment. <em>TVCG</em>,
<em>30</em>(10), 6707–6724. (<a
href="https://doi.org/10.1109/TVCG.2023.3338359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of objective point cloud quality assessment (PCQA) research is to develop quantitative metrics that measure point cloud quality in a perceptually consistent manner. Merging the research of cognitive science and intuition of the human visual system (HVS), in this article, we evaluate the point cloud quality by measuring the complexity of transforming the distorted point cloud back to its reference, which in practice can be approximated by the code length of one point cloud when the other is given. For this purpose, we first make space segmentation for the reference and distorted point clouds based on a 3D Voronoi diagram to obtain a series of local patch pairs. Next, inspired by the predictive coding theory, we utilize a space-aware vector autoregressive (SA-VAR) model to encode the geometry and color channels of each reference patch with and without the distorted patch, respectively. Assuming that the residual errors follow the multi-variate Gaussian distributions, the self-complexity of the reference and transformational complexity between the reference and distorted samples are computed using covariance matrices. Additionally, the prediction terms generated by SA-VAR are introduced as one auxiliary feature to promote the final quality prediction. The effectiveness of the proposed transformational complexity based distortion metric (TCDM) is evaluated through extensive experiments conducted on five public point cloud quality assessment databases. The results demonstrate that TCDM achieves state-of-the-art (SOTA) performance, and further analysis confirms its robustness in various scenarios.},
  archive      = {J_TVCG},
  author       = {Yujie Zhang and Qi Yang and Yifei Zhou and Xiaozhong Xu and Le Yang and Yiling Xu},
  doi          = {10.1109/TVCG.2023.3338359},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6707-6724},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TCDM: Transformational complexity based distortion metric for perceptual point cloud quality assessment},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Memory recall for data visualizations in mixed reality,
virtual reality, 3D and 2D. <em>TVCG</em>, <em>30</em>(10), 6691–6706.
(<a href="https://doi.org/10.1109/TVCG.2023.3336588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article explores how the ability to recall information in data visualizations depends on the presentation technology. Participants viewed 10 Isotype visualizations on a 2D screen, in 3D, in Virtual Reality (VR) and in Mixed Reality (MR). To provide a fair comparison between the three 3D conditions, we used LIDAR to capture the details of the physical rooms, and used this information to create our textured 3D models. For all environments, we measured the number of visualizations recalled and their order (2D) or spatial location (3D, VR, MR). We also measured the number of syntactic and semantic features recalled. Results of our study show increased recall and greater richness of data understanding in the MR condition. Not only did participants recall more visualizations and ordinal/spatial positions in MR, but they also remembered more details about graph axes and data mappings, and more information about the shape of the data. We discuss how differences in the spatial and kinesthetic cues provided in these different environments could contribute to these results, and reasons why we did not observe comparable performance in the 3D and VR conditions.},
  archive      = {J_TVCG},
  author       = {Christophe Hurter and Bernice Rogowitz and Guillaume Truong and Tiffany Andry and Hugo Romat and Ludovic Gardy and Fereshteh Amini and Nathalie Henry Riche},
  doi          = {10.1109/TVCG.2023.3336588},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6691-6706},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Memory recall for data visualizations in mixed reality, virtual reality, 3D and 2D},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Preliminary guidelines for combining data integration and
visual data analysis. <em>TVCG</em>, <em>30</em>(10), 6678–6690. (<a
href="https://doi.org/10.1109/TVCG.2023.3334513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data integration is often performed to consolidate information from multiple disparate data sources during visual data analysis. However, integration operations are usually separate from visual analytics operations such as encode and filter in both interface design and empirical research. We conducted a preliminary user study to investigate whether and how data integration should be incorporated directly into the visual analytics process. We used two interface alternatives featuring contrasting approaches to the data preparation and analysis workflow: manual file-based ex-situ integration as a separate step from visual analytics operations; and automatic UI-based in-situ integration merged with visual analytics operations. Participants were asked to complete specific and free-form tasks with each interface, browsing for patterns, generating insights, and summarizing relationships between attributes distributed across multiple files. Analyzing participants’ interactions and feedback, we found both task completion time and total interactions to be similar across interfaces and tasks, as well as unique integration strategies between interfaces and emergent behaviors related to satisficing and cognitive bias. Participants’ time spent and interactions revealed that in-situ integration enabled users to spend more time on analysis tasks compared with ex-situ integration. Participants’ integration strategies and analytical behaviors revealed differences in interface usage for generating and tracking hypotheses and insights. With these results, we synthesized preliminary guidelines for designing future visual analytics interfaces that can support integrating attributes throughout an active analysis process.},
  archive      = {J_TVCG},
  author       = {Adam Coscia and Ashley Suh and Remco Chang and Alex Endert},
  doi          = {10.1109/TVCG.2023.3334513},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6678-6690},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Preliminary guidelines for combining data integration and visual data analysis},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VisionCoach: Design and effectiveness study on VR vision
training for basketball passing. <em>TVCG</em>, <em>30</em>(10),
6665–6677. (<a href="https://doi.org/10.1109/TVCG.2023.3335312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Training is important for basketball players to effectively search for teammates who has wide-open opportunities to shoot, observe the defenders around the wide-open teammates and quickly choose a proper way to pass the ball to the most suitable one. We develop an immersive virtual reality (VR) system called VisionCoach to simulate the player&#39;s viewing perspective and generate three designed systematic vision training tasks to benefit the cultivating procedure. By recording the player&#39;s eye gazing and dribbling video sequence, the proposed system can analyze the vision-related behavior to understand the training effectiveness. To demonstrate the proposed VR training system can facilitate the cultivation of vision ability, we recruited 14 experienced players to participate in a 6-week between-subject study, and conducted a study by comparing the most frequently used 2D vision training method called Vision Performance Enhancement (VPE) program with the proposed system. Qualitative experiences and quantitative training results are reported to show that the proposed immersive VR training system can effectively improve player&#39;s vision ability in terms of gaze behavior and dribbling stability. Furthermore, training in the VR-VisionCoach Condition can transfer the learned abilities to real scenario more easily than training in the 2D-VPE Condition.},
  archive      = {J_TVCG},
  author       = {Pin-Xuan Liu and Tse-Yu Pan and Hsin-Shih Lin and Hung-Kuo Chu and Min-Chun Hu},
  doi          = {10.1109/TVCG.2023.3335312},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6665-6677},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VisionCoach: Design and effectiveness study on VR vision training for basketball passing},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of medical visualization through the lens of
metaphors. <em>TVCG</em>, <em>30</em>(10), 6639–6664. (<a
href="https://doi.org/10.1109/TVCG.2023.3330546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide an overview of metaphors that were used in medical visualization and related user interfaces. Metaphors are employed to translate concepts from a source domain to a target domain. The survey is grounded in a discussion of metaphor-based design involving the identification and reflection of candidate metaphors. We consider metaphors that have a source domain in one branch of medicine, e.g., the virtual mirror that solves problems in orthopedics and laparoscopy with a mirror that resembles the dentist&#39;s mirror. Other metaphors employ the physical world as the source domain, such as crepuscular rays that inspire a solution for access planning in tumor therapy. Aviation is another source of inspiration, leading to metaphors, such as surgical cockpits, surgical control towers, and surgery navigation according to an instrument flight . This paper should raise awareness for metaphors and their potential to focus the design of computer-assisted systems on useful features and a positive user experience. Limitations and potential drawbacks of a metaphor-based user interface design for medical applications are also considered.},
  archive      = {J_TVCG},
  author       = {Bernhard Preim and Monique Meuschke and Veronika Weiß},
  doi          = {10.1109/TVCG.2023.3330546},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6639-6664},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A survey of medical visualization through the lens of metaphors},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient binocular rendering of volumetric density fields
with coupled adaptive cube-map ray marching for virtual reality.
<em>TVCG</em>, <em>30</em>(10), 6625–6638. (<a
href="https://doi.org/10.1109/TVCG.2023.3322416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creating visualizations of multiple volumetric density fields is demanding in virtual reality (VR) applications, which often include divergent volumetric density distributions mixed with geometric models and physics-based simulations. Real-time rendering of such complex environments poses significant challenges for rendering quality and performance. This article presents a novel scheme for efficient real-time rendering of varying translucent volumetric density fields with global illumination (GI) effects on high-resolution binocular VR displays. Our scheme proposes creative solutions to address three challenges involved in the target problem. First, to tackle the doubled heavy workloads of binocular ray marching, we explore the anti-aliasing principles and more advanced potentials of ray marching on interior cube-map faces, and propose a coupled ray-marching technique that converges to multi-resolution cube maps with interleaved adaptive sampling. Second, we devise a fully dynamic ambient GI approximation method that leverages spherical-harmonics (SH) transform information of the phase function to reduce the huge amount of ray sampling required for GI while ensuring fidelity. The method catalyzes spatial ray-marching reuse and adaptive temporal accumulation. Third, we deploy a two-phase ray-tracing algorithm with a tiled k-buffer to achieve fast processing of order-independent transparency (OIT) for multiple volume instances. Consequently, high-quality and high-performance real-time dynamic volume rendering can be achieved under constrained budgets controlled by developers. As our solution supports mixed mesh-volume rendering, the test results prove the practical usefulness of our approach for high-resolution binocular VR rendering on hybrid multi-volumetric and geometric environments.},
  archive      = {J_TVCG},
  author       = {Tianchen Xu and Xiaohua Ren and Jiale Yang and Bin Sheng and Enhua Wu},
  doi          = {10.1109/TVCG.2023.3322416},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6625-6638},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Efficient binocular rendering of volumetric density fields with coupled adaptive cube-map ray marching for virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Task-aware sampling layer for point-wise analysis.
<em>TVCG</em>, <em>30</em>(10), 6612–6624. (<a
href="https://doi.org/10.1109/TVCG.2022.3171794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sampling, grouping, and aggregation are three important components in the multi-scale analysis of point clouds. In this paper, we present a novel data-driven sampler learning strategy for point-wise analysis tasks. Unlike the widely used sampling technique, Farthest Point Sampling (FPS), we propose to learn sampling and downstream applications jointly. Our key insight is that uniform sampling methods like FPS are not always optimal for different tasks: sampling more points around boundary areas can make the point-wise classification easier for segmentation. Towards this end, we propose a novel sampler learning strategy that learns sampling point displacement supervised by task-related ground truth information and can be trained jointly with the underlying tasks. We further demonstrate our methods in various point-wise analysis tasks, including semantic part segmentation, point cloud completion, and keypoint detection. Our experiments show that jointly learning of the sampler and task brings better performance than using FPS in various point-based networks.},
  archive      = {J_TVCG},
  author       = {Yiqun Lin and Lichang Chen and Haibin Huang and Chongyang Ma and Xiaoguang Han and Shuguang Cui},
  doi          = {10.1109/TVCG.2022.3171794},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  number       = {10},
  pages        = {6612-6624},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Task-aware sampling layer for point-wise analysis},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). “Being simple on complex issues” – accounts on visual data
communication about climate change. <em>TVCG</em>, <em>30</em>(9),
6598–6611. (<a href="https://doi.org/10.1109/TVCG.2024.3352282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data visualizations play a critical role in both communicating scientific evidence about climate change and in stimulating engagement and action. To investigate how visualizations can be better utilized to communicate the complexities of climate change to different audiences, we conducted interviews with 17 experts in the fields of climate change, data visualization, and science communication, as well as with 12 laypersons. Besides questions about climate change communication and various aspects of data visualizations, we also asked participants to share what they think is the main takeaway message for two exemplary climate change data visualizations. Through a thematic analysis, we observe differences regarding the included contents, the length and abstraction of messages, and the sensemaking process between and among the participant groups. On average, experts formulated shorter and more abstract messages, often referring to higher-level conclusions rather than specific details. We use our findings to reflect on design decisions for creating more effective visualizations, particularly in news media sources geared toward lay audiences. We hereby discuss the adaption of contents according to the needs of the audience, the trade-off between simplification and accuracy, as well as techniques to make a visualization attractive.},
  archive      = {J_TVCG},
  author       = {Regina Schuster and Kathleen Gregory and Torsten Möller and Laura Koesten},
  doi          = {10.1109/TVCG.2024.3352282},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6598-6611},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {“Being simple on complex issues” – accounts on visual data communication about climate change},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual-preserving mesh repair. <em>TVCG</em>,
<em>30</em>(9), 6586–6597. (<a
href="https://doi.org/10.1109/TVCG.2023.3348829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mesh repair is a long-standing challenge in computer graphics and related fields. Converting defective meshes into watertight manifold meshes can greatly benefit downstream applications such as geometric processing, simulation, fabrication, learning, and synthesis. In this work, by assuming the model is visually correct, we first introduce three visual measures for visibility, orientation, and openness, based on ray-tracing. We then present a novel mesh repair framework incorporating visual measures with several critical steps, i.e., open surface closing, face reorientation, and global optimization, to effectively repair meshes with defects (e.g., gaps, holes, self-intersections, degenerate elements, and inconsistent orientations) and preserve visual appearances. Our method reduces unnecessary mesh complexity without compromising geometric accuracy or visual quality while preserving input attributes such as UV coordinates for rendering. We evaluate our approach on hundreds of models randomly selected from ShapeNet and Thingi10K, demonstrating its effectiveness and robustness compared to existing approaches.},
  archive      = {J_TVCG},
  author       = {Zhongtian Zheng and Xifeng Gao and Zherong Pan and Wei Li and Peng-Shuai Wang and Guoping Wang and Kui Wu},
  doi          = {10.1109/TVCG.2023.3348829},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6586-6597},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual-preserving mesh repair},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design concerns for integrated scripting and interactive
visualization in notebook environments. <em>TVCG</em>, <em>30</em>(9),
6572–6585. (<a href="https://doi.org/10.1109/TVCG.2024.3354561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive visualization can support fluid exploration but is often limited to predetermined tasks. Scripting can support a vast range of queries but may be more cumbersome for free-form exploration. Embedding interactive visualization in scripting environments, such as computational notebooks, provides an opportunity to leverage the strengths of both direct manipulation and scripting. We investigate interactive visualization design methodology, choices, and strategies under this paradigm through a design study of calling context trees used in performance analysis, a field which exemplifies typical exploratory data analysis workflows with Big Data and hard to define problems. We first produce a formal task analysis assigning tasks to graphical or scripting contexts based on their specificity, frequency, and suitability. We then design a notebook-embedded interactive visualization and validate it with intended users. In a follow-up study, we present participants with multiple graphical and scripting interaction modes to elicit feedback about notebook-embedded visualization design, finding consensus in support of the interaction model. We report and reflect on observations regarding the process and design implications for combining visualization and scripting in notebooks.},
  archive      = {J_TVCG},
  author       = {Connor Scully-Allison and Ian Lumsden and Katy Williams and Jesse Bartels and Michela Taufer and Stephanie Brink and Abhinav Bhatele and Olga Pearce and Katherine E. Isaacs},
  doi          = {10.1109/TVCG.2024.3354561},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6572-6585},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Design concerns for integrated scripting and interactive visualization in notebook environments},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inclusion depth for contour ensembles. <em>TVCG</em>,
<em>30</em>(9), 6560–6571. (<a
href="https://doi.org/10.1109/TVCG.2024.3350076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensembles of contours arise in various applications like simulation, computer-aided design, and semantic segmentation. Uncovering ensemble patterns and analyzing individual members is a challenging task that suffers from clutter. Ensemble statistical summarization can alleviate this issue by permitting analyzing ensembles’ distributional components like the mean and median, confidence intervals, and outliers. Contour boxplots, powered by Contour Band Depth (CBD), are a popular non-parametric ensemble summarization method that benefits from CBD&#39;s generality, robustness, and theoretical properties. In this work, we introduce Inclusion Depth (ID), a new notion of contour depth with three defining characteristics. First, ID is a generalization of functional Half-Region Depth, which offers several theoretical guarantees. Second, ID relies on a simple principle: the inside/outside relationships between contours. This facilitates implementing ID and understanding its results. Third, the computational complexity of ID scales quadratically in the number of members of the ensemble, improving CBD&#39;s cubic complexity. This also in practice speeds up the computation enabling the use of ID for exploring large contour ensembles or in contexts requiring multiple depth evaluations like clustering. In a series of experiments on synthetic data and case studies with meteorological and segmentation data, we evaluate ID&#39;s performance and demonstrate its capabilities for the visual analysis of contour ensembles.},
  archive      = {J_TVCG},
  author       = {Nicolas F. Chaves-de-Plaza and Prerak Mody and Marius Staring and René van Egmond and Anna Vilanova and Klaus Hildebrandt},
  doi          = {10.1109/TVCG.2024.3350076},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6560-6571},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Inclusion depth for contour ensembles},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A self-supervised network-based smoke removal and depth
estimation for monocular endoscopic videos. <em>TVCG</em>,
<em>30</em>(9), 6547–6559. (<a
href="https://doi.org/10.1109/TVCG.2023.3347438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In minimally invasive surgery videos, label-free monocular laparoscopic depth estimation is challenging due to smoke. For this reason, we propose a self-supervised collaborative network-based depth estimation method with smoke-removal for monocular endoscopic video, which is decomposed into two steps of smoke-removal and depth estimation. In the first step, we develop a de-endoscopic smoke for cyclic GAN (DS-cGAN) to mitigate the smoke components at different concentrations. The designed generator network comprises sharpened guide encoding module (SGEM), residual dense bottleneck module (RDBM) and refined upsampling convolution module (RUCM), which restores more detailed organ edges and tissue structures. In the second step, high resolution residual U-Net (HRR-UNet) consisting of a DepthNet and two PoseNets is designed to improve the depth estimation accuracy, and adjacent frames are used for camera self-motion estimation. In particular, the proposed method requires neither manual labeling nor patient computed tomography scans during the training and inference phases. Experimental studies on the laparoscopic data set of the Hamlyn Centre show that our method can effectively achieve accurate depth information after net smoking in real surgical scenes while preserving the blood vessels, contours and textures of the surgical site. The experimental results demonstrate that the proposed method outperforms existing state-of-the-art methods in effectiveness and achieves a frame rate of 94.45fps in real time, making it a promising clinical application.},
  archive      = {J_TVCG},
  author       = {Guo Zhang and Xinbo Gao and Hongying Meng and Yu Pang and Xixi Nie},
  doi          = {10.1109/TVCG.2023.3347438},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6547-6559},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A self-supervised network-based smoke removal and depth estimation for monocular endoscopic videos},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sketch beautification: Learning part beautification and
structure refinement for sketches of man-made objects. <em>TVCG</em>,
<em>30</em>(9), 6533–6546. (<a
href="https://doi.org/10.1109/TVCG.2023.3346995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel freehand sketch beautification method, which takes as input a freely drawn sketch of a man-made object and automatically beautifies it both geometrically and structurally. Beautifying a sketch is challenging because of its highly abstract and heavily diverse drawing manner. Existing methods are usually confined to their limited training samples and thus cannot beautify freely drawn sketches with both geometric and structural variations. To address this challenge, we adopt a divide-and-combine strategy. Specifically, we first parse an input sketch into semantic components, beautify individual components by a learned part beautification module based on part-level implicit manifolds, and then reassemble the beautified components through a structure beautification module. With this strategy, our method can go beyond the training samples and handle novel freehand sketches. We demonstrate the effectiveness of our system with extensive experiments and a perceptual study.},
  archive      = {J_TVCG},
  author       = {Deng Yu and Manfred Lau and Lin Gao and Hongbo Fu},
  doi          = {10.1109/TVCG.2023.3346995},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6533-6546},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Sketch beautification: Learning part beautification and structure refinement for sketches of man-made objects},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). KnowledgeVIS: Interpreting language models by comparing
fill-in-the-blank prompts. <em>TVCG</em>, <em>30</em>(9), 6520–6532. (<a
href="https://doi.org/10.1109/TVCG.2023.3346713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent growth in the popularity of large language models has led to their increased usage for summarizing, predicting, and generating text, making it vital to help researchers and engineers understand how and why they work. We present KnowledgeVIS , a human-in-the-loop visual analytics system for interpreting language models using fill-in-the-blank sentences as prompts. By comparing predictions between sentences, KnowledgeVIS reveals learned associations that intuitively connect what language models learn during training to natural language tasks downstream, helping users create and test multiple prompt variations, analyze predicted words using a novel semantic clustering technique, and discover insights using interactive visualizations. Collectively, these visualizations help users identify the likelihood and uniqueness of individual predictions, compare sets of predictions between prompts, and summarize patterns and relationships between predictions across all prompts. We demonstrate the capabilities of KnowledgeVIS with feedback from six NLP experts as well as three different use cases: (1) probing biomedical knowledge in two domain-adapted models; and (2) evaluating harmful identity stereotypes and (3) discovering facts and relationships between three general-purpose models.},
  archive      = {J_TVCG},
  author       = {Adam Coscia and Alex Endert},
  doi          = {10.1109/TVCG.2023.3346713},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6520-6532},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {KnowledgeVIS: Interpreting language models by comparing fill-in-the-blank prompts},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient computation of geodesics in color space.
<em>TVCG</em>, <em>30</em>(9), 6507–6519. (<a
href="https://doi.org/10.1109/TVCG.2023.3346673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although scientists agree that a perceptual color space is not Euclidean and color difference measures, such as CIELAB&#39;s $\Delta E_{2000}$ , model these aspects of color perception, colormaps are still mostly evaluated through piecewise linear interpolation in a Euclidean color space. In a non-Euclidean setting, the piecewise linear interpolation of a colormap through control points translates to finding shortest paths. Alternatively, a smooth interpolation can be generalized to finding the straightest path. Both approaches are difficult to solve and are compute intensive. We compare the 11 most promising optimization algorithms for the computation of a geodesic either as the shortest or as the straightest path to find the most efficient one to use for colormap interpolation in real-world applications. For two control points, the zero curvature algorithms excelled, especially the 2D relaxation method. For multiple control points, only the mimimal curvature algorithms can produce smooth curves, amongst which the 1D relaxation method performed best.},
  archive      = {J_TVCG},
  author       = {Roxana Bujack and Elektra Caffrey and Emily Teti and Terece L. Turton and David H. Rogers and Jonah Miller},
  doi          = {10.1109/TVCG.2023.3346673},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6507-6519},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Efficient computation of geodesics in color space},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and evaluation of controller-based raycasting methods
for efficient alphanumeric and special character entry in virtual
reality. <em>TVCG</em>, <em>30</em>(9), 6493–6506. (<a
href="https://doi.org/10.1109/TVCG.2024.3349428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alphanumeric and special characters are essential during text entry. Text entry in virtual reality (VR) is usually performed on a virtual Qwerty keyboard to minimize the need to learn new layouts. As such, entering capitals, symbols, and numbers in VR is often a direct migration from a physical/touchscreen Qwerty keyboard—that is, using the mode-switching keys to switch between different types of characters and symbols. However, there are inherent differences between a keyboard in VR and a physical/touchscreen keyboard, and as such, a direct adaptation of mode-switching via switch keys may not be suitable for VR. The high flexibility afforded by VR opens up more possibilities for entering alphanumeric and special characters using the Qwerty layout. In this work, we designed two controller-based raycasting text entry methods for alphanumeric and special characters input ( Layer-ButtonSwitch and Key-ButtonSwitch ) and compared them with two other methods ( Standard Qwerty Keyboard and Layer-PointSwitch ) that were derived from physical and soft Qwerty keyboards. We explored the performance and user preference of these four methods via two user studies (one short-term and one prolonged use), where participants were instructed to input text containing alphanumeric and special characters. Our results show that Layer-ButtonSwitch led to the highest statistically significant performance, followed by Key-ButtonSwitch and Standard Qwerty Keyboard , while Layer-PointSwitch had the slowest speed. With continuous practice, participants’ performance using Key-ButtonSwitch reached that of Layer-ButtonSwitch . Further, the results show that the key-level layout used in Key-ButtonSwitch led users to parallel mode switching and character input operations because this layout showed all characters on one layer. We distill three recommendations from the results that can help guide the design of text entry techniques for alphanumeric and special characters in VR.},
  archive      = {J_TVCG},
  author       = {Tingjie Wan and Yushi Wei and Rongkai Shi and Junxiao Shen and Per Ola Kristensson and Katie Atkinson and Hai-Ning Liang},
  doi          = {10.1109/TVCG.2024.3349428},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6493-6506},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Design and evaluation of controller-based raycasting methods for efficient alphanumeric and special character entry in virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Text2Face: Text-based face generation with geometry and
appearance control. <em>TVCG</em>, <em>30</em>(9), 6481–6492. (<a
href="https://doi.org/10.1109/TVCG.2023.3349050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the emergence of various techniques proposed for text-based human face generation and manipulation. Such methods, targeting bridging the semantic gap between text and visual contents, provide users with a deft hand to turn ideas into visuals via text interface and enable more diversified multimedia applications. However, due to the flexibility of linguistic expressiveness, the mapping from sentences to desired facial images is clearly many-to-many, causing ambiguities during text-to-face generation. To alleviate these ambiguities, we introduce a local-to-global framework with two graph neural networks (one for geometry and the other for appearance) embedded to model the inter-dependency among facial parts. This is based upon our key observation that the geometry and appearance attributes among different facial components are not mutually independent, i.e., the combinations of part-level facial features are not arbitrary and thus do not conform to a uniform distribution. By learning from the dataset distribution and enabling recommendations given partial descriptions of human faces, these networks are highly suitable for our text-to-face task. Our method is capable of generating high-quality attribute-conditioned facial images from text. Extensive experiments have confirmed the superiority and usability of our method over the prior art.},
  archive      = {J_TVCG},
  author       = {Zhaoyang Zhang and Junliang Chen and Hongbo Fu and Jianjun Zhao and Shu-Yu Chen and Lin Gao},
  doi          = {10.1109/TVCG.2023.3349050},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6481-6492},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Text2Face: Text-based face generation with geometry and appearance control},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Studying the effect of material and geometry on perceptual
outdoor illumination. <em>TVCG</em>, <em>30</em>(9), 6468–6480. (<a
href="https://doi.org/10.1109/TVCG.2023.3347560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding and modeling perceived properties of sky-dome illumination is an important but challenging problem due to the interplay of several factors such as the materials and geometries of the objects present in the scene being observed. Existing models of sky-dome illumination focus on the physical properties of the sky. However, these parametric models often do not align well with the properties perceived by a human observer. In this work, drawing inspiration from the Hosek-Wilkie sky-dome model, we investigate the perceptual properties of outdoor illumination. For this purpose, we perform a large-scale user study via crowdsourcing to collect a dataset of perceived illumination properties ( scattering , glare , and brightness ) for different combinations of geometries and materials under a variety of outdoor illuminations, totaling 5,000 distinct images. We perform a thorough statistical analysis of the collected data which reveals several interesting effects. For instance, our analysis shows that when there are objects in the scene made of rough materials, the perceived scattering of the sky increases. Furthermore, we utilize our extensive collection of images and their corresponding perceptual attributes to train a predictor. This predictor, when provided with a single image as input, generates an estimation of perceived illumination properties that align with human perceptual judgments. Accurately estimating perceived illumination properties can greatly enhance the overall quality of integrating virtual objects into real scene photographs. Consequently, we showcase various applications of our predictor. For instance, we demonstrate its utility as a luminance editing tool for showcasing virtual objects in outdoor scenes.},
  archive      = {J_TVCG},
  author       = {Miao Wang and Jin-Chao Zhou and Wei-Qi Feng and Yu-Zhu Jiang and Ana Serrano},
  doi          = {10.1109/TVCG.2023.3347560},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6468-6480},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Studying the effect of material and geometry on perceptual outdoor illumination},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on progressive visualization. <em>TVCG</em>,
<em>30</em>(9), 6447–6467. (<a
href="https://doi.org/10.1109/TVCG.2023.3346641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, growing data sources and long-running algorithms impede user attention and interaction with visual analytics applications. Progressive visualization (PV) and visual analytics (PVA) alleviate this problem by allowing immediate feedback and interaction with large datasets and complex computations, avoiding waiting for complete results by using partial results improving with time. Yet, creating a progressive visualization requires more effort than a regular visualization but also opens up new possibilities, such as steering the computations towards more relevant parts of the data, thus saving computational resources. However, there is currently no comprehensive overview of the design space for progressive visualization systems. We surveyed the related work of PV and derived a new taxonomy for progressive visualizations by systematically categorizing all PV publications that included visualizations with progressive features. Progressive visualizations can be categorized by well-known visualization taxonomies, but we also found that progressive visualizations can be distinguished by the way they manage their data processing, data domain, and visual update. Furthermore, we identified key properties such as uncertainty, steering, visual stability, and real-time processing that are significantly different with progressive applications. We also collected evaluation methodologies reported by the publications and conclude with statistical findings, research gaps, and open challenges.},
  archive      = {J_TVCG},
  author       = {Alex Ulmer and Marco Angelini and Jean-Daniel Fekete and Jörn Kohlhammer and Thorsten May},
  doi          = {10.1109/TVCG.2023.3346641},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6447-6467},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A survey on progressive visualization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MSL-net: Sharp feature detection network for 3D point
clouds. <em>TVCG</em>, <em>30</em>(9), 6433–6446. (<a
href="https://doi.org/10.1109/TVCG.2023.3346907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a significant geometric feature of 3D point clouds, sharp features play an important role in shape analysis, 3D reconstruction, registration, localization, etc. Current sharp feature detection methods are still sensitive to the quality of the input point cloud, and the detection performance is affected by random noisy points and non-uniform densities. In this paper, using the prior knowledge of geometric features, we propose a Multi-scale Laplace Network (MSL-Net), a new deep-learning-based method based on an intrinsic neighbor shape descriptor, to detect sharp features from 3D point clouds. First, we establish a discrete intrinsic neighborhood of the point cloud based on the Laplacian graph, which reduces the error of local implicit surface estimation. Then, we design a new intrinsic shape descriptor based on the intrinsic neighborhood, combined with enhanced normal extraction and cosine-based field estimation function. Finally, we present the backbone of MSL-Net based on the intrinsic shape descriptor. Benefiting from the intrinsic neighborhood and shape descriptor, our MSL-Net has simple architecture and is capable of establishing accurate feature prediction that satisfies the manifold distribution while avoiding complex intrinsic metric calculations. Extensive experimental results demonstrate that with the multi-scale structure, MSL-Net has a strong analytical ability for local perturbations of point clouds. Compared with state-of-the-art methods, our MSL-Net is more robust and accurate.},
  archive      = {J_TVCG},
  author       = {Xianhe Jiao and Chenlei Lv and Ran Yi and Junli Zhao and Zhenkuan Pan and Zhongke Wu and Yong-Jin Liu},
  doi          = {10.1109/TVCG.2023.3346907},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6433-6446},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MSL-net: Sharp feature detection network for 3D point clouds},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cone-traced supersampling with subpixel edge reconstruction.
<em>TVCG</em>, <em>30</em>(9), 6421–6432. (<a
href="https://doi.org/10.1109/TVCG.2023.3343166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While signed distance fields (SDFs) in theory offer infinite level of detail, they are typically rendered using the sphere tracing algorithm at finite resolutions, which causes the common rasterized image synthesis problem of aliasing. Most existing optimized antialiasing solutions rely on polygon mesh representations; SDF-based geometry can only be directly antialiased with the computationally expensive supersampling or with post-processing filters that may produce undesirable blurriness and ghosting. In this work, we present cone-traced supersampling (CTSS), an efficient and robust spatial antialiasing solution that naturally complements the sphere tracing algorithm, does not require casting additional rays per pixel or offline pre-filtering, and can be easily implemented in existing real-time SDF renderers. CTSS performs supersampling along the traced ray near surfaces with partial visibility – object contours – identified by evaluating cone intersections within a pixel&#39;s view frustum. We further introduce subpixel edge reconstruction (SER), a technique that extends CTSS to locate and resolve complex pixels with geometric edges in relatively flat regions, which are otherwise undetected by cone intersections. Our combined solution relies on a specialized sampling strategy to minimize the number of shading computations and correlates sample visibility to aggregate the samples. With comparable antialiasing quality at significantly lower computational cost, CTSS is a reliable practical alternative to conventional supersampling.},
  archive      = {J_TVCG},
  author       = {Andrei Chubarau and Yangyang Zhao and Ruby Rao and Derek Nowrouzezahrai and Paul G. Kry},
  doi          = {10.1109/TVCG.2023.3343166},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6421-6432},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Cone-traced supersampling with subpixel edge reconstruction},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CRefNet: Learning consistent reflectance estimation with a
decoder-sharing transformer. <em>TVCG</em>, <em>30</em>(9), 6407–6420.
(<a href="https://doi.org/10.1109/TVCG.2023.3337870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present CRefNet, a hybrid transformer-convolutional deep neural network for consistent reflectance estimation in intrinsic image decomposition. Estimating consistent reflectance is particularly challenging when the same material appears differently due to changes in illumination. Our method achieves enhanced global reflectance consistency via a novel transformer module that converts image features to reflectance features. At the same time, this module also exploits long-range data interactions. We introduce reflectance reconstruction as a novel auxiliary task that shares a common decoder with the reflectance estimation task, and which substantially improves the quality of reconstructed reflectance maps. Finally, we improve local reflectance consistency via a new rectified gradient filter that effectively suppresses small variations in predictions without any overhead at inference time. Our experiments show that our contributions enable CRefNet to predict highly consistent reflectance maps and to outperform the state of the art by 10% WHDR.},
  archive      = {J_TVCG},
  author       = {Jundan Luo and Nanxuan Zhao and Wenbin Li and Christian Richardt},
  doi          = {10.1109/TVCG.2023.3337870},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6407-6420},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CRefNet: Learning consistent reflectance estimation with a decoder-sharing transformer},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wasserstein auto-encoders of merge trees (and persistence
diagrams). <em>TVCG</em>, <em>30</em>(9), 6390–6406. (<a
href="https://doi.org/10.1109/TVCG.2023.3334755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a computational framework for the Wasserstein auto-encoding of merge trees (MT-WAE), a novel extension of the classical auto-encoder neural network architecture to the Wasserstein metric space of merge trees. In contrast to traditional auto-encoders which operate on vectorized data, our formulation explicitly manipulates merge trees on their associated metric space at each layer of the network, resulting in superior accuracy and interpretability. Our novel neural network approach can be interpreted as a non-linear generalization of previous linear attempts (Pont et al. 2023) at merge tree encoding. It also trivially extends to persistence diagrams. Extensive experiments on public ensembles demonstrate the efficiency of our algorithms, with MT-WAE computations in the orders of minutes on average. We show the utility of our contributions in two applications adapted from previous work on merge tree encoding (Pont et al. 2023). First, we apply MT-WAE to merge tree compression , by concisely representing them with their coordinates in the final layer of our auto-encoder. Second, we document an application to dimensionality reduction , by exploiting the latent space of our auto-encoder, for the visual analysis of ensemble data. We illustrate the versatility of our framework by introducing two penalty terms, to help preserve in the latent space both the Wasserstein distances between merge trees, as well as their clusters. In both applications, quantitative experiments assess the relevance of our framework. Finally, we provide a C++ implementation that can be used for reproducibility.},
  archive      = {J_TVCG},
  author       = {Mathieu Pont and Julien Tierny},
  doi          = {10.1109/TVCG.2023.3334755},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6390-6406},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Wasserstein auto-encoders of merge trees (and persistence diagrams)},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HazARdSnap: Gazed-based augmentation delivery for safe
information access while cycling. <em>TVCG</em>, <em>30</em>(9),
6378–6389. (<a href="https://doi.org/10.1109/TVCG.2023.3333336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During cycling activities, cyclists often monitor a variety of information such as heart rate, distance, and navigation using a bike-mounted phone or cyclocomputer. In many cases, cyclists also ride on sidewalks or paths that contain pedestrians and other obstructions such as potholes, so monitoring information on a bike-mounted interface can slow the cyclist down or cause accidents and injury. In this article, we present HazARdSnap, an augmented reality-based information delivery approach that improves the ease of access to cycling information and at the same time preserves the user&#39;s awareness of hazards. To do so, we implemented real-time outdoor hazard detection using a combination of computer vision and motion and position data from a head mounted display (HMD). We then developed an algorithm that snaps information to detected hazards when they are also viewed so that users can simultaneously view both rendered virtual cycling information and the real-world cues such as depth, position, time to hazard, and speed that are needed to assess and avoid hazards. Results from a study with 24 participants that made use of real-world cycling and virtual hazards showed that both HazARdSnap and forward-fixed augmented reality (AR) user interfaces (UIs) can effectively help cyclists access virtual information without having to look down, which resulted in fewer collisions (51% and 43% reduction compared to baseline, respectively) with virtual hazards.},
  archive      = {J_TVCG},
  author       = {Guanghan Zhao and Jason Orlosky and Joseph Gabbard and Kiyoshi Kiyokawa},
  doi          = {10.1109/TVCG.2023.3333336},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6378-6389},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HazARdSnap: Gazed-based augmentation delivery for safe information access while cycling},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Submerse: Visualizing storm surge flooding simulations in
immersive display ecologies. <em>TVCG</em>, <em>30</em>(9), 6365–6377.
(<a href="https://doi.org/10.1109/TVCG.2023.3332511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Submerse , an end-to-end framework for visualizing flooding scenarios on large and immersive display ecologies. Specifically, we reconstruct a surface mesh from input flood simulation data and generate a to-scale 3D virtual scene by incorporating geographical data such as terrain, textures, buildings, and additional scene objects. To optimize computation and memory performance for large simulation datasets, we discretize the data on an adaptive grid using dynamic quadtrees and support level-of-detail based rendering. Moreover, to provide a perception of flooding direction for a time instance, we animate the surface mesh by synthesizing water waves. As interaction is key for effective decision-making and analysis, we introduce two novel techniques for flood visualization in immersive systems: (1) an automatic scene-navigation method using optimal camera viewpoints generated for marked points-of-interest based on the display layout, and (2) an AR-based focus+context technique using an aux display system. Submerse is developed in collaboration between computer scientists and atmospheric scientists. We evaluate the effectiveness of our system and application by conducting workshops with emergency managers, domain experts, and concerned stakeholders in the Stony Brook Reality Deck, an immersive gigapixel facility, to visualize a superstorm flooding scenario in New York City.},
  archive      = {J_TVCG},
  author       = {Saeed Boorboor and Yoonsang Kim and Ping Hu and Josef M. Moses and Brian A. Colle and Arie E. Kaufman},
  doi          = {10.1109/TVCG.2023.3332511},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6365-6377},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Submerse: Visualizing storm surge flooding simulations in immersive display ecologies},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring mid-air hand interaction in data visualization.
<em>TVCG</em>, <em>30</em>(9), 6347–6364. (<a
href="https://doi.org/10.1109/TVCG.2023.3332647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interacting with data visualizations without an instrument or touch surface is typically characterized by the use of mid-air hand gestures. While mid-air expressions can be quite intuitive for interacting with digital content at a distance, they frequently lack precision and necessitate a different way of expressing users’ data-related intentions. In this work, we aim to identify new designs for mid-air hand gesture manipulations that can facilitate instrument-free, touch-free, and embedded interactions with visualizations, while utilizing the three-dimensional (3D) interaction space that mid-air gestures afford. We explore mid-air hand gestures for data visualization by searching for natural means to interact with content. We employ three studies—an Elicitation Study, a User Study, and an Expert Study, to provide insight into the users’ mental models, explore the design space, and suggest considerations for future mid-air hand gesture design. In addition to forming strong associations with physical manipulations, we discovered that mid-air hand gestures can: promote space-multiplexed interaction, which allows for a greater degree of expression; play a functional role in visual cognition and comprehension; and enhance creativity and engagement. We further highlight the challenges that designers in this field may face to help set the stage for developing effective gestures for a wide range of touchless interactions with visualizations.},
  archive      = {J_TVCG},
  author       = {Zona Kostic and Catherine Dumas and Sarah Pratt and Johanna Beyer},
  doi          = {10.1109/TVCG.2023.3332647},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6347-6364},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring mid-air hand interaction in data visualization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving depth perception in immersive media devices by
addressing vergence-accommodation conflict. <em>TVCG</em>,
<em>30</em>(9), 6334–6346. (<a
href="https://doi.org/10.1109/TVCG.2023.3331902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, immersive media devices have seen a boost in popularity. However, many problems still remain. Depth perception is a crucial part of how humans behave and interact with their environment. Convergence and accommodation are two physiological mechanisms that provide important depth cues. However, when humans are immersed in virtual environments, they experience a mismatch between these cues. This mismatch causes users to feel discomfort while also hindering their ability to fully perceive object distances. To address the conflict, we have developed a technique that encompasses inverse blurring into immersive media devices. For the inverse blurring, we utilize the classical Wiener deconvolution approach by proposing a novel technique that is applied without the need for an eye-tracker and implemented in a commercial immersive media device. The technique&#39;s ability to compensate for the vergence-accommodation conflict was verified through two user studies aimed at reaching and spatial awareness, respectively. The two studies yielded a statistically significant 36% and 48% error reduction in user performance to estimate distances, respectively. Overall, the work done demonstrates how visual stimuli can be modified to allow users to achieve a more natural perception and interaction with the virtual environment.},
  archive      = {J_TVCG},
  author       = {Razeen Hussain and Manuela Chessa and Fabio Solari},
  doi          = {10.1109/TVCG.2023.3331902},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6334-6346},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Improving depth perception in immersive media devices by addressing vergence-accommodation conflict},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). QuantumEyes: Towards better interpretability of quantum
circuits. <em>TVCG</em>, <em>30</em>(9), 6321–6333. (<a
href="https://doi.org/10.1109/TVCG.2023.3332999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum computing offers significant speedup compared to classical computing, which has led to a growing interest among users in learning and applying quantum computing across various applications. However, quantum circuits, which are fundamental for implementing quantum algorithms, can be challenging for users to understand due to their underlying logic, such as the temporal evolution of quantum states and the effect of quantum amplitudes on the probability of basis quantum states. To fill this research gap, we propose QuantumEyes , an interactive visual analytics system to enhance the interpretability of quantum circuits through both global and local levels. For the global-level analysis, we present three coupled visualizations to delineate the changes of quantum states and the underlying reasons: a Probability Summary View to overview the probability evolution of quantum states; a State Evolution View to enable an in-depth analysis of the influence of quantum gates on the quantum states; a Gate Explanation View to show the individual qubit states and facilitate a better understanding of the effect of quantum gates. For the local-level analysis, we design a novel geometrical visualization dandelion chart to explicitly reveal how the quantum amplitudes affect the probability of the quantum state. We thoroughly evaluated QuantumEyes as well as the novel dandelion chart integrated into it through two case studies on different types of quantum algorithms and in-depth expert interviews with 12 domain experts. The results demonstrate the effectiveness and usability of our approach in enhancing the interpretability of quantum circuits.},
  archive      = {J_TVCG},
  author       = {Shaolun Ruan and Qiang Guan and Paul Griffin and Ying Mao and Yong Wang},
  doi          = {10.1109/TVCG.2023.3332999},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6321-6333},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {QuantumEyes: Towards better interpretability of quantum circuits},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Painterly style transfer with learned brush strokes.
<em>TVCG</em>, <em>30</em>(9), 6309–6320. (<a
href="https://doi.org/10.1109/TVCG.2023.3332950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world paintings are made, by artists, using brush strokes as the rendering primitive to depict semantic content. The bulk of the Neural Style Transfer (NST) is known transferring style using texture patches, not strokes. The output looks like the content image, but some are traced over using the style texture: it does not look painterly. We adopt a very different approach that uses strokes. Our contribution is to analyse paintings to learn stroke families —that is, distributions of strokes based on their shape (a dot, straight lines, curved arcs, etc.). When synthesising a new output, these distributions are sampled to ensure the output is painted with the correct style of stroke. Consequently, our output looks more “painterly” than NST output based on texture. Furthermore, where strokes are placed is an important contributing factor in determining output quality, and we have also addressed this aspect. Humans place strokes to emphasize salient semantically meaningful image content. Conventional NST uses a content loss premised on filter responses that is agnostic to salience. We show that replacing that loss with one based on the language-image model benefits the output through greater emphasis of salient content.},
  archive      = {J_TVCG},
  author       = {Xiao-Chang Liu and Yu-Chen Wu and Peter Hall},
  doi          = {10.1109/TVCG.2023.3332950},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6309-6320},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Painterly style transfer with learned brush strokes},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PointSee: Image enhances point cloud. <em>TVCG</em>,
<em>30</em>(9), 6291–6308. (<a
href="https://doi.org/10.1109/TVCG.2023.3331779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a prevailing trend towards fusing multi-modal information for 3D object detection (3OD). However, challenges related to computational efficiency, plug-and-play capabilities, and accurate feature alignment have not been adequately addressed in the design of multi-modal fusion networks. In this paper, we present PointSee , a lightweight, flexible, and effective multi-modal fusion solution to facilitate various 3OD networks by se mantic feature e nhancement of point clouds (e.g., LiDAR or RGB-D data) assembled with scene images. Beyond the existing wisdom of 3OD, PointSee consists of a hidden module (HM) and a seen module (SM): HM decorates point clouds using 2D image information in an offline fusion manner, leading to minimal or even no adaptations of existing 3OD networks; SM further enriches the point clouds by acquiring point-wise representative semantic features, leading to enhanced performance of existing 3OD networks. Besides the new architecture of PointSee, we propose a simple yet efficient training strategy, to ease the potential inaccurate regressions of 2D object detection networks. Extensive experiments on the popular outdoor/indoor benchmarks show quantitative and qualitative improvements of our PointSee over thirty-five state-of-the-art methods.},
  archive      = {J_TVCG},
  author       = {Lipeng Gu and Xuefeng Yan and Peng Cui and Lina Gong and Haoran Xie and Fu Lee Wang and Jing Qin and Mingqiang Wei},
  doi          = {10.1109/TVCG.2023.3331779},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6291-6308},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PointSee: Image enhances point cloud},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SenseMap: Urban performance visualization and analytics via
semantic textual similarity. <em>TVCG</em>, <em>30</em>(9), 6275–6290.
(<a href="https://doi.org/10.1109/TVCG.2023.3333356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As urban populations grow, effectively accessing urban performance measures such as livability and comfort becomes increasingly important due to their significant socioeconomic impacts. While Point of Interest (POI) data has been utilized for various applications in location-based services, its potential for urban performance analytics remains unexplored. In this article, we present SenseMap, a novel approach for analyzing urban performance by leveraging POI data as a semantic representation of urban functions. We quantify the contribution of POIs to different urban performance measures by calculating semantic textual similarities on our constructed corpus. We propose Semantic-adaptive Kernel Density Estimation which takes into account POIs’ influential areas across different Traffic Analysis Zones and semantic contributions to generate semantic density maps for measures. We design and implement a feature-rich, real-time visual analytics system for users to explore the urban performance of their surroundings. Evaluations with human judgment and reference data demonstrate the feasibility and validity of our method. Usage scenarios and user studies demonstrate the capability, usability and explainability of our system.},
  archive      = {J_TVCG},
  author       = {Juntong Chen and Qiaoyun Huang and Changbo Wang and Chenhui Li},
  doi          = {10.1109/TVCG.2023.3333356},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6275-6290},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SenseMap: Urban performance visualization and analytics via semantic textual similarity},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On a structural similarity index approach for floating-point
data. <em>TVCG</em>, <em>30</em>(9), 6261–6274. (<a
href="https://doi.org/10.1109/TVCG.2023.3332843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data visualization is typically a critical component of post-processing analysis workflows for floating-point output data from large simulation codes, such as global climate models. For example, images are often created from the raw data as a means for evaluation against a reference dataset or image. While the popular Structural Similarity Index Measure (SSIM) is a useful tool for such image comparisons, generating large numbers of images can be costly when simulation data volumes are substantial. In fact, computational cost considerations motivated our development of an alternative to the SSIM, which we refer to as the Data SSIM (DSSIM). The DSSIM is conceptually similar to the SSIM, but can be applied directly to the floating-point data as a means of assessing data quality. We present the DSSIM in the context of quantifying differences due to lossy compression on large volumes of simulation data from a popular climate model. Bypassing image creation results in a sizeable performance gain for this case study. In addition, we show that the DSSIM is useful in terms of avoiding plot-specific (but data-independent) choices that can affect the SSIM. While our work is motivated by and evaluated with climate model output data, the DSSIM may prove useful for other applications involving large volumes of simulation data.},
  archive      = {J_TVCG},
  author       = {Allison H. Baker and Alexander Pinard and Dorit M. Hammerling},
  doi          = {10.1109/TVCG.2023.3332843},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6261-6274},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {On a structural similarity index approach for floating-point data},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VQ-NeRF: Neural reflectance decomposition and editing with
vector quantization. <em>TVCG</em>, <em>30</em>(9), 6247–6260. (<a
href="https://doi.org/10.1109/TVCG.2023.3330518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose VQ-NeRF, a two-branch neural network model that incorporates Vector Quantization (VQ) to decompose and edit reflectance fields in 3D scenes. Conventional neural reflectance fields use only continuous representations to model 3D scenes, despite the fact that objects are typically composed of discrete materials in reality. This lack of discretization can result in noisy material decomposition and complicated material editing. To address these limitations, our model consists of a continuous branch and a discrete branch. The continuous branch follows the conventional pipeline to predict decomposed materials, while the discrete branch uses the VQ mechanism to quantize continuous materials into individual ones. By discretizing the materials, our model can reduce noise in the decomposition process and generate a segmentation map of discrete materials. Specific materials can be easily selected for further editing by clicking on the corresponding area of the segmentation outcomes. Additionally, we propose a dropout-based VQ codeword ranking strategy to predict the number of materials in a scene, which reduces redundancy in the material segmentation process. To improve usability, we also develop an interactive interface to further assist material editing. We evaluate our model on both computer-generated and real-world scenes, demonstrating its superior performance. To the best of our knowledge, our model is the first to enable discrete material editing in 3D scenes.},
  archive      = {J_TVCG},
  author       = {Hongliang Zhong and Jingbo Zhang and Jing Liao},
  doi          = {10.1109/TVCG.2023.3330518},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6247-6260},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VQ-NeRF: Neural reflectance decomposition and editing with vector quantization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised fragment alignment with gaps. <em>TVCG</em>,
<em>30</em>(9), 6235–6246. (<a
href="https://doi.org/10.1109/TVCG.2023.3330859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image alignment and registration methods typically rely on visual correspondences across common regions and boundaries to guide the alignment process. Without them, the problem becomes significantly more challenging. Nevertheless, in real world, image fragments may be corrupted with no common boundaries and little or no overlap. In this work, we address the problem of learning the alignment of image fragments with gaps (i.e., without common boundaries or overlapping regions). Our setting is unsupervised, having only the fragments at hand with no ground truth to guide the alignment process. This is usually the situation in the restoration of unique archaeological artifacts such as frescoes and mosaics. Hence, we suggest a self-supervised approach utilizing self-examples which we generate from the existing data and then feed into an adversarial neural network. Our idea is that available information inside fragments is often sufficiently rich to guide their alignment with good accuracy. Following this observation, our method splits the initial fragments into sub-fragments yielding a set of aligned pieces. Thus, sub-fragmentation allows exposing new alignment relations and revealing inner structures and feature statistics. In fact, the new sub-fragments construct true and false alignment relations between fragments. We feed this data to a spatial transformer GAN which learns to predict the alignment between fragments gaps. We test our technique on various synthetic datasets as well as large scale frescoes and mosaics. Results demonstrate our method&#39;s capability to learn the alignment of deteriorated image fragments in a self-supervised manner, by examining inner image statistics for both synthetic and real data.},
  archive      = {J_TVCG},
  author       = {Mingxin Yang and Yonatan Svirsky and Zhanglin Cheng and Andrei Sharf},
  doi          = {10.1109/TVCG.2023.3330859},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6235-6246},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Self-supervised fragment alignment with gaps},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EGST: Enhanced geometric structure transformer for point
cloud registration. <em>TVCG</em>, <em>30</em>(9), 6222–6234. (<a
href="https://doi.org/10.1109/TVCG.2023.3329578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore the effect of geometric structure descriptors on extracting reliable correspondences and obtaining accurate registration for point cloud registration. The point cloud registration task involves the estimation of rigid transformation motion in unorganized point cloud, hence it is crucial to capture the contextual features of the geometric structure in point cloud. Recent coordinates-only methods ignore numerous geometric information in the point cloud which weaken ability to express the global context. We propose Enhanced Geometric Structure Transformer to learn enhanced contextual features of the geometric structure in point cloud and model the structure consistency between point clouds for extracting reliable correspondences, which encodes three explicit enhanced geometric structures and provides significant cues for point cloud registration. More importantly, we report empirical results that Enhanced Geometric Structure Transformer can learn meaningful geometric structure features using none of the following: (i) explicit positional embeddings, (ii) additional feature exchange module such as cross-attention, which can simplify network structure compared with plain Transformer. Extensive experiments on the synthetic dataset and real-world datasets illustrate that our method can achieve competitive results.},
  archive      = {J_TVCG},
  author       = {Yongzhe Yuan and Yue Wu and Xiaolong Fan and Maoguo Gong and Wenping Ma and Qiguang Miao},
  doi          = {10.1109/TVCG.2023.3329578},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6222-6234},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {EGST: Enhanced geometric structure transformer for point cloud registration},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Influence of scenarios and player traits on flow in virtual
reality. <em>TVCG</em>, <em>30</em>(9), 6208–6221. (<a
href="https://doi.org/10.1109/TVCG.2023.3332261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many studies have investigated how interpersonal differences between users influence their experience in Virtual Reality (VR) and it is now well recognized that user&#39;s subjective experiences and responses to the same VR environment can vary widely. In this study, we focus on player traits, which correspond to users’ preferences for game mechanics, arguing that players react differently when experiencing VR scenarios. We developed three scenarios in the same VR environment that rely on different game mechanics, and evaluate the influence of the scenarios, the player traits and the time of practice of the VR environment on users’ perceived flow. Our results show that 1) the type of scenario has an impact on specific dimensions of flow; 2) the scenarios have different effects on flow depending on the order they are performed, the flow preconditions being stronger when performed at last; 3) almost all dimensions of flow are influenced by the player traits, these influences depending on the scenario, 4) the Aesthetic trait has the most influences in the three scenarios. We finally discuss the findings and limitations of the present study that we believe have strong implications for the design of scenarios in VR experiences.},
  archive      = {J_TVCG},
  author       = {Élise Lavoué and Sophie Villenave and Audrey Serna and Clémentine Didier and Patrick Baert and Guillaume Lavoué},
  doi          = {10.1109/TVCG.2023.3332261},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6208-6221},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Influence of scenarios and player traits on flow in virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GR-PSN: Learning to estimate surface normal and reconstruct
photometric stereo images. <em>TVCG</em>, <em>30</em>(9), 6192–6207. (<a
href="https://doi.org/10.1109/TVCG.2023.3329817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel method, namely GR-PSN, which learns surface normals from photometric stereo images and generates the photometric images under distant illumination from different lighting directions and surface materials. The framework is composed of two subnetworks, named GeometryNet and ReconstructNet, which are cascaded to perform shape reconstruction and image rendering in an end-to-end manner. ReconstructNet introduces additional supervision for surface-normal recovery, forming a closed-loop structure with GeometryNet. We also encode lighting and surface reflectance in ReconstructNet, to achieve arbitrary rendering. In training, we set up a parallel framework to simultaneously learn two arbitrary materials for an object, providing an additional transform loss. Therefore, our method is trained based on the supervision by three different loss functions, namely the surface-normal loss, reconstruction loss, and transform loss. We alternately input the predicted surface-normal map and the ground-truth into ReconstructNet, to achieve stable training for ReconstructNet. Experiments show that our method can accurately recover the surface normals of an object with an arbitrary number of inputs, and can re-render images of the object with arbitrary surface materials. Extensive experimental results show that our proposed method outperforms those methods based on a single surface recovery network and shows realistic rendering results on 100 different materials.},
  archive      = {J_TVCG},
  author       = {Yakun Ju and Boxin Shi and Yang Chen and Huiyu Zhou and Junyu Dong and Kin-Man Lam},
  doi          = {10.1109/TVCG.2023.3329817},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6192-6207},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GR-PSN: Learning to estimate surface normal and reconstruct photometric stereo images},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 360<span class="math inline"><sup>∘</sup></span>∘ stereo
image composition with depth adaption. <em>TVCG</em>, <em>30</em>(9),
6177–6191. (<a href="https://doi.org/10.1109/TVCG.2023.3327943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {360 $^\circ$ images and videos have become an economic and popular way to provide VR experiences using real-world content. However, the manipulation of the stereo panoramic content remains less explored. In this article, we focus on the 360 $^\circ$ image composition problem, and develop a solution that can take an object from a stereo image pair and insert it at a given 3D position in a target stereo panorama, with well-preserved geometry information. Our method uses recovered 3D point clouds to guide the composited image generation. More specifically, we observe that using only a one-off operation to insert objects into equirectangular images will never produce satisfactory depth perception and generate ghost artifacts when users are watching the result from different view directions. Therefore, we propose a novel per-view projection method that segments the object in 3D spherical space with the stereo camera pair facing in that direction. A deep depth densification network is proposed to generate depth guidance for the stereo image generation of each view segment according to the desired position and pose of the inserted object. We finally combine the synthesized view segments and blend the objects into the target stereo 360 $^\circ$ scene. A user study demonstrates that our method can provide good depth perception and removes ghost artifacts. The per-view solution is a potential paradigm for other content manipulation methods for 360 $^\circ$ images and videos.},
  archive      = {J_TVCG},
  author       = {Kun Huang and Fang-Lue Zhang and Junhong Zhao and Yiheng Li and Neil Dodgson},
  doi          = {10.1109/TVCG.2023.3327943},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6177-6191},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {360$^\circ$∘ stereo image composition with depth adaption},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Retargeting video with an end-to-end framework.
<em>TVCG</em>, <em>30</em>(9), 6164–6176. (<a
href="https://doi.org/10.1109/TVCG.2023.3327825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video holds significance in computer graphics applications. Because of the heterogeneous of digital devices, retargeting videos becomes an essential function to enhance user viewing experience in such applications. In the research of video retargeting, preserving the relevant visual content in videos, avoiding flicking, and processing time are the vital challenges. Extending image retargeting techniques to the video domain is challenging due to the high running time. Prior work of video retargeting mainly utilizes time-consuming preprocessing to analyze frames. Plus, being tolerant of different video content, avoiding important objects from shrinking, and the ability to play with arbitrary ratios are the limitations that need to be resolved in these systems requiring investigation. In this paper, we present an end-to-end RETVI method to retarget videos to arbitrary aspect ratios. We eliminate the computational bottleneck in the conventional approaches by designing RETVI with two modules, content feature analyzer (CFA) and adaptive deforming estimator (ADE). The extensive experiments and evaluations show that our system outperforms previous work in quality and running time.},
  archive      = {J_TVCG},
  author       = {Thi-Ngoc-Hanh Le and HuiGuang Huang and Yi-Ru Chen and Tong-Yee Lee},
  doi          = {10.1109/TVCG.2023.3327825},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6164-6176},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Retargeting video with an end-to-end framework},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Impact of socio-demographic attributes and mutual gaze of
virtual humans on users’ visual attention and collision avoidance in VR.
<em>TVCG</em>, <em>30</em>(9), 6146–6163. (<a
href="https://doi.org/10.1109/TVCG.2023.3329515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigated the extent that the non-verbal behaviors of virtual humans (VHs) and their socio-demographic attributes altered users’ collision avoidance behaviors in Virtual Reality (VR). Users interacted with VHs representing different levels of ethnicities and gender, exhibiting different conditions of physical movement, and gaze behaviors. The VHs were depicted in three major ethnic conditions namely Asian, Caucasian, and Black. The physical movement states of the VHs were either static in the path of the user or walking toward the user in the opposite direction. The non-verbal gaze behavior of the VHs was either direct gaze or averted gaze. We used an HTC Vive tracking system to track users’ performing real walking while we collected objective measures (i,e., continuous gaze, fixation gaze, clearance distance, and travel length), and subjective variables (i.e., game experiences and social presence). The results showed that the ethnicity of the VHs significantly impacted the gaze behavior of the users, and the gender of the VHs affected the user avoidance movement and their reciprocal gaze behavior. Our results revealed that users’ physical movement, gaze behaviors, and collision avoidance were moderated by the VHs’ perceived ethnicity, gender, and gaze behaviors. Understanding the impact of the socio-demographics attributes of VHs and their gaze behavior on users’ collision avoidance is critical for applications in which users are navigating through virtual traffic, crowd, and other inter-personal simulations.},
  archive      = {J_TVCG},
  author       = {Wei-Chia Huang and Sai-Keung Wong and Matias Volonte and Sabarish V. Babu},
  doi          = {10.1109/TVCG.2023.3329515},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6146-6163},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Impact of socio-demographic attributes and mutual gaze of virtual humans on users’ visual attention and collision avoidance in VR},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactive table synthesis with natural language.
<em>TVCG</em>, <em>30</em>(9), 6130–6145. (<a
href="https://doi.org/10.1109/TVCG.2023.3329120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tables are a ubiquitous data format for insight communication. However, transforming data into consumable tabular views remains a challenging and time-consuming task. To lower the barrier of such a task, research efforts have been devoted to developing interactive approaches for data transformation, but many approaches still presume that their users have considerable knowledge of various data transformation concepts and functions. In this study, we leverage natural language (NL) as the primary interaction modality to improve the accessibility of average users to performing complex data transformation and facilitate intuitive table generation and editing. Designing an NL-driven data transformation approach introduces two challenges: 1) NL-driven synthesis of interpretable pipelines and 2) incremental refinement of synthesized tables. To address these challenges, we present NL2Rigel, an interactive tool that assists users in synthesizing and improving tables from semi-structured text with NL instructions. Based on a large language model and prompting techniques, NL2Rigel can interpret the given NL instructions into a table synthesis pipeline corresponding to Rigel specifications, a declarative language for tabular data transformation. An intuitive interface is designed to visualize the synthesis pipeline and the generated tables, helping users understand the transformation process and refine the results efficiently with targeted NL instructions. The comprehensiveness of NL2Rigel is demonstrated with an example gallery, and we further confirmed NL2Rigel&#39;s usability with a comparative user study by showing that the task completion time with NL2Rigel is significantly shorter than that with the original version of Rigel with comparable completion rates.},
  archive      = {J_TVCG},
  author       = {Yanwei Huang and Yunfan Zhou and Ran Chen and Changhao Pan and Xinhuan Shu and Di Weng and Yingcai Wu},
  doi          = {10.1109/TVCG.2023.3329120},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6130-6145},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive table synthesis with natural language},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collaborative completion and segmentation for partial point
clouds with outliers. <em>TVCG</em>, <em>30</em>(9), 6118–6129. (<a
href="https://doi.org/10.1109/TVCG.2023.3328354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outliers will inevitably creep into the captured point cloud during 3D scanning, degrading cutting-edge models on various geometric tasks heavily. This paper looks at an intriguing question that whether point cloud completion and segmentation can promote each other to defeat outliers. To answer it, we propose a collaborative completion and segmentation network, termed CS-Net, for partial point clouds with outliers. Unlike most of existing methods, CS-Net does not need any clean (or say outlier-free) point cloud as input or any outlier removal operation. CS-Net is a new learning paradigm that makes completion and segmentation networks work collaboratively. With a cascaded architecture, our method refines the prediction progressively. Specifically, after the segmentation network, a cleaner point cloud is fed into the completion network. We design a novel completion network which harnesses the labels obtained by segmentation together with farthest point sampling to purify the point cloud and leverages KNN-grouping for better generation. Benefited from segmentation, the completion module can utilize the filtered point cloud which is cleaner for completion. Meanwhile, the segmentation module is able to distinguish outliers from target objects more accurately with the help of the clean and complete shape inferred by completion. Besides the designed collaborative mechanism of CS-Net, we establish a benchmark dataset of partial point clouds with outliers. Extensive experiments show clear improvements of our CS-Net over its competitors, in terms of outlier robustness and completion accuracy.},
  archive      = {J_TVCG},
  author       = {Changfeng Ma and Yang Yang and Jie Guo and Mingqiang Wei and Chongjun Wang and Yanwen Guo and Wenping Wang},
  doi          = {10.1109/TVCG.2023.3328354},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6118-6129},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Collaborative completion and segmentation for partial point clouds with outliers},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accurate raycasting selection with rotation gesture using a
6-DOF tracking device. <em>TVCG</em>, <em>30</em>(9), 6104–6117. (<a
href="https://doi.org/10.1109/TVCG.2023.3324373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A six degrees of freedom (6-DOF) controller is a commonly used input device in three-dimensional user interface (3DUI) applications. However, for the fundamental task of target selection in 3D space, the selection accuracy decreases owing to the Heisenberg effect during the manipulation of the 6-DOF controller. Based on the pointing action of a 6-DOF device, we establish the mathematical model for raycasting and analyze the possibility of using a rotation gesture to select a target. This study proposes a target selection method using the axial rotation of the user&#39;s wrist, which reduces the negative impact of a discrete input for triggering the selection on accuracy. The detection model can identify the start time of the user&#39;s rotation action. The custom designed control display gain (CD gain) function can maintain the ray&#39;s stability during the rotation gesture. This method was verified using a 6-DOF pen and Vive controller in three experiments. The results show that the accuracy of the proposed rotation gesture-based raycasting target selection method is superior to that of the traditional button-pressing method, and it can be integrated into existing tracking systems for further application.},
  archive      = {J_TVCG},
  author       = {Jiaxin Yu and Weizhi Nai and Xiaoying Sun},
  doi          = {10.1109/TVCG.2023.3324373},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6104-6117},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Accurate raycasting selection with rotation gesture using a 6-DOF tracking device},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IPUNet: Iterative cross field guided point cloud upsampling.
<em>TVCG</em>, <em>30</em>(9), 6089–6103. (<a
href="https://doi.org/10.1109/TVCG.2023.3324924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point clouds acquired by 3D scanning devices are often sparse, noisy, and non-uniform, causing a loss of geometric features. To facilitate the usability of point clouds in downstream applications, given such input, we present a learning-based point upsampling method, i.e., iPUNet , which generates dense and uniform points at arbitrary ratios and better captures sharp features. To generate feature-aware points, we introduce cross fields that are aligned to sharp geometric features by self-supervision to guide point generation. Given cross field defined frames, we enable arbitrary ratio upsampling by learning at each input point a local parameterized surface. The learned surface consumes the neighboring points and 2D tangent plane coordinates as input, and maps onto a continuous surface in 3D where arbitrary ratios of output points can be sampled. To solve the non-uniformity of input points, on top of the cross field guided upsampling, we further introduce an iterative strategy that refines the point distribution by moving sparse points onto the desired continuous 3D surface in each iteration. Within only a few iterations, the sparse points are evenly distributed and their corresponding dense samples are more uniform and better capture geometric features. Through extensive evaluations on diverse scans of objects and scenes, we demonstrate that iPUNet is robust to handle noisy and non-uniformly distributed inputs, and outperforms state-of-the-art point cloud upsampling methods.},
  archive      = {J_TVCG},
  author       = {Guangshun Wei and Hao Pan and Shaojie Zhuang and Yuanfeng Zhou and Changjian Li},
  doi          = {10.1109/TVCG.2023.3324924},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6089-6103},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {IPUNet: Iterative cross field guided point cloud upsampling},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). What do we mean when we say “insight”? A formal synthesis of
existing theory. <em>TVCG</em>, <em>30</em>(9), 6075–6088. (<a
href="https://doi.org/10.1109/TVCG.2023.3326698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers have derived many theoretical models for specifying users’ insights as they interact with a visualization system. These representations are essential for understanding the insight discovery process, such as when inferring user interaction patterns that lead to insight or assessing the rigor of reported insights. However, theoretical models can be difficult to apply to existing tools and user studies, often due to discrepancies in how insight and its constituent parts are defined. This article calls attention to the consistent structures that recur across the visualization literature and describes how they connect multiple theoretical representations of insight. We synthesize a unified formalism for insights using these structures, enabling a wider audience of researchers and developers to adopt the corresponding models. Through a series of theoretical case studies, we use our formalism to compare and contrast existing theories, revealing interesting research challenges in reasoning about a user&#39;s domain knowledge and leveraging synergistic approaches in data mining and data management research.},
  archive      = {J_TVCG},
  author       = {Leilani Battle and Alvitta Ottley},
  doi          = {10.1109/TVCG.2023.3326698},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6075-6088},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {What do we mean when we say “Insight”? a formal synthesis of existing theory},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive evaluation framework of software
visualizations effectiveness. <em>TVCG</em>, <em>30</em>(9), 6056–6074.
(<a href="https://doi.org/10.1109/TVCG.2023.3321211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualizations are useful in dealing with complex software systems, especially in maintenance and evolution tasks. Software visualization tools can help reduce the cognitive burden on practitioners when trying to understand these systems. However, a major challenge in designing new visualization techniques and tools is evaluating their effectiveness for specific tasks and users. If a visualization tool is not effective for practitioners, they are unlikely to adopt it. Existing evaluation frameworks for visualizations mainly focus on expressiveness , which refers to the ability of the visualization to show all necessary information. However, evaluating the effectiveness of visualizations is an open research problem, especially in terms of quantifying it. To address this problem, we propose a multi-dimensional evaluation framework that focuses on evaluating visualizations in terms of their qualitative, quantitative, and cognitive aspects. The framework includes seven main dimensions and twenty-eight features, with the effectiveness dimension being further subdivided into four sub-dimensions. We validate our framework by using it to evaluate a number of software visualization tools. This validation demonstrates that the framework can be applied to design and evaluate new software visualization techniques and tools.},
  archive      = {J_TVCG},
  author       = {Hakam W. Alomari and Christopher Vendome and Lane Rizkallah},
  doi          = {10.1109/TVCG.2023.3321211},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6056-6074},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A comprehensive evaluation framework of software visualizations effectiveness},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interpreting high-dimensional projections with capacity.
<em>TVCG</em>, <em>30</em>(9), 6038–6055. (<a
href="https://doi.org/10.1109/TVCG.2023.3324851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction (DR) algorithms are diverse and widely used for analyzing high-dimensional data. Various metrics and tools have been proposed to evaluate and interpret the DR results. However, most metrics and methods fail to be well generalized to measure any DR results from the perspective of original distribution fidelity or lack interactive exploration of DR results. There is still a need for more intuitive and quantitative analysis to interactively explore high-dimensional data and improve interpretability. We propose a metric and a generalized algorithm-agnostic approach based on the concept of capacity to evaluate and analyze the DR results. Based on our approach, we develop a visual analytic system HiLow for exploring high-dimensional data and projections. We also propose a mixed-initiative recommendation algorithm that assists users in interactively DR results manipulation. Users can compare the differences in data distribution after the interaction through HiLow. Furthermore, we propose a novel visualization design focusing on quantitative analysis of differences between high and low-dimensional data distributions. Finally, through user study and case studies, we validate the effectiveness of our approach and system in enhancing the interpretability of projections and analyzing the distribution of high and low-dimensional data.},
  archive      = {J_TVCG},
  author       = {Yang Zhang and Jisheng Liu and Chufan Lai and Yuan Zhou and Siming Chen},
  doi          = {10.1109/TVCG.2023.3324851},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6038-6055},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interpreting high-dimensional projections with capacity},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3DFaceShop: Explicitly controllable 3D-aware portrait
generation. <em>TVCG</em>, <em>30</em>(9), 6020–6037. (<a
href="https://doi.org/10.1109/TVCG.2023.3323578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contrast to the traditional avatar creation pipeline which is a costly process, contemporary generative approaches directly learn the data distribution from photographs. While plenty of works extend unconditional generative models and achieve some levels of controllability, it is still challenging to ensure multi-view consistency, especially in large poses. In this work, we propose a network that generates 3D-aware portraits while being controllable according to semantic parameters regarding pose, identity, expression and illumination. Our network uses neural scene representation to model 3D-aware portraits, whose generation is guided by a parametric face model that supports explicit control. While the latent disentanglement can be further enhanced by contrasting images with partially different attributes, there still exists noticeable inconsistency in non-face areas when animating expressions. We solve this by proposing a volume blending strategy in which we form a composite output by blending dynamic and static areas, with two parts segmented from the jointly learned semantic field. Our method outperforms prior arts in extensive experiments, producing realistic portraits with vivid expression in natural lighting when viewed from free viewpoints. It also demonstrates generalization ability to real images as well as out-of-domain data, showing great promise in real applications.},
  archive      = {J_TVCG},
  author       = {Junshu Tang and Bo Zhang and Binxin Yang and Ting Zhang and Dong Chen and Lizhuang Ma and Fang Wen},
  doi          = {10.1109/TVCG.2023.3323578},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6020-6037},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {3DFaceShop: Explicitly controllable 3D-aware portrait generation},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Eliciting model steering interactions from users via data
and visual design probes. <em>TVCG</em>, <em>30</em>(9), 6005–6019. (<a
href="https://doi.org/10.1109/TVCG.2023.3322898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual and interactive machine learning systems (IML) are becoming ubiquitous as they empower individuals with varied machine learning expertise to analyze data. However, it remains complex to align interactions with visual marks to a user&#39;s intent for steering machine learning models. We explore using data and visual design probes to elicit users’ desired interactions to steer ML models via visual encodings within IML interfaces. We conducted an elicitation study with 20 data analysts with varying expertise in ML. We summarize our findings as pairs of target-interaction, which we compare to prior systems to assess the utility of the probes. We additionally surfaced insights about factors influencing how and why participants chose to interact with visual encodings, including refraining from interacting. Finally, we reflect on the value of gathering such formative empirical evidence via data and visual design probes ahead of developing IML prototypes.},
  archive      = {J_TVCG},
  author       = {Anamaria Crisan and Maddie Shang and Eric Brochu},
  doi          = {10.1109/TVCG.2023.3322898},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {6005-6019},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Eliciting model steering interactions from users via data and visual design probes},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Piecewise developable modeling via implicit neural
deformation and feature-guided cutting. <em>TVCG</em>, <em>30</em>(9),
5993–6004. (<a href="https://doi.org/10.1109/TVCG.2023.3319487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel and automatic method to model shapes using a small set of discrete developable patches. Central to our approach is using implicit neural shape representation that makes our algorithm independent of tessellation and allows us to obtain the Gaussian curvature of each point analytically. With this powerful representation, we first deform the input shape to be an almost developable shape with clear and sparse salient feature curves. Then, we convert the deformed implicit field to a triangle mesh, which is further cut to disk topology along parts of the sparse feature curves. Finally, we achieve the resulting piecewise developable mesh by alternatingly optimizing discrete developability, enforcing manufacturability constraints, and merging patches. The feasibility and practicability of our method are demonstrated over various shapes. Compared to the state-of-the-art methods, our method achieves a better tradeoff between the number of developable patches and the approximation error.},
  archive      = {J_TVCG},
  author       = {Kang Wu and Zheng-Yu Zhao and Zheng Zhang and Ligang Liu and Xiao-Ming Fu},
  doi          = {10.1109/TVCG.2023.3319487},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {5993-6004},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Piecewise developable modeling via implicit neural deformation and feature-guided cutting},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Investigating the correlation between presence and reaction
time in mixed reality. <em>TVCG</em>, <em>30</em>(9), 5976–5992. (<a
href="https://doi.org/10.1109/TVCG.2023.3319563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring presence is critical to improving user involvement and performance in Mixed Reality (MR). Presence , a crucial aspect of MR, is traditionally gauged using subjective questionnaires, leading to a lack of time-varying responses and susceptibility to user bias. Inspired by the existing literature on the relationship between presence and human performance, the proposed methodology systematically measures a user&#39;s reaction time to a visual stimulus as they interact within a manipulated MR environment. We explore the user reaction time as a quantity that can be easily measured using the systemic tools available in modern MR devices. We conducted an exploratory study (N = 40) with two experiments designed to alter the users’ sense of presence by manipulating place illusion and plausibility illusion . We found a significant correlation between presence scores and reaction times with a correlation coefficient −0.65, suggesting that users with a higher sense of presence responded more swiftly to stimuli. We develop a model that estimates a user&#39;s presence level using the reaction time values with high accuracy of up to 80%. While our study suggests that reaction time can be used as a measure of presence, further investigation is needed to improve the accuracy of the model.},
  archive      = {J_TVCG},
  author       = {Yasra Chandio and Noman Bashir and Victoria Interrante and Fatima M. Anwar},
  doi          = {10.1109/TVCG.2023.3319563},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {5976-5992},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Investigating the correlation between presence and reaction time in mixed reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SuperUDF: Self-supervised UDF estimation for surface
reconstruction. <em>TVCG</em>, <em>30</em>(9), 5965–5975. (<a
href="https://doi.org/10.1109/TVCG.2023.3318085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based surface reconstruction based on unsigned distance functions (UDF) has many advantages such as handling open surfaces. We propose SuperUDF, a self-supervised UDF learning which exploits a learned geometry prior for efficient training and a novel regularization for robustness to sparse sampling. The core idea of SuperUDF draws inspiration from the classical surface approximation operator of locally optimal projection (LOP). The key insight is that if the UDF is estimated correctly, the 3D points should be locally projected onto the underlying surface following the gradient of the UDF. Based on that, a number of inductive biases on UDF geometry and a pre-learned geometry prior are devised to learn UDF estimation efficiently. A novel regularization loss is proposed to make SuperUDF robust to sparse sampling. Furthermore, we also contribute a learning-based mesh extraction from the estimated UDFs. Extensive evaluations demonstrate that SuperUDF outperforms the state of the arts on several public datasets in terms of both quality and efficiency. Code will be released after accteptance.},
  archive      = {J_TVCG},
  author       = {Hui Tian and Chenyang Zhu and Yifei Shi and Kai Xu},
  doi          = {10.1109/TVCG.2023.3318085},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {5965-5975},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SuperUDF: Self-supervised UDF estimation for surface reconstruction},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image inpainting via correlated multi-resolution feature
projection. <em>TVCG</em>, <em>30</em>(9), 5953–5964. (<a
href="https://doi.org/10.1109/TVCG.2023.3315061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement in image editing applications, image inpainting is gaining more attention due to its ability to recover corrupted images efficiently. Also, the existing methods for image inpainting either use two-stage coarse-to-fine architectures or single-stage architectures with a deeper network. On the other hand, shallow network architectures lack the quality of results and the methods with remarkable inpainting quality have high complexity in terms of number of parameters or average run time. Despite the improvement in the inpainting quality, these methods still lack the correlated local and global information. In this work, we propose a single-stage multi-resolution generator architecture for image inpainting with moderate complexity and superior outcomes. Here, a multi-kernel non-local (MKNL) attention block is proposed to merge the feature maps from all the resolutions. Further, a feature projection block is proposed to project features of MKNL to respective decoder for effective reconstruction of image. Also, a valid feature fusion block is proposed to merge encoder skip connection features at valid region and respective decoder features at hole region. This ensures that there will not be any redundant feature merging while reconstruction of image. Effectiveness of the proposed architecture is verified on CelebA-HQ Liu, et al. 2015, Karras et al. 2017, and Places2 Zhou et al. 2018 datasets corrupted with publicly available NVIDIA mask dataset Liu et al. 2018. The detailed ablation study, extensive result analysis, and application of object removal prove the robustness of the proposed method over existing state-of-the-art methods for image inpainting.},
  archive      = {J_TVCG},
  author       = {Shruti S. Phutke and Subrahmanyam Murala},
  doi          = {10.1109/TVCG.2023.3315061},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {5953-5964},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Image inpainting via correlated multi-resolution feature projection},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A topological distance between multi-fields based on
multi-dimensional persistence diagrams. <em>TVCG</em>, <em>30</em>(9),
5939–5952. (<a href="https://doi.org/10.1109/TVCG.2023.3314763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of computing topological distance between two scalar fields based on Reeb graphs or contour trees has been studied and applied successfully to various problems in topological shape matching, data analysis, and visualization. However, generalizing such results for computing distance measures between two multi-fields based on their Reeb spaces is still in its infancy. Towards this, in the current article we propose a technique to compute an effective distance measure between two multi-fields by computing a novel multi-dimensional persistence diagram (MDPD) corresponding to each of the (quantized) Reeb spaces. First, we construct a multi-dimensional Reeb graph (MDRG), which is a hierarchical decomposition of the Reeb space into a collection of Reeb graphs. The MDPD corresponding to each MDRG is then computed based on the persistence diagrams of the component Reeb graphs of the MDRG. Our distance measure extends the Wasserstein distance between two persistence diagrams of Reeb graphs to MDPDs of MDRGs. We prove that the proposed measure is a pseudo-metric and satisfies a stability property. Effectiveness of the proposed distance measure has been demonstrated in (i) shape retrieval contest data - SHREC 2010 and (ii) Pt-CO bond detection data from computational chemistry. Experimental results show that the proposed distance measure based on the Reeb spaces has more discriminating power in clustering the shapes and detecting the formation of a stable Pt-CO bond as compared to the similar measures between Reeb graphs.},
  archive      = {J_TVCG},
  author       = {Yashwanth Ramamurthi and Amit Chattopadhyay},
  doi          = {10.1109/TVCG.2023.3314763},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {5939-5952},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A topological distance between multi-fields based on multi-dimensional persistence diagrams},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AdaVis: Adaptive and explainable visualization
recommendation for tabular data. <em>TVCG</em>, <em>30</em>(9),
5923–5938. (<a href="https://doi.org/10.1109/TVCG.2023.3316469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated visualization recommendation facilitates the rapid creation of effective visualizations, which is especially beneficial for users with limited time and limited knowledge of data visualization. There is an increasing trend in leveraging machine learning (ML) techniques to achieve an end-to-end visualization recommendation. However, existing ML-based approaches implicitly assume that there is only one appropriate visualization for a specific dataset, which is often not true for real applications. Also, they often work like a black box, and are difficult for users to understand the reasons for recommending specific visualizations. To fill the research gap, we propose AdaVis , an adaptive and explainable approach to recommend one or multiple appropriate visualizations for a tabular dataset. It leverages a box embedding-based knowledge graph to well model the possible one-to-many mapping relations among different entities (i.e., data features, dataset columns, datasets, and visualization choices). The embeddings of the entities and relations can be learned from dataset-visualization pairs. Also, AdaVis incorporates the attention mechanism into the inference framework. Attention can indicate the relative importance of data features for a dataset and provide fine-grained explainability. Our extensive evaluations through quantitative metric evaluations, case studies, and user interviews demonstrate the effectiveness of AdaVis .},
  archive      = {J_TVCG},
  author       = {Songheng Zhang and Haotian Li and Huamin Qu and Yong Wang},
  doi          = {10.1109/TVCG.2023.3316469},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {5923-5938},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {AdaVis: Adaptive and explainable visualization recommendation for tabular data},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforced labels: Multi-agent deep reinforcement learning
for point-feature label placement. <em>TVCG</em>, <em>30</em>(9),
5908–5922. (<a href="https://doi.org/10.1109/TVCG.2023.3313729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the recent years, Reinforcement Learning combined with Deep Learning techniques has successfully proven to solve complex problems in various domains, including robotics, self-driving cars, and finance. In this article, we are introducing Reinforcement Learning (RL) to label placement , a complex task in data visualization that seeks optimal positioning for labels to avoid overlap and ensure legibility. Our novel point-feature label placement method utilizes Multi-Agent Deep Reinforcement Learning to learn the label placement strategy, the first machine-learning-driven labeling method, in contrast to the existing hand-crafted algorithms designed by human experts. To facilitate RL learning, we developed an environment where an agent acts as a proxy for a label, a short textual annotation that augments visualization. Our results show that the strategy trained by our method significantly outperforms the random strategy of an untrained agent and the compared methods designed by human experts in terms of completeness (i.e., the number of placed labels). The trade-off is increased computation time, making the proposed method slower than the compared methods. Nevertheless, our method is ideal for scenarios where the labeling can be computed in advance, and completeness is essential, such as cartographic maps, technical drawings, and medical atlases. Additionally, we conducted a user study to assess the perceived performance. The outcomes revealed that the participants considered the proposed method to be significantly better than the other examined methods. This indicates that the improved completeness is not just reflected in the quantitative metrics but also in the subjective evaluation by the participants.},
  archive      = {J_TVCG},
  author       = {Petr Bobák and Ladislav Čmolík and Martin Čadík},
  doi          = {10.1109/TVCG.2023.3313729},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  number       = {9},
  pages        = {5908-5922},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Reinforced labels: Multi-agent deep reinforcement learning for point-feature label placement},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A phenomenological approach to interactive knot diagrams.
<em>TVCG</em>, <em>30</em>(8), 5901–5907. (<a
href="https://doi.org/10.1109/TVCG.2024.3405369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knot diagrams are among the most common visual tools in topology. Computer programs now make it possible to draw, manipulate and render them digitally, which proves to be useful in knot theory teaching and research. Still, an openly available tool to manipulate knot diagrams in a real-time, interactive way is yet to be developed. We introduce a method of operating on the geometry of the knot diagram itself without any underlying three-dimensional structure that can underpin such an application. This allows us to directly interact with vector graphics knot diagrams while at the same time computing knot invariants in ways proposed by previous work. An implementation of this method is provided.},
  archive      = {J_TVCG},
  author       = {Lennart Finke and Edmund Weitz},
  doi          = {10.1109/TVCG.2024.3405369},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5901-5907},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A phenomenological approach to interactive knot diagrams},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling the intent to interact with VR using physiological
features. <em>TVCG</em>, <em>30</em>(8), 5893–5900. (<a
href="https://doi.org/10.1109/TVCG.2023.3308787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objective: Mixed-Reality (XR) technologies promise a user experience (UX) that rivals the interactive experience with the real-world. The key facilitators in the design of such a natural UX are that the interaction has zero lag and that users experience no excess mental load. This is difficult to achieve due to technical constraints such as motion-to-photon latency as well as false-positives during gesture-based interaction. Methods: In this paper, we explored the use of physiological features to model the user&#39;s intent to interact with a virtual reality (VR) environment. Accurate predictions about when users want to express an interaction intent could overcome the limitations of an interactive device that lags behind the intention of a user. We computed time-domain features from electroencephalography (EEG) and electromyography (EMG) recordings during a grab-and-drop task in VR and cross-validated a Linear Discriminant Analysis (LDA) for three different combinations of (1) EEG, (2) EMG and (3) EEG-EMG features. Results &amp;amp; Conclusion: We found the classifiers to detect the presence of a pre-movement state from background idle activity reflecting the users’ intent to interact with the virtual objects (EEG: 62 $\%$ $\pm$ 10 $\%$ , EMG: 72 $\%$ $\pm$ 9 $\%$ , EEG-EMG: 69 $\%$ $\pm$ 10 $\%$ ) above simulated chance level. The features leveraged in our classification scheme have a low computational cost and are especially useful for fast decoding of users’ mental states. Our work is a further step towards a useful classification of users’ intent to interact, as a high temporal resolution and speed of detection is crucial. This facilitates natural experiences through zero-lag adaptive interfaces.},
  archive      = {J_TVCG},
  author       = {Willy Nguyen and Klaus Gramann and Lukas Gehrke},
  doi          = {10.1109/TVCG.2023.3308787},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5893-5900},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Modeling the intent to interact with VR using physiological features},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TTK is getting MPI-ready. <em>TVCG</em>, <em>30</em>(8),
5875–5892. (<a href="https://doi.org/10.1109/TVCG.2024.3390219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This system paper documents the technical foundations for the extension of the Topology ToolKit (TTK) to distributed-memory parallelism with the Message Passing Interface (MPI). While several recent papers introduced topology-based approaches for distributed-memory environments, these were reporting experiments obtained with tailored, mono-algorithm implementations. In contrast, we describe in this paper a versatile approach (supporting both triangulated domains and regular grids) for the support of topological analysis pipelines , i.e., a sequence of topological algorithms interacting together, possibly on distinct numbers of processes. While developing this extension, we faced several algorithmic and software engineering challenges, which we document in this paper. Specifically, we describe an MPI extension of TTK’s data structure for triangulation representation and traversal, a central component to the global performance and generality of TTK’s topological implementations. We also introduce an intermediate interface between TTK and MPI, both at the global pipeline level, and at the fine-grain algorithmic level. We provide a taxonomy for the distributed-memory topological algorithms supported by TTK, depending on their communication needs and provide examples of hybrid MPI+thread parallelizations. Detailed performance analyses show that parallel efficiencies range from 20% to 80% (depending on the algorithms), and that the MPI-specific preconditioning introduced by our framework induces a negligible computation time overhead. We illustrate the new distributed-memory capabilities of TTK with an example of advanced analysis pipeline, combining multiple algorithms, run on the largest publicly available dataset we have found (120 billion vertices) on a standard cluster with 64 nodes (for a total of 1536 cores). Finally, we provide a roadmap for the completion of TTK’s MPI extension, along with generic recommendations for each algorithm communication category.},
  archive      = {J_TVCG},
  author       = {Eve Le Guillou and Michael Will and Pierre Guillou and Jonas Lukasczyk and Pierre Fortin and Christoph Garth and Julien Tierny},
  doi          = {10.1109/TVCG.2024.3390219},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5875-5892},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TTK is getting MPI-ready},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging tendon vibration to enhance pseudo-haptic
perceptions in VR. <em>TVCG</em>, <em>30</em>(8), 5861–5874. (<a
href="https://doi.org/10.1109/TVCG.2023.3310001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pseudo-haptic techniques are used to modify haptic perception by appropriately changing visual feedback to body movements. Based on the knowledge that tendon vibration can affect our somatosensory perception, this article proposes a method for leveraging tendon vibration to enhance pseudo-haptics during free arm motion. Three experiments were performed to examine the impact of tendon vibration on the range and resolution of pseudo-haptics. The first experiment investigated the effect of tendon vibration on the detection threshold of the discrepancy between visual and physical motion. The results indicated that vibrations applied to the inner tendons of the wrist and elbow increased the threshold, suggesting that tendon vibration can augment the applicable visual motion gain by approximately 13% without users detecting the visual/physical discrepancy. Furthermore, the results demonstrate that tendon vibration acts as noise on haptic motion cues. The second experiment assessed the impact of tendon vibration on the resolution of pseudo-haptics by determining the just noticeable difference in pseudo-weight perception. The results suggested that the tendon vibration does not largely compromise the resolution of pseudo-haptics. The third experiment evaluated the equivalence between the weight perception triggered by tendon vibration and that by visual motion gain, that is, the point of subjective equality. The results revealed that vibration amplifies the weight perception and its effect was equivalent to that obtained using a gain of 0.64 without vibration, implying that the tendon vibration also functions as an additional haptic cue. Our results provide design guidelines and future work for enhancing pseudo-haptics with tendon vibration.},
  archive      = {J_TVCG},
  author       = {Yutaro Hirao and Tomohiro Amemiya and Takuji Narumi and Ferran Argelaguet and Anatole Lécuyer},
  doi          = {10.1109/TVCG.2023.3310001},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5861-5874},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Leveraging tendon vibration to enhance pseudo-haptic perceptions in VR},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On network structural and temporal encodings: A space and
time odyssey. <em>TVCG</em>, <em>30</em>(8), 5847–5860. (<a
href="https://doi.org/10.1109/TVCG.2023.3310019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dynamic network visualization design space consists of two major dimensions: network structural and temporal representation. As more techniques are developed and published, a clear need for evaluation and experimental comparisons between them emerges. Most studies explore the temporal dimension and diverse interaction techniques supporting the participants, focusing on a single structural representation. Empirical evidence about performance and preference for different visualization approaches is scattered over different studies, experimental settings, and tasks. This paper aims to comprehensively investigate the dynamic network visualization design space in two evaluations. First, a controlled study assessing participants’ response times, accuracy, and preferences for different combinations of network structural and temporal representations on typical dynamic network exploration tasks, with and without the support of standard interaction methods. Second, the best-performing combinations from the first study are enhanced based on participants’ feedback and evaluated in a heuristic-based qualitative study with visualization experts on a real-world network. Our results highlight node-link with animation and playback controls as the best-performing combination and the most preferred based on ratings. Matrices achieve similar performance to node-link in the first study but have considerably lower scores in our second evaluation. Similarly, juxtaposition exhibits evident scalability issues in more realistic analysis contexts.},
  archive      = {J_TVCG},
  author       = {Velitchko Filipov and Alessio Arleo and Markus Bögl and Silvia Miksch},
  doi          = {10.1109/TVCG.2023.3310019},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5847-5860},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {On network structural and temporal encodings: A space and time odyssey},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Text entry performance and situation awareness of a joint
optical see-through head-mounted display and smartphone system.
<em>TVCG</em>, <em>30</em>(8), 5830–5846. (<a
href="https://doi.org/10.1109/TVCG.2023.3309316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical see-through head-mounted displays (OST HMDs) are a popular output medium for mobile Augmented Reality (AR) applications. To date, they lack efficient text entry techniques. Smartphones are a major text entry medium in mobile contexts but attentional demands can contribute to accidents while typing on the go. Mobile multi-display ecologies, such as combined OST HMD-smartphone systems, promise performance and situation awareness benefits over single-device use. We study the joint performance of text entry on mobile phones with text output on optical see-through head-mounted displays. A series of five experiments with a total of 86 participants indicate that, as of today, the challenges in such a joint interactive system outweigh the potential benefits.},
  archive      = {J_TVCG},
  author       = {Jens Grubert and Lukas Witzani and Alexander Otte and Travis Gesslein and Matthias Kranz and Per Ola Kristensson},
  doi          = {10.1109/TVCG.2023.3309316},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5830-5846},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Text entry performance and situation awareness of a joint optical see-through head-mounted display and smartphone system},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning approaches for 3D motion synthesis and
musculoskeletal dynamics estimation: A survey. <em>TVCG</em>,
<em>30</em>(8), 5810–5829. (<a
href="https://doi.org/10.1109/TVCG.2023.3308753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inference of 3D motion and dynamics of the human musculoskeletal system has traditionally been solved using physics-based methods that exploit physical parameters to provide realistic simulations. Yet, such methods suffer from computational complexity and reduced stability, hindering their use in computer graphics applications that require real-time performance. With the recent explosion of data capture (mocap, video) machine learning (ML) has started to become popular as it is able to create surrogate models harnessing the huge amount of data stemming from various sources, minimizing computational time (instead of resource usage), and most importantly, approximate real-time solutions. The main purpose of this paper is to provide a review and classification of the most recent works regarding motion prediction, motion synthesis as well as musculoskeletal dynamics estimation problems using ML techniques, in order to offer sufficient insight into the state-of-the-art and draw new research directions. While the study of motion may appear distinct to musculoskeletal dynamics, these application domains provide jointly the link for more natural computer graphics character animation, since ML-based musculoskeletal dynamics estimation enables modeling of more long-term, temporally evolving, ergonomic effects, while offering automated and fast solutions. Overall, our review offers an in-depth presentation and classification of ML applications in human motion analysis, unlike previous survey articles focusing on specific aspects of motion prediction.},
  archive      = {J_TVCG},
  author       = {Iliana Loi and Evangelia I. Zacharaki and Konstantinos Moustakas},
  doi          = {10.1109/TVCG.2023.3308753},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5810-5829},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Machine learning approaches for 3D motion synthesis and musculoskeletal dynamics estimation: A survey},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DeepTree: Modeling trees with situated latents.
<em>TVCG</em>, <em>30</em>(8), 5795–5809. (<a
href="https://doi.org/10.1109/TVCG.2023.3307887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose DeepTree , a novel method for modeling trees based on learning developmental rules for branching structures instead of manually defining them. We call our deep neural model “situated latent” because its behavior is determined by the intrinsic state -encoded as a latent space of a deep neural model- and by the extrinsic (environmental) data that is “situated” as the location in the 3D space and on the tree structure. We use a neural network pipeline to train a situated latent space that allows us to locally predict branch growth only based on a single node in the branch graph of a tree model. We use this representation to progressively develop new branch nodes, thereby mimicking the growth process of trees. Starting from a root node, a tree is generated by iteratively querying the neural network on the newly added nodes resulting in the branching structure of the whole tree. Our method enables generating a wide variety of tree shapes without the need to define intricate parameters that control their growth and behavior. Furthermore, we show that the situated latents can also be used to encode the environmental response of tree models, e.g., when trees grow next to obstacles. We validate the effectiveness of our method by measuring the similarity of our tree models and by procedurally generated ones based on a number of established metrics for tree form.},
  archive      = {J_TVCG},
  author       = {Xiaochen Zhou and Bosheng Li and Bedrich Benes and Songlin Fei and Sören Pirk},
  doi          = {10.1109/TVCG.2023.3307887},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5795-5809},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DeepTree: Modeling trees with situated latents},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring visual-auditory redirected walking using auditory
cues in reality. <em>TVCG</em>, <em>30</em>(8), 5782–5794. (<a
href="https://doi.org/10.1109/TVCG.2023.3309267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine the effect of auditory cues occurring in reality on redirection. Specifically, we set two hypotheses: the auditory cues emanating from fixed positions in reality (Fixed sound, FS) increase the noticeability of redirection, while the auditory cues whose positions are manipulated consistently with the visual manipulation (Redirected sound, RDS) decrease the noticeability of redirection. To verify these hypotheses, we implemented an experimental environment that virtually reproduced FS and RDS conditions using binaural recording, and then we conducted a user study ( $N=18$ ) to investigate the detection thresholds (DTs) for rotational manipulation and the sound localization accuracy of the auditory cues under FS and RDS, as well as the baseline condition without auditory cues (No sound, NS). The results show, against the hypotheses, FS gave a wider range of DTs than NS, while RDS gave a similar range of DTs to NS. Combining these results with those of sound localization accuracy reveals that, rather than the auditory cues affecting the participants’ spatial perception in VR, the visual manipulation made their sound localization less accurate, which would be a reason for the increased range of DTs under FS. Furthermore, we conducted a follow-up user study ( $N=11$ ) to measure the sound localization accuracy of FS where the auditory cues were actually placed in a real setting, and we found that the accuracy tended to be similar to that of virtually reproduced FS, suggesting the validity of the auditory cues used in this study. Given these findings, we also discuss potential applications.},
  archive      = {J_TVCG},
  author       = {Kumpei Ogawa and Kazuyuki Fujita and Shuichi Sakamoto and Kazuki Takashima and Yoshifumi Kitamura},
  doi          = {10.1109/TVCG.2023.3309267},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5782-5794},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring visual-auditory redirected walking using auditory cues in reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LoCoMoTe – a framework for classification of natural
locomotion in VR by task, technique and modality. <em>TVCG</em>,
<em>30</em>(8), 5765–5781. (<a
href="https://doi.org/10.1109/TVCG.2023.3313439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) research has provided overviews of locomotion techniques, how they work, their strengths and overall user experience. Considerable research has investigated new methodologies, particularly machine learning to develop redirection algorithms. To best support the development of redirection algorithms through machine learning, we must understand how best to replicate human navigation and behaviour in VR, which can be supported by the accumulation of results produced through live-user experiments. However, it can be difficult to identify, select and compare relevant research without a pre-existing framework in an ever-growing research field. Therefore, this work aimed to facilitate the ongoing structuring and comparison of the VR-based natural walking literature by providing a standardised framework for researchers to utilise. We applied thematic analysis to study methodology descriptions from 140 VR-based papers that contained live-user experiments. From this analysis, we developed the LoCoMoTe framework with three themes: navigational decisions, technique implementation, and modalities. The LoCoMoTe framework provides a standardised approach to structuring and comparing experimental conditions. The framework should be continually updated to categorise and systematise knowledge and aid in identifying research gaps and discussions.},
  archive      = {J_TVCG},
  author       = {Charlotte Croucher and Wendy Powell and Brett Stevens and Matt Miller-Dicks and Vaughan Powell and Travis J. Wiltshire and Pieter Spronck},
  doi          = {10.1109/TVCG.2023.3313439},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5765-5781},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LoCoMoTe – a framework for classification of natural locomotion in VR by task, technique and modality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TraInterSim: Adaptive and planning-aware hybrid-driven
traffic intersection simulation. <em>TVCG</em>, <em>30</em>(8),
5750–5764. (<a href="https://doi.org/10.1109/TVCG.2023.3307882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic intersections are important scenes that can be seen almost everywhere in the traffic system. Currently, most simulation methods perform well at highways and urban traffic networks. In intersection scenarios, the challenge lies in the lack of clearly defined lanes, where agents with various motion plannings converge in the central area from different directions. Traditional model-based methods are difficult to drive agents to move realistically at intersections without enough predefined lanes, while data-driven methods often require a large amount of high-quality input data. Simultaneously, tedious parameter tuning is inevitable involved to obtain the desired simulation results. In this paper, we present a novel adaptive and planning-aware hybrid-driven method (TraInterSim) to simulate traffic intersection scenarios. Our hybrid-driven method combines an optimization-based data-driven scheme with a velocity continuity model. It guides the agent&#39;s movements using real-world data and can generate those behaviors not present in the input data. Our optimization method fully considers velocity continuity, desired speed, direction guidance, and planning-aware collision avoidance. Agents can perceive others’ motion plannings and relative distances to avoid possible collisions. To preserve the individual flexibility of different agents, the parameters in our method are automatically adjusted during the simulation. TraInterSim can generate realistic behaviors of heterogeneous agents in different traffic intersection scenarios in interactive rates. Through extensive experiments as well as user studies, we validate the effectiveness and rationality of the proposed simulation method.},
  archive      = {J_TVCG},
  author       = {Pei Lv and Xinming Pei and Xinyu Ren and Yuzhen Zhang and Chaochao Li and Mingliang Xu},
  doi          = {10.1109/TVCG.2023.3307882},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5750-5764},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TraInterSim: Adaptive and planning-aware hybrid-driven traffic intersection simulation},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A grid-based method for removing overlaps of dimensionality
reduction scatterplot layouts. <em>TVCG</em>, <em>30</em>(8), 5733–5749.
(<a href="https://doi.org/10.1109/TVCG.2023.3309941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality Reduction (DR) scatterplot layouts have become a ubiquitous visualization tool for analyzing multidimensional datasets. Despite their popularity, such scatterplots suffer from occlusion, especially when informative glyphs are used to represent data instances, potentially obfuscating critical information for the analysis under execution. Different strategies have been devised to address this issue, either producing overlap-free layouts that lack the powerful capabilities of contemporary DR techniques in uncovering interesting data patterns or eliminating overlaps as a post-processing strategy. Despite the good results of post-processing techniques, most of the best methods typically expand or distort the scatterplot area, thus reducing glyphs’ size (sometimes) to unreadable dimensions, defeating the purpose of removing overlaps. This article presents Distance Grid (DGrid) , a novel post-processing strategy to remove overlaps from DR layouts that faithfully preserves the original layout&#39;s characteristics and bounds the minimum glyph sizes. We show that DGrid surpasses the state-of-the-art in overlap removal (through an extensive comparative evaluation considering multiple different metrics) while also being one of the fastest techniques, especially for large datasets. A user study with 51 participants also shows that DGrid is consistently ranked among the top techniques for preserving the original scatterplots’ visual characteristics and the aesthetics of the final results.},
  archive      = {J_TVCG},
  author       = {Gladys M. Hilasaca and Wilson E. Marcílio-Jr and Danilo M. Eler and Rafael M. Martins and Fernando V. Paulovich},
  doi          = {10.1109/TVCG.2023.3309941},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5733-5749},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A grid-based method for removing overlaps of dimensionality reduction scatterplot layouts},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural novel actor: Learning a generalized animatable neural
representation for human actors. <em>TVCG</em>, <em>30</em>(8),
5719–5732. (<a href="https://doi.org/10.1109/TVCG.2023.3305433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new method for learning a generalized animatable neural human representation from a sparse set of multi-view imagery of multiple persons. The learned representation can be used to synthesize novel view images of an arbitrary person and further animate them with the user&#39;s pose control. While most existing methods can either generalize to new persons or synthesize animations with user control, none of them can achieve both at the same time. We attribute this accomplishment to the employment of a 3D proxy for a shared multi-person human model, and further the warping of the spaces of different poses to a shared canonical pose space, in which we learn a neural field and predict the person- and pose-dependent deformations, as well as appearance with the features extracted from input images. To cope with the complexity of the large variations in body shapes, poses, and clothing deformations, we design our neural human model with disentangled geometry and appearance. Furthermore, we utilize the image features both at the spatial point and on the surface points of the 3D proxy for predicting person- and pose-dependent properties. Experiments show that our method significantly outperforms the state-of-the-arts on both tasks.},
  archive      = {J_TVCG},
  author       = {Qingzhe Gao and Yiming Wang and Libin Liu and Lingjie Liu and Christian Theobalt and Baoquan Chen},
  doi          = {10.1109/TVCG.2023.3305433},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5719-5732},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Neural novel actor: Learning a generalized animatable neural representation for human actors},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synthesize personalized training for robot-assisted upper
limb rehabilitation with diversity enhancement. <em>TVCG</em>,
<em>30</em>(8), 5705–5718. (<a
href="https://doi.org/10.1109/TVCG.2023.3308940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For upper limb rehabilitation, the robot-assisted technique in combination with serious games requires well-specified training plans. For the best quality of the rehabilitation process, customized game levels for each user are desired, while it is labor-intensive to design and adjust game levels for different individuals. We work on generating training content for a desktop end-effector rehabilitation robot and propose a method to automatically generate individualized training plans. By modeling the search of the training motions as finding optimal hand paths and trajectories, we introduce solving the design problem with a multi-objective optimization (MO) solver. We further improve the MO solver to enhance the diversity of the solutions. With the proposed approach, our system is capable of automatically generating various training plans considering the training intensity and dexterity of each joint in the upper limb. In addition, the enhanced diversity avoids repeated training plans, which helps motivate the user in the rehabilitation. We test our method with different requirements on the training plans and validate the solutions.},
  archive      = {J_TVCG},
  author       = {Yuting Fan and Lifeng Zhu and Hui Wang and Aiguo Song},
  doi          = {10.1109/TVCG.2023.3308940},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5705-5718},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Synthesize personalized training for robot-assisted upper limb rehabilitation with diversity enhancement},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Motion in-betweening via deep <span
class="math inline"><em>Δ</em></span>δ-interpolator. <em>TVCG</em>,
<em>30</em>(8), 5693–5704. (<a
href="https://doi.org/10.1109/TVCG.2023.3309107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that the task of synthesizing human motion conditioned on a set of key frames can be solved more accurately and effectively if a deep learning based interpolator operates in the delta mode using the spherical linear interpolator as a baseline. We empirically demonstrate the strength of our approach on publicly available datasets achieving state-of-the-art performance. We further generalize these results by showing that the $\Delta$ -regime is viable with respect to the reference of the last known frame (also known as the zero-velocity model). This supports the more general conclusion that operating in the reference frame local to input frames is more accurate and robust than in the global (world) reference frame advocated in previous work.},
  archive      = {J_TVCG},
  author       = {Boris N. Oreshkin and Antonios Valkanas and Félix G. Harvey and Louis-Simon Ménard and Florent Bocquelet and Mark J. Coates},
  doi          = {10.1109/TVCG.2023.3309107},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5693-5704},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Motion in-betweening via deep $\Delta$Δ-interpolator},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IntrinsicNGP: Intrinsic coordinate based hash encoding for
human NeRF. <em>TVCG</em>, <em>30</em>(8), 5679–5692. (<a
href="https://doi.org/10.1109/TVCG.2023.3306078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, many works have been proposed to use the neural radiance field for novel view synthesis of human performers. However, most of these methods require hours of training, making them difficult for practical use. To address this challenging problem, we propose IntrinsicNGP, which can be trained from scratch and achieve high-fidelity results in a few minutes with videos of a human performer. To achieve this goal, we introduce a continuous and optimizable intrinsic coordinate instead of the original explicit euclidean coordinate in the hash encoding module of InstantNGP. With this novel intrinsic coordinate, IntrinsicNGP can aggregate interframe information for dynamic objects using proxy geometry shapes. Moreover, the results trained with the given rough geometry shapes can be further refined with an optimizable offset field based on the intrinsic coordinate. Extensive experimental results on several datasets demonstrate the effectiveness and efficiency of IntrinsicNGP. We also illustrate the ability of our approach to edit the shape of reconstructed objects.},
  archive      = {J_TVCG},
  author       = {Bo Peng and Jun Hu and Jingtao Zhou and Xuan Gao and Juyong Zhang},
  doi          = {10.1109/TVCG.2023.3306078},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5679-5692},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {IntrinsicNGP: Intrinsic coordinate based hash encoding for human NeRF},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SmartGD: A GAN-based graph drawing framework for diverse
aesthetic goals. <em>TVCG</em>, <em>30</em>(8), 5666–5678. (<a
href="https://doi.org/10.1109/TVCG.2023.3306356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While a multitude of studies have been conducted on graph drawing, many existing methods only focus on optimizing a single aesthetic aspect of graph layouts, which can lead to sub-optimal results. There are a few existing methods that have attempted to develop a flexible solution for optimizing different aesthetic aspects measured by different aesthetic criteria. Furthermore, thanks to the significant advance in deep learning techniques, several deep learning-based layout methods were proposed recently. These methods have demonstrated the advantages of deep learning approaches for graph drawing. However, none of these existing methods can be directly applied to optimizing non-differentiable criteria without special accommodation. In this work, we propose a novel Generative Adversarial Network (GAN) based deep learning framework for graph drawing, called SmartGD, which can optimize different quantitative aesthetic goals, regardless of their differentiability. To demonstrate the effectiveness and efficiency of SmartGD, we conducted experiments on minimizing stress, minimizing edge crossing, maximizing crossing angle, maximizing shape-based metrics, and a combination of multiple aesthetics. Compared with several popular graph drawing algorithms, the experimental results show that SmartGD achieves good performance both quantitatively and qualitatively.},
  archive      = {J_TVCG},
  author       = {Xiaoqi Wang and Kevin Yen and Yifan Hu and Han-Wei Shen},
  doi          = {10.1109/TVCG.2023.3306356},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5666-5678},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SmartGD: A GAN-based graph drawing framework for diverse aesthetic goals},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ProactiV: Studying deep learning model behavior under input
transformations. <em>TVCG</em>, <em>30</em>(8), 5651–5665. (<a
href="https://doi.org/10.1109/TVCG.2023.3301722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) models have shown performance benefits across many applications, from classification to image-to-image translation. However, low interpretability often leads to unexpected model behavior once deployed in the real world. Usually, this unexpected behavior is because the training data domain does not reflect the deployment data domain. Identifying a model&#39;s breaking points under input conditions and domain shifts, i.e., input transformations, is essential to improve models. Although visual analytics (VA) has shown promise in studying the behavior of model outputs under continually varying inputs, existing methods mainly focus on per-class or instance-level analysis. We aim to generalize beyond classification where classes do not exist and provide a global view of model behavior under co-occurring input transformations. We present a DL model-agnostic VA method (ProactiV) to help model developers proactively study output behavior under input transformations to identify and verify breaking points. ProactiV relies on a proposed input optimization method to determine the changes to a given transformed input to achieve the desired output. The data from this optimization process allows the study of global and local model behavior under input transformations at scale. Additionally, the optimization method provides insights into the input characteristics that result in desired outputs and helps recognize model biases. We highlight how ProactiV effectively supports studying model behavior with example classification and image-to-image translation tasks.},
  archive      = {J_TVCG},
  author       = {Vidya Prasad and Ruud J. G. van Sloun and Anna Vilanova and Nicola Pezzotti},
  doi          = {10.1109/TVCG.2023.3301722},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5651-5665},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ProactiV: Studying deep learning model behavior under input transformations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). This is the table i want! Interactive data transformation on
desktop and in virtual reality. <em>TVCG</em>, <em>30</em>(8),
5635–5650. (<a href="https://doi.org/10.1109/TVCG.2023.3299602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data transformation is an essential step in data science. While experts primarily use programming to transform their data, there is an increasing need to support non-programmers with user interface-based tools. With the rapid development in interaction techniques and computing environments, we report our empirical findings about the effects of interaction techniques and environments on performing data transformation tasks. Specifically, we studied the potential benefits of direct interaction and virtual reality (VR) for data transformation. We compared gesture interaction versus a standard WIMP user interface, each on the desktop and in VR. With the tested data and tasks, we found time performance was similar between desktop and VR. Meanwhile, VR demonstrates preliminary evidence to better support provenance and sense-making throughout the data transformation process. Our exploration of performing data transformation in VR also provides initial affirmation for enabling an iterative and fully immersive data science workflow.},
  archive      = {J_TVCG},
  author       = {Sungwon In and Tica Lin and Chris North and Hanspeter Pfister and Yalong Yang},
  doi          = {10.1109/TVCG.2023.3299602},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5635-5650},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {This is the table i want! interactive data transformation on desktop and in virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stylizing ribbons: Computing surface contours with
temporally coherent orientations. <em>TVCG</em>, <em>30</em>(8),
5623–5634. (<a href="https://doi.org/10.1109/TVCG.2023.3304641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Line work is a core element for the stylization of computer animations used by recent shows. However, existing stylization techniques are limited to edge treatments based on brush strokes or textures applied solely on top of curves. In this work, we propose new stylization effects by offering artists direct control over the inside and outside of surface contours. To this end, we introduce a method that creates ribbons , geometry strips of possibly varying width, that extrude from each side of the surface contour with temporally coherent orientations. Our contributions include the generation of spatially and temporally consistent normal orientations along visible contours and a trimming routine that converts arrangements of offset curves into ribbons free of intersections. We demonstrate the expressiveness and versatility of stylized ribbons by applying various effects on both character and shadow edges from animation sequences.},
  archive      = {J_TVCG},
  author       = {Nora S Willett and Fernando de Goes and Kurt Fleischer and Mark Meyer and Chris Burrows},
  doi          = {10.1109/TVCG.2023.3304641},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5623-5634},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Stylizing ribbons: Computing surface contours with temporally coherent orientations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ViewR: Architectural-scale multi-user mixed reality with
mobile head-mounted displays. <em>TVCG</em>, <em>30</em>(8), 5609–5622.
(<a href="https://doi.org/10.1109/TVCG.2023.3299781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of mobile head-mounted displays with robust “inside-out” markerless tracking and video-passthrough permits the creation of novel mixed reality (MR) experiences in which architectural spaces of arbitrary size can be transformed into immersive multi-user visualisation arenas. Here we outline ViewR, an open-source framework for rapidly constructing and deploying architectural-scale multi-user MR experiences. ViewR includes tools for rapid alignment of real and virtual worlds, tracking loss detection and recovery, user trajectory visualisation and world state synchronisation between users with persistence across sessions. ViewR also provides control over the blending of the real and the virtual, specification of site-specific blending zones, and video-passthrough avatars, allowing users to see and interact with one another directly. Using ViewR, we explore the transformation of large architectural structures into immersive arenas by creating a range of experiences in various locations, with a particular focus on architectural affordances such as mezzanines, stairs, gangways and elevators. Our tests reveal that ViewR allows for experiences that would not be possible with pure virtual reality, and indicate that, with certain strategies for recovering from tracking errors, it is possible to construct large scale multi-user MR experiences using contemporary consumer virtual reality head-mounted displays.},
  archive      = {J_TVCG},
  author       = {Florian Schier and Daniel Zeidler and Krishnan Chandran and Zhongyuan Yu and Matthew McGinity},
  doi          = {10.1109/TVCG.2023.3299781},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5609-5622},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ViewR: Architectural-scale multi-user mixed reality with mobile head-mounted displays},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Augmented incremental potential contact for sticky
interactions. <em>TVCG</em>, <em>30</em>(8), 5596–5608. (<a
href="https://doi.org/10.1109/TVCG.2023.3295656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a variational formulation for simulating sticky interactions between elastoplastic solids. Our method brings a wider range of material behaviors into the reach of the Incremental Potential Contact (IPC) solver recently developed by (Li et al. 2020). Extending IPC requires several contributions. We first augment IPC with the classical Raous-Cangemi-Cocou (RCC) adhesion model. This allows us to robustly simulate the sticky interactions between arbitrary codimensional-0, 1, and 2 geometries. To enable user-friendly practical adoptions of our method, we further introduce a physically parametrized, easily controllable normal adhesion formulation based on the unsigned distance , which is fully compatible with IPC&#39;s barrier formulation. Furthermore, we propose a smoothly clamped tangential adhesion model that naturally models intricate behaviors including debonding. Lastly, we perform benchmark studies comparing our method with the classical models as well as real-world experimental results to demonstrate the efficacy of our method.},
  archive      = {J_TVCG},
  author       = {Yu Fang and Minchen Li and Yadi Cao and Xuan Li and Joshuah Wolper and Yin Yang and Chenfanfu Jiang},
  doi          = {10.1109/TVCG.2023.3295656},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5596-5608},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Augmented incremental potential contact for sticky interactions},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Building coarse to fine convex hulls with auxiliary vertices
for palette-based image recoloring. <em>TVCG</em>, <em>30</em>(8),
5581–5595. (<a href="https://doi.org/10.1109/TVCG.2023.3296386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constructing a convex hull for the pixel colors of an image by viewing them as 3D points can extract a set of palette colors for the image, then image recoloring can be achieved by modifying the palette colors. For better recoloring effect, the convex hull should contain more pixels (inclusive) and be more compact. Otherwise, reconstruction error would occur or the extracted palette color would be less representative, yielding wrong recoloring results or less effective edit. We observe that convex hulls constructed by prior methods can contain all the image pixels, but are far from compact. Efforts have been made to optimize the vertices of convex hull to increase the compactness but are still not perfect. In this paper, we propose a novel coarse to fine convex hull construction scheme with auxiliary vertices. We start by constructing a coarse convex hull whose vertices are directly image pixels which is thus the most compact but cannot contain all pixels. We then make a remedy by adding auxiliary vertices into the coarse convex hull to obtain a fine convex hull. More auxiliary vertices are added, more image pixels will be contained into the fine convex hull. The auxiliary vertices are image pixels too so that the compactness can still be maintained. During editing, the auxiliary vertices are not allowed to be edited for edit convenience, but deformed as-rigid-as-possible with the adjusting of other vertices. Our convex hull is both inclusive and compact. Extensive experiments validate the effectiveness of the proposed method.},
  archive      = {J_TVCG},
  author       = {Qiwei Sun and Yongwei Nie and Qing Zhang and Guiqing Li},
  doi          = {10.1109/TVCG.2023.3296386},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5581-5595},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Building coarse to fine convex hulls with auxiliary vertices for palette-based image recoloring},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Age and gender differences in the pseudo-haptic effect on
computer mouse operation in a desktop environment. <em>TVCG</em>,
<em>30</em>(8), 5566–5580. (<a
href="https://doi.org/10.1109/TVCG.2023.3295389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pseudo-haptics is a method that can provide a haptic sensation without requiring a physical haptic device. The effect of pseudo-haptics is known to depend on the individual, but it is unclear which factors cause individual differences. As the first study establishing a calibration method for these differences in future research, we examined the differences in the pseudo-haptic effect on mouse cursor operation in a desktop environment depending on the age and gender of the user. We conducted an online experiment and collected data from more than 400 participants. The participants performed a task of lifting a virtual object with a mouse pointer. We found that the effect of pseudo-haptics was greater in younger or male participants than in older or female participants. We also found that the effect of pseudo-haptics, which varied with age and gender, can be explained by habituation to the mouse in daily life and the accuracy of detecting the pointer position using vision or proprioception. Specifically, the pseudo-haptic effect was higher for those who used the mouse more frequently and had higher accuracy in identifying the pointer position using proprioception or vision. The results of the present study not only indicate the factors that cause age and gender differences but also provide hints for calibrating these differences.},
  archive      = {J_TVCG},
  author       = {Yuki Ban and Yusuke Ujitoko},
  doi          = {10.1109/TVCG.2023.3295389},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5566-5580},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Age and gender differences in the pseudo-haptic effect on computer mouse operation in a desktop environment},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supervertex sampling network: A geodesic differential SLIC
approach for 3D mesh. <em>TVCG</em>, <em>30</em>(8), 5553–5565. (<a
href="https://doi.org/10.1109/TVCG.2023.3294845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of 3D meshes with deep learning has become prevalent in computer graphics. As an essential structure, hierarchical representation is critical for mesh pooling in multiscale analysis. Existing clustering-based mesh hierarchy construction methods involve nonlinear discretization optimization operations, making them nondifferential and challenging to embed in other trainable networks for learning. Inspired by deep superpixel learning methods in image processing, we extend them from 2D images to 3D meshes by proposing a novel differentiable chart-based segmentation method named geodesic differential supervertex (GDSV). The key to the GDSV method is to ensure that the geodesic position updates are differentiable while satisfying the constraint that the renewed supervertices lie on the manifold surface. To this end, in addition to using the differential SLIC clustering algorithm to update the nonpositional features of the supervertices, a reparameterization trick, the Gumbel-Softmax trick, is employed to renew the geodesic positions of the supervertices. Therefore, the geodesic position update problem is converted into a linear matrix multiplication issue. The GDSV method can be an independent module for chart-based segmentation tasks. Meanwhile, it can be combined with the front-end feature learning network and the back-end task-specific network as a plug-in-plug-out module for training; and be applied to tasks such as shape classification, part segmentation, and 3D scene understanding. Experimental results show the excellent performance of our proposed algorithm on a range of datasets.},
  archive      = {J_TVCG},
  author       = {Jiafu Zhuang and Pan Zeng and Wei Zhuang and Xiaoyu Guo and Peizhong Liu},
  doi          = {10.1109/TVCG.2023.3294845},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5553-5565},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Supervertex sampling network: A geodesic differential SLIC approach for 3D mesh},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive color transfer from images to terrain
visualizations. <em>TVCG</em>, <em>30</em>(8), 5538–5552. (<a
href="https://doi.org/10.1109/TVCG.2023.3295122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Terrain mapping is not only dedicated to communicating how high or steep a landscape is but can also help to indicate how we feel about a place. However, crafting effective and expressive elevation colors is challenging for both nonexperts and experts. In this article, we present a two-step image-to-terrain color transfer method that can transfer color from arbitrary images to diverse terrain models. First, we present a new image color organization method that organizes discrete, irregular image colors into a continuous, regular color grid that facilitates a series of color operations, such as local and global searching, categorical color selection and sequential color interpolation. Second, we quantify a series of cartographic concerns about elevation color crafting, such as the “lower, higher” principle, color conventions, and aerial perspectives. We also define color similarity between images and terrain visualizations with aesthetic quality. We then mathematically formulate image-to-terrain color transfer as a dual-objective optimization problem and offer a heuristic searching method to solve the problem. Finally, we compare elevation colors from our method with a standard color scheme and a representative color scale generation tool based on four test terrains. The evaluations show that the elevation colors from the proposed method are most effective and that our results are visually favorable. We also showcase that our method can transfer emotion from images to terrain visualizations.},
  archive      = {J_TVCG},
  author       = {Mingguang Wu and Yanjie Sun and Shangjing Jiang},
  doi          = {10.1109/TVCG.2023.3295122},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5538-5552},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Adaptive color transfer from images to terrain visualizations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A collaborative, interactive and context-aware drawing agent
for co-creative design. <em>TVCG</em>, <em>30</em>(8), 5525–5537. (<a
href="https://doi.org/10.1109/TVCG.2023.3293853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in text-conditioned generative models have provided us with neural networks capable of creating images of astonishing quality, be they realistic, abstract, or even creative. These models have in common that (more or less explicitly) they all aim to produce a high-quality one-off output given certain conditions, and in that they are not well suited for a creative collaboration framework. Drawing on theories from cognitive science that model how professional designers and artists think, we argue how this setting differs from the former and introduce CICADA: a Collaborative, Interactive Context-Aware Drawing Agent. CICADA uses a vector-based synthesis-by-optimisation method to take a partial sketch (such as might be provided by a user) and develop it towards a goal by adding and/or sensibly modifying traces. Given that this topic has been scarcely explored, we also introduce a way to evaluate desired characteristics of a model in this context by means of proposing a diversity measure. CICADA is shown to produce sketches of quality comparable to a human user’s, enhanced diversity and most importantly to be able to cope with change by continuing the sketch minding the user&#39;s contributions in a flexible manner.},
  archive      = {J_TVCG},
  author       = {Francisco Ibarrola and Tomas Lawton and Kazjon Grace},
  doi          = {10.1109/TVCG.2023.3293853},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5525-5537},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A collaborative, interactive and context-aware drawing agent for co-creative design},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactive virtual ankle movement controlled by wrist sEMG
improves motor imagery: An exploratory study. <em>TVCG</em>,
<em>30</em>(8), 5507–5524. (<a
href="https://doi.org/10.1109/TVCG.2023.3294342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) techniques can significantly enhance motor imagery training by creating a strong illusion of action for central sensory stimulation. In this article, we establish a precedent by using surface electromyography (sEMG) of contralateral wrist movement to trigger virtual ankle movement through an improved data-driven approach with a continuous sEMG signal for fast and accurate intention recognition. Our developed VR interactive system can provide feedback training for stroke patients in the early stages, even if there is no active ankle movement. Our objectives are to evaluate: 1) the effects of VR immersion mode on body illusion, kinesthetic illusion, and motor imagery performance in stroke patients; 2) the effects of motivation and attention when utilizing wrist sEMG as a trigger signal for virtual ankle motion; 3) the acute effects on motor function in stroke patients. Through a series of well-designed experiments, we have found that, compared to the 2D condition, VR significantly increases the degree of kinesthetic illusion and body ownership of the patients, and improves their motor imagery performance and motor memory. When compared to conditions without feedback, using contralateral wrist sEMG signals as trigger signals for virtual ankle movement enhances patients’ sustained attention and motivation during repetitive tasks. Furthermore, the combination of VR and feedback has an acute impact on motor function. Our exploratory study suggests that the sEMG-based immersive virtual interactive feedback provides an effective option for active rehabilitation training for severe hemiplegia patients in the early stages, with great potential for clinical application.},
  archive      = {J_TVCG},
  author       = {Yanqing Xiao and Hongming Bai and Yang Gao and Ben Hu and Jia Zheng and XiaoE Cai and Jiasheng Rao and Xiaoguang Li and Aimin Hao},
  doi          = {10.1109/TVCG.2023.3294342},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5507-5524},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive virtual ankle movement controlled by wrist sEMG improves motor imagery: An exploratory study},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). To stick or not to stick? Studying the impact of offset
recovery techniques during mid-air interactions. <em>TVCG</em>,
<em>30</em>(8), 5493–5506. (<a
href="https://doi.org/10.1109/TVCG.2023.3295209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During mid-air interactions, common approaches (such as the god-object method) typically rely on visually constraining the user&#39;s avatar to avoid visual interpenetrations with the virtual environment in the absence of kinesthetic feedback. This paper explores two methods which influence how the position mismatch (positional offset) between users’ real and virtual hands is recovered when releasing the contact with virtual objects. The first method (sticky) constrains the user&#39;s virtual hand until the mismatch is recovered, while the second method (unsticky) employs an adaptive offset recovery method. In the first study, we explored the effect of positional offset and of motion alteration on users’ behavioral adjustments and users’ perception. In a second study, we evaluated variations in the sense of embodiment and the preference between the two control laws. Overall, both methods presented similar results in terms of performance and accuracy, yet, positional offsets strongly impacted motion profiles and users’ performance. Both methods also resulted in comparable levels of embodiment. Finally, participants usually expressed strong preferences toward one of the two methods, but these choices were individual-specific and did not appear to be correlated solely with characteristics external to the individuals. Taken together, these results highlight the relevance of exploring the customization of motion control algorithms for avatars.},
  archive      = {J_TVCG},
  author       = {Maé Mavromatis and Ludovic Hoyet and Anatole Lécuyer and Diane Dewez and Ferran Argelaguet},
  doi          = {10.1109/TVCG.2023.3295209},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5493-5506},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {To stick or not to stick? studying the impact of offset recovery techniques during mid-air interactions},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HVTR++: Image and pose driven human avatars using hybrid
volumetric-textural rendering. <em>TVCG</em>, <em>30</em>(8), 5478–5492.
(<a href="https://doi.org/10.1109/TVCG.2023.3297721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent neural rendering methods have made great progress in generating photorealistic human avatars. However, these methods are generally conditioned only on low-dimensional driving signals (e.g., body poses), which are insufficient to encode the complete appearance of a clothed human. Hence they fail to generate faithful details. To address this problem, we exploit driving view images (e.g., in telepresence systems) as additional inputs. We propose a novel neural rendering pipeline, Hybrid Volumetric-Textural Rendering (HVTR++), which synthesizes 3D human avatars from arbitrary driving poses and views while staying faithful to appearance details efficiently and at high quality. First, we learn to encode the driving signals of pose and view image on a dense UV manifold of the human body surface and extract UV-aligned features, preserving the structure of a skeleton-based parametric model. To handle complicated motions (e.g., self-occlusions), we then leverage the UV-aligned features to construct a 3D volumetric representation based on a dynamic neural radiance field. While this allows us to represent 3D geometry with changing topology, volumetric rendering is computationally heavy. Hence we employ only a rough volumetric representation using a pose- and image-conditioned downsampled neural radiance field (PID-NeRF), which we can render efficiently at low resolutions. In addition, we learn 2D textural features that are fused with rendered volumetric features in image space. The key advantage of our approach is that we can then convert the fused features into a high-resolution, high-quality avatar by a fast GAN-based textural renderer. We demonstrate that hybrid rendering enables HVTR++ to handle complicated motions, render high-quality avatars under user-controlled poses/shapes, and most importantly, be efficient at inference time. Our experimental results also demonstrate state-of-the-art quantitative results.},
  archive      = {J_TVCG},
  author       = {Tao Hu and Hongyi Xu and Linjie Luo and Tao Yu and Zerong Zheng and He Zhang and Yebin Liu and Matthias Zwicker},
  doi          = {10.1109/TVCG.2023.3297721},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5478-5492},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HVTR++: Image and pose driven human avatars using hybrid volumetric-textural rendering},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual illusion created by a striped pattern through
augmented reality for the prevention of tumbling on stairs.
<em>TVCG</em>, <em>30</em>(8), 5466–5477. (<a
href="https://doi.org/10.1109/TVCG.2023.3295425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fall on stairs can be a dangerous accident. An important indicator of falling risk is the foot clearance, which is the height of the foot when ascending stairs or the distance of the foot from the step when descending. We developed an augmented reality system with a holographic lens using a visual illusion to improve the foot clearance on stairs. The system draws a vertical striped pattern on the stair riser as the participant ascends the stairs to create the illusion that the steps are higher than the actual steps, and draws a horizontal striped pattern on the stair tread as the participant descends the stairs to create the illusion of narrower stairs. We experimentally evaluated the accuracy of the system and fitted a model to determine the appropriate stripe thickness. Finally, participants ascended and descended stairs before, during, and after using the augmented reality system. The foot clearance significantly improved, not only while the participants used the system but also after they used the system compared with before.},
  archive      = {J_TVCG},
  author       = {Satoshi Miura and Ryota Fukumoto and Naomi Okamura and Masakatsu G. Fujie and Shigeki Sugano},
  doi          = {10.1109/TVCG.2023.3295425},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5466-5477},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual illusion created by a striped pattern through augmented reality for the prevention of tumbling on stairs},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Personalized language model selection through gamified
elicitation of contrastive concept preferences. <em>TVCG</em>,
<em>30</em>(8), 5449–5465. (<a
href="https://doi.org/10.1109/TVCG.2023.3296905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Language models are widely used for different Natural Language Processing tasks while suffering from a lack of personalization. Personalization can be achieved by, e.g., fine-tuning the model on training data that is created by the user (e.g., social media posts). Previous work shows that the acquisition of such data can be challenging. Instead of adapting the model&#39;s parameters, we thus suggest selecting a model that matches the user&#39;s mental model of different thematic concepts in language. In this article, we attempt to capture such individual language understanding of users. In this process, two challenges have to be considered. First, we need to counteract disengagement since the task of communicating one&#39;s language understanding typically encompasses repetitive and time-consuming actions. Second, we need to enable users to externalize their mental models in different contexts, considering that language use changes depending on the environment. In this article, we integrate methods of gamification into a visual analytics (VA) workflow to engage users in sharing their knowledge within various contexts. In particular, we contribute the design of a gameful VA playground called Concept Universe. During the four-phased game, the users build personalized concept descriptions by explaining given concept names through representative keywords. Based on their performance, the system reacts with constant visual, verbal, and auditory feedback. We evaluate the system in a user study with six participants, showing that users are engaged and provide more specific input when facing a virtual opponent. We use the generated concepts to make personalized language model suggestions.},
  archive      = {J_TVCG},
  author       = {Rita Sevastjanova and Hanna Hauptmann and Sebastian Deterding and Mennatallah El-Assady},
  doi          = {10.1109/TVCG.2023.3296905},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5449-5465},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Personalized language model selection through gamified elicitation of contrastive concept preferences},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LC-NeRF: Local controllable face generation in neural
radiance field. <em>TVCG</em>, <em>30</em>(8), 5437–5448. (<a
href="https://doi.org/10.1109/TVCG.2023.3293653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D face generation has achieved high visual quality and 3D consistency thanks to the development of neural radiance fields (NeRF). However, these methods model the whole face as a neural radiance field, which limits the controllability of the local regions. In other words, previous methods struggle to independently control local regions, such as the mouth, nose, and hair. To improve local controllability in NeRF-based face generation, we propose LC-NeRF, which is composed of a Local Region Generators Module (LRGM) and a Spatial-Aware Fusion Module (SAFM) , allowing for geometry and texture control of local facial regions. The LRGM models different facial regions as independent neural radiance fields and the SAFM is responsible for merging multiple independent neural radiance fields into a complete representation. Finally, LC-NeRF enables the modification of the latent code associated with each individual generator, thereby allowing precise control over the corresponding local region. Qualitative and quantitative evaluations show that our method provides better local controllability than state-of-the-art 3D-aware face generation methods. A perception study reveals that our method outperforms existing state-of-the-art methods in terms of image quality, face consistency, and editing effects. Furthermore, our method exhibits favorable performance in downstream tasks, including real image editing and text-driven facial image editing.},
  archive      = {J_TVCG},
  author       = {Wen-Yang Zhou and Lu Yuan and Shu-Yu Chen and Lin Gao and Shi-Min Hu},
  doi          = {10.1109/TVCG.2023.3293653},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5437-5448},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LC-NeRF: Local controllable face generation in neural radiance field},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PCDNF: Revisiting learning-based point cloud denoising via
joint normal filtering. <em>TVCG</em>, <em>30</em>(8), 5419–5436. (<a
href="https://doi.org/10.1109/TVCG.2023.3292464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud denoising is a fundamental and challenging problem in geometry processing. Existing methods typically involve direct denoising of noisy input or filtering raw normals followed by point position updates. Recognizing the crucial relationship between point cloud denoising and normal filtering, we re-examine this problem from a multitask perspective and propose an end-to-end network called PCDNF for joint normal filtering-based point cloud denoising. We introduce an auxiliary normal filtering task to enhance the network&#39;s ability to remove noise while preserving geometric features more accurately. Our network incorporates two novel modules. First, we design a shape-aware selector to improve noise removal performance by constructing latent tangent space representations for specific points, taking into account learned point and normal features as well as geometric priors. Second, we develop a feature refinement module to fuse point and normal features, capitalizing on the strengths of point features in describing geometric details and normal features in representing geometric structures, such as sharp edges and corners. This combination overcomes the limitations of each feature type and better recovers geometric information. Extensive evaluations, comparisons, and ablation studies demonstrate that the proposed method outperforms state-of-the-art approaches in both point cloud denoising and normal filtering.},
  archive      = {J_TVCG},
  author       = {Zheng Liu and Yaowu Zhao and Sijing Zhan and Yuanyuan Liu and Renjie Chen and Ying He},
  doi          = {10.1109/TVCG.2023.3292464},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5419-5436},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PCDNF: Revisiting learning-based point cloud denoising via joint normal filtering},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactive volume visualization via multi-resolution hash
encoding based neural representation. <em>TVCG</em>, <em>30</em>(8),
5404–5418. (<a href="https://doi.org/10.1109/TVCG.2023.3293121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implicit neural networks have demonstrated immense potential in compressing volume data for visualization. However, despite their advantages, the high costs of training and inference have thus far limited their application to offline data processing and non-interactive rendering. In this article, we present a novel solution that leverages modern GPU tensor cores, a well-implemented CUDA machine learning framework, an optimized global-illumination-capable volume rendering algorithm, and a suitable acceleration data structure to enable real-time direct ray tracing of volumetric neural representations. Our approach produces high-fidelity neural representations with a peak signal-to-noise ratio (PSNR) exceeding 30 dB, while reducing their size by up to three orders of magnitude. Remarkably, we show that the entire training step can fit within a rendering loop, bypassing the need for pre-training. Additionally, we introduce an efficient out-of-core training strategy to support extreme-scale volume data, making it possible for our volumetric neural representation training to scale up to terascale on a workstation with an NVIDIA RTX 3090 GPU. Our method significantly outperforms state-of-the-art techniques in terms of training time, reconstruction quality, and rendering performance, making it an ideal choice for applications where fast and accurate visualization of large-scale volume data is paramount.},
  archive      = {J_TVCG},
  author       = {Qi Wu and David Bauer and Michael J. Doyle and Kwan-Liu Ma},
  doi          = {10.1109/TVCG.2023.3293121},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5404-5418},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive volume visualization via multi-resolution hash encoding based neural representation},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SpectrumVA: Visual analysis of astronomical spectra for
facilitating classification inspection. <em>TVCG</em>, <em>30</em>(8),
5386–5403. (<a href="https://doi.org/10.1109/TVCG.2023.3294958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In astronomical spectral analysis, class recognition is essential and fundamental for subsequent scientific research. The experts often perform the visual inspection after automatic classification to deal with low-quality spectra to improve accuracy. However, given the enormous spectral volume and inadequacy of the current inspection practice, such inspection is tedious and time-consuming. This article presents a visual analytics system named SpectrumVA to promote the efficiency of visual inspection while guaranteeing accuracy. We abstract inspection as a visual parameter space analysis process, using redshifts and spectral lines as parameters. Different navigation strategies are employed in the “selection-inspection-promotion” workflow. At the selection stage, we help the experts identify a spectrum of interest through spectral representations and auxiliary information. Several possible redshifts and corresponding important spectral lines are also recommended through a global-to-local strategy to provide an appropriate entry point for the inspection. The inspection stage adopts a variety of instant visual feedback to help the experts adjust the redshift and select spectral lines in an informed trial-and-error manner. Similar spectra to the inspected one rather than different ones are visualized at the promotion stage, making the inspection process more fluent. We demonstrate the effectiveness of SpectrumVA through a quantitative algorithmic assessment, a case study, interviews with domain experts, and a user study.},
  archive      = {J_TVCG},
  author       = {Jincheng Li and Chufan Lai and Youfen Wang and Ali Luo and Xiaoru Yuan},
  doi          = {10.1109/TVCG.2023.3294958},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5386-5403},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SpectrumVA: Visual analysis of astronomical spectra for facilitating classification inspection},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Investigating the visual utility of differentially private
scatterplots. <em>TVCG</em>, <em>30</em>(8), 5370–5385. (<a
href="https://doi.org/10.1109/TVCG.2023.3292391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasingly, visualization practitioners are working with, using, and studying private and sensitive data. There can be many stakeholders interested in the resulting analyses—but widespread sharing of the data can cause harm to individuals, companies, and organizations. Practitioners are increasingly turning to differential privacy to enable public data sharing with a guaranteed amount of privacy. Differential privacy algorithms do this by aggregating data statistics with noise, and this now-private data can be released visually with differentially private scatterplots. While the private visual output is affected by the algorithm choice, privacy level, bin number, data distribution, and user task, there is little guidance on how to choose and balance the effect of these parameters. To address this gap, we had experts examine 1,200 differentially private scatterplots created with a variety of parameter choices and tested their ability to see aggregate patterns in the private output (i.e. the visual utility of the chart). We synthesized these results to provide easy-to-use guidance for visualization practitioners releasing private data through scatterplots. Our findings also provide a ground truth for visual utility , which we use to benchmark automated utility metrics from various fields. We demonstrate how multi-scale structural similarity (MS-SSIM), the metric most strongly correlated with our study&#39;s utility results, can be used to optimize parameter selection .},
  archive      = {J_TVCG},
  author       = {Liudas Panavas and Tarik Crnovrsanin and Jane Lydia Adams and Jonathan Ullman and Ali Sargavad and Melanie Tory and Cody Dunne},
  doi          = {10.1109/TVCG.2023.3292391},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5370-5385},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Investigating the visual utility of differentially private scatterplots},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The effects of auditory, visual, and cognitive distractions
on cybersickness in virtual reality. <em>TVCG</em>, <em>30</em>(8),
5350–5369. (<a href="https://doi.org/10.1109/TVCG.2023.3293405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cybersickness (CS) is one of the challenges that has hindered the widespread adoption of Virtual Reality (VR). Consequently, researchers continue to explore novel means to mitigate the undesirable effects associated with this affliction, one that may require a combination of remedies as opposed to a solitary stratagem. Inspired by research probing into the use of distractions as a means to control pain, we investigated the efficacy of this countermeasure against CS, studying how the introduction of temporally time-gated distractions affects this malady during a virtual experience featuring active exploration. Downstream of this, we studied how other aspects of the VR experience are affected by this intervention. We discuss the results of a between-subjects study manipulating the presence, sensory modality, and nature of periodic and short-lived (5–12 seconds) distractor stimuli across four experimental conditions: 1) no-distractors (ND); 2) auditory distractors (AD); 3) visual distractors (VD); 4) cognitive distractors (CD). Two of these conditions (VD and AD) formed a yoked control design wherein every matched pair of ‘seers’ and ‘hearers’ was periodically exposed to distractors that were identical in terms of content, temporality, duration, and sequence. In the CD condition, each participant had to periodically perform a 2-back working memory task, the duration and temporality of which was matched to distractors presented in each matched pair of the yoked conditions. These three conditions were compared to a baseline control group featuring no distractions. Results indicated that the reported sickness levels were lower in all three distraction groups in comparison to the control group. The intervention also increased the amount of time users were able to endure the VR simulation and avoided causing detriments to spatial memory and virtual travel efficiency. Overall, it appears that it may be possible to make users less consciously aware and bothered by the symptoms of CS, thereby reducing its perceived severity.},
  archive      = {J_TVCG},
  author       = {Rohith Venkatakrishnan and Roshan Venkatakrishnan and Balagopal Raveendranath and Dawn M. Sarno and Andrew C. Robb and Wen-Chieh Lin and Sabarish V. Babu},
  doi          = {10.1109/TVCG.2023.3293405},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5350-5369},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The effects of auditory, visual, and cognitive distractions on cybersickness in virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving knowledge retention and perceived control through
serious games: A study about assisted emergency evacuation.
<em>TVCG</em>, <em>30</em>(8), 5339–5349. (<a
href="https://doi.org/10.1109/TVCG.2023.3292473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital games for education and training, also called serious games (SGs), have shown beneficial effects on learning in several studies. In addition, some studies are suggesting that SGs could improve user&#39;s perceived control, which affects the likelihood that the learned content will be applied in the real world. However, most SG studies tend to focus on immediate effects, providing no indication on knowledge and perceived control over time, especially in contrast with nongame approaches. Moreover, SG research on perceived control has focused mainly on self-efficacy, disregarding the complementary construct of locus of control (LOC). This article advances both lines of research, assessing user&#39;s knowledge and LOC over time, with a SG as well as traditional printed materials that teach the same content. Results show that the SG was more effective than printed materials for knowledge retention over time, and a better retention outcome was found also for LOC. An additional contribution of the paper is the proposal of a novel SG that targets the inclusivity goal of safe evacuation for all, extending SG research to a domain not dealt with before, i.e., assisting persons with disabilities in emergencies.},
  archive      = {J_TVCG},
  author       = {Luca Chittaro},
  doi          = {10.1109/TVCG.2023.3292473},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5339-5349},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Improving knowledge retention and perceived control through serious games: A study about assisted emergency evacuation},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HDhuman: High-quality human novel-view rendering from sparse
views. <em>TVCG</em>, <em>30</em>(8), 5328–5338. (<a
href="https://doi.org/10.1109/TVCG.2023.3290543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we aim to address the challenge of novel view rendering of human performers that wear clothes with complex texture patterns using a sparse set of camera views. Although some recent works have achieved remarkable rendering quality on humans with relatively uniform textures using sparse views, the rendering quality remains limited when dealing with complex texture patterns as they are unable to recover the high-frequency geometry details that are observed in the input views. To this end, we propose HDhuman, which uses a human reconstruction network with a pixel-aligned spatial transformer and a rendering network with geometry-guided pixel-wise feature integration to achieve high-quality human reconstruction and rendering. The designed pixel-aligned spatial transformer calculates the correlations between the input views and generates human reconstruction results with high-frequency details. Based on the surface reconstruction results, the geometry-guided pixel-wise visibility reasoning provides guidance for multi-view feature integration, enabling the rendering network to render high-quality images at 2k resolution on novel views. Unlike previous neural rendering works that always need to train or fine-tune an independent network for a different scene, our method is a general framework that is able to generalize to novel subjects. Experiments show that our approach outperforms all the prior generic or specific methods on both synthetic data and real-world data. Source code and test data will be made publicly available for research purposes at http://cic.tju.edu.cn/faculty/likun/projects/HDhuman/index.html .},
  archive      = {J_TVCG},
  author       = {Tiansong Zhou and Jing Huang and Tao Yu and Ruizhi Shao and Kun Li},
  doi          = {10.1109/TVCG.2023.3290543},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5328-5338},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HDhuman: High-quality human novel-view rendering from sparse views},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient and robust from-point visibility. <em>TVCG</em>,
<em>30</em>(8), 5313–5327. (<a
href="https://doi.org/10.1109/TVCG.2023.3291138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents two from-point visibility algorithms: one aggressive and one exact. The aggressive algorithm efficiently computes a nearly complete visible set, with the guarantee of finding all triangles of a front surface, no matter how small their image footprint. The exact algorithm starts from the aggressive visible set and finds the remaining visible triangles efficiently and robustly. The algorithms are based on the idea of generalizing the set of sampling locations defined by the pixels of an image. Starting from a conventional image with one sampling location at each pixel center, the aggressive algorithm adds sampling locations to make sure that a triangle is sampled at all the pixels it touches. Thereby, the aggressive algorithm finds all triangles that are completely visible at a pixel regardless of geometric level of detail, distance from viewpoint, or view direction. The exact algorithm builds an initial visibility subdivision from the aggressive visible set, which it then uses to find most of the hidden triangles. The triangles whose visibility status is yet to be determined are processed iteratively, with the help of additional sampling locations. Since the initial visible set is almost complete, and since each additional sampling location finds a new visible triangle, the algorithm converges in a few iterations.},
  archive      = {J_TVCG},
  author       = {Voicu Popescu and Elisha Sacks and Jian Cui and Rohan Ashok},
  doi          = {10.1109/TVCG.2023.3291138},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5313-5327},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Efficient and robust from-point visibility},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive 3D mesh steganography based on feature-preserving
distortion. <em>TVCG</em>, <em>30</em>(8), 5299–5312. (<a
href="https://doi.org/10.1109/TVCG.2023.3289234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current 3D mesh steganography algorithms relying on geometric modification are prone to detection by steganalyzers. In traditional steganography, adaptive steganography has proven to be an efficient means of enhancing steganography security. Taking inspiration from this, we propose a highly adaptive embedding algorithm, guided by the principle of minimizing a carefully crafted distortion through efficient steganography codes. Specifically, we tailor a payload-limited embedding optimization problem for 3D settings and devise a feature-preserving distortion (FPD) to measure the impact of message embedding. The distortion takes on an additive form and is defined as a weighted difference of the effective steganalytic subfeatures utilized by the current 3D steganalyzers. With practicality in mind, we refine the distortion to enhance robustness and computational efficiency. By minimizing the FPD, our algorithm can preserve mesh features to a considerable extent, including steganalytic and geometric features, while achieving a high embedding capacity. During the practical embedding phase, we employ the $Q$ -layered syndrome trellis code (STC). However, calculating the bit modification probability (BMP) for each layer of the $Q$ -layered STC, given the variation of $Q$ , can be cumbersome. To address this issue, we design a universal and automatic approach for the BMP calculation. The experimental results demonstrate that our algorithm achieves state-of-the-art performance in countering 3D steganalysis.},
  archive      = {J_TVCG},
  author       = {Yushu Zhang and Jiahao Zhu and Mingfu Xue and Xinpeng Zhang and Xiaochun Cao},
  doi          = {10.1109/TVCG.2023.3289234},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5299-5312},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Adaptive 3D mesh steganography based on feature-preserving distortion},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Local geometric indexing of high resolution data for facial
reconstruction from sparse markers. <em>TVCG</em>, <em>30</em>(8),
5289–5298. (<a href="https://doi.org/10.1109/TVCG.2023.3289495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When considering sparse motion capture marker data, one typically struggles to balance its overfitting via a high dimensional blendshape system versus underfitting caused by smoothness constraints. With the current trend towards using more and more data, our aim is not to fit the motion capture markers with a parameterized (blendshape) model or to smoothly interpolate a surface through the marker positions, but rather to find an instance in the high resolution dataset that contains local geometry to fit each marker. Just as is true for typical machine learning applications, this approach benefits from a plethora of data, and thus we also consider augmenting the dataset via specially designed physical simulations that target the high resolution dataset such that the simulation output lies on the same so-called manifold as the data targeted.},
  archive      = {J_TVCG},
  author       = {Matthew Cong and Lana Lan and Ronald Fedkiw},
  doi          = {10.1109/TVCG.2023.3289495},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5289-5298},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Local geometric indexing of high resolution data for facial reconstruction from sparse markers},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AutoTitle: An interactive title generator for
visualizations. <em>TVCG</em>, <em>30</em>(8), 5276–5288. (<a
href="https://doi.org/10.1109/TVCG.2023.3290241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose AutoTitle, an interactive visualization title generator satisfying multifarious user requirements. Factors making a good title, namely, the feature importance, coverage, preciseness, general information richness, conciseness, and non-technicality, are summarized based on the feedback from user interviews. Visualization authors need to trade off among these factors to fit specific scenarios, resulting in a wide design space of visualization titles. AutoTitle generates various titles through the process of visualization facts traversing, deep learning-based fact-to-title generation, and quantitative evaluation of the six factors. AutoTitle also provides users with an interactive interface to explore the desired titles by filtering the metrics. We conduct a user study to validate the quality of generated titles as well as the rationality and helpfulness of these metrics.},
  archive      = {J_TVCG},
  author       = {Can Liu and Yuhan Guo and Xiaoru Yuan},
  doi          = {10.1109/TVCG.2023.3290241},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5276-5288},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {AutoTitle: An interactive title generator for visualizations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SketchMetaFace: A learning-based sketching interface for
high-fidelity 3D character face modeling. <em>TVCG</em>, <em>30</em>(8),
5260–5275. (<a href="https://doi.org/10.1109/TVCG.2023.3291703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling 3D avatars benefits various application scenarios such as AR/VR, gaming, and filming. Character faces contribute significant diversity and vividity as a vital component of avatars. However, building 3D character face models usually requires a heavy workload with commercial tools, even for experienced artists. Various existing sketch-based tools fail to support amateurs in modeling diverse facial shapes and rich geometric details. In this article, we present SketchMetaFace - a sketching system targeting amateur users to model high-fidelity 3D faces in minutes. We carefully design both the user interface and the underlying algorithm. First, curvature-aware strokes are adopted to better support the controllability of carving facial details. Second, considering the key problem of mapping a 2D sketch map to a 3D model, we develop a novel learning-based method termed “Implicit and Depth Guided Mesh Modeling” (IDGMM). It fuses the advantages of mesh, implicit, and depth representations to achieve high-quality results with high efficiency. In addition, to further support usability, we present a coarse-to-fine 2D sketching interface design and a data-driven stroke suggestion tool. User studies demonstrate the superiority of our system over existing modeling tools in terms of the ease to use and visual quality of results. Experimental analyses also show that IDGMM reaches a better trade-off between accuracy and efficiency.},
  archive      = {J_TVCG},
  author       = {Zhongjin Luo and Dong Du and Heming Zhu and Yizhou Yu and Hongbo Fu and Xiaoguang Han},
  doi          = {10.1109/TVCG.2023.3291703},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5260-5275},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SketchMetaFace: A learning-based sketching interface for high-fidelity 3D character face modeling},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). More than data stories: Broadening the role of
visualization in contemporary journalism. <em>TVCG</em>, <em>30</em>(8),
5240–5259. (<a href="https://doi.org/10.1109/TVCG.2023.3287585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data visualization and journalism are deeply connected. From early infographics to recent data-driven storytelling, visualization has become an integrated part of contemporary journalism, primarily as a communication artifact to inform the general public. Data journalism, harnessing the power of data visualization, has emerged as a bridge between the growing volume of data and our society. Visualization research that centers around data storytelling has sought to understand and facilitate such journalistic endeavors. However, a recent metamorphosis in journalism has brought broader challenges and opportunities that extend beyond mere communication of data. We present this article to enhance our understanding of such transformations and thus broaden visualization research&#39;s scope and practical contribution to this evolving field. We first survey recent significant shifts, emerging challenges, and computational practices in journalism. We then summarize six roles of computing in journalism and their implications. Based on these implications, we provide propositions for visualization research concerning each role. Ultimately, by mapping the roles and propositions onto a proposed ecological model and contextualizing existing visualization research, we surface seven general topics and a series of research agendas that can guide future visualization research at this intersection.},
  archive      = {J_TVCG},
  author       = {Yu Fu and John Stasko},
  doi          = {10.1109/TVCG.2023.3287585},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5240-5259},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {More than data stories: Broadening the role of visualization in contemporary journalism},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Practical integer-constrained cone construction for
conformal parameterizations. <em>TVCG</em>, <em>30</em>(8), 5227–5239.
(<a href="https://doi.org/10.1109/TVCG.2023.3287303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a practical method to construct sparse integer-constrained cone singularities with low distortion constraints for conformal parameterizations. Our solution for this combinatorial problem is a two-stage procedure that first enhances sparsity for generating an initialization and then optimizes to reduce the number of cones and the parameterization distortion. Central to the first stage is a progressive process to determine the combinatorial variables, i.e., numbers, locations, and angles of cones. The second stage iteratively conducts adaptive cone relocations and merges close cones for optimization. We extensively test our method on a data set containing 3885 models, demonstrating practical robustness and performance. Our method achieves fewer cone singularities and lower parameterization distortion than state-of-the-art methods.},
  archive      = {J_TVCG},
  author       = {Zheng Zhang and Mo Li and Zheng-Yu Zhao and Qing Fang and Xiao-Ming Fu},
  doi          = {10.1109/TVCG.2023.3287303},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5227-5239},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Practical integer-constrained cone construction for conformal parameterizations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Characteristic-preserving latent space for unpaired
cross-domain translation of 3D point clouds. <em>TVCG</em>,
<em>30</em>(8), 5212–5226. (<a
href="https://doi.org/10.1109/TVCG.2023.3287923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article aims at unpaired shape-to-shape transformation for 3D point clouds, for instance, turning a chair to its table counterpart. Recent work for 3D shape transfer or deformation highly relies on paired inputs or specific correspondences. However, it is usually not feasible to assign precise correspondences or prepare paired data from two domains. A few methods start to study unpaired learning, but the characteristics of a source model may not be preserved after transformation. To overcome the difficulty of unpaired learning for transformation, we propose alternately training the autoencoder and translators to construct shape-aware latent space. This latent space based on novel loss functions enables our translators to transform 3D point clouds across domains and maintain the consistency of shape characteristics. We also crafted a test dataset to objectively evaluate the performance of point-cloud translation. The experiments demonstrate that our framework can construct high-quality models and retain more shape characteristics during cross-domain translation compared to the state-of-the-art methods. Moreover, we also present shape editing applications with our proposed latent space, including shape-style mixing and shape-type shifting, which do not require retraining a model.},
  archive      = {J_TVCG},
  author       = {Jia-Wen Zheng and Jhen-Yung Hsu and Chih-Chia Li and I-Chen Lin},
  doi          = {10.1109/TVCG.2023.3287923},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5212-5226},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Characteristic-preserving latent space for unpaired cross-domain translation of 3D point clouds},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Creating emordle: Animating word cloud for emotion
expression. <em>TVCG</em>, <em>30</em>(8), 5198–5211. (<a
href="https://doi.org/10.1109/TVCG.2023.3286392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose emordle, a conceptual design that animates wordles (compact word clouds) to deliver their emotional context to audiences. To inform the design, we first reviewed online examples of animated texts and animated wordles, and summarized strategies for injecting emotion into the animations. We introduced a composite approach that extends an existing animation scheme for one word to multiple words in a wordle with two global factors: the randomness of text animation (entropy) and the animation speed (speed). To create an emordle, general users can choose one predefined animated scheme that matches the intended emotion class and fine-tune the emotion intensity with the two parameters. We designed proof-of-concept emordle examples for four basic emotion classes, namely happiness, sadness, anger, and fear. We conducted two controlled crowdsourcing studies to evaluate our approach. The first study confirmed that people generally agreed on the conveyed emotions from well-crafted animations, and the second one demonstrated that our identified factors helped fine-tune the extent of the emotion delivered. We also invited general users to create their own emordles based on our proposed framework. Through this user study, we confirmed the effectiveness of the approach. We concluded with implications for future research opportunities of supporting emotion expression in visualizations.},
  archive      = {J_TVCG},
  author       = {Liwenhan Xie and Xinhuan Shu and Jeon Cheol Su and Yun Wang and Siming Chen and Huamin Qu},
  doi          = {10.1109/TVCG.2023.3286392},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5198-5211},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Creating emordle: Animating word cloud for emotion expression},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large-strain surface modeling using plasticity.
<em>TVCG</em>, <em>30</em>(8), 5183–5197. (<a
href="https://doi.org/10.1109/TVCG.2023.3289811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling arbitrarily large deformations of surfaces smoothly embedded in three-dimensional space is challenging. We give a new method to represent surfaces undergoing large spatially varying rotations and strains, based on differential geometry, and surface first and second fundamental forms. Methods that penalize the difference between the current shape and the rest shape produce sharp spikes under large strains, and variational methods produce wiggles, whereas our method naturally supports large strains and rotations without any special treatment. For stable and smooth results, we demonstrate that the deformed surface has to locally satisfy compatibility conditions (Gauss-Codazzi equations) on the first and second fundamental forms. We then give a method to locally modify the surface first and second fundamental forms in a compatible way. We use those fundamental forms to define surface plastic deformations, and finally recover output surface vertex positions by minimizing the surface elastic energy under the plastic deformations. We demonstrate that our method makes it possible to smoothly deform triangle meshes to large spatially varying strains and rotations, while meeting user constraints.},
  archive      = {J_TVCG},
  author       = {Jiahao Wen and Bohan Wang and Jernej Barbič},
  doi          = {10.1109/TVCG.2023.3289811},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5183-5197},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Large-strain surface modeling using plasticity},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FS/DS: A theoretical framework for the dual analysis of
feature space and data space. <em>TVCG</em>, <em>30</em>(8), 5165–5182.
(<a href="https://doi.org/10.1109/TVCG.2023.3288356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the surge of data-driven analysis techniques, there is a rising demand for enhancing the exploration of large high-dimensional data by enabling interactions for the joint analysis of features (i.e., dimensions). Such a dual analysis of the feature space and data space is characterized by three components, 1) a view visualizing feature summaries, 2) a view that visualizes the data records, and 3) a bidirectional linking of both plots triggered by human interaction in one of both visualizations, e.g., Linking &amp; Brushing. Dual analysis approaches span many domains, e.g., medicine, crime analysis, and biology. The proposed solutions encapsulate various techniques, such as feature selection or statistical analysis. However, each approach establishes a new definition of dual analysis. To address this gap, we systematically reviewed published dual analysis methods to investigate and formalize the key elements, such as the techniques used to visualize the feature space and data space, as well as the interaction between both spaces. From the information elicited during our review, we propose a unified theoretical framework for dual analysis, encompassing all existing approaches extending the field. We apply our proposed formalization describing the interactions between each component and relate them to the addressed tasks. Additionally, we categorize the existing approaches using our framework and derive future research directions to advance dual analysis by including state-of-the-art visual analysis techniques to improve data exploration.},
  archive      = {J_TVCG},
  author       = {Frederik L. Dennig and Matthias Miller and Daniel A. Keim and Mennatallah El-Assady},
  doi          = {10.1109/TVCG.2023.3288356},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5165-5182},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FS/DS: A theoretical framework for the dual analysis of feature space and data space},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The reality of the situation: A survey of situated
analytics. <em>TVCG</em>, <em>30</em>(8), 5147–5164. (<a
href="https://doi.org/10.1109/TVCG.2023.3285546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of low-cost, accessible, and high-performance augmented reality (AR) has shed light on a situated form of analytics where in-situ visualizations embedded in the real world can facilitate sensemaking based on the user&#39;s physical location. In this work, we identify prior literature in this emerging field with a focus on situated analytics. After collecting 47 relevant situated analytics systems, we classify them using a taxonomy of three dimensions: situating triggers, view situatedness, and data depiction. We then identify four archetypical patterns in our classification using an ensemble cluster analysis. We also assess the level which these systems support the sensemaking process. Finally, we discuss insights and design guidelines that we learned from our analysis.},
  archive      = {J_TVCG},
  author       = {Sungbok Shin and Andrea Batch and Peter William Scott Butcher and Panagiotis D. Ritsos and Niklas Elmqvist},
  doi          = {10.1109/TVCG.2023.3285546},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5147-5164},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The reality of the situation: A survey of situated analytics},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards better modeling with missing data: A contrastive
learning-based visual analytics perspective. <em>TVCG</em>,
<em>30</em>(8), 5129–5146. (<a
href="https://doi.org/10.1109/TVCG.2023.3285210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data can pose a challenge for machine learning (ML) modeling. To address this, current approaches are categorized into feature imputation and label prediction and are primarily focused on handling missing data to enhance ML performance. These approaches rely on the observed data to estimate the missing values and therefore encounter three main shortcomings in imputation, including the need for different imputation methods for various missing data mechanisms, heavy dependence on the assumption of data distribution, and potential introduction of bias. This study proposes a Contrastive Learning (CL) framework to model observed data with missing values, where the ML model learns the similarity between an incomplete sample and its complete counterpart and the dissimilarity between other samples. Our proposed approach demonstrates the advantages of CL without requiring any imputation. To enhance interpretability, we introduce CIVis , a visual analytics system that incorporates interpretable techniques to visualize the learning process and diagnose the model status. Users can leverage their domain knowledge through interactive sampling to identify negative and positive pairs in CL. The output of CIVis is an optimized model that takes specified features and predicts downstream tasks. We provide two usage scenarios in regression and classification tasks and conduct quantitative experiments, expert interviews, and a qualitative user study to demonstrate the effectiveness of our approach. In short, this study offers a valuable contribution to addressing the challenges associated with ML modeling in the presence of missing data by providing a practical solution that achieves high predictive accuracy and model interpretability.},
  archive      = {J_TVCG},
  author       = {Laixin Xie and Yang Ouyang and Longfei Chen and Ziming Wu and Quan Li},
  doi          = {10.1109/TVCG.2023.3285210},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5129-5146},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Towards better modeling with missing data: A contrastive learning-based visual analytics perspective},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pose guided person image generation via dual-task
correlation and affinity learning. <em>TVCG</em>, <em>30</em>(8),
5111–5128. (<a href="https://doi.org/10.1109/TVCG.2023.3286394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pose Guided Person Image Generation (PGPIG) is the task of transforming a person&#39;s image from the source pose to a target pose. Existing PGPIG methods often tend to learn an end-to-end transformation between the source image and the target image, but do not seriously consider two issues: 1) the PGPIG is an ill-posed problem, and 2) the texture mapping requires effective supervision. In order to alleviate these two challenges, we propose a novel method by incorporating D ual-task P ose T ransformer N etwork and T exture A ffinity learning mechanism (DPTN-TA). To assist the ill-posed source-to-target task learning, DPTN-TA introduces an auxiliary task, i.e., source-to-source task, by a Siamese structure and further explores the dual-task correlation. Specifically, the correlation is built by the proposed Pose Transformer Module (PTM), which can adaptively capture the fine-grained mapping between sources and targets and can promote the source texture transmission to enhance the details of the generated images. Moreover, we propose a novel texture affinity loss to better supervise the learning of texture mapping. In this way, the network is able to learn complex spatial transformations effectively. Extensive experiments show that our DPTN-TA can produce perceptually realistic person images under significant pose changes. Furthermore, our DPTN-TA is not limited to processing human bodies but can be flexibly extended to view synthesis of other objects, i.e., faces and chairs, outperforming the state-of-the-arts in terms of both LPIPS and FID. Our code is available at: https://github.com/PangzeCheung/Dual-task-Pose-Transformer-Network .},
  archive      = {J_TVCG},
  author       = {Pengze Zhang and Lingxiao Yang and Xiaohua Xie and Jianhuang Lai},
  doi          = {10.1109/TVCG.2023.3286394},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5111-5128},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Pose guided person image generation via dual-task correlation and affinity learning},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). What does the chart say? Grouping cues guide viewer
comparisons and conclusions in bar charts. <em>TVCG</em>,
<em>30</em>(8), 5097–5110. (<a
href="https://doi.org/10.1109/TVCG.2023.3289292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reading a visualization is like reading a paragraph. Each sentence is a comparison: the mean of these is higher than those; this difference is smaller than that. What determines which comparisons are made first? The viewer&#39;s goals and expertise matter, but the way that values are visually grouped together within the chart also impacts those comparisons. Research from psychology suggests that comparisons involve multiple steps. First, the viewer divides the visualization into a set of units. This might include a single bar or a grouped set of bars. Then the viewer selects and compares two of these units, perhaps noting that one pair of bars is longer than another. Viewers might take an additional third step and perform a second-order comparison, perhaps determining that the difference between one pair of bars is greater than the difference between another pair. We create a visual comparison taxonomy that allows us to develop and test a sequence of hypotheses about which comparisons people are more likely to make when reading a visualization. We find that people tend to compare two groups before comparing two individual bars and that second-order comparisons are rare. Visual cues like spatial proximity and color can influence which elements are grouped together and selected for comparison, with spatial proximity being a stronger grouping cue. Interestingly, once the viewer grouped together and compared a set of bars, regardless of whether the group is formed by spatial proximity or color similarity, they no longer consider other possible groupings in their comparisons.},
  archive      = {J_TVCG},
  author       = {Cindy Xiong Bearfield and Chase Stokes and Andrew Lovett and Steven Franconeri},
  doi          = {10.1109/TVCG.2023.3289292},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5097-5110},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {What does the chart say? grouping cues guide viewer comparisons and conclusions in bar charts},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Survey of annotations in extended reality systems.
<em>TVCG</em>, <em>30</em>(8), 5074–5096. (<a
href="https://doi.org/10.1109/TVCG.2023.3288869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Annotation in 3D user interfaces such as Augmented Reality (AR) and Virtual Reality (VR) is a challenging and promising area; however, there are not currently surveys reviewing these contributions. In order to provide a survey of annotations for Extended Reality (XR) environments, we conducted a structured literature review of papers that used annotation in their AR/VR systems from the period between 2001 and 2021. Our literature review process consists of several filtering steps which resulted in 103 XR publications with a focus on annotation. We classified these papers based on the display technologies, input devices, annotation types, target object under annotation, collaboration type, modalities, and collaborative technologies. A survey of annotation in XR is an invaluable resource for researchers and newcomers. Finally, we provide a database of the collected information for each reviewed paper. This information includes applications, the display technologies and its annotator, input devices, modalities, annotation types, interaction techniques, collaboration types, and tasks for each paper. This database provides a rapid access to collected data and gives users the ability to search or filter the required information. This survey provides a starting point for anyone interested in researching annotation in XR environments.},
  archive      = {J_TVCG},
  author       = {Zahra Borhani and Prashast Sharma and Francisco R. Ortega},
  doi          = {10.1109/TVCG.2023.3288869},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5074-5096},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Survey of annotations in extended reality systems},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A visual environment for data driven protein modeling and
validation. <em>TVCG</em>, <em>30</em>(8), 5063–5073. (<a
href="https://doi.org/10.1109/TVCG.2023.3286582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In structural biology, validation and verification of new atomic models are crucial and necessary steps which limit the production of reliable molecular models for publications and databases. An atomic model is the result of meticulous modeling and matching and is evaluated using a variety of metrics that provide clues to improve and refine the model so it fits our understanding of molecules and physical constraints. In cryo electron microscopy (cryo-EM) the validation is also part of an iterative modeling process in which there is a need to judge the quality of the model during the creation phase. A shortcoming is that the process and results of the validation are rarely communicated using visual metaphors. This work presents a visual framework for molecular validation. The framework was developed in close collaboration with domain experts in a participatory design process. Its core is a novel visual representation based on 2D heatmaps that shows all available validation metrics in a linear fashion, presenting a global overview of the atomic model and provide domain experts with interactive analysis tools. Additional information stemming from the underlying data, such as a variety of local quality measures, is used to guide the user&#39;s attention toward regions of higher relevance. Linked with the heatmap is a three-dimensional molecular visualization providing the spatial context of the structures and chosen metrics. Additional views of statistical properties of the structure are included in the visual framework. We demonstrate the utility of the framework and its visual guidance with examples from cryo-EM.},
  archive      = {J_TVCG},
  author       = {Martin Falk and Victor Tobiasson and Alexander Bock and Charles Hansen and Anders Ynnerman},
  doi          = {10.1109/TVCG.2023.3286582},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5063-5073},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A visual environment for data driven protein modeling and validation},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A two-part transformer network for controllable motion
synthesis. <em>TVCG</em>, <em>30</em>(8), 5047–5062. (<a
href="https://doi.org/10.1109/TVCG.2023.3284402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although part-based motion synthesis networks have been investigated to reduce the complexity of modeling heterogeneous human motions, their computational cost remains prohibitive in interactive applications. To this end, we propose a novel two-part transformer network that aims to achieve high-quality, controllable motion synthesis results in real-time. Our network separates the skeleton into the upper and lower body parts, reducing the expensive cross-part fusion operations, and models the motions of each part separately through two streams of auto-regressive modules formed by multi-head attention layers. However, such a design might not sufficiently capture the correlations between the parts. We thus intentionally let the two parts share the features of the root joint and design a consistency loss to penalize the difference in the estimated root features and motions by these two auto-regressive modules, significantly improving the quality of synthesized motions. After training on our motion dataset, our network can synthesize a wide range of heterogeneous motions, like cartwheels and twists. Experimental and user study results demonstrate that our network is superior to state-of-the-art human motion synthesis networks in the quality of generated motions.},
  archive      = {J_TVCG},
  author       = {Shuaiying Hou and Hongyu Tao and Hujun Bao and Weiwei Xu},
  doi          = {10.1109/TVCG.2023.3284402},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5047-5062},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A two-part transformer network for controllable motion synthesis},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimally ordered orthogonal neighbor joining trees for
hierarchical cluster analysis. <em>TVCG</em>, <em>30</em>(8), 5034–5046.
(<a href="https://doi.org/10.1109/TVCG.2023.3284499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose to use optimally ordered orthogonal neighbor-joining (O $^{3}$ NJ) trees as a new way to visually explore cluster structures and outliers in multi-dimensional data. Neighbor-joining (NJ) trees are widely used in biology, and their visual representation is similar to that of dendrograms. The core difference to dendrograms, however, is that NJ trees correctly encode distances between data points, resulting in trees with varying edge lengths. We optimize NJ trees for their use in visual analysis in two ways. First, we propose to use a novel leaf sorting algorithm that helps users to better interpret adjacencies and proximities within such a tree. Second, we provide a new method to visually distill the cluster tree from an ordered NJ tree. Numerical evaluation and three case studies illustrate the benefits of this approach for exploring multi-dimensional data in areas such as biology or image analysis.},
  archive      = {J_TVCG},
  author       = {Tong Ge and Xu Luo and Yunhai Wang and Michael Sedlmair and Zhanglin Cheng and Ying Zhao and Xin Liu and Oliver Deussen and Baoquan Chen},
  doi          = {10.1109/TVCG.2023.3284499},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5034-5046},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Optimally ordered orthogonal neighbor joining trees for hierarchical cluster analysis},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural-IMLS: Self-supervised implicit moving least-squares
network for surface reconstruction. <em>TVCG</em>, <em>30</em>(8),
5018–5033. (<a href="https://doi.org/10.1109/TVCG.2023.3284233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface reconstruction is a challenging task when input point clouds, especially real scans, are noisy and lack normals. Observing that the Multilayer Perceptron (MLP) and the implicit moving least-square function (IMLS) provide a dual representation of the underlying surface, we introduce Neural-IMLS , a novel approach that directly learns a noise-resistant signed distance function (SDF) from unoriented raw point clouds in a self-supervised manner. In particular, IMLS regularizes MLP by providing estimated SDFs near the surface and helps enhance its ability to represent geometric details and sharp features, while MLP regularizes IMLS by providing estimated normals. We prove that at convergence, our neural network produces a faithful SDF whose zero-level set approximates the underlying surface due to the mutual learning mechanism between the MLP and the IMLS. Extensive experiments on various benchmarks, including synthetic and real scans, show that Neural-IMLS can reconstruct faithful shapes even with noise and missing parts. The source code can be found at https://github.com/bearprin/Neural-IMLS .},
  archive      = {J_TVCG},
  author       = {Zixiong Wang and Pengfei Wang and Peng-Shuai Wang and Qiujie Dong and Junjie Gao and Shuangmin Chen and Shiqing Xin and Changhe Tu and Wenping Wang},
  doi          = {10.1109/TVCG.2023.3284233},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {5018-5033},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Neural-IMLS: Self-supervised implicit moving least-squares network for surface reconstruction},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). What’s the situation with intelligent mesh generation: A
survey and perspectives. <em>TVCG</em>, <em>30</em>(8), 4997–5017. (<a
href="https://doi.org/10.1109/TVCG.2023.3281781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent Mesh Generation (IMG) represents a novel and promising field of research, utilizing machine learning techniques to generate meshes. Despite its relative infancy, IMG has significantly broadened the adaptability and practicality of mesh generation techniques, delivering numerous breakthroughs and unveiling potential future pathways. However, a noticeable void exists in the contemporary literature concerning comprehensive surveys of IMG methods. This paper endeavors to fill this gap by providing a systematic and thorough survey of the current IMG landscape. With a focus on 113 preliminary IMG methods, we undertake a meticulous analysis from various angles, encompassing core algorithm techniques and their application scope, agent learning objectives, data types, targeted challenges, as well as advantages and limitations. We have curated and categorized the literature, proposing three unique taxonomies based on key techniques, output mesh unit elements, and relevant input data types. This paper also underscores several promising future research directions and challenges in IMG.},
  archive      = {J_TVCG},
  author       = {Na Lei and Zezeng Li and Zebin Xu and Ying Li and Xianfeng Gu},
  doi          = {10.1109/TVCG.2023.3281781},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4997-5017},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {What&#39;s the situation with intelligent mesh generation: A survey and perspectives},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NeRF-art: Text-driven neural radiance fields stylization.
<em>TVCG</em>, <em>30</em>(8), 4983–4996. (<a
href="https://doi.org/10.1109/TVCG.2023.3283400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a powerful representation of 3D scenes, the neural radiance field ( NeRF ) enables high-quality novel view synthesis from multi-view images. Stylizing NeRF , however, remains challenging, especially in simulating a text-guided style with both the appearance and the geometry altered simultaneously. In this paper, we present NeRF-Art , a text-guided NeRF stylization approach that manipulates the style of a pre-trained NeRF model with a simple text prompt. Unlike previous approaches that either lack sufficient geometry deformations and texture details or require meshes to guide the stylization, our method can shift a 3D scene to the target style characterized by desired geometry and appearance variations without any mesh guidance. This is achieved by introducing a novel global-local contrastive learning strategy, combined with the directional constraint to simultaneously control both the trajectory and the strength of the target style. Moreover, we adopt a weight regularization method to effectively suppress cloudy artifacts and geometry noises which arise easily when the density field is transformed during geometry stylization. Through extensive experiments on various styles, we demonstrate that our method is effective and robust regarding both single-view stylization quality and cross-view consistency.},
  archive      = {J_TVCG},
  author       = {Can Wang and Ruixiang Jiang and Menglei Chai and Mingming He and Dongdong Chen and Jing Liao},
  doi          = {10.1109/TVCG.2023.3283400},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4983-4996},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {NeRF-art: Text-driven neural radiance fields stylization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physics-based efficient full projector compensation using
only natural images. <em>TVCG</em>, <em>30</em>(8), 4968–4982. (<a
href="https://doi.org/10.1109/TVCG.2023.3281681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving practical full projector compensation requires the projection display to adapt quickly to textured projection surfaces and unexpected movements without interrupting the display procedure. A possible solution to achieve this involves using a projector and an RGB camera and correcting both color and geometry by directly capturing and analyzing the projected natural image content, without the need for additional patterns. In this study, we approach full projector compensation as a numerical optimization problem and present a physics-based framework that can handle both geometric calibration and radiometric compensation for a Projector-camera system (Procams), using only a few sampling natural images. Within the framework, we decouple and estimate the Procams’ factors, such as the response function of the projector, the correspondence between the projector and camera, and the reflectance of projection surfaces. This approach provides an interpretable and flexible solution to adapt to the changes in geometry and reflectance caused by movements. Benefitting from the physics-based scheme, our method guarantees both accurate color calculation and efficient movement and reflectance estimation. Our experimental results demonstrate that our method surpasses other state-of-the-art end-to-end full projector compensation methods, with superior image quality, reduced computational time, lower memory consumption, greater geometric accuracy, and a more compact network architecture.},
  archive      = {J_TVCG},
  author       = {Yuqi Li and Wenting Yin and Jiabao Li and Xijiong Xie},
  doi          = {10.1109/TVCG.2023.3281681},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4968-4982},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Physics-based efficient full projector compensation using only natural images},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GPA-net: No-reference point cloud quality assessment with
multi-task graph convolutional network. <em>TVCG</em>, <em>30</em>(8),
4955–4967. (<a href="https://doi.org/10.1109/TVCG.2023.3282802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of 3D vision, point cloud has become an increasingly popular 3D visual media content. Due to the irregular structure, point cloud has posed novel challenges to the related research, such as compression, transmission, rendering and quality assessment. In these latest researches, point cloud quality assessment (PCQA) has attracted wide attention due to its significant role in guiding practical applications, especially in many cases where the reference point cloud is unavailable. However, current no-reference metrics which based on prevalent deep neural network have apparent disadvantages. For example, to adapt to the irregular structure of point cloud, they require preprocessing such as voxelization and projection that introduce extra distortions, and the applied grid-kernel networks, such as Convolutional Neural Networks, fail to extract effective distortion-related features. Besides, they rarely consider the various distortion patterns and the philosophy that PCQA should exhibit shift, scaling, and rotation invariance. In this paper, we propose a novel no-reference PCQA metric named the Graph convolutional PCQA network (GPA-Net). To extract effective features for PCQA, we propose a new graph convolution kernel, i.e., GPAConv, which attentively captures the perturbation of structure and texture. Then, we propose the multi-task framework consisting of one main task (quality regression) and two auxiliary tasks (distortion type and degree predictions). Finally, we propose a coordinate normalization module to stabilize the results of GPAConv under shift, scale and rotation transformations. Experimental results on two independent databases show that GPA-Net achieves the best performance compared to the state-of-the-art no-reference PCQA metrics, even better than some full-reference metrics in some cases.},
  archive      = {J_TVCG},
  author       = {Ziyu Shan and Qi Yang and Rui Ye and Yujie Zhang and Yiling Xu and Xiaozhong Xu and Shan Liu},
  doi          = {10.1109/TVCG.2023.3282802},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4955-4967},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GPA-net: No-reference point cloud quality assessment with multi-task graph convolutional network},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shading-guided manga screening from reference.
<em>TVCG</em>, <em>30</em>(8), 4941–4954. (<a
href="https://doi.org/10.1109/TVCG.2023.3282223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manga screening is a critical process in manga production, which still requires intensive labor and cost. Existing manga screening methods either generate simple dotted screentones only or rely on color information and manual hints during screentone selection. Due to the large domain gap between line drawings and screened manga, and the difficulties in generating high-quality, properly selected and shaded screentones, even state-of-the-art deep learning methods cannot convert line drawings to screened manga well. Besides, ambiguity exists in the screening process since different artists may screen differently for the same line drawing. In this article, we propose to introduce shaded line drawing as the intermediate counterpart of the screened manga so that the manga screening task can be decomposed into two sub-tasks, generating shading from a line drawing and replacing shading with proper screentones. The reference image is adopted to resolve the ambiguity issue and provides options and controls on the generated screened manga. We proposed a reference-based shading generation network and a reference-based screentone generation module to achieve the two sub-tasks individually. We conduct extensive visual and quantitative experiments to verify the effectiveness of our system. Results and statistics show that our method outperforms existing methods on the manga screening task.},
  archive      = {J_TVCG},
  author       = {Huisi Wu and Ziheng Ma and Wenliang Wu and Xueting Liu and Chengze Li and Zhenkun Wen},
  doi          = {10.1109/TVCG.2023.3282223},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4941-4954},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Shading-guided manga screening from reference},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MeshWGAN: Mesh-to-mesh wasserstein GAN with multi-task
gradient penalty for 3D facial geometric age transformation.
<em>TVCG</em>, <em>30</em>(8), 4927–4940. (<a
href="https://doi.org/10.1109/TVCG.2023.3284500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the metaverse develops rapidly, 3D facial age transformation is attracting increasing attention, which may bring many potential benefits to a wide variety of users, e.g., 3D aging figures creation, 3D facial data augmentation and editing. Compared with 2D methods, 3D face aging is an underexplored problem. To fill this gap, we propose a new mesh-to-mesh Wasserstein generative adversarial network (MeshWGAN) with a multi-task gradient penalty to model a continuous bi-directional 3D facial geometric aging process. To the best of our knowledge, this is the first architecture to achieve 3D facial geometric age transformation via real 3D scans. As previous image-to-image translation methods cannot be directly applied to the 3D facial mesh, which is totally different from 2D images, we built a mesh encoder, decoder, and multi-task discriminator to facilitate mesh-to-mesh transformations. To mitigate the lack of 3D datasets containing children&#39;s faces, we collected scans from 765 subjects aged 5-17 in combination with existing 3D face databases, which provided a large training dataset. Experiments have shown that our architecture can predict 3D facial aging geometries with better identity preservation and age closeness compared to 3D trivial baselines. We also demonstrated the advantages of our approach via various 3D face-related graphics applications.},
  archive      = {J_TVCG},
  author       = {Jie Zhang and Kangneng Zhou and Yan Luximon and Tong-Yee Lee and Ping Li},
  doi          = {10.1109/TVCG.2023.3284500},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4927-4940},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MeshWGAN: Mesh-to-mesh wasserstein GAN with multi-task gradient penalty for 3D facial geometric age transformation},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non-rigid registration via intelligent adaptive feedback
control. <em>TVCG</em>, <em>30</em>(8), 4910–4926. (<a
href="https://doi.org/10.1109/TVCG.2023.3283990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preserving features or local shape characteristics of a mesh using conventional non-rigid registration methods is always difficult, as the preservation and deformation are competing with each other. The challenge is to find a balance between these two terms in the process of the registration, especially in presence of artefacts in the mesh. We present a non-rigid Iterative Closest Points (ICP) algorithm which addresses the challenge as a control problem. An adaptive feedback control scheme with global asymptotic stability is derived to control the stiffness ratio for maximum feature preservation and minimum mesh quality loss during the registration process. A cost function is formulated with the distance term and the stiffness term where the initial stiffness ratio value is defined by an Adaptive Neuro-Fuzzy Inference System (ANFIS)-based predictor regarding the source mesh and the target mesh topology, and the distance between the correspondences. During the registration process, the stiffness ratio of each vertex is continuously adjusted by the intrinsic information, represented by shape descriptors, of the surrounding surface as well as the steps in the registration process. Besides, the estimated process-dependent stiffness ratios are used as dynamic weights for establishing the correspondences in each step of the registration. Experiments on simple geometric shapes as well as 3D scanning datasets indicated that the proposed approach outperforms current methodologies, especially for the regions where features are not eminent and/or there exist interferences between/among features, due to its ability to embed the inherent properties of the surface in the process of the mesh registration.},
  archive      = {J_TVCG},
  author       = {Farzam Tajdari and Toon Huysmans and Yu Song},
  doi          = {10.1109/TVCG.2023.3283990},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4910-4926},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Non-rigid registration via intelligent adaptive feedback control},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PanVA: Pangenomic variant analysis. <em>TVCG</em>,
<em>30</em>(8), 4895–4909. (<a
href="https://doi.org/10.1109/TVCG.2023.3282364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genomics researchers increasingly use multiple reference genomes to comprehensively explore genetic variants underlying differences in detectable characteristics between organisms. Pangenomes allow for an efficient data representation of multiple related genomes and their associated metadata. However, current visual analysis approaches for exploring these complex genotype-phenotype relationships are often based on single reference approaches or lack adequate support for interpreting the variants in the genomic context with heterogeneous (meta)data. This design study introduces PanVA, a visual analytics design for pangenomic variant analysis developed with the active participation of genomics researchers. The design uniquely combines tailored visual representations with interactions such as sorting, grouping, and aggregation, allowing users to navigate and explore different perspectives on complex genotype-phenotype relations. Through evaluation in the context of plants and pathogen research, we show that PanVA helps researchers explore variants in genes and generate hypotheses about their role in phenotypic variation.},
  archive      = {J_TVCG},
  author       = {Astrid van den Brandt and Eef M. Jonkheer and Dirk-Jan M. van Workum and Huub van de Wetering and Sandra Smit and Anna Vilanova},
  doi          = {10.1109/TVCG.2023.3282364},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4895-4909},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PanVA: Pangenomic variant analysis},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using multi-level consistency learning for
partial-to-partial point cloud registration. <em>TVCG</em>,
<em>30</em>(8), 4881–4894. (<a
href="https://doi.org/10.1109/TVCG.2023.3280171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud registration is a basic task in computer vision and computer graphics. Recently, deep learning-based end-to-end methods have made great progress in this field. One of the challenges of these methods is to deal with partial-to-partial registration tasks. In this work, we propose a novel end-to-end framework called MCLNet that makes full use of multi-level consistency for point cloud registration. First, the point-level consistency is exploited to prune points located outside overlapping regions. Second, we propose a multi-scale attention module to perform consistency learning at the correspondence-level for obtaining reliable correspondences. To further improve the accuracy of our method, we propose a novel scheme to estimate the transformation based on geometric consistency between correspondences. Compared to baseline methods, experimental results show that our method performs well on smaller-scale data, especially with exact matches. The reference time and memory footprint of our method are relatively balanced, which is more beneficial for practical applications.},
  archive      = {J_TVCG},
  author       = {Boyuan Tan and Hongxing Qin and Xiaoxi Zhang and Yiqun Wang and Tao Xiang and Baoquan Chen},
  doi          = {10.1109/TVCG.2023.3280171},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4881-4894},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Using multi-level consistency learning for partial-to-partial point cloud registration},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Complete 3D relationships extraction modality alignment
network for 3D dense captioning. <em>TVCG</em>, <em>30</em>(8),
4867–4880. (<a href="https://doi.org/10.1109/TVCG.2023.3279204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D dense captioning aims to semantically describe each object detected in a 3D scene, which plays a significant role in 3D scene understanding. Previous works lack a complete definition of 3D spatial relationships and the directly integrate visual and language modalities, thus ignoring the discrepancies between the two modalities. To address these issues, we propose a novel complete 3D relationship extraction modality alignment network, which consists of three steps: 3D object detection, complete 3D relationships extraction, and modality alignment caption. To comprehensively capture the 3D spatial relationship features, we define a complete set of 3D spatial relationships, including the local spatial relationship between objects and the global spatial relationship between each object and the entire scene. To this end, we propose a complete 3D relationships extraction module based on message passing and self-attention to mine multi-scale spatial relationship features and inspect the transformation to obtain features in different views. In addition, we propose the modality alignment caption module to fuse multi-scale relationship features and generate descriptions to bridge the semantic gap from the visual space to the language space with the prior information in the word embedding, and help generate improved descriptions for the 3D scene. Extensive experiments demonstrate that the proposed model outperforms the state-of-the-art methods on the ScanRefer and Nr3D datasets.},
  archive      = {J_TVCG},
  author       = {Aihua Mao and Zhi Yang and Wanxin Chen and Ran Yi and Yong-Jin Liu},
  doi          = {10.1109/TVCG.2023.3279204},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4867-4880},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Complete 3D relationships extraction modality alignment network for 3D dense captioning},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MediVizor: Visual mediation analysis of nominal variables.
<em>TVCG</em>, <em>30</em>(8), 4853–4866. (<a
href="https://doi.org/10.1109/TVCG.2023.3282801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis is crucial for diagnosing indirect causal relations in many scientific fields. However, mediation analysis of nominal variables requires examining and comparing multiple total effects and their corresponding direct/indirect causal effects derived from mediation models. This process is tedious and challenging to achieve with classical analysis tools such as Excel tables. In this study, we worked closely with experts from two scientific domains to design MediVizor, a visualization system that enables experts to conduct visual mediation analysis of nominal variables. The visualization design allows users to browse and compare multiple total effects together with the direct/indirect effects that compose them. The design also allows users to examine to what extent the positive and negative direct/indirect effects contribute to and reduce the total effects, respectively. We conducted two case studies separately with the experts from the two domains, sports and communication science, and a user study with common users to evaluate the system and design. The positive feedback from experts and common users demonstrates the effectiveness and generalizability of the system.},
  archive      = {J_TVCG},
  author       = {Ji Lan and Zheng Zhou and Xiao Xie and Yanhong Wu and Hui Zhang and Yingcai Wu},
  doi          = {10.1109/TVCG.2023.3282801},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4853-4866},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MediVizor: Visual mediation analysis of nominal variables},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Taming reversible halftoning via predictive luminance.
<em>TVCG</em>, <em>30</em>(8), 4841–4852. (<a
href="https://doi.org/10.1109/TVCG.2023.3278691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional halftoning usually drops colors when dithering images with binary dots, which makes it difficult to recover the original color information. We proposed a novel halftoning technique that converts a color image into a binary halftone with full restorability to its original version. Our novel base halftoning technique consists of two convolutional neural networks (CNNs) to produce the reversible halftone patterns, and a noise incentive block (NIB) to mitigate the flatness degradation issue of CNNs. Furthermore, to tackle the conflicts between the blue-noise quality and restoration accuracy in our novel base method, we proposed a predictor-embedded approach to offload predictable information from the network, which in our case is the luminance information resembling from the halftone pattern. Such an approach allows the network to gain more flexibility to produce halftones with better blue-noise quality without compromising the restoration quality. Detailed studies on the multiple-stage training method and loss weightings have been conducted. We have compared our predictor-embedded method and our novel method regarding spectrum analysis on halftone, halftone accuracy, restoration accuracy, and the data embedding studies. Our entropy evaluation evidences our halftone contains less encoding information than our novel base method. The experiments show our predictor-embedded method gains more flexibility to improve the blue-noise quality of halftones and maintains a comparable restoration quality with a higher tolerance for disturbances.},
  archive      = {J_TVCG},
  author       = {Cheuk-Kit Lau and Menghan Xia and Tien-Tsin Wong},
  doi          = {10.1109/TVCG.2023.3278691},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4841-4852},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Taming reversible halftoning via predictive luminance},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards visualization thumbnail designs that entice reading
data-driven articles. <em>TVCG</em>, <em>30</em>(8), 4825–4840. (<a
href="https://doi.org/10.1109/TVCG.2023.3278304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As online news increasingly include data journalism, there is a corresponding increase in the incorporation of visualization in article thumbnail images. However, little research exists on the design rationale for visualization thumbnails, such as resizing, cropping, simplifying, and embellishing charts that appear within the body of the associated article. Therefore, in this paper we aim to understand these design choices and determine what makes a visualization thumbnail inviting and interpretable. To this end, we first survey visualization thumbnails collected online and discuss visualization thumbnail practices with data journalists and news graphics designers. Based on the survey and discussion results, we then define a design space for visualization thumbnails and conduct a user study with four types of visualization thumbnails derived from the design space. The study results indicate that different chart components play different roles in attracting reader attention and enhancing reader understandability of the visualization thumbnails. We also find various thumbnail design strategies for effectively combining the charts’ components, such as a data summary with highlights and data labels, and a visual legend with text labels and Human Recognizable Objects (HROs), into thumbnails. Ultimately, we distill our findings into design implications that allow effective visualization thumbnail designs for data-rich news articles. Our work can thus be seen as a first step toward providing structured guidance on how to design compelling thumbnails for data stories.},
  archive      = {J_TVCG},
  author       = {Hwiyeon Kim and Joohee Kim and Yunha Han and Hwajung Hong and Oh-Sang Kwon and Young-Woo Park and Niklas Elmqvist and Sungahn Ko and Bum Chul Kwon},
  doi          = {10.1109/TVCG.2023.3278304},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4825-4840},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Towards visualization thumbnail designs that entice reading data-driven articles},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tailorable sampling for progressive visual analytics.
<em>TVCG</em>, <em>30</em>(8), 4809–4824. (<a
href="https://doi.org/10.1109/TVCG.2023.3278084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Progressive visual analytics (PVA) allows analysts to maintain their flow during otherwise long-running computations by producing early, incomplete results that refine over time, for example, by running the computation over smaller partitions of the data. These partitions are created using sampling, whose goal it isto draw samples of the dataset such that the progressive visualization becomes as useful as possible as soon as possible. What makes the visualization useful depends on the analysis task and, accordingly, some task-specific sampling methods have been proposed for PVA to address this need. However, as analysts see more and more of their data during the progression, the analysis task at hand often changes, which means that analysts need to restart the computation to switch the sampling method, causing them to lose their analysis flow. This poses a clear limitation to the proposed benefits of PVA. Hence, we propose a pipeline for PVA-sampling that allows tailoring the data partitioning to analysis scenarios by switching out modules in a way that does not require restarting the analysis. To that end, we characterize the problem of PVA-sampling, formalize the pipeline in terms of data structures, discuss on-the-fly tailoring, and present additional examples demonstrating its usefulness.},
  archive      = {J_TVCG},
  author       = {Marius Hogräfer and Hans-Jörg Schulz},
  doi          = {10.1109/TVCG.2023.3278084},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4809-4824},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Tailorable sampling for progressive visual analytics},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pose-aware attention network for flexible motion retargeting
by body part. <em>TVCG</em>, <em>30</em>(8), 4792–4808. (<a
href="https://doi.org/10.1109/TVCG.2023.3277918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion retargeting is a fundamental problem in computer graphics and computer vision. Existing approaches usually have many strict requirements, such as the source-target skeletons needing to have the same number of joints or share the same topology. To tackle this problem, we note that skeletons with different structure may have some common body parts despite the differences in joint numbers. Following this observation, we propose a novel, flexible motion retargeting framework. The key idea of our method is to regard the body part as the basic retargeting unit rather than directly retargeting the whole body motion. To enhance the spatial modeling capability of the motion encoder, we introduce a pose-aware attention network (PAN) in the motion encoding phase. The PAN is pose-aware since it can dynamically predict the joint weights within each body part based on the input pose, and then construct a shared latent space for each body part by feature pooling. Extensive experiments show that our approach can generate better motion retargeting results both qualitatively and quantitatively than state-of-the-art methods. Moreover, we also show that our framework can generate reasonable results even for a more challenging retargeting scenario, like retargeting between bipedal and quadrupedal skeletons because of the body part retargeting strategy and PAN.},
  archive      = {J_TVCG},
  author       = {Lei Hu and Zihao Zhang and Chongyang Zhong and Boyuan Jiang and Shihong Xia},
  doi          = {10.1109/TVCG.2023.3277918},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4792-4808},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Pose-aware attention network for flexible motion retargeting by body part},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic 3D teeth reconstruction from five intra-oral
photos using parametric teeth model. <em>TVCG</em>, <em>30</em>(8),
4780–4791. (<a href="https://doi.org/10.1109/TVCG.2023.3277914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orthodontic treatment is a lengthy process that requires regular in-person dental monitoring, making remote dental monitoring a viable alternative when face-to-face consultation is not possible. In this study, we propose an improved 3D teeth reconstruction framework that automatically restores the shape, arrangement, and dental occlusion of upper and lower teeth from five intra-oral photographs to aid orthodontists in visualizing the condition of patients in virtual consultations. The framework comprises a parametric model that leverages statistical shape modeling to describe the shape and arrangement of teeth, a modified U-net that extracts teeth contours from intra-oral images, and an iterative process that alternates between finding point correspondences and optimizing a compound loss function to fit the parametric teeth model to predicted teeth contours. We perform a five-fold cross-validation on a dataset of 95 orthodontic cases and report an average Chamfer distance of 1.0121 $mm^{2}$ and an average Dice similarity coefficient of 0.7672 on all the test samples in the cross-validation, demonstrating a significant improvement compared with the previous work. Our teeth reconstruction framework provides a feasible solution for visualizing 3D teeth models in remote orthodontic consultations.},
  archive      = {J_TVCG},
  author       = {Yizhou Chen and Shuojie Gao and Puxun Tu and Xiaojun Chen},
  doi          = {10.1109/TVCG.2023.3277914},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4780-4791},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Automatic 3D teeth reconstruction from five intra-oral photos using parametric teeth model},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Does multi-actuator vibrotactile feedback within tangible
objects enrich VR manipulation? <em>TVCG</em>, <em>30</em>(8),
4767–4779. (<a href="https://doi.org/10.1109/TVCG.2023.3279398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rich, informative and realistic haptic feedback is key to enhancing Virtual Reality (VR) manipulation. Tangible objects provide convincing grasping and manipulation interactions with haptic feedback of e.g., shape, mass and texture properties. But these properties are static, and cannot respond to interactions in the virtual environment. On the other hand, vibrotactile feedback provides the opportunity for delivering dynamic cues rendering many different contact properties, such as impacts, object vibrations or textures. Handheld objects or controllers in VR are usually restricted to vibrating in a monolithic fashion. In this article, we investigate how spatialiazing vibrotactile cues within handheld tangibles could enable a wider range of sensations and interactions. We conduct a set of perception studies, investigating the extent to which spatialization of vibrotactile feedback within tangible objects is possible as well as the benefits of proposed rendering schemes leveraging multiple actuators in VR. Results show that vibrotactile cues from localized actuators can be discriminated and are beneficial for certain rendering schemes.},
  archive      = {J_TVCG},
  author       = {Pierre-Antoine Cabaret and Thomas Howard and Guillaume Gicquel and Claudio Pacchierotti and Marie Babel and Maud Marchal},
  doi          = {10.1109/TVCG.2023.3279398},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4767-4779},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Does multi-actuator vibrotactile feedback within tangible objects enrich VR manipulation?},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Audio2Gestures: Generating diverse gestures from audio.
<em>TVCG</em>, <em>30</em>(8), 4752–4766. (<a
href="https://doi.org/10.1109/TVCG.2023.3276973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People may perform diverse gestures affected by various mental and physical factors when speaking the same sentences. This inherent one-to-many relationship makes co-speech gesture generation from audio particularly challenging. Conventional CNNs/RNNs assume one-to-one mapping, and thus tend to predict the average of all possible target motions, easily resulting in plain/boring motions during inference. So we propose to explicitly model the one-to-many audio-to-motion mapping by splitting the cross-modal latent code into shared code and motion-specific code. The shared code is expected to be responsible for the motion component that is more correlated to the audio while the motion-specific code is expected to capture diverse motion information that is more independent of the audio. However, splitting the latent code into two parts poses extra training difficulties. Several crucial training losses/strategies, including relaxed motion loss, bicycle constraint, and diversity loss, are designed to better train the VAE. Experiments on both 3D and 2D motion datasets verify that our method generates more realistic and diverse motions than previous state-of-the-art methods, quantitatively and qualitatively. Besides, our formulation is compatible with discrete cosine transformation (DCT) modeling and other popular backbones (i.e., RNN, Transformer). As for motion losses and quantitative motion evaluation, we find structured losses/metrics (e.g. STFT) that consider temporal and/or spatial context complement the most commonly used point-wise losses (e.g. PCK), resulting in better motion dynamics and more nuanced motion details. Finally, we demonstrate that our method can be readily used to generate motion sequences with user-specified motion clips on the timeline.},
  archive      = {J_TVCG},
  author       = {Jing Li and Di Kang and Wenjie Pei and Xuefei Zhe and Ying Zhang and Linchao Bao and Zhenyu He},
  doi          = {10.1109/TVCG.2023.3276973},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4752-4766},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Audio2Gestures: Generating diverse gestures from audio},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PlanNet: A generative model for component-based plan
synthesis. <em>TVCG</em>, <em>30</em>(8), 4739–4751. (<a
href="https://doi.org/10.1109/TVCG.2023.3275200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel generative model named as PlanNet for component-based plan synthesis. The proposed model consists of three modules, a wave function collapse algorithm to create large-scale wireframe patterns as the embryonic forms of floor plans, and two deep neural networks to outline the plausible boundary from each squared pattern, and meanwhile estimate the potential semantic labels for the components. In this manner, we use PlanNet to generate a large-scale component-based plan dataset with 10 K examples. Given an input boundary, our method retrieves dataset plan examples with similar configurations to the input, and then transfers the space layout from a user-selected plan example to the input. Benefiting from our interactive workflow, users can recursively subdivide individual components of the plans to enrich the plan contents, thus designing more complex plans for larger scenes. Moreover, our method also adopts a random selection algorithm to make the variations on semantic labels of the plan components, aiming at enriching the 3D scenes that the output plans are suited for. To demonstrate the quality and versatility of our generative model, we conduct intensive experiments, including the analysis of plan examples and their evaluations, plan synthesis with both hard and soft boundary constraints, and 3D scenes designed with the plan subdivision on different scales. We also compare our results with the state-of-the-art floor plan synthesis methods to validate the feasibility and efficacy of the proposed generative model.},
  archive      = {J_TVCG},
  author       = {Qiang Fu and Shuhan He and Xueming Li and Hongbo Fu},
  doi          = {10.1109/TVCG.2023.3275200},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4739-4751},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PlanNet: A generative model for component-based plan synthesis},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D gamut morphing for non-rectangular multi-projector
displays. <em>TVCG</em>, <em>30</em>(8), 4724–4738. (<a
href="https://doi.org/10.1109/TVCG.2023.3277436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a spatially augmented reality system, multiple projectors are tiled on a complex shaped surface to create a seamless display on it. This has several applications in visualization, gaming, education and entertainment. The main challenges in creating seamless and undistorted imagery on such complex shaped surfaces are geometric registration and color correction. Prior methods that provide solutions for the spatial color variation in multi-projector displays assume rectangular overlap regions across the projectors that is possible only on flat surfaces with extremely constrained projector placement. In this article, we present a novel and fully automated method for removing color variations in a multi-projector display on arbitrary shaped smooth surfaces using a general color gamut morphing algorithm that can handle any arbitrarily shaped overlap between the projectors and assures imperceptible color variations across the display surface.},
  archive      = {J_TVCG},
  author       = {Mahdi Abbaspour Tehrani and Muhammad Twaha Ibrahim and Aditi Majumder and M. Gopi},
  doi          = {10.1109/TVCG.2023.3277436},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4724-4738},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {3D gamut morphing for non-rectangular multi-projector displays},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hypothesis testing for progressive kernel estimation and VCM
framework. <em>TVCG</em>, <em>30</em>(8), 4709–4723. (<a
href="https://doi.org/10.1109/TVCG.2023.3274595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying an appropriate radius for unbiased kernel estimation is crucial for the efficiency of radiance estimation. However, determining both the radius and unbiasedness still faces big challenges. In this paper, we first propose a statistical model of photon samples and associated contributions for progressive kernel estimation, under which the kernel estimation is unbiased if the null hypothesis of this statistical model stands. Then, we present a method to decide whether to reject the null hypothesis about the statistical population (i.e., photon samples) by the F-test in the Analysis of Variance. Hereby, we implement a progressive photon mapping (PPM) algorithm, wherein the kernel radius is determined by this hypothesis test for unbiased radiance estimation. Second, we propose VCM+, a reinforcement of Vertex Connection and Merging (VCM), and derive its theoretically unbiased formulation. VCM+ combines hypothesis testing-based PPM with bidirectional path tracing (BDPT) via multiple importance sampling (MIS), wherein our kernel radius can leverage the contributions from PPM and BDPT. We test our new algorithms, improved PPM and VCM+, on diverse scenarios with different lighting settings. The experimental results demonstrate that our method can alleviate light leaks and visual blur artifacts of prior radiance estimate algorithms. We also evaluate the asymptotic performance of our approach and observe an overall improvement over the baseline in all testing scenarios.},
  archive      = {J_TVCG},
  author       = {Zehui Lin and Chenxiao Hu and Jinzhu Jia and Sheng Li},
  doi          = {10.1109/TVCG.2023.3274595},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4709-4723},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Hypothesis testing for progressive kernel estimation and VCM framework},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MPMNet: A data-driven MPM framework for dynamic fluid-solid
interaction. <em>TVCG</em>, <em>30</em>(8), 4694–4708. (<a
href="https://doi.org/10.1109/TVCG.2023.3272156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-accuracy, high-efficiency physics-based fluid-solid interaction is essential for reality modeling and computer animation in online games or real-time Virtual Reality (VR) systems. However, the large-scale simulation of incompressible fluid and its interaction with the surrounding solid environment is either time-consuming or suffering from the reduced time/space resolution due to the complicated iterative nature pertinent to numerical computations of involved Partial Differential Equations (PDEs). In recent years, we have witnessed significant growth in exploring a different, alternative data-driven approach to addressing some of the existing technical challenges in conventional model-centric graphics and animation methods. This article showcases some of our exploratory efforts in this direction. One technical concern of our research is to address the central key challenge of how to best construct the numerical solver effectively and how to best integrate spatiotemporal/dimensional neural networks with the available MPM&#39;s pressure solvers. In particular, we devise the MPMNet, a hybrid data-driven framework supporting the popular and powerful MPM, to combine the comprehensive properties of MPM in numerically handling physical behaviors ranging from fluid to deformable solids and the high efficiency of data-driven models. At the architectural level, our MPMNet comprises three primary components: A data processing module to describe the physical properties by way of the input fields; A deep neural network group to learn the spatiotemporal features; And an iterative refinement process to continue to reduce possible numerical errors. The goal of these special technical developments is to aim at involved numerical acceleration while preserving physical accuracy, realizing efficient and accurate fluid-solid interactions in a data-driven fashion. The extensive experimental results verify that our MPMNet can tremendously speed up the computation compared with the popular numerical methods as the complexity of interaction scenes increases while better retaining the numerical accuracy.},
  archive      = {J_TVCG},
  author       = {Jin Li and Yang Gao and Ju Dai and Shuai Li and Aimin Hao and Hong Qin},
  doi          = {10.1109/TVCG.2023.3272156},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4694-4708},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MPMNet: A data-driven MPM framework for dynamic fluid-solid interaction},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Patching non-uniform extraordinary points. <em>TVCG</em>,
<em>30</em>(8), 4683–4693. (<a
href="https://doi.org/10.1109/TVCG.2023.3271669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smooth surfaces from an arbitrary topological control grid have been widely studied, which are mostly generalized from splines with uniform knot intervals. These methods fail to work well on extraordinary points (EPs) whose edges have varying knot intervals. This article presents a patching solution for arbitrary topological 2-manifold control grid with non-uniform knots that defines one bi-cubic Bézier patch per control grid face except those faces with EPs. Experimental results demonstrate that the new solution can improve the surface quality for non-uniform parameterization. Applications in surface reconstruction, arbitrary sharp features on the complex surface and tool path planning for the new surface representation are also provided in the paper.},
  archive      = {J_TVCG},
  author       = {Yi-Fei Feng and Li-Yong Shen and Xin Li and Chun-Ming Yuan and Xing Jiang},
  doi          = {10.1109/TVCG.2023.3271669},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4683-4693},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Patching non-uniform extraordinary points},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leaning-based interfaces improve simultaneous locomotion and
object interaction in VR compared to the handheld controller.
<em>TVCG</em>, <em>30</em>(8), 4665–4682. (<a
href="https://doi.org/10.1109/TVCG.2023.3275111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical walking is often considered the gold standard for VR travel whenever feasible. However, limited free-space walking areas in the real-world do not allow exploring larger-scale virtual environments by actual walking. Therefore, users often require handheld controllers for navigation, which can reduce believability, interfere with simultaneous interaction tasks, and exacerbate adverse effects such as motion sickness and disorientation. To investigate alternative locomotion options, we compared handheld Controller (thumbstick-based) and physical walking versus a seated ( HeadJoystick ) and standing/stepping ( NaviBoard ) leaning-based locomotion interface, where seated/standing users travel by moving their head toward the target direction. Rotations were always physically performed. To compare these interfaces, we designed a novel simultaneous locomotion and object interaction task, where users needed to keep touching the center of upward moving target balloons with their virtual lightsaber, while simultaneously staying inside a horizontally moving enclosure. Walking resulted in the best locomotion, interaction, and combined performances while the controller performed worst. Leaning-based interfaces improved user experience and performance compared to Controller, especially when standing/stepping using NaviBoard, but did not reach walking performance. That is, leaning-based interfaces HeadJoystick (sitting) and NaviBoard (standing) that provided additional physical self-motion cues compared to controller improved enjoyment, preference, spatial presence, vection intensity, motion sickness, as well as performance for locomotion, object interaction, and combined locomotion and object interaction. Our results also showed that less embodied interfaces (and in particular the controller) caused a more pronounced performance deterioration when increasing locomotion speed. Moreover, observed differences between our interfaces were not affected by repeated interface usage.},
  archive      = {J_TVCG},
  author       = {Abraham M. Hashemian and Ashu Adhikari and Ivan A. Aguilar and Ernst Kruijff and Markus von der Heyde and Bernhard E. Riecke},
  doi          = {10.1109/TVCG.2023.3275111},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4665-4682},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Leaning-based interfaces improve simultaneous locomotion and object interaction in VR compared to the handheld controller},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward more comprehensive evaluations of 3D immersive
sketching, drawing, and painting. <em>TVCG</em>, <em>30</em>(8),
4648–4664. (<a href="https://doi.org/10.1109/TVCG.2023.3276291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To understand current practice and explore the potential for more comprehensive evaluations of 3D immersive sketching, drawing, and painting, we present a survey of evaluation methodologies used in existing 3D sketching research, a breakdown and discussion of important phases (sub-tasks) in the 3D sketching process, and a framework that suggests how these factors can inform evaluation strategies in future 3D sketching research. Existing evaluations identified in the survey are organized and discussed within three high-level categories: 1) evaluating the 3D sketching activity, 2) evaluating 3D sketching tools, and 3) evaluating 3D sketching artifacts. The new framework suggests targeting evaluations to one or more of these categories and identifying relevant user populations. In addition, building upon the discussion of the different phases of the 3D sketching process, the framework suggests to evaluate relevant sketching tasks, which may range from low-level perception and hand movements to high-level conceptual design. Finally, we discuss limitations and challenges that arise when evaluating 3D sketching, including a lack of standardization of evaluation methods and multiple, potentially conflicting, ways to evaluate the same task and user interface usability; we also identify opportunities for more holistic evaluations. We hope the results can contribute to accelerating research in this domain and, ultimately, broad adoption of immersive sketching systems.},
  archive      = {J_TVCG},
  author       = {Mayra Donaji Barrera Machuca and Johann Habakuk Israel and Daniel F. Keefe and Wolfgang Stuerzlinger},
  doi          = {10.1109/TVCG.2023.3276291},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4648-4664},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Toward more comprehensive evaluations of 3D immersive sketching, drawing, and painting},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effectiveness of area-to-value legends and grid lines in
contiguous area cartograms. <em>TVCG</em>, <em>30</em>(8), 4631–4647.
(<a href="https://doi.org/10.1109/TVCG.2023.3275925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A contiguous area cartogram is a geographic map in which the area of each region is proportional to numerical data (e.g., population size) while keeping neighboring regions connected. In this study, we investigated whether value-to-area legends (square symbols next to the values represented by the squares’ areas) and grid lines aid map readers in making better area judgments. We conducted an experiment to determine the accuracy, speed, and confidence with which readers infer numerical data values for the mapped regions. We found that, when only informed about the total numerical value represented by the whole cartogram without any legend, the distribution of estimates for individual regions was centered near the true value with substantial spread. Legends with grid lines significantly reduced the spread but led to a tendency to underestimate the values. Comparing differences between regions or between cartograms revealed that legends and grid lines slowed the estimation without improving accuracy. However, participants were more likely to complete the tasks when legends and grid lines were present, particularly when the area units represented by these features could be interactively selected. We recommend considering the cartogram&#39;s use case and purpose before deciding whether to include grid lines or an interactive legend.},
  archive      = {J_TVCG},
  author       = {Kelvin L. T. Fung and Simon T. Perrault and Michael T. Gastner},
  doi          = {10.1109/TVCG.2023.3275925},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4631-4647},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effectiveness of area-to-value legends and grid lines in contiguous area cartograms},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SceneFusion: Room-scale environmental fusion for efficient
traveling between separate virtual environments. <em>TVCG</em>,
<em>30</em>(8), 4615–4630. (<a
href="https://doi.org/10.1109/TVCG.2023.3271709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traveling between scenes has become a major requirement for navigation in numerous virtual reality (VR) social platforms and game applications, allowing users to efficiently explore multiple virtual environments (VEs). To facilitate scene transition, prevalent techniques such as instant teleportation and virtual portals have been extensively adopted. However, these techniques exhibit limitations when there is a need for frequent travel between separate VEs, particularly within indoor environments, resulting in low efficiency. In this article, we first analyze the design rationale for a novel navigation method supporting efficient travel between virtual indoor scenes. Based on the analysis, we introduce the SceneFusion technique that fuses separate virtual rooms into an integrated environment. SceneFusion enables users to perceive rich visual information from both rooms simultaneously, achieving high visual continuity and spatial awareness. While existing teleportation techniques passively transport users, SceneFusion allows users to actively access the fused environment using short-range locomotion techniques. User experiments confirmed that SceneFusion outperforms instant teleportation and virtual portal techniques in terms of efficiency, workload, and preference for both single-user exploration and multi-user collaboration tasks in separate VEs. Thus, SceneFusion presents an effective solution for seamless traveling between virtual indoor scenes.},
  archive      = {J_TVCG},
  author       = {Miao Wang and Yi-Jun Li and Jinchuan Shi and Frank Steinicke},
  doi          = {10.1109/TVCG.2023.3271709},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4615-4630},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SceneFusion: Room-scale environmental fusion for efficient traveling between separate virtual environments},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Projective peridynamic modeling of hyperelastic membranes
with contact. <em>TVCG</em>, <em>30</em>(8), 4601–4614. (<a
href="https://doi.org/10.1109/TVCG.2023.3271511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time simulation of hyperelastic membranes like cloth still faces a lot of challenges, such as hyperplasticity modeling and contact handling. In this study, we propose projective peridynamics that uses a local-global strategy to enable fast and robust simulation of hyperelastic membranes with contact. In the global step, we propose a semi-implicit strategy to linearize the governing equation for hyperelastic materials that are modeled with peridynamics. By decomposing the first Piola-Kirchhoff stress tensor into a positive and a negative part, successive substitutions can be taken to solve the nonlinear problems. Convergence is guaranteed by further addressing the overshooting problem. Since our global step solve requires no energy summation and dot product operations over the entire problem, it fits into GPU implementation perfectly. In the local step, we further present a GPU-friendly gradient descent method to prevent interpenetration by solving an optimization problem independently. Putting the global and local solves together, experiments show that our method is robust and efficient in simulating complex models of membranes involving hyperelastic materials and contact.},
  archive      = {J_TVCG},
  author       = {Zixuan Lu and Xiaowei He and Yuzhong Guo and Xuehui Liu and Huamin Wang},
  doi          = {10.1109/TVCG.2023.3271511},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4601-4614},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Projective peridynamic modeling of hyperelastic membranes with contact},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Measuring embodiment: Movement complexity and the impact of
personal characteristics. <em>TVCG</em>, <em>30</em>(8), 4588–4600. (<a
href="https://doi.org/10.1109/TVCG.2023.3270725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A user&#39;s personal experiences and characteristics may impact the strength of an embodiment illusion and affect resulting behavioral changes in unknown ways. This paper presents a novel re-analysis of two fully-immersive embodiment user-studies (n = 189 and n = 99) using structural equation modeling, to test the effects of personal characteristics on subjective embodiment. Results demonstrate that individual characteristics (gender, participation in science, technology, engineering or math – Experiment 1, age, video gaming experience – Experiment 2) predicted differing self-reported experiences of embodiment Results also indicate that increased self-reported embodiment predicts environmental response, in this case faster and more accurate responses within the virtual environment. Importantly, head-tracking data is shown to be an effective objective measure for predicting embodiment, without requiring researchers to utilize additional equipment.},
  archive      = {J_TVCG},
  author       = {Tabitha C. Peck and Jessica J. Good},
  doi          = {10.1109/TVCG.2023.3270725},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4588-4600},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Measuring embodiment: Movement complexity and the impact of personal characteristics},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Topology optimization via spatially-varying TPMS.
<em>TVCG</em>, <em>30</em>(8), 4570–4587. (<a
href="https://doi.org/10.1109/TVCG.2023.3268068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural design with multi-family triply periodic minimal surfaces (TPMS) is a meaningful work that can combine the advantages of different types of TPMS. However, very few methods consider the influence of the blending of different TPMS on structural performance, and the manufacturability of final structure. Therefore, this work proposes a method to design manufacturable microstructures with topology optimization (TO) based on spatially-varying TPMS. In our method, different types of TPMS are simultaneously considered in the optimization to maximize the performance of designed microstructure. The geometric and mechanical properties of the unit cells generated with TPMS, that is minimal surface lattice cell (MSLC), are analyzed to obtain the performance of different types of TPMS. In the designed microstructure, MSLCs of different types are smoothly blended with an interpolation method. To analyze the influence of deformed MSLCs on the performance of the final structure, the blending blocks are introduced to describe the connection cases between different types of MSLCs. The mechanical properties of deformed MSLCs are analyzed and applied in TO process to reduce the influence of deformed MSLCs on the performance of final structure. The infill resolution of MSLC within a given design domain is determined according to the minimal printable wall thickness of MSLC and structural stiffness. Both numerical and physical experimental results demonstrate the effectiveness of the proposed method.},
  archive      = {J_TVCG},
  author       = {Wenpeng Xu and Peng Zhang and Menglin Yu and Li Yang and Weiming Wang and Ligang Liu},
  doi          = {10.1109/TVCG.2023.3268068},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4570-4587},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Topology optimization via spatially-varying TPMS},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SceneDirector: Interactive scene synthesis by simultaneously
editing multiple objects in real-time. <em>TVCG</em>, <em>30</em>(8),
4558–4569. (<a href="https://doi.org/10.1109/TVCG.2023.3268115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent tools for creating synthetic scenes have been developed significantly in recent years. Existing techniques on interactive scene synthesis only incorporate a single object at every interaction, i.e., crafting a scene through a sequence of single-object insertions with user preferences. These techniques suggest objects by considering existent objects in the scene instead of fully picturing the eventual result, which is inherently problematic since the sets of objects to be inserted are seldom fixed during interactive processes. In this article, we introduce SceneDirector, a novel interactive scene synthesis tool to help users quickly picture various potential synthesis results by simultaneously editing groups of objects. Specifically, groups of objects are rearranged in real-time with respect to a position of an object specified by a mouse cursor or gesture, i.e., a movement of a single object would trigger the rearrangement of the existing object group, the insertions of potentially appropriate objects, and the removal of redundant objects. To achieve this, we first propose an idea of coherent group set which expresses various concepts of layout strategies. Subsequently, we present layout attributes, where users can adjust how objects are arranged by tuning the weights of the attributes. Thus, our method gives users intuitive control of both how to arrange groups of objects and where to place them. Through extensive experiments and two applications, we demonstrate the potentiality of our framework and how it enables concurrently effective and efficient interactions of editing groups of objects.},
  archive      = {J_TVCG},
  author       = {Shao-Kui Zhang and Hou Tam and Yike Li and Ke-Xin Ren and Hongbo Fu and Song-Hai Zhang},
  doi          = {10.1109/TVCG.2023.3268115},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4558-4569},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SceneDirector: Interactive scene synthesis by simultaneously editing multiple objects in real-time},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TopicRefiner: Coherence-guided steerable LDA for visual
topic enhancement. <em>TVCG</em>, <em>30</em>(8), 4542–4557. (<a
href="https://doi.org/10.1109/TVCG.2023.3266890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a new Human-steerable Topic Modeling (HSTM) technique. Unlike existing techniques commonly relying on matrix decomposition-based topic models, we extend LDA as the fundamental component for extracting topics. LDA&#39;s high popularity and technical characteristics, such as better topic quality and no need to cherry-pick terms to construct the document-term matrix, ensure better applicability. Our research revolves around two inherent limitations of LDA. First, the principle of LDA is complex. Its calculation process is stochastic and difficult to control. We thus give a weighting method to incorporate users’ refinements into the Gibbs sampling to control LDA. Second, LDA often runs on a corpus with massive terms and documents, forming a vast search space for users to find semantically relevant or irrelevant objects. We thus design a visual editing framework based on the coherence metric, proven to be the most consistent with human perception in assessing topic quality, to guide users’ interactive refinements. Cases on two open real-world datasets, participants’ performance in a user study, and quantitative experiment results demonstrate the usability and effectiveness of the proposed technique.},
  archive      = {J_TVCG},
  author       = {Huan Yang and Jie Li and Siming Chen},
  doi          = {10.1109/TVCG.2023.3266890},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4542-4557},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TopicRefiner: Coherence-guided steerable LDA for visual topic enhancement},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contrastive learning for joint normal estimation and point
cloud filtering. <em>TVCG</em>, <em>30</em>(8), 4527–4541. (<a
href="https://doi.org/10.1109/TVCG.2023.3263866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud filtering and normal estimation are two fundamental research problems in the 3D field. Existing methods usually perform normal estimation and filtering separately and often show sensitivity to noise and/or inability to preserve sharp geometric features such as corners and edges. In this article, we propose a novel deep learning method to jointly estimate normals and filter point clouds. We first introduce a 3D patch based contrastive learning framework, with noise corruption as an augmentation, to train a feature encoder capable of generating faithful representations of point cloud patches while remaining robust to noise. These representations are consumed by a simple regression network and supervised by a novel joint loss, simultaneously estimating point normals and displacements that are used to filter the patch centers. Experimental results show that our method well supports the two tasks simultaneously and preserves sharp features and fine details. It generally outperforms state-of-the-art techniques on both tasks.},
  archive      = {J_TVCG},
  author       = {Dasith de Silva Edirimuni and Xuequan Lu and Gang Li and Antonio Robles-Kelly},
  doi          = {10.1109/TVCG.2023.3263866},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4527-4541},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Contrastive learning for joint normal estimation and point cloud filtering},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ANISE: Assembly-based neural implicit surface
reconstruction. <em>TVCG</em>, <em>30</em>(8), 4514–4526. (<a
href="https://doi.org/10.1109/TVCG.2023.3265306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present ANISE, a method that reconstructs a 3D shape from partial observations (images or sparse point clouds) using a part-aware neural implicit shape representation. The shape is formulated as an assembly of neural implicit functions, each representing a different part instance. In contrast to previous approaches, the prediction of this representation proceeds in a coarse-to-fine manner. Our model first reconstructs a structural arrangement of the shape in the form of geometric transformations of its part instances. Conditioned on them, the model predicts part latent codes encoding their surface geometry. Reconstructions can be obtained in two ways: (i) by directly decoding the part latent codes to part implicit functions, then combining them into the final shape; or (ii) by using part latents to retrieve similar part instances in a part database and assembling them in a single shape. We demonstrate that, when performing reconstruction by decoding part representations into implicit functions, our method achieves state-of-the-art part-aware reconstruction results from both images and sparse point clouds. When reconstructing shapes by assembling parts retrieved from a dataset, our approach significantly outperforms traditional shape retrieval methods even when significantly restricting the database size. We present our results in well-known sparse point cloud reconstruction and single-view reconstruction benchmarks.},
  archive      = {J_TVCG},
  author       = {Dmitry Petrov and Matheus Gadelha and Radomír Měch and Evangelos Kalogerakis},
  doi          = {10.1109/TVCG.2023.3265306},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4514-4526},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ANISE: Assembly-based neural implicit surface reconstruction},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A visual analytics conceptual framework for explorable and
steerable partial dependence analysis. <em>TVCG</em>, <em>30</em>(8),
4497–4513. (<a href="https://doi.org/10.1109/TVCG.2023.3263739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning techniques are a driving force for research in various fields, from credit card fraud detection to stock analysis. Recently, a growing interest in increasing human involvement has emerged, with the primary goal of improving the interpretability of machine learning models. Among different techniques, Partial Dependence Plots (PDP) represent one of the main model-agnostic approaches for interpreting how the features influence the prediction of a machine learning model. However, its limitations (i.e., visual interpretation, aggregation of heterogeneous effects, inaccuracy, and computability) could complicate or misdirect the analysis. Moreover, the resulting combinatorial space can be challenging to explore both computationally and cognitively when analyzing the effects of more features at the same time. This article proposes a conceptual framework that enables effective analysis workflows, mitigating state-of-the-art limitations. The proposed framework allows for exploring and refining computed partial dependences, observing incrementally accurate results, and steering the computation of new partial dependences on user-selected subspaces of the combinatorial and intractable space. With this approach, the user can save both computational and cognitive costs, in contrast with the standard monolithic approach that computes all the possible combinations of features on all their domains in batch. The framework is the result of a careful design process involving experts’ knowledge during its validation and informed the development of a prototype, W4SP 1 , that demonstrates its applicability traversing its different paths. A case study shows the advantages of the proposed approach.},
  archive      = {J_TVCG},
  author       = {Marco Angelini and Graziano Blasilli and Simone Lenti and Giuseppe Santucci},
  doi          = {10.1109/TVCG.2023.3263739},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4497-4513},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A visual analytics conceptual framework for explorable and steerable partial dependence analysis},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An overview of enhancing distance learning through emerging
augmented and virtual reality technologies. <em>TVCG</em>,
<em>30</em>(8), 4480–4496. (<a
href="https://doi.org/10.1109/TVCG.2023.3264577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although distance learning presents a number of interesting educational advantages as compared to in-person instruction, it is not without its downsides. We first assess the educational challenges presented by distance learning as a whole and identify 4 main challenges that distance learning currently presents as compared to in-person instruction: the lack of social interaction, reduced student engagement and focus, reduced comprehension and information retention, and the lack of flexible and customizable instructor resources. After assessing each of these challenges in-depth, we examine how AR/VR technologies might serve to address each challenge along with their current shortcomings, and finally outline the further research that is required to fully understand the potential of AR/VR technologies as they apply to distance learning.},
  archive      = {J_TVCG},
  author       = {Elizabeth Childs and Ferzam Mohammad and Logan Stevens and Hugo Burbelo and Amanuel Awoke and Nicholas Rewkowski and Dinesh Manocha},
  doi          = {10.1109/TVCG.2023.3264577},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4480-4496},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {An overview of enhancing distance learning through emerging augmented and virtual reality technologies},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RadVolViz: An information display-inspired transfer function
editor for multivariate volume visualization. <em>TVCG</em>,
<em>30</em>(8), 4464–4479. (<a
href="https://doi.org/10.1109/TVCG.2023.3263856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In volume visualization transfer functions are widely used for mapping voxel properties to color and opacity. Typically, volume density data are scalars which require simple 1D transfer functions to achieve this mapping. If the volume densities are vectors of three channels, one can straightforwardly map each channel to either red, green or blue, which requires a trivial extension of the 1D transfer function editor. We devise a new method that applies to volume data with more than three channels. These types of data often arise in scientific scanning applications, where the data are separated into spectral bands or chemical elements. Our method expands on prior work in which a multivariate information display, RadViz, was fused with a radial color map, in order to visualize multi-band 2D images. In this work, we extend this joint interface to blended volume rendering. The information display allows users to recognize the presence and value distribution of the multivariate voxels and the joint volume rendering display visualizes their spatial distribution. We design a set of operators and lenses that allow users to interactively control the mapping of the multivariate voxels to opacity and color. This enables users to isolate or emphasize volumetric structures with desired multivariate properties. Furthermore, it turns out that our method also enables more insightful displays even for RGB data. We demonstrate our method with three datasets obtained from spectral electron microscopy, high energy X-ray scanning, and atmospheric science.},
  archive      = {J_TVCG},
  author       = {Ayush Kumar and Xinyu Zhang and Huolin L. Xin and Hanfei Yan and Xiaojing Huang and Wei Xu and Klaus Mueller},
  doi          = {10.1109/TVCG.2023.3263856},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4464-4479},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {RadVolViz: An information display-inspired transfer function editor for multivariate volume visualization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image collage on arbitrary shape via shape-aware slicing and
optimization. <em>TVCG</em>, <em>30</em>(8), 4449–4463. (<a
href="https://doi.org/10.1109/TVCG.2023.3262039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image collage is a very useful tool for visualizing an image collection. Most of the existing methods and commercial applications for generating image collages are designed on simple shapes, such as rectangular and circular layouts. This greatly limits the use of image collages in some artistic and creative settings. Although there are some methods that can generate irregularly-shaped image collages, they often suffer from severe image overlapping and excessive blank space. This prevents such methods from being effective information communication tools. In this article, we present a shape slicing algorithm and an optimization scheme that can create image collages of arbitrary shapes in an informative and visually pleasing manner given an input shape and an image collection. To overcome the challenge of irregular shapes, we propose a novel algorithm, called Shape-Aware Slicing , which partitions the input shape into cells based on medial axis and binary slicing tree. Shape-Aware Slicing ,which is designed specifically for irregular shapes, takes human perception and shape structure into account to generate visually pleasing partitions. Then, the layout is optimized by analyzing input images with the goal of maximizing the total salient regions of the images. To evaluate our method, we conduct extensive experiments and compare our results against previous work. The evaluations show that our proposed algorithm can efficiently arrange image collections on irregular shapes and create visually superior results than prior work and existing commercial tools.},
  archive      = {J_TVCG},
  author       = {Dong-Yi Wu and Thi-Ngoc-Hanh Le and Sheng-Yi Yao and Yun-Chen Lin and Tong-Yee Lee},
  doi          = {10.1109/TVCG.2023.3262039},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4449-4463},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Image collage on arbitrary shape via shape-aware slicing and optimization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How does automation shape the process of narrative
visualization: A survey of tools. <em>TVCG</em>, <em>30</em>(8),
4429–4448. (<a href="https://doi.org/10.1109/TVCG.2023.3261320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, narrative visualization has gained much attention. Researchers have proposed different design spaces for various narrative visualization genres and scenarios to facilitate the creation process. As users’ needs grow and automation technologies advance, increasingly more tools have been designed and developed. In this study, we summarized six genres of narrative visualization (annotated charts, infographics, timelines &amp; storylines, data comics, scrollytelling &amp; slideshow, and data videos) based on previous research and four types of tools (design spaces, authoring tools, ML/AI-supported tools and ML/AI-generator tools) based on the intelligence and automation level of the tools. We surveyed 105 papers and tools to study how automation can progressively engage in visualization design and narrative processes to help users easily create narrative visualizations. This research aims to provide an overview of current research and development in the automation involvement of narrative visualization tools. We discuss key research problems in each category and suggest new opportunities to encourage further research in the related domain.},
  archive      = {J_TVCG},
  author       = {Qing Chen and Shixiong Cao and Jiazhe Wang and Nan Cao},
  doi          = {10.1109/TVCG.2023.3261320},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {8},
  number       = {8},
  pages        = {4429-4448},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {How does automation shape the process of narrative visualization: A survey of tools},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intrinsic omnidirectional image decomposition with
illumination pre-extraction. <em>TVCG</em>, <em>30</em>(7), 4416–4428.
(<a href="https://doi.org/10.1109/TVCG.2024.3366343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capturing an omnidirectional image with a 360-degree field of view entails capturing intricate spatial and lighting details of the scene. Consequently, existing intrinsic image decomposition methods face significant challenges when attempting to separate reflectance and shading components from a low dynamic range (LDR) omnidirectional images. To address this, our article introduces a novel method specifically designed for the intrinsic decomposition of omnidirectional images. Leveraging the unique characteristics of the 360-degree scene representation, we employ a pre-extraction technique to isolate specific illumination information. Subsequently, we establish new constraints based on these extracted details and the inherent characteristics of omnidirectional images. These constraints limit the illumination intensity range and incorporate spherical-based illumination variation. By formulating and solving an objective function that accounts for these constraints, our method achieves a more accurate separation of reflectance and shading components. Comprehensive qualitative and quantitative evaluations demonstrate the superiority of our proposed method over state-of-the-art intrinsic decomposition methods.},
  archive      = {J_TVCG},
  author       = {Rong-Kai Xu and Lei Zhang and Fang-Lue Zhang},
  doi          = {10.1109/TVCG.2024.3366343},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4416-4428},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Intrinsic omnidirectional image decomposition with illumination pre-extraction},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accelerating hyperbolic t-SNE. <em>TVCG</em>,
<em>30</em>(7), 4403–4415. (<a
href="https://doi.org/10.1109/TVCG.2024.3364841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need to understand the structure of hierarchical or high-dimensional data is present in a variety of fields. Hyperbolic spaces have proven to be an important tool for embedding computations and analysis tasks as their non-linear nature lends itself well to tree or graph data. Subsequently, they have also been used in the visualization of high-dimensional data, where they exhibit increased embedding performance. However, none of the existing dimensionality reduction methods for embedding into hyperbolic spaces scale well with the size of the input data. That is because the embeddings are computed via iterative optimization schemes and the computation cost of every iteration is quadratic in the size of the input. Furthermore, due to the non-linear nature of hyperbolic spaces, euclidean acceleration structures cannot directly be translated to the hyperbolic setting. This article introduces the first acceleration structure for hyperbolic embeddings, building upon a polar quadtree. We compare our approach with existing methods and demonstrate that it computes embeddings of similar quality in significantly less time. Implementation and scripts for the experiments can be found at https://graphics.tudelft.nl/accelerating-hyperbolic-tsne .},
  archive      = {J_TVCG},
  author       = {Martin Skrodzki and Hunter van Geffen and Nicolas F. Chaves-de-Plaza and Thomas Höllt and Elmar Eisemann and Klaus Hildebrandt},
  doi          = {10.1109/TVCG.2024.3364841},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4403-4415},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Accelerating hyperbolic t-SNE},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FROST-BRDF: A fast and robust optimal sampling technique for
BRDF acquisition. <em>TVCG</em>, <em>30</em>(7), 4390–4402. (<a
href="https://doi.org/10.1109/TVCG.2024.3355200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient and accurate BRDF acquisition of real world materials is a challenging research problem that requires sampling millions of incident light and viewing directions. To accelerate the acquisition process, one needs to find a minimal set of sampling directions such that the recovery of the full BRDF is accurate and robust given such samples. In this article, we formulate BRDF acquisition as a compressed sensing problem, where the sensing operator is one that performs sub-sampling of the BRDF signal according to a set of optimal sample directions. To solve this problem, we propose the Fast and Robust Optimal Sampling Technique (FROST) for designing a provably optimal sub-sampling operator that places light-view samples such that the recovery error is minimized. FROST casts the problem of designing an optimal sub-sampling operator for compressed sensing into a sparse representation formulation under the Multiple Measurement Vector (MMV) signal model. The proposed reformulation is exact, i.e. without any approximations, hence it converts an intractable combinatorial problem into one that can be solved with standard optimization techniques. As a result, FROST is accompanied by strong theoretical guarantees from the field of compressed sensing. We perform a thorough analysis of FROST-BRDF using a 10-fold cross-validation with publicly available BRDF datasets and show significant advantages compared to the state-of-the-art with respect to reconstruction quality. Finally, FROST is simple, both conceptually and in terms of implementation, it produces consistent results at each run, and it is at least two orders of magnitude faster than the prior art.},
  archive      = {J_TVCG},
  author       = {Ehsan Miandji and Tanaboon Tongbuasirilai and Saghi Hajisharif and Behnaz Kavoosighafi and Jonas Unger},
  doi          = {10.1109/TVCG.2024.3355200},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4390-4402},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FROST-BRDF: A fast and robust optimal sampling technique for BRDF acquisition},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The impact of elicitation and contrasting narratives on
engagement, recall and attitude change with news articles containing
data visualization. <em>TVCG</em>, <em>30</em>(7), 4375–4389. (<a
href="https://doi.org/10.1109/TVCG.2024.3355884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {News articles containing data visualizations play an important role in informing the public on issues ranging from public health to politics. Recent research on the persuasive appeal of data visualizations suggests that prior attitudes can be notoriously difficult to change. Inspired by an NYT article, we designed two experiments to evaluate the impact of elicitation and contrasting narratives on attitude change, recall, and engagement. We hypothesized that eliciting prior beliefs leads to more elaborative thinking that ultimately results in higher attitude change, better recall, and engagement. Our findings revealed that visual elicitation leads to higher engagement in terms of feelings of surprise. While there is an overall attitude change across all experiment conditions, we did not observe a significant effect of belief elicitation on attitude change. With regard to recall error, while participants in the draw trend elicitation exhibited significantly lower recall error than participants in the categorize trend condition, we found no significant difference in recall error when comparing elicitation conditions to no elicitation. In a follow-up study, we added contrasting narratives with the purpose of making the main visualization (communicating data on the focal issue) appear strikingly different. Compared to the results of Study 1, we found that contrasting narratives improved engagement in terms of surprise and interest but interestingly resulted in higher recall error and no significant change in attitude. We discuss the effects of elicitation and contrasting narratives in the context of topic involvement and the strengths of temporal trends encoded in the data visualization.},
  archive      = {J_TVCG},
  author       = {Milad Rogha and Subham Sah and Alireza Karduni and Douglas Markant and Wenwen Dou},
  doi          = {10.1109/TVCG.2024.3355884},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4375-4389},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The impact of elicitation and contrasting narratives on engagement, recall and attitude change with news articles containing data visualization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comparative evaluation of optical see-through augmented
reality in surgical guidance. <em>TVCG</em>, <em>30</em>(7), 4362–4374.
(<a href="https://doi.org/10.1109/TVCG.2023.3260001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During traditional surgeries, planning and instrument guidance is displayed on an external screen. Recent developments of augmented reality (AR) techniques can overcome obstacles including hand-eye discoordination and heavy mental load. Among these AR technologies, optical see-through (OST) schemes with stereoscopic displays can provide depth perception and retain the physical scene for safety considerations. However, limitations still exist in certain AR systems and the influence of these factors on surgical performance is yet to explore. To this end, experiments of multi-scale surgical tasks were carried out to compare head-mounted display (HMD) AR and autostereoscopic image overlay (AIO) AR, concerning objective performance and subjective evaluation. To solely analyze effects brought by display techniques, the tracking system in each included display system was identical and similar tracking accuracy was proved by a preliminary experiment. Focus and context rendering was utilized to enhance in-situ visualization for surgical guidance. Latency values of all display systems were assessed and a delay experiment proved the latency differences had no significant impact on user performance. Results of multi-scale surgical tasks showed that HMD outperformed in detailed operations probably due to stable resolution along the depth axis, while AIO had better performance in larger-scale operations for better depth perception. This article helps point out the critical limitations of current OST AR techniques and potentially promotes the progress of AR applications in surgical guidance.},
  archive      = {J_TVCG},
  author       = {Ruiyang Li and Boxuan Han and Haowei Li and Longfei Ma and Xinran Zhang and Zhe Zhao and Hongen Liao},
  doi          = {10.1109/TVCG.2023.3260001},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4362-4374},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A comparative evaluation of optical see-through augmented reality in surgical guidance},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Laplacian2Mesh: Laplacian-based mesh understanding.
<em>TVCG</em>, <em>30</em>(7), 4349–4361. (<a
href="https://doi.org/10.1109/TVCG.2023.3259044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geometric deep learning has sparked a rising interest in computer graphics to perform shape understanding tasks, such as shape classification and semantic segmentation. When the input is a polygonal surface, one has to suffer from the irregular mesh structure. Motivated by the geometric spectral theory, we introduce Laplacian2Mesh , a novel and flexible convolutional neural network (CNN) framework for coping with irregular triangle meshes (vertices may have any valence). By mapping the input mesh surface to the multi-dimensional Laplacian-Beltrami space, Laplacian2Mesh enables one to perform shape analysis tasks directly using the mature CNNs, without the need to deal with the irregular connectivity of the mesh structure. We further define a mesh pooling operation such that the receptive field of the network can be expanded while retaining the original vertex set as well as the connections between them. Besides, we introduce a channel-wise self-attention block to learn the individual importance of feature ingredients. Laplacian2Mesh not only decouples the geometry from the irregular connectivity of the mesh structure but also better captures the global features that are central to shape classification and segmentation. Extensive tests on various datasets demonstrate the effectiveness and efficiency of Laplacian2Mesh, particularly in terms of the capability of being vulnerable to noise to fulfill various learning tasks.},
  archive      = {J_TVCG},
  author       = {Qiujie Dong and Zixiong Wang and Manyi Li and Junjie Gao and Shuangmin Chen and Zhenyu Shu and Shiqing Xin and Changhe Tu and Wenping Wang},
  doi          = {10.1109/TVCG.2023.3259044},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4349-4361},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Laplacian2Mesh: Laplacian-based mesh understanding},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NeRC: Rendering planar caustics by learning implicit neural
representations. <em>TVCG</em>, <em>30</em>(7), 4339–4348. (<a
href="https://doi.org/10.1109/TVCG.2023.3259382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Caustics are challenging light transport effects for photo-realistic rendering. Photon mapping techniques play a fundamental role in rendering caustics. However, photon mapping methods render single caustics under the stationary light source in a fixed scene view. They require significant storage and computing resources to produce high-quality results. In this paper, we propose efficiently rendering more diverse caustics of a scene with the camera and the light source moving. We present a novel learning-based volume rendering approach with implicit representations for our proposed task. Considering the variety of materials and textures of planar caustic receivers, we decompose the output appearance into two components: the diffuse and specular parts with a probabilistic module. Unlike NeRF, we construct weights for rendering each component from the implicit signed distance function (SDF). Moreover, we introduce the centering calibration and the sine activation function to improve the performance of the color prediction network. Extensive experiments on the synthetic and real-world datasets illustrate that our method achieves much better performance than baselines in the quantitative and qualitative comparison, for rendering caustics in novel views with the dynamic light source. Especially, our method outperforms the baseline on the temporal consistency across frames.},
  archive      = {J_TVCG},
  author       = {Jiaxiong Qiu and Ze-Xin Yin and Ming-Ming Cheng and Bo Ren},
  doi          = {10.1109/TVCG.2023.3259382},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4339-4348},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {NeRC: Rendering planar caustics by learning implicit neural representations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Progressive tree-based compression of large-scale particle
data. <em>TVCG</em>, <em>30</em>(7), 4321–4338. (<a
href="https://doi.org/10.1109/TVCG.2023.3260628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientific simulations and observations using particles have been creating large datasets that require effective and efficient data reduction to store, transfer, and analyze. However, current approaches either compress only small data well while being inefficient for large data, or handle large data but with insufficient compression. Toward effective and scalable compression/decompression of particle positions, we introduce new kinds of particle hierarchies and corresponding traversal orders that quickly reduce reconstruction error while being fast and low in memory footprint. Our solution to compression of large-scale particle data is a flexible block-based hierarchy that supports progressive, random-access, and error-driven decoding, where error estimation heuristics can be supplied by the user. For low-level node encoding, we introduce new schemes that effectively compress both uniform and densely structured particle distributions. Our proposed methods thus target all three phases of a tree-based particle compression pipeline, namely tree construction, tree traversal, and node encoding. The improved efficacy and flexibility of these methods over existing compressors are demonstrated through extensive experimentation, using a wide range of scientific particle datasets.},
  archive      = {J_TVCG},
  author       = {Duong Hoang and Harsh Bhatia and Peter Lindstrom and Valerio Pascucci},
  doi          = {10.1109/TVCG.2023.3260628},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4321-4338},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Progressive tree-based compression of large-scale particle data},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MobileSky: Real-time sky replacement for mobile AR.
<em>TVCG</em>, <em>30</em>(7), 4304–4320. (<a
href="https://doi.org/10.1109/TVCG.2023.3257840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present MobileSky , the first automatic method for real-time high-quality sky replacement for mobile AR applications. The primary challenge of this task is how to extract sky regions in camera feed both quickly and accurately. While the problem of sky replacement is not new, previous methods mainly concern extraction quality rather than efficiency, limiting their application to our task. We aim to provide higher quality, both spatially and temporally consistent sky mask maps for all camera frames in real time. To this end, we develop a novel framework that combines a new deep semantic network called FSNet with novel post-processing refinement steps. By leveraging IMU data, we also propose new sky-aware constraints such as temporal consistency, position consistency, and color consistency to help refine the weakly classified part of the segmentation output. Experiments show that our method achieves an average of around 30 FPS on off-the-shelf smartphones and outperforms the state-of-the-art sky replacement methods in terms of execution speed and quality. In the meantime, our mask maps appear to be visually more stable across frames. Our fast sky replacement method enables several applications, such as AR advertising, art making, generating fantasy celestial objects, visually learning about weather phenomena, and advanced video-based visual effects. To facilitate future research, we also create a new video dataset containing annotated sky regions with IMU data.},
  archive      = {J_TVCG},
  author       = {Xinjie Wang and Qingxuan Lv and Guo Chen and Jing Zhang and Zhiqiang Wei and Junyu Dong and Hongbo Fu and Zhipeng Zhu and Jingxin Liu and Xiaogang Jin},
  doi          = {10.1109/TVCG.2023.3257840},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4304-4320},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MobileSky: Real-time sky replacement for mobile AR},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analysis of wildfire visualization systems for research and
training: Are they up for the challenge of the current state of
wildfires? <em>TVCG</em>, <em>30</em>(7), 4285–4303. (<a
href="https://doi.org/10.1109/TVCG.2023.3258440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wildfires affect many regions across the world. The accelerated progression of global warming has amplified their frequency and scale, deepening their impact on human life, the economy, and the environment. The temperature rise has been driving wildfires to behave unpredictably compared to those previously observed, challenging researchers and fire management agencies to understand the factors behind this behavioral change. Furthermore, this change has rendered fire personnel training outdated and lost its ability to adequately prepare personnel to respond to these new fires. Immersive visualization can play a key role in tackling the growing issue of wildfires. Therefore, this survey reviews various studies that use immersive and non-immersive data visualization techniques to depict wildfire behavior and train first responders and planners. This paper identifies the most useful characteristics of these systems. While these studies support knowledge creation for certain situations, there is still scope to comprehensively improve immersive systems to address the unforeseen dynamics of wildfires.},
  archive      = {J_TVCG},
  author       = {Carlos A. Tirado Cortes and Susanne Thurow and Alex Ong and Jason J. Sharples and Tomasz Bednarz and Grant Stevens and Dennis Del Favero},
  doi          = {10.1109/TVCG.2023.3258440},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4285-4303},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Analysis of wildfire visualization systems for research and training: Are they up for the challenge of the current state of wildfires?},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MNSS: Neural supersampling framework for real-time rendering
on mobile devices. <em>TVCG</em>, <em>30</em>(7), 4271–4284. (<a
href="https://doi.org/10.1109/TVCG.2023.3259141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although neural supersampling has achieved great success in various applications for improving image quality, it is still difficult to apply it to a wide range of real-time rendering applications due to the high computational power demand. Most existing methods are computationally expensive and require high-performance hardware, preventing their use on platforms with limited hardware, such as smartphones. To this end, we propose a new supersampling framework for real-time rendering applications to reconstruct a high-quality image out of a low-resolution one, which is sufficiently lightweight to run on smartphones within a real-time budget. Our model takes as input the renderer-generated low resolution content and produces high resolution and anti-aliased results. To maximize sampling efficiency, we propose using an alternate sub-pixel sample pattern during the rasterization process. This allows us to create a relatively small reconstruction model while maintaining high image quality. By accumulating new samples into a high-resolution history buffer, an efficient history check and re-usage scheme is introduced to improve temporal stability. To our knowledge, this is the first research in pushing real-time neural supersampling on mobile devices. Due to the absence of training data, we present a new dataset containing 57 training and test sequences from three game scenes. Furthermore, based on the rendered motion vectors and a visual perception study, we introduce a new metric called inter-frame structural similarity (IF-SSIM) to quantitatively measure the temporal stability of rendered videos. Extensive evaluations demonstrate that our supersampling model outperforms existing or alternative solutions in both performance and temporal stability.},
  archive      = {J_TVCG},
  author       = {Sipeng Yang and Yunlu Zhao and Yuzhe Luo and He Wang and Hongyu Sun and Chen Li and Binghuang Cai and Xiaogang Jin},
  doi          = {10.1109/TVCG.2023.3259141},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4271-4284},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MNSS: Neural supersampling framework for real-time rendering on mobile devices},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An evaluation of view rotation techniques for seated
navigation in virtual reality. <em>TVCG</em>, <em>30</em>(7), 4257–4270.
(<a href="https://doi.org/10.1109/TVCG.2023.3258693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Head tracking is commonly used in VR applications to allow users to naturally view 3D content using physical head movement, but many applications also support turning with hand-held controllers. Controller and joystick controls are convenient for practical settings where full 360-degree physical rotation is not possible, such as when the user is sitting at a desk. Though controller-based rotation provides the benefit of convenience, previous research has demonstrated that virtual or joystick-controlled view rotation to have drawbacks of sickness and disorientation compared to physical turning. To combat such issues, researchers have considered various techniques such as speed adjustments or reduced field of view, but data is limited on how different variations for joystick rotation influences sickness and orientation perception. Our studies include different variations of techniques such as joystick rotation, resetting, and field-of-view reduction. We investigate trade-offs among different techniques in terms of sickness and the ability to maintain spatial orientation. In two controlled experiments, participants traveled through a sequence of rooms and were tested on spatial orientation, and we also collected subjective measures of sickness and preference. Our findings indicate a preference by users towards directly-manipulated joystick-based rotations compared to user-initiated resetting and minimal effects of technique on spatial awareness.},
  archive      = {J_TVCG},
  author       = {Brett Benda and Shyam Prathish Sargunam and Mahsan Nourani and Eric D. Ragan},
  doi          = {10.1109/TVCG.2023.3258693},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4257-4270},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {An evaluation of view rotation techniques for seated navigation in virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient reflectance capture with a deep gated
mixture-of-experts. <em>TVCG</em>, <em>30</em>(7), 4246–4256. (<a
href="https://doi.org/10.1109/TVCG.2023.3261872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel framework to efficiently acquire anisotropic reflectance in a pixel-independent fashion, using a deep gated mixture-of-experts. While existing work employs a unified network to handle all possible input, our network automatically learns to condition on the input for enhanced reconstruction. We train a gating module that takes photometric measurements as input and selects one out of a number of specialized decoders for reflectance reconstruction, essentially trading generality for quality. A common pre-trained latent-transform module is also appended to each decoder, to offset the burden of the increased number of decoders. In addition, the illumination conditions during acquisition can be jointly optimized. The effectiveness of our framework is validated on a wide variety of challenging near-planar samples with a lightstage. Compared with the state-of-the-art technique, our quality is improved with the same number of input images, and our input image number can be reduced to about 1/3 for equal-quality results. We further generalize the framework to enhance a state-of-the-art technique on non-planar reflectance scanning.},
  archive      = {J_TVCG},
  author       = {Xiaohe Ma and Yaxin Yu and Hongzhi Wu and Kun Zhou},
  doi          = {10.1109/TVCG.2023.3261872},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4246-4256},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Efficient reflectance capture with a deep gated mixture-of-experts},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust coarse cage construction with small approximation
errors. <em>TVCG</em>, <em>30</em>(7), 4234–4245. (<a
href="https://doi.org/10.1109/TVCG.2023.3255207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a robust and automatic method to construct manifold cages for 3D triangular meshes. The cage contains hundreds of triangles to tightly enclose the input mesh without self-intersections. To generate such cages, our algorithm consists of two phases: (1) construct manifold cages satisfying the tightness, enclosing, and intersection-free requirements and (2) reduce mesh complexities and approximation errors without violating the enclosing and intersection-free requirements. To theoretically make the first stage have those properties, we combine the conformal tetrahedral meshing and tetrahedral mesh subdivision. The second step is a constrained remeshing process using explicit checks to ensure that the enclosing and intersection-free constraints are always satisfied. Both phases use a hybrid coordinate representation, i.e., rational numbers and floating point numbers, combined with exact arithmetic and floating point filtering techniques to guarantee the robustness of geometric predicates with a favorable speed. We extensively test our method on a data set of over 8500 models, demonstrating robustness and performance. Compared to other state-of-the-art methods, our method possesses much stronger robustness.},
  archive      = {J_TVCG},
  author       = {Jia-Peng Guo and Wen-Xiang Zhang and Chunyang Ye and Xiao-Ming Fu},
  doi          = {10.1109/TVCG.2023.3255207},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4234-4245},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Robust coarse cage construction with small approximation errors},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient pooling operator for 3D morphable models.
<em>TVCG</em>, <em>30</em>(7), 4225–4233. (<a
href="https://doi.org/10.1109/TVCG.2023.3255820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning the latent representation of three-dimensional (3D) morphable geometry is useful for several tasks, such as 3D face tracking, human motion analysis, and character generation and animation. For unstructured surface meshes, previous state-of-the-art methods focus on designing convolution operators and share the same pooling and unpooling operations to encode neighborhood information. Previous models use a mesh pooling operation based on edge contraction, which is based on the euclidean distance of vertices rather than the actual topology. In this study, we investigated whether such a pooling operation can be improved, introducing an improved pooling layer that combines the vertex normals and adjacent faces area. Furthermore, to prevent template overfitting, we increased the receptive field and improved low-resolution projection in the unpooling stage. This increase did not affect processing efficiency because the operation was implemented once on the mesh. We performed experiments to evaluate the proposed method, whose results indicated that the proposed operations outperformed Neural3DMM with 14% lower reconstruction errors and outperformed CoMA by 15% by modifying the pooling and unpooling matrices.},
  archive      = {J_TVCG},
  author       = {Haoliang Zhang and Samuel Cheng and Christian El Amm and Jonghoon Kim},
  doi          = {10.1109/TVCG.2023.3255820},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4225-4233},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Efficient pooling operator for 3D morphable models},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mesh neural networks based on dual graph pyramids.
<em>TVCG</em>, <em>30</em>(7), 4211–4224. (<a
href="https://doi.org/10.1109/TVCG.2023.3257035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have been widely used for mesh processing in recent years. However, current DNNs can not process arbitrary meshes efficiently. On the one hand, most DNNs expect 2-manifold, watertight meshes, but many meshes, whether manually designed or automatically generated, may have gaps, non-manifold geometry, or other defects. On the other hand, the irregular structure of meshes also brings challenges to building hierarchical structures and aggregating local geometric information, which is critical to conduct DNNs. In this paper, we present DGNet, an efficient, effective and generic deep neural mesh processing network based on dual graph pyramids; it can handle arbitrary meshes. First, we construct dual graph pyramids for meshes to guide feature propagation between hierarchical levels for both downsampling and upsampling. Second, we propose a novel convolution to aggregate local features on the proposed hierarchical graphs. By utilizing both geodesic neighbors and euclidean neighbors, the network enables feature aggregation both within local surface patches and between isolated mesh components. Experimental results demonstrate that DGNet can be applied to both shape analysis and large-scale scene understanding. Furthermore, it achieves superior performance on various benchmarks, including ShapeNetCore, HumanBody, ScanNet and Matterport3D. Code and models will be available at https://github.com/li-xl/DGNet .},
  archive      = {J_TVCG},
  author       = {Xiang-Li Li and Zheng-Ning Liu and Tuo Chen and Tai-Jiang Mu and Ralph R. Martin and Shi-Min Hu},
  doi          = {10.1109/TVCG.2023.3257035},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4211-4224},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Mesh neural networks based on dual graph pyramids},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactive subspace cluster analysis guided by semantic
attribute associations. <em>TVCG</em>, <em>30</em>(7), 4197–4210. (<a
href="https://doi.org/10.1109/TVCG.2023.3256376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate datasets with many variables are increasingly common in many application areas. Most methods approach multivariate data from a singular perspective. Subspace analysis techniques, on the other hand. provide the user a set of subspaces which can be used to view the data from multiple perspectives. However, many subspace analysis methods produce a huge amount of subspaces, a number of which are usually redundant. The enormity of the number of subspaces can be overwhelming to analysts, making it difficult for them to find informative patterns in the data. In this article, we propose a new paradigm that constructs semantically consistent subspaces. These subspaces can then be expanded into more general subspaces by ways of conventional techniques. Our framework uses the labels/meta-data of a dataset to learn the semantic meanings and associations of the attributes. We employ a neural network to learn a semantic word embedding of the attributes and then divide this attribute space into semantically consistent subspaces. The user is provided with a visual analytics interface that guides the analysis process. We show via various examples that these semantic subspaces can help organize the data and guide the user in finding interesting patterns in the dataset.},
  archive      = {J_TVCG},
  author       = {Salman Mahmood and Klaus Mueller},
  doi          = {10.1109/TVCG.2023.3256376},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4197-4210},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive subspace cluster analysis guided by semantic attribute associations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). StyleVR: Stylizing character animations with normalizing
flows. <em>TVCG</em>, <em>30</em>(7), 4183–4196. (<a
href="https://doi.org/10.1109/TVCG.2023.3259183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The significance of artistry in creating animated virtual characters is widely acknowledged, and motion style is a crucial element in this process. There has been a long-standing interest in stylizing character animations with style transfer methods. However, this kind of models can only deal with short-term motions and yield deterministic outputs. To address this issue, we propose a generative model based on normalizing flows for stylizing long and aperiodic animations in the VR scene. Our approach breaks down this task into two sub-problems: motion style transfer and stylized motion generation, both formulated as the instances of conditional normalizing flows with multi-class latent space. Specifically, we encode high-frequency style features into the latent space for varied results and control the generation process with style-content labels for disentangled edits of style and content. We have developed a prototype, StyleVR, in Unity, which allows casual users to apply our method in VR. Through qualitative and quantitative comparisons, we demonstrate that our system outperforms other methods in terms of style transfer as well as stochastic stylized motion generation.},
  archive      = {J_TVCG},
  author       = {Bin Ji and Ye Pan and Yichao Yan and Ruizhao Chen and Xiaokang Yang},
  doi          = {10.1109/TVCG.2023.3259183},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4183-4196},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {StyleVR: Stylizing character animations with normalizing flows},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VR-HandNet: A visually and physically plausible hand
manipulation system in virtual reality. <em>TVCG</em>, <em>30</em>(7),
4170–4182. (<a href="https://doi.org/10.1109/TVCG.2023.3255991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to allow users to perform dexterous hand manipulation of objects in virtual environments with hand-held VR controllers. To this end, the VR controller is mapped to the virtual hand and the hand motions are dynamically synthesized when the virtual hand approaches an object. At each frame, given the information about the virtual hand, VR controller input, and hand-object spatial relations, the deep neural network determines the desired joint orientations of the virtual hand model in the next frame. The desired orientations are then converted into a set of torques acting on hand joints and applied to a physics simulation to determine the hand pose at the next frame. The deep neural network, named VR-HandNet, is trained with a reinforcement learning-based approach. Therefore, it can produce physically plausible hand motion since the trial-and-error training process can learn how the interaction between hand and object is performed under the environment that is simulated by a physics engine. Furthermore, we adopted an imitation learning paradigm to increase visual plausibility by mimicking the reference motion datasets. Through the ablation studies, we validated the proposed method is effectively constructed and successfully serves our design goal. A live demo is demonstrated in the supplementary video.},
  archive      = {J_TVCG},
  author       = {DongHeun Han and RoUn Lee and KyeongMin Kim and HyeongYeop Kang},
  doi          = {10.1109/TVCG.2023.3255991},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4170-4182},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VR-HandNet: A visually and physically plausible hand manipulation system in virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Softness perception of visual objects controlled by
touchless inputs: The role of effective distance of hand movements.
<em>TVCG</em>, <em>30</em>(7), 4154–4169. (<a
href="https://doi.org/10.1109/TVCG.2023.3254522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feedback on the material properties of a visual object is essential in enhancing the users’ perceptual experience of the object when users control the object with touchless inputs. Focusing on the softness perception of the object, we examined how the effective distance of hand movements influenced the degree of the object&#39;s softness perceived by users. In the experiments, participants moved their right hand in front of a camera which tracked their hand position. A textured 2D or 3D object on display deformed depending on the participant&#39;s hand position. In addition to establishing a ratio of deformation magnitude to the distance of hand movements, we altered the effective distance of hand movement, within which the hand movement could deform the object. Participants rated the strength of perceived softness (Experiments 1 and 2) and other perceptual impressions (Experiment 3). A longer effective distance produced a softer impression of the 2D and 3D objects. The saturation speed of object deformation due to the effective distance was not a critical determinant. The effective distance also modulated other perceptual impressions than softness. The role of the effective distance of hand movements on perceptual impressions of objects under touchless control is discussed.},
  archive      = {J_TVCG},
  author       = {Takahiro Kawabe and Yusuke Ujitoko},
  doi          = {10.1109/TVCG.2023.3254522},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4154-4169},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Softness perception of visual objects controlled by touchless inputs: The role of effective distance of hand movements},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Are metrics enough? Guidelines for communicating and
visualizing predictive models to subject matter experts. <em>TVCG</em>,
<em>30</em>(7), 4137–4153. (<a
href="https://doi.org/10.1109/TVCG.2023.3259341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presenting a predictive model&#39;s performance is a communication bottleneck that threatens collaborations between data scientists and subject matter experts. Accuracy and error metrics alone fail to tell the whole story of a model – its risks, strengths, and limitations – making it difficult for subject matter experts to feel confident in their decision to use a model. As a result, models may fail in unexpected ways or go entirely unused, as subject matter experts disregard poorly presented models in favor of familiar, yet arguably substandard methods. In this paper, we describe an iterative study conducted with both subject matter experts and data scientists to understand the gaps in communication between these two groups. We find that, while the two groups share common goals of understanding the data and predictions of the model, friction can stem from unfamiliar terms, metrics, and visualizations – limiting the transfer of knowledge to SMEs and discouraging clarifying questions being asked during presentations. Based on our findings, we derive a set of communication guidelines that use visualization as a common medium for communicating the strengths and weaknesses of a model. We provide a demonstration of our guidelines in a regression modeling scenario and elicit feedback on their use from subject matter experts. From our demonstration, subject matter experts were more comfortable discussing a model&#39;s performance, more aware of the trade-offs for the presented model, and better equipped to assess the model&#39;s risks – ultimately informing and contextualizing the model&#39;s use beyond text and numbers.},
  archive      = {J_TVCG},
  author       = {Ashley Suh and Gabriel Appleby and Erik W. Anderson and Luca Finelli and Remco Chang and Dylan Cashman},
  doi          = {10.1109/TVCG.2023.3259341},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4137-4153},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Are metrics enough? guidelines for communicating and visualizing predictive models to subject matter experts},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Studying the influence of multisensory stimuli on a
firefighting training virtual environment. <em>TVCG</em>,
<em>30</em>(7), 4122–4136. (<a
href="https://doi.org/10.1109/TVCG.2023.3251188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How we perceive and experience the world around us is inherently multisensory. Most of the Virtual Reality (VR) literature is based on the senses of sight and hearing. However, there is a lot of potential for integrating additional stimuli into Virtual Environments (VEs), especially in a training context. Identifying the relevant stimuli for obtaining a virtual experience that is perceptually equivalent to a real experience will lead users to behave the same across environments, which adds substantial value for several training areas, such as firefighters. In this article, we present an experiment aiming to assess the impact of different sensory stimuli on stress, fatigue, cybersickness, Presence and knowledge transfer of users during a firefighter training VE. The results suggested that the stimulus that significantly impacted the user&#39;s response was wearing a firefighter&#39;s uniform and combining all sensory stimuli under study: heat, weight, uniform, and mask. The results also showed that the VE did not induce cybersickness and that it was successful in the task of transferring knowledge.},
  archive      = {J_TVCG},
  author       = {David Narciso and Miguel Melo and Susana Rodrigues and João Paulo Cunha and José Vasconcelos-Raposo and Maximino Bessa},
  doi          = {10.1109/TVCG.2023.3251188},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4122-4136},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Studying the influence of multisensory stimuli on a firefighting training virtual environment},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DMiner: Dashboard design mining and recommendation.
<em>TVCG</em>, <em>30</em>(7), 4108–4121. (<a
href="https://doi.org/10.1109/TVCG.2023.3251344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dashboards, which comprise multiple views on a single display, help analyze and communicate multiple perspectives of data simultaneously. However, creating effective and elegant dashboards is challenging since it requires careful and logical arrangement and coordination of multiple visualizations. To solve the problem, we propose a data-driven approach for mining design rules from dashboards and automating dashboard organization. Specifically, we focus on two prominent aspects of the organization: arrangement , which describes the position, size, and layout of each view in the display space; and coordination , which indicates the interaction between pairwise views. We build a new dataset containing 854 dashboards crawled online, and develop feature engineering methods for describing the single views and view-wise relationships in terms of data, encoding, layout, and interactions. Further, we identify design rules among those features and develop a recommender for dashboard design. We demonstrate the usefulness of DMiner through an expert study and a user study. The expert study shows that our extracted design rules are reasonable and conform to the design practice of experts. Moreover, a comparative user study shows that our recommender could help automate dashboard organization and reach human-level performance. In summary, our work offers a promising starting point for design mining visualizations to build recommenders.},
  archive      = {J_TVCG},
  author       = {Yanna Lin and Haotian Li and Aoyu Wu and Yong Wang and Huamin Qu},
  doi          = {10.1109/TVCG.2023.3251344},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4108-4121},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DMiner: Dashboard design mining and recommendation},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating the impact of uncertainty visualization on model
reliance. <em>TVCG</em>, <em>30</em>(7), 4093–4107. (<a
href="https://doi.org/10.1109/TVCG.2023.3251950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning models have gained traction as decision support tools for tasks that require processing copious amounts of data. However, to achieve the primary benefits of automating this part of decision-making, people must be able to trust the machine learning model&#39;s outputs. In order to enhance people&#39;s trust and promote appropriate reliance on the model, visualization techniques such as interactive model steering, performance analysis, model comparison, and uncertainty visualization have been proposed. In this study, we tested the effects of two uncertainty visualization techniques in a college admissions forecasting task, under two task difficulty levels, using Amazon&#39;s Mechanical Turk platform. Results show that (1) people&#39;s reliance on the model depends on the task difficulty and level of machine uncertainty and (2) ordinal forms of expressing model uncertainty are more likely to calibrate model usage behavior. These outcomes emphasize that reliance on decision support tools can depend on the cognitive accessibility of the visualization technique and perceptions of model performance and task difficulty.},
  archive      = {J_TVCG},
  author       = {Jieqiong Zhao and Yixuan Wang and Michelle V. Mancenido and Erin K. Chiou and Ross Maciejewski},
  doi          = {10.1109/TVCG.2023.3251950},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4093-4107},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Evaluating the impact of uncertainty visualization on model reliance},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ReenactArtFace: Artistic face image reenactment.
<em>TVCG</em>, <em>30</em>(7), 4080–4092. (<a
href="https://doi.org/10.1109/TVCG.2023.3253184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale datasets and deep generative models have enabled impressive progress in human face reenactment. Existing solutions for face reenactment have focused on processing real face images through facial landmarks by generative models. Different from real human faces, artistic human faces (e.g., those in paintings, cartoons, etc.) often involve exaggerated shapes and various textures. Therefore, directly applying existing solutions to artistic faces often fails to preserve the characteristics of the original artistic faces (e.g., face identity and decorative lines along face contours) due to the domain gap between real and artistic faces. To address these issues, we present ReenactArtFace , the first effective solution for transferring the poses and expressions from human videos to various artistic face images. We achieve artistic face reenactment in a coarse-to-fine manner. First, we perform 3D artistic face reconstruction , which reconstructs a textured 3D artistic face through a 3D morphable model (3DMM) and a 2D parsing map from an input artistic image. The 3DMM can not only rig the expressions better than facial landmarks but also render images under different poses/expressions as coarse reenactment results robustly. However, these coarse results suffer from self-occlusions and lack contour lines. Second, we thus perform artistic face refinement by using a personalized conditional adversarial generative model (cGAN) fine-tuned on the input artistic image and the coarse reenactment results. For high-quality refinement, we propose a contour loss to supervise the cGAN to faithfully synthesize contour lines. Quantitative and qualitative experiments demonstrate that our method achieves better results than the existing solutions.},
  archive      = {J_TVCG},
  author       = {Linzi Qu and Jiaxiang Shang and Xiaoguang Han and Hongbo Fu},
  doi          = {10.1109/TVCG.2023.3253184},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4080-4092},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ReenactArtFace: Artistic face image reenactment},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Magic furniture: Design paradigm of multi-function assembly.
<em>TVCG</em>, <em>30</em>(7), 4068–4079. (<a
href="https://doi.org/10.1109/TVCG.2023.3250488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assembly-based furniture with movable parts enables shape and structure reconfiguration, thus supporting multiple functions. Although a few attempts have been made for facilitating the creation of multi-function objects, designing such a multi-function assembly with the existing solutions often requires high imagination of designers. We develop the Magic Furniture system for users to easily create such designs simply given multiple cross-category objects. Our system automatically leverages the given objects as references to generate a 3D model with movable boards driven by back-and-forth movement mechanisms. By controlling the states of these mechanisms, a designed multi-function furniture object can be reconfigured to approximate the shapes and functions of the given objects. To ensure the designed furniture easy to transform between different functions, we perform an optimization algorithm to choose a proper number of movable boards and determine their shapes and sizes, following a set of design guidelines. We demonstrate the effectiveness of our system through various multi-function furniture designed with different sets of reference inputs and various movement constraints. We also evaluate the design results through several experiments including comparative and user studies.},
  archive      = {J_TVCG},
  author       = {Qiang Fu and Fan Zhang and Xueming Li and Hongbo Fu},
  doi          = {10.1109/TVCG.2023.3250488},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4068-4079},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Magic furniture: Design paradigm of multi-function assembly},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accurate registration of cross-modality geometry via
consistent clustering. <em>TVCG</em>, <em>30</em>(7), 4055–4067. (<a
href="https://doi.org/10.1109/TVCG.2023.3247169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The registration of unitary-modality geometric data has been successfully explored over past decades. However, existing approaches typically struggle to handle cross-modality data due to the intrinsic difference between different models. To address this problem, in this article, we formulate the cross-modality registration problem as a consistent clustering process . First, we study the structure similarity between different modalities based on an adaptive fuzzy shape clustering, from which a coarse alignment is successfully operated. Then, we optimize the result using fuzzy clustering consistently, in which the source and target models are formulated as clustering memberships and centroids , respectively. This optimization casts new insight into point set registration, and substantially improves the robustness against outliers. Additionally, we investigate the effect of fuzzier in fuzzy clustering on the cross-modality registration problem, from which we theoretically prove that the classical Iterative Closest Point (ICP) algorithm is a special case of our newly defined objective function. Comprehensive experiments and analysis are conducted on both synthetic and real-world cross-modality datasets. Qualitative and quantitative results demonstrate that our method outperforms state-of-the-art approaches with higher accuracy and robustness. Our code is publicly available at https://github.com/zikai1/CrossModReg .},
  archive      = {J_TVCG},
  author       = {Mingyang Zhao and Xiaoshui Huang and Jingen Jiang and Luntian Mou and Dong-Ming Yan and Lei Ma},
  doi          = {10.1109/TVCG.2023.3247169},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4055-4067},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Accurate registration of cross-modality geometry via consistent clustering},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attitudinal effects of data visualizations and illustrations
in data stories. <em>TVCG</em>, <em>30</em>(7), 4039–4054. (<a
href="https://doi.org/10.1109/TVCG.2023.3248319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Journalism has become more data-driven and inherently visual in recent years. Photographs, illustrations, infographics, data visualizations, and general images help convey complex topics to a wide audience. The way that visual artifacts influence how readers form an opinion beyond the text is an important issue to research, but there are few works about this topic. In this context, we research the persuasive, emotional and memorable dimensions of data visualizations and illustrations in journalistic storytelling for long-form articles. We conducted a user study and compared the effects which data visualizations and illustrations have on changing attitude towards a presented topic. While visual representations are usually studied along one dimension, in this experimental study, we explore the effects on readers’ attitudes along three: persuasion, emotion, and information retention. By comparing different versions of the same article, we observe how attitudes differ based on the visual stimuli present, and how they are perceived when combined. Results indicate that the narrative using only data visualization elicits a stronger emotional impact than illustration-only visual support, as well as a significant change in the initial attitude about the topic. Our findings contribute to a growing body of literature on how visual artifacts may be used to inform and influence public opinion and debate. We present ideas for future work to generalize the results beyond the domain studied, the water crisis.},
  archive      = {J_TVCG},
  author       = {Manuela Garretón and Francesca Morini and Pablo Celhay and Marian Dörk and Denis Parra},
  doi          = {10.1109/TVCG.2023.3248319},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4039-4054},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Attitudinal effects of data visualizations and illustrations in data stories},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sensory attenuation with a virtual robotic arm controlled
using facial movements. <em>TVCG</em>, <em>30</em>(7), 4023–4038. (<a
href="https://doi.org/10.1109/TVCG.2023.3246092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When humans generate stimuli voluntarily, they perceive the stimuli more weakly than those produced by others, which is called sensory attenuation (SA). SA has been investigated in various body parts, but it is unclear whether an extended body induces SA. This study investigated the SA of audio stimuli generated by an extended body. SA was assessed using a sound comparison task in a virtual environment. We prepared robotic arms as extended bodies, and the robotic arms were controlled by facial movements. To evaluate the SA of robotic arms, we conducted two experiments. Experiment 1 investigated the SA of the robotic arms under four conditions. The results showed that robotic arms manipulated by voluntary actions attenuated audio stimuli. Experiment 2 investigated the SA of the robotic arm and innate body under five conditions. The results indicated that the innate body and robotic arm induced SA, while there were differences in the sense of agency between the innate body and robotic arm. Analysis of the results indicated three findings regarding the SA of the extended body. First, controlling the robotic arm with voluntary actions in a virtual environment attenuates the audio stimuli. Second, there were differences in the sense of agency related to SA between extended and innate bodies. Third, the SA of the robotic arm was correlated with the sense of body ownership.},
  archive      = {J_TVCG},
  author       = {Masaaki Fukuoka and Fumihiko Nakamura and Adrien Verhulst and Masahiko Inami and Michiteru Kitazaki and Maki Sugimoto},
  doi          = {10.1109/TVCG.2023.3246092},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4023-4038},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Sensory attenuation with a virtual robotic arm controlled using facial movements},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anchorage: Visual analysis of satisfaction in customer
service videos via anchor events. <em>TVCG</em>, <em>30</em>(7),
4008–4022. (<a href="https://doi.org/10.1109/TVCG.2023.3245609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Delivering customer services through video communications has brought new opportunities to analyze customer satisfaction for quality management. However, due to the lack of reliable self-reported responses, service providers are troubled by the inadequate estimation of customer services and the tedious investigation into multimodal video recordings. We introduce Anchorage , a visual analytics system to evaluate customer satisfaction by summarizing multimodal behavioral features in customer service videos and revealing abnormal operations in the service process. We leverage the semantically meaningful operations to introduce structured event understanding into videos which help service providers quickly navigate to events of their interest. Anchorage supports a comprehensive evaluation of customer satisfaction from the service and operation levels and efficient analysis of customer behavioral dynamics via multifaceted visualization views. We extensively evaluate Anchorage through a case study and a carefully-designed user study. The results demonstrate its effectiveness and usability in assessing customer satisfaction using customer service videos. We found that introducing event contexts in assessing customer satisfaction can enhance its performance without compromising annotation precision. Our approach can be adapted in situations where unlabelled and unstructured videos are collected along with sequential records.},
  archive      = {J_TVCG},
  author       = {Kam Kwai Wong and Xingbo Wang and Yong Wang and Jianben He and Rong Zhang and Huamin Qu},
  doi          = {10.1109/TVCG.2023.3245609},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {4008-4022},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Anchorage: Visual analysis of satisfaction in customer service videos via anchor events},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling realistic clothing from a single image under normal
guide. <em>TVCG</em>, <em>30</em>(7), 3995–4007. (<a
href="https://doi.org/10.1109/TVCG.2023.3245583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a robust and highly realistic clothing modeling method to generate a 3D clothing model with visually consistent clothing style and wrinkles distribution from a single RGB image. Notably, this entire process only takes a few seconds. Our high-quality clothing results benefit from the idea of combining learning and optimization, making it highly robust. First, we use the neural networks to predict the normal map, a clothing mask, and a learning-based clothing model from input images. The predicted normal map can effectively capture high-frequency clothing deformation from image observations. Then, by introducing a normal-guided clothing fitting optimization, the normal maps are used to guide the clothing model to generate realistic wrinkles details. Finally, we utilize a clothing collar adjustment strategy to stylize clothing results using predicted clothing masks. An extended multi-view version of the clothing fitting is naturally developed, which can further improve the realism of the clothing without tedious effort. Extensive experiments have proven that our method achieves state-of-the-art clothing geometric accuracy and visual realism. More importantly, it is highly adaptable and robust to in-the-wild images. Further, our method can be easily extended to multi-view inputs to improve realism. In summary, our method can provide a low-cost and user-friendly solution to achieve realistic clothing modeling.},
  archive      = {J_TVCG},
  author       = {Xinqi Liu and Jituo Li and Guodong Lu},
  doi          = {10.1109/TVCG.2023.3245583},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3995-4007},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Modeling realistic clothing from a single image under normal guide},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Monocular depth decomposition of semi-transparent volume
renderings. <em>TVCG</em>, <em>30</em>(7), 3981–3994. (<a
href="https://doi.org/10.1109/TVCG.2023.3245305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks have shown great success in extracting geometric information from color images. Especially, monocular depth estimation networks are increasingly reliable in real-world scenes. In this work we investigate the applicability of such monocular depth estimation networks to semi-transparent volume rendered images. As depth is notoriously difficult to define in a volumetric scene without clearly defined surfaces, we consider different depth computations that have emerged in practice, and compare state-of-the-art monocular depth estimation approaches for these different interpretations during an evaluation considering different degrees of opacity in the renderings. Additionally, we investigate how these networks can be extended to further obtain color and opacity information, in order to create a layered representation of the scene based on a single color image. This layered representation consists of spatially separated semi-transparent intervals that composite to the original input rendering. In our experiments we show that existing approaches to monocular depth estimation can be adapted to perform well on semi-transparent volume renderings, which has several applications in the area of scientific visualization, like re-composition with additional objects and labels or additional shading.},
  archive      = {J_TVCG},
  author       = {Dominik Engel and Sebastian Hartwig and Timo Ropinski},
  doi          = {10.1109/TVCG.2023.3245305},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3981-3994},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Monocular depth decomposition of semi-transparent volume renderings},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PalmEx: Adding palmar force-feedback for 3D manipulation
with haptic exoskeleton gloves. <em>TVCG</em>, <em>30</em>(7),
3973–3980. (<a href="https://doi.org/10.1109/TVCG.2023.3244076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Haptic exoskeleton gloves are a widespread solution for providing force-feedback in Virtual Reality (VR), especially for 3D object manipulations. However, they are still lacking an important feature regarding in-hand haptic sensations: the palmar contact. In this paper, we present PalmEx, a novel approach which incorporates palmar force-feedback into exoskeleton gloves to improve the overall grasping sensations and manual haptic interactions in VR. PalmEx&#39;s concept is demonstrated through a self-contained hardware system augmenting a hand exoskeleton with an encountered palmar contact interface – physically encountering the users’ palm. We build upon current taxonomies to elicit PalmEx&#39;s capabilities for both the exploration and manipulation of virtual objects. We first conduct a technical evaluation optimising the delay between the virtual interactions and their physical counterparts. We then empirically evaluate PalmEx&#39;s proposed design space in a user study (n=12) to assess the potential of a palmar contact for augmenting an exoskeleton. Results show that PalmEx offers the best rendering capabilities to perform believable grasps in VR. PalmEx highlights the importance of the palmar stimulation, and provides a low-cost solution to augment existing high-end consumer hand exoskeletons.},
  archive      = {J_TVCG},
  author       = {Elodie Bouzbib and Marc Teyssier and Thomas Howard and Claudio Pacchierotti and Anatole Lécuyer},
  doi          = {10.1109/TVCG.2023.3244076},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3973-3980},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PalmEx: Adding palmar force-feedback for 3D manipulation with haptic exoskeleton gloves},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring classification of topological priors with machine
learning for feature extraction. <em>TVCG</em>, <em>30</em>(7),
3959–3972. (<a href="https://doi.org/10.1109/TVCG.2023.3248632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many scientific endeavors, increasingly abstract representations of data allow for new interpretive methodologies and conceptualization of phenomena. For example, moving from raw imaged pixels to segmented and reconstructed objects allows researchers new insights and means to direct their studies toward relevant areas. Thus, the development of new and improved methods for segmentation remains an active area of research. With advances in machine learning and neural networks, scientists have been focused on employing deep neural networks such as U-Net to obtain pixel-level segmentations, namely, defining associations between pixels and corresponding/referent objects and gathering those objects afterward. Topological analysis, such as the use of the Morse-Smale complex to encode regions of uniform gradient flow behavior, offers an alternative approach: first, create geometric priors, and then apply machine learning to classify. This approach is empirically motivated since phenomena of interest often appear as subsets of topological priors in many applications. Using topological elements not only reduces the learning space but also introduces the ability to use learnable geometries and connectivity to aid the classification of the segmentation target. In this article, we describe an approach to creating learnable topological elements, explore the application of ML techniques to classification tasks in a number of areas, and demonstrate this approach as a viable alternative to pixel-level classification, with similar accuracy, improved execution time, and requiring marginal training data.},
  archive      = {J_TVCG},
  author       = {Samuel Leventhal and Attila Gyulassy and Mark Heimann and Valerio Pascucci},
  doi          = {10.1109/TVCG.2023.3248632},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3959-3972},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring classification of topological priors with machine learning for feature extraction},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A visual interface for exploring hypotheses about neural
circuits. <em>TVCG</em>, <em>30</em>(7), 3945–3958. (<a
href="https://doi.org/10.1109/TVCG.2023.3243668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the fundamental problems in neurobiological research is to understand how neural circuits generate behaviors in response to sensory stimuli. Elucidating such neural circuits requires anatomical and functional information about the neurons that are active during the processing of the sensory information and generation of the respective response, as well as an identification of the connections between these neurons. With modern imaging techniques, both morphological properties of individual neurons as well as functional information related to sensory processing, information integration and behavior can be obtained. Given the resulting information, neurobiologists are faced with the task of identifying the anatomical structures down to individual neurons that are linked to the studied behavior and the processing of the respective sensory stimuli. Here, we present a novel interactive tool that assists neurobiologists in the aforementioned tasks by allowing them to extract hypothetical neural circuits constrained by anatomical and functional data. Our approach is based on two types of structural data: brain regions that are anatomically or functionally defined, and morphologies of individual neurons. Both types of structural data are interlinked and augmented with additional information. The presented tool allows the expert user to identify neurons using Boolean queries. The interactive formulation of these queries is supported by linked views, using, among other things, two novel 2D abstractions of neural circuits. The approach was validated in two case studies investigating the neural basis of vision-based behavioral responses in zebrafish larvae. Despite this particular application, we believe that the presented tool will be of general interest for exploring hypotheses about neural circuits in other species, genera and taxa.},
  archive      = {J_TVCG},
  author       = {Sumit Kumar Vohra and Philipp Harth and Yasuko Isoe and Armin Bahl and Haleh Fotowat and Florian Engert and Hans-Christian Hege and Daniel Baum},
  doi          = {10.1109/TVCG.2023.3243668},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3945-3958},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A visual interface for exploring hypotheses about neural circuits},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RipViz: Finding rip currents by learning pathline behavior.
<em>TVCG</em>, <em>30</em>(7), 3930–3944. (<a
href="https://doi.org/10.1109/TVCG.2023.3243834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a hybrid machine learning and flow analysis feature detection method, RipViz, to extract rip currents from stationary videos. Rip currents are dangerous strong currents that can drag beachgoers out to sea. Most people are either unaware of them or do not know what they look like. In some instances, even trained personnel such as lifeguards have difficulty identifying them. RipViz produces a simple, easy to understand visualization of rip location overlaid on the source video. With RipViz, we first obtain an unsteady 2D vector field from the stationary video using optical flow. Movement at each pixel is analyzed over time. At each seed point, sequences of short pathlines, rather a single long pathline, are traced across the frames of the video to better capture the quasi-periodic flow behavior of wave activity. Because of the motion on the beach, the surf zone, and the surrounding areas, these pathlines may still appear very cluttered and incomprehensible. Furthermore, lay audiences are not familiar with pathlines and may not know how to interpret them. To address this, we treat rip currents as a flow anomaly in an otherwise normal flow. To learn about the normal flow behavior, we train an LSTM autoencoder with pathline sequences from normal ocean, foreground, and background movements. During test time, we use the trained LSTM autoencoder to detect anomalous pathlines (i.e., those in the rip zone). The origination points of such anomalous pathlines, over the course of the video, are then presented as points within the rip zone. RipViz is fully automated and does not require user input. Feedback from domain expert suggests that RipViz has the potential for wider use.},
  archive      = {J_TVCG},
  author       = {Akila de Silva and Mona Zhao and Donald Stewart and Fahim Hasan Khan and Gregory Dusek and James Davis and Alex Pang},
  doi          = {10.1109/TVCG.2023.3243834},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3930-3944},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {RipViz: Finding rip currents by learning pathline behavior},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual diagnostics of parallel performance in training
large-scale DNN models. <em>TVCG</em>, <em>30</em>(7), 3915–3929. (<a
href="https://doi.org/10.1109/TVCG.2023.3243228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnosing the cluster-based performance of large-scale deep neural network (DNN) models during training is essential for improving training efficiency and reducing resource consumption. However, it remains challenging due to the incomprehensibility of the parallelization strategy and the sheer volume of complex data generated in the training processes. Prior works visually analyze performance profiles and timeline traces to identify anomalies from the perspective of individual devices in the cluster, which is not amenable for studying the root cause of anomalies. In this article, we present a visual analytics approach that empowers analysts to visually explore the parallel training process of a DNN model and interactively diagnose the root cause of a performance issue. A set of design requirements is gathered through discussions with domain experts. We propose an enhanced execution flow of model operators for illustrating parallelization strategies within the computational graph layout. We design and implement an enhanced Marey&#39;s graph representation, which introduces the concept of time-span and a banded visual metaphor to convey training dynamics and help experts identify inefficient training processes. We also propose a visual aggregation technique to improve visualization efficiency. We evaluate our approach using case studies, a user study and expert interviews on two large-scale models run in a cluster, namely, the PanGu- $\alpha$ 13B model (40 layers), and the Resnet model (50 layers).},
  archive      = {J_TVCG},
  author       = {Yating Wei and Zhiyong Wang and Zhongwei Wang and Yong Dai and Gongchang Ou and Han Gao and Haitao Yang and Yue Wang and Caleb Chen Cao and Luoxuan Weng and Jiaying Lu and Rongchen Zhu and Wei Chen},
  doi          = {10.1109/TVCG.2023.3243228},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3915-3929},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual diagnostics of parallel performance in training large-scale DNN models},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scanpath prediction on information visualisations.
<em>TVCG</em>, <em>30</em>(7), 3902–3914. (<a
href="https://doi.org/10.1109/TVCG.2023.3242293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Unified Model of Saliency and Scanpaths (UMSS)– a model that learns to predict multi-duration saliency and scanpaths (i.e. sequences of eye fixations) on information visualisations. Although scanpaths provide rich information about the importance of different visualisation elements during the visual exploration process, prior work has been limited to predicting aggregated attention statistics, such as visual saliency. We present in-depth analyses of gaze behaviour for different information visualisation elements (e.g. Title, Label, Data) on the popular MASSVIS dataset. We show that while, overall, gaze patterns are surprisingly consistent across visualisations and viewers, there are also structural differences in gaze dynamics for different elements. Informed by our analyses, UMSS first predicts multi-duration element-level saliency maps, then probabilistically samples scanpaths from them. Extensive experiments on MASSVIS show that our method consistently outperforms state-of-the-art methods with respect to several, widely used scanpath and saliency evaluation metrics. Our method achieves a relative improvement in sequence score of 11.5% for scanpath prediction, and a relative improvement in Pearson correlation coefficient of up to 23.6% for saliency prediction. These results are auspicious and point towards richer user models and simulations of visual attention on visualisations without the need for any eye tracking equipment.},
  archive      = {J_TVCG},
  author       = {Yao Wang and Mihai Bâce and Andreas Bulling},
  doi          = {10.1109/TVCG.2023.3242293},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3902-3914},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Scanpath prediction on information visualisations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Redirected walking on omnidirectional treadmill.
<em>TVCG</em>, <em>30</em>(7), 3884–3901. (<a
href="https://doi.org/10.1109/TVCG.2023.3244359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Redirected walking (RDW) and omnidirectional treadmill (ODT) are two effective solutions to the natural locomotion interface in virtual reality. ODT fully compresses the physical space and can be used as the integration carrier of all kinds of devices. However, the user experience varies in different directions of ODT, and the premise of interaction between users and integrated devices is a good match between virtual and real objects. RDW technology uses visual cues to guide the user&#39;s location in physical space. Based on this principle, combining RDW technology with ODT to guide the user&#39;s walking direction through visual cues can effectively improve user experience on ODT and make full use of various devices integrated on ODT. This paper explores the novel prospects of combining RDW technology with ODT and formally puts forward the concept of O-RDW (ODT-based RDW). Two baseline algorithms, i.e., OS2MD (ODT-based steer to multi-direction), and OS2MT (ODT-based steer to multi-target), are proposed to combine the merits of both RDW and ODT. With the help of the simulation environment, this paper quantitatively analyzes the applicable scenarios of the two algorithms and the influence of several main factors on the performance. Based on the conclusions of the simulation experiments, the two O-RDW algorithms are successfully applied in the practical application case of multi-target haptic feedback. Combined with the user study, the practicability and effectiveness of O-RDW technology in practical use are further verified.},
  archive      = {J_TVCG},
  author       = {Ziyao Wang and Yiye Wang and Shiqi Yan and Zhongzheng Zhu and KanJian Zhang and Haikun Wei},
  doi          = {10.1109/TVCG.2023.3244359},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3884-3901},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Redirected walking on omnidirectional treadmill},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time multi-map saliency-driven gaze behavior for
non-conversational characters. <em>TVCG</em>, <em>30</em>(7), 3871–3883.
(<a href="https://doi.org/10.1109/TVCG.2023.3244679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaze behavior of virtual characters in video games and virtual reality experiences is a key factor of realism and immersion. Indeed, gaze plays many roles when interacting with the environment; not only does it indicate what characters are looking at, but it also plays an important role in verbal and non-verbal behaviors and in making virtual characters alive. Automated computing of gaze behaviors is however a challenging problem, and to date none of the existing methods are capable of producing close-to-real results in an interactive context. We therefore propose a novel method that leverages recent advances in several distinct areas related to visual saliency, attention mechanisms, saccadic behavior modelling, and head-gaze animation techniques. Our approach articulates these advances to converge on a multi-map saliency-driven model which offers real-time realistic gaze behaviors for non-conversational characters, together with additional user-control over customizable features to compose a wide variety of results. We first evaluate the benefits of our approach through an objective evaluation that confronts our gaze simulation with ground truth data using an eye-tracking dataset specifically acquired for this purpose. We then rely on subjective evaluation to measure the level of realism of gaze animations generated by our method, in comparison with gaze animations captured from real actors. Our results show that our method generates gaze behaviors that cannot be distinguished from captured gaze animations. Overall, we believe that these results will open the way for more natural and intuitive design of realistic and coherent gaze animations for real-time applications.},
  archive      = {J_TVCG},
  author       = {Ific Goudé and Alexandre Bruckert and Anne-Hélène Olivier and Julien Pettré and Rémi Cozot and Kadi Bouatouch and Marc Christie and Ludovic Hoyet},
  doi          = {10.1109/TVCG.2023.3244679},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3871-3883},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Real-time multi-map saliency-driven gaze behavior for non-conversational characters},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HairStyle editing via parametric controllable strokes.
<em>TVCG</em>, <em>30</em>(7), 3857–3870. (<a
href="https://doi.org/10.1109/TVCG.2023.3241894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a stroke-based hairstyle editing network, dubbed HairstyleNet, allowing users to conveniently change the hairstyles of an image in an interactive fashion. Different from previous works, we simplify the hairstyle editing process where users can manipulate local or entire hairstyles by adjusting the parameterized hair regions. Our HairstyleNet consists of two stages: a stroke parameterization stage and a stroke-to-hair generation stage. In the stroke parameterization stage, we first introduce parametric strokes to approximate the hair wisps, where the stroke shape is controlled by a quadratic Bézier curve and a thickness parameter. Since rendering strokes with thickness to an image is not differentiable, we opt to leverage a neural renderer to construct the mapping from stroke parameters to a stroke image. Thus, the stroke parameters can be directly estimated from hair regions in a differentiable way, enabling us to flexibly edit the hairstyles of input images. In the stroke-to-hair generation stage, we design a hairstyle refinement network that first encodes coarsely composed images of hair strokes, face, and background into latent representations and then generates high-fidelity face images with desirable new hairstyles from the latent codes. Extensive experiments demonstrate that our HairstyleNet achieves state-of-the-art performance and allows flexible hairstyle manipulation.},
  archive      = {J_TVCG},
  author       = {Xinhui Song and Chen Liu and Youyi Zheng and Zunlei Feng and Lincheng Li and Kun Zhou and Xin Yu},
  doi          = {10.1109/TVCG.2023.3241894},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3857-3870},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HairStyle editing via parametric controllable strokes},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UxSense: Supporting user experience analysis with
visualization and computer vision. <em>TVCG</em>, <em>30</em>(7),
3841–3856. (<a href="https://doi.org/10.1109/TVCG.2023.3241581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing user behavior from usability evaluation can be a challenging and time-consuming task, especially as the number of participants and the scale and complexity of the evaluation grows. We propose uxSense , a visual analytics system using machine learning methods to extract user behavior from audio and video recordings as parallel time-stamped data streams. Our implementation draws on pattern recognition, computer vision, natural language processing, and machine learning to extract user sentiment, actions, posture, spoken words, and other features from such recordings. These streams are visualized as parallel timelines in a web-based front-end, enabling the researcher to search, filter, and annotate data across time and space. We present the results of a user study involving professional UX researchers evaluating user data using uxSense. In fact, we used uxSense itself to evaluate their sessions.},
  archive      = {J_TVCG},
  author       = {Andrea Batch and Yipeng Ji and Mingming Fan and Jian Zhao and Niklas Elmqvist},
  doi          = {10.1109/TVCG.2023.3241581},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3841-3856},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {UxSense: Supporting user experience analysis with visualization and computer vision},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A qualitative interview study of distributed tracing
visualisation: A characterisation of challenges and opportunities.
<em>TVCG</em>, <em>30</em>(7), 3828–3840. (<a
href="https://doi.org/10.1109/TVCG.2023.3241596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed tracing tools have emerged in recent years to enable operators of modern internet applications to troubleshoot cross-component problems in deployed applications. Due to the rich, detailed diagnostic data captured by distributed tracing tools, effectively presenting this data is important. However, use of visualisation to enable sensemaking of this complex data in distributed tracing tools has received relatively little attention. Consequently, operators struggle to make effective use of existing tools. In this article we present the first characterisation of distributed tracing visualisation through a qualitative interview study with six practitioners from two large internet companies. Across two rounds of 1-on-1 interviews we use grounded theory coding to establish users, extract concrete use cases and identify shortcomings of existing distributed tracing tools. We derive guidelines for development of future distributed tracing tools and expose several open research problems that have wide reaching implications for visualisation research and other domains.},
  archive      = {J_TVCG},
  author       = {Thomas Davidson and Emily Wall and Jonathan Mace},
  doi          = {10.1109/TVCG.2023.3241596},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3828-3840},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A qualitative interview study of distributed tracing visualisation: A characterisation of challenges and opportunities},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). XNLI: Explaining and diagnosing NLI-based visual data
analysis. <em>TVCG</em>, <em>30</em>(7), 3813–3827. (<a
href="https://doi.org/10.1109/TVCG.2023.3240003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language interfaces (NLIs) enable users to flexibly specify analytical intentions in data visualization. However, diagnosing the visualization results without understanding the underlying generation process is challenging. Our research explores how to provide explanations for NLIs to help users locate the problems and further revise the queries. We present XNLI, an explainable NLI system for visual data analysis. The system introduces a Provenance Generator to reveal the detailed process of visual transformations, a suite of interactive widgets to support error adjustments, and a Hint Generator to provide query revision hints based on the analysis of user queries and interactions. Two usage scenarios of XNLI and a user study verify the effectiveness and usability of the system. Results suggest that XNLI can significantly enhance task accuracy without interrupting the NLI-based analysis process.},
  archive      = {J_TVCG},
  author       = {Yingchaojie Feng and Xingbo Wang and Bo Pan and Kam Kwai Wong and Yi Ren and Shi Liu and Zihan Yan and Yuxin Ma and Huamin Qu and Wei Chen},
  doi          = {10.1109/TVCG.2023.3240003},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3813-3827},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {XNLI: Explaining and diagnosing NLI-based visual data analysis},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-automatic layout adaptation for responsive
multiple-view visualization design. <em>TVCG</em>, <em>30</em>(7),
3798–3812. (<a href="https://doi.org/10.1109/TVCG.2023.3240356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple-view (MV) visualizations have become ubiquitous for visual communication and exploratory data visualization. However, most existing MV visualizations are designed for the desktop, which can be unsuitable for the continuously evolving displays of varying screen sizes. In this article, we present a two-stage adaptation framework that supports the automated retargeting and semi-automated tailoring of a desktop MV visualization for rendering on devices with displays of varying sizes. First, we cast layout retargeting as an optimization problem and propose a simulated annealing technique that can automatically preserve the layout of multiple views. Second, we enable fine-tuning for the visual appearance of each view, using a rule-based auto configuration method complemented with an interactive interface for chart-oriented encoding adjustment. To demonstrate the feasibility and expressivity of our proposed approach, we present a gallery of MV visualizations that have been adapted from the desktop to small displays. We also report the result of a user study comparing visualizations generated using our approach with those by existing methods. The outcome indicates that the participants generally prefer visualizations generated using our approach and find them to be easier to use.},
  archive      = {J_TVCG},
  author       = {Wei Zeng and Xi Chen and Yihan Hou and Lingdan Shao and Zhe Chu and Remco Chang},
  doi          = {10.1109/TVCG.2023.3240356},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3798-3812},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Semi-automatic layout adaptation for responsive multiple-view visualization design},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual explanation for open-domain question answering with
BERT. <em>TVCG</em>, <em>30</em>(7), 3779–3797. (<a
href="https://doi.org/10.1109/TVCG.2023.3243676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-domain question answering (OpenQA) is an essential but challenging task in natural language processing that aims to answer questions in natural language formats on the basis of large-scale unstructured passages. Recent research has taken the performance of benchmark datasets to new heights, especially when these datasets are combined with techniques for machine reading comprehension based on Transformer models. However, as identified through our ongoing collaboration with domain experts and our review of literature, three key challenges limit their further improvement: (i) complex data with multiple long texts, (ii) complex model architecture with multiple modules, and (iii) semantically complex decision process. In this paper, we present VEQA, a visual analytics system that helps experts understand the decision reasons of OpenQA and provides insights into model improvement. The system summarizes the data flow within and between modules in the OpenQA model as the decision process takes place at the summary, instance and candidate levels. Specifically, it guides users through a summary visualization of dataset and module response to explore individual instances with a ranking visualization that incorporates context. Furthermore, VEQA supports fine-grained exploration of the decision flow within a single module through a comparative tree visualization. We demonstrate the effectiveness of VEQA in promoting interpretability and providing insights into model enhancement through a case study and expert evaluation.},
  archive      = {J_TVCG},
  author       = {Zekai Shao and Shuran Sun and Yuheng Zhao and Siyuan Wang and Zhongyu Wei and Tao Gui and Cagatay Turkay and Siming Chen},
  doi          = {10.1109/TVCG.2023.3243676},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3779-3797},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual explanation for open-domain question answering with BERT},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ClockRay: A wrist-rotation based technique for
occluded-target selection in virtual reality. <em>TVCG</em>,
<em>30</em>(7), 3767–3778. (<a
href="https://doi.org/10.1109/TVCG.2023.3239951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Target selection is one of essential operation made available by interaction techniques in virtual reality (VR) environments. However, effectively positioning or selecting occluded objects is under-investigated in VR, especially in the context of high-density or a high-dimensional data visualization with VR. In this paper, we propose ClockRay , an occluded-object selection technique that can maximize the intrinsic human wrist rotation skills through the integration of emerging ray selection techniques in VR environments. We describe the design space of the ClockRay technique and then evaluate its performance in a series of user studies. Drawing on the experimental results, we discuss the benefits of ClockRay compared to two popular ray selection techniques – RayCursor and RayCasting . Our findings can inform the design of VR-based interactive visualization systems for high-density data.},
  archive      = {J_TVCG},
  author       = {Huiyue Wu and Xiaoxuan Sun and Huawei Tu and Xiaolong Zhang},
  doi          = {10.1109/TVCG.2023.3239951},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3767-3778},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ClockRay: A wrist-rotation based technique for occluded-target selection in virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IF-city: Intelligible fair city planning to measure, explain
and mitigate inequality. <em>TVCG</em>, <em>30</em>(7), 3749–3766. (<a
href="https://doi.org/10.1109/TVCG.2023.3239909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing pervasiveness of Artificial Intelligence (AI), many visual analytics tools have been proposed to examine fairness, but they mostly focus on data scientist users. Instead, tackling fairness must be inclusive and involve domain experts with specialized tools and workflows. Thus, domain-specific visualizations are needed for algorithmic fairness. Furthermore, while much work on AI fairness has focused on predictive decisions, less has been done for fair allocation and planning, which require human expertise and iterative design to integrate myriad constraints. We propose the Intelligible Fair Allocation (IF-Alloc) Framework that leverages explanations of causal attribution (Why), contrastive (Why Not) and counterfactual reasoning (What If, How To) to aid domain experts to assess and alleviate unfairness in allocation problems. We apply the framework to fair urban planning for designing cities that provide equal access to amenities and benefits for diverse resident types. Specifically, we propose an interactive visual tool, Intelligible Fair City Planner (IF-City), to help urban planners to perceive inequality across groups, identify and attribute sources of inequality, and mitigate inequality with automatic allocation simulations and constraint-satisfying recommendations (IF-Plan). We demonstrate and evaluate the usage and usefulness of IF-City on a real neighborhood in New York City, US, with practicing urban planners from multiple countries, and discuss generalizing our findings, application, and framework to other use cases and applications of fair allocation.},
  archive      = {J_TVCG},
  author       = {Yan Lyu and Hangxin Lu and Min Kyung Lee and Gerhard Schmitt and Brian Y. Lim},
  doi          = {10.1109/TVCG.2023.3239909},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3749-3766},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {IF-city: Intelligible fair city planning to measure, explain and mitigate inequality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Skeleton extraction for articulated objects with the
spherical unwrapping profiles. <em>TVCG</em>, <em>30</em>(7), 3731–3748.
(<a href="https://doi.org/10.1109/TVCG.2023.3239370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embedding unified skeletons into unregistered scans is fundamental to finding correspondences, depicting motions, and capturing underlying structures among the articulated objects in the same category. Some existing approaches rely on laborious registration to adapt a predefined LBS model to each input, while others require the input to be set to a canonical pose, e.g., T-pose or A-pose. However, their effectiveness is always influenced by the water-tightness, face topology, and vertex density of the input mesh. At the core of our approach lies a novel unwrapping method, named SUPPLE (Spherical UnwraPping ProfiLEs), which maps a surface into image planes independent of mesh topologies. Based on this lower-dimensional representation, a learning-based framework is further designed to localize and connect skeletal joints with fully convolutional architectures. Experiments demonstrate that our framework yields reliable skeleton extractions across a broad range of articulated categories, from raw scans to online CADs.},
  archive      = {J_TVCG},
  author       = {Zimeng Zhao and Wei Xie and Binghui Zuo and Yangang Wang},
  doi          = {10.1109/TVCG.2023.3239370},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3731-3748},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Skeleton extraction for articulated objects with the spherical unwrapping profiles},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A parametric design method for engraving patterns on thin
shells. <em>TVCG</em>, <em>30</em>(7), 3719–3730. (<a
href="https://doi.org/10.1109/TVCG.2023.3240503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing thin-shell structures that are diverse, lightweight, and physically viable is a challenging task for traditional heuristic methods. To address this challenge, we present a novel parametric design framework for engraving regular, irregular, and customized patterns on thin-shell structures. Our method optimizes pattern parameters such as size and orientation, to ensure structural stiffness while minimizing material consumption. Our method is unique in that it works directly with shapes and patterns represented by functions, and can engrave patterns through simple function operations. By eliminating the need for remeshing in traditional FEM methods, our method is more computationally efficient in optimizing mechanical properties and can significantly increase the diversity of shell structure design. Quantitative evaluation confirms the convergence of the proposed method. We conduct experiments on regular, irregular, and customized patterns and present 3D printed results to demonstrate the effectiveness of our approach.},
  archive      = {J_TVCG},
  author       = {Jiangbei Hu and Shengfa Wang and Ying He and Zhongxuan Luo and Na Lei and Ligang Liu},
  doi          = {10.1109/TVCG.2023.3240503},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3719-3730},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A parametric design method for engraving patterns on thin shells},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time high-quality computer-generated hologram using
complex-valued convolutional neural network. <em>TVCG</em>,
<em>30</em>(7), 3709–3718. (<a
href="https://doi.org/10.1109/TVCG.2023.3239670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Holographic displays are ideal display technologies for virtual and augmented reality because all visual cues are provided. However, real-time high-quality holographic displays are difficult to achieve because the generation of high-quality computer-generated hologram (CGH) is inefficient in existing algorithms. Here, complex-valued convolutional neural network (CCNN) is proposed for phase-only CGH generation. The CCNN-CGH architecture is effective with a simple network structure based on the character design of complex amplitude. A holographic display prototype is set up for optical reconstruction. Experiments verify that state-of-the-art performance is achieved in terms of quality and generation speed in existing end-to-end neural holography methods using the ideal wave propagation model. The generation speed is three times faster than HoloNet and one-sixth faster than Holo-encoder, and the Peak Signal to Noise Ratio (PSNR) is increased by 3 dB and 9 dB, respectively. Real-time high-quality CGHs are generated in 1920 × 1072 and 3840 × 2160 resolutions for dynamic holographic displays.},
  archive      = {J_TVCG},
  author       = {Chongli Zhong and Xinzhu Sang and Binbin Yan and Hui Li and Duo Chen and Xiujuan Qin and Shuo Chen and Xiaoqian Ye},
  doi          = {10.1109/TVCG.2023.3239670},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3709-3718},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Real-time high-quality computer-generated hologram using complex-valued convolutional neural network},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph exploration with embedding-guided layouts.
<em>TVCG</em>, <em>30</em>(7), 3693–3708. (<a
href="https://doi.org/10.1109/TVCG.2023.3238909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Node-link diagrams are widely used to visualize graphs. Most graph layout algorithms only use graph topology for aesthetic goals (e.g., minimize node occlusions and edge crossings) or use node attributes for exploration goals (e.g., preserve visible communities). Existing hybrid methods that bind the two perspectives still suffer from various generation restrictions (e.g., limited input types and required manual adjustments and prior knowledge of graphs) and the imbalance between aesthetic and exploration goals. In this article, we propose a flexible embedding-based graph exploration pipeline to enjoy the best of both graph topology and node attributes. First, we leverage embedding algorithms for attributed graphs to encode the two perspectives into latent space. Then, we present an embedding-driven graph layout algorithm, GEGraph, which can achieve aesthetic layouts with better community preservation to support an easy interpretation of the graph structure. Next, graph explorations are extended based on the generated graph layout and insights extracted from the embedding vectors. Illustrated with examples, we build a layout-preserving aggregation method with Focus+Context interaction and a related nodes searching approach with multiple proximity strategies. Finally, we conduct quantitative and qualitative evaluations, a user study, and two case studies to validate our approach.},
  archive      = {J_TVCG},
  author       = {Leixian Shen and Zhiwei Tai and Enya Shen and Jianmin Wang},
  doi          = {10.1109/TVCG.2023.3238909},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3693-3708},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Graph exploration with embedding-guided layouts},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VR blowing: A physically plausible interaction method for
blowing air in virtual reality. <em>TVCG</em>, <em>30</em>(7),
3680–3692. (<a href="https://doi.org/10.1109/TVCG.2023.3238478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces an interaction method allowing virtual reality (VR) users to interact with virtual objects by blowing air. The proposed method allows users to interact with virtual objects in a physically plausible way by recognizing the intensity of the wind generated by the user&#39;s actual wind blowing activity in the physical world. This is expected to provide immersed VR experience since it enables users to interact with virtual objects in the same way they do in the real world. Three experiments were carried out to develop and improve this method. In the first experiment, we collected the user&#39;s blowing data and used it to model a formula to estimate the speed of the wind from the sound waves obtained through a microphone. In the second experiment, we investigated how much gain can be applied to the formula obtained in the first experiment. The aim is to reduce the lung capacity required to generate wind without compromising physical plausibility. In the third experiment, the advantages and disadvantages of the proposed method compared to the controller-based method were investigated in two scenarios of blowing a ball and a pinwheel. According to the experimental results and participant interview, participants felt a stronger sense of presence and found the VR experience more fun with the proposed blowing interaction method.},
  archive      = {J_TVCG},
  author       = {MinYeong Seo and KyungEun Kang and HyeongYeop Kang},
  doi          = {10.1109/TVCG.2023.3238478},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3680-3692},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VR blowing: A physically plausible interaction method for blowing air in virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Separating shading and reflectance from cartoon
illustrations. <em>TVCG</em>, <em>30</em>(7), 3664–3679. (<a
href="https://doi.org/10.1109/TVCG.2023.3239364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shading plays an important role in cartoon drawings to present the 3D lighting and depth information in a 2D image to improve the visual information and pleasantness. But it also introduces apparent challenges in analyzing and processing the cartoon drawings for different computer graphics and vision applications, such as segmentation, depth estimation, and relighting. Extensive research has been made in removing or separating the shading information to facilitate these applications. Unfortunately, the existing researches only focused on natural images, which are natively different from cartoons since the shading in natural images is physically correct and can be modeled based on physical priors. However, shading in cartoons is manually created by artists, which may be imprecise, abstract, and stylized. This makes it extremely difficult to model the shading in cartoon drawings. Without modeling the shading prior, in the paper, we propose a learning-based solution to separate the shading from the original colors using a two-branch system consisting of two subnetworks. To the best of our knowledge, our method is the first attempt in separating shading information from cartoon drawings. Our method significantly outperforms the methods tailored for natural images. Extensive evaluations have been performed with convincing results in all cases.},
  archive      = {J_TVCG},
  author       = {Ziheng Ma and Chengze Li and Xueting Liu and Huisi Wu and Zhenkun Wen},
  doi          = {10.1109/TVCG.2023.3239364},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3664-3679},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Separating shading and reflectance from cartoon illustrations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Force-directed graph layouts revisited: A new force based on
the t-distribution. <em>TVCG</em>, <em>30</em>(7), 3650–3663. (<a
href="https://doi.org/10.1109/TVCG.2023.3238821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose the t-FDP model, a force-directed placement method based on a novel bounded short-range force (t-force) defined by Student&#39;s t-distribution. Our formulation is flexible, exerts limited repulsive forces for nearby nodes and can be adapted separately in its short- and long-range effects. Using such forces in force-directed graph layouts yields better neighborhood preservation than current methods, while maintaining low stress errors. Our efficient implementation using a Fast Fourier Transform is one order of magnitude faster than state-of-the-art methods and two orders faster on the GPU, enabling us to perform parameter tuning by globally and locally adjusting the t-force in real-time for complex graphs. We demonstrate the quality of our approach by numerical evaluation against state-of-the-art approaches and extensions for interactive exploration.},
  archive      = {J_TVCG},
  author       = {Fahai Zhong and Mingliang Xue and Jian Zhang and Fan Zhang and Rui Ban and Oliver Deussen and Yunhai Wang},
  doi          = {10.1109/TVCG.2023.3238821},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3650-3663},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Force-directed graph layouts revisited: A new force based on the T-distribution},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An examination of the relationship between visualization
media and consumer product evaluation. <em>TVCG</em>, <em>30</em>(7),
3636–3649. (<a href="https://doi.org/10.1109/TVCG.2023.3238428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual product presentations that rely on static images and text are often insufficient to communicate all the information that is necessary to accurately evaluate a product. Technologies such as Virtual Reality (VR) or Augmented Reality (AR) have enabled more sophisticated representation methods, but certain product characteristics are difficult to assess and may result in perceptual differences when a product is evaluated in different visual media. In this article, we report two case studies in which a group of participants evaluated three designs of two product typologies (i.e., a desktop telephone and a coffee maker) as presented in three different visual media (i.e., photorealistic renderings, AR, and VR for the first case study; and photographs, a non-immersive virtual environment, and AR for the second case study) using eight semantic scales. An inferential statistical method using Aligned Rank Transform (ART) proceedings was applied to determine perceptual differences between groups. Our results show that in both cases product attributes in Jordan&#39;s physio-pleasure category are the most affected by the presentation media. The socio-pleasure category was also affected for the case of the coffee makers. The level of immersion afforded by the medium significantly affects product evaluation.},
  archive      = {J_TVCG},
  author       = {Almudena Palacios-Ibáñez and Simon Pirault and Francesc Ochando-Martí and Manuel Contero and Jorge D. Camba},
  doi          = {10.1109/TVCG.2023.3238428},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3636-3649},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {An examination of the relationship between visualization media and consumer product evaluation},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regenerating arbitrary video sequences with distillation
path-finding. <em>TVCG</em>, <em>30</em>(7), 3622–3635. (<a
href="https://doi.org/10.1109/TVCG.2023.3237739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {If the video has long been mentioned as a widespread visualization form, the animation sequence in the video is mentioned as storytelling for people. Producing an animation requires intensive human labor from skilled professional artists to obtain plausible animation in both content and motion direction, incredibly for animations with complex content, multiple moving objects, and dense movement. This article presents an interactive framework to generate new sequences according to the users’ preference on the starting frame. The critical contrast of our approach versus prior work and existing commercial applications is that novel sequences with arbitrary starting frame are produced by our system with a consistent degree in both content and motion direction. To achieve this effectively, we first learn the feature correlation on the frameset of the given video through a proposed network called RSFNet. Then, we develop a novel path-finding algorithm, SDPF, which formulates the knowledge of motion directions of the source video to estimate the smooth and plausible sequences. The extensive experiments show that our framework can produce new animations on the cartoon and natural scenes and advance prior works and commercial applications to enable users to obtain more predictable results.},
  archive      = {J_TVCG},
  author       = {Thi-Ngoc-Hanh Le and Sheng-Yi Yao and Chun-Te Wu and Tong-Yee Lee},
  doi          = {10.1109/TVCG.2023.3237739},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3622-3635},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Regenerating arbitrary video sequences with distillation path-finding},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Specular path generation and near-reflective diffraction in
interactive acoustical simulations. <em>TVCG</em>, <em>30</em>(7),
3609–3621. (<a href="https://doi.org/10.1109/TVCG.2023.3238662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most systems for simulating sound propagation in a virtual environment for interactive applications use ray- or path-based models of sound. With these models, the “early” (low-order) specular reflection paths play a key role in defining the “sound” of the environment. However, the wave nature of sound, and the fact that smooth objects are approximated by triangle meshes, pose challenges for creating realistic approximations of the reflection results. Existing methods which produce accurate results are too slow to be used in most interactive applications with dynamic scenes. This paper presents a method for reflections modeling called spatially sampled near-reflective diffraction (SSNRD), based on an existing approximate diffraction model, Volumetric Diffraction and Transmission (VDaT). The SSNRD model addresses the challenges mentioned above, produces results accurate to within 1-2 dB on average compared to edge diffraction, and is fast enough to generate thousands of paths in a few milliseconds in large scenes. This method encompasses scene geometry processing, path trajectory generation, spatial sampling for diffraction modeling, and a small deep neural network (DNN) to produce the final response of each path. All steps of the method are GPU-accelerated, and NVIDIA RTX real-time ray tracing hardware is used for spatial computing tasks beyond just traditional ray tracing.},
  archive      = {J_TVCG},
  author       = {Louis Pisha and Shahrokh Yadegari},
  doi          = {10.1109/TVCG.2023.3238662},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3609-3621},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Specular path generation and near-reflective diffraction in interactive acoustical simulations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ShortcutLens: A visual analytics approach for exploring
shortcuts in natural language understanding dataset. <em>TVCG</em>,
<em>30</em>(7), 3594–3608. (<a
href="https://doi.org/10.1109/TVCG.2023.3236380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benchmark datasets play an important role in evaluating Natural Language Understanding (NLU) models. However, shortcuts—unwanted biases in the benchmark datasets—can damage the effectiveness of benchmark datasets in revealing models’ real capabilities. Since shortcuts vary in coverage, productivity, and semantic meaning, it is challenging for NLU experts to systematically understand and avoid them when creating benchmark datasets. In this paper, we develop a visual analytics system, ShortcutLens , to help NLU experts explore shortcuts in NLU benchmark datasets. The system allows users to conduct multi-level exploration of shortcuts. Specifically, Statistics View helps users grasp the statistics such as coverage and productivity of shortcuts in the benchmark dataset. Template View employs hierarchical and interpretable templates to summarize different types of shortcuts. Instance View allows users to check the corresponding instances covered by the shortcuts. We conduct case studies and expert interviews to evaluate the effectiveness and usability of the system. The results demonstrate that ShortcutLens supports users in gaining a better understanding of benchmark dataset issues through shortcuts, inspiring them to create challenging and pertinent benchmark datasets.},
  archive      = {J_TVCG},
  author       = {Zhihua Jin and Xingbo Wang and Furui Cheng and Chunhui Sun and Qun Liu and Huamin Qu},
  doi          = {10.1109/TVCG.2023.3236380},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3594-3608},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ShortcutLens: A visual analytics approach for exploring shortcuts in natural language understanding dataset},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STTAR: Surgical tool tracking using off-the-shelf augmented
reality head-mounted displays. <em>TVCG</em>, <em>30</em>(7), 3578–3593.
(<a href="https://doi.org/10.1109/TVCG.2023.3238309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of Augmented Reality (AR) for navigation purposes has shown beneficial in assisting physicians during the performance of surgical procedures. These applications commonly require knowing the pose of surgical tools and patients to provide visual information that surgeons can use during the performance of the task. Existing medical-grade tracking systems use infrared cameras placed inside the Operating Room (OR) to identify retro-reflective markers attached to objects of interest and compute their pose. Some commercially available AR Head-Mounted Displays (HMDs) use similar cameras for self-localization, hand tracking, and estimating the objects’ depth. This work presents a framework that uses the built-in cameras of AR HMDs to enable accurate tracking of retro-reflective markers without the need to integrate any additional electronics into the HMD. The proposed framework can simultaneously track multiple tools without having previous knowledge of their geometry and only requires establishing a local network between the headset and a workstation. Our results show that the tracking and detection of the markers can be achieved with an accuracy of $0.09\pm 0.06\ mm$ on lateral translation, $0.42 \pm 0.32\ mm$ on longitudinal translation and $0.80 \pm 0.39^\circ$ for rotations around the vertical axis. Furthermore, to showcase the relevance of the proposed framework, we evaluate the system&#39;s performance in the context of surgical procedures. This use case was designed to replicate the scenarios of k-wire insertions in orthopedic procedures. For evaluation, seven surgeons were provided with visual navigation and asked to perform 24 injections using the proposed framework. A second study with ten participants served to investigate the capabilities of the framework in the context of more general scenarios. Results from these studies provided comparable accuracy to those reported in the literature for AR-based navigation procedures.},
  archive      = {J_TVCG},
  author       = {Alejandro Martin-Gomez and Haowei Li and Tianyu Song and Sheng Yang and Guangzhi Wang and Hui Ding and Nassir Navab and Zhe Zhao and Mehran Armand},
  doi          = {10.1109/TVCG.2023.3238309},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3578-3593},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {STTAR: Surgical tool tracking using off-the-shelf augmented reality head-mounted displays},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Path tracing in 2D, 3D, and physicalized networks.
<em>TVCG</em>, <em>30</em>(7), 3564–3577. (<a
href="https://doi.org/10.1109/TVCG.2023.3238989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is common to advise against using 3D to visualize abstract data such as networks, however Ware and Mitchell&#39;s 2008 study showed that path tracing in a network is less error prone in 3D than in 2D. It is unclear, however, if 3D retains its advantage when the 2D presentation of a network is improved using edge-routing, and when simple interaction techniques for exploring the network are available. We address this with two studies of path tracing under new conditions. The first study was preregistered, involved 34 users, and compared 2D and 3D layouts that the user could rotate and move in virtual reality with a handheld controller. Error rates were lower in 3D than in 2D, despite the use of edge-routing in 2D and the use of mouse-driven interactive highlighting of edges. The second study involved 12 users and investigated data physicalization, comparing 3D layouts in virtual reality versus physical 3D printouts of networks augmented with a Microsoft HoloLens headset. No difference was found in error rate, but users performed a variety of actions with their fingers in the physical condition which can inform new interaction techniques.},
  archive      = {J_TVCG},
  author       = {Michael J. McGuffin and Ryan Servera and Marie Forest},
  doi          = {10.1109/TVCG.2023.3238989},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3564-3577},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Path tracing in 2D, 3D, and physicalized networks},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CSDN: Cross-modal shape-transfer dual-refinement network for
point cloud completion. <em>TVCG</em>, <em>30</em>(7), 3545–3563. (<a
href="https://doi.org/10.1109/TVCG.2023.3236061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How will you repair a physical object with some missings? You may imagine its original shape from previously captured images, recover its overall (global) but coarse shape first, and then refine its local details. We are motivated to imitate the physical repair procedure to address point cloud completion. To this end, we propose a cross-modal shape-transfer dual-refinement network (termed CSDN), a coarse-to-fine paradigm with images of full-cycle participation, for quality point cloud completion. CSDN mainly consists of “shape fusion” and “dual-refinement” modules to tackle the cross-modal challenge. The first module transfers the intrinsic shape characteristics from single images to guide the geometry generation of the missing regions of point clouds, in which we propose IPAdaIN to embed the global features of both the image and the partial point cloud into completion. The second module refines the coarse output by adjusting the positions of the generated points, where the local refinement unit exploits the geometric relation between the novel and the input points by graph convolution, and the global constraint unit utilizes the input image to fine-tune the generated offset. Different from most existing approaches, CSDN not only explores the complementary information from images but also effectively exploits cross-modal data in the whole coarse-to-fine completion procedure. Experimental results indicate that CSDN performs favorably against twelve competitors on the cross-modal benchmark.},
  archive      = {J_TVCG},
  author       = {Zhe Zhu and Liangliang Nan and Haoran Xie and Honghua Chen and Jun Wang and Mingqiang Wei and Jing Qin},
  doi          = {10.1109/TVCG.2023.3236061},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3545-3563},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CSDN: Cross-modal shape-transfer dual-refinement network for point cloud completion},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continuous scatterplot operators for bivariate analysis and
study of electronic transitions. <em>TVCG</em>, <em>30</em>(7),
3532–3544. (<a href="https://doi.org/10.1109/TVCG.2023.3237768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic transitions in molecules due to the absorption or emission of light is a complex quantum mechanical process. Their study plays an important role in the design of novel materials. A common yet challenging task in the study is to determine the nature of electronic transitions, namely which subgroups of the molecule are involved in the transition by donating or accepting electrons, followed by an investigation of the variation in the donor-acceptor behavior for different transitions or conformations of the molecules. In this article, we present a novel approach for the analysis of a bivariate field and show its applicability to the study of electronic transitions. This approach is based on two novel operators, the continuous scatterplot (CSP) lens operator and the CSP peel operator, that enable effective visual analysis of bivariate fields. Both operators can be applied independently or together to facilitate analysis. The operators motivate the design of control polygon inputs to extract fiber surfaces of interest in the spatial domain. The CSPs are annotated with a quantitative measure to further support the visual analysis. We study different molecular systems and demonstrate how the CSP peel and CSP lens operators help identify and study donor and acceptor characteristics in molecular systems.},
  archive      = {J_TVCG},
  author       = {Mohit Sharma and Talha Bin Masood and Signe Sidwall Thygesen and Mathieu Linares and Ingrid Hotz and Vijay Natarajan},
  doi          = {10.1109/TVCG.2023.3237768},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3532-3544},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Continuous scatterplot operators for bivariate analysis and study of electronic transitions},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stripe sensitive convolution for omnidirectional image
dehazing. <em>TVCG</em>, <em>30</em>(7), 3516–3531. (<a
href="https://doi.org/10.1109/TVCG.2022.3233900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The haze in a scenario may affect the 360 photo/video quality and the immersive 360 $^{\circ }$ virtual reality (VR) experience. The recent single image dehazing methods, to date, have been only focused on plane images. In this work, we propose a novel neural network pipeline for single omnidirectional image dehazing. To create the pipeline, we build the first hazy omnidirectional image dataset, which contains both synthetic and real-world samples. Then, we propose a new stripe sensitive convolution (SSConv) to handle the distortion problems due to the equirectangular projections. The SSConv calibrates distortion in two steps: 1) extracting features using different rectangular filters and, 2) learning to select the optimal features by a weighting of the feature stripes (a series of rows in the feature maps). Subsequently, using SSConv, we design an end-to-end network that jointly learns haze removal and depth estimation from a single omnidirectional image. The estimated depth map is leveraged as the intermediate representation and provides global context and geometric information to the dehazing module. Extensive experiments on challenging synthetic and real-world omnidirectional image datasets demonstrate the effectiveness of SSConv, and our network attains superior dehazing performance. The experiments on practical applications also demonstrate that our method can significantly improve the 3-D object detection and 3-D layout performances for hazy omnidirectional images.},
  archive      = {J_TVCG},
  author       = {Dong Zhao and Jia Li and Hongyu Li and Long Xu},
  doi          = {10.1109/TVCG.2022.3233900},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3516-3531},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Stripe sensitive convolution for omnidirectional image dehazing},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparative study and evaluation of hybrid visualizations of
graphs. <em>TVCG</em>, <em>30</em>(7), 3503–3515. (<a
href="https://doi.org/10.1109/TVCG.2022.3233389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid visualizations combine different metaphors into a single network layout, in order to help humans in finding the “right way” of displaying the different portions of the network, especially when it is globally sparse and locally dense. We investigate hybrid visualizations in two complementary directions: (i) On the one hand, we evaluate the effectiveness of different hybrid visualization models through a comparative user study; (ii) On the other hand, we estimate the usefulness of an interactive visualization that integrates all the considered hybrid models together. The results of our study provide some hints about the usefulness of the different hybrid visualizations for specific tasks of analysis and indicates that integrating different hybrid models into a single visualization may offer a valuable tool of analysis.},
  archive      = {J_TVCG},
  author       = {Emilio Di Giacomo and Walter Didimo and Giuseppe Liotta and Fabrizio Montecchiani and Alessandra Tappini},
  doi          = {10.1109/TVCG.2022.3233389},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3503-3515},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Comparative study and evaluation of hybrid visualizations of graphs},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reasoning affordances with tables and bar charts.
<em>TVCG</em>, <em>30</em>(7), 3487–3502. (<a
href="https://doi.org/10.1109/TVCG.2022.3232959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A viewer&#39;s existing beliefs can prevent accurate reasoning with data visualizations. In particular, confirmation bias can cause people to overweigh information that confirms their beliefs, and dismiss information that disconfirms them. We tested whether confirmation bias exists when people reason with visualized data and whether certain visualization designs can elicit less biased reasoning strategies. We asked crowdworkers to solve reasoning problems that had the potential to evoke both poor reasoning strategies and confirmation bias. We created two scenarios, one in which we primed people with a belief before asking them to make a decision, and another in which people held pre-existing beliefs. The data was presented as either a table, a bar table, or a bar chart. To correctly solve the problem, participants should use a complex reasoning strategy to compare two ratios, each between two pairs of values. But participants could also be tempted to use simpler, superficial heuristics, shortcuts, or biased strategies to reason about the problem. Presenting the data in a table format helped participants reason with the correct ratio strategy while showing the data as a bar table or a bar chart led participants towards incorrect heuristics. Confirmation bias was not significantly present when beliefs were primed, but it was present when beliefs were pre-existing. Additionally, the table presentation format was more likely to afford the ratio reasoning strategy, and the use of ratio strategy was more likely to lead to the correct answer. These findings suggest that data presentation formats can affect affordances for reasoning.},
  archive      = {J_TVCG},
  author       = {Cindy Xiong and Elsie Lee-Robbins and Icy Zhang and Aimen Gaba and Steven Franconeri},
  doi          = {10.1109/TVCG.2022.3232959},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3487-3502},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Reasoning affordances with tables and bar charts},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Keyframe control of music-driven 3D dance generation.
<em>TVCG</em>, <em>30</em>(7), 3474–3486. (<a
href="https://doi.org/10.1109/TVCG.2023.3235538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For 3D animators, choreography with artificial intelligence has attracted more attention recently. However, most existing deep learning methods mainly rely on music for dance generation and lack sufficient control over generated dance motions. To address this issue, we introduce the idea of keyframe interpolation for music-driven dance generation and present a novel transition generation technique for choreography. Specifically, this technique synthesizes visually diverse and plausible dance motions by using normalizing flows to learn the probability distribution of dance motions conditioned on a piece of music and a sparse set of key poses. Thus, the generated dance motions respect both the input musical beats and the key poses. To achieve a robust transition of varying lengths between the key poses, we introduce a time embedding at each timestep as an additional condition. Extensive experiments show that our model generates more realistic, diverse, and beat-matching dance motions than the compared state-of-the-art methods, both qualitatively and quantitatively. Our experimental results demonstrate the superiority of the keyframe-based control for improving the diversity of the generated dance motions.},
  archive      = {J_TVCG},
  author       = {Zhipeng Yang and Yu-Hui Wen and Shu-Yu Chen and Xiao Liu and Yuan Gao and Yong-Jin Liu and Lin Gao and Hongbo Fu},
  doi          = {10.1109/TVCG.2023.3235538},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3474-3486},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Keyframe control of music-driven 3D dance generation},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MitoVis: A unified visual analytics system for end-to-end
neuronal mitochondria analysis. <em>TVCG</em>, <em>30</em>(7),
3457–3473. (<a href="https://doi.org/10.1109/TVCG.2022.3233548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurons have a polarized structure, with dendrites and axons, and compartment-specific functions can be affected by the dwelling mitochondria. Recent studies have shown that the morphology of mitochondria is closely related to the functions of neurons and neurodegenerative diseases. However, the conventional mitochondria analysis workflow mainly relies on manual annotations and generic image-processing software. Moreover, even though there have been recent developments in automatic mitochondria analysis using deep learning, the application of existing methods in a daily analysis remains challenging because the performance of a pretrained deep learning model can vary depending on the target data, and there are always errors in inference time, requiring human proofreading. To address these issues, we introduce MitoVis , a novel visualization system for end-to-end data processing and an interactive analysis of the morphology of neuronal mitochondria. MitoVis introduces a novel active learning framework based on recent contrastive learning, which allows accurate fine-tuning of the neural network model. MitoVis also provides novel visual guides for interactive proofreading so that users can quickly identify and correct errors in the result with minimal effort. We demonstrate the usefulness and efficacy of the system via case studies conducted by neuroscientists. The results show that MitoVis achieved up to 13.3× faster total analysis time in the case study compared to the conventional manual analysis workflow.},
  archive      = {J_TVCG},
  author       = {JunYoung Choi and Hyun-Jic Oh and Hakjun Lee and Suyeon Kim and Seok-Kyu Kwon and Won-Ki Jeong},
  doi          = {10.1109/TVCG.2022.3233548},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3457-3473},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MitoVis: A unified visual analytics system for end-to-end neuronal mitochondria analysis},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identity-aware and shape-aware propagation of face editing
in videos. <em>TVCG</em>, <em>30</em>(7), 3444–3456. (<a
href="https://doi.org/10.1109/TVCG.2023.3235364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of deep generative models has inspired various facial image editing methods, but many of them are difficult to be directly applied to video editing due to various challenges ranging from imposing 3D constraints, preserving identity consistency, ensuring temporal coherence, etc. To address these challenges, we propose a new framework operating on the StyleGAN2 latent space for identity-aware and shape-aware edit propagation on face videos. In order to reduce the difficulties of maintaining the identity, keeping the original 3D motion, and avoiding shape distortions, we disentangle the StyleGAN2 latent vectors of human face video frames to decouple the appearance, shape, expression, and motion from identity. An edit encoding module is used to map a sequence of image frames to continuous latent codes with 3D parametric control and is trained in a self-supervised manner with identity loss and triple shape losses. Our model supports propagation of edits in various forms: I. direct appearance editing on a specific keyframe, II. implicit editing of face shape via a given reference image, and III. existing latent-based semantic edits. Experiments show that our method works well for various forms of videos in the wild and outperforms an animation-based approach and the recent deep generative techniques.},
  archive      = {J_TVCG},
  author       = {Yue-Ren Jiang and Shu-Yu Chen and Hongbo Fu and Lin Gao},
  doi          = {10.1109/TVCG.2023.3235364},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3444-3456},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Identity-aware and shape-aware propagation of face editing in videos},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From invisible to visible: Impacts of metadata in
communicative data visualization. <em>TVCG</em>, <em>30</em>(7),
3427–3443. (<a href="https://doi.org/10.1109/TVCG.2022.3231716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leaving the context of visualizations invisible can have negative impacts on understanding and transparency. While common wisdom suggests that recontextualizing visualizations with metadata (e.g., disclosing the data source or instructions for decoding the visualizations’ encoding) may counter these effects, the impact remains largely unknown. To fill this gap, we conducted two experiments. In Experiment 1, we explored how chart type, topic, and user goal impacted which categories of metadata participants deemed most relevant. We presented 64 participants with four real-world visualizations. For each visualization, participants were given four goals and selected the type of metadata they most wanted from a set of 18 types. Our results indicated that participants were most interested in metadata which explained the visualization&#39;s encoding for goals related to understanding and metadata about the source of the data for assessing trustworthiness. In Experiment 2, we explored how these two types of metadata impact transparency, trustworthiness and persuasiveness, information relevance, and understanding. We asked 144 participants to explain the main message of two pairs of visualizations (one with metadata and one without); rate them on scales of transparency and relevance; and then predict the likelihood that they were selected for a presentation to policymakers. Our results suggested that visualizations with metadata were perceived as more thorough than those without metadata, but similarly relevant, accurate, clear, and complete. Additionally, we found that metadata did not impact the accuracy of the information extracted from visualizations, but may have influenced which information participants remembered as important or interesting.},
  archive      = {J_TVCG},
  author       = {Alyxander Burns and Christiana Lee and Thai On and Cindy Xiong and Evan Peck and Narges Mahyar},
  doi          = {10.1109/TVCG.2022.3231716},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3427-3443},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {From invisible to visible: Impacts of metadata in communicative data visualization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From noise addition to denoising: A self-variation capture
network for point cloud optimization. <em>TVCG</em>, <em>30</em>(7),
3413–3426. (<a href="https://doi.org/10.1109/TVCG.2022.3231680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point clouds obtained from 3D scanners are often noisy and cannot be directly used for subsequent high-level tasks. In this article, we propose a novel point cloud optimization method capable of denoising and homogenizing point clouds. Our idea is based on the assumption that the noise is generally much smaller than the effective signal. We perform noise perturbation on the noisy point cloud to get a new noisy point cloud, called self-variation point cloud. The noisy point cloud and self-variation point cloud have different noise distribution, but the same point cloud distribution. We compute the potential commonality between two noisy point clouds to obtain a clean point cloud. To implement our idea, we propose a Self-Variation Capture Network (SVCNet). We perturb the point cloud features in the latent space to obtain self-variation feature vectors, and capture the commonality between two noisy feature vectors through the feature aggregation and averaging. In addition, an edge constraint module is introduced to suppress low-pass effects during denoising. Our denoising method does not take into account the noise characteristics, and can filter the drift noise located on the underlying surface, resulting in a uniform distribution of the generated point cloud. The experimental results show that our algorithm outperforms the current state-of-the-art algorithms, especially in generating more uniform point clouds. In addition, extended experiments demonstrate the potential of our algorithm for point clouds upsampling.},
  archive      = {J_TVCG},
  author       = {Tianming Zhao and Peng Gao and Tian Tian and Jiayi Ma and Jinwen Tian},
  doi          = {10.1109/TVCG.2022.3231680},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3413-3426},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {From noise addition to denoising: A self-variation capture network for point cloud optimization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tasks and visualizations used for data profiling: A survey
and interview study. <em>TVCG</em>, <em>30</em>(7), 3400–3412. (<a
href="https://doi.org/10.1109/TVCG.2023.3234337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of good-quality data to inform decision making is entirely dependent on robust processes to ensure it is fit for purpose. Such processes vary between organisations, and between those tasked with designing and following them. In this article we report on a survey of 53 data analysts from many industry sectors, 24 of whom also participated in in-depth interviews, about computational and visual methods for characterizing data and investigating data quality. The paper makes contributions in two key areas. The first is to data science fundamentals, because our lists of data profiling tasks and visualization techniques are more comprehensive than those published elsewhere. The second concerns the application question “what does good profiling look like to those who routinely perform it?”, which we answer by highlighting the diversity of profiling tasks, unusual practice and exemplars of visualization, and recommendations about formalizing processes and creating rulebooks.},
  archive      = {J_TVCG},
  author       = {Roy A. Ruddle and James Cheshire and Sara Johansson Fernstad},
  doi          = {10.1109/TVCG.2023.3234337},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3400-3412},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Tasks and visualizations used for data profiling: A survey and interview study},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SubLinearForce: Fully sublinear-time force computation for
large complex graph drawing. <em>TVCG</em>, <em>30</em>(7), 3386–3399.
(<a href="https://doi.org/10.1109/TVCG.2022.3233287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent works in graph visualization attempt to reduce the runtime of repulsion force computation of force-directed algorithms using sampling. However, they fail to reduce the runtime for attraction force computation to sublinear in the number of edges. We present the SubLinearForce framework for a fully sublinear-time force computation algorithm for drawing large complex graphs. More precisely, we present new sublinear-time algorithms for the attraction force computation of force-directed algorithms. We then integrate them with sublinear-time repulsion force computation to give a fully sublinear-time force computation. Extensive experiments show that our algorithms compute layouts on average 80% faster than the existing linear-time force computation algorithm, while obtaining significantly better quality metrics such as edge crossing and shape-based metrics.},
  archive      = {J_TVCG},
  author       = {Amyra Meidiana and Seok-Hee Hong and Shijun Cai and Marnijati Torkel and Peter Eades},
  doi          = {10.1109/TVCG.2022.3233287},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3386-3399},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SubLinearForce: Fully sublinear-time force computation for large complex graph drawing},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D surface-closed mesh clipping based on polygonal
partitioning for surgical planning. <em>TVCG</em>, <em>30</em>(7),
3374–3385. (<a href="https://doi.org/10.1109/TVCG.2022.3230739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to create an efficient and accurate interactive tool for triangular mesh clipping is one of the key problems to be solved in computer-assisted surgical planning. Although the existing algorithms can realize three-dimensional model clipping, problems still remain unsolved regarding the flexibility of clipping paths and the capping of clipped cross-sections. In this study, we propose a mesh clipping algorithm for surgical planning based on polygonal convex partitioning. First, two-dimensional polygonal regions are extended to three-dimensional clipping paths generated from selected reference points. Second, the convex regions are partitioned with a recursive algorithm to obtain the clipped and residual models with closed surfaces. Finally, surgical planning software with the function of mesh clipping has been developed, which is capable to create complex clipping paths by normal vector adjustment and thickness control. The robustness and efficiency of our algorithm have been demonstrated by surgical planning of craniomaxillofacial osteotomy, pelvis tumor resection and cranial vault remodeling.},
  archive      = {J_TVCG},
  author       = {Mingjun Gong and Xiaojun Chen},
  doi          = {10.1109/TVCG.2022.3230739},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3374-3385},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {3D surface-closed mesh clipping based on polygonal partitioning for surgical planning},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards efficient visual simplification of computational
graphs in deep neural networks. <em>TVCG</em>, <em>30</em>(7),
3359–3373. (<a href="https://doi.org/10.1109/TVCG.2022.3230832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A computational graph in a deep neural network (DNN) denotes a specific data flow diagram (DFD) composed of many tensors and operators. Existing toolkits for visualizing computational graphs are not applicable when the structure is highly complicated and large-scale (e.g., BERT (Devlin et al. 2019)). To address this problem, we propose leveraging a suite of visual simplification techniques, including a cycle-removing method, a module-based edge-pruning algorithm, and an isomorphic subgraph stacking strategy. We design and implement an interactive visualization system that is suitable for computational graphs with up to 10 thousand elements. Experimental results and usage scenarios demonstrate that our tool reduces 60% elements on average and hence enhances the performance for recognizing and diagnosing DNN models. Our contributions are integrated into an open-source DNN visualization toolkit, namely, MindInsight [2].},
  archive      = {J_TVCG},
  author       = {Rusheng Pan and Zhiyong Wang and Yating Wei and Han Gao and Gongchang Ou and Caleb Chen Cao and Jingli Xu and Tong Xu and Wei Chen},
  doi          = {10.1109/TVCG.2022.3230832},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3359-3373},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Towards efficient visual simplification of computational graphs in deep neural networks},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatio-temporal visual analysis of turbulent superstructures
in unsteady flow. <em>TVCG</em>, <em>30</em>(7), 3346–3358. (<a
href="https://doi.org/10.1109/TVCG.2022.3232367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The large-scale motions in 3D turbulent channel flows, known as Turbulent Superstructures (TSS), play an essential role in the dynamics of small-scale structures within the turbulent boundary layer. However, as of today, there is no common agreement on the spatial and temporal relationships between these multiscale structures. We propose a novel space-time visualization technique for analyzing the temporal evolution of these multiscale structures in their spatial context and, thus, to further shed light on the conceptually different explanations of their dynamics. Since the temporal dynamics of TSS are believed to influence the structures in the turbulent boundary layer, we propose a combination of a 2D space-time velocity plot with an orthogonal 2D plot of projected 3D flow structures, which can interactively span the time and the space axis. Besides flow structures indicating the fluid motion, we propose showing the variations in derived fields as an additional source of explanation. The relationships between the structures in different spatial and temporal scales can be more effectively resolved by using various filtering operations and image registration algorithms. To reduce the information loss due to the non-injective nature of projection, spatial information is encoded into transparency or color. Since the proposed visualization is heavily demanding computational resources and memory bandwidth to stream unsteady flow fields and instantly compute derived 3D flow structures, the implementation exploits data compression, parallel computation capabilities, and high memory bandwidth on recent GPUs via the CUDA compute library.},
  archive      = {J_TVCG},
  author       = {Behdad Ghaffari and Davide Gatti and Rüdiger Westermann},
  doi          = {10.1109/TVCG.2022.3232367},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3346-3358},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Spatio-temporal visual analysis of turbulent superstructures in unsteady flow},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GPU accelerated 3D tomographic reconstruction and
visualization from noisy electron microscopy tilt-series. <em>TVCG</em>,
<em>30</em>(7), 3331–3345. (<a
href="https://doi.org/10.1109/TVCG.2022.3230445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel framework for 3D tomographic reconstruction and visualization of tomograms from noisy electron microscopy tilt-series. Our technique takes as an input aligned tilt-series from cryogenic electron microscopy and creates denoised 3D tomograms using a proximal jointly-optimized approach that iteratively performs reconstruction and denoising, relieving the users of the need to select appropriate denoising algorithms in the pre-reconstruction or post-reconstruction steps. The whole process is accelerated by exploiting parallelism on modern GPUs, and the results can be visualized immediately after the reconstruction using volume rendering tools incorporated in the framework. We show that our technique can be used with multiple combinations of reconstruction algorithms and regularizers, thanks to the flexibility provided by proximal algorithms. Additionally, the reconstruction framework is open-source and can be easily extended with additional reconstruction and denoising methods. Furthermore, our approach enables visualization of reconstruction error throughout the iterative process within the reconstructed tomogram and on projection planes of the input tilt-series. We evaluate our approach in comparison with state-of-the-art approaches and additionally show how our error visualization can be used for reconstruction evaluation.},
  archive      = {J_TVCG},
  author       = {Julio Rey Ramirez and Peter Rautek and Ciril Bohak and Ondřej Strnad and Zheyuan Zhang and Sai Li and Ivan Viola and Wolfgang Heidrich},
  doi          = {10.1109/TVCG.2022.3230445},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3331-3345},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GPU accelerated 3D tomographic reconstruction and visualization from noisy electron microscopy tilt-series},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalability in visualization. <em>TVCG</em>, <em>30</em>(7),
3314–3330. (<a href="https://doi.org/10.1109/TVCG.2022.3231230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a conceptual model for scalability designed for visualization research. With this model, we systematically analyze over 120 visualization publications from 1990 to 2020 to characterize the different notions of scalability in these works. While many article have addressed scalability issues, our survey identifies a lack of consistency in the use of the term in the visualization research community. We address this issue by introducing a consistent terminology meant to help visualization researchers better characterize the scalability aspects in their research. It also helps in providing multiple methods for supporting the claim that a work is “scalable.” Our model is centered around an effort function with inputs and outputs. The inputs are the problem size and resources, whereas the outputs are the actual efforts, for instance, in terms of computational run time or visual clutter. We select representative examples to illustrate different approaches and facets of what scalability can mean in visualization literature. Finally, targeting the diverse crowd of visualization researchers without a scalability tradition, we provide a set of recommendations for how scalability can be presented in a clear and consistent way to improve fair comparison between visualization techniques and systems and foster reproducibility.},
  archive      = {J_TVCG},
  author       = {Gaëlle Richer and Alexis Pister and Moataz Abdelaal and Jean-Daniel Fekete and Michael Sedlmair and Daniel Weiskopf},
  doi          = {10.1109/TVCG.2022.3231230},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3314-3330},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Scalability in visualization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kine-appendage: Enhancing freehand VR interaction through
transformations of virtual appendages. <em>TVCG</em>, <em>30</em>(7),
3298–3313. (<a href="https://doi.org/10.1109/TVCG.2022.3230746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kinesthetic feedback, the feeling of restriction or resistance when hands contact objects, is essential for natural freehand interaction in VR. However, inducing kinesthetic feedback using mechanical hardware can be cumbersome and hard to control in commodity VR systems. We propose the kine-appendage concept to compensate for the loss of kinesthetic feedback in virtual environments, i.e., a virtual appendage is added to the user&#39;s avatar hand; when the appendage contacts a virtual object, it exhibits transformations (rotation and deformation); when it disengages from the contact, it recovers its original appearance. A proof-of-concept kine-appendage technique, BrittleStylus , was designed to enhance isomorphic typing. Our empirical evaluations demonstrated that (i) BrittleStylus significantly reduced the uncorrected error rate of naive isomorphic typing from 6.53% to 1.92% without compromising the typing speed; (ii) BrittleStylus could induce the sense of kinesthetic feedback, the degree of which was parity with that induced by pseudo-haptic (+ visual cue) methods; and (iii) participants preferred BrittleStylus over pseudo-haptic (+ visual cue) methods because of not only good performance but also fluent hand movements.},
  archive      = {J_TVCG},
  author       = {Yang Tian and Hualong Bai and Shengdong Zhao and Chi-Wing Fu and Chun Yu and Haozhao Qin and Qiong Wang and Pheng-Ann Heng},
  doi          = {10.1109/TVCG.2022.3230746},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3298-3313},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Kine-appendage: Enhancing freehand VR interaction through transformations of virtual appendages},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Line-based 3D building abstraction and polygonal surface
reconstruction from images. <em>TVCG</em>, <em>30</em>(7), 3283–3297.
(<a href="https://doi.org/10.1109/TVCG.2022.3230369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Textureless objects, repetitive patterns and limited computational resources pose significant challenges to man-made structure reconstruction from images, because feature-points-based reconstruction methods usually fail due to the lack of distinct texture or ambiguous point matches. Meanwhile multi-view stereo approaches also suffer from high computational complexity. In this article, we present a new framework to reconstruct 3D surfaces for buildings from multi-view images by leveraging another fundamental geometric primitive: line segments. To this end, we first propose a new multi-resolution line segment detector to extract 2D line segments from each image. Then, we construct a 3D line cloud by introducing an improved Line3D++ algorithm to match 2D line segments from different images. Finally, we reconstruct a complete and manifold surface mesh from 3D line segments by formulating a Bayesian probabilistic modeling problem , which accurately generates a set of underlying planes. This output model is simple and has low performance requirements for hardware devices. Experimental results demonstrate the validity of the proposed approach and its ability to generate abstract and compact surface meshes from the 3D line cloud with low computational costs.},
  archive      = {J_TVCG},
  author       = {Jianwei Guo and Yanchao Liu and Xin Song and Haoyu Liu and Xiaopeng Zhang and Zhanglin Cheng},
  doi          = {10.1109/TVCG.2022.3230369},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3283-3297},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Line-based 3D building abstraction and polygonal surface reconstruction from images},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A visual comparison of silent error propagation.
<em>TVCG</em>, <em>30</em>(7), 3268–3282. (<a
href="https://doi.org/10.1109/TVCG.2022.3230636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-performance computing (HPC) systems play a critical role in facilitating scientific discoveries. Their scale and complexity (e.g., the number of computational units and software stack) continue to grow as new systems are expected to process increasingly more data and reduce computing time. However, with more processing elements, the probability that these systems will experience a random bit-flip error that corrupts a program&#39;s output also increases, which is often recognized as silent data corruption. Analyzing the resiliency of HPC applications in extreme-scale computing to silent data corruption is crucial but difficult. An HPC application often contains a large number of computation units that need to be tested, and error propagation caused by error corruption is complex and difficult to interpret. To accommodate this challenge, we propose an interactive visualization system that helps HPC researchers understand the resiliency of HPC applications and compare their error propagation. Our system models an application&#39;s error propagation to study a program&#39;s resiliency by constructing and visualizing its fault tolerance boundary. Coordinating with multiple interactive designs, our system enables domain experts to efficiently explore the complicated spatial and temporal correlation between error propagations. At the end, the system integrated a nonmonotonic error propagation analysis with an adjustable graph propagation visualization to help domain experts examine the details of error propagation and answer such questions as why an error is mitigated or amplified by program execution.},
  archive      = {J_TVCG},
  author       = {Zhimin Li and Harshitha Menon and Kathryn Mohror and Shusen Liu and Luanzheng Guo and Peer-Timo Bremer and Valerio Pascucci},
  doi          = {10.1109/TVCG.2022.3230636},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3268-3282},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A visual comparison of silent error propagation},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Does interactive conditioning help users better understand
the structure of probabilistic models? <em>TVCG</em>, <em>30</em>(7),
3256–3267. (<a href="https://doi.org/10.1109/TVCG.2022.3231967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite growing interest in probabilistic modeling approaches and availability of learning tools, people are hesitant to use them. There is a need for tools to communicate probabilistic models more intuitively and help users build, validate, use effectively or trust probabilistic models. We focus on visual representations of probabilistic models and introduce the Interactive Pair Plot (IPP) for visualization of a model&#39;s uncertainty, a scatter plot matrix of a probabilistic model allowing interactive conditioning on the model&#39;s variables. We investigate whether the use of interactive conditioning in a scatter plot matrix of a model helps users better understand variables’ relations. We conducted a user study and the findings suggest that improvements in the understanding of the interaction group are the most pronounced for more exotic structures, such as hierarchical models or unfamiliar parameterizations, in comparison to the understanding of the static group. As the detail of the inferred information increases, interactive conditioning does not lead to considerably longer response times. Finally, interactive conditioning improves participants’ confidence about their responses.},
  archive      = {J_TVCG},
  author       = {Evdoxia Taka and Sebastian Stein and John H. Williamson},
  doi          = {10.1109/TVCG.2022.3231967},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3256-3267},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Does interactive conditioning help users better understand the structure of probabilistic models?},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automorphism faithfulness metrics for symmetric graph
drawings. <em>TVCG</em>, <em>30</em>(7), 3241–3255. (<a
href="https://doi.org/10.1109/TVCG.2022.3229354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present new quality metrics for symmetric graph drawing based on group theory. Roughly speaking, the new metrics are faithfulness metrics, i.e., they measure how faithfully a drawing of a graph displays the ground truth (i.e., geometric automorphisms) of the graph as symmetries. More specifically, we introduce two types of automorphism faithfulness metrics for displaying: (1) a single geometric automorphism as a symmetry ( axial or rotational ), and (2) a group of geometric automorphisms ( cyclic or dihedral ). We present algorithms to compute the automorphism faithfulness metrics in $O(n \log n)$ time. Moreover, we also present efficient algorithms to detect exact symmetries in a graph drawing. We then validate our automorphism faithfulness metrics using deformation experiments. Finally, we use the metrics to evaluate existing graph drawing algorithms to compare how faithfully they display geometric automorphisms of a graph as symmetries.},
  archive      = {J_TVCG},
  author       = {Amyra Meidiana and Seok-Hee Hong and Peter Eades and Daniel Keim},
  doi          = {10.1109/TVCG.2022.3229354},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3241-3255},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Automorphism faithfulness metrics for symmetric graph drawings},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VISAtlas: An image-based exploration and query system for
large visualization collections via neural image embedding.
<em>TVCG</em>, <em>30</em>(7), 3224–3240. (<a
href="https://doi.org/10.1109/TVCG.2022.3229023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-quality visualization collections are beneficial for a variety of applications including visualization reference and data-driven visualization design. The visualization community has created many visualization collections, and developed interactive exploration systems for the collections. However, the systems are mainly based on extrinsic attributes like authors and publication years, whilst neglect intrinsic property (i.e., visual appearance) of visualizations, hindering visual comparison and query of visualization designs. This paper presents VISAtlas , an image-based approach empowered by neural image embedding, to facilitate exploration and query for visualization collections. To improve embedding accuracy, we create a comprehensive collection of synthetic and real-world visualizations, and use it to train a convolutional neural network (CNN) model with a triplet loss for taxonomical classification of visualizations. Next, we design a coordinated multiple view (CMV) system that enables multi-perspective exploration and design retrieval based on visualization embeddings. Specifically, we design a novel embedding overview that leverages contextual layout framework to preserve the context of the embedding vectors with the associated visualization taxonomies, and density plot and sampling techniques to address the overdrawing problem. We demonstrate in three case studies and one user study the effectiveness of VISAtlas in supporting comparative analysis of visualization collections, exploration of composite visualizations, and image-based retrieval of visualization designs. The studies reveal that real-world visualization collections (e.g., Beagle and VIS30K) better accord with the richness and diversity of visualization designs than synthetic collections (e.g., Data2Vis), inspiring composite visualizations are identified in real-world collections, and distinct design patterns exist in visualizations from different sources.},
  archive      = {J_TVCG},
  author       = {Yilin Ye and Rong Huang and Wei Zeng},
  doi          = {10.1109/TVCG.2022.3229023},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3224-3240},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VISAtlas: An image-based exploration and query system for large visualization collections via neural image embedding},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revisiting walking-in-place by introducing step-height
control, elastic input, and pseudo-haptic feedback. <em>TVCG</em>,
<em>30</em>(7), 3210–3223. (<a
href="https://doi.org/10.1109/TVCG.2022.3228171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Walking-in-place (WIP) is a locomotion technique that enables users to “walk infinitely” through vast virtual environments using walking-like gestures within a limited physical space. This article investigates alternative interaction schemes for WIP, addressing successively the control, input, and output of WIP. First, we introduce a novel height-based control to increase advanced speed. Second, we introduce a novel input system for WIP based on elastic and passive strips. Third, we introduce the use of pseudo-haptic feedback as a novel output for WIP meant to alter walking sensations. The results of a series of user studies show that height and frequency based control of WIP can facilitate higher virtual speed with greater efficacy and ease than in frequency-based WIP. Second, using an upward elastic input system can result in a stable virtual speed control, although excessively strong elastic forces may impact the usability and user experience. Finally, using a pseudo-haptic approach can improve the perceived realism of virtual slopes. Taken together, our results suggest that, for future VR applications, there is value in further research into the use of alternative interaction schemes for walking-in-place.},
  archive      = {J_TVCG},
  author       = {Yutaro Hirao and Takuji Narumi and Ferran Argelaguet and Anatole Lécuyer},
  doi          = {10.1109/TVCG.2022.3228171},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3210-3223},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Revisiting walking-in-place by introducing step-height control, elastic input, and pseudo-haptic feedback},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptively isotropic remeshing based on curvature smoothed
field. <em>TVCG</em>, <em>30</em>(7), 3196–3209. (<a
href="https://doi.org/10.1109/TVCG.2022.3227970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of 3D digital geometry technology, 3D triangular meshes are becoming more useful and valuable in industrial manufacturing and digital entertainment. A high quality triangular mesh can be used to represent a real world object with geometric and physical characteristics. While anisotropic meshes have advantages of representing shapes with sharp features (such as trimmed surfaces) more efficiently and accurately, isotropic meshes allow more numerically stable computations. When there is no anisotropic mesh requirement, isotropic triangles are always a good choice. In this paper, we propose a remeshing method to convert an input mesh into an adaptively isotropic one based on a curvature smoothed field (CSF). With the help of the CSF, adaptively isotropic remeshing can retain the curvature sensitivity, which enables more geometric features to be kept, and avoid the occurrence of obtuse triangles in the remeshed model as much as possible. The remeshed triangles with locally isotropic property benefit various geometric processes such as neighbor-based feature extraction and analysis. The experimental results show that our method achieves better balance between geometric feature preservation and mesh quality improvement compared to peers. We provide the implementation codes of our resampling method at github.com/vvvwo/Adaptively-Isotropic-Remeshing .},
  archive      = {J_TVCG},
  author       = {Chenlei Lv and Weisi Lin and Jianmin Zheng},
  doi          = {10.1109/TVCG.2022.3227970},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3196-3209},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Adaptively isotropic remeshing based on curvature smoothed field},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PoseCoach: A customizable analysis and visualization system
for video-based running coaching. <em>TVCG</em>, <em>30</em>(7),
3180–3195. (<a href="https://doi.org/10.1109/TVCG.2022.3230855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Videos are an accessible form of media for analyzing sports postures and providing feedback to athletes. Existing sport-specific systems embed bespoke human pose attributes and thus can be hard to scale for new attributes, especially for users without programming experiences. Some systems retain scalability by directly showing the differences between two poses, but they might not clearly visualize the key differences that viewers would like to pursue. Besides, video-based coaching systems often present feedback on the correctness of poses by augmenting videos with visual markers or reference poses. However, previewing and augmenting videos limit the analysis and visualization of human poses due to the fixed viewpoints in videos, which confine the observation of captured human movements and cause ambiguity in the augmented feedback. To address these issues, we study customizable human pose data analysis and visualization in the context of running pose attributes, such as joint angles and step distances. Based on existing literature and a formative study, we have designed and implemented a system, PoseCoach , to provide feedback on running poses for amateurs by comparing the running poses between a novice and an expert. PoseCoach adopts a customizable data analysis model to allow users’ controllability in defining pose attributes of their interests through our interface. To avoid the influence of viewpoint differences and provide intuitive feedback, PoseCoach visualizes the pose differences as part-based 3D animations on a human model to imitate the demonstration of a human coach. We conduct a user study to verify our design components and conduct expert interviews to evaluate the usefulness of the system.},
  archive      = {J_TVCG},
  author       = {Jingyuan Liu and Nazmus Saquib and Chen Zhutian and Rubaiat Habib Kazi and Li-Yi Wei and Hongbo Fu and Chiew-Lan Tai},
  doi          = {10.1109/TVCG.2022.3230855},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3180-3195},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PoseCoach: A customizable analysis and visualization system for video-based running coaching},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MOUNT: Learning 6DoF motion prediction based on uncertainty
estimation for delayed AR rendering. <em>TVCG</em>, <em>30</em>(7),
3166–3179. (<a href="https://doi.org/10.1109/TVCG.2022.3228807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The delay of rendering on AR devices requires prediction of head motion using sensor data acquired tens of even one hundred milliseconds ago to avoid misalignment between the virtual content and the physical world, where the misalignment will lead to a sense of time latency and dizziness for users. To solve the problem, we propose a method for the 6DoF motion prediction to compensate for the time latency. Compared with traditional hand-crafted methods, our method is based on deep learning, which has better motion prediction ability to deal with complex human motion. In particular, we propose a MOtion UNcerTainty encode decode network (MOUNT) that estimates the uncertainty of input data and predicts the uncertainty of output motion to improve the prediction accuracy and smoothness. Experiments on the EuRoC and our collected dataset demonstrate that our method significantly outperforms the traditional method and greatly improves AR visual effects.},
  archive      = {J_TVCG},
  author       = {Haoran Chen and Lantian Wei and Haomin Liu and Boxin Shi and Guofeng Zhang and Hongbin Zha},
  doi          = {10.1109/TVCG.2022.3228807},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3166-3179},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MOUNT: Learning 6DoF motion prediction based on uncertainty estimation for delayed AR rendering},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SAC-GAN: Structure-aware image composition. <em>TVCG</em>,
<em>30</em>(7), 3151–3165. (<a
href="https://doi.org/10.1109/TVCG.2022.3226689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an end-to-end learning framework for image-to-image composition , aiming to plausibly compose an object represented as a cropped patch from an object image into a background scene image. As our approach emphasizes more on semantic and structural coherence of the composed images, rather than their pixel-level RGB accuracies, we tailor the input and output of our network with structure-aware features and design our network losses accordingly, with ground truth established in a self-supervised setting through the object cropping. Specifically, our network takes the semantic layout features from the input scene image, features encoded from the edges and silhouette in the input object patch, as well as a latent code as inputs, and generates a 2D spatial affine transform defining the translation and scaling of the object patch. The learned parameters are further fed into a differentiable spatial transformer network to transform the object patch into the target image, where our model is trained adversarially using an affine transform discriminator and a layout discriminator. We evaluate our network, coined SAC-GAN, for various image composition scenarios in terms of quality, composability, and generalizability of the composite images. Comparisons are made to state-of-the-art alternatives, including Instance Insertion, ST-GAN, CompGAN and PlaceNet, confirming superiority of our method.},
  archive      = {J_TVCG},
  author       = {Hang Zhou and Rui Ma and Ling-Xiao Zhang and Lin Gao and Ali Mahdavi-Amiri and Hao Zhang},
  doi          = {10.1109/TVCG.2022.3226689},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3151-3165},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SAC-GAN: Structure-aware image composition},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multilevel visual analysis of aggregate geo-networks.
<em>TVCG</em>, <em>30</em>(7), 3135–3150. (<a
href="https://doi.org/10.1109/TVCG.2022.3229953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous patterns found in urban phenomena, such as air pollution and human mobility, can be characterized as many directed geospatial networks (geo-networks) that represent spreading processes in urban space. These geo-networks can be analyzed from multiple levels, ranging from the macro -level of summarizing all geo-networks, meso -level of comparing or summarizing parts of geo-networks, and micro -level of inspecting individual geo-networks. Most of the existing visualizations cannot support multilevel analysis well. These techniques work by: 1) showing geo-networks separately with multiple maps leads to heavy context switching costs between different maps; 2) summarizing all geo-networks into a single network can lead to the loss of individual information; 3) drawing all geo-networks onto one map might suffer from the visual scalability issue in distinguishing individual geo-networks. In this study, we propose GeoNetverse , a novel visualization technique for analyzing aggregate geo-networks from multiple levels. Inspired by metro maps, GeoNetverse balances the overview and details of the geo-networks by placing the edges shared between geo-networks in a stacked manner. To enhance the visual scalability, GeoNetverse incorporates a level-of-detail rendering, a progressive crossing minimization, and a coloring technique. A set of evaluations was conducted to evaluate GeoNetverse from multiple perspectives.},
  archive      = {J_TVCG},
  author       = {Zikun Deng and Shifu Chen and Xiao Xie and Guodao Sun and Mingliang Xu and Di Weng and Yingcai Wu},
  doi          = {10.1109/TVCG.2022.3229953},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3135-3150},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multilevel visual analysis of aggregate geo-networks},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual cue based corrective feedback for motor skill
training in mixed reality: A survey. <em>TVCG</em>, <em>30</em>(7),
3121–3134. (<a href="https://doi.org/10.1109/TVCG.2022.3227999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When learning a motor skill it is helpful to get corrective feedback from an instructor. This will support the learner to execute the movement correctly. With modern technology, it is possible to provide this feedback via mixed reality. In most cases, this involves visual cues to help the user understand the corrective feedback. We analyzed recent research approaches utilizing visual cues for feedback in mixed reality. The scope of this article is visual feedback for motor skill learning, which involves physical therapy, exercise, rehabilitation etc. While some of the surveyed literature discusses therapeutic effects of the training, this article focuses on visualization techniques. We categorized the literature from a visualization standpoint, including visual cues, technology and characteristics of the feedback. This provided insights into how visual feedback in mixed reality is applied in the literature and how different aspects of the feedback are related. The insights obtained can help to better adjust future feedback systems to the target group and their needs. This article also provides a deeper understanding of the characteristics of the visual cues in general and promotes future, more detailed research on this topic.},
  archive      = {J_TVCG},
  author       = {Florian Diller and Gerik Scheuermann and Alexander Wiebel},
  doi          = {10.1109/TVCG.2022.3227999},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3121-3134},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual cue based corrective feedback for motor skill training in mixed reality: A survey},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Appearance-preserved portrait-to-anime translation via
proxy-guided domain adaptation. <em>TVCG</em>, <em>30</em>(7),
3104–3120. (<a href="https://doi.org/10.1109/TVCG.2022.3228707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Converting a human portrait to anime style is a desirable but challenging problem. Existing methods fail to resolve this problem due to the large inherent gap between two domains that cannot be overcome by a simple direct mapping. For this reason, these methods struggle to preserve the appearance features in the original photo. In this article, we discover an intermediate domain, the coser portrait (portraits of humans costuming as anime characters), that helps bridge this gap. It alleviates the learning ambiguity and loosens the mapping difficulty in a progressive manner. Specifically, we start from learning the mapping between coser and anime portraits, and present a proxy-guided domain adaptation learning scheme with three progressive adaptation stages to shift the initial model to the human portrait domain. In this way, our model can generate visually pleasant anime portraits with well-preserved appearances given the human portrait. Our model adopts a disentangled design by breaking down the translation problem into two specific subtasks of face deformation and portrait stylization. This further elevates the generation quality. Extensive experimental results show that our model can achieve visually compelling translation with better appearance preservation and perform favorably against the existing methods both qualitatively and quantitatively. Our code and datasets are available at https://github.com/NeverGiveU/PDA-Translation .},
  archive      = {J_TVCG},
  author       = {Wenpeng Xiao and Cheng Xu and Jiajie Mai and Xuemiao Xu and Yue Li and Chengze Li and Xueting Liu and Shengfeng He},
  doi          = {10.1109/TVCG.2022.3228707},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3104-3120},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Appearance-preserved portrait-to-anime translation via proxy-guided domain adaptation},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image-driven harmonious color palette generation for diverse
information visualization. <em>TVCG</em>, <em>30</em>(7), 3089–3103. (<a
href="https://doi.org/10.1109/TVCG.2022.3226218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color has been widely used to encode data in all types of visualizations. Effective color palettes contain discriminable and harmonious colors, which allow information from visualizations to be accurately and aesthetically conveyed. However, predefined color palettes not only lack the flexibility of custom color palette generation but also ignore the context in which the visualizations are used. Designing an effective color palette is a time-consuming and challenging process for users, even experts. In this work, we propose the generation of an image-based visualization color palette to exploit the human perception of visually appealing images while considering visualization cognition. By analyzing color palette constraints, including harmony, discrimination, and context, we propose an image-driven color generation method. We design a color clustering method in the saliency-hue plane based on visual importance detection and then select the palette based on the visualization color constraints. In addition, we design two color optimization and assignment strategies for visualizations of different data types. Evaluations through numeric indicators and user experiments demonstrate that the palettes predicted by our method are visually related to the original images and are aesthetically pleasing, supporting diverse visualization contexts and data types in practical applications.},
  archive      = {J_TVCG},
  author       = {Shuqi Liu and Mingtian Tao and Yifei Huang and Changbo Wang and Chenhui Li},
  doi          = {10.1109/TVCG.2022.3226218},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3089-3103},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Image-driven harmonious color palette generation for diverse information visualization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GraphDecoder: Recovering diverse network graphs from
visualization images via attention-aware learning. <em>TVCG</em>,
<em>30</em>(7), 3074–3088. (<a
href="https://doi.org/10.1109/TVCG.2022.3225554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DNGs are diverse network graphs with texts and different styles of nodes and edges, including mind maps, modeling graphs, and flowcharts. They are high-level visualizations that are easy for humans to understand but difficult for machines. Inspired by the process of human perception of graphs, we propose a method called GraphDecoder to extract data from raster images. Given a raster image, we extract the content based on a neural network. We built a semantic segmentation network based on U-Net. We increase the attention mechanism module, simplify the network model, and design a specific loss function to improve the model&#39;s ability to extract graph data. After this semantic segmentation network, we can extract the data of all nodes and edges. We then combine these data to obtain the topological relationship of the entire DNG. We also provide an interactive interface for users to redesign the DNGs. We verify the effectiveness of our method by evaluations and user studies on datasets collected on the internet and generated datasets.},
  archive      = {J_TVCG},
  author       = {Sicheng Song and Chenhui Li and Dong Li and Juntong Chen and Changbo Wang},
  doi          = {10.1109/TVCG.2022.3225554},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3074-3088},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GraphDecoder: Recovering diverse network graphs from visualization images via attention-aware learning},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bidirectional hybrid LSTM based recurrent neural network for
multi-view stereo. <em>TVCG</em>, <em>30</em>(7), 3062–3073. (<a
href="https://doi.org/10.1109/TVCG.2022.3165860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning based multi-view stereo (MVS) networks have demonstrated their excellent performance on various benchmarks. In this paper, we present an effective and efficient recurrent neural network (RNN) for accurate and complete dense point cloud reconstruction. Instead of regularizing the cost volume via conventional 3D CNN or unidirectional RNN like previous attempts, we adopt a bidirectional hybrid Long Short-Term Memory (LSTM) based structure for cost volume regularization. The proposed bidirectional recurrent regularization is able to perceive full-space context information comparable to 3D CNNs while saving runtime memory. For post-processing, we introduce a visibility based approach for depth map refinement to obtain more accurate dense point clouds. Extensive experiments on DTU, Tanks and Temples and ETH3D datasets demonstrate that our method outperforms previous state-of-the-art MVS methods and exhibits high memory efficiency at runtime.},
  archive      = {J_TVCG},
  author       = {Zizhuang Wei and Qingtian Zhu and Chen Min and Yisong Chen and Guoping Wang},
  doi          = {10.1109/TVCG.2022.3165860},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {7},
  number       = {7},
  pages        = {3062-3073},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Bidirectional hybrid LSTM based recurrent neural network for multi-view stereo},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoInsight: Visual storytelling for hierarchical tables with
connected insights. <em>TVCG</em>, <em>30</em>(6), 3049–3061. (<a
href="https://doi.org/10.1109/TVCG.2024.3388553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting data insights and generating visual data stories from tabular data are critical parts of data analysis. However, most existing studies primarily focus on tabular data stored as flat tables, typically without leveraging the relations between cells in the headers of hierarchical tables. When properly used, rich table headers can enable the extraction of many additional data stories. To assist analysts in visual data storytelling, an approach is needed to organize these data insights efficiently. In this work, we propose CoInsight, a system to facilitate visual storytelling for hierarchical tables by connecting insights. CoInsight extracts data insights from hierarchical tables and builds insight relations according to the structure of table headers. It further visualizes related data insights using a nested graph with edge bundling. We evaluate the CoInsight system through a usage scenario and a user experiment. The results demonstrate the utility and usability of CoInsight for converting data insights in hierarchical tables into visual data stories.},
  archive      = {J_TVCG},
  author       = {Guozheng Li and Runfei Li and Yunshan Feng and Yu Zhang and Yuyu Luo and Chi Harold Liu},
  doi          = {10.1109/TVCG.2024.3388553},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {6},
  number       = {6},
  pages        = {3049-3061},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CoInsight: Visual storytelling for hierarchical tables with connected insights},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Struggles and strategies in understanding information
visualizations. <em>TVCG</em>, <em>30</em>(6), 3035–3048. (<a
href="https://doi.org/10.1109/TVCG.2024.3388560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the visualization community is increasingly aware that people often find visualizations difficult to understand, there is less information about what we need to do to create comprehensible visualizations. To help visualization creators and designers improve their visualizations, we need to better understand what kind of support people are looking for in their sensemaking process. Empirical studies are needed to tease apart the details of what makes the process of understanding difficult for visualization viewers. We conducted a qualitative study with 14 participants, observing them as they described how they were trying to make sense of 20 information visualizations. We identified the challenges participants faced throughout their sensemaking process and the strategies they employed to help themselves in overcoming the challenges. Our findings show how details and nuances within visualizations can impact comprehensibility and offer research suggestions to help us move toward more understandable visualizations.},
  archive      = {J_TVCG},
  author       = {Maryam Rezaie and Melanie Tory and Sheelagh Carpendale},
  doi          = {10.1109/TVCG.2024.3388560},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {6},
  number       = {6},
  pages        = {3035-3048},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Struggles and strategies in understanding information visualizations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PM-vis: A visual analytics system for tracing and analyzing
the evolution of pottery motifs. <em>TVCG</em>, <em>30</em>(6),
3022–3034. (<a href="https://doi.org/10.1109/TVCG.2024.3388525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Chinese archaeological research, analyzing the evolution of motifs in ancient pottery is crucial for studying the spread and growth of cultures across various eras and regions. However, such analyses are often challenging due to the complexities of identifying motifs with evolutionary connections that may manifest concurrent changes in appearance, space, and time, compounded by ineffective documentation. We propose PM-Vis, a visual analytics system for tracing and analyzing the evolution of pottery motifs. PM-Vis is anchored in a “selection-organization-documentation” workflow. In the selection stage, we design a three-fold projection paired with a motif-based search mechanism, displaying the appearance similarity and temporal and spatial proximities of all motifs or a specific motif, aiding users in selecting motifs with evolutionary connections. The organization stage helps users establish the evolutionary sequence and segment the selected motifs into distinct evolutionary phases. Finally, the documentation stage enables users to record their observations and insights through various forms of annotation. We demonstrate the usefulness and effectiveness of PM-Vis through two case studies, expert feedback, and a user study.},
  archive      = {J_TVCG},
  author       = {Jincheng Li and Chufan Lai and Hai Zhang and Xiaoru Yuan},
  doi          = {10.1109/TVCG.2024.3388525},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {6},
  number       = {6},
  pages        = {3022-3034},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PM-vis: A visual analytics system for tracing and analyzing the evolution of pottery motifs},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). JsonCurer: Data quality management for JSON based on an
aggregated schema. <em>TVCG</em>, <em>30</em>(6), 3008–3021. (<a
href="https://doi.org/10.1109/TVCG.2024.3388556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-quality data is critical to deriving useful and reliable information. However, real-world data often contains quality issues undermining the value of the derived information. Most existing research on data quality management focuses on tabular data, leaving semi-structured data under-exploited. Due to the schema-less and hierarchical features of semi-structured data, discovering and fixing quality issues is challenging and time-consuming. To address the challenge, this paper presents JsonCurer, an interactive visualization system to assist with data quality management in the context of JSON data. To have an overview of quality issues, we first construct a taxonomy based on interviews with data practitioners and a review of 119 real-world JSON files. Then we highlight a schema visualization that presents structural information, statistical features, and quality issues of JSON data. Based on a similarity-based aggregation technique, the visualization depicts the entire JSON data with a concise tree, where summary visualizations are given above each node, and quality issues are illustrated using Bubble Sets across nodes. We evaluate the effectiveness and usability of JsonCurer with two case studies. One is in the domain of data analysis while the other concerns quality assurance in MongoDB documents.},
  archive      = {J_TVCG},
  author       = {Kai Xiong and Xinyi Xu and Siwei Fu and Di Weng and Yongheng Wang and Yingcai Wu},
  doi          = {10.1109/TVCG.2024.3388556},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {6},
  number       = {6},
  pages        = {3008-3021},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {JsonCurer: Data quality management for JSON based on an aggregated schema},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Same data, diverging perspectives: The power of
visualizations to elicit competing interpretations. <em>TVCG</em>,
<em>30</em>(6), 2995–3007. (<a
href="https://doi.org/10.1109/TVCG.2024.3388515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People routinely rely on data to make decisions, but the process can be riddled with biases. We show that patterns in data might be noticed first or more strongly, depending on how the data is visually represented or what the viewer finds salient. We also demonstrate that viewer interpretation of data is similar to that of ‘ambiguous figures’ such that two people looking at the same data can come to different decisions. In our studies, participants read visualizations depicting competitions between two entities, where one has a historical lead (A) but the other has been gaining momentum (B) and predicted a winner, across two chart types and three annotation approaches. They either saw the historical lead as salient and predicted that A would win, or saw the increasing momentum as salient and predicted B to win. These results suggest that decisions can be influenced by both how data are presented and what patterns people find visually salient.},
  archive      = {J_TVCG},
  author       = {Cindy Xiong Bearfield and Lisanne van Weelden and Adam Waytz and Steven Franconeri},
  doi          = {10.1109/TVCG.2024.3388515},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {6},
  number       = {6},
  pages        = {2995-3007},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Same data, diverging perspectives: The power of visualizations to elicit competing interpretations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ScrollTimes: Tracing the provenance of paintings as a window
into history. <em>TVCG</em>, <em>30</em>(6), 2981–2994. (<a
href="https://doi.org/10.1109/TVCG.2024.3388523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of cultural artifact provenance, tracing ownership and preservation, holds significant importance in archaeology and art history. Modern technology has advanced this field, yet challenges persist, including recognizing evidence from diverse sources, integrating sociocultural context, and enhancing interactive automation for comprehensive provenance analysis. In collaboration with art historians, we examined the handscroll, a traditional Chinese painting form that provides a rich source of historical data and a unique opportunity to explore history through cultural artifacts. We present a three-tiered methodology encompassing artifact, contextual, and provenance levels, designed to create a “Biography” for handscroll. Our approach incorporates the application of image processing techniques and language models to extract, validate, and augment elements within handscroll using various cultural heritage databases. To facilitate efficient analysis of non-contiguous extracted elements, we have developed a distinctive layout. Additionally, we introduce ScrollTimes, a visual analysis system tailored to support the three-tiered analysis of handscroll, allowing art historians to interactively create biographies tailored to their interests. Validated through case studies and expert interviews, our approach offers a window into history, fostering a holistic understanding of handscroll provenance and historical significance.},
  archive      = {J_TVCG},
  author       = {Wei Zhang and Wong Kam-Kwai and Yitian Chen and Ailing Jia and Luwei Wang and Jian-Wei Zhang and Lechao Cheng and Huamin Qu and Wei Chen},
  doi          = {10.1109/TVCG.2024.3388523},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {6},
  number       = {6},
  pages        = {2981-2994},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ScrollTimes: Tracing the provenance of paintings as a window into history},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sticky links: Encoding quantitative data of graph edges.
<em>TVCG</em>, <em>30</em>(6), 2968–2980. (<a
href="https://doi.org/10.1109/TVCG.2024.3388562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TVCG},
  author       = {Min Lu and Xiangfang Zeng and Joel Lanir and Xiaoqin Sun and Guozheng Li and Daniel Cohen-Or and Hui Huang},
  doi          = {10.1109/TVCG.2024.3388562},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {6},
  number       = {6},
  pages        = {2968-2980},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Sticky links: Encoding quantitative data of graph edges},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TacPrint: Visualizing the biomechanical fingerprint in table
tennis. <em>TVCG</em>, <em>30</em>(6), 2955–2967. (<a
href="https://doi.org/10.1109/TVCG.2024.3388555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Table tennis is a sport that demands high levels of technical proficiency and body coordination from players. Biomechanical fingerprints can provide valuable insights into players’ habitual movement patterns and characteristics, allowing them to identify and improve technical weaknesses. Despite the potential, few studies have developed effective methods for generating such fingerprints. To address this gap, we propose TacPrint, a framework for generating a biomechanical fingerprint for each player. TacPrint leverages machine learning techniques to extract comprehensive features from biomechanics data collected by inertial measurement units (IMU) and employs the attention mechanism to enhance model interpretability. After generating fingerprints, TacPrint provides a visualization system to facilitate the exploration and investigation of these fingerprints. In order to validate the effectiveness of the framework, we designed an experiment to evaluate the model’s performance and conducted a case study with the system. The results of our experiment demonstrated the high accuracy and effectiveness of the model. Additionally, we discussed the potential of TacPrint to be extended to other sports.},
  archive      = {J_TVCG},
  author       = {Jiachen Wang and Ji Ma and Zheng Zhou and Xiao Xie and Hui Zhang and Yingcai Wu and Huamin Qu},
  doi          = {10.1109/TVCG.2024.3388555},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {6},
  number       = {6},
  pages        = {2955-2967},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TacPrint: Visualizing the biomechanical fingerprint in table tennis},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Animating hypothetical trips to communicate space-based
temporal uncertainty on digital maps. <em>TVCG</em>, <em>30</em>(6),
2942–2954. (<a href="https://doi.org/10.1109/TVCG.2024.3388517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores a novel approach to communicating plausible space-based temporal variability of travel durations. Digital maps most often only convey single numerical values as the estimated duration for a path and this piece of information does not account for the multiple scenarios hidden behind this point estimate, nor for the temporal uncertainty along the route (e.g., the likelihood of being slowed down at an intersection). We explore conveying this uncertainty by animating hypothetical trips onto maps in the form of moving dots along one or more paths. We conducted a study with 16 participants and observed that they were able to correctly extract and infer simple information from our uncertainty visualizations but that identifying moving dots’ changes in speed is a more complex task. We discuss design challenges and implications for future visualizations of space-based temporal uncertainty.},
  archive      = {J_TVCG},
  author       = {Morgane Koval and Yvonne Jansen and Fanny Chevalier},
  doi          = {10.1109/TVCG.2024.3388517},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {6},
  number       = {6},
  pages        = {2942-2954},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Animating hypothetical trips to communicate space-based temporal uncertainty on digital maps},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparative evaluation of animated scatter plot transitions.
<em>TVCG</em>, <em>30</em>(6), 2929–2941. (<a
href="https://doi.org/10.1109/TVCG.2024.3388558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scatter plots are popular for displaying 2D data, but in practice, many data sets have more than two dimensions. For the analysis of such multivariate data, it is often necessary to switch between scatter plots of different dimension pairs, e.g., in a scatter plot matrix (SPLOM). Alternative approaches include a “grand tour” for an overview of the entire data set or creating artificial axes from dimensionality reduction (DR). A cross-cutting concern in all techniques is the ability of viewers to find correspondence between data points in different views. Previous work proposed animations to preserve the mental map between view changes and to trace points as well as clusters between scatter plots of the same underlying data set. In this article, we evaluate a variety of spline- and rotation-based view transitions in a crowdsourced user study focusing on ecological validity. Using the study results, we assess each animation’s suitability for tracing points and clusters across view changes. We evaluate whether the order of horizontal and vertical rotation is relevant for task accuracy. The results show that rotations with an orthographic camera or staged expansion of a depth axis significantly outperform all other animation techniques for the traceability of individual points. Further, we provide a ranking of the animated transition techniques for traceability of individual points. However, we could not find any significant differences for the traceability of clusters. Furthermore, we identified differences by animation direction that could guide further studies to determine potential confounds for these differences. We publish the study data for reuse and provide the animation framework as a D3.js plug-in.},
  archive      = {J_TVCG},
  author       = {Nils Rodrigues and Frederik L. Dennig and Vincent Brandt and Daniel A. Keim and Daniel Weiskopf},
  doi          = {10.1109/TVCG.2024.3388558},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {6},
  number       = {6},
  pages        = {2929-2941},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Comparative evaluation of animated scatter plot transitions},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ColorNetVis: An interactive color network analysis system
for exploring the color composition of traditional chinese painting.
<em>TVCG</em>, <em>30</em>(6), 2916–2928. (<a
href="https://doi.org/10.1109/TVCG.2024.3388520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of digital humanities, color research aims to discover explanations for painting history and color usage habits. However, researchers analyzing color relationships is challenging and time-consuming, as it requires color extraction and a detailed review of many painting images for reference and comparison of color relationships. In our work, we propose ColorNetVis, an interactive color network analysis tool that enables researchers to explore color relationships through color networks. The core of ColorNetVis is a bipartite network model that establishes a bipartite relationship between colors and Chinese painting within a scope based on color difference measurement. It constructs a one-mode color network through projection algorithms and similarity calculation methods to discover the relationship between colors. We propose a coordinated set of views to demonstrate the combination of determined color networks with painting types and real-world attributes. We use color space view, color attribute distribution view, and single color query components to assist researchers in conducting detailed color analysis and validation. Through case studies, researcher reviews, and user studies, we demonstrate that ColorNetVis can effectively help researchers discover knowledge of color relationships and potential color research directions.},
  archive      = {J_TVCG},
  author       = {Xiaojiao Chen and Qinghua Liu and Yonghao Chen and Ruihan Wang and Yang You and Wanxin Deng and Wei Chen and Xiaosong Wang},
  doi          = {10.1109/TVCG.2024.3388520},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {6},
  number       = {6},
  pages        = {2916-2928},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ColorNetVis: An interactive color network analysis system for exploring the color composition of traditional chinese painting},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing single-frame supervision for better temporal
action localization. <em>TVCG</em>, <em>30</em>(6), 2903–2915. (<a
href="https://doi.org/10.1109/TVCG.2024.3388521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TVCG},
  author       = {Changjian Chen and Jiashu Chen and Weikai Yang and Haoze Wang and Johannes Knittel and Xibin Zhao and Steffen Koch and Thomas Ertl and Shixia Liu},
  doi          = {10.1109/TVCG.2024.3388521},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {6},
  number       = {6},
  pages        = {2903-2915},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Enhancing single-frame supervision for better temporal action localization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of designs for combined 2D+3D visual
representations. <em>TVCG</em>, <em>30</em>(6), 2888–2902. (<a
href="https://doi.org/10.1109/TVCG.2024.3388516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine visual representations of data that make use of combinations of both 2D and 3D data mappings. Combining 2D and 3D representations is a common technique that allows viewers to understand multiple facets of the data with which they are interacting. While 3D representations focus on the spatial character of the data or the dedicated 3D data mapping, 2D representations often show abstract data properties and take advantage of the unique benefits of mapping to a plane. Many systems have used unique combinations of both types of data mappings effectively. Yet there are no systematic reviews of the methods in linking 2D and 3D representations. We systematically survey the relationships between 2D and 3D visual representations in major visualization publications—IEEE VIS, IEEE TVCG, and EuroVis—from 2012 to 2022. We closely examined 105 articles where 2D and 3D representations are connected visually, interactively, or through animation. These approaches are designed based on their visual environment, the relationships between their visual representations, and their possible layouts. Through our analysis, we introduce a design space as well as provide design guidelines for effectively linking 2D and 3D visual representations.},
  archive      = {J_TVCG},
  author       = {Jiayi Hong and Rostyslav Hnatyshyn and Ebrar A. D. Santos and Ross Maciejewski and Tobias Isenberg},
  doi          = {10.1109/TVCG.2024.3388516},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {6},
  number       = {6},
  pages        = {2888-2902},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A survey of designs for combined 2D+3D visual representations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Visual analytics for efficient image exploration and
user-guided image captioning. <em>TVCG</em>, <em>30</em>(6), 2875–2887.
(<a href="https://doi.org/10.1109/TVCG.2024.3388514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in pre-trained language-image models have ushered in a new era of visual comprehension. Leveraging the power of these models, this article tackles two issues within the realm of visual analytics: (1) the efficient exploration of large-scale image datasets and identification of data biases within them; (2) the evaluation of image captions and steering of their generation process. On the one hand, by visually examining the captions generated from language-image models for an image dataset, we gain deeper insights into the visual contents, unearthing data biases that may be entrenched within the dataset. On the other hand, by depicting the association between visual features and textual captions, we expose the weaknesses of pre-trained language-image models in their captioning capability and propose an interactive interface to steer caption generation. The two parts have been coalesced into a coordinated visual analytics system, fostering the mutual enrichment of visual and textual contents. We validate the effectiveness of the system with domain practitioners through concrete case studies with large-scale image datasets.},
  archive      = {J_TVCG},
  author       = {Yiran Li and Junpeng Wang and Prince Aboagye and Chin-Chia Michael Yeh and Yan Zheng and Liang Wang and Wei Zhang and Kwan-Liu Ma},
  doi          = {10.1109/TVCG.2024.3388514},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {6},
  number       = {6},
  pages        = {2875-2887},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual analytics for efficient image exploration and user-guided image captioning},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VIOLET: Visual analytics for explainable quantum neural
networks. <em>TVCG</em>, <em>30</em>(6), 2862–2874. (<a
href="https://doi.org/10.1109/TVCG.2024.3388557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TVCG},
  author       = {Shaolun Ruan and Zhiding Liang and Qiang Guan and Paul Griffin and Xiaolin Wen and Yanna Lin and Yong Wang},
  doi          = {10.1109/TVCG.2024.3388557},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {6},
  number       = {6},
  pages        = {2862-2874},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VIOLET: Visual analytics for explainable quantum neural networks},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Editorial guest editors’ introduction. <em>TVCG</em>,
<em>30</em>(6), 2860–2861. (<a
href="https://doi.org/10.1109/TVCG.2024.3373233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TVCG},
  author       = {Niklas Elmqvist and Shixia Liu and Valerio Pascucci},
  doi          = {10.1109/TVCG.2024.3373233},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {6},
  number       = {6},
  pages        = {2860-2861},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Editorial guest editors’ introduction},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Message from the editor-in-chief and from the associate
editor-in-chief. <em>TVCG</em>, <em>30</em>(5), viii. (<a
href="https://doi.org/10.1109/TVCG.2024.3369809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Welcome to the 13th IEEE Transactions on Visualization and Computer Graphics (TVCG) special issue on IEEE Virtual Reality and 3D User Interfaces. This volume contains a total of 80 full papers selected for and presented at the IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2024), held in Orlando, Florida, USA, from March 16 to 21, 2024.},
  archive      = {J_TVCG},
  author       = {Han-Wei Shen and Kiyoshi Kiyokawa},
  doi          = {10.1109/TVCG.2024.3369809},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {viii},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Message from the editor-in-chief and from the associate editor-in-chief},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IEEE VR 2024 message from the program chairs and guest
editors. <em>TVCG</em>, <em>30</em>(5), ix–x. (<a
href="https://doi.org/10.1109/TVCG.2024.3369810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this special issue of IEEE Transactions on Visualization and Computer Graphics (TVCG), we are pleased to present the top papers from the 31th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2024), held March 16-21, 2024, in Orlando, Florida, US.},
  archive      = {J_TVCG},
  author       = {Yuta Itoh and Voicu Popescu and Tabitha Peck and Stefanie Zollmann},
  doi          = {10.1109/TVCG.2024.3369810},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {ix-x},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {IEEE VR 2024 message from the program chairs and guest editors},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fumos: Neural compression and progressive refinement for
continuous point cloud video streaming. <em>TVCG</em>, <em>30</em>(5),
2849–2859. (<a href="https://doi.org/10.1109/TVCG.2024.3372096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud video (PCV) offers watching experiences in photorealistic 3D scenes with six-degree-of-freedom (6-DoF), enabling a variety of VR and AR applications. The user&#39;s Field of View (FoV) is more fickle with 6-DoF movement than 3-DoF movement in 360-degree video. PCV streaming is extremely bandwidth-intensive. However, current streaming systems require hundreds of Mbps bandwidth, exceeding the bandwidth capabilities of commodity devices. To save bandwidth, FoV-adaptive streaming predicts a user&#39;s FoV and only downloads point cloud data falling in the predicted FoV. But it is difficult to accurately predict the user&#39;s FoV even 2–3 seconds before playback due to 6-DoF. Misprediction of FoV or network bandwidth dips results in frequent stalls. To avoid rebuffering, existing systems would cause incomplete FoV and degraded experience, deteriorating the user&#39;s quality of experience (QoE). In this paper, we describe Fumos, a novel system that preserves interactive experience by avoiding playback stalls while maintaining high perceptual quality and high compression rate. We find a research gap in inter-frame redundant utilization and progressive mechaism. Fumos has three crucial designs, including (1) Neural compression framework with inter-frame coding, namely N-PCC, which achieves both bandwidth efficiency and high fidelity. (2) Progressive refinement streaming framework that enables continuous playback by incrementally upgrading a fetched portion to a higher quality (3) System-level adaptation that employs Lyapunov optimization to jointly optimize the long-term user QoE. Experimental results demonstrate that Fumos significantly outperforms Draco, achieving an average decoding rate acceleration of over 260×. Moreover, the proposed compression framework N-PCC attains remarkable BD-Rate gains, averaging 91.7% and 51.7% against the state-of-the-art point cloud compression methods G-PCC and V-PCC, respectively.},
  archive      = {J_TVCG},
  author       = {Zhicheng Liang and Junhua Liu and Mallesham Dasari and Fangxin Wang},
  doi          = {10.1109/TVCG.2024.3372096},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2849-2859},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Fumos: Neural compression and progressive refinement for continuous point cloud video streaming},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visuo-haptic VR and AR guidance for dental nerve block
education. <em>TVCG</em>, <em>30</em>(5), 2839–2848. (<a
href="https://doi.org/10.1109/TVCG.2024.3372125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inferior alveolar nerve block (IANB) is a dental anesthetic injection that is critical to the performance of many dental procedures. Dental students typically learn to administer an IANB through videos and practice on silicone molds and, in many dental schools, on other students. This causes significant stress for both the students and their early patients. To reduce discomfort and improve clinical outcomes, we created an anatomically informed virtual reality headset-based educational system for the IANB. It combines a layered 3D anatomical model, dynamic visual guidance for syringe position and orientation, and active force feedback to emulate syringe interaction with tissue. A companion mobile augmented reality application allows students to step through a visualization of the procedure on a phone or tablet. We conducted a user study to determine the advantages of preclinical training with our IANB simulator. We found that in comparison to dental students who were exposed only to traditional supplementary study materials, dental students who used our IANB simulator were more confident administering their first clinical injections, had less need for syringe readjustments, and had greater success in numbing patients.},
  archive      = {J_TVCG},
  author       = {Sara Samuel and Carmine Elvezio and Salaar Khan and Laureen Zubiaurre Bitzer and Letty Moss-Salentijn and Steven Feiner},
  doi          = {10.1109/TVCG.2024.3372125},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2839-2848},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visuo-haptic VR and AR guidance for dental nerve block education},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IntenSelect+: Enhancing score-based selection in virtual
reality. <em>TVCG</em>, <em>30</em>(5), 2829–2838. (<a
href="https://doi.org/10.1109/TVCG.2024.3372077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object selection in virtual environments is one of the most common and recurring interaction tasks. Therefore, the used technique can critically influence a system&#39;s overall efficiency and usability. IntenSelect is a scoring-based selection-by-volume technique that was shown to offer improved selection performance over conventional raycasting in virtual reality. This initial method, however, is most pronounced for small spherical objects that converge to a point-like appearance only, is challenging to parameterize, and has inherent limitations in terms of flexibility. We present an enhanced version of IntenSelect called IntenSelect+ designed to overcome multiple shortcomings of the original IntenSelect approach. In an empirical within-subjects user study with 42 participants, we compared IntenSelect+ to IntenSelect and conventional raycasting on various complex object configurations motivated by prior work. In addition to replicating the previously shown benefits of IntenSelect over raycasting, our results demonstrate significant advantages of IntenSelect+ over IntenSelect regarding selection performance, task load, and user experience. We, therefore, conclude that IntenSelect+ is a promising enhancement of the original approach that enables faster, more precise, and more comfortable object selection in immersive virtual environments.},
  archive      = {J_TVCG},
  author       = {Marcel Krüger and Tim Gerrits and Timon Römer and Torsten Kuhlen and Tim Weissker},
  doi          = {10.1109/TVCG.2024.3372077},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2829-2838},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {IntenSelect+: Enhancing score-based selection in virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring audio interfaces for vertical guidance in
augmented reality via hand-based feedback. <em>TVCG</em>,
<em>30</em>(5), 2818–2828. (<a
href="https://doi.org/10.1109/TVCG.2024.3372040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research proposes an evaluation of pitch-based sonification methods via user experiments in real-life scenarios, specifically vertical guidance, with the aim of standardizing the use of audio interfaces in AR in guidance tasks. Using literature on assistive technology for people who are blind or visually impaired, we aim to generalize their applicability to a broader population and for different use cases. We propose and test sonification methods for vertical guidance in a series of hand-navigation assessments with users without visual feedback. Including feedback from a visually impaired expert in digital accessibility, results (N=19) outlined that methods that do not rely on memorizing pitch had the most promising accuracy and self-reported workload performances. Ultimately, we argue for audio AR&#39;s ability to enhance user performance in different scenarios, from video games to finding objects in a pantry.},
  archive      = {J_TVCG},
  author       = {Renan Guarese and Emma Pretty and Aidan Renata and Deb Polson and Fabio Zambetta},
  doi          = {10.1109/TVCG.2024.3372040},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2818-2828},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring audio interfaces for vertical guidance in augmented reality via hand-based feedback},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning middle-latitude cyclone formation up in the air:
Student learning experience, outcomes, and perceptions in a CAVE-enabled
meteorology class. <em>TVCG</em>, <em>30</em>(5), 2807–2817. (<a
href="https://doi.org/10.1109/TVCG.2024.3372072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cave Automatic Virtual Environment (CAVE) is a virtual reality (VR) environment that has not been fully studied due to its high cost and complexity in system integration. Previous CAVE-related studies mainly focused on comparing its effectiveness with other learning media, such as textbooks, desktop VR, or head-mounted display (HMD) VR. In this study, through the utilization of CAVE in a meteorology class, we concentrated on CAVE itself, measured how CAVE impacted learners&#39; learning outcomes before and after using CAVE in an actual ongoing undergraduate-level class, and investigated how learners perceived their learning experiences. Quantitative data were collected to examine the students&#39; knowledge acquisition and learning experience. We also triangulated the quantitative results with qualitative data from the interviews regarding learners&#39; perceptions of the CAVE-enabled class and their knowledge mastery. The results indicated that their learning outcomes increased through learning with CAVE and that their perceptions of immersion, presence, and engagement significantly correlated with each other. The interview results showed a great fondness of and satisfaction with the learning experience, group collaboration, and effectiveness of the CAVE-enabled class from the learners. We also learned that the learners&#39; learning experiences in CAVE could be further improved if we provided them with more learner-environment interaction, offered them a better sense of immersion, and reduced cybersickness. Implications of these findings are discussed.},
  archive      = {J_TVCG},
  author       = {Hao He and Xinhao Xu and Shangman Li and Fang Wang and Isaac Schroeder and Eric M. Aldrich and Scottie D. Murrell and Lanxin Xue and Yuanyuan Gu},
  doi          = {10.1109/TVCG.2024.3372072},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2807-2817},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Learning middle-latitude cyclone formation up in the air: Student learning experience, outcomes, and perceptions in a CAVE-enabled meteorology class},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hold tight: Identifying behavioral patterns during prolonged
work in VR through video analysis. <em>TVCG</em>, <em>30</em>(5),
2796–2806. (<a href="https://doi.org/10.1109/TVCG.2024.3372048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {VR devices have recently been actively promoted as tools for knowledge workers and prior work has demonstrated that VR can support some knowledge worker tasks. However, only a few studies have explored the effects of prolonged use of VR such as a study observing 16 participants working in VR and a physical environment for one work-week each and reporting mainly on subjective feedback. As a nuanced understanding of participants&#39; behavior in VR and how it evolves over time is still missing, we report on the results from an analysis of 559 hours of video material obtained in this prior study. Among other findings, we report that (1) the frequency of actions related to adjusting the headset reduced by 46% and the frequency of actions related to supporting the headset reduced by 42% over the five days; (2) the HMD was removed 31% less frequently over the five days but for 41% longer periods; (3) wearing an HMD is disruptive to normal patterns of eating and drinking, but not to social interactions, such as talking. The combined findings in this work demonstrate the value of long-term studies of deployed VR systems and can be used to inform the design of better, more ergonomic VR systems as tools for knowledge workers.},
  archive      = {J_TVCG},
  author       = {Verena Biener and Forouzan Farzinnejad and Rinaldo Schuster and Seyedmasih Tabaei and Leon Lindlein and Jinghui Hu and Negar Nouri and John J. Dudley and Per Ola Krlstensson and Jörg Müller and Jens Grubert},
  doi          = {10.1109/TVCG.2024.3372048},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2796-2806},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Hold tight: Identifying behavioral patterns during prolonged work in VR through video analysis},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). With or without you: Effect of contextual and responsive
crowds on VR-based crowd motion capture. <em>TVCG</em>, <em>30</em>(5),
2785–2795. (<a href="https://doi.org/10.1109/TVCG.2024.3372038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While data is vital to better understand and model interactions within human crowds, capturing real crowd motions is extremely challenging. Virtual Reality (VR) demonstrated its potential to help, by immersing users into either simulated virtual crowds based on autonomous agents, or within motion-capture-based crowds. In the latter case, users&#39; own captured motion can be used to progressively extend the size of the crowd, a paradigm called Record-and-Replay (2R). However, both approaches demonstrated several limitations which impact the quality of the acquired crowd data. In this paper, we propose the new concept of contextual crowds to leverage both crowd simulation and the 2R paradigm towards more consistent crowd data. We evaluate two different strategies to implement it, namely a Replace-Record-Replay (3R) paradigm where users are initially immersed into a simulated crowd whose agents are successively replaced by the user&#39;s captured-data, and a Replace-Record-Replay-Responsive (4R) paradigm where the pre-recorded agents are additionally endowed with responsive capabilities. These two paradigms are evaluated through two real-world-based scenarios replicated in VR. Our results suggest that the behaviors observed in VR users with surrounding agents from the beginning of the recording process are made much more natural, enabling 3R or 4R paradigms to improve the consistency of captured crowd datasets.},
  archive      = {J_TVCG},
  author       = {Tairan Yin and Ludovic Hoyet and Marc Christie and Marie-Paule Cani and Julien Pettré},
  doi          = {10.1109/TVCG.2024.3372038},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2785-2795},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {With or without you: Effect of contextual and responsive crowds on VR-based crowd motion capture},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparing synchronous and asynchronous task delivery in
mixed reality environments. <em>TVCG</em>, <em>30</em>(5), 2776–2784.
(<a href="https://doi.org/10.1109/TVCG.2024.3372034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asynchronous digital communication is a widely applied and well-known form of information exchange. Most pieces of technology make use of some variation of asynchronous communication systems, be it messaging or email applications. This allows recipients to process digital messages immediately (synchronous) or whenever they have time (asynchronous), meaning that purely digital interruptions can be mitigated easily. Mixed Reality systems have the potential to not only handle digital interruptions but also interruptions in physical space, e.g., caused by co-workers in workspaces or learning environments. However, the benefits of such systems previously remained untested in the context of Mixed Reality. We conducted a user study ($\mathrm{N}=26$) to investigate the impact that the timing of task delivery has on the participants&#39; performance, workflow, and emotional state. Participants had to perform several cognitively demanding tasks in a Mixed Reality workspace. Inside the virtual workspace, we simulated in-person task delivery either during tasks (i.e., interrupting the participant) or between tasks (i.e., delaying the interruption). Our results show that delaying interruptions has a significant impact on subjective metrics like the perceived performance and workload.},
  archive      = {J_TVCG},
  author       = {Lara Sofie Lenz and Andreas Rene Fender and Julia Chatain and Christian Holz},
  doi          = {10.1109/TVCG.2024.3372034},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2776-2784},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Comparing synchronous and asynchronous task delivery in mixed reality environments},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HoloCamera: Advanced volumetric capture for
cinematic-quality VR applications. <em>TVCG</em>, <em>30</em>(5),
2767–2775. (<a href="https://doi.org/10.1109/TVCG.2024.3372123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-precision virtual environments are increasingly important for various education, simulation, training, performance, and entertainment applications. We present HoloCamera, an innovative volumetric capture instrument to rapidly acquire, process, and create cinematic-quality virtual avatars and scenarios. The HoloCamera consists of a custom-designed free-standing structure with 300 high-resolution RGB cameras mounted with uniform spacing spanning the four sides and the ceiling of a room-sized studio. The light field acquired from these cameras is streamed through a distributed array of GPUs that interleave the processing and transmission of 4K resolution images. The distributed compute infrastructure that powers these RGB cameras consists of 50 Jetson AGX Xavier boards, with each processing unit dedicated to driving and processing imagery from six cameras. A high-speed Gigabit Ethernet network fabric seamlessly interconnects all computing boards. In this systems paper, we provide an in-depth description of the steps involved and lessons learned in constructing such a cutting-edge volumetric capture facility that can be generalized to other such facilities. We delve into the techniques employed to achieve precise frame synchronization and spatial calibration of cameras, careful determination of angled camera mounts, image processing from the camera sensors, and the need for a resilient and robust network infrastructure. To advance the field of volumetric capture, we are releasing a high-fidelity static light-field dataset, which will serve as a benchmark for further research and applications of cinematic-quality volumetric light fields.},
  archive      = {J_TVCG},
  author       = {Jonathan Heagerty and Sida Li and Eric Lee and Shuvra Bhattacharyya and Sujal Bista and Barbara Brawn and Brandon Y. Feng and Susmija Jabbireddy and Joseph JaJa and Hernisa Kacorri and David Li and Derek Yarnell and Matthias Zwicker and Amitabh Varshney},
  doi          = {10.1109/TVCG.2024.3372123},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2767-2775},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HoloCamera: Advanced volumetric capture for cinematic-quality VR applications},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Investigating the effects of avatarization and interaction
techniques on near-field mixed reality interactions with physical
components. <em>TVCG</em>, <em>30</em>(5), 2756–2766. (<a
href="https://doi.org/10.1109/TVCG.2024.3372050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed reality (MR) interactions feature users interacting with a combination of virtual and physical components. Inspired by research investigating aspects associated with near-field interactions in augmented and virtual reality (AR &amp; VR), we investigated how avatarization, the physicality of the interacting components, and the interaction technique used to manipulate a virtual object affected performance and perceptions of user experience in a mixed reality fundamentals of laparoscopic peg-transfer task wherein users had to transfer a virtual ring from one peg to another for a number of trials. We employed a 3 (Physicality of pegs) X 3 (Augmented Avatar Representation) X 2 (Interaction Technique) multi-factorial design, manipulating the physicality of the pegs as a between-subjects factor, the type of augmented self-avatar representation, and the type of interaction technique used for object-manipulation as within-subjects factors. Results indicated that users were significantly more accurate when the pegs were virtual rather than physical because of the increased salience of the task-relevant visual information. From an avatar perspective, providing users with a reach envelope-extending representation, though useful, was found to worsen performance, while co-located avatarization significantly improved performance. Choosing an interaction technique to manipulate objects depends on whether accuracy or efficiency is a priority. Finally, the relationship between the avatar representation and interaction technique dictates just how usable mixed reality interactions are deemed to be.},
  archive      = {J_TVCG},
  author       = {Roshan Venkatakrishnan and Rohith Venkatakrishnan and Ryan Canales and Balagopal Raveendranath and Christopher C. Pagano and Andrew C. Robb and Wen-Chieh Lin and Sabarish V. Babu},
  doi          = {10.1109/TVCG.2024.3372050},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2756-2766},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Investigating the effects of avatarization and interaction techniques on near-field mixed reality interactions with physical components},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The effects of secondary task demands on cybersickness in
active exploration virtual reality experiences. <em>TVCG</em>,
<em>30</em>(5), 2745–2755. (<a
href="https://doi.org/10.1109/TVCG.2024.3372080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active exploration in virtual reality (VR) involves users navigating immersive virtual environments, going from one place to another. While navigating, users often engage in secondary tasks that require attentional resources, as in the case of distracted driving. Inspired by research generally studying the effects of task demands on cybersickness (CS), we investigated how the attentional demands specifically associated with secondary tasks performed during exploration affect CS. Downstream of this, we studied how increased attentional demands from secondary tasks affect spatial memory and navigational performance. We discuss the results of a multi-factorial between-subjects study, manipulating a secondary task&#39;s demand across two levels and studying its effects on CS in two different sickness-inducing levels of an exploration experience. The secondary task&#39;s demand was manipulated by parametrically varying $n$ in an aural $n$-back working memory task and the provocativeness of the experience was manipulated by varying how frequently users experienced a yaw-rotational reorientation effect during the exploration. Results revealed that increases in the secondary task&#39;s demand increased sickness levels, also resulting in a higher temporal onset rate, especially when the experience was not already highly sickening. Increased attentional demand from the secondary task also vitiated navigational performance and spatial memory. Overall, increased demands from secondary tasks performed during navigation produce deleterious effects on the VR experience.},
  archive      = {J_TVCG},
  author       = {Rohith Venkatakrishnan and Roshan Venkatakrishnan and Balagopal Raveendranath and Ryan Canales and Dawn M. Sarno and Andrew C. Robb and Wen–Chieh Lin and Sabarish V. Babu},
  doi          = {10.1109/TVCG.2024.3372080},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2745-2755},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The effects of secondary task demands on cybersickness in active exploration virtual reality experiences},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dream360: Diverse and immersive outdoor virtual scene
creation via transformer-based 360° image outpainting. <em>TVCG</em>,
<em>30</em>(5), 2734–2744. (<a
href="https://doi.org/10.1109/TVCG.2024.3372085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {360° images, with a field-of-view (FoV) of $180^{\circ}\times 360^{\circ}$, provide immersive and realistic environments for emerging virtual reality (VR) applications, such as virtual tourism, where users desire to create diverse panoramic scenes from a narrow FoV photo they take from a viewpoint via portable devices. It thus brings us to a technical challenge: ‘How to allow the users to freely create diverse and immersive virtual scenes from a narrow FoV image with a specified viewport?’ To this end, we propose a transformer-based 360° image outpainting framework called Dream360, which can generate diverse, high-fidelity, and high-resolution panoramas from user-selected viewports, considering the spherical properties of 360° images. Compared with existing methods, e.g., [3], which primarily focus on inputs with rectangular masks and central locations while overlooking the spherical property of 360° images, our Dream360 offers higher outpainting flexibility and fidelity based on the spherical representation. Dream360 comprises two key learning stages: (I) codebook-based panorama outpainting via Spherical-VQGAN (S-VQGAN), and (II) frequency-aware refinement with a novel frequency-aware consistency loss. Specifically, S-VQGAN learns a sphere-specific codebook from spherical harmonic (SH) values, providing a better representation of spherical data distribution for scene modeling. The frequency-aware refinement matches the resolution and further improves the semantic consistency and visual fidelity of the generated results. Our Dream360 achieves significantly lower Frechet Inception Distance (FID) scores and better visual fidelity than existing methods. We also conducted a user study involving 15 participants to interactively evaluate the quality of the generated results in VR, demonstrating the flexibility and superiority of our Dream360 framework.},
  archive      = {J_TVCG},
  author       = {Hao Ai and Zidong Cao and Haonan Lu and Chen Chen and Jian Ma and Pengyuan Zhou and Tae-Kyun Kim and Pan Hui and Lin Wang},
  doi          = {10.1109/TVCG.2024.3372085},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2734-2744},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dream360: Diverse and immersive outdoor virtual scene creation via transformer-based 360° image outpainting},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Workspace guardian: Investigating awareness of personal
workspace between co-located augmented reality users. <em>TVCG</em>,
<em>30</em>(5), 2724–2733. (<a
href="https://doi.org/10.1109/TVCG.2024.3372073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As augmented reality (AR) systems proliferate and the technology gets smaller and less intrusive, we imagine a future where many AR users will interact in the same physical locations (e.g., in shared work places and public spaces). While previous research has explored AR collaboration in these spaces, our focus is on co-located but independent work. In this paper, we explore co-located AR user behavior and investigate techniques for promoting awareness of personal workspace boundaries. Specifically, we compare three techniques: showing all virtual content, visualizing bounding box outlines of content, and a self-defined workspace boundary. The findings suggest that a self-defined boundary led to significantly more personal workspace encroachments.},
  archive      = {J_TVCG},
  author       = {Bret Jackson and Linda Lor and Brianna C. Heggeseth},
  doi          = {10.1109/TVCG.2024.3372073},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2724-2733},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Workspace guardian: Investigating awareness of personal workspace between co-located augmented reality users},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing tai chi training system: Towards group-based and
hyper-realistic training experiences. <em>TVCG</em>, <em>30</em>(5),
2713–2723. (<a href="https://doi.org/10.1109/TVCG.2024.3372099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a lightweight and flexible enhanced Tai Chi training system composed of multiple standalone virtual reality (VR) devices. The system aims to enable a hyper-realistic multi-user action training platform at low cost by displaying real-time action guidance trajectories, providing real-world impossible visual effects and functions, and rapidly enhancing movement precision and communication interest for learners. We objectively evaluate participants&#39; action quality at different levels of immersion, including traditional coach guidance (TCG), VR, and mixed reality (MR), along with subjective measures like motion sickness, quality of interaction, social meaning, presence/immersion to comprehensively explore the system&#39;s feasibility. The results indicate VR performs the best in training accuracy, but MR provides superior social experience and relatively high accuracy. Unlike TCG, MR offers hyper-realistic hand movement trajectories and Tai Chi social references. Compared with VR, MR provides more realistic avatar companions and a safer environment. In summary, MR balances accuracy and social experience.},
  archive      = {J_TVCG},
  author       = {Feng Tian and Shuting Ni and Xiaoyue Zhang and Fei Chen and Qiaolian Zhu and Chunyi Xu and Yuzhi Li},
  doi          = {10.1109/TVCG.2024.3372099},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2713-2723},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Enhancing tai chi training system: Towards group-based and hyper-realistic training experiences},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The differential effects of multisensory attentional cues on
task performance in VR depending on the level of cognitive load and
cognitive capacity. <em>TVCG</em>, <em>30</em>(5), 2703–2712. (<a
href="https://doi.org/10.1109/TVCG.2024.3372126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the utilization of VR is expanding across diverse fields, research on devising attentional cues that could optimize users&#39; task performance in VR has become crucial. Since the cognitive load imposed by the context and the individual&#39;s cognitive capacity are representative factors that are known to determine task performance, we aimed to examine how the effects of multisensory attentional cues on task performance are modulated by the two factors. For this purpose, we designed a new experimental paradigm in which participants engaged in dual (N-back, visual search) tasks under different levels of cognitive load while an attentional cue (visual, tactile, or visuotactile) was presented to facilitate search performance. The results showed that multi-sensory attentional cues are generally more effective than uni-sensory cues in enhancing task performance, but the benefit of multi-sensory cues changes according to the level of cognitive load and the individual&#39;s cognitive capacity; the amount of benefit increases as the cognitive load is higher and the cognitive capacity is lower. The findings of this study provide practical implications for designing attentional cues to enhance VR task performance, considering both the complexity of the VR context and users&#39; internal characteristics.},
  archive      = {J_TVCG},
  author       = {Sihyun Jeong and Jinwook Kim and Jeongmi Lee},
  doi          = {10.1109/TVCG.2024.3372126},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2703-2712},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The differential effects of multisensory attentional cues on task performance in VR depending on the level of cognitive load and cognitive capacity},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BiRD: Using bidirectional rotation gain differences to
redirect users during back-and-forth head turns in walking.
<em>TVCG</em>, <em>30</em>(5), 2693–2702. (<a
href="https://doi.org/10.1109/TVCG.2024.3372094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Redirected walking (RDW) facilitates user navigation within expansive virtual spaces despite the constraints of limited physical spaces. It employs discrepancies between human visual-proprioceptive sensations, known as gains, to enable the remapping of virtual and physical environments. In this paper, we explore how to apply rotation gain while the user is walking. We propose to apply a rotation gain to let the user rotate by a different angle when reciprocating from a previous head rotation, to achieve the aim of steering the user to a desired direction. To apply the gains imperceptibly based on such a Bidirectional Rotation gain Difference (BiRD), we conduct both measurement and verification experiments on the detection thresholds of the rotation gain for reciprocating head rotations during walking. Unlike previous rotation gains which are measured when users are turning around in place (standing or sitting), BiRD is measured during users&#39; walking. Our study offers a critical assessment of the acceptable range of rotational mapping differences for different rotational orientations across the user&#39;s walking experience, contributing to an effective tool for redirecting users in virtual environments.},
  archive      = {J_TVCG},
  author       = {Sen-Zhe Xu and Fiona Xiao Yu Chen and Ran Gong and Fang-Lue Zhang and Song-Hai Zhang},
  doi          = {10.1109/TVCG.2024.3372094},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2693-2702},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {BiRD: Using bidirectional rotation gain differences to redirect users during back-and-forth head turns in walking},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). StainedSweeper: Compact, variable-intensity
light-attenuation display with sweeping tunable retarders.
<em>TVCG</em>, <em>30</em>(5), 2682–2692. (<a
href="https://doi.org/10.1109/TVCG.2024.3372058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light Attenuation Displays (LADs) are a type of Optical See-Through Head-Mounted Display (OST-HMD) that present images by attenuating incoming light with a pixel-wise polarizing color filter. Although LADs can display images in bright environments, there is a trade-off between the number of Spatial Light Modulators (SLMs) and the color gamut and contrast that can be expressed, making it difficult to achieve both high-fidelity image display and a small form factor. To address this problem, we propose StainedSweeper, a LAD that achieves both the wide color gamut and the variable intensity with a single SLM. Our system synchronously controls a pixel-wise Digital Micromirror Device (DMD) and a nonpixel polarizing color filter to pass light when each pixel is the desired color. By sweeping this control at high speed, the human eye perceives images in a time-multiplexed, integrated manner. To achieve this, we develop the OST-HMD design using a reflective Solc filter as a polarized color filter and a color reproduction algorithm based on the optimization of the time-multiplexing matrix for the selected primary color filters. Our proof-of-concept prototype showed that our single SLM design can produce subtractive images with variable contrast and a wider color gamut than conventional LADs.},
  archive      = {J_TVCG},
  author       = {Yuichi Hiroi and Takefumi Hiraki and Yuta Itoh},
  doi          = {10.1109/TVCG.2024.3372058},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2682-2692},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {StainedSweeper: Compact, variable-intensity light-attenuation display with sweeping tunable retarders},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CAEVR: Biosignals-driven context-aware empathy in virtual
reality. <em>TVCG</em>, <em>30</em>(5), 2671–2681. (<a
href="https://doi.org/10.1109/TVCG.2024.3372130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is little research on how Virtual Reality (VR) applications can identify and respond meaningfully to users&#39; emotional changes. In this paper, we investigate the impact of Context-Aware Empathic VR (CAEVR) on the emotional and cognitive aspects of user experience in VR. We developed a real-time emotion prediction model using electroencephalography (EEG), electrodermal activity (EDA), and heart rate variability (HRV) and used this in personalized and generalized models for emotion recognition. We then explored the application of this model in a context-aware empathic (CAE) virtual agent and an emotion-adaptive (EA) VR environment. We found a significant increase in positive emotions, cognitive load, and empathy toward the CAE agent, suggesting the potential of CAEVR environments to refine user-agent interactions. We identify lessons learned from this study and directions for future work.},
  archive      = {J_TVCG},
  author       = {Kunal Gupta and Yuewei Zhang and Tamil Selvan Gunasekaran and Nanditha Krishna and Yun Suen Pai and Mark Billinghurst},
  doi          = {10.1109/TVCG.2024.3372130},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2671-2681},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CAEVR: Biosignals-driven context-aware empathy in virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beyond the wizard of oz: Negative effects of imperfect
machine learning to examine the impact of reliability of augmented
reality cues on visual search performance. <em>TVCG</em>,
<em>30</em>(5), 2662–2670. (<a
href="https://doi.org/10.1109/TVCG.2024.3372062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite knowing exactly what an object looks like, searching for it in a person&#39;s visual field is a time-consuming and error-prone experience. In Augmented Reality systems, new algorithms are proposed to speed up search time and reduce human errors. However, these algorithms might not always provide 100% accurate visual cues, which might affect users&#39; perceived reliability of the algorithm and, thus, search performance. Here, we examined the detrimental effects of automation bias caused by imperfect cues presented in the Augmented Reality head-mounted display using the YOLOv5 machine learning model. 53 participants in the two groups received either 100% accurate visual cues or 88.9% accurate visual cues. Their performance was compared with the control condition, which did not include any additional cues. The results show how cueing may increase performance and shorten search times. The results also showed that performance with imperfect automation was much worse than perfect automation and that, consistent with automation bias, participants were frequently enticed by incorrect cues.},
  archive      = {J_TVCG},
  author       = {Aditya Raikwar and Domenick Mifsud and Christopher D. Wickens and Anil Ufuk Batmaz and Amelia C. Warden and Brendan Kelley and Benjamin A. Clegg and Francisco R. Ortega},
  doi          = {10.1109/TVCG.2024.3372062},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2662-2670},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Beyond the wizard of oz: Negative effects of imperfect machine learning to examine the impact of reliability of augmented reality cues on visual search performance},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Examining effects of technique awareness on the detection of
remapped hands in virtual reality. <em>TVCG</em>, <em>30</em>(5),
2651–2661. (<a href="https://doi.org/10.1109/TVCG.2024.3372054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Input remapping techniques have been widely explored to allow users in virtual reality to exceed both their own physical abilities, the limitations of physical space, or to facilitate interactions with real-world objects. Often considered is how these techniques can be applied to achieve maximum utility, but still be undetectable to users to maintain a sense of immersion and presence. Existing psychophysical methods used to determine these detection thresholds have known limitations: they are highly conservative lower bounds for detection and do not account for complex usage of the technique. Our work describes and evaluates a method for estimating detection that reduces these limitations and yields meaningful upper bounds. We present the findings of our work where we apply this method to a well-explored hand motion scaling technique. In wholly unaware cases, we determined that users may detect their hand speed as abnormal at around 3.37 times the normal speed, compared to a scale factor of 1.47 that was estimated using traditional methods when users knew the motion scaling was occurring. A considerable number of participants in unaware cases (12 of 56) never detected their hand speed increasing at all, even at the maximum scale factor of 5.0. The study demonstrates just how conservative the thresholds generated by traditional psychophysical methods can be compared to detection during naive usage, and our method can be modified and applied easily to other techniques.},
  archive      = {J_TVCG},
  author       = {Brett Benda and Benjamin Rheault and Yanna Lin and Eric D. Ragan},
  doi          = {10.1109/TVCG.2024.3372054},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2651-2661},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Examining effects of technique awareness on the detection of remapped hands in virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Animatable virtual humans: Learning pose-dependent human
representations in UV space for interactive performance synthesis.
<em>TVCG</em>, <em>30</em>(5), 2644–2650. (<a
href="https://doi.org/10.1109/TVCG.2024.3372117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel representation of virtual humans for highly realistic real-time animation and rendering in 3D applications. We learn pose dependent appearance and geometry from highly accurate dynamic mesh sequences obtained from state-of-the-art multiview-video reconstruction. Learning pose-dependent appearance and geometry from mesh sequences poses significant challenges, as it requires the network to learn the intricate shape and articulated motion of a human body. However, statistical body models like SMPL provide valuable a-priori knowledge which we leverage in order to constrain the dimension of the search space, enabling more efficient and targeted learning and to define pose-dependency. Instead of directly learning absolute pose-dependent geometry, we learn the difference between the observed geometry and the fitted SMPL model. This allows us to encode both pose-dependent appearance and geometry in the consistent UV space of the SMPL model. This approach not only ensures a high level of realism but also facilitates streamlined processing and rendering of virtual humans in real-time scenarios.},
  archive      = {J_TVCG},
  author       = {Wieland Morgenstern and Milena T. Bagdasarian and Anna Hilsmann and Peter Eisert},
  doi          = {10.1109/TVCG.2024.3372117},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2644-2650},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Animatable virtual humans: Learning pose-dependent human representations in UV space for interactive performance synthesis},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A study on collaborative visual data analysis in augmented
reality with asymmetric display types. <em>TVCG</em>, <em>30</em>(5),
2633–2643. (<a href="https://doi.org/10.1109/TVCG.2024.3372103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaboration is a key aspect of immersive visual data analysis. Due to its inherent benefit of seeing co-located collaborators, augmented reality is often useful in such collaborative scenarios. However, to enable the augmentation of the real environment, there are different types of technology available. While there are constant developments in specific devices, each of these device types provide different premises for collaborative visual data analysis. In our work we combine handheld, optical see-through and video see-through displays to explore and understand the impact of these different device types in collaborative immersive analytics. We conducted a mixed-methods collaborative user study where groups of three performed a shared data analysis task in augmented reality with each user working on a different device, to explore differences in collaborative behaviour, user experience and usage patterns. Both quantitative and qualitative data revealed differences in user experience and usage patterns. For collaboration, the different display types influenced how well participants could participate in the collaborative data analysis, nevertheless, there was no measurable effect in verbal communication.},
  archive      = {J_TVCG},
  author       = {Judith Friedl-Knirsch and Christian Stach and Fabian Pointecker and Christoph Anthes and Daniel Roth},
  doi          = {10.1109/TVCG.2024.3372103},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2633-2643},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A study on collaborative visual data analysis in augmented reality with asymmetric display types},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling the impact of head-body rotations on audio-visual
spatial perception for virtual reality applications. <em>TVCG</em>,
<em>30</em>(5), 2624–2632. (<a
href="https://doi.org/10.1109/TVCG.2024.3372112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans perceive the world by integrating multimodal sensory feedback, including visual and auditory stimuli, which holds true in virtual reality (VR) environments. Proper synchronization of these stimuli is crucial for perceiving a coherent and immersive VR experience. In this work, we focus on the interplay between audio and vision during localization tasks involving natural head-body rotations. We explore the impact of audio-visual offsets and rotation velocities on users&#39; directional localization acuity for various viewing modes. Using psychometric functions, we model perceptual disparities between visual and auditory cues and determine offset detection thresholds. Our findings reveal that target localization accuracy is affected by perceptual audio-visual disparities during head-body rotations, but remains consistent in the absence of stimuli-head relative motion. We then showcase the effectiveness of our approach in predicting and enhancing users&#39; localization accuracy within realistic VR gaming applications. To provide additional support for our findings, we implement a natural VR game wherein we apply a compensatory audio-visual offset derived from our measured psychometric functions. As a result, we demonstrate a substantial improvement of up to 40% in participants&#39; target localization accuracy. We additionally provide guidelines for content creation to ensure coherent and seamless VR experiences.},
  archive      = {J_TVCG},
  author       = {Edurne Bernal-Berdun and Mateo Vallejo and Qi Sun and Ana Serrano and Diego Gutierrez},
  doi          = {10.1109/TVCG.2024.3372112},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2624-2632},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Modeling the impact of head-body rotations on audio-visual spatial perception for virtual reality applications},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analyzing user behaviour patterns in a cross-virtuality
immersive analytics system. <em>TVCG</em>, <em>30</em>(5), 2613–2623.
(<a href="https://doi.org/10.1109/TVCG.2024.3372129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent work in immersive analytics suggests benefits for systems that support work across both 2D and 3D data visualizations, i.e., cross-virtuality analytics systems. Here, we introduce HybridAxes, an immersive visual analytics system that enables users to conduct their analysis either in 2D on desktop monitors or in 3D within an immersive AR environment - while enabling them to seamlessly switch and transfer their graphs between modes. Our user study results show that the cross-virtuality sub-systems in HybridAxes complement each other well in helping the users in their data-understanding journey. We show that users preferred using the AR component for exploring the data, while they used the desktop to work on more detail-intensive tasks. Despite encountering some minor challenges in switching between the two virtuality modes, users consistently rated the whole system as highly engaging, user-friendly, and helpful in streamlining their analytics processes. Finally, we present suggestions for designers of cross-virtuality visual analytics systems and identify avenues for future work.},
  archive      = {J_TVCG},
  author       = {Mohammad Rajabi Seraji and Parastoo Piray and Vahid Zahednejad and Wolfgang Stuerzlinger},
  doi          = {10.1109/TVCG.2024.3372129},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2613-2623},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Analyzing user behaviour patterns in a cross-virtuality immersive analytics system},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating text reading speed in VR scenes and 3D particle
visualizations. <em>TVCG</em>, <em>30</em>(5), 2602–2612. (<a
href="https://doi.org/10.1109/TVCG.2024.3372093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work reports how text size and other rendering conditions affect reading speeds in a virtual reality environment and a scientific data analysis application. Displaying text legibly yet space-efficiently is a challenging problem in immersive displays. Effective text displays that enable users to read at their maximum speed must consider the variety of virtual reality (VR) display hardware and possible visual exploration tasks. We investigate how text size and display parameters affect reading speed and legibility in three state-of-the-art VR displays: two head-mounted displays and one CAVE. In our perception experiments, we establish limits where reading speed declines as the text size approaches the so-called critical print sizes (CPS) of individual displays, which can inform the design of uniform reading experiences across different VR systems. We observe an inverse correlation between display resolution and CPS. Yet, even in high-fidelity VR systems, the measured CPS was larger than in comparable physical text displays, highlighting the value of increased VR display resolutions in certain visualization scenarios. Our findings indicate that CPS can be an effective metric for evaluating VR display usability. Additionally, we evaluate the effects of text panel placement, orientation, and occlusion-reducing rendering methods on reading speeds in generic volumetric particle visualizations. Our study provides insights into the trade-off between text representation and legibility in cluttered immersive environments with specific suggestions for visualization designers and highlight areas for further research.},
  archive      = {J_TVCG},
  author       = {Johannes Novotny and David H. Laidlaw},
  doi          = {10.1109/TVCG.2024.3372093},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2602-2612},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Evaluating text reading speed in VR scenes and 3D particle visualizations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Empowering persons with autism through cross-reality and
conversational agents. <em>TVCG</em>, <em>30</em>(5), 2591–2601. (<a
href="https://doi.org/10.1109/TVCG.2024.3372110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autism Spectrum Disorder is a neurodevelopmental condition that can affect autonomy and independence. Our research explores the integration of Cross-Reality and Conversational Agents for Autistic persons to improve ability and confidence in everyday life situations. We combine two technologies of the Virtual-Real continuum. User experiences unfold from the simulation of tasks in VR to the execution of similar tasks supported by AR in the real world. A speech-based Conversational Agent is integrated with both VR and AR. It provides contextualized help, promotes generalization, and stimulates users to apply what they learned in the virtual space. The paper presents the approach and describes an empirical study involving 17 young Autistic persons.},
  archive      = {J_TVCG},
  author       = {Franca Garzotto and Mattia Gianotti and Alberto Patti and Francesca Pentimalli and Francesco Vona},
  doi          = {10.1109/TVCG.2024.3372110},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2591-2601},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Empowering persons with autism through cross-reality and conversational agents},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Breaking the isolation: Exploring the impact of passthrough
in shared spaces on player performance and experience in VR exergames.
<em>TVCG</em>, <em>30</em>(5), 2580–2590. (<a
href="https://doi.org/10.1109/TVCG.2024.3372114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {VR exergames offer an engaging solution to combat sedentary behavior and promote physical activity. However, challenges emerge when playing these games in shared spaces, particularly due to the presence of bystanders. VR&#39;s passthrough functionality enables players to maintain awareness of their surrounding environment while immersed in VR gaming, rendering it a promising solution to improve users&#39; awareness of the environment. This study investigates the passthrough&#39;s impact on player performance and experiences in shared spaces, involving an experiment with 24 participants that examines Space (Office vs. Corridor) and Passthrough Function (With vs. Without). Results reveal that Passthrough enhances game performance and environmental awareness while reducing immersion. Players prefer an open area to an enclosed room, whether with or without Passthrough, finding it more socially acceptable. Additionally, Passthrough appears to encourage participation among players with higher self-consciousness, potentially alleviating their concerns about being observed by bystanders. Our findings provide valuable insights for designing VR experiences in shared spaces, underscoring the potential of VR&#39;s passthrough to enhance user experiences and promote VR adoption in these environments.},
  archive      = {J_TVCG},
  author       = {Zixuan Guo and Hongyu Wang and Hanxiao Deng and Wenge Xu and Nilufar Baghaei and Cheng-Hung Lo and Hai-Ning Liang},
  doi          = {10.1109/TVCG.2024.3372114},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2580-2590},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Breaking the isolation: Exploring the impact of passthrough in shared spaces on player performance and experience in VR exergames},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Perceptual thresholds for radial optic flow distortion in
near-eye stereoscopic displays. <em>TVCG</em>, <em>30</em>(5),
2570–2579. (<a href="https://doi.org/10.1109/TVCG.2024.3372075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide the first perceptual quantification of user&#39;s sensitivity to radial optic flow artifacts and demonstrate a promising approach for masking this optic flow artifact via blink suppression. Near-eye HMOs allow users to feel immersed in virtual environments by providing visual cues, like motion parallax and stereoscopy, that mimic how we view the physical world. However, these systems exhibit a variety of perceptual artifacts that can limit their usability and the user&#39;s sense of presence in VR. One well-known artifact is the vergence-accommodation conflict (VAC). Varifocal displays can mitigate VAC, but bring with them other artifacts such as a change in virtual image size (radial optic flow) when the focal plane changes. We conducted a set of psychophysical studies to measure users&#39; ability to perceive this radial flow artifact before, during, and after self-initiated blinks. Our results showed that visual sensitivity was reduced by a factor of 10 at the start and for ~70 ms after a blink was detected. Pre- and post-blink sensitivity was, on average, ~O.15% image size change during normal viewing and increased to ~1.5- 2.0% during blinks. Our results imply that a rapid (under 70 ms) radial optic flow distortion can go unnoticed during a blink. Furthermore, our results provide empirical data that can be used to inform engineering requirements for both hardware design and software-based graphical correction algorithms for future varifocal near-eye displays. Our project website is available at https://gamma.umd.edu/ROF/.},
  archive      = {J_TVCG},
  author       = {Mohammad R. Saeedpour-Parizi and Niall L. Williams and Tim Wong and Phillip Guan and Dinesh Manocha and Ian M. Erkelens},
  doi          = {10.1109/TVCG.2024.3372075},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2570-2579},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Perceptual thresholds for radial optic flow distortion in near-eye stereoscopic displays},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PetPresence: Investigating the integration of real-world pet
activities in virtual reality. <em>TVCG</em>, <em>30</em>(5), 2559–2569.
(<a href="https://doi.org/10.1109/TVCG.2024.3372095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For VR interaction, the home environment with complicated spatial setup and dynamics may hinder the VR user experience. In particular, pets&#39; movement may be more unpredictable. In this paper, we investigate the integration of real-world pet activities into immersive VR interaction. Our pilot study showed that the active pet movements, especially dogs, could negatively impact users&#39; performance and experience in immersive VR. We proposed three different types of pet integration, namely semitransparent real-world portal, non-interactive object in VR, and interactive object in VR. We conducted the user study with 16 pet owners and their pets. The results showed that compared to the baseline condition without any pet-integration technique, the approach of integrating the pet as interactive objects in VR yielded significantly higher participant ratings in perceived realism, joy, multisensory engagement, and connection with their pets in VR.},
  archive      = {J_TVCG},
  author       = {Ningchang Xiong and Qingqin Liu and Kening Zhu},
  doi          = {10.1109/TVCG.2024.3372095},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2559-2569},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PetPresence: Investigating the integration of real-world pet activities in virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ProtoColVR: Requirements gathering and collaborative rapid
prototyping of VR training simulators for multidisciplinary teams.
<em>TVCG</em>, <em>30</em>(5), 2549–2558. (<a
href="https://doi.org/10.1109/TVCG.2024.3372057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present ProtoColVR, a methodology and a plugin designed for gathering requirements and collaborative rapid prototyping of virtual reality training simulators. Our methodology outlines the utilization of current technologies, the involvement of stakeholders during design and development, and the implementation of simulator creation through multiple iterations. We incorporate open-source tools and freely available environments like Twine and Unity to establish a reference implementation for requirements gathering and rapid prototyping. ProtoColVR is the outcome of our collaboration with a hospital and our Navy, and it has undergone testing in a development Jam. From these tests, we have gained valuable insights, including the ability to create functional prototypes within multidisciplinary teams, enhance communication among different roles, and streamline requirements gathering while improving our understanding of the virtualized environment.},
  archive      = {J_TVCG},
  author       = {Vivian Gómez and Pablo Figueroa},
  doi          = {10.1109/TVCG.2024.3372057},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2549-2558},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ProtoColVR: Requirements gathering and collaborative rapid prototyping of VR training simulators for multidisciplinary teams},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Expressive talking avatars. <em>TVCG</em>, <em>30</em>(5),
2538–2548. (<a href="https://doi.org/10.1109/TVCG.2024.3372047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stylized avatars are common virtual representations used in VR to support interaction and communication between remote collaborators. However, explicit expressions are notoriously difficult to create, mainly because most current methods rely on geometric markers and features modeled for human faces, not stylized avatar faces. To cope with the challenge of emotional and expressive generating talking avatars, we build the Emotional Talking Avatar Dataset which is a talking-face video corpus featuring 6 different stylized characters talking with 7 different emotions. Together with the dataset, we also release an emotional talking avatar generation method which enables the manipulation of emotion. We validated the effectiveness of our dataset and our method in generating audio based puppetry examples, including comparisons to state-of-the-art techniques and a user study. Finally, various applications of this method are discussed in the context of animating avatars in VR.},
  archive      = {J_TVCG},
  author       = {Ye Pan and Shuai Tan and Shengran Cheng and Qunfen Lin and Zijiao Zeng and Kenny Mitchell},
  doi          = {10.1109/TVCG.2024.3372047},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2538-2548},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Expressive talking avatars},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time seamless multi-projector displays on deformable
surfaces. <em>TVCG</em>, <em>30</em>(5), 2527–2537. (<a
href="https://doi.org/10.1109/TVCG.2024.3372097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior works on multi-projector displays have focused primarily on static rigid objects, some focusing on dynamic rigid objects. However, works on projection based displays on deformable dynamic objects have focused only on small scale single projector displays. Tracking a deformable dynamic surface and updating projections precisely in real time on it is a significantly challenging task, even for a single projector system. In this paper, we present the first end-to-end solution for achieving a real-time, seamless display on deformable surfaces using mutliple unsychronized projectors without requiring any prior knowledge of the surface or device parameters. The system first accurately calibrates multiple RGB-D cameras and projectors using the deformable display surface itself, and then using those calibrated devices, tracks the continuous changes in the surface shape. Based on the deformation and projector calibration, the system warps and blends the image content in real-time to create a seamless display on a surface that continuously changes shape. Using multiple projectors and RGB-D cameras, we provide the much desired aspect of scale to the displays on deformable surfaces. Most prior dynamic multi-projector systems assume rigid objects and depend critically on the constancy of surface normals and non-existence of local shape deformations. These assumptions break in deformable surfaces making prior techniques inapplicable. Point-based correspondences become inadequate for calibration, exacerbated with no synchronization between the projectors. A few works address non-rigid objects with several restrictions like targeting semi-deformable surfaces (e.g. human face), or using single coaxial (optically aligned) projector-camera pairs, or temporally synchronized cameras. We break loose from such restrictions and handle multiple projector systems for dynamic deformable fabric-like objects using temporally unsynchronized devices. We devise novel methods using ray and plane-based constraints imposed by the pinhole camera model to address these issues and design new blending methods dependent on 3D distances suitable for deformable surfaces. Finally, unlike all prior work with rigid dynamic surfaces that use a single RGB-D camera, we devise a method that involve all RGB-D cameras for tracking since the surface is not seen completely by a single camera. These methods enable a seamless display at scale in the presence of continuous movements and deformations. This work has tremendous applications on mobile and expeditionary systems where environmentals (e.g. wind, vibrations, suction) cannot be avoided. One can create large displays on tent walls in remote, austere military or emergency operations in minutes to support large scale command and control, mission rehearsal or training operations. It can be used to create displays on mobile and inflatable objects for tradeshows/events and touring edutainment applications.},
  archive      = {J_TVCG},
  author       = {Muhammad Twaha Ibrahim and M. Gopi and Aditi Majumder},
  doi          = {10.1109/TVCG.2024.3372097},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2527-2537},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Real-time seamless multi-projector displays on deformable surfaces},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Task and environment-aware virtual scene rearrangement for
enhanced safety in virtual reality. <em>TVCG</em>, <em>30</em>(5),
2517–2526. (<a href="https://doi.org/10.1109/TVCG.2024.3372115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging VR applications have revolutionized user experiences by immersing individuals in digitally crafted environments. However, fully immersive experiences introduce new challenges, notably the risk of physical hazards when users are unaware of their surroundings. Existing solutions, including guardian spaces and locomotion systems, present trade-offs that either disrupt the immersive experience or risk inducing motion sickness. To address these challenges, we propose a novel approach that dynamically rearranges VR scenes according to users&#39; physical spaces, seamlessly embedding physical constraints and interaction tasks into the virtual environment. We design a computational model to optimize the rearranged scene through a cost function, ensuring collision-free interactions while maintaining visual fidelity and the goal of interaction tasks. The experiments demonstrate improvements in user experience and safety, presenting an innovative solution to harmonize physical and virtual environments in VR applications.},
  archive      = {J_TVCG},
  author       = {Bing Ning and Mingtao Pei},
  doi          = {10.1109/TVCG.2024.3372115},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2517-2526},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Task and environment-aware virtual scene rearrangement for enhanced safety in virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust dual-modal speech keyword spotting for XR headsets.
<em>TVCG</em>, <em>30</em>(5), 2507–2516. (<a
href="https://doi.org/10.1109/TVCG.2024.3372092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While speech interaction finds widespread utility within the Extended Reality (XR) domain, conventional vocal speech keyword spotting systems continue to grapple with formidable challenges, including suboptimal performance in noisy environments, impracticality in situations requiring silence, and susceptibility to inadvertent activations when others speak nearby. These challenges, however, can potentially be surmounted through the cost-effective fusion of voice and lip movement information. Consequently, we propose a novel vocal-echoic dual-modal keyword spotting system designed for XR headsets. We devise two different modal fusion approches and conduct experiments to test the system&#39;s performance across diverse scenarios. The results show that our dual-modal system not only consistently outperforms its single-modal counterparts, demonstrating higher precision in both typical and noisy environments, but also excels in accurately identifying silent utterances. Furthermore, we have successfully applied the system in real-time demonstrations, achieving promising results. The code is available at https://github.com/caizhuojiang/VE-KWS.},
  archive      = {J_TVCG},
  author       = {Zhuojiang Cai and Yuhan Ma and Feng Lu},
  doi          = {10.1109/TVCG.2024.3372092},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2507-2516},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Robust dual-modal speech keyword spotting for XR headsets},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Eye-hand typing: Eye gaze assisted finger typing via
bayesian processes in AR. <em>TVCG</em>, <em>30</em>(5), 2496–2506. (<a
href="https://doi.org/10.1109/TVCG.2024.3372106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, AR HMDs are widely used in scenarios such as intelligent manufacturing and digital factories. In a factory environment, fast and accurate text input is crucial for operators&#39; efficiency and task completion quality. However, the traditional AR keyboard may not meet this requirement, and the noisy environment is unsuitable for voice input. In this article, we introduce Eye-Hand Typing, an intelligent AR keyboard. We leverage the speed advantage of eye gaze and use a Bayesian process based on the information of gaze points to infer users&#39; text input intentions. We improve the underlying keyboard algorithm without changing user input habits, thereby improving factory users&#39; text input speed and accuracy. In real-time applications, when the user&#39;s gaze point is on the keyboard, the Bayesian process can predict the most likely characters, vocabulary, or commands that the user will input based on the position and duration of the gaze point and input history. The system can enlarge and highlight recommended text input options based on the predicted results, thereby improving user input efficiency. A user study showed that compared with the current HoloLens 2 system keyboard, Eye-Hand Typing could reduce input error rates by 28.31 % and improve text input speed by 14.5%. It also outperformed a gaze-only technique, being 43.05% more accurate and 39.55% faster. And it was no significant compromise in eye fatigue. Users also showed positive preferences.},
  archive      = {J_TVCG},
  author       = {Yunlei Ren and Yan Zhang and Zhitao Liu and Ning Xie},
  doi          = {10.1109/TVCG.2024.3372106},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2496-2506},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Eye-hand typing: Eye gaze assisted finger typing via bayesian processes in AR},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Measurement of empathy in virtual reality with head-mounted
displays: A systematic review. <em>TVCG</em>, <em>30</em>(5), 2485–2495.
(<a href="https://doi.org/10.1109/TVCG.2024.3372076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a systematic review of 111 papers that measure the impact of virtual experiences created through head-mounted displays (HMDs) on empathy. Our goal was to analyze the conditions and the extent to which virtual reality (VR) enhances empathy. To achieve this, we categorized the relevant literature according to measurement methods, correlated human factors, viewing experiences, topics, and participants. Meta-analysis was performed based on categorized themes, and under specified conditions, we found that VR can improve empathy. Emotional empathy increased temporarily after the VR experience and returned to its original level over time, whereas cognitive empathy remained enhanced. Furthermore, while VR did not surpass 2D video in improving emotional empathy, it did enhance cognitive empathy, which is associated with embodiment. Our results are consistent with existing research suggesting differentiation between cognitive empathy (influenced by environmental factors and learnable) and emotional empathy (highly heritable and less variable). Interactivity, target of empathy, and point of view were not found to significantly affect empathy, but participants&#39; age and nationality were found to influence empathy levels. It can be concluded that VR enhances cognitive empathy by immersing individuals in the perspective of others and that storytelling and personal characteristics are more important than the composition of the VR scene. Our findings provide guiding information for creating empathy content in VR and designing experiments to measure empathy.},
  archive      = {J_TVCG},
  author       = {Yongho Lee and Heesook Shin and Youn-Hee Gil},
  doi          = {10.1109/TVCG.2024.3372076},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2485-2495},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Measurement of empathy in virtual reality with head-mounted displays: A systematic review},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Redirection strategy switching: Selective redirection
controller for dynamic environment adaptation. <em>TVCG</em>,
<em>30</em>(5), 2474–2484. (<a
href="https://doi.org/10.1109/TVCG.2024.3372056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present the Selective Redirection Controller (SRC), which selects the optimal redirection controller based on the physical and virtual environment in Redirected Walking (RDW). The primary advantage of SRC over existing controllers is its dynamic switching among four different redirection controllers (S2C, TAPF, ARC, and SRL) based on the user&#39;s environment, as opposed to using a single fixed controller throughout the experience. By switching between redirection controllers based on the context around the user, SRC aims to optimize the advantages of each redirection strategy. The SRC model is trained using reinforcement learning to dynamically and instantaneously switch redirection controllers based on the user&#39;s environment. We evaluated the performance of SRC against traditional redirection controllers through simulations and user studies conducted in various physical and virtual environments. The findings indicate that SRC reduces the number of resets significantly compared to traditional redirection controllers. Heat map visualization was utilized during the development process to analyze which redirection controller SRC chooses based on the different environments around the user. SRC alternates between redirection techniques based on the user&#39;s environment, maximizing the advantages of each strategy for a superior RDW experience.},
  archive      = {J_TVCG},
  author       = {Ho Jung Lee and Sang-Bin Jeon and Yong-Hun Cho and In-Kwon Lee},
  doi          = {10.1109/TVCG.2024.3372056},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2474-2484},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Redirection strategy switching: Selective redirection controller for dynamic environment adaptation},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). APF-S2T: Steering to target redirection walking based on
artificial potential fields. <em>TVCG</em>, <em>30</em>(5), 2464–2473.
(<a href="https://doi.org/10.1109/TVCG.2024.3372052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Redirected walking (RDW) enables users to walk naturally within a virtual environment that is larger than the physical environment. Recently, several artificial potential field (APF) and alignment-based redirected controllers have been developed and have been demonstrated to significantly outperform conventional controllers. APF Steer-to-Gradient (APF-S2G) and APF Redirected Walking (APF-RDW) utilize the negative gradient and the total force vector, respectively, which are localized to the user&#39;s position. These vectors usually point towards the opposite wall when the user is in corridors, resulting in frequent resets within those regions. This paper introduces the APF Steer-to-Target (APF-S2T), a redirected controller that first finds the target sample point with the lowest score in the user&#39;s walkable area in both physical and virtual environments. The score of a sample point is determined by the APF value at the point and the distance from the user&#39;s position. The direction from the user&#39;s position to the target point is then used as the steering direction for setting RDW gains. We conducted a simulation-based evaluation to compare APF-S2T, APF-S2G, APF-RDW, Visibility Polygon-based alignment (Vis.-Poly.) and Alignment-Optimized controllers in terms of the number of resets and the average distance between resets. The results indicated that APF-S2T significantly outperformed the state-of-the-art controllers.},
  archive      = {J_TVCG},
  author       = {Jun-Jie Chen and Huan-Chang Hung and Yu-Ru Sun and Jung-Hong Chuang},
  doi          = {10.1109/TVCG.2024.3372052},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2464-2473},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {APF-S2T: Steering to target redirection walking based on artificial potential fields},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PreVR: Variable-distance previews for higher-order
disocclusion in VR. <em>TVCG</em>, <em>30</em>(5), 2454–2463. (<a
href="https://doi.org/10.1109/TVCG.2024.3372068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper introduces PreVR, a method for allowing the user of a VR application to preview a virtual environment (VE) around any number of corners. This way the user can gain line of sight to any part of the VE, no matter how distant or how heavily occluded it is. PreVR relies on a multiperspective visualization that implements a higher-order disocclusion effect with piecewise linear rays that bend multiple times as needed to reach the visualization target. PreVR was evaluated in a user study ($\mathrm{N}=88$) that investigates four points on the VR interface design continuum defined by the maximum disocclusion order $\delta$. In a first control condition (CC0), $\delta=0$, corresponds to conventional VR exploration with no preview capability. In a second control condition (CC1), $\delta=1$, corresponds to the prior art approach of giving the user a preview around the first corner. In a first experimental condition (EC3), $\delta=3$, so PreVR provided up to third-order disocclusion. In a second experimental condition (ECN), $\delta$ was not capped, so PreVR could provide a disocclusion effect of any order, as needed to reach any location in the VE. Participants searched for a stationary target, for a dynamic target moving on a random continuous trajectory, and for a transient dynamic target that appeared at random locations in the maze and disappeared 5s later. The study quantified VE exploration efficiency with four metrics: viewpoint translation, view direction rotation, number of teleportations, and task completion time. Results show that the previews afforded by PreVR bring a significant VE exploration efficiency advantage. ECN outperforms EC3, CC1, and CC0 for all metrics and all tasks, and EC3 frequently outperforms CC1 and CC0.},
  archive      = {J_TVCG},
  author       = {Shuqi Liao and Vetria Byrd and Voicu Popescu},
  doi          = {10.1109/TVCG.2024.3372068},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2454-2463},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PreVR: Variable-distance previews for higher-order disocclusion in VR},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial contraction based on velocity variation for natural
walking in virtual reality. <em>TVCG</em>, <em>30</em>(5), 2444–2453.
(<a href="https://doi.org/10.1109/TVCG.2024.3372109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality (VR) offers an immersive 3D digital environment, but enabling natural walking sensations without the constraints of physical space remains a technological challenge. Previous VR locomotion methods, including game controller, teleportation, treadmills, walking-in-place, and redirected walking (RDW), have made strides towards overcoming this challenge. However, these methods also face limitations such as possible unnaturalness, additional hardware requirements, or motion sickness risks. This paper introduces “Spatial Contraction (SC)”, an innovative VR locomotion method inspired by the phenomenon of Lorentz contraction in Special Relativity. Similar to the Lorentz contraction, our SC contracts the virtual space along the user&#39;s velocity direction in response to velocity variation. The virtual space contracts more when the user&#39;s speed is high, whereas minimal or no contraction happens at low speeds. We provide a virtual space transformation method for spatial contraction and optimize the user experience in smoothness and stability. Through SC, VR users can effectively traverse a longer virtual distance with a shorter physical walking. Different from locomotion gains, the spatial contraction effect is observable by the user and aligns with their intentions, so there is no inconsistency between the user&#39;s proprioception and visual perception. SC is a general locomotion method that has no special requirements for VR scenes. The experimental results of our live user studies in various virtual scenarios demonstrate that SC has a significant effect in reducing both the number of resets and the physical walking distance users need to cover. Furthermore, experiments have also demonstrated that SC has the potential for integration with existing locomotion techniques such as RDW.},
  archive      = {J_TVCG},
  author       = {Sen-Zhe Xu and Kui Huang and Cheng-Wei Fan and Song-Hai Zhang},
  doi          = {10.1109/TVCG.2024.3372109},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2444-2453},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Spatial contraction based on velocity variation for natural walking in virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stepping into the right shoes: The effects of user-matched
avatar ethnicity and gender on sense of embodiment in virtual reality.
<em>TVCG</em>, <em>30</em>(5), 2434–2443. (<a
href="https://doi.org/10.1109/TVCG.2024.3372067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many consumer virtual reality (VR) applications, users embody predefined characters that offer minimal customization options, frequently emphasizing storytelling over user choice. We explore whether matching a user&#39;s physical characteristics, specifically ethnicity and gender, with their virtual self-avatar affects their sense of embodiment in VR. We conducted a $2\times 2$ within-subjects experiment ($\mathrm{n}=32$) with a diverse user population to explore the impact of matching or not matching a user&#39;s self-avatar to their ethnicity and gender on their sense of embodiment. Our results indicate that matching the ethnicity of the user and their self-avatar significantly enhances sense of embodiment regardless of gender, extending across various aspects, including appearance, response, and ownership. We also found that matching gender significantly enhanced ownership, suggesting that this aspect is influenced by matching both ethnicity and gender. Interestingly, we found that matching ethnicity specifically affects self-location while matching gender specifically affects one&#39;s body ownership.},
  archive      = {J_TVCG},
  author       = {Tiffany D. Do and Camille Isabella Protko and Ryan P. McMahan},
  doi          = {10.1109/TVCG.2024.3372067},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2434-2443},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Stepping into the right shoes: The effects of user-matched avatar ethnicity and gender on sense of embodiment in virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring bimanual haptic feedback for spatial search in
virtual reality. <em>TVCG</em>, <em>30</em>(5), 2422–2433. (<a
href="https://doi.org/10.1109/TVCG.2024.3372045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial search tasks are common and crucial in many Virtual Reality (VR) applications. Traditional methods to enhance the performance of spatial search often employ sensory cues such as visual, auditory, or haptic feedback. However, the design and use of bimanual haptic feedback with two VR controllers for spatial search in VR remains largely unexplored. In this work, we explored bimanual haptic feedback with various combinations of haptic properties, where four types of bimanual haptic feedback were designed, for spatial search tasks in VR. Two experiments were designed to evaluate the effectiveness of bimanual haptic feedback on spatial direction guidance and search in VR. The results from the first experiment reveal that our proposed bimanual haptic schemes significantly enhanced the recognition of spatial directions in terms of accuracy and speed compared to spatial audio feedback. The second experiment&#39;s findings suggest that the performance of bimanual haptic feedback was comparable to or even better than the visual arrow, especially in reducing the angle of head movement and enhancing searching targets behind the participants, which was supported by subjective feedback as well. Based on these findings, we have derived a set of design recommendations for spatial search using bimanual haptic feedback in VR.},
  archive      = {J_TVCG},
  author       = {BoYu Gao and Tong Shao and Huawei Tu and Qizi Ma and Zitao Liu and Teng Han},
  doi          = {10.1109/TVCG.2024.3372045},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2422-2433},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring bimanual haptic feedback for spatial search in virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Investigating whether the mass of a tool replica influences
virtual training learning outcomes. <em>TVCG</em>, <em>30</em>(5),
2411–2421. (<a href="https://doi.org/10.1109/TVCG.2024.3372041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality (VR) has emerged as a promising solution to address the pressing concern of transferring know-how in the manufacturing industry. Making an immersive training experience often involves designing an instrumented replica of a tool whose use is to be learned through virtual training. The process of making a replica can alter its mass, making it different from that of the original tool. As far as we know, the influence of this difference on learning outcomes has never been evaluated. To investigate this subject, an immersive training experience was designed with pre and post-training phases under real conditions, dedicated to learning the use of a rotary tool. 80 participants took part in this study, split into three groups: a control group performing the virtual training using a replica with the same mass as the original tool ($\mathrm{m}=100\%$), a second group that used a replica with a lighter mass than the original tool ($\mathrm{m}= 50\%$) and a third group using a replica heavier than the original tool ($\mathrm{m}=150\%$). Despite variations in the mass of the replica used for training, this study revealed that the learning outcomes remained comparable across all groups, while also demonstrating significant enhancements in certain performance measures, including task completion time. Overall, these findings provide useful insights regarding the design of tool replicas for immersive training.},
  archive      = {J_TVCG},
  author       = {Julien Cauquis and Etienne Peillard and Lionel Dominjon and Thierry Duval and Guillaume Moreau},
  doi          = {10.1109/TVCG.2024.3372041},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2411-2421},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Investigating whether the mass of a tool replica influences virtual training learning outcomes},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human factors at play: Understanding the impact of
conditioning on presence and reaction time in mixed reality.
<em>TVCG</em>, <em>30</em>(5), 2400–2410. (<a
href="https://doi.org/10.1109/TVCG.2024.3372120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A prerequisite to improving the presence of a user in mixed reality (MR) is the ability to measure and quantify presence. Traditionally, subjective questionnaires have been used to assess the level of presence. However, recent studies have shown that presence is correlated with objective and systemic human performance measures such as reaction time. These studies analyze the correlation between presence and reaction time when technical factors such as object realism and plausibility of the object&#39;s behavior change. However, additional psychological and physiological human factors can also impact presence. It is unclear if presence can be mapped to and correlated with reaction time when human factors such as conditioning are involved. To answer this question, we conducted an exploratory study ($N=60$) where the relationship between presence and reaction time was assessed under three different conditioning scenarios: control, positive, and negative. We demonstrated that human factors impact presence. We found that presence scores and reaction times are significantly correlated (correlation coefficient of −0.64), suggesting that the impact of human factors on reaction time correlates with its effect on presence. In demonstrating that, our study takes another important step toward using objective and systemic measures like reaction time as a presence measure.},
  archive      = {J_TVCG},
  author       = {Yasra Chandio and Victoria Interrante and Fatima M. Anwar},
  doi          = {10.1109/TVCG.2024.3372120},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2400-2410},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Human factors at play: Understanding the impact of conditioning on presence and reaction time in mixed reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Virtual reality self co-embodiment: An alternative to mirror
therapy for post-stroke upper limb rehabilitation. <em>TVCG</em>,
<em>30</em>(5), 2390–2399. (<a
href="https://doi.org/10.1109/TVCG.2024.3372035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Virtual Reality Self Co-embodiment, a new method for post-stroke upper limb rehabilitation. It is inspired by mirror therapy, where the patient&#39;s healthy arm is involved in recovering the affected arm&#39;s motion. By tracking the user&#39;s head, wrists, and fingers&#39; positions, our new approach allows the handicapped arm to control a digital avatar in order to pursue a reaching task. We apply the concept of virtual co-embodiment to use the information from the unaffected arm and complete the affected limb&#39;s impaired motion, which is our added unique feature. This requires users to mechanically involve the incapacitated area as much as possible, prioritizing actual movement rather than the sole imagination of it. As a result, subjects will see a seemingly normally functional virtual arm primarily controlled by their handicapped extremity, but with the constant support of their healthy limb&#39;s motion. Our experiment compares the task execution performance and embodiment perceived when interacting with both mirror therapy and our proposed technique. We found that our approach&#39;s provided sense of ownership is mildly impacted by users&#39; motion planning response times, which mirror therapy does not exhibit. We also observed that mirror therapy&#39;s sense of ownership is moderately affected by the subject&#39;s proficiency while executing the assigned task, which our new method did not display. The results indicate that our proposed method provides similar embodiment and rehabilitation capabilities to those perceived from existing mirror therapy. This experiment was performed in healthy individuals to have an unbiased comparison of how mirror therapy&#39;s and VRSelfCo&#39;s task performance and degree of virtual embodiment compare, but future work explores the possibility of applying this new approach to actual post-stroke patients.},
  archive      = {J_TVCG},
  author       = {Rodrigo Cerecero Curiel and Takuto Nakamura and Hideaki Kuzuoka and Takafumi Kanaya and Cosima Prahm and Keigo Matsumoto},
  doi          = {10.1109/TVCG.2024.3372035},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2390-2399},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Virtual reality self co-embodiment: An alternative to mirror therapy for post-stroke upper limb rehabilitation},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Who says you are so sick? An investigation on individual
susceptibility to cybersickness triggers using EEG, EGG and ECG.
<em>TVCG</em>, <em>30</em>(5), 2379–2389. (<a
href="https://doi.org/10.1109/TVCG.2024.3372066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research paper, we conducted a study to investigate the connection between three objective measures: Electrocardio-gram(EGG), Electrogastrogram (EGG), and Electroencephalogram (EEG), and individuals&#39; susceptibility to cybersickness. Our primary objective was to identify which of these factors plays a central role in causing discomfort when experiencing rotations along three different axes: Roll, Pitch, and Yaw. This study involved 35 participants who were tasked with destroying asteroids using their eye gaze while undergoing passive rotations in four separate sessions. The results, when combined with subjective measurements (specifically, Fast motion sickness questionnaire (FMS) and Simulator sickness questionnaire (SSQ) score), demonstrated that EGG measurements were superior in detecting symptoms associated with nausea. As for ECG measurements, our observations did reveal significant changes in Heart Rate Variability (HRV) parameters. However, we caution against relying solely on ECG as a dependable indicator for assessing the extent of cybersickness. Most notably, EEG signals emerged as a crucial resource for discerning individual differences related to these rotational axes. Our findings were significant not only in the context of periodic activities but also underscored the potential of aperiodic activities in detecting the severity of cybersickness and an individual&#39;s susceptibility to rotational triggers.},
  archive      = {J_TVCG},
  author       = {Nana Tian and Ronan Boulic},
  doi          = {10.1109/TVCG.2024.3372066},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2379-2389},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Who says you are so sick? an investigation on individual susceptibility to cybersickness triggers using EEG, EGG and ECG},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Investigating personalization techniques for improved
cybersickness prediction in virtual reality environments. <em>TVCG</em>,
<em>30</em>(5), 2368–2378. (<a
href="https://doi.org/10.1109/TVCG.2024.3372122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent cybersickness research, there has been a growing interest in predicting cybersickness using real-time physiological data such as heart rate, galvanic skin response, eye tracking, postural sway, and electroencephalogram. However, the impact of individual factors such as age and gender, which are pivotal in determining cybersickness susceptibility, remains unknown in predictive models. Our research seeks to address this gap, underscoring the necessity for a more personalized approach to cybersickness prediction to ensure a better, more inclusive virtual reality experience. We hypothesize that a personalized cybersickness prediction model would outperform non-personalized models in predicting cybersickness. Evaluating this, we explored four personalization techniques: 1) data grouping, 2) transfer learning, 3) early shaping, and 4) sample weighing using an open-source cybersickness dataset. Our empirical results indicate that personalized models significantly improve prediction accuracy. For instance, with early shaping, the Deep Temporal Convolutional Neural Network (DeepTCN) model achieved a 69.7% reduction in RMSE compared to its non-personalized version. Our study provides evidence of personalization techniques&#39; benefits in improving cybersickness prediction. These findings have implications for developing personalized cybersickness prediction models tailored to individual differences, which can be used to develop personalized cybersickness reduction techniques in the future.},
  archive      = {J_TVCG},
  author       = {Umama Tasnim and Rifatul Islam and Kevin Desai and John Quarles},
  doi          = {10.1109/TVCG.2024.3372122},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2368-2378},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Investigating personalization techniques for improved cybersickness prediction in virtual reality environments},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Handwriting for text input and the impact of XR displays,
surface alignments, and sentence complexities. <em>TVCG</em>,
<em>30</em>(5), 2357–2367. (<a
href="https://doi.org/10.1109/TVCG.2024.3372124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text input is desirable across various eXtended Reality (XR) use cases and is particularly crucial for knowledge and office work. This article compares handwriting text input between Virtual Reality (VR) and Video See-Through Augmented Reality (VST AR), facilitated by physically aligned and mid-air surfaces when writing simple and complex sentences. In a $2\times 2\times 2$ experimental design, 72 participants performed two ten-minute handwriting sessions, each including ten simple and ten complex sentences representing text input in real-world scenarios. Our developed handwriting application supports different XR displays, surface alignments, and handwriting recognition based on digital ink. We evaluated usability, user experience, task load, text input performance, and handwriting style. Our results indicate high usability with a successful transfer of handwriting skills to the virtual domain. XR displays and surface alignments did not impact text input speed and error rate. However, sentence complexities did, with participants achieving higher input speeds and fewer errors for simple sentences (17.85 WPM, 0.51% MSD ER) than complex sentences (15.07 WPM, 1.74% MSD ER). Handwriting on physically aligned surfaces showed higher learnability and lower physical demand, making them more suitable for prolonged handwriting sessions. Handwriting on mid-air surfaces yielded higher novelty and stimulation ratings, which might diminish with more experience. Surface alignments and sentence complexities significantly affected handwriting style, leading to enlarged and more connected cursive writing in both mid-air and for simple sentences. The study also demonstrated the benefits of using XR controllers in a pen-like posture to mimic styluses and pressure-sensitive tips on physical surfaces for input detection. We additionally provide a phrase set of simple and complex sentences as a basis for future text input studies, which can be expanded and adapted.},
  archive      = {J_TVCG},
  author       = {Florian Kern and Jonathan Tschanter and Marc Erich Latoschik},
  doi          = {10.1109/TVCG.2024.3372124},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2357-2367},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Handwriting for text input and the impact of XR displays, surface alignments, and sentence complexities},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ViComp: Video compensation for projector-camera systems.
<em>TVCG</em>, <em>30</em>(5), 2347–2356. (<a
href="https://doi.org/10.1109/TVCG.2024.3372079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Projector video compensation aims to cancel the geometric and photometric distortions caused by non-ideal projection surfaces and environments when projecting videos. Most existing projector compensation methods start by projecting and capturing a set of sampling images, followed by an offline compensation model training step. Thus, abundant user effort is required before the users can watch the video. Moreover, the sampling images have little prior knowledge of the video content and may lead to suboptimal results. To address these issues, this paper builds a video compensation system that can online adapt the compensation parameters. Our approach consists of five threads and can perform compensation, projection, capturing, and short-term and long-term model updates in parallel. Due to the parallel mechanism, rather than projecting and capturing hundreds of sampling images and training the model offline, we can directly use the projected and captured video frames for model updates on the fly. To quickly apply to the new environment, we introduce a deep learning-based compensation model that integrates a fixed transformer-based method and a novel CNN-based network. Moreover, for fast convergence and to reduce error accumulation during fine-tuning, we present a strategy that cooperates with short-term and long-term memory model updates. Experiments show that it significantly outperforms state-of-the-art baselines.},
  archive      = {J_TVCG},
  author       = {Yuxi Wang and Haibin Ling and Bingyao Huang},
  doi          = {10.1109/TVCG.2024.3372079},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2347-2356},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ViComp: Video compensation for projector-camera systems},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CloVR: Fast-startup low-latency cloud VR. <em>TVCG</em>,
<em>30</em>(5), 2337–2346. (<a
href="https://doi.org/10.1109/TVCG.2024.3372059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {VR headsets have limited rendering capability, which limits the size and detail of the virtual environment (VE) that can be used in VR applications. One solution is cloud VR, where the “thin” VR clients are assisted by a server. This paper describes Cio VR, a cloud VR system that provides fast loading times, as needed to let users see and interact with the VE quickly at session startup or after teleportation. The server reduces the original VE to a compact representation through near-far partitioning. The server renders the far region to an environment map which it sends to the client together with the near region geometry, from which the client renders quality frames locally, with low latency. The near region starts out small and grows progressively, with strict visual continuity, minimizing startup time. The low-latency and fast-startup advantages of CloVR have been validated in a user study where groups of 8 participants wearing all-in-one VR headsets (Quest 2&#39;s) were supported by a laptop server to run a collaborative VR application with a 25 million triangle VE.},
  archive      = {J_TVCG},
  author       = {Yuqi Zhou and Voicu Popescu},
  doi          = {10.1109/TVCG.2024.3372059},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2337-2346},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CloVR: Fast-startup low-latency cloud VR},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VR.net: A real-world dataset for virtual reality motion
sickness research. <em>TVCG</em>, <em>30</em>(5), 2330–2336. (<a
href="https://doi.org/10.1109/TVCG.2024.3372044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers have used machine learning approaches to identify motion sickness in VR experience. These approaches would certainly benefit from an accurately labeled, real-world, diverse dataset that enables the development of generalizable ML models. We introduce ‘VR.net’, a dataset comprising 165-hour gameplay videos from 100 real-world games spanning ten diverse genres, evaluated by 500 participants. VR.net accurately assigns 24 motion sickness-related labels for each video frame, such as camera/object movement, depth of field, and motion flow. Building such a dataset is challenging since manual labeling would require an infeasible amount of time. Instead, we implement a tool to automatically and precisely extract ground truth data from 3D engines&#39; rendering pipelines without accessing VR games&#39; source code. We illustrate the utility of VR.net through several applications, such as risk factor detection and sickness level prediction. We believe that the scale, accuracy, and diversity of VR.net can offer unparalleled opportunities for VR motion sickness research and beyond.We also provide access to our data collection tool, enabling researchers to contribute to the expansion of VR.net.},
  archive      = {J_TVCG},
  author       = {Elliott Wen and Chitralekha Gupta and Prasanth Sasikumar and Mark Billinghurst and James Wilmott and Emily Skow and Arindam Dey and Suranga Nanayakkara},
  doi          = {10.1109/TVCG.2024.3372044},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2330-2336},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VR.net: A real-world dataset for virtual reality motion sickness research},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Instant segmentation and fitting of excavations in
subsurface utility engineering. <em>TVCG</em>, <em>30</em>(5),
2319–2329. (<a href="https://doi.org/10.1109/TVCG.2024.3372064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using augmented reality for subsurface utility engineering (SUE) has benefited from recent advances in sensing hardware, enabling the first practical and commercial applications. However, this progress has uncovered a latent problem – the insufficient quality of existing SUE data in terms of completeness and accuracy. In this work, we present a novel approach to automate the process of aligning existing SUE databases with measurements taken during excavation works, with the potential to correct the deviation from the as-planned to as-built documentation, which is still a big challenge for traditional workers at sight. Our segmentation algorithm performs infrastructure segmentation based on the live capture of an excavation on site. Our fitting approach correlates the inferred position and orientation with the existing digital plan and registers the as-planned model into the as-built state. Our approach is the first to circumvent tedious postprocessing, as it corrects data online and on-site. In our experiments, we show the results of our proposed method on both synthetic data and a set of real excavations.},
  archive      = {J_TVCG},
  author       = {Marco Stranner and Philipp Fleck and Dieter Schmalstieg and Clemens Arth},
  doi          = {10.1109/TVCG.2024.3372064},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2319-2329},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Instant segmentation and fitting of excavations in subsurface utility engineering},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards co-operative beaming displays: Dual steering
projectors for extended projection volume and head orientation range.
<em>TVCG</em>, <em>30</em>(5), 2309–2318. (<a
href="https://doi.org/10.1109/TVCG.2024.3372118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing near-eye displays (NEDs) have trade-offs related to size, weight, computational resources, battery life, and body temperature. A recent paradigm, beaming display, addresses these trade-offs by separating the NED into a steering projector (SP) for image presentation and a passive headset worn by the user. However, the beaming display has issues with the projection area of a single SP and has severe limitations on the head orientation and pose that the user can move. In this study, we distribute dual steering projectors in the scene to extend the head orientation and pose of the beaming display by coordinating the dual projections on a passive headset. For cooperative control of each SP, we define a geometric model of the SPs and propose a calibration and projection control method designed for multiple projectors. We present implementations of the system along with evaluations showing that the precision and delay are 1.8 ∼ 5.7 mm and 14.46 ms, respectively, at a distance of about 1 m from the SPs. From this result, our prototype with multiple SPs can project images in the projection area ($20\ \text{mm} \times 30\ \text{mm}$) of the passive headset while extending the projectable head orientation. Furthermore, as applications of cooperative control by multiple SPs, we show the possibility of multiple users, improving dynamic range and binocular presentation.},
  archive      = {J_TVCG},
  author       = {Hiroto Aoki and Takumi Tochimoto and Yuichi Hiroi and Yuta Itoh},
  doi          = {10.1109/TVCG.2024.3372118},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2309-2318},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Towards co-operative beaming displays: Dual steering projectors for extended projection volume and head orientation range},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Try this for size: Multi-scale teleportation in immersive
virtual reality. <em>TVCG</em>, <em>30</em>(5), 2298–2308. (<a
href="https://doi.org/10.1109/TVCG.2024.3372043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability of a user to adjust their own scale while traveling through virtual environments enables them to inspect tiny features being ant-sized and to gain an overview of the surroundings as a giant. While prior work has almost exclusively focused on steering-based interfaces for multi-scale travel, we present three novel teleportation-based techniques that avoid continuous motion flow to reduce the risk of cybersickness. Our approaches build on the extension of known teleportation workflows and suggest specifying scale adjustments either simultaneously with, as a connected second step after, or separately from the user&#39;s new horizontal position. The results of a two-part user study with 30 participants indicate that the simultaneous and connected specification paradigms are both suitable candidates for effective and comfortable multi-scale teleportation with nuanced individual benefits. Scale specification as a separate mode, on the other hand, was considered less beneficial. We compare our findings to prior research and publish the executable of our user study to facilitate replication and further analyses.},
  archive      = {J_TVCG},
  author       = {Tim Weissker and Matthis Franzgrote and Torsten Kuhlen},
  doi          = {10.1109/TVCG.2024.3372043},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2298-2308},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Try this for size: Multi-scale teleportation in immersive virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). “May i speak?”: Multi-modal attention guidance in social VR
group conversations. <em>TVCG</em>, <em>30</em>(5), 2287–2297. (<a
href="https://doi.org/10.1109/TVCG.2024.3372119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel multi-modal attention guidance method designed to address the challenges of turn-taking dynamics in meetings and enhance group conversations within virtual reality (VR) environments. Recognizing the difficulties posed by a confined field of view and the absence of detailed gesture tracking in VR, our proposed method aims to mitigate the challenges of noticing new speakers attempting to join the conversation. This approach tailors attention guidance, providing a nuanced experience for highly engaged participants while offering subtler cues for those less engaged, thereby enriching the overall meeting dynamics. Through group interview studies, we gathered insights to guide our design, resulting in a prototype that employs light as a diegetic guidance mechanism, complemented by spatial audio. The combination creates an intuitive and immersive meeting environment, effectively directing users&#39; attention to new speakers. An evaluation study, comparing our method to state-of-the-art attention guidance approaches, demonstrated significantly faster response times ($p &lt; 0.001$), heightened perceived conversation satisfaction ($p &lt; 0.001$), and preference ($p &lt; 0.001$) for our method. Our findings contribute to the understanding of design implications for VR social attention guidance, opening avenues for future research and development.},
  archive      = {J_TVCG},
  author       = {Geonsun Lee and Dae Yeol Lee and Guan-Ming Su and Dinesh Manocha},
  doi          = {10.1109/TVCG.2024.3372119},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2287-2297},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {“May i speak?”: Multi-modal attention guidance in social VR group conversations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RedirectedDoors+: Door-opening redirection with dynamic
haptics in room-scale VR. <em>TVCG</em>, <em>30</em>(5), 2276–2286. (<a
href="https://doi.org/10.1109/TVCG.2024.3372105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RedirectedDoors is a visuo-haptic door-opening redirection technique in VR, and it has shown promise in its ability to efficiently compress the physical space required for a room-scale VR experience. However, its previous implementation has only supported laboratory experiments with a single door opening at a fixed location. To significantly expand this technique for room-scale VR, we have developed RedirectedDoors+, a robot-based system that permits consecutive door-opening redirection with haptics. Specifically, our system is mainly achieved with the use of three components: (1) door robots, a small number of wheeled robots equipped with a doorknob-like prop, (2) a robot-positioning algorithm that arbitrarily positions the door robots to provide the user with just-in-time haptic feedback during door opening, and (3) a user-steering algorithm that determines the redirection gain for every instance of door opening to keep the user away from the boundary of the play area. Results of simulated VR exploration in six virtual environments reveal our system&#39;s performance relative to user walking speed, paths, and number of door robots, from which we derive its usage guidelines. We then conduct a user study ($N=12$) in which participants experience a walkthrough application using the actual system. The results demonstrate that the system is able to provide haptic feedback while redirecting the user within a limited play area.},
  archive      = {J_TVCG},
  author       = {Yukai Hoshikawa and Kazuyuki Fujita and Kazuki Takashima and Morten Fjeld and Yoshifumi Kitamura},
  doi          = {10.1109/TVCG.2024.3372105},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2276-2286},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {RedirectedDoors+: Door-opening redirection with dynamic haptics in room-scale VR},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Molecular docking improved with human spatial perception
using virtual reality. <em>TVCG</em>, <em>30</em>(5), 2269–2275. (<a
href="https://doi.org/10.1109/TVCG.2024.3372128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive steered molecular dynamics (ASMD) is a computational biophysics method in which an external force is applied to a selected set of atoms or a specific reaction coordinate to induce a particular molecular motion. Virtual reality (VR) based methods for protein-ligand docking are beneficial for visualizing on-the-fly interactive molecular dynamics and performing promising docking trajectories. In this paper, we propose a novel method to guide ASMD with optimal trajectories collected from human experiences using interactive molecular dynamics in virtual reality (iMD-VR). We also explain the benefits of using VR as a tool for expediting the process of ligand binding, outlining an experimental protocol that enables iMD-VR users to guide Amprenavir into and out of the binding pockets of HIV-1 protease and recreate their respective crystallographic binding poses within 5 minutes. Later, we discuss our analysis of the results from iMD-VR-assisted ASMD simulation and assess its performance compared to a standard ASMD simulation. From the accuracy point of view, our proposed method calculates higher Potential Mean Force (PMF) values consistently relative to a standard ASMD simulation with an almost twofold increase in all the experiments. Finally, we describe the novelty of the research and discuss results showcasing a faster and more effective convergence of the ligand to the protein&#39;s binding site as compared to a standard molecular dynamics simulation, proving the effectiveness of VR in the field of drug discovery. Future work includes the development of an artificial intelligence algorithm capable of predicting optimal binding trajectories for many protein-ligand pairs, as well as the required force needed to steer the ligand to follow the said trajectory.},
  archive      = {J_TVCG},
  author       = {Shivam Mishra and Missael Corro-Flores and David Krum and Negin Forouzesh},
  doi          = {10.1109/TVCG.2024.3372128},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2269-2275},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Molecular docking improved with human spatial perception using virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy-preserving gaze data streaming in immersive
interactive virtual reality: Robustness and user experience.
<em>TVCG</em>, <em>30</em>(5), 2257–2268. (<a
href="https://doi.org/10.1109/TVCG.2024.3372032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eye tracking is routinely being incorporated into virtual reality (VR) systems. Prior research has shown that eye tracking data, if exposed, can be used for re-identification attacks [14]. The state of our knowledge about currently existing privacy mechanisms is limited to privacy-utility trade-off curves based on data-centric metrics of utility, such as prediction error, and black-box threat models. We propose that for interactive VR applications, it is essential to consider user-centric notions of utility and a variety of threat models. We develop a methodology to evaluate real-time privacy mechanisms for interactive VR applications that incorporate subjective user experience and task performance metrics. We evaluate selected privacy mechanisms using this methodology and find that re-identification accuracy can be decreased to as low as 14% while maintaining a high usability score and reasonable task performance. Finally, we elucidate three threat scenarios (black-box, black-box with exemplars, and white-box) and assess how well the different privacy mechanisms hold up to these adversarial scenarios. This work advances the state of the art in VR privacy by providing a methodology for end-to-end assessment of the risk of re-identification attacks and potential mitigating solutions. f},
  archive      = {J_TVCG},
  author       = {Ethan Wilson and Azim Ibragimov and Michael J. Proulx and Sai Deep Tetali and Kevin Butler and Eakta Jain},
  doi          = {10.1109/TVCG.2024.3372032},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2257-2268},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Privacy-preserving gaze data streaming in immersive interactive virtual reality: Robustness and user experience},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MusiKeys: Exploring haptic-to-auditory sensory substitution
to improve mid-air text-entry. <em>TVCG</em>, <em>30</em>(5), 2247–2256.
(<a href="https://doi.org/10.1109/TVCG.2024.3372065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical QWERTY keyboards are the current standard for performing precision text-entry with extended reality devices. Ideally, there would exist a comparable, self-contained solution that works anywhere, without requiring external keyboards. Unfortunately, when physical keyboards are recreated virtually, we currently lose critical haptic feedback information from the sense of touch, which impedes typing. In this paper, we introduce the MusiKeys Technique, which uses auditory feedback in virtual reality to communicate missing haptic feedback information typists normally receive when using a physical keyboard. To examine this concept, we conducted a user study with 24 participants which encompassed four mid-air virtual keyboards augmented with increasing amounts of feedback information, along with a fifth physical keyboard for reference. Results suggest that providing clicking feedback on key-press and key-release improves typing performance compared to not providing auditory feedback, which is consistent with the literature. We also found that audio can serve as a substitute for information contained in haptic feedback, in that users can accurately perceive the presented information. However, under our specific study conditions, this awareness of the feedback information did not yield significant differences in typing performance. Our results suggest this kind of feedback replacement can be perceived by users but needs more research to tune and improve the specific techniques.},
  archive      = {J_TVCG},
  author       = {Alexander Krasner and Joseph Gabbard},
  doi          = {10.1109/TVCG.2024.3372065},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2247-2256},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MusiKeys: Exploring haptic-to-auditory sensory substitution to improve mid-air text-entry},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Berkeley open extended reality recordings 2023 (BOXRR-23):
4.7 million motion capture recordings from 105,000 XR users.
<em>TVCG</em>, <em>30</em>(5), 2239–2246. (<a
href="https://doi.org/10.1109/TVCG.2024.3372087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extended reality (XR) devices such as the Meta Quest and Apple Vision Pro have seen a recent surge in attention, with motion tracking “telemetry” data lying at the core of nearly all XR and metaverse experiences. Researchers are just beginning to understand the implications of this data for security, privacy, usability, and more, but currently lack large-scale human motion datasets to study. The BOXRR-23 dataset contains 4,717,215 motion capture recordings, voluntarily submitted by 105,852 XR device users from over 50 countries. BOXRR-23 is over 200 times larger than the largest existing motion capture research dataset and uses a new, highly efficient and purpose-built XR Open Recording (XROR) file format.},
  archive      = {J_TVCG},
  author       = {Vivek Nair and Wenbo Guo and Rui Wang and James F. O&#39;Brien and Louis Rosenberg and Dawn Song},
  doi          = {10.1109/TVCG.2024.3372087},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2239-2246},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Berkeley open extended reality recordings 2023 (BOXRR-23): 4.7 million motion capture recordings from 105,000 XR users},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low-latency ocular parallax rendering and investigation of
its effect on depth perception in virtual reality. <em>TVCG</em>,
<em>30</em>(5), 2228–2238. (<a
href="https://doi.org/10.1109/TVCG.2024.3372078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With a demand for an immersive experience in virtual/augmented reality (VR/AR) displays, recent efforts have incorporated eye states, such as focus and fixation, into display graphics. Among these, ocular parallax, a small parallax generated by eye rotation, has received considerable attention for its impact on depth perception. However, the substantial latency of head-mounted displays (HMDs) has made it challenging to accurately assess its true effect during free eye movements. To address this issue, we propose a high-speed (360 Hz) and low-latency (4.8 ms) ocular parallax rendering system with a custom-built eye tracker. Using this proposed system, we conducted an investigation to determine the latency requirements necessary for achieving perceptually stable ocular parallax rendering. Our findings indicate that, in binocular viewing, ocular parallax rendering is perceived as significantly less stable than conventional rendering when the latency exceeds 43.72 ms at 1.3 D and 21.50 ms at 2.0 D. We also evaluated the effects of ocular parallax rendering on binocular fusion and monocular depth perception under free viewing conditions. The results demonstrated that ocular parallax rendering can enhance binocular fusion but has a limited impact on depth perception under monocular viewing conditions when latency is minimized.},
  archive      = {J_TVCG},
  author       = {Yuri Mikawa and Taiki Fukiage},
  doi          = {10.1109/TVCG.2024.3372078},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2228-2238},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Low-latency ocular parallax rendering and investigation of its effect on depth perception in virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Projection mapping with a brightly lit surrounding using a
mixed light field approach. <em>TVCG</em>, <em>30</em>(5), 2217–2227.
(<a href="https://doi.org/10.1109/TVCG.2024.3372132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Projection mapping (PM) exhibits suboptimal performance in well-lit environments because of the interference caused by ambient light. This interference degrades the contrast of the projected images. Consequently, conventional methodologies restrict the application of PM to dimly lit settings, leading to an unnatural visual experience, as only the PM target is prominently illuminated. To overcome these limitations, we introduce an innovative approach that leverages a mixed light field, blending traditional PM with ray-controllable ambient lighting. This methodological combination, despite its simplicity, is effective because it ensures that the projector exclusively illuminates the PM target, preserving the optimal contrast. Precise control of ambient light rays is essential to prevent them from illuminating the PM target while adequately illuminating the surrounding environment. Furthermore, we propose the integration of a kaleidoscopic array with integral photography to generate dense light fields for ray-controllable ambient lighting. Additionally, we present an efficient binary-search-based calibration method tailored to this intricate optical system. Our optical simulations and the developed system collectively validate the effectiveness of our approach. Our results show that PM targets and ordinary objects coexist naturally in environments that are brightly lit as a result of our method, enhancing the overall visual experience.},
  archive      = {J_TVCG},
  author       = {Masahiko Yasui and Ryota Iwataki and Masatoshi Ishikawa and Yoshihiro Watanabe},
  doi          = {10.1109/TVCG.2024.3372132},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2217-2227},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Projection mapping with a brightly lit surrounding using a mixed light field approach},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring the influence of virtual avatar heads in mixed
reality on social presence, performance and user experience in
collaborative tasks. <em>TVCG</em>, <em>30</em>(5), 2206–2216. (<a
href="https://doi.org/10.1109/TVCG.2024.3372051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Mixed Reality (MR), users&#39; heads are largely (if not completely) occluded by the MR Head-Mounted Display (HMD) they are wearing. As a consequence, one cannot see their facial expressions and other communication cues when interacting locally. In this paper, we investigate how displaying virtual avatars&#39; heads on-top of the (HMD-occluded) heads of participants in a Video See-Through (VST) Mixed Reality local collaborative task could improve their collaboration as well as social presence. We hypothesized that virtual heads would convey more communicative cues (such as eye direction or facial expressions) hidden by the MR HMDs and lead to better collaboration and social presence. To do so, we conducted a between-subject study ($\mathrm{n}=88$) with two independent variables: the type of avatar (CartoonAvatar/RealisticAvatar/NoAvatar) and the level of facial expressions provided (HighExpr/LowExpr). The experiment involved two dyadic communication tasks: (i) the “20-question” game where one participant asks questions to guess a hidden word known by the other participant and (ii) a urban planning problem where participants have to solve a puzzle by collaborating. Each pair of participants performed both tasks using a specific type of avatar and facial animation. Our results indicate that while adding an avatar&#39;s head does not necessarily improve social presence, the amount of facial expressions provided through the social interaction does have an impact. Moreover, participants rated their performance higher when observing a realistic avatar but rated the cartoon avatars as less uncanny. Taken together, our results contribute to a better understanding of the role of partial avatars in local MR collaboration and pave the way for further research exploring collaboration in different scenarios, with different avatar types or MR setups.},
  archive      = {J_TVCG},
  author       = {Theo Combe and Rebecca Fribourg and Lucas Detto and Jean-Marie Norm},
  doi          = {10.1109/TVCG.2024.3372051},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2206-2216},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring the influence of virtual avatar heads in mixed reality on social presence, performance and user experience in collaborative tasks},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research trends in virtual reality music concert technology:
A systematic literature review. <em>TVCG</em>, <em>30</em>(5),
2195–2205. (<a href="https://doi.org/10.1109/TVCG.2024.3372069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in virtual reality (VR) technology have sparked novel avenues of growth in the musical domain. Following the COVID-19 pandemic, the rise of VR technology has led to growing interest in VR music concerts as an alternative to traditional live concerts. These virtual settings can provide immersion like attending real concerts for physically distant audiences and performers, and also can offer new creative possibilities. VR music concert research is still in its infancy, and advances in technologies such as multimodal devices are rapidly expanding the diversity of research, requiring a unified understanding of the field. To identify trends in VR music concert technology, we conducted a PRISMA-based systematic literature review covering the period from 2018 to 2023. After a thorough screening process, a total of 27 papers were selected for review. The studies were classified and analyzed based on the research topic (audience, performer, concert venue), interaction type (user-environment, user-user), and hardware used (head-mounted display, additional hardware). Furthermore, we categorized the evaluation metrics into user experience, usability, and performance. Our review contributes to advancing the understanding of recent developments in VR music concert technology, shedding light on the diversification and potential of this emerging field.},
  archive      = {J_TVCG},
  author       = {Jieun Park and Youjin Choi and Kyung Myun Lee},
  doi          = {10.1109/TVCG.2024.3372069},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2195-2205},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Research trends in virtual reality music concert technology: A systematic literature review},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Locomotion techniques for dynamic environments: Effects on
spatial knowledge and user experiences. <em>TVCG</em>, <em>30</em>(5),
2184–2194. (<a href="https://doi.org/10.1109/TVCG.2024.3372074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various locomotion techniques are used to navigate and find way through space in virtual environments (VE), and each technique provides different experiences and performances to users. Previous studies have primarily focused on static environments, whereas there is a need for research from a different perspective of dynamic environments because there are many moving objects in VE, such as other users. In this study, we compare the effects of different locomotion techniques on the user&#39;s spatial knowledge and experience, depending on whether the virtual objects are moving or not. The investigated locomotion techniques include joystick, teleportation, and redirected walking (RDW), all commonly used for VR navigation. The results showed that the differences in spatial knowledge and user experience provided by different locomotion techniques can vary depending on whether the environment is static or dynamic. Our results also showed that for a given VE, there are different locomotion techniques that induce fewer collisions between the user and other objects, or reduce the time it takes the user to perform a given task. This study suggests that when designing a locomotion interface for a specific VR application, it is possible to improve the user&#39;s spatial knowledge and experience by recommending different locomotion techniques depending on the degree of environment dynamism and and type of task.},
  archive      = {J_TVCG},
  author       = {Hyunjeong Kim and Sang-Bin Jeon and In-Kwon Lee},
  doi          = {10.1109/TVCG.2024.3372074},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2184-2194},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Locomotion techniques for dynamic environments: Effects on spatial knowledge and user experiences},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Corr-track: Category-level 6D pose tracking with
soft-correspondence matrix estimation. <em>TVCG</em>, <em>30</em>(5),
2173–2183. (<a href="https://doi.org/10.1109/TVCG.2024.3372111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Category-level pose tracking methods can continuously track the pose of objects without requiring any prior knowledge of the specific shape of the tracked instance. This makes them advantageous in augmented reality and virtual reality applications. The key challenge is how to train neural networks to accurately predict the poses of objects they have never seen before and exhibit strong generalization performance. We propose a novel category-level 6D pose tracking method Corr-Track, which is capable of accurately tracking objects belonging to the same category from depth video streams. Our approach utilizes direct soft correspondence constraints to train a neural network, which estimates bidirectional soft correspondences between sparsely sampled point clouds of objects in two frames. We first introduce a soft correspondence matrix for pose tracking tasks and establish effective constraints through direct spatial point-to-point correspondence representations in the sparse point cloud correspondence matrix. We propose the “point cloud expansion” strategy to address the “point cloud shrinkage” problem resulting from soft correspondences. This strategy ensures that the corresponding point cloud accurately reproduces the shape of the target point cloud, leading to precise pose tracking results. We evaluated our approach on the NOCS-REAL275 and Wild6D dataset and observed superior performance compared to previous methods. Additionally, we conducted cross-category experiments that further demonstrated its generalization capability.},
  archive      = {J_TVCG},
  author       = {Xin Cao and Jia Li and Panpan Zhao and Jiachen Li and Xueying Qin},
  doi          = {10.1109/TVCG.2024.3372111},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2173-2183},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Corr-track: Category-level 6D pose tracking with soft-correspondence matrix estimation},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The utilitarian virtual self – using embodied personalized
avatars to investigate moral decision-making in semi-autonomous vehicle
dilemmas. <em>TVCG</em>, <em>30</em>(5), 2162–2172. (<a
href="https://doi.org/10.1109/TVCG.2024.3372121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embodied personalized avatars are a promising new tool to investigate moral decision-making by transposing the user into the “middle of the action” in moral dilemmas. Here, we tested whether avatar personalization and motor control could impact moral decision-making, physiological reactions and reaction times, as well as embodiment, presence and avatar perception. Seventeen participants, who had their personalized avatars created in a previous study, took part in a range of incongruent (i.e., harmful action led to better overall outcomes) and congruent (i.e., harmful action led to trivial outcomes) moral dilemmas as the drivers of a semi-autonomous car. They embodied four different avatars (counterbalanced - personalized motor control, personalized no motor control, generic motor control, generic no motor control). Overall, participants took a utilitarian approach by performing harmful actions only to maximize outcomes. We found increased physiological arousal (SCRs and heart rate) for personalized avatars compared to generic avatars, and increased SCRs in motor control conditions compared to no motor control. Participants had slower reaction times when they had motor control over their avatars, possibly hinting at more elaborate decision-making processes. Presence was also higher in motor control compared to no motor control conditions. Embodiment ratings were higher for personalized avatars, and generally, personalization and motor control were perceptually positive features. These findings highlight the utility of personalized avatars and open up a range of future research possibilities that could benefit from the affordances of this technology and simulate, more closely than ever, real-life action.},
  archive      = {J_TVCG},
  author       = {Anca Salagean and Michelle Wu and George Fletcher and Darren Cosker and Danaë Stanton Fraser},
  doi          = {10.1109/TVCG.2024.3372121},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2162-2172},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The utilitarian virtual self – using embodied personalized avatars to investigate moral decision-making in semi-autonomous vehicle dilemmas},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Projection mapping under environmental lighting by replacing
room lights with heterogeneous projectors. <em>TVCG</em>,
<em>30</em>(5), 2151–2161. (<a
href="https://doi.org/10.1109/TVCG.2024.3372031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Projection mapping (PM) is a technique that enhances the appearance of real-world surfaces using projected images, enabling multiple people to view augmentations simultaneously, thereby facilitating communication and collaboration. However, PM typically requires a dark environment to achieve high-quality projections, limiting its practicality. In this paper, we overcome this limitation by replacing conventional room lighting with heterogeneous projectors. These projectors replicate environmental lighting by selectively illuminating the scene, excluding the projection target. Our contributions include a distributed projector optimization framework designed to effectively replicate environmental lighting and the incorporation of a large-aperture projector, in addition to standard projectors, to reduce high-luminance emitted rays and hard shadows-undesirable factors for collaborative tasks in PM. We conducted a series of quantitative and qualitative experiments, including user studies, to validate our approach. Our findings demonstrate t hat our projector-based lighting system significantly enhancesthe contrast and realism of PM results even under e nvironmental lighting compared to typical lights. Furthermore, our method facilitates a substantial shift in the perceived color mode from the undesirable aperture-color mode, where observers perceive the projected object as self-luminous, to the surface-color mode in PM.},
  archive      = {J_TVCG},
  author       = {Masaki Takeuchi and Hiroki Kusuyama and Daisuke Iwai and Kosuke Sato},
  doi          = {10.1109/TVCG.2024.3372031},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2151-2161},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Projection mapping under environmental lighting by replacing room lights with heterogeneous projectors},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessing depth perception in VR and video see-through AR: A
comparison on distance judgment, performance, and preference.
<em>TVCG</em>, <em>30</em>(5), 2140–2150. (<a
href="https://doi.org/10.1109/TVCG.2024.3372061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial User Interfaces along the Reality-Virtuality continuum heavily depend on accurate depth perception. However, current display technologies still exhibit shortcomings in the simulation of accurate depth cues, and these shortcomings also vary between Virtual or Augmented Reality (VR, AR: eXtended Reality (XR) for short). This article compares depth perception between VR and Video See-Through (VST) AR. We developed a digital twin of an existing office room where users had top erform five depth-dependent tasks in VR and VST AR. Thirty-two participants took part in a user study using a 1 × 4 within-subjects design. Our results reveal higher misjudgment rates in VST AR due to conflicting depth cues between virtual and physical content. Increased head movements observed in participants were interpreted as a compensatory response to these conflicting cues. Furthermore, a longer task completion time in the VST AR condition indicates a lower task performance in VST AR. Interestingly, while participants rated the VR condition as easier and contrary to the increased misjudgments and lower performance with the VST AR display, a majority still expressed a preference for the VST AR experience. We discuss and explain these findings with the high visual dominance and referential power of the physical content in the VST AR condition, leading to a higher spatial presence and plausibility.},
  archive      = {J_TVCG},
  author       = {Franziska Westermeier and Larissa Brübach and Carolin Wienrich and Marc Erich Latoschik},
  doi          = {10.1109/TVCG.2024.3372061},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2140-2150},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Assessing depth perception in VR and video see-through AR: A comparison on distance judgment, performance, and preference},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NeRF-NQA: No-reference quality assessment for scenes
generated by NeRF and neural view synthesis methods. <em>TVCG</em>,
<em>30</em>(5), 2129–2139. (<a
href="https://doi.org/10.1109/TVCG.2024.3372037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural View Synthesis (NVS) has demonstrated efficacy in generating high-fidelity dense viewpoint videos using a image set with sparse views. However, existing quality assessment methods like PSNR, SSIM, and LPIPS are not tailored for the scenes with dense viewpoints synthesized by NVS and NeRF variants, thus, they often fall short in capturing the perceptual quality, including spatial and angular aspects of NVS-synthesized scenes. Furthermore, the lack of dense ground truth views makes the full reference quality assessment on NVS-synthesized scenes challenging. For instance, datasets such as LLFF provide only sparse images, insufficient for complete full-reference assessments. To address the issues above, we propose NeRF-NQA, the first no-reference quality assessment method for densely-observed scenes synthesized from the NVS and NeRF variants. NeRF-NQA employs a joint quality assessment strategy, integrating both viewwise and pointwise approaches, to evaluate the quality of NVS-generated scenes. The viewwise approach assesses the spatial quality of each individual synthesized view and the overall inter-views consistency, while the pointwise approach focuses on the angular qualities of scene surface points and their compound inter-point quality. Extensive evaluations are conducted to compare NeRF-NQA with 23 mainstream visual quality assessment methods (from fields of image, video, and light-field assessment). The results demonstrate NeRF-NQA outperforms the existing assessment methods significantly and it shows substantial superiority on assessing NVS-synthesized scenes without references. An implementation of this paper are available at https://github.com/VincentQQu/NeRF-NQA.},
  archive      = {J_TVCG},
  author       = {Qiang Qu and Hanxue Liang and Xiaoming Chen and Yuk Ying Chung and Yiran Shen},
  doi          = {10.1109/TVCG.2024.3372037},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2129-2139},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {NeRF-NQA: No-reference quality assessment for scenes generated by NeRF and neural view synthesis methods},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-guided DMT: Exploring a novel paradigm of dance
movement therapy in mixed reality for children with ASD. <em>TVCG</em>,
<em>30</em>(5), 2119–2128. (<a
href="https://doi.org/10.1109/TVCG.2024.3372063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Children diagnosed with Autism Spectrum Disorder (ASD) often exhibit motor disorders. Dance Movement Therapy (DMT) has shown great potential for improving the motor control ability of children with ASD. However, traditional DMT methods often lack vividness and are difficult to implement effectively. To address this issue, we propose a Mixed Reality DMT approach, utilizing interactive virtual agents. This approach offers immersive training content and multi-sensory feedback. To improve the training performance of children with ASD, we introduce a novel training paradigm featuring a self-guided mode. This paradigm enables the rapid creation of a virtual twin agent of the child with ASD using a single photo to embody oneself, which can then guide oneself during training. We conducted an experiment with the participation of 24 children diagnosed with ASD (or ASD propensity), recording their training performance under various experimental conditions. Through expert rating, behavior coding of training sessions, and statistical analysis, our findings revealed that the use of the twin agent for self-guidance resulted in noticeable improvements in the training performance of children with ASD. These improvements were particularly evident in terms of enhancing movement quality and refining overall target-related responses. Our study holds clinical potential in the field of medical treatment and rehabilitation for children with ASD.},
  archive      = {J_TVCG},
  author       = {Weiying Liu and Yanyan Zhang and Baiqiao Zhang and Qianqian Xiong and Hong Zhao and Sheng Li and Juan Liu and Yulong Bian},
  doi          = {10.1109/TVCG.2024.3372063},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2119-2128},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Self-guided DMT: Exploring a novel paradigm of dance movement therapy in mixed reality for children with ASD},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). In case of doubt, one follows one’s self: The implicit
guidance of the embodied self-avatar. <em>TVCG</em>, <em>30</em>(5),
2109–2118. (<a href="https://doi.org/10.1109/TVCG.2024.3372042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sense of embodiment in virtual reality (VR) is commonly understood as the subjective experience that one&#39;s physical body is substituted by a virtual counterpart, and is typically achieved when the avatar&#39;s body, seen from a first-person view, moves like one&#39;s physical body. Embodiment can also be experienced in other circumstances (e.g., in third-person view) or with imprecise or distorted visuo-motor coupling. It was moreover observed, in various cases of small or progressive temporal and spatial manipulations of avatars&#39; movements, that participants may spontaneously follow the movement shown by the avatar. The present work investigates whether, in some specific contexts, participants would follow what their avatar does even when large movement discrepancies occur, thereby extending the scope of understanding of the self-avatar follower effect beyond subtle changes of motion or speed manipulations. We conducted an experimental study in which we introduced uncertainty about which movement to perform at specific times and analyzed participants&#39; movements and subjective feedback after their avatar showed them an incorrect movement. Results show that, when in doubt, participants were influenced by their avatar&#39;s movements, leading them to perform that particular error twice more often than normal. Importantly, results of the embodiment score indicate that participants experienced a dissociation with their avatar at those times. Overall, these observations not only demonstrate the possibility of provoking situations in which participants follow the guidance of their avatar for large motor distortions, despite their awareness about the avatar movement disruption and on the possible influence it had on their choice, and, importantly, exemplify how the cognitive mechanism of embodiment is deeply rooted in the necessity of having a body.},
  archive      = {J_TVCG},
  author       = {Loën Boban and Ronan Boulic and Bruno Herbelin},
  doi          = {10.1109/TVCG.2024.3372042},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2109-2118},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {In case of doubt, one follows one&#39;s self: The implicit guidance of the embodied self-avatar},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 100-phones: A large VI-SLAM dataset for augmented reality
towards mass deployment on mobile phones. <em>TVCG</em>, <em>30</em>(5),
2098–2108. (<a href="https://doi.org/10.1109/TVCG.2024.3372133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual-inertial SLAM (VI-SLAM) is a key technology for Augmented Reality (AR), which allows the AR device to recover its 6-DoF motion in real-time in order to render the virtual content with the corresponding pose. Nowadays, smartphones are still the mainstream devices for ordinary users to experience AR. However the current VI-SLAM methods, although performing well on high-end phones, still face robustness challenges when deployed on a larger stock of mid- and low-end phones. Existing VI-SLAM datasets use either very ideal sensors or only a limited number of devices for data collection, which cannot reflect the capability gaps that VI-SLAM methods need to solve when deployed on a large variety of phone models. This work proposes 100-Phones. the first VI-SLAM dataset covering a wide range of mainstream phones in the market. The dataset consists of 350 sequences collected by 100 different models of phones. Through analysis and experiments on the collected data, we conclude that the quality of visual-inertial data vary greatly among the mainstream phones, and the current open source VI-SLAM methods still have serious robustness issues when it comes to mass deployment on mobile phones. We release the dataset to facilitate the robustness improvement of VI-SLAM and to promote the mass popularization of AR. Project page: https://github.com/zju3dv/100-Phones.},
  archive      = {J_TVCG},
  author       = {Guofeng Zhang and Jin Yuan and Haomin Liu and Zhen Peng and Chunlei Li and Zibin Wang and Hujun Bao},
  doi          = {10.1109/TVCG.2024.3372133},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2098-2108},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {100-phones: A large VI-SLAM dataset for augmented reality towards mass deployment on mobile phones},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PLUME: Record, replay, analyze and share user behavior in
6DoF XR experiences. <em>TVCG</em>, <em>30</em>(5), 2087–2097. (<a
href="https://doi.org/10.1109/TVCG.2024.3372107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From education to medicine to entertainment, a wide range of industrial and academic fields now utilize eXtended Reality (XR) technologies. This diversity and growing use are boosting research and leading to an increasing number of XR experiments involving human subjects. The main aim of these studies is to understand the user experience in the broadest sense, such as the user cognitive and emotional states. Behavioral data collected during XR experiments, such as user movements, gaze, actions, and physiological signals constitute precious assets for analyzing and understanding the user experience. While they contribute to overcome the intrinsic flaws of explicit data such as post-experiment questionnaires, the required acquisition and analysis tools are costly and challenging to develop, especially for 6DoF (Degrees of Freedom) XR experiments. Moreover, there is no common format for XR behavioral data, which restrains data-sharing, and thus hinders wide usages across the community, replicability of studies, and the constitution of large datasets or meta-analysis. In this context, we present PLUME, an open-source software toolbox (PLUME Recorder, PLUME Viewer, PLUME Python) that allows for the exhaustive record of XR behavioral data (including synchronous physiological signals), their offline interactive replay and analysis (with a standalone application), and their easy sharing due to our compact and interoperable data format. We believe that PLUME can greatly benefit the scientific community by making the use of behavioral and physiological data available for the greatest, contributing to the reproducibility and replicability of XR user studies, enabling the creation of large datasets, and contributing to a deeper understanding of user experience.},
  archive      = {J_TVCG},
  author       = {Charles Javerliat and Sophie Villenave and Pierre Raimbaud and Guillaume Lavoué},
  doi          = {10.1109/TVCG.2024.3372107},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2087-2097},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PLUME: Record, replay, analyze and share user behavior in 6DoF XR experiences},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Swift-eye: Towards anti-blink pupil tracking for precise and
robust high-frequency near-eye movement analysis with event cameras.
<em>TVCG</em>, <em>30</em>(5), 2077–2086. (<a
href="https://doi.org/10.1109/TVCG.2024.3372039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eye tracking has shown great promise in many scientific fields and daily applications, ranging from the early detection of mental health disorders to foveated rendering in virtual reality (VR). These applications all call for a robust system for high-frequency near-eye movement sensing and analysis in high precision, which cannot be guaranteed by the existing eye tracking solutions with CCD/CMOS cameras. To bridge the gap, in this paper, we propose Swift-Eye, an offline precise and robust pupil estimation and tracking framework to support high-frequency near-eye movement analysis, especially when the pupil region is partially occluded. Swift-Eye is built upon the emerging event cameras to capture the high-speed movement of eyes in high temporal resolution. Then, a series of bespoke components are designed to generate high-quality near-eye movement video at a high frame rate over kilohertz and deal with the occlusion over the pupil caused by involuntary eye blinks. According to our extensive evaluations on EV-Eye, a large-scale public dataset for eye tracking using event cameras, Swift-Eye shows high robustness against significant occlusion. It can improve the IoU and F1-score of the pupil estimation by 20% and 12.5% respectively, compared with the second-best competing approach, when over 80% of the pupil region is occluded by the eyelid. Lastly, it provides continuous and smooth traces of pupils in extremely high temporal resolution and can support high-frequency eye movement analysis and a number of potential applications, such as mental health diagnosis, behaviour-brain association, etc. The implementation details and source codes can be found at https://github.com/ztysdu/Swift-Eye.},
  archive      = {J_TVCG},
  author       = {Tongyu Zhang and Yiran Shen and Guangrong Zhao and Lin Wang and Xiaoming Chen and Lu Bai and Yuanfeng Zhou},
  doi          = {10.1109/TVCG.2024.3372039},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2077-2086},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Swift-eye: Towards anti-blink pupil tracking for precise and robust high-frequency near-eye movement analysis with event cameras},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Embodying a self-avatar with a larger leg: Its impacts on
motor control and dynamic stability. <em>TVCG</em>, <em>30</em>(5),
2066–2076. (<a href="https://doi.org/10.1109/TVCG.2024.3372084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several studies have shown that users of immersive virtual reality can feel high levels of embodiment in self-avatars that have different morphological proportions than those of their actual bodies. Deformed and unrealistic morphological modifications are accepted by embodied users, underlying the adaptability of one&#39;s mental map of their body (body schema) in response to incoming sensory feedback. Before initiating a motor action, the brain uses the body schema to plan and sequence the necessary movements. Therefore, embodiment in a self-avatar with a different morphology, such as one with deformed proportions, could lead to changes in motor planning and execution. In this study, we aimed to measure the effects on movement planning and execution of embodying a self-avatar with an enlarged lower leg on one side. Thirty participants embodied an avatar without any deformations, and with an enlarged dominant or non-dominant leg, in randomized order. Two different levels of embodiment were induced, using synchronous or asynchronous visuotactile stimuli. In each condition, participants performed a gait initiation task. Their center of mass and center of pressure were measured, and the margin of stability (MoS) was computed from these values. Their perceived level of embodiment was also measured, using a validated questionnaire. Results show no significant changes on the biomechenical variables related to dynamic stability. Embodiment scores decreased with asynchronous stimuli, without impacting the measures related to stability. The body schema may not have been impacted by the larger virtual leg. However, deforming the self-avatar&#39;s morphology could have important implications when addressing individuals with impaired physical mobility by subtly influencing action execution during a rehabilitation protocol.},
  archive      = {J_TVCG},
  author       = {Valentin Vallageas and Rachid Aissaoui and Iris Willaert and David R. Labbé},
  doi          = {10.1109/TVCG.2024.3372084},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2066-2076},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Embodying a self-avatar with a larger leg: Its impacts on motor control and dynamic stability},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Designing and evaluating a VR lobby for a socially enriching
remote opera watching experience. <em>TVCG</em>, <em>30</em>(5),
2055–2065. (<a href="https://doi.org/10.1109/TVCG.2024.3372081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The latest social VR technologies have enabled users to attend traditional media and arts performances together while being geographically removed, making such experiences accessible despite budget, distance, and other restrictions. In this work, we aim at improving the way remote performances are shared by designing and evaluating a VR theatre lobby which serves as a space for users to gather, interact, and relive the common experience of watching a virtual opera. We conducted an initial test with experts ($\mathrm{N}=10$, i.e., designers and opera enthusiasts) in pairs using our VR lobby prototype, developed based on the theoretical lobby design concept. A unique aspect of our experience is its highly realistic representation of users in the virtual space. The test results guided refinements to the VR lobby structure and implementation, aiming to improve the user experience and align it more closely with the social VR lobby&#39;s intended purpose. With the enhanced prototype, we ran a between-subject controlled study ($\mathrm{N}=40$) to compare the user experience in the social VR lobby between individuals and paired participants. To do so, we designed and validated a questionnaire to measure the user experience in the VR lobby. Results of our mixed-methods analysis, including interviews, questionnaire results, and user behavior, reveal the strength of our social VR lobby in connecting with other users, consuming the opera in a deeper manner, and exploring new possibilities beyond what is common in real life. All supplemental materials are available at https://github.com/cwi-dis/IEEEVR2024-VRLobby.},
  archive      = {J_TVCG},
  author       = {Sueyoon Lee and Irene Viola and Silvia Rossi and Zhirui Guo and Ignacio Reimat and Kinga Ławicka and Alina Striner and Pablo Cesar},
  doi          = {10.1109/TVCG.2024.3372081},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2055-2065},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Designing and evaluating a VR lobby for a socially enriching remote opera watching experience},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal physiological analysis of impact of emotion on
cognitive control in VR. <em>TVCG</em>, <em>30</em>(5), 2044–2054. (<a
href="https://doi.org/10.1109/TVCG.2024.3372101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive control is often perplexing to elucidate and can be easily influenced by emotions. Understanding the individual cognitive control level is crucial for enhancing VR interaction and designing adaptive and self-correcting VR/AR applications. Emotions can reallocate processing resources and influence cognitive control performance. However, current research has primarily emphasized the impact of emotional valence on cognitive control tasks, neglecting emotional arousal. In this study, we comprehensively investigate the influence of emotions on cognitive control based on the arousal-valence model. A total of 26 participants are recruited, inducing emotions through VR videos with high ecological validity and then performing related cognitive control tasks. Leveraging physiological data including EEG, HRV, and EDA, we employ classification techniques such as SVM, KNN, and deep learning to categorize cognitive control levels. The experiment results demonstrate that high-arousal emotions significantly enhance users&#39; cognitive control abilities. Utilizing complementary information among multi-modal physiological signal features, we achieve an accuracy of 84.52% in distinguishing between high and low cognitive control. Additionally, time-frequency analysis results confirm the existence of neural patterns related to cognitive control, contributing to a better understanding of the neural mechanisms underlying cognitive control in VR. Our research indicates that physiological signals measured from both the central and autonomic nervous systems can be employed for cognitive control classification, paving the way for novel approaches to improve VR/AR interactions.},
  archive      = {J_TVCG},
  author       = {Ming Li and Junjun Pan and Yu Li and Yang Gao and Hong Qin and Yang Shen},
  doi          = {10.1109/TVCG.2024.3372101},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2044-2054},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multimodal physiological analysis of impact of emotion on cognitive control in VR},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Omnidirectional virtual visual acuity: A user-centric visual
clarity metric for virtual reality head-mounted displays and
environments. <em>TVCG</em>, <em>30</em>(5), 2033–2043. (<a
href="https://doi.org/10.1109/TVCG.2024.3372127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Users&#39; perceived image quality of virtual reality head-mounted displays (VR HMDs) is determined by multiple factors, including the HMD&#39;s structure, optical system, display and render resolution, and users&#39; visual acuity (VA). Existing metrics such as pixels per degree (PPD) have limitations that prevent accurate comparison of different VR HMDs. One of the main limitations is that not all VR HMD manufacturers released the official PPD or details of their HMDs&#39; optical systems. Without these details, developers and users cannot know the precise PPD or calculate it for a given HMD. The other issue is that the visual clarity varies with the VR environment. Our work has identified a gap in having a feasible metric that can measure the visual clarity of VR HMDs. To address this gap, we present an end-to-end and user-centric visual clarity metric, omnidirectional virtual visual acuity (OVVA), for VR HMDs. OVVA extends the physical visual acuity chart into a virtual format to measure the virtual visual acuity of an HMD&#39;s central focal area and its degradation in its noncentral area. OVVA provides a new perspective to measure visual clarity and can serve as an intuitive and accurate reference for VR applications sensitive to visual accuracy. Our results show that OVVA is a simple yet effective metric for comparing VR HMDs and environments.},
  archive      = {J_TVCG},
  author       = {Jialin Wang and Rongkai Shi and Xiaodong Li and Yushi Wei and Hai-Ning Liang},
  doi          = {10.1109/TVCG.2024.3372127},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {5},
  number       = {5},
  pages        = {2033-2043},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Omnidirectional virtual visual acuity: A user-centric visual clarity metric for virtual reality head-mounted displays and environments},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VIPurPCA: Visualizing and propagating uncertainty in
principal component analysis. <em>TVCG</em>, <em>30</em>(4), 2011–2022.
(<a href="https://doi.org/10.1109/TVCG.2023.3345532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variables obtained by experimental measurements or statistical inference typically carry uncertainties. When an algorithm uses such quantities as input variables, this uncertainty should propagate to the algorithm&#39;s output. Concretely, we consider the classic notion of principal component analysis (PCA): If it is applied to a finite data matrix containing imperfect (i.e., uncertain) multidimensional measurements, its output—a lower-dimensional representation—is itself subject to uncertainty. We demonstrate that this uncertainty can be approximated by appropriate linearization of the algorithm&#39;s nonlinear functionality, using automatic differentiation. By itself, however, this structured, uncertain output is difficult to interpret for users. We provide an animation method that effectively visualizes the uncertainty of the lower dimensional map. Implemented as an open-source software package, it allows researchers to assess the reliability of PCA embeddings.},
  archive      = {J_TVCG},
  author       = {Susanne Zabel and Philipp Hennig and Kay Nieselt},
  doi          = {10.1109/TVCG.2023.3345532},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {2011-2022},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VIPurPCA: Visualizing and propagating uncertainty in principal component analysis},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified particle-based solver for non-newtonian behaviors
simulation. <em>TVCG</em>, <em>30</em>(4), 1998–2010. (<a
href="https://doi.org/10.1109/TVCG.2023.3341453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a unified framework to simulate non-Newtonian behaviors. We combine viscous and elasto-plastic stress into a unified particle solver to achieve various non-Newtonian behaviors ranging from fluid-like to solid-like. Our constitutive model is based on a Generalized Maxwell model, which incorporates viscosity, elasticity and plasticity in one non-linear framework by a unified way. On the one hand, taking advantage of the viscous term, we construct a series of strain-rate dependent models for classical non-Newtonian behaviors such as shear-thickening, shear-thinning, Bingham plastic, etc. On the other hand, benefiting from the elasto-plastic model, we empower our framework with the ability to simulate solid-like non-Newtonian behaviors, i.e., visco-elasticity/plasticity. In addition, we enrich our method with a heat diffusion model to make our method flexible in simulating phase change. Through sufficient experiments, we demonstrate a wide range of non-Newtonian behaviors ranging from viscous fluid to deformable objects. We believe this non-Newtonian model will enhance the realism of physically-based animation, which has great potential for computer graphics.},
  archive      = {J_TVCG},
  author       = {Chunlei Li and Yang Gao and Jiayi He and Tianwei Cheng and Shuai Li and Aimin Hao and Hong Qin},
  doi          = {10.1109/TVCG.2023.3341453},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {1998-2010},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A unified particle-based solver for non-newtonian behaviors simulation},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). InVADo: Interactive visual analysis of molecular docking
data. <em>TVCG</em>, <em>30</em>(4), 1984–1997. (<a
href="https://doi.org/10.1109/TVCG.2023.3337642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular docking is a key technique in various fields like structural biology, medicinal chemistry, and biotechnology. It is widely used for virtual screening during drug discovery, computer-assisted drug design, and protein engineering. A general molecular docking process consists of the target and ligand selection, their preparation, and the docking process itself, followed by the evaluation of the results. However, the most commonly used docking software provides no or very basic evaluation possibilities. Scripting and external molecular viewers are often used, which are not designed for an efficient analysis of docking results. Therefore, we developed InVADo, a comprehensive interactive visual analysis tool for large docking data. It consists of multiple linked 2D and 3D views. It filters and spatially clusters the data, and enriches it with post-docking analysis results of protein-ligand interactions and functional groups, to enable well-founded decision-making. In an exemplary case study, domain experts confirmed that InVADo facilitates and accelerates the analysis workflow. They rated it as a convenient, comprehensive, and feature-rich tool, especially useful for virtual screening.},
  archive      = {J_TVCG},
  author       = {Marco Schäfer and Nicolas Brich and Jan Byška and Sérgio M. Marques and David Bednář and Philipp Thiel and Barbora Kozlíková and Michael Krone},
  doi          = {10.1109/TVCG.2023.3337642},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {1984-1997},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {InVADo: Interactive visual analysis of molecular docking data},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual exploratory analysis for designing large-scale
network-on-chip architectures: A domain expert-led design study.
<em>TVCG</em>, <em>30</em>(4), 1970–1983. (<a
href="https://doi.org/10.1109/TVCG.2023.3337173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualization design studies bring together visualization researchers and domain experts to address yet unsolved data analysis challenges stemming from the needs of the domain experts. Typically, the visualization researchers lead the design study process and implementation of any visualization solutions. This setup leverages the visualization researchers’ knowledge of methodology, design, and programming, but the availability to synchronize with the domain experts can hamper the design process. We consider an alternative setup where the domain experts take the lead in the design study, supported by the visualization experts. In this study, the domain experts are computer architecture experts who simulate and analyze novel computer chip designs. These chips rely on a Network-on-Chip (NOC) to connect components. The experts want to understand how the chip designs perform and what in the design led to their performance. To aid this analysis, we develop Vis4Mesh, a visualization system that provides spatial, temporal, and architectural context to simulated NOC behavior. Integration with an existing computer architecture visualization tool enables architects to perform deep-dives into specific architecture component behavior. We validate Vis4Mesh through a case study and a user study with computer architecture researchers. We reflect on our design and process, discussing advantages, disadvantages, and guidance for engaging in a domain expert-led design studies.},
  archive      = {J_TVCG},
  author       = {Shaoyu Wang and Hang Yan and Katherine E. Isaacs and Yifan Sun},
  doi          = {10.1109/TVCG.2023.3337173},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {1970-1983},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual exploratory analysis for designing large-scale network-on-chip architectures: A domain expert-led design study},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visualizing and comparing machine learning predictions to
improve human-AI teaming on the example of cell lineage. <em>TVCG</em>,
<em>30</em>(4), 1956–1969. (<a
href="https://doi.org/10.1109/TVCG.2023.3302308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We visualize the predictions of multiple machine learning models to help biologists as they interactively make decisions about cell lineage —the development of a (plant) embryo from a single ovum cell . Based on a confocal microscopy dataset, traditionally biologists manually constructed the cell lineage, starting from this observation and reasoning backward in time to establish their inheritance. To speed up this tedious process, we make use of machine learning (ML) models trained on a database of manually established cell lineages to assist the biologist in cell assignment. Most biologists, however, are not familiar with ML, nor is it clear to them which model best predicts the embryo&#39;s development. We thus have developed a visualization system that is designed to support biologists in exploring and comparing ML models, checking the model predictions, detecting possible ML model mistakes, and deciding on the most likely embryo development. To evaluate our proposed system, we deployed our interface with six biologists in an observational study. Our results show that the visual representations of machine learning are easily understandable, and our tool, LineageD+, could potentially increase biologists’ working efficiency and enhance the understanding of embryos.},
  archive      = {J_TVCG},
  author       = {Jiayi Hong and Ross Maciejewski and Alain Trubuil and Tobias Isenberg},
  doi          = {10.1109/TVCG.2023.3302308},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {1956-1969},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualizing and comparing machine learning predictions to improve human-AI teaming on the example of cell lineage},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parallel computation of piecewise linear morse-smale
segmentations. <em>TVCG</em>, <em>30</em>(4), 1942–1955. (<a
href="https://doi.org/10.1109/TVCG.2023.3261981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a well-scaling parallel algorithm for the computation of Morse-Smale (MS) segmentations, including the region separators and region boundaries. The segmentation of the domain into ascending and descending manifolds, solely defined on the vertices, improves the computational time using path compression and fully segments the border region. Region boundaries and region separators are generated using a multi-label marching tetrahedra algorithm. This enables a fast and simple solution to find optimal parameter settings in preliminary exploration steps by generating an MS complex preview. It also poses a rapid option to generate a fast visual representation of the region geometries for immediate utilization. Two experiments demonstrate the performance of our approach with speedups of over an order of magnitude in comparison to two publicly available implementations. The example section shows the similarity to the MS complex, the useability of the approach, and the benefits of this method with respect to the presented datasets. We provide our implementation with the paper.},
  archive      = {J_TVCG},
  author       = {Robin G. C. Maack and Jonas Lukasczyk and Julien Tierny and Hans Hagen and Ross Maciejewski and Christoph Garth},
  doi          = {10.1109/TVCG.2023.3261981},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {1942-1955},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Parallel computation of piecewise linear morse-smale segmentations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MoReVis: A visual summary for spatiotemporal moving regions.
<em>TVCG</em>, <em>30</em>(4), 1927–1941. (<a
href="https://doi.org/10.1109/TVCG.2023.3250166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial and temporal interactions are central and fundamental in many activities in our world. A common problem faced when visualizing this type of data is how to provide an overview that helps users navigate efficiently. Traditional approaches use coordinated views or 3D metaphors like the Space-time cube to tackle this problem. However, they suffer from overplotting and often lack spatial context, hindering data exploration. More recent techniques, such as MotionRugs , propose compact temporal summaries based on 1D projection. While powerful, these techniques do not support the situation for which the spatial extent of the objects and their intersections is relevant, such as the analysis of surveillance videos or tracking weather storms. In this article, we propose MoReVis, a visual overview of spatiotemporal data that considers the objects’ spatial extent and strives to show spatial interactions among these objects by displaying spatial intersections. Like previous techniques, our method involves projecting the spatial coordinates to 1D to produce compact summaries. However, our solution&#39;s core consists of performing a layout optimization step that sets the size and positions of the visual marks on the summary to resemble the actual values on the original space. We also provide multiple interactive mechanisms to make interpreting the results more straightforward for the user. We perform an extensive experimental evaluation and usage scenarios. Moreover, we evaluated the usefulness of MoReVis in a study with 9 participants. The results point out the effectiveness and suitability of our method in representing different datasets compared to traditional techniques.},
  archive      = {J_TVCG},
  author       = {Giovani Valdrighi and Nivan Ferreira and Jorge Poco},
  doi          = {10.1109/TVCG.2023.3250166},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {1927-1941},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MoReVis: A visual summary for spatiotemporal moving regions},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-user redirected walking in separate physical spaces
for online VR scenarios. <em>TVCG</em>, <em>30</em>(4), 1916–1926. (<a
href="https://doi.org/10.1109/TVCG.2023.3251648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the recent rise of Metaverse, online multiplayer VR applications are becoming increasingly prevalent worldwide. However, as multiple users are located in different physical environments, different reset frequencies and timings can lead to serious fairness issues for online collaborative/competitive VR applications. For the fairness of online VR apps/games, an ideal online RDW strategy must make the locomotion opportunities of different users equal, regardless of different physical environment layouts. The existing RDW methods lack the scheme to coordinate multiple users in different PEs, and thus have the issue of triggering too many resets for all the users under the locomotion fairness constraint. We propose a novel multi-user RDW method that is able to significantly reduce the overall reset number and give users a better immersive experience by providing a fair exploration. Our key idea is to first find out the ”bottleneck” user that may cause all users to be reset and estimate the time to reset given the users’ next targets, and then redirect all the users to favorable poses during that maximized bottleneck time to ensure the subsequent resets can be postponed as much as possible. More particularly, we develop methods to estimate the time of possibly encountering obstacles and the reachable area for a specific pose to enable the prediction of the next reset caused by any user. Our experiments and user study found that our method outperforms existing RDW methods in online VR applications.},
  archive      = {J_TVCG},
  author       = {Sen-Zhe Xu and Jia-Hong Liu and Miao Wang and Fang-Lue Zhang and Song-Hai Zhang},
  doi          = {10.1109/TVCG.2023.3251648},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {1916-1926},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multi-user redirected walking in separate physical spaces for online VR scenarios},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discrete morse sandwich: Fast computation of persistence
diagrams for scalar data – an algorithm and a benchmark. <em>TVCG</em>,
<em>30</em>(4), 1897–1915. (<a
href="https://doi.org/10.1109/TVCG.2023.3238008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces an efficient algorithm for persistence diagram computation, given an input piecewise linear scalar field $f$ defined on a $d$ -dimensional simplicial complex $\mathcal {K}$ , with $d \leq 3$ . Our work revisits the seminal algorithm “PairSimplices” (Edelsbrunner et al. 2002), (Zomorodian, 2010) with discrete Morse theory (DMT) (Forman, 1998), (Robins et al. 2011), which greatly reduces the number of input simplices to consider. Further, we also extend to DMT and accelerate the stratification strategy described in “PairSimplices” (Edelsbrunner et al. 2002), (Zomorodian, 2010) for the fast computation of the $0^{th}$ and $(d-1)^{th}$ diagrams, noted $\mathcal {D}_{0}(f)$ and $\mathcal {D}_{d-1}(f)$ . Minima-saddle persistence pairs ( $\mathcal {D}_{0}(f)$ ) and saddle-maximum persistence pairs ( $\mathcal {D}_{d-1}(f)$ ) are efficiently computed by processing, with a Union-Find, the unstable sets of 1-saddles and the stable sets of $(d-1)$ -saddles. This fast pre-computation for the dimensions 0 and $(d-1)$ enables an aggressive specialization of (Bauer et al. 2014) to the 3D case, which results in a drastic reduction of the number of input simplices for the computation of $\mathcal {D}_{1}(f)$ , the intermediate layer of the sandwich . Finally, we document several performance improvements via shared-memory parallelism. We provide an open-source implementation of our algorithm for reproducibility purposes. Extensive experiments indicate that our algorithm improves by two orders of magnitude the time performance of the seminal “PairSimplices” algorithm it extends. Moreover, it also improves memory footprint and time performance over a selection of 14 competing approaches, with a substantial gain over the fastest available approaches, while producing a strictly identical output.},
  archive      = {J_TVCG},
  author       = {Pierre Guillou and Jules Vidal and Julien Tierny},
  doi          = {10.1109/TVCG.2023.3238008},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {1897-1915},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Discrete morse sandwich: Fast computation of persistence diagrams for scalar data – an algorithm and a benchmark},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-fidelity specular SVBRDF acquisition from flash
photographs. <em>TVCG</em>, <em>30</em>(4), 1885–1896. (<a
href="https://doi.org/10.1109/TVCG.2023.3235277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining accurate SVBRDFs from 2D photographs of shiny, heterogeneous 3D objects is a highly sought-after goal for domains like cultural heritage archiving, where it is critical to document color appearance in high fidelity. In prior work such as the promising framework by Nam et al., the problem is simplified by assuming that specular highlights exhibit symmetry and isotropy about an estimated surface normal. The present work builds on this foundation with several significant modifications. Recognizing the importance of the surface normal as an axis of symmetry, we compare nonlinear optimization for normals with a linear approximation proposed by Nam et al. and find that nonlinear optimization is superior to the linear approximation, while noting that the surface normal estimates generally have a very significant impact on the reconstructed color appearance of the object. We also examine the use of a monotonicity constraint for reflectance and develop a generalization that also enforces continuity and smoothness when optimizing continuous monotonic functions like a microfacet distribution. Finally, we explore the impact of simplifying from an arbitrary 1D basis function to a traditional parametric microfacet distribution (GGX), and we find this to be a reasonable approximation that trades some fidelity for practicality in certain applications. Both representations can be used in existing rendering architectures like game engines or online 3D viewers, while retaining accurate color appearance for fidelity-critical applications like cultural heritage or online sales.},
  archive      = {J_TVCG},
  author       = {Michael Tetzlaff},
  doi          = {10.1109/TVCG.2023.3235277},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {1885-1896},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {High-fidelity specular SVBRDF acquisition from flash photographs},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating glyph design for showing large-magnitude-range
quantum spins. <em>TVCG</em>, <em>30</em>(4), 1868–1884. (<a
href="https://doi.org/10.1109/TVCG.2022.3232591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present experimental results to explore a form of bivariate glyphs for representing large-magnitude-range vectors. The glyphs meet two conditions: (1) two visual dimensions are separable; and (2) one of the two visual dimensions uses a categorical representation (e.g., a categorical colormap). We evaluate how much these two conditions determine the bivariate glyphs’ effectiveness. The first experiment asks participants to perform three local tasks requiring reading no more than two glyphs. The second experiment scales up the search space in global tasks when participants must look at the entire scene of hundreds of vector glyphs to get an answer. Our results support that the first condition is necessary for local tasks when a few items are compared. But it is not enough for understanding a large amount of data. The second condition is necessary for perceiving global structures of examining very complex datasets. Participants’ comments reveal that the categorical features in the bivariate glyphs trigger emergent optimal viewers’ behaviors. This work contributes to perceptually accurate glyph representations for revealing patterns from large scientific results. We release source code, quantum physics data, training documents, participants’ answers, and statistical analyses for reproducible science at https://osf.io/4xcf5/?view_only=94123139df9c4ac984a1e0df811cd580 .},
  archive      = {J_TVCG},
  author       = {Henan Zhao and Garnett W. Bryant and Wesley Griffin and Judith E. Terrill and Jian Chen},
  doi          = {10.1109/TVCG.2022.3232591},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {1868-1884},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Evaluating glyph design for showing large-magnitude-range quantum spins},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). V-mail: 3D-enabled correspondence about spatial data on
(almost) all your devices. <em>TVCG</em>, <em>30</em>(4), 1853–1867. (<a
href="https://doi.org/10.1109/TVCG.2022.3229017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present V-Mail, a framework of cross-platform applications, interactive techniques, and communication protocols for improved multi-person correspondence about spatial 3D datasets. Inspired by the daily use of e-mail, V-Mail seeks to enable a similar style of rapid, multi-person communication accessible on any device; however, it aims to do this in the new context of spatial 3D communication, where limited access to 3D graphics hardware typically prevents such communication. The approach integrates visual data storytelling with data exploration, spatial annotations, and animated transitions. V-Mail “data stories” are exported in a standard video file format to establish a common baseline level of access on (almost) any device. The V-Mail framework also includes a series of complementary client applications and plugins that enable different degrees of story co-authoring and data exploration, adjusted automatically to match the capabilities of various devices. A lightweight, phone-based V-Mail app makes it possible to annotate data by adding captions to the video. These spatial annotations are then immediately accessible to team members running high-end 3D graphics visualization systems that also include a V-Mail client, implemented as a plugin. Results and evaluation from applying V-Mail to assist communication within an interdisciplinary science team studying Antarctic ice sheets confirm the utility of the asynchronous, cross-platform collaborative framework while also highlighting some current limitations and opportunities for future work.},
  archive      = {J_TVCG},
  author       = {Jung Who Nam and Tobias Isenberg and Daniel F. Keefe},
  doi          = {10.1109/TVCG.2022.3229017},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {1853-1867},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {V-mail: 3D-enabled correspondence about spatial data on (Almost) all your devices},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactive reweighting for mitigating label quality issues.
<em>TVCG</em>, <em>30</em>(3), 1837–1852. (<a
href="https://doi.org/10.1109/TVCG.2023.3345340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label quality issues, such as noisy labels and imbalanced class distributions, have negative effects on model performance. Automatic reweighting methods identify problematic samples with label quality issues by recognizing their negative effects on validation samples and assigning lower weights to them. However, these methods fail to achieve satisfactory performance when the validation samples are of low quality. To tackle this, we develop Reweighter, a visual analysis tool for sample reweighting. The reweighting relationships between validation samples and training samples are modeled as a bipartite graph. Based on this graph, a validation sample improvement method is developed to improve the quality of validation samples. Since the automatic improvement may not always be perfect, a co-cluster-based bipartite graph visualization is developed to illustrate the reweighting relationships and support the interactive adjustments to validation samples and reweighting results. The adjustments are converted into the constraints of the validation sample improvement method to further improve validation samples. We demonstrate the effectiveness of Reweighter in improving reweighting results through quantitative evaluation and two case studies.},
  archive      = {J_TVCG},
  author       = {Weikai Yang and Yukai Guo and Jing Wu and Zheng Wang and Lan-Zhe Guo and Yu-Feng Li and Shixia Liu},
  doi          = {10.1109/TVCG.2023.3345340},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1837-1852},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive reweighting for mitigating label quality issues},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Designing for visualization in motion: Embedding
visualizations in swimming videos. <em>TVCG</em>, <em>30</em>(3),
1821–1836. (<a href="https://doi.org/10.1109/TVCG.2023.3341990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We report on challenges and considerations for supporting design processes for visualizations in motion embedded in sports videos. We derive our insights from analyzing swimming race visualizations and motion-related data, building a technology probe, as well as a study with designers. Understanding how to design situated visualizations in motion is important for a variety of contexts. Competitive sports coverage, in particular, increasingly includes information on athlete or team statistics and records. Although moving visual representations attached to athletes or other targets are starting to appear, systematic investigations on how to best support their design process in the context of sports videos are still missing. Our work makes several contributions in identifying opportunities for visualizations to be added to swimming competition coverage but, most importantly, in identifying requirements and challenges for designing situated visualizations in motion. Our investigations include the analysis of a survey with swimming enthusiasts on their motion-related information needs, an ideation workshop to collect designs and elicit design challenges, the design of a technology probe that allows to create embedded visualizations in motion based on real data (Fig. 1), and an evaluation with visualization designers that aimed to understand the benefits of designing directly on videos.},
  archive      = {J_TVCG},
  author       = {Lijie Yao and Romain Vuillemot and Anastasia Bezerianos and Petra Isenberg},
  doi          = {10.1109/TVCG.2023.3341990},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1821-1836},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Designing for visualization in motion: Embedding visualizations in swimming videos},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Personalized audio-driven 3D facial animation via
style-content disentanglement. <em>TVCG</em>, <em>30</em>(3), 1803–1820.
(<a href="https://doi.org/10.1109/TVCG.2022.3230541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a learning-based approach for generating 3D facial animations with the motion style of a specific subject from arbitrary audio inputs. The subject style is learned from a video clip (1-2 minutes) either downloaded from the Internet or captured through an ordinary camera. Traditional methods often require many hours of the subject&#39;s video to learn a robust audio-driven model and are thus unsuitable for this task. Recent research efforts aim to train a model from video collections of a few subjects but ignore the discrimination between the subject style and underlying speech content within facial motions, leading to inaccurate style or articulation. To solve the problem, we propose a novel framework that disentangles subject-specific style and speech content from facial motions. The disentanglement is enabled by two novel training mechanisms. One is two-pass style swapping between two random subjects, and the other is joint training of the decomposition network and audio-to-motion network with a shared decoder. After training, the disentangled style is combined with arbitrary audio inputs to generate stylized audio-driven 3D facial animations. Compared with start-of-the-art methods, our approach achieves better results qualitatively and quantitatively, especially in difficult cases like bilabial plosive and bilabial nasal phonemes.},
  archive      = {J_TVCG},
  author       = {Yujin Chai and Tianjia Shao and Yanlin Weng and Kun Zhou},
  doi          = {10.1109/TVCG.2022.3230541},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1803-1820},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Personalized audio-driven 3D facial animation via style-content disentanglement},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PhraseMap: Attention-based keyphrases recommendation for
information seeking. <em>TVCG</em>, <em>30</em>(3), 1787–1802. (<a
href="https://doi.org/10.1109/TVCG.2022.3225114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many Information Retrieval (IR) approaches have been proposed to extract relevant information from a large corpus. Among these methods, phrase-based retrieval methods have been proven to capture more concrete and concise information than word-based and paragraph-based methods. However, due to the complex relationship among phrases and a lack of proper visual guidance, achieving user-driven interactive information-seeking and retrieval remains challenging. In this study, we present a visual analytic approach for users to seek information from an extensive collection of documents efficiently. The main component of our approach is a PhraseMap, where nodes and edges represent the extracted keyphrases and their relationships, respectively, from a large corpus. To build the PhraseMap, we extract keyphrases from each document and link the phrases according to word attention determined using modern language models, i.e., BERT. As can be imagined, the graph is complex due to the extensive volume of information and the massive amount of relationships. Therefore, we develop a navigation algorithm to facilitate information seeking. It includes (1) a question-answering (QA) model to identify phrases related to users’ queries and (2) updating relevant phrases based on users’ feedback. To better present the PhraseMap, we introduce a resource-controlled self-organizing map (RC-SOM) to evenly and regularly display phrases on grid cells while expecting phrases with similar semantics to stay close in the visualization. To evaluate our approach, we conducted case studies with three domain experts in diverse literature. The results and feedback demonstrate its effectiveness, usability, and intelligence.},
  archive      = {J_TVCG},
  author       = {Yamei Tu and Rui Qiu and Yu-Shuen Wang and Po-Yin Yen and Han-Wei Shen},
  doi          = {10.1109/TVCG.2022.3225114},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1787-1802},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PhraseMap: Attention-based keyphrases recommendation for information seeking},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D question answering. <em>TVCG</em>, <em>30</em>(3),
1772–1786. (<a href="https://doi.org/10.1109/TVCG.2022.3225327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual question answering (VQA) has experienced tremendous progress in recent years. However, most efforts have only focused on 2D image question-answering tasks. In this article, we extend VQA to its 3D counterpart, 3D question answering (3DQA), which can facilitate a machine&#39;s perception of 3D real-world scenarios. Unlike 2D image VQA, 3DQA takes the color point cloud as input and requires both appearance and 3D geometrical comprehension to answer the 3D-related questions. To this end, we propose a novel transformer-based 3DQA framework “3DQA-TR” , which consists of two encoders to exploit the appearance and geometry information, respectively. Finally, the multi-modal information about the appearance, geometry, and linguistic question can attend to each other via a 3D-linguistic Bert to predict the target answers. To verify the effectiveness of our proposed 3DQA framework, we further develop the first 3DQA dataset “ScanQA” , which builds on the ScanNet dataset and contains over 10 K question-answer pairs for 806 scenes. To the best of our knowledge, ScanQA is the first large-scale dataset with natural-language questions and free-form answers in 3D environments that is fully human-annotated . We also use several visualizations and experiments to investigate the astonishing diversity of the collected questions and the significant differences between this task from 2D VQA and 3D captioning. Extensive experiments on this dataset demonstrate the obvious superiority of our proposed 3DQA framework over state-of-the-art VQA frameworks and the effectiveness of our major designs. Our code and dataset will be made publicly available to facilitate research in this direction. The code and data are available at http://shuquanye.com/3DQA_website/ .},
  archive      = {J_TVCG},
  author       = {Shuquan Ye and Dongdong Chen and Songfang Han and Jing Liao},
  doi          = {10.1109/TVCG.2022.3225327},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1772-1786},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {3D question answering},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The risks of ranking: Revisiting graphical perception to
model individual differences in visualization performance.
<em>TVCG</em>, <em>30</em>(3), 1756–1771. (<a
href="https://doi.org/10.1109/TVCG.2022.3226463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphical perception studies typically measure visualization encoding effectiveness using the error of an “average observer”, leading to canonical rankings of encodings for numerical attributes: e.g., position $&amp;gt;$ area $&amp;gt;$ angle $&amp;gt;$ volume. Yet different people may vary in their ability to read different visualization types, leading to variance in this ranking across individuals not captured by population-level metrics using “average observer” models. One way we can bridge this gap is by recasting classic visual perception tasks as tools for assessing individual performance, in addition to overall visualization performance. In this article we replicate and extend Cleveland and McGill&#39;s graphical comparison experiment using Bayesian multilevel regression, using these models to explore individual differences in visualization skill from multiple perspectives. The results from experiments and modeling indicate that some people show patterns of accuracy that credibly deviate from the canonical rankings of visualization effectiveness. We discuss implications of these findings, such as a need for new ways to communicate visualization effectiveness to designers, how patterns in individuals’ responses may show systematic biases and strategies in visualization judgment, and how recasting classic visual perception tasks as tools for assessing individual performance may offer new ways to quantify aspects of visualization literacy. Experiment data, source code, and analysis scripts are available at the following repository: https://osf.io/8ub7t/?view_only=9be4798797404a4397be3c6fc2a68cc0 .},
  archive      = {J_TVCG},
  author       = {Russell Davis and Xiaoying Pu and Yiren Ding and Brian D. Hall and Karen Bonilla and Mi Feng and Matthew Kay and Lane Harrison},
  doi          = {10.1109/TVCG.2022.3226463},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1756-1771},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The risks of ranking: Revisiting graphical perception to model individual differences in visualization performance},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vox-surf: Voxel-based implicit surface representation.
<em>TVCG</em>, <em>30</em>(3), 1743–1755. (<a
href="https://doi.org/10.1109/TVCG.2022.3225844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual content creation and interaction play an important role in modern 3D applications. Recovering detailed 3D models from real scenes can significantly expand the scope of its applications and has been studied for decades in the computer vision and computer graphics community. In this work, we propose Vox-Surf, a voxel-based implicit surface representation. Our Vox-Surf divides the space into finite sparse voxels, where each voxel is a basic geometry unit that stores geometry and appearance information on its corner vertices. Due to the sparsity inherited from the voxel representation, Vox-Surf is suitable for almost any scene and can be easily trained end-to-end from multiple view images. We utilize a progressive training process to gradually cull out empty voxels and keep only valid voxels for further optimization, which greatly reduces the number of sample points and improves inference speed. Experiments show that our Vox-Surf representation can learn fine surface details and accurate colors with less memory and faster rendering than previous methods. The resulting fine voxels can also be considered as the bounding volumes for collision detection, which is useful in 3D interactions. We also show the potential application of Vox-Surf in scene editing and augmented reality. The source code is publicly available at https://github.com/zju3dv/Vox-Surf .},
  archive      = {J_TVCG},
  author       = {Hai Li and Xingrui Yang and Hongjia Zhai and Yuqian Liu and Hujun Bao and Guofeng Zhang},
  doi          = {10.1109/TVCG.2022.3225844},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1743-1755},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Vox-surf: Voxel-based implicit surface representation},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Marching windows: Scalable mesh generation for volumetric
data with multiple materials. <em>TVCG</em>, <em>30</em>(3), 1728–1742.
(<a href="https://doi.org/10.1109/TVCG.2022.3225526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Volumetric data abounds in medical imaging and other fields. With the improved imaging quality and the increased resolution, volumetric datasets are getting so large that the existing tools have become inadequate for processing and analyzing the data. Here we consider the problem of computing tetrahedral meshes to represent large volumetric datasets with labeled multiple materials, which are often encountered in medical imaging or microscopy optical slice tomography. Such tetrahedral meshes are a more compact and expressive geometric representation so are in demand for efficient visualization and simulation of the data, which are impossible if the original large volumetric data are used directly due to the large memory requirement. Existing methods for meshing volumetric data are not scalable for handling large datasets due to their sheer demand on excessively large run-time memory or failure to produce a tet-mesh that preserves the multi-material structure of the original volumetric data. In this article we propose a novel approach, called Marching Windows , that uses a moving window and a disk-swap strategy to reduce the run-time memory footprint, devise a new scheme that guarantees to preserve the topological structure of the original dataset, and adopt an error-guided optimization technique to improve both geometric approximation error and mesh quality. Extensive experiments show that our method is capable of processing very large volumetric datasets beyond the capability of the existing methods and producing tetrahedral meshes of high quality.},
  archive      = {J_TVCG},
  author       = {Wenhua Zhang and Yating Yue and Hao Pan and Zhonggui Chen and Chuan Wang and Hanspeter Pfister and Wenping Wang},
  doi          = {10.1109/TVCG.2022.3225526},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1728-1742},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Marching windows: Scalable mesh generation for volumetric data with multiple materials},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DMT-EV: An explainable deep network for dimension reduction.
<em>TVCG</em>, <em>30</em>(3), 1710–1727. (<a
href="https://doi.org/10.1109/TVCG.2022.3223399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimension reduction (DR) is commonly utilized to capture the intrinsic structure and transform high-dimensional data into low-dimensional space while retaining meaningful properties of the original data. It is used in various applications, such as image recognition, single-cell sequencing analysis, and biomarker discovery. However, contemporary parametric-free and parametric DR techniques suffer from several significant shortcomings, such as the inability to preserve global and local features and the poor generalisation performance. On the other hand, regarding explainability, it is crucial to comprehend the embedding process, especially the contribution of each part to the embedding process, while understanding how each feature affects the embedding results that identify critical components and help diagnose the embedding process. To address these problems, we have developed a deep neural network method called DMT-EV, which provides not only excellent performance in structural maintainability but also explainability to the DR therein. DMT-EV starts with data augmentation and a manifold-based loss function to improve embedding performance. The explanation is based on saliency maps and aims to examine the trained DMT-EV parameters and contributions of components during the embedding process. The proposed techniques are integrated with a visual interface to help the user to adjust DMT-EV to achieve better DR performance and explainability. The interactive visual interface makes it easier to illustrate the data features, compare different DR techniques, and investigate DR. An in-depth experimental comparison shows that DMT-EV consistently outperforms the state-of-the-art methods in both performance measures and explainability.},
  archive      = {J_TVCG},
  author       = {Zelin Zang and Shenghui Cheng and Hanchen Xia and Liangyu Li and Yaoting Sun and Yongjie Xu and Lei Shang and Baigui Sun and Stan Z. Li},
  doi          = {10.1109/TVCG.2022.3223399},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1710-1727},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DMT-EV: An explainable deep network for dimension reduction},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transferable virtual-physical environmental alignment with
redirected walking. <em>TVCG</em>, <em>30</em>(3), 1696–1709. (<a
href="https://doi.org/10.1109/TVCG.2022.3224073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several advanced redirected walking techniques have been proposed in recent years to improve natural walking in virtual environments. One active and important research challenge of redirected walking focuses on the alignment of virtual and physical environments by redirection gains. If both environments are aligned, physical objects appear at the same positions as their virtual counterparts. When a user arrives at such a virtual object, she can touch the corresponding physical object providing passive haptic feedback. When multiple transferable virtual or physical target positions exist, the alignment can exploit multiple options, but the process requires more complicated solutions. In this paper, we study the problem of virtual-physical environmental alignment at multiple transferable target positions, and introduce a novel reinforcement learning-based redirected walking method. We design a novel comprehensive reward function that dynamically determines virtual-physical target matching and updates virtual target weights for reward computation. We evaluate our method through various simulated experiments as well as real user tests. The results show that our method obtains less physical distance error for environmental alignment and requires fewer resets than state-of-the-art techniques.},
  archive      = {J_TVCG},
  author       = {Miao Wang and Ze-Yin Chen and Wen-Chuan Cai and Frank Steinicke},
  doi          = {10.1109/TVCG.2022.3224073},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1696-1709},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Transferable virtual-physical environmental alignment with redirected walking},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural subspaces for light fields. <em>TVCG</em>,
<em>30</em>(3), 1685–1695. (<a
href="https://doi.org/10.1109/TVCG.2022.3224674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a framework for compactly representing light field content with the novel concept of neural subspaces. While the recently proposed neural light field representation achieves great compression results by encoding a light field into a single neural network, the unified design is not optimized for the composite structures exhibited in light fields. Moreover, encoding every part of the light field into one network is not ideal for applications that require rapid transmission and decoding. We recognize this problem&#39;s connection to subspace learning. We present a method that uses several small neural networks, specializing in learning the neural subspace for a particular light field segment. Moreover, we propose an adaptive weight sharing strategy among those small networks, improving parameter efficiency. In effect, this strategy enables a concerted way to track the similarity among nearby neural subspaces by leveraging the layered structure of neural networks. Furthermore, we develop a soft-classification technique to enhance the color prediction accuracy of neural representations. Our experimental results show that our method better reconstructs the light field than previous methods on various light field scenes. We further demonstrate its successful deployment on encoding light fields with irregular viewpoint layout and dynamic scene content.},
  archive      = {J_TVCG},
  author       = {Brandon Yushan Feng and Amitabh Varshney},
  doi          = {10.1109/TVCG.2022.3224674},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1685-1695},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Neural subspaces for light fields},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LF2MV: Learning an editable meta-view towards light field
representation. <em>TVCG</em>, <em>30</em>(3), 1672–1684. (<a
href="https://doi.org/10.1109/TVCG.2022.3220773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light fields are 4D scene representations that are typically structured as arrays of views or several directional samples per pixel in a single view. However, this highly correlated structure is not very efficient to transmit and manipulate, especially for editing. To tackle this issue, we propose a novel representation learning framework that can encode the light field into a single meta-view that is both compact and editable. Specifically, the meta-view composes of three visual channels and a complementary meta channel that is embedded with geometric and residual appearance information. The visual channels can be edited using existing 2D image editing tools, before reconstructing the whole edited light field. To facilitate edit propagation against occlusion, we design a special editing-aware decoding network that consistently propagates the visual edits to the whole light field upon reconstruction. Extensive experiments show that our proposed method achieves competitive representation accuracy and meanwhile enables consistent edit propagation.},
  archive      = {J_TVCG},
  author       = {Menghan Xia and Jose Echevarria and Minshan Xie and Tien-Tsin Wong},
  doi          = {10.1109/TVCG.2022.3220773},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1672-1684},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LF2MV: Learning an editable meta-view towards light field representation},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of smooth vector graphics: Recent advances in repr
esentation, creation, rasterization, and image vectorization.
<em>TVCG</em>, <em>30</em>(3), 1652–1671. (<a
href="https://doi.org/10.1109/TVCG.2022.3220575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of smooth vector graphics explores the representation, creation, rasterization, and automatic generation of light-weight image representations, frequently used for scalable image content. Over the past decades, several conceptual approaches on the representation of images with smooth gradients have emerged that each led to separate research threads, including the popular gradient meshes and diffusion curves. As the computational models matured, the mathematical descriptions diverged and article started to focus more narrowly on subproblems, such as on the representation and creation of vector graphics, or the automatic vectorization from raster images. Most of the work concentrated on a specific mathematical model only. With this survey, we describe the established computational models in a consistent notation to spur further knowledge transfer, leveraging the recent advances in each field. We therefore categorize vector graphics article from the last decades based on their underlying mathematical representations as well as on their contribution to the vector graphics content creation pipeline, comprising representation, creation, rasterization, and automatic image vectorization. This survey is meant as an entry point for both artists and researchers. We conclude this survey with an outlook on promising research directions and challenges to overcome in the future.},
  archive      = {J_TVCG},
  author       = {Xingze Tian and Tobias Günther},
  doi          = {10.1109/TVCG.2022.3220575},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1652-1671},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A survey of smooth vector graphics: Recent advances in repr esentation, creation, rasterization, and image vectorization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wasserstein dictionaries of persistence diagrams.
<em>TVCG</em>, <em>30</em>(2), 1638–1651. (<a
href="https://doi.org/10.1109/TVCG.2023.3330262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a computational framework for the concise encoding of an ensemble of persistence diagrams, in the form of weighted Wasserstein barycenters Turner et al. (2014), Vidal et al. (2020) of a dictionary of atom diagrams . We introduce a multi-scale gradient descent approach for the efficient resolution of the corresponding minimization problem, which interleaves the optimization of the barycenter weights with the optimization of the atom diagrams . Our approach leverages the analytic expressions for the gradient of both sub-problems to ensure fast iterations and it additionally exploits shared-memory parallelism. Extensive experiments on public ensembles demonstrate the efficiency of our approach, with Wasserstein dictionary computations in the orders of minutes for the largest examples. We show the utility of our contributions in two applications. First, we apply Wassserstein dictionaries to data reduction and reliably compress persistence diagrams by concisely representing them with their weights in the dictionary. Second, we present a dimensionality reduction framework based on a Wasserstein dictionary defined with a small number of atoms (typically three) and encode the dictionary as a low dimensional simplex embedded in a visual space (typically in 2D). In both applications, quantitative experiments assess the relevance of our framework. Finally, we provide a C++ implementation that can be used to reproduce our results.},
  archive      = {J_TVCG},
  author       = {Keanu Sisouk and Julie Delon and Julien Tierny},
  doi          = {10.1109/TVCG.2023.3330262},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  number       = {2},
  pages        = {1638-1651},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Wasserstein dictionaries of persistence diagrams},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Eliciting multimodal and collaborative interactions for data
exploration on large vertical displays. <em>TVCG</em>, <em>30</em>(2),
1624–1637. (<a href="https://doi.org/10.1109/TVCG.2023.3323150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examined user preferences to combine multiple interaction modalities for collaborative interaction with data shown on large vertical displays. Large vertical displays facilitate visual data exploration and allow the use of diverse interaction modalities by multiple users at different distances from the screen. Yet, how to offer multiple interaction modalities is a non-trivial problem. We conducted an elicitation study with 20 participants that generated 1015 interaction proposals combining touch, speech, pen, and mid-air gestures. Given the opportunity to interact using these four modalities, participants preferred speech interaction in 10 of 15 low-level tasks and direct manipulation for straightforward tasks such as showing a tooltip or selecting. In contrast to previous work, participants most favored unimodal and personal interactions. We identified what we call collaborative synonyms among their interaction proposals and found that pairs of users collaborated either unimodally and simultaneously or multimodally and sequentially. We provide insights into how end-users associate visual exploration tasks with certain modalities and how they collaborate at different interaction distances using specific interaction modalities. 1},
  archive      = {J_TVCG},
  author       = {Gabriela Molina León and Petra Isenberg and Andreas Breiter},
  doi          = {10.1109/TVCG.2023.3323150},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  number       = {2},
  pages        = {1624-1637},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Eliciting multimodal and collaborative interactions for data exploration on large vertical displays},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive sampling of 3D spatial correlations for
focus+context visualization. <em>TVCG</em>, <em>30</em>(2), 1608–1623.
(<a href="https://doi.org/10.1109/TVCG.2023.3326855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualizing spatial correlations in 3D ensembles is challenging due to the vast amounts of information that need to be conveyed. Memory and time constraints make it unfeasible to pre-compute and store the correlations between all pairs of domain points. We propose the embedding of adaptive correlation sampling into chord diagrams with hierarchical edge bundling to alleviate these constraints. Entities representing spatial regions are arranged along the circular chord layout via a space-filling curve, and Bayesian optimal sampling is used to efficiently estimate the maximum occurring correlation between any two points from different regions. Hierarchical edge bundling reduces visual clutter and emphasizes the major correlation structures. By selecting an edge, the user triggers a focus diagram in which only the two regions connected via this edge are refined and arranged in a specific way in a second chord layout. For visualizing correlations between two different variables, which are not symmetric anymore, we switch to showing a full correlation matrix. This avoids drawing the same edges twice with different correlation values. We introduce GPU implementations of both linear and non-linear correlation measures to further reduce the time that is required to generate the context and focus views, and to even enable the analysis of correlations in a 1000-member ensemble.},
  archive      = {J_TVCG},
  author       = {Christoph Neuhauser and Josef Stumpfegger and Rüdiger Westermann},
  doi          = {10.1109/TVCG.2023.3326855},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  number       = {2},
  pages        = {1608-1623},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Adaptive sampling of 3D spatial correlations for Focus+Context visualization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GeoLinter: A linting framework for choropleth maps.
<em>TVCG</em>, <em>30</em>(2), 1592–1607. (<a
href="https://doi.org/10.1109/TVCG.2023.3322372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualization linting is a proven effective tool in assisting users to follow established visualization guidelines. Despite its success, visualization linting for choropleth maps, one of the most popular visualizations on the internet, has yet to be investigated. In this paper, we present GeoLinter, a linting framework for choropleth maps that assists in creating accurate and robust maps. Based on a set of design guidelines and metrics drawing upon a collection of best practices from the cartographic literature, GeoLinter detects potentially suboptimal design decisions and provides further recommendations on design improvement with explanations at each step of the design process. We perform a validation study to evaluate the proposed framework&#39;s functionality with respect to identifying and fixing errors and apply its results to improve the robustness of GeoLinter. Finally, we demonstrate the effectiveness of the GeoLinter - validated through empirical studies - by applying it to a series of case studies using real-world datasets.},
  archive      = {J_TVCG},
  author       = {Fan Lei and Arlen Fan and Alan M. MacEachren and Ross Maciejewski},
  doi          = {10.1109/TVCG.2023.3322372},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  number       = {2},
  pages        = {1592-1607},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GeoLinter: A linting framework for choropleth maps},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). V4D: Voxel for 4D novel view synthesis. <em>TVCG</em>,
<em>30</em>(2), 1579–1591. (<a
href="https://doi.org/10.1109/TVCG.2023.3312127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural radiance fields have made a remarkable breakthrough in the novel view synthesis task at the 3D static scene. However, for the 4D circumstance (e.g., dynamic scene), the performance of the existing method is still limited by the capacity of the neural network, typically in a multilayer perceptron network (MLP). In this article, we utilize 3D Voxel to model the 4D neural radiance field, short as V4D, where the 3D voxel has two formats. The first one is to regularly model the 3D space and then use the sampled local 3D feature with the time index to model the density field and the texture field by a tiny MLP. The second one is in look-up tables (LUTs) format that is for the pixel-level refinement, where the pseudo-surface produced by the volume rendering is utilized as the guidance information to learn a 2D pixel-level refinement mapping. The proposed LUTs-based refinement module achieves the performance gain with little computational cost and could serve as the plug-and-play module in the novel view synthesis task. Moreover, we propose a more effective conditional positional encoding toward the 4D data that achieves performance gain with negligible computational burdens. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance at a low computational cost.},
  archive      = {J_TVCG},
  author       = {Wanshui Gan and Hongbin Xu and Yi Huang and Shifeng Chen and Naoto Yokoya},
  doi          = {10.1109/TVCG.2023.3312127},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  number       = {2},
  pages        = {1579-1591},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {V4D: Voxel for 4D novel view synthesis},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A scalable method for readable tree layouts. <em>TVCG</em>,
<em>30</em>(2), 1564–1578. (<a
href="https://doi.org/10.1109/TVCG.2023.3274572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large tree structures are ubiquitous and real-world relational datasets often have information associated with nodes (e.g., labels or other attributes) and edges (e.g., weights or distances) that need to be communicated to the viewers. Yet, scalable, easy to read tree layouts are difficult to achieve. We consider tree layouts to be readable if they meet some basic requirements: node labels should not overlap, edges should not cross, edge lengths should be preserved, and the output should be compact. There are many algorithms for drawing trees, although very few take node labels or edge lengths into account, and none optimizes all requirements above. With this in mind, we propose a new scalable method for readable tree layouts. The algorithm guarantees that the layout has no edge crossings and no label overlaps, and optimizes one of the remaining aspects: desired edge lengths and compactness. We evaluate the performance of the new algorithm by comparison with related earlier approaches using several real-world datasets, ranging from a few thousand nodes to hundreds of thousands of nodes. Tree layout algorithms can be used to visualize large general graphs, by extracting a hierarchy of progressively larger trees. We illustrate this functionality by presenting several map-like visualizations generated by the new tree layout algorithm.},
  archive      = {J_TVCG},
  author       = {Kathryn Gray and Mingwei Li and Reyan Ahmed and Md. Khaledur Rahman and Ariful Azad and Stephen Kobourov and Katy Börner},
  doi          = {10.1109/TVCG.2023.3274572},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  number       = {2},
  pages        = {1564-1578},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A scalable method for readable tree layouts},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multivariate data explanation by jumping emerging patterns
visualization. <em>TVCG</em>, <em>30</em>(2), 1549–1563. (<a
href="https://doi.org/10.1109/TVCG.2022.3223529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate or multidimensional visualization plays an essential role in exploratory data analysis by allowing users to derive insights and formulate hypotheses. Despite their popularity, it is usually users’ responsibility to (visually) discover the data patterns, which can be cumbersome and time-consuming. Visual Analytics (VA) and machine learning techniques can be instrumental in mitigating this problem by automatically discovering and representing such patterns. One example is the integration of classification models with (visual) interpretability strategies, where models are used as surrogates for data patterns so that understanding a model enables understanding the phenomenon represented by the data. Although useful and inspiring, the few proposed solutions are based on visual representations of so-called black-box models, so the interpretation of the patterns captured by the models is not straightforward, requiring mechanisms to transform them into human-understandable pieces of information. This paper presents multiVariate dAta eXplanation (VAX) , a new VA method to support identifying and visual interpreting patterns in multivariate datasets. Unlike the existing similar approaches, VAX uses the concept of Jumping Emerging Patterns, inherent interpretable logic statements representing class-variable relationships (patterns) derived from random Decision Trees. The potential of VAX is shown through use cases employing two real-world datasets covering different scenarios where intricate patterns are discovered and represented, something challenging to be done using usual exploratory approaches.},
  archive      = {J_TVCG},
  author       = {Mário Popolin Neto and Fernando V. Paulovich},
  doi          = {10.1109/TVCG.2022.3223529},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  number       = {2},
  pages        = {1549-1563},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multivariate data explanation by jumping emerging patterns visualization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DocFlow: A visual analytics system for question-based
document retrieval and categorization. <em>TVCG</em>, <em>30</em>(2),
1533–1548. (<a href="https://doi.org/10.1109/TVCG.2022.3219762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A systematic review (SR) is essential with up-to-date research evidence to support clinical decisions and practices. However, the growing literature volume makes it challenging for SR reviewers and clinicians to discover useful information efficiently. Many human-in-the-loop information retrieval approaches (HIR) have been proposed to rank documents semantically similar to users’ queries and provide interactive visualizations to facilitate document retrieval. Given that the queries are mainly composed of keywords and keyphrases retrieving documents that are semantically similar to a query does not necessarily respond to the clinician&#39;s need. Clinicians still have to review many documents to find the solution. The problem motivates us to develop a visual analytics system, DocFlow, to facilitate information-seeking. One of the features of our DocFlow is accepting natural language questions. The detailed description enables retrieving documents that can answer users’ questions. Additionally, clinicians often categorize documents based on their backgrounds and with different purposes (e.g., populations, treatments). Since the criteria are unknown and cannot be pre-defined in advance, existing methods can only achieve categorization by considering the entire information in documents. In contrast, by locating answers in each document, our DocFlow can intelligently categorize documents based on users’ questions. The second feature of our DocFlow is a flexible interface where users can arrange a sequence of questions to customize their rules for document retrieval and categorization. The two features of this visual analytics system support a flexible information-seeking process. The case studies and the feedback from domain experts demonstrate the usefulness and effectiveness of our DocFlow.},
  archive      = {J_TVCG},
  author       = {Rui Qiu and Yamei Tu and Yu-Shuen Wang and Po-Yin Yen and Han-Wei Shen},
  doi          = {10.1109/TVCG.2022.3219762},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  number       = {2},
  pages        = {1533-1548},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DocFlow: A visual analytics system for question-based document retrieval and categorization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward efficient deep learning for graph drawing (DL4GD).
<em>TVCG</em>, <em>30</em>(2), 1516–1532. (<a
href="https://doi.org/10.1109/TVCG.2022.3222186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to their great performance in many challenges, Deep Learning (DL) techniques keep gaining popularity in many fields. They have been adapted to process graph data structures to solve various complicated tasks such as graph classification and edge prediction. Eventually, they reached the Graph Drawing (GD) task. This article is an extended version of the previously published (DNN)2 and presents a framework to leverage DL techniques for graph drawing (DL4GD). We demonstrate how it is possible to train a Deep Learning model to extract features from a graph and project them into a graph layout. The method proposes to leverage efficient Convolutional Neural Networks, adapting them to graphs using Graph Convolutions. The graph layout projection is learned by optimizing a cost function that does not require any ground truth layout, as opposed to prior work. This paper also proposes an implementation and benchmark of the framework to study its sensitivity to certain deep learning-related conditions. As the field is novel, and many questions remain to be answered, we do not focus on finding the most optimal implementation of the method, but rather contribute toward a better understanding of the approach potential. More precisely, we study different learning strategies relative to the models training datasets. Finally, we discuss the main advantages and limitations of DL4GD.},
  archive      = {J_TVCG},
  author       = {Loann Giovannangeli and Frederic Lalanne and David Auber and Romain Giot and Romain Bourqui},
  doi          = {10.1109/TVCG.2022.3222186},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  number       = {2},
  pages        = {1516-1532},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Toward efficient deep learning for graph drawing (DL4GD)},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The transform-and-perform framework: Explainable deep
learning beyond classification. <em>TVCG</em>, <em>30</em>(2),
1502–1515. (<a href="https://doi.org/10.1109/TVCG.2022.3219248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, visual analytics (VA) has shown promise in alleviating the challenges of interpreting black-box deep learning (DL) models. While the focus of VA for explainable DL has been mainly on classification problems, DL is gaining popularity in high-dimensional-to-high-dimensional ( H-H ) problems such as image-to-image translation. In contrast to classification, H-H problems have no explicit instance groups or classes to study. Each output is continuous, high-dimensional, and changes in an unknown non-linear manner with changes in the input. These unknown relations between the input, model and output necessitate the user to analyze them in conjunction, leveraging symmetries between them. Since classification tasks do not exhibit some of these challenges, most existing VA systems and frameworks allow limited control of the components required to analyze models beyond classification. Hence, we identify the need for and present a unified conceptual framework, the Transform-and-Perform framework ( T&amp;amp;P ), to facilitate the design of VA systems for DL model analysis focusing on H-H problems. T&amp;amp;P provides a checklist to structure and identify workflows and analysis strategies to design new VA systems, and understand existing ones to uncover potential gaps for improvements. The goal is to aid the creation of effective VA systems that support the structuring of model understanding and identifying actionable insights for model improvements. We highlight the growing need for new frameworks like T&amp;amp;P with a real-world image-to-image translation application. We illustrate how T&amp;amp;P effectively supports the understanding and identification of potential gaps in existing VA systems.},
  archive      = {J_TVCG},
  author       = {Vidya Prasad and Ruud J. G. van Sloun and Stef van den Elzen and Anna Vilanova and Nicola Pezzotti},
  doi          = {10.1109/TVCG.2022.3219248},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  number       = {2},
  pages        = {1502-1515},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The transform-and-perform framework: Explainable deep learning beyond classification},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal coherence-based distributed ray tracing of massive
scenes. <em>TVCG</em>, <em>30</em>(2), 1489–1501. (<a
href="https://doi.org/10.1109/TVCG.2022.3219982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed ray tracing algorithms are widely used when rendering massive scenes, where data utilization and load balancing are the keys to improving performance. One essential observation is that rays are temporally coherent, which indicates that temporal information can be used to improve computational efficiency. In this paper, we use temporal coherence to optimize the performance of distributed ray tracing. First, we propose a temporal coherence-based scheduling algorithm to guide the task/data assignment and scheduling. Then, we propose a virtual portal structure to predict the radiance of rays based on the previous frame, and send the rays with low radiance to a precomputed simplified model for further tracing, which can dramatically reduce the traversal complexity and the overhead of network data transmission. The approach was validated on scenes of sizes up to 355 GB. Our algorithm can achieve a speedup of up to 81% compared to previous algorithms, with a very small mean squared error.},
  archive      = {J_TVCG},
  author       = {Xiang Xu and Lu Wang and Arsène Pérard-Gayot and Richard Membarth and Cuiyu Li and Chenglei Yang and Philipp Slusallek},
  doi          = {10.1109/TVCG.2022.3219982},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  number       = {2},
  pages        = {1489-1501},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Temporal coherence-based distributed ray tracing of massive scenes},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual exploration of machine learning model behavior with
hierarchical surrogate rule sets. <em>TVCG</em>, <em>30</em>(2),
1470–1488. (<a href="https://doi.org/10.1109/TVCG.2022.3219232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the potential solutions for model interpretation is to train a surrogate model: a more transparent model that approximates the behavior of the model to be explained. Typically, classification rules or decision trees are used due to their logic-based expressions. However, decision trees can grow too deep, and rule sets can become too large to approximate a complex model. Unlike paths on a decision tree that must share ancestor nodes (conditions), rules are more flexible. However, the unstructured visual representation of rules makes it hard to make inferences across rules. In this paper, we focus on tabular data and present novel algorithmic and interactive solutions to address these issues. First, we present H ierarchical S urrogate R ules (HSR), an algorithm that generates hierarchical rules based on user-defined parameters. We also contribute SuRE , a visual analytics (VA) system that integrates HSR and an interactive surrogate rule visualization, the Feature-Aligned Tree , which depicts rules as trees while aligning features for easier comparison. We evaluate the algorithm in terms of parameter sensitivity, time performance, and comparison with surrogate decision trees and find that it scales reasonably well and overcomes the shortcomings of surrogate decision trees. We evaluate the visualization and the system through a usability study and an observational study with domain experts. Our investigation shows that the participants can use feature-aligned trees to perform non-trivial tasks with very high accuracy. We also discuss many interesting findings, including a rule analysis task characterization, that can be used for visualization design and future research.},
  archive      = {J_TVCG},
  author       = {Jun Yuan and Brian Barr and Kyle Overton and Enrico Bertini},
  doi          = {10.1109/TVCG.2022.3219232},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  number       = {2},
  pages        = {1470-1488},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual exploration of machine learning model behavior with hierarchical surrogate rule sets},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tooth alignment network based on landmark constraints and
hierarchical graph structure. <em>TVCG</em>, <em>30</em>(2), 1457–1469.
(<a href="https://doi.org/10.1109/TVCG.2022.3218028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic tooth alignment target prediction is vital in shortening the planning time of orthodontic treatments and aligner designs. Generally, the quality of alignment targets greatly depends on the experience and ability of dentists and has enormous subjective factors. Therefore, many knowledge-driven alignment prediction methods have been proposed to help inexperienced dentists. Unfortunately, existing methods tend to directly regress tooth motion, which lacks clinical interpretability. Tooth anatomical landmarks play a critical role in orthodontics because they are effective in aiding the assessment of whether teeth are in close arrangement and normal occlusion. Thus, we consider anatomical landmark constraints to improve tooth alignment results. In this article, we present a novel tooth alignment neural network for alignment target predictions based on tooth landmark constraints and a hierarchical graph structure. We detect the landmarks of each tooth first and then construct a hierarchical graph of jaw-tooth-landmark to characterize the relationship between teeth and landmarks. Then, we define the landmark constraints to guide the network to learn the normal occlusion and predict the rigid transformation of each tooth during alignment. Our method achieves better results with the architecture built for tooth data and landmark constraints and has better explainability than previous methods with regard to clinical tooth alignments.},
  archive      = {J_TVCG},
  author       = {Chen Wang and Guangshun Wei and Guodong Wei and Wenping Wang and Yuanfeng Zhou},
  doi          = {10.1109/TVCG.2022.3218028},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  number       = {2},
  pages        = {1457-1469},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Tooth alignment network based on landmark constraints and hierarchical graph structure},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Preface. <em>TVCG</em>, <em>30</em>(1), xvii–xxvi. (<a
href="https://doi.org/10.1109/TVCG.2023.3325958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This January 2024 issue of the IEEE Transactions on Visualization and Computer Graphics (TVCG) contains the proceedings of IEEE VIS 2023, held in an in person format on October 22–27 October, 2023 in Melbourne, Australia, with three General Chairs from Monash University: Tim Dwyer, Sarah Goodwin, and Michael Wybrow. With IEEE VIS 2023, the conference series is in its 34th year.},
  archive      = {J_TVCG},
  doi          = {10.1109/TVCG.2023.3325958},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {xvii-xxvi},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Preface},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Welcome. <em>TVCG</em>, <em>30</em>(1), xv–xvi. (<a
href="https://doi.org/10.1109/TVCG.2023.3326051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are thrilled this year to welcome the IEEE VIS conference for its first ever visit to Australia. It is, for that matter, the first time for this conference to come to the Southern Hemisphere, as well as its first time in an Asian time zone. We acknowledge Traditional Owners of Narrm, the Boon Wurrung and Wurundjeri Woi Wurrung people of the Kulin nation, on whose land VIS 2023 will be held. We pay our respects to their Elders past, present and emerging. We know Melbourne (Narrm) is a long way from Europe and the US, as we&#39;ve been doing the reverse trip ourselves for many years. But we are grateful to all from those regions who are able to join us, and we are delighted to welcome many new participants from the Asia-Pacific region who are now able to attend in-person for the first time. This sojourn down-under marks a significant milestone for IEEE VIS towards becoming a truly global conference.},
  archive      = {J_TVCG},
  author       = {Tim Dwyer and Sarah Goodwin and Michael Wybrow},
  doi          = {10.1109/TVCG.2023.3326051},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {xv-xvi},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Welcome},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Message from the editor-in-chief. <em>TVCG</em>,
<em>30</em>(1), xiv. (<a
href="https://doi.org/10.1109/TVCG.2023.3326046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Welcome to the January 2024 issue of the IEEE Transactions on Visualization and Computer Graphics ( $TVCG$ ). This is my first IEEE Vis special issue as Editor-in-Chief, and I am verry excited to introduce it to you all. As in the previous two years, the papers submitted to IEEE VIS were categorized into six major research subareas: Theoretical &amp; Empirical (123 papers), Applications (123), Systems &amp; Rendering (57), Representations &amp; Interaction (86), Data Transformations (68) and Analytics &amp; Decisions (82). The conference took place in the vibrant city of Melbourne, Australia during October 22–27, 2023. Included in this special issue are the top 133 papers selected by the Program Committee from a total of 539 submissions.},
  archive      = {J_TVCG},
  author       = {Han-Wei Shen},
  doi          = {10.1109/TVCG.2023.3326046},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {xiv},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Message from the editor-in-chief},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Polarizing political polls: How visualization design choices
can shape public opinion and increase political polarization.
<em>TVCG</em>, <em>30</em>(1), 1446–1456. (<a
href="https://doi.org/10.1109/TVCG.2023.3326512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While we typically focus on data visualization as a tool for facilitating cognitive tasks (e.g. learning facts, making decisions), we know relatively little about their second-order impacts on our opinions, attitudes, and values. For example, could design or framing choices interact with viewers&#39; social cognitive biases in ways that promote political polarization? When reporting on U.S. attitudes toward public policies, it is popular to highlight the gap between Democrats and Republicans (e.g. with blue vs red connected dot plots). But these charts may encourage social-normative conformity, influencing viewers&#39; attitudes to match the divided opinions shown in the visualization. We conducted three experiments examining visualization framing in the context of social conformity and polarization. Crowdworkers viewed charts showing simulated polling results for public policy proposals. We varied framing (aggregating data as non-partisan “All US Adults,” or partisan “Democrat” / “Republican”) and the visualized groups&#39; support levels. Participants then reported their own support for each policy. We found that participants&#39; attitudes biased significantly toward the group attitudes shown in the stimuli and this can increase inter-party attitude divergence. These results demonstrate that data visualizations can induce social conformity and accelerate political polarization. Choosing to visualize partisan divisions can divide us further.},
  archive      = {J_TVCG},
  author       = {Eli Holder and Cindy Xiong Bearfield},
  doi          = {10.1109/TVCG.2023.3326512},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1446-1456},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Polarizing political polls: How visualization design choices can shape public opinion and increase political polarization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enthusiastic and grounded, avoidant and cautious:
Understanding public receptivity to data and visualizations.
<em>TVCG</em>, <em>30</em>(1), 1435–1445. (<a
href="https://doi.org/10.1109/TVCG.2023.3326917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite an abundance of open data initiatives aimed to inform and empower “general” audiences, we still know little about the ways people outside of traditional data analysis communities experience and engage with public data and visualizations. To investigate this gap, we present results from an in-depth qualitative interview study with 19 participants from diverse ethnic, occupational, and demographic backgrounds. Our findings characterize a set of lived experiences with open data and visualizations in the domain of energy consumption, production, and transmission. This work exposes information receptivity — an individual&#39;s transient state of willingness or openness to receive information —as a blind spot for the data visualization community, complementary to but distinct from previous notions of data visualization literacy and engagement. We observed four clusters of receptivity responses to data- and visualization-based rhetoric: Information-Avoidant, Data-Cautious, Data-Enthusiastic, and Domain-Grounded. Based on our findings, we highlight research opportunities for the visualization community. This exploratory work identifies the existence of diverse receptivity responses, highlighting the need to consider audiences with varying levels of openness to new information. Our findings also suggest new approaches for improving the accessibility and inclusivity of open data and visualization initiatives targeted at broad audiences. A free copy of this paper and all supplemental materials are available at https://OSF.IO/MPQ32 .},
  archive      = {J_TVCG},
  author       = {Helen Ai He and Jagoda Walny and Sonja Thoma and Sheelagh Carpendale and Wesley Willett},
  doi          = {10.1109/TVCG.2023.3326917},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1435-1445},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Enthusiastic and grounded, avoidant and cautious: Understanding public receptivity to data and visualizations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Embellishments revisited: Perceptions of embellished
visualisations through the viewer’s lens. <em>TVCG</em>, <em>30</em>(1),
1424–1434. (<a href="https://doi.org/10.1109/TVCG.2023.3326914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embellishments are features commonly used in everyday visualisations which are demonstrated to enhance assimilation and memorability. Despite their popularity, little is known about their impact on enticing readers to explore visualisations. To address this gap, we conducted 18 interviews with a diverse group of participants who were consumers of news media but non-experts in visualisation and design. Participants were shown ten embellished and plain visualisations collected from the news and asked to rank them based on enticement and ease of understanding. Extending prior work, our interview results suggest that visualisations with multiple embellishment types might make a visualisation perceived as more enticing. An important finding from our study is that the widespread of certain embellishments in the media might have made them part of visualisation conventions, making a visualisation appear more objective but less enticing. Based on these findings, we ran a follow-up online user study showing participants variations of the visualisations with multiple embellishments to isolate each embellishment type and investigate its effect. We found that variations with salient embellishments were perceived as more enticing. We argue that to unpack the concept of embellishments; we must consider two factors: embellishment saliency and editorial styles. Our study contributes concept and design considerations to the literature concerned with visualisation design for non-experts in visualisation and design.},
  archive      = {J_TVCG},
  author       = {Muna Alebri and Enrico Costanza and Georgia Panagiotidou and Duncan P. Brumby},
  doi          = {10.1109/TVCG.2023.3326914},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1424-1434},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Embellishments revisited: Perceptions of embellished visualisations through the viewer&#39;s lens},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From shock to shift: Data visualization for constructive
climate journalism. <em>TVCG</em>, <em>30</em>(1), 1413–1423. (<a
href="https://doi.org/10.1109/TVCG.2023.3327185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a multi-dimensional, multi-level, and multi-channel approach to data visualization for the purpose of constructive climate journalism. Data visualization has assumed a central role in environmental journalism and is often used in data stories to convey the dramatic consequences of climate change and other ecological crises. However, the emphasis on the catastrophic impacts of climate change tends to induce feelings of fear, anxiety, and apathy in readers. Climate mitigation, adaptation, and protection—all highly urgent in the face of the climate crisis—are at risk of being overlooked. These topics are more difficult to communicate as they are hard to convey on varying levels of locality, involve multiple interconnected sectors, and need to be mediated across various channels from the printed newspaper to social media platforms. So far, there has been little research on data visualization to enhance affective engagement with data about climate protection as part of solution-oriented reporting of climate change. With this research we characterize the unique challenges of constructive climate journalism for data visualization and share findings from a research and design study in collaboration with a national newspaper in Germany. Using the affordances and aesthetics of travel postcards, we present Klimakarten, a data journalism project on the progress of climate protection at multiple spatial scales (from national to local), across five key sectors (agriculture, buildings, energy, mobility, and waste), and for print and online use. The findings from quantitative and qualitative analysis of reader feedback confirm our overall approach and suggest implications for future work.},
  archive      = {J_TVCG},
  author       = {Francesca Morini and Anna Eschenbacher and Johanna Hartmann and Marian Dörk},
  doi          = {10.1109/TVCG.2023.3327185},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1413-1423},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {From shock to shift: Data visualization for constructive climate journalism},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The urban toolkit: A grammar-based framework for urban
visual analytics. <em>TVCG</em>, <em>30</em>(1), 1402–1412. (<a
href="https://doi.org/10.1109/TVCG.2023.3326598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While cities around the world are looking for smart ways to use new advances in data collection, management, and analysis to address their problems, the complex nature of urban issues and the overwhelming amount of available data have posed significant challenges in translating these efforts into actionable insights. In the past few years, urban visual analytics tools have significantly helped tackle these challenges. When analyzing a feature of interest, an urban expert must transform, integrate, and visualize different thematic (e.g., sunlight access, demographic) and physical (e.g., buildings, street networks) data layers, oftentimes across multiple spatial and temporal scales. However, integrating and analyzing these layers require expertise in different fields, increasing development time and effort. This makes the entire visual data exploration and system implementation difficult for programmers and also sets a high entry barrier for urban experts outside of computer science. With this in mind, in this paper, we present the Urban Toolkit (UTK), a flexible and extensible visualization framework that enables the easy authoring of web-based visualizations through a new high-level grammar specifically built with common urban use cases in mind. In order to facilitate the integration and visualization of different urban data, we also propose the concept of knots to merge thematic and physical urban layers. We evaluate our approach through use cases and a series of interviews with experts and practitioners from different domains, including urban accessibility, urban planning, architecture, and climate science. UTK is available at urbantk.org.},
  archive      = {J_TVCG},
  author       = {Gustavo Moreira and Maryam Hosseini and Md Nafiul Alam Nipu and Marcos Lage and Nivan Ferreira and Fabio Miranda},
  doi          = {10.1109/TVCG.2023.3326598},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1402-1412},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The urban toolkit: A grammar-based framework for urban visual analytics},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GeoExplainer: A visual analytics framework for spatial
modeling contextualization and report generation. <em>TVCG</em>,
<em>30</em>(1), 1391–1401. (<a
href="https://doi.org/10.1109/TVCG.2023.3327359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geographic regression models of various descriptions are often applied to identify patterns and anomalies in the determinants of spatially distributed observations. These types of analyses focus on answering why questions about underlying spatial phenomena, e.g., why is crime higher in this locale, why do children in one school district outperform those in another, etc.? Answers to these questions require explanations of the model structure, the choice of parameters, and contextualization of the findings with respect to their geographic context. This is particularly true for local forms of regression models which are focused on the role of locational context in determining human behavior. In this paper, we present GeoExplainer, a visual analytics framework designed to support analysts in creating explanative documentation that summarizes and contextualizes their spatial analyses. As analysts create their spatial models, our framework flags potential issues with model parameter selections, utilizes template-based text generation to summarize model outputs, and links with external knowledge repositories to provide annotations that help to explain the model results. As analysts explore the model results, all visualizations and annotations can be captured in an interactive report generation widget. We demonstrate our framework using a case study modeling the determinants of voting in the 2016 US Presidential Election.},
  archive      = {J_TVCG},
  author       = {Fan Lei and Yuxin Ma and A. Stewart Fotheringham and Elizabeth A. Mack and Ziqi Li and Mehak Sachdeva and Sarah Bardin and Ross Maciejewski},
  doi          = {10.1109/TVCG.2023.3327359},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1391-1401},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GeoExplainer: A visual analytics framework for spatial modeling contextualization and report generation},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Residency octree: A hybrid approach for scalable web-based
multi-volume rendering. <em>TVCG</em>, <em>30</em>(1), 1380–1390. (<a
href="https://doi.org/10.1109/TVCG.2023.3327193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a hybrid multi-volume rendering approach based on a novel Residency Octree that combines the advantages of out-of-core volume rendering using page tables with those of standard octrees. Octree approaches work by performing hierarchical tree traversal. However, in octree volume rendering, tree traversal and the selection of data resolution are intrinsically coupled. This makes fine-grained empty-space skipping costly. Page tables, on the other hand, allow access to any cached brick from any resolution. However, they do not offer a clear and efficient strategy for substituting missing high-resolution data with lower-resolution data. We enable flexible mixed-resolution out-of-core multi-volume rendering by decoupling the cache residency of multi-resolution data from a resolution-independent spatial subdivision determined by the tree. Instead of one-to-one node-to-brick correspondences, each residency octree node is mapped to a set of bricks from different resolution levels. This makes it possible to efficiently and adaptively choose and mix resolutions, adapt sampling rates, and compensate for cache misses. At the same time, residency octrees support fine-grained empty-space skipping, independent of the data subdivision used for caching. Finally, to facilitate collaboration and outreach, and to eliminate local data storage, our implementation is a web-based, pure client-side renderer using WebGPU and WebAssembly. Our method is faster than prior approaches and efficient for many data channels with a flexible and adaptive choice of data resolution.},
  archive      = {J_TVCG},
  author       = {Lukas Herzberger and Markus Hadwiger and Robert Krüger and Peter Sorger and Hanspeter Pfister and Eduard Gröller and Johanna Beyer},
  doi          = {10.1109/TVCG.2023.3327193},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1380-1390},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Residency octree: A hybrid approach for scalable web-based multi-volume rendering},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differentiable design galleries: A differentiable approach
to explore the design space of transfer functions. <em>TVCG</em>,
<em>30</em>(1), 1369–1379. (<a
href="https://doi.org/10.1109/TVCG.2023.3327371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transfer function is crucial for direct volume rendering (DVR) to create an informative visual representation of volumetric data. However, manually adjusting the transfer function to achieve the desired DVR result can be time-consuming and unintuitive. In this paper, we propose Differentiable Design Galleries, an image-based transfer function design approach to help users explore the design space of transfer functions by taking advantage of the recent advances in deep learning and differentiable rendering. Specifically, we leverage neural rendering to learn a latent design space, which is a continuous manifold representing various types of implicit transfer functions. We further provide a set of interactive tools to support intuitive query, navigation, and modification to obtain the target design, which is represented as a neural-rendered design exemplar. The explicit transfer function can be reconstructed from the target design with a differentiable direct volume renderer. Experimental results on real volumetric data demonstrate the effectiveness of our method.},
  archive      = {J_TVCG},
  author       = {Bo Pan and Jiaying Lu and Haoxuan Li and Weifeng Chen and Yiyao Wang and Minfeng Zhu and Chenhao Yu and Wei Chen},
  doi          = {10.1109/TVCG.2023.3327371},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1369-1379},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Differentiable design galleries: A differentiable approach to explore the design space of transfer functions},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A general framework for progressive data compression and
retrieval. <em>TVCG</em>, <em>30</em>(1), 1358–1368. (<a
href="https://doi.org/10.1109/TVCG.2023.3327186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In scientific simulations, observations, and experiments, the transfer of data to and from disk and across networks has become a major bottleneck for data analysis and visualization. Compression techniques have been employed to tackle this challenge, but traditional lossy methods often demand conservative error tolerances to meet the numerical accuracy requirements of both anticipated and unknown data analysis tasks. Progressive data compression and retrieval has emerged as a promising solution, where each analysis task dictates its own accuracy needs. However, few analysis algorithms inherently support progressive data processing, and adapting compression techniques, file formats, client/server frameworks, and APIs to support progressivity can be challenging. This paper presents a framework that enables progressive-precision data queries for any data compressor or numerical representation. Our strategy hinges on a multi-component representation that successively reduces the error between the original and compressed field, allowing each field in the progressive sequence to be expressed as a partial sum of components. We have implemented this approach with four established scientific data compressors and assessed its effectiveness using real-world data sets from the SDRBench collection. The results show that our framework competes in accuracy with the standalone compressors it is based upon. Additionally, (de)compression time is proportional to the number of components requested by the user. Finally, our framework allows for fully lossless compression using lossy compressors when a sufficient number of components are employed.},
  archive      = {J_TVCG},
  author       = {Victor A. P. Magri and Peter Lindstrom},
  doi          = {10.1109/TVCG.2023.3327186},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1358-1368},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A general framework for progressive data compression and retrieval},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RL-LABEL: A deep reinforcement learning approach intended
for AR label placement in dynamic scenarios. <em>TVCG</em>,
<em>30</em>(1), 1347–1357. (<a
href="https://doi.org/10.1109/TVCG.2023.3326568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Labels are widely used in augmented reality (AR) to display digital information. Ensuring the readability of AR labels requires placing them in an occlusion-free manner while keeping visual links legible, especially when multiple labels exist in the scene. Although existing optimization-based methods, such as force-based methods, are effective in managing AR labels in static scenarios, they often struggle in dynamic scenarios with constantly moving objects. This is due to their focus on generating layouts optimal for the current moment, neglecting future moments and leading to sub-optimal or unstable layouts over time. In this work, we present RL-L ABEL , a deep reinforcement learning-based method intended for managing the placement of AR labels in scenarios involving moving objects. RL-L ABEL considers both the current and predicted future states of objects and labels, such as positions and velocities, as well as the user&#39;s viewpoint, to make informed decisions about label placement. It balances the trade-offs between immediate and long-term objectives. We tested RL-L ABEL in simulated AR scenarios on two real-world datasets, showing that it effectively learns the decision-making process for long-term optimization, outperforming two baselines (i.e., no view management and a force-based method) by minimizing label occlusions, line intersections, and label movement distance. Additionally, a user study involving 18 participants indicates that, within our simulated environment, RL-L ABEL excels over the baselines in aiding users to identify, compare, and summarize data on labels in dynamic scenes.},
  archive      = {J_TVCG},
  author       = {Chen Zhu-Tian and Daniele Chiappalupi and Tica Lin and Yalong Yang and Johanna Beyer and Hanspeter Pfister},
  doi          = {10.1109/TVCG.2023.3326568},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1347-1357},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {RL-LABEL: A deep reinforcement learning approach intended for AR label placement in dynamic scenarios},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Handling non-visible referents in situated visualizations.
<em>TVCG</em>, <em>30</em>(1), 1336–1346. (<a
href="https://doi.org/10.1109/TVCG.2023.3327361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Situated visualizations are a type of visualization where data is presented next to its physical referent (i.e., the physical object, space, or person it refers to), often using augmented-reality displays. While situated visualizations can be beneficial in various contexts and have received research attention, they are typically designed with the assumption that the physical referent is visible. However, in practice, a physical referent may be obscured by another object, such as a wall, or may be outside the user&#39;s visual field. In this paper, we propose a conceptual framework and a design space to help researchers and user interface designers handle non-visible referents in situated visualizations. We first provide an overview of techniques proposed in the past for dealing with non-visible objects in the areas of 3D user interfaces, 3D visualization, and mixed reality. From this overview, we derive a design space that applies to situated visualizations and employ it to examine various trade-offs, challenges, and opportunities for future research in this area.},
  archive      = {J_TVCG},
  author       = {Ambre Assor and Arnaud Prouzeau and Martin Hachet and Pierre Dragicevic},
  doi          = {10.1109/TVCG.2023.3327361},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1336-1346},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Handling non-visible referents in situated visualizations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design patterns for situated visualization in augmented
reality. <em>TVCG</em>, <em>30</em>(1), 1324–1335. (<a
href="https://doi.org/10.1109/TVCG.2023.3327398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Situated visualization has become an increasingly popular research area in the visualization community, fueled by advancements in augmented reality (AR) technology and immersive analytics. Visualizing data in spatial proximity to their physical referents affords new design opportunities and considerations not present in traditional visualization, which researchers are now beginning to explore. However, the AR research community has an extensive history of designing graphics that are displayed in highly physical contexts. In this work, we leverage the richness of AR research and apply it to situated visualization. We derive design patterns which summarize common approaches of visualizing data in situ. The design patterns are based on a survey of 293 papers published in the AR and visualization communities, as well as our own expertise. We discuss design dimensions that help to describe both our patterns and previous work in the literature. This discussion is accompanied by several guidelines which explain how to apply the patterns given the constraints imposed by the real world. We conclude by discussing future research directions that will help establish a complete understanding of the design of situated visualization, including the role of interactivity, tasks, and workflows.},
  archive      = {J_TVCG},
  author       = {Benjamin Lee and Michael Sedlmair and Dieter Schmalstieg},
  doi          = {10.1109/TVCG.2023.3327398},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1324-1335},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Design patterns for situated visualization in augmented reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ARGUS: Visualization of AI-assisted task guidance in AR.
<em>TVCG</em>, <em>30</em>(1), 1313–1323. (<a
href="https://doi.org/10.1109/TVCG.2023.3327396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of augmented reality (AR) assistants has captured the human imagination for decades, becoming a staple of modern science fiction. To pursue this goal, it is necessary to develop artificial intelligence (AI)-based methods that simultaneously perceive the 3D environment, reason about physical tasks, and model the performer, all in real-time. Within this framework, a wide variety of sensors are needed to generate data across different modalities, such as audio, video, depth, speech, and time-of-flight. The required sensors are typically part of the AR headset, providing performer sensing and interaction through visual, audio, and haptic feedback. AI assistants not only record the performer as they perform activities, but also require machine learning (ML) models to understand and assist the performer as they interact with the physical world. Therefore, developing such assistants is a challenging task. We propose ARGUS, a visual analytics system to support the development of intelligent AR assistants. Our system was designed as part of a multi-year-long collaboration between visualization researchers and ML and AR experts. This co-design process has led to advances in the visualization of ML in AR. Our system allows for online visualization of object, action, and step detection as well as offline analysis of previously recorded AR sessions. It visualizes not only the multimodal sensor data streams but also the output of the ML models. This allows developers to gain insights into the performer activities as well as the ML models, helping them troubleshoot, improve, and fine-tune the components of the AR assistant.},
  archive      = {J_TVCG},
  author       = {Sonia Castelo and Joao Rulff and Erin McGowan and Bea Steers and Guande Wu and Shaoyu Chen and Iran Roman and Roque Lopez and Ethan Brewer and Chen Zhao and Jing Qian and Kyunghyun Cho and He He and Qi Sun and Huy Vo and Juan Bello and Michael Krone and Claudio Silva},
  doi          = {10.1109/TVCG.2023.3327396},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1313-1323},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ARGUS: Visualization of AI-assisted task guidance in AR},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TopoSZ: Preserving topology in error-bounded lossy
compression. <em>TVCG</em>, <em>30</em>(1), 1302–1312. (<a
href="https://doi.org/10.1109/TVCG.2023.3326920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing error-bounded lossy compression techniques control the pointwise error during compression to guarantee the integrity of the decompressed data. However, they typically do not explicitly preserve the topological features in data. When performing post hoc analysis with decompressed data using topological methods, preserving topology in the compression process to obtain topologically consistent and correct scientific insights is desirable. In this paper, we introduce TopoSZ, an error-bounded lossy compression method that preserves the topological features in 2D and 3D scalar fields. Specifically, we aim to preserve the types and locations of local extrema as well as the level set relations among critical points captured by contour trees in the decompressed data. The main idea is to derive topological constraints from contour-tree-induced segmentation from the data domain, and incorporate such constraints with a customized error-controlled quantization strategy from the SZ compressor (version 1.4). Our method allows users to control the pointwise error and the loss of topological features during the compression process with a global error bound and a persistence threshold.},
  archive      = {J_TVCG},
  author       = {Lin Yan and Xin Liang and Hanqi Guo and Bei Wang},
  doi          = {10.1109/TVCG.2023.3326920},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1302-1312},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TopoSZ: Preserving topology in error-bounded lossy compression},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactive design and optics-based visualization of
arbitrary non-euclidean kaleidoscopic orbifolds. <em>TVCG</em>,
<em>30</em>(1), 1292–1301. (<a
href="https://doi.org/10.1109/TVCG.2023.3326927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orbifolds are a modern mathematical concept that arises in the research of hyperbolic geometry with applications in computer graphics and visualization. In this paper, we make use of rooms with mirrors as the visual metaphor for orbifolds. Given any arbitrary two-dimensional kaleidoscopic orbifold, we provide an algorithm to construct a Euclidean, spherical, or hyperbolic polygon to match the orbifold. This polygon is then used to create a room for which the polygon serves as the floor and the ceiling. With our system that implements Möbius transformations, the user can interactively edit the scene and see the reflections of the edited objects. To correctly visualize non-Euclidean orbifolds, we adapt the rendering algorithms to account for the geodesics in these spaces, which light rays follow. Our interactive orbifold design system allows the user to create arbitrary two-dimensional kaleidoscopic orbifolds. In addition, our mirror-based orbifold visualization approach has the potential of helping our users gain insight on the orbifold, including its orbifold notation as well as its universal cover, which can also be the spherical space and the hyperbolic space.},
  archive      = {J_TVCG},
  author       = {Jinta Zheng and Eugene Zhang and Yue Zhang},
  doi          = {10.1109/TVCG.2023.3326927},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1292-1301},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive design and optics-based visualization of arbitrary non-euclidean kaleidoscopic orbifolds},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global topology of 3D symmetric tensor fields.
<em>TVCG</em>, <em>30</em>(1), 1282–1291. (<a
href="https://doi.org/10.1109/TVCG.2023.3326933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There have been recent advances in the analysis and visualization of 3D symmetric tensor fields, with a focus on the robust extraction of tensor field topology. However, topological features such as degenerate curves and neutral surfaces do not live in isolation. Instead, they intriguingly interact with each other. In this paper, we introduce the notion of topological graph for 3D symmetric tensor fields to facilitate global topological analysis of such fields. The nodes of the graph include degenerate curves and regions bounded by neutral surfaces in the domain. The edges in the graph denote the adjacency information between the regions and degenerate curves. In addition, we observe that a degenerate curve can be a loop and even a knot and that two degenerate curves (whether in the same region or not) can form a link. We provide a definition and theoretical analysis of individual degenerate curves in order to help understand why knots and links may occur. Moreover, we differentiate between wedges and trisectors, thus making the analysis more detailed about degenerate curves. We incorporate this information into the topological graph. Such a graph can not only reveal the global structure in a 3D symmetric tensor field but also allow two symmetric tensor fields to be compared. We demonstrate our approach by applying it to solid mechanics and material science data sets.},
  archive      = {J_TVCG},
  author       = {Shih-Hsuan Hung and Yue Zhang and Eugene Zhang},
  doi          = {10.1109/TVCG.2023.3326933},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1282-1291},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Global topology of 3D symmetric tensor fields},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A task-parallel approach for localized topological data
structures. <em>TVCG</em>, <em>30</em>(1), 1271–1281. (<a
href="https://doi.org/10.1109/TVCG.2023.3327182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unstructured meshes are characterized by data points irregularly distributed in the Euclidian space. Due to the irregular nature of these data, computing connectivity information between the mesh elements requires much more time and memory than on uniformly distributed data. To lower storage costs, dynamic data structures have been proposed. These data structures compute connectivity information on the fly and discard them when no longer needed. However, on-the-fly computation slows down algorithms and results in a negative impact on the time performance. To address this issue, we propose a new task-parallel approach to proactively compute mesh connectivity. Unlike previous approaches implementing data-parallel models, where all threads run the same type of instructions, our task-parallel approach allows threads to run different functions. Specifically, some threads run the algorithm of choice while other threads compute connectivity information before they are actually needed. The approach was implemented in the new Accelerated Clustered TOPOlogical ( ACTOPO ) data structure, which can support any processing algorithm requiring mesh connectivity information. Our experiments show that ACTOPO combines the benefits of state-of-the-art memory-efficient (TTK CompactTriangulation) and time-efficient (TTK ExplicitTriangulation) topological data structures. It occupies a similar amount of memory as TTK CompactTriangulation while providing up to 5x speedup. Moreover, it achieves comparable time performance as TTK ExplicitTriangulation while using only half of the memory space.},
  archive      = {J_TVCG},
  author       = {Guoxi Liu and Federico Iuricich},
  doi          = {10.1109/TVCG.2023.3327182},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1271-1281},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A task-parallel approach for localized topological data structures},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A local iterative approach for the extraction of 2D
manifolds from strongly curved and folded thin-layer structures.
<em>TVCG</em>, <em>30</em>(1), 1260–1270. (<a
href="https://doi.org/10.1109/TVCG.2023.3327403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ridge surfaces represent important features for the analysis of 3-dimensional (3D) datasets in diverse applications and are often derived from varying underlying data including flow fields, geological fault data, and point data, but they can also be present in the original scalar images acquired using a plethora of imaging techniques. Our work is motivated by the analysis of image data acquired using micro-computed tomography ( $\mu\text{CT}$ ) of ancient, rolled and folded thin-layer structures such as papyrus, parchment, and paper as well as silver and lead sheets. From these documents we know that they are 2-dimensional (2D) in nature. Hence, we are particularly interested in reconstructing 2D manifolds that approximate the document&#39;s structure. The image data from which we want to reconstruct the 2D manifolds are often very noisy and represent folded, densely-layered structures with many artifacts, such as ruptures or layer splitting and merging. Previous ridge-surface extraction methods fail to extract the desired 2D manifold for such challenging data. We have therefore developed a novel method to extract 2D manifolds. The proposed method uses a local fast marching scheme in combination with a separation of the region covered by fast marching into two sub-regions. The 2D manifold of interest is then extracted as the surface separating the two sub-regions. The local scheme can be applied for both automatic propagation as well as interactive analysis. We demonstrate the applicability and robustness of our method on both artificial data as well as real-world data including folded silver and papyrus sheets.},
  archive      = {J_TVCG},
  author       = {Nicolas Klenert and Verena Lepper and Daniel Baum},
  doi          = {10.1109/TVCG.2023.3327403},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1260-1270},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A local iterative approach for the extraction of 2D manifolds from strongly curved and folded thin-layer structures},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TROPHY: A topologically robust physics-informed tracking
framework for tropical cyclones. <em>TVCG</em>, <em>30</em>(1),
1249–1259. (<a href="https://doi.org/10.1109/TVCG.2023.3326905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tropical cyclones (TCs) are among the most destructive weather systems. Realistically and efficiently detecting and tracking TCs are critical for assessing their impacts and risks. In particular, the eye is a signature feature of a mature TC. Therefore, knowing the eyes&#39; locations and movements is crucial for both operational weather forecasts and climate risk assessments. Recently, a multilevel robustness framework has been introduced to study the critical points of time-varying vector fields. The framework quantifies the robustness (i.e., structural stability) of critical points across varying neighborhoods. By relating the multilevel robustness with critical point tracking, the framework has demonstrated its potential in cyclone tracking. An advantage is that it identifies cyclonic features using only 2D wind vector fields, which is encouraging as most tracking algorithms require multiple dynamic and thermodynamic variables at different altitudes. A disadvantage is that the framework does not scale well computationally for datasets containing a large number of cyclones. This paper introduces a topologically robust physics-informed tracking framework (TROPHY) for TC tracking. The main idea is to integrate physical knowledge of TC to drastically improve the computational efficiency of multilevel robustness framework for large-scale climate datasets. First, during preprocessing, we propose a physics-informed feature selection strategy to filter 90% of critical points that are short-lived and have low stability, thus preserving good candidates for TC tracking. Second, during in-processing, we impose constraints during the multilevel robustness computation to focus only on physics-informed neighborhoods of TCs. We apply TROPHY to 30 years of 2D wind fields from reanalysis data in ERA5 and generate a number of TC tracks. In comparison with the observed tracks, we demonstrate that TROPHY can capture TC characteristics (e.g., frequency, intensity, duration, latitudes with maximum intensity, and genesis) that are comparable to and sometimes even better than a well-validated TC tracking algorithm that requires multiple dynamic and thermodynamic scalar fields.},
  archive      = {J_TVCG},
  author       = {Lin Yan and Hanqi Guo and Thomas Peterka and Bei Wang and Jiali Wang},
  doi          = {10.1109/TVCG.2023.3326905},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1249-1259},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TROPHY: A topologically robust physics-informed tracking framework for tropical cyclones},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging historical medical records as a proxy via
multimodal modeling and visualization to enrich medical diagnostic
learning. <em>TVCG</em>, <em>30</em>(1), 1238–1248. (<a
href="https://doi.org/10.1109/TVCG.2023.3326929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulation-based Medical Education (SBME) has been developed as a cost-effective means of enhancing the diagnostic skills of novice physicians and interns, thereby mitigating the need for resource-intensive mentor-apprentice training. However, feedback provided in most SBME is often directed towards improving the operational proficiency of learners, rather than providing summative medical diagnoses that result from experience and time. Additionally, the multimodal nature of medical data during diagnosis poses significant challenges for interns and novice physicians, including the tendency to overlook or over-rely on data from certain modalities, and difficulties in comprehending potential associations between modalities. To address these challenges, we present DiagnosisAssistant , a visual analytics system that leverages historical medical records as a proxy for multimodal modeling and visualization to enhance the learning experience of interns and novice physicians. The system employs elaborately designed visualizations to explore different modality data, offer diagnostic interpretive hints based on the constructed model, and enable comparative analyses of specific patients. Our approach is validated through two case studies and expert interviews, demonstrating its effectiveness in enhancing medical training.},
  archive      = {J_TVCG},
  author       = {Yang Ouyang and Yuchen Wu and He Wang and Chenyang Zhang and Furui Cheng and Chang Jiang and Lixia Jin and Yuanwu Cao and Quan Li},
  doi          = {10.1109/TVCG.2023.3326929},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1238-1248},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Leveraging historical medical records as a proxy via multimodal modeling and visualization to enrich medical diagnostic learning},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Roses have thorns: Understanding the downside of oncological
care delivery through visual analytics and sequential rule mining.
<em>TVCG</em>, <em>30</em>(1), 1227–1237. (<a
href="https://doi.org/10.1109/TVCG.2023.3326939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized head and neck cancer therapeutics have greatly improved survival rates for patients, but are often leading to understudied long-lasting symptoms which affect quality of life. Sequential rule mining (SRM) is a promising unsupervised machine learning method for predicting longitudinal patterns in temporal data which, however, can output many repetitive patterns that are difficult to interpret without the assistance of visual analytics. We present a data-driven, human-machine analysis visual system developed in collaboration with SRM model builders in cancer symptom research, which facilitates mechanistic knowledge discovery in large scale, multivariate cohort symptom data. Our system supports multivariate predictive modeling of post-treatment symptoms based on during-treatment symptoms. It supports this goal through an SRM, clustering, and aggregation back end, and a custom front end to help develop and tune the predictive models. The system also explains the resulting predictions in the context of therapeutic decisions typical in personalized care delivery. We evaluate the resulting models and system with an interdisciplinary group of modelers and head and neck oncology researchers. The results demonstrate that our system effectively supports clinical and symptom research.},
  archive      = {J_TVCG},
  author       = {Carla Floricel and Andrew Wentzel and Abdallah Mohamed and C.David Fuller and Guadalupe Canahuate and G.Elisabeta Marai},
  doi          = {10.1109/TVCG.2023.3326939},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1227-1237},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Roses have thorns: Understanding the downside of oncological care delivery through visual analytics and sequential rule mining},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Marjorie: Visualizing type 1 diabetes data to support
pattern exploration. <em>TVCG</em>, <em>30</em>(1), 1216–1226. (<a
href="https://doi.org/10.1109/TVCG.2023.3326936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we propose Marjorie, a visual analytics approach to address the challenge of analyzing patients&#39; diabetes data during brief regular appointments with their diabetologists. Designed in consultation with diabetologists, Marjorie uses a combination of visual and algorithmic methods to support the exploration of patterns in the data. Patterns of interest include seasonal variations of the glucose profiles, and non-periodic patterns such as fluctuations around mealtimes or periods of hypoglycemia (i.e., glucose levels below the normal range). We introduce a unique representation of glucose data based on modified horizon graphs and hierarchical clustering of adjacent carbohydrate or insulin entries. Semantic zooming allows the exploration of patterns on different levels of temporal detail. We evaluated our solution in a case study, which demonstrated Marjorie&#39;s potential to provide valuable insights into therapy parameters and unfavorable eating habits, among others. The study results and informal feedback collected from target users suggest that Marjorie effectively supports patients and diabetologists in the joint exploration of patterns in diabetes data, potentially enabling more informed treatment decisions. A free copy of this paper and all supplemental materials are available at https://osf.io/34t8c/ .},
  archive      = {J_TVCG},
  author       = {Anna Scimone and Klaus Eckelt and Marc Streit and Andreas Hinterreiter},
  doi          = {10.1109/TVCG.2023.3326936},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1216-1226},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Marjorie: Visualizing type 1 diabetes data to support pattern exploration},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HealthPrism: A visual analytics system for exploring
children’s physical and mental health profiles with multimodal data.
<em>TVCG</em>, <em>30</em>(1), 1205–1215. (<a
href="https://doi.org/10.1109/TVCG.2023.3326943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The correlation between children&#39;s personal and family characteristics (e.g., demographics and socioeconomic status) and their physical and mental health status has been extensively studied across various research domains, such as public health, medicine, and data science. Such studies can provide insights into the underlying factors affecting children&#39;s health and aid in the development of targeted interventions to improve their health outcomes. However, with the availability of multiple data sources, including context data (i.e., the background information of children) and motion data (i.e., sensor data measuring activities of children), new challenges have arisen due to the large-scale, heterogeneous, and multimodal nature of the data. Existing statistical hypothesis-based and learning model-based approaches have been inadequate for comprehensively analyzing the complex correlation between multimodal features and multi-dimensional health outcomes due to the limited information revealed. In this work, we first distill a set of design requirements from multiple levels through conducting a literature review and iteratively interviewing 11 experts from multiple domains (e.g., public health and medicine). Then, we propose HealthPrism , an interactive visual and analytics system for assisting researchers in exploring the importance and influence of various context and motion features on children&#39;s health status from multi-level perspectives. Within HealthPrism , a multimodal learning model with a gate mechanism is proposed for health profiling and cross-modality feature importance comparison. A set of visualization components is designed for experts to explore and understand multimodal data freely. We demonstrate the effectiveness and usability of HealthPrism through quantitative evaluation of the model performance, case studies, and expert interviews in associated domains.},
  archive      = {J_TVCG},
  author       = {Zhihan Jiang and Handi Chen and Rui Zhou and Jing Deng and Xinchen Zhang and Running Zhao and Cong Xie and Yifang Wang and Edith C.H. Ngai},
  doi          = {10.1109/TVCG.2023.3326943},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1205-1215},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HealthPrism: A visual analytics system for exploring children&#39;s physical and mental health profiles with multimodal data},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visualizing large-scale spatial time series with GeoChron.
<em>TVCG</em>, <em>30</em>(1), 1194–1204. (<a
href="https://doi.org/10.1109/TVCG.2023.3327162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In geo-related fields such as urban informatics, atmospheric science, and geography, large-scale spatial time (ST) series (i.e., geo-referred time series) are collected for monitoring and understanding important spatiotemporal phenomena. ST series visualization is an effective means of understanding the data and reviewing spatiotemporal phenomena, which is a prerequisite for in-depth data analysis. However, visualizing these series is challenging due to their large scales, inherent dynamics, and spatiotemporal nature. In this study, we introduce the notion of patterns of evolution in ST series. Each evolution pattern is characterized by 1) a set of ST series that are close in space and 2) a time period when the trends of these ST series are correlated. We then leverage Storyline techniques by considering an analogy between evolution patterns and sessions, and finally design a novel visualization called GeoChron , which is capable of visualizing large-scale ST series in an evolution pattern-aware and narrative-preserving manner. GeoChron includes a mining framework to extract evolution patterns and two-level visualizations to enhance its visual scalability. We evaluate GeoChron with two case studies, an informal user study, an ablation study, parameter analysis, and running time analysis.},
  archive      = {J_TVCG},
  author       = {Zikun Deng and Shifu Chen and Tobias Schreck and Dazhen Deng and Tan Tang and Mingliang Xu and Di Weng and Yingcai Wu},
  doi          = {10.1109/TVCG.2023.3327162},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1194-1204},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualizing large-scale spatial time series with GeoChron},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TimeTuner: Diagnosing time representations for time-series
forecasting with counterfactual explanations. <em>TVCG</em>,
<em>30</em>(1), 1183–1193. (<a
href="https://doi.org/10.1109/TVCG.2023.3327389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) approaches are being increasingly used for time-series forecasting, with many efforts devoted to designing complex DL models. Recent studies have shown that the DL success is often attributed to effective data representations, fostering the fields of feature engineering and representation learning. However, automated approaches for feature learning are typically limited with respect to incorporating prior knowledge, identifying interactions among variables, and choosing evaluation metrics to ensure that the models are reliable. To improve on these limitations, this paper contributes a novel visual analytics framework, namely TimeTuner , designed to help analysts understand how model behaviors are associated with localized correlations, stationarity, and granularity of time-series representations. The system mainly consists of the following two-stage technique: We first leverage counterfactual explanations to connect the relationships among time-series representations, multivariate features and model predictions. Next, we design multiple coordinated views including a partition-based correlation matrix and juxtaposed bivariate stripes, and provide a set of interactions that allow users to step into the transformation selection process, navigate through the feature space, and reason the model performance. We instantiate TimeTuner with two transformation methods of smoothing and sampling, and demonstrate its applicability on real-world time-series forecasting of univariate sunspots and multivariate air pollutants. Feedback from domain experts indicates that our system can help characterize time-series representations and guide the feature engineering processes.},
  archive      = {J_TVCG},
  author       = {Jianing Hao and Qing Shi and Yilin Ye and Wei Zeng},
  doi          = {10.1109/TVCG.2023.3327389},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1183-1193},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TimeTuner: Diagnosing time representations for time-series forecasting with counterfactual explanations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supporting guided exploratory visual analysis on time series
data with reinforcement learning. <em>TVCG</em>, <em>30</em>(1),
1172–1182. (<a href="https://doi.org/10.1109/TVCG.2023.3327200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exploratory visual analysis (EVA) of time series data uses visualization as the main output medium and input interface for exploring new data. However, for users who lack visual analysis expertise, interpreting and manipulating EVA can be challenging. Thus, providing guidance on EVA is necessary and two relevant questions need to be answered. First, how to recommend interesting insights to provide a first glance at data and help develop an exploration goal. Second, how to provide step-by-step EVA suggestions to help identify which parts of the data to explore. In this work, we present a reinforcement learning (RL)-based system, Visail, which generates EVA sequences to guide the exploration of time series data. As a user uploads a time series dataset, Visail can generate step-by-step EVA suggestions, while each step is visualized as an annotated chart combined with textual descriptions. The RL-based algorithm uses exploratory data analysis knowledge to construct the state and action spaces for the agent to imitate human analysis behaviors in data exploration tasks. In this way, the agent learns the strategy of generating coherent EVA sequences through a well-designed network. To evaluate the effectiveness of our system, we conducted an ablation study, a user study, and two case studies. The results of our evaluation suggested that Visail can provide effective guidance on supporting EVA on time series data.},
  archive      = {J_TVCG},
  author       = {Yang Shi and Bingchang Chen and Ying Chen and Zhuochen Jin and Ke Xu and Xiaohan Jiao and Tian Gao and Nan Cao},
  doi          = {10.1109/TVCG.2023.3327200},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1172-1182},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Supporting guided exploratory visual analysis on time series data with reinforcement learning},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reclaiming the horizon: Novel visualization designs for
time-series data with large value ranges. <em>TVCG</em>, <em>30</em>(1),
1161–1171. (<a href="https://doi.org/10.1109/TVCG.2023.3326576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce two novel visualization designs to support practitioners in performing identification and discrimination tasks on large value ranges (i.e., several orders of magnitude) in time-series data: (1) The order of magnitude horizon graph, which extends the classic horizon graph; and (2) the order of magnitude line chart, which adapts the log-line chart. These new visualization designs visualize large value ranges by explicitly splitting the mantissa $m$ and exponent $e$ of a value $v=m\cdot 10^{e}$ . We evaluate our novel designs against the most relevant state-of-the-art visualizations in an empirical user study. It focuses on four main tasks commonly employed in the analysis of time-series and large value ranges visualization: identification, discrimination, estimation, and trend detection. For each task we analyze error, confidence, and response time. The new order of magnitude horizon graph performs better or equal to all other designs in identification, discrimination, and estimation tasks. Only for trend detection tasks, the more traditional horizon graphs reported better performance. Our results are domain-independent, only requiring time-series data with large value ranges.},
  archive      = {J_TVCG},
  author       = {Daniel Braun and Rita Borgo and Max Sondag and Tatiana von Landesberger},
  doi          = {10.1109/TVCG.2023.3326576},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1161-1171},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Reclaiming the horizon: Novel visualization designs for time-series data with large value ranges},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attribute-aware RBFs: Interactive visualization of time
series particle volumes using RT core range queries. <em>TVCG</em>,
<em>30</em>(1), 1150–1160. (<a
href="https://doi.org/10.1109/TVCG.2023.3327366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smoothed-particle hydrodynamics (SPH) is a mesh-free method used to simulate volumetric media in fluids, astrophysics, and solid mechanics. Visualizing these simulations is problematic because these datasets often contain millions, if not billions of particles carrying physical attributes and moving over time. Radial basis functions (RBFs) are used to model particles, and overlapping particles are interpolated to reconstruct a high-quality volumetric field; however, this interpolation process is expensive and makes interactive visualization difficult. Existing RBF interpolation schemes do not account for color-mapped attributes and are instead constrained to visualizing just the density field. To address these challenges, we exploit ray tracing cores in modern GPU architectures to accelerate scalar field reconstruction. We use a novel RBF interpolation scheme to integrate per-particle colors and densities, and leverage GPU-parallel tree construction and refitting to quickly update the tree as the simulation animates over time or when the user manipulates particle radii. We also propose a Hilbert reordering scheme to cluster particles together at the leaves of the tree to reduce tree memory consumption. Finally, we reduce the noise of volumetric shadows by adopting a spatially temporal blue noise sampling scheme. Our method can provide a more detailed and interactive view of these large, volumetric, time-series particle datasets than traditional methods, leading to new insights into these physics simulations.},
  archive      = {J_TVCG},
  author       = {Nate Morrical and Stefan Zellmann and Alper Sahistan and Patrick Shriwise and Valerio Pascucci},
  doi          = {10.1109/TVCG.2023.3327366},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1150-1160},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Attribute-aware RBFs: Interactive visualization of time series particle volumes using RT core range queries},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). InvVis: Large-scale data embedding for invertible
visualization. <em>TVCG</em>, <em>30</em>(1), 1139–1149. (<a
href="https://doi.org/10.1109/TVCG.2023.3326597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present InvVis, a new approach for invertible visualization, which is reconstructing or further modifying a visualization from an image. InvVis allows the embedding of a significant amount of data, such as chart data, chart information, source code, etc., into visualization images. The encoded image is perceptually indistinguishable from the original one. We propose a new method to efficiently express chart data in the form of images, enabling large-capacity data embedding. We also outline a model based on the invertible neural network to achieve high-quality data concealing and revealing. We explore and implement a variety of application scenarios of InvVis. Additionally, we conduct a series of evaluation experiments to assess our method from multiple perspectives, including data embedding quality, data restoration accuracy, data encoding capacity, etc. The result of our experiments demonstrates the great potential of InvVis in invertible visualization.},
  archive      = {J_TVCG},
  author       = {Huayuan Ye and Chenhui Li and Yang Li and Changbo Wang},
  doi          = {10.1109/TVCG.2023.3326597},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1139-1149},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {InvVis: Large-scale data embedding for invertible visualization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data formulator: AI-powered concept-driven visualization
authoring. <em>TVCG</em>, <em>30</em>(1), 1128–1138. (<a
href="https://doi.org/10.1109/TVCG.2023.3326585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With most modern visualization tools, authors need to transform their data into tidy formats to create visualizations they want. Because this requires experience with programming or separate data processing tools, data transformation remains a barrier in visualization authoring. To address this challenge, we present a new visualization paradigm, concept binding , that separates high-level visualization intents and low-level data transformation steps, leveraging an AI agent. We realize this paradigm in Data Formulator, an interactive visualization authoring tool. With Data Formulator, authors first define data concepts they plan to visualize using natural languages or examples, and then bind them to visual channels. Data Formulator then dispatches its AI-agent to automatically transform the input data to surface these concepts and generate desired visualizations. When presenting the results (transformed table and output visualizations) from the AI agent, Data Formulator provides feedback to help authors inspect and understand them. A user study with 10 participants shows that participants could learn and use Data Formulator to create visualizations that involve challenging data transformations, and presents interesting future research directions.},
  archive      = {J_TVCG},
  author       = {Chenglong Wang and John Thompson and Bongshin Lee},
  doi          = {10.1109/TVCG.2023.3326585},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1128-1138},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Data formulator: AI-powered concept-driven visualization authoring},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LiveRetro: Visual analytics for strategic retrospect in
livestream e-commerce. <em>TVCG</em>, <em>30</em>(1), 1117–1127. (<a
href="https://doi.org/10.1109/TVCG.2023.3326911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Livestream e-commerce integrates live streaming and online shopping, allowing viewers to make purchases while watching. However, effective marketing strategies remain a challenge due to limited empirical research and subjective biases from the absence of quantitative data. Current tools fail to capture the interdependence between live performances and feedback. This study identified computational features, formulated design requirements, and developed LiveRetro , an interactive visual analytics system. It enables comprehensive retrospective analysis of livestream e-commerce for streamers, viewers, and merchandise. LiveRetro employs enhanced visualization and time-series forecasting models to align performance features and feedback, identifying influences at channel, merchandise, feature, and segment levels. Through case studies and expert interviews, the system provides deep insights into the relationship between live performance and streaming statistics, enabling efficient strategic analysis from multiple perspectives.},
  archive      = {J_TVCG},
  author       = {Yuchen Wu and Yuansong Xu and Shenghan Gao and Xingbo Wang and Wenkai Song and Zhiheng Nie and Xiaomeng Fan and Quan Li},
  doi          = {10.1109/TVCG.2023.3326911},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1117-1127},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LiveRetro: Visual analytics for strategic retrospect in livestream E-commerce},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data type agnostic visual sensitivity analysis.
<em>TVCG</em>, <em>30</em>(1), 1106–1116. (<a
href="https://doi.org/10.1109/TVCG.2023.3327203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern science and industry rely on computational models for simulation, prediction, and data analysis. Spatial blind source separation (SBSS) is a model used to analyze spatial data. Designed explicitly for spatial data analysis, it is superior to popular non-spatial methods, like PCA. However, a challenge to its practical use is setting two complex tuning parameters, which requires parameter space analysis. In this paper, we focus on sensitivity analysis (SA). SBSS parameters and outputs are spatial data, which makes SA difficult as few SA approaches in the literature assume such complex data on both sides of the model. Based on the requirements in our design study with statistics experts, we developed a visual analytics prototype for data type agnostic visual sensitivity analysis that fits SBSS and other contexts. The main advantage of our approach is that it requires only dissimilarity measures for parameter settings and outputs ( Fig. 1 ). We evaluated the prototype heuristically with visualization experts and through interviews with two SBSS experts. In addition, we show the transferability of our approach by applying it to microclimate simulations. Study participants could confirm suspected and known parameter-output relations, find surprising associations, and identify parameter subspaces to examine in the future. During our design study and evaluation, we identified challenging future research opportunities.},
  archive      = {J_TVCG},
  author       = {Nikolaus Piccolotto and Markus Bögl and Christoph Muehlmann and Klaus Nordhausen and Peter Filzmoser and Johanna Schmidt and Silvia Miksch},
  doi          = {10.1109/TVCG.2023.3327203},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1106-1116},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Data type agnostic visual sensitivity analysis},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Merge tree geodesics and barycenters with path mappings.
<em>TVCG</em>, <em>30</em>(1), 1095–1105. (<a
href="https://doi.org/10.1109/TVCG.2023.3326601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparative visualization of scalar fields is often facilitated using similarity measures such as edit distances. In this paper, we describe a novel approach for similarity analysis of scalar fields that combines two recently introduced techniques: Wasserstein geodesics/barycenters as well as path mappings, a branch decomposition-independent edit distance. Effectively, we are able to leverage the reduced susceptibility of path mappings to small perturbations in the data when compared with the original Wasserstein distance. Our approach therefore exhibits superior performance and quality in typical tasks such as ensemble summarization, ensemble clustering, and temporal reduction of time series, while retaining practically feasible runtimes. Beyond studying theoretical properties of our approach and discussing implementation aspects, we describe a number of case studies that provide empirical insights into its utility for comparative visualization, and demonstrate the advantages of our method in both synthetic and real-world scenarios. We supply a C++ implementation that can be used to reproduce our results.},
  archive      = {J_TVCG},
  author       = {Florian Wetzels and Mathieu Pont and Julien Tierny and Christoph Garth},
  doi          = {10.1109/TVCG.2023.3326601},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1095-1105},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Merge tree geodesics and barycenters with path mappings},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ExTreeM: Scalable augmented merge tree computation via
extremum graphs. <em>TVCG</em>, <em>30</em>(1), 1085–1094. (<a
href="https://doi.org/10.1109/TVCG.2023.3326526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last decade merge trees have been proven to support a plethora of visualization and analysis tasks since they effectively abstract complex datasets. This paper describes the ExTreeM-Algorithm: A scalable algorithm for the computation of merge trees via extremum graphs. The core idea of ExTreeM is to first derive the extremum graph $\mathcal{G}$ of an input scalar field $f$ defined on a cell complex $\mathcal{K}$ , and subsequently compute the unaugmented merge tree of $f$ on $\mathcal{G}$ instead of $\mathcal{K}$ ; which are equivalent. Any merge tree algorithm can be carried out significantly faster on $\mathcal{G}$ , since $\mathcal{K}$ in general contains substantially more cells than $\mathcal{G}$ . To further speed up computation, ExTreeM includes a tailored procedure to derive merge trees of extremum graphs. The computation of the fully augmented merge tree, i.e., a merge tree domain segmentation of $\mathcal{K}$ , can then be performed in an optional post-processing step. All steps of ExTreeM consist of procedures with high parallel efficiency, and we provide a formal proof of its correctness. Our experiments, performed on publicly available datasets, report a speedup of up to one order of magnitude over the state-of-the-art algorithms included in the TTK and VTK-m software libraries, while also requiring significantly less memory and exhibiting excellent scaling behavior.},
  archive      = {J_TVCG},
  author       = {Jonas Lukasczyk and Michael Will and Florian Wetzels and Gunther H. Weber and Christoph Garth},
  doi          = {10.1109/TVCG.2023.3326526},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1085-1094},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ExTreeM: Scalable augmented merge tree computation via extremum graphs},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comparative study of the perceptual sensitivity of
topological visualizations to feature variations. <em>TVCG</em>,
<em>30</em>(1), 1074–1084. (<a
href="https://doi.org/10.1109/TVCG.2023.3326592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color maps are a commonly used visualization technique in which data are mapped to optical properties, e.g., color or opacity. Color maps, however, do not explicitly convey structures (e.g., positions and scale of features) within data. Topology-based visualizations reveal and explicitly communicate structures underlying data. Although our understanding of what types of features are captured by topological visualizations is good, our understanding of people&#39;s perception of those features is not. This paper evaluates the sensitivity of topology-based isocontour, Reeb graph, and persistence diagram visualizations compared to a reference color map visualization for synthetically generated scalar fields on 2-manifold triangular meshes embedded in 3D. In particular, we built and ran a human-subject study that evaluated the perception of data features characterized by Gaussian signals and measured how effectively each visualization technique portrays variations of data features arising from the position and amplitude variation of a mixture of Gaussians. For positional feature variations, the results showed that only the Reeb graph visualization had high sensitivity. For amplitude feature variations, persistence diagrams and color maps demonstrated the highest sensitivity, whereas isocontours showed only weak sensitivity. These results take an important step toward understanding which topology-based tools are best for various data and task scenarios and their effectiveness in conveying topological variations as compared to conventional color mapping.},
  archive      = {J_TVCG},
  author       = {Tushar M. Athawale and Bryan Triana and Tanmay Kotha and Dave Pugmire and Paul Rosen},
  doi          = {10.1109/TVCG.2023.3326592},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1074-1084},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A comparative study of the perceptual sensitivity of topological visualizations to feature variations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Too many cooks: Exploring how graphical perception studies
influence visualization recommendations in draco. <em>TVCG</em>,
<em>30</em>(1), 1063–1073. (<a
href="https://doi.org/10.1109/TVCG.2023.3326527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Findings from graphical perception can guide visualization recommendation algorithms in identifying effective visualization designs. However, existing algorithms use knowledge from, at best, a few studies, limiting our understanding of how complementary (or contradictory) graphical perception results influence generated recommendations. In this paper, we present a pipeline of applying a large body of graphical perception results to develop new visualization recommendation algorithms and conduct an exploratory study to investigate how results from graphical perception can alter the behavior of downstream algorithms. Specifically, we model graphical perception results from 30 papers in Draco—a framework to model visualization knowledge—to develop new recommendation algorithms. By analyzing Draco-generated algorithms, we showcase the feasibility of our method to (1) identify gaps in existing graphical perception literature informing recommendation algorithms, (2) cluster papers by their preferred design rules and constraints, and (3) investigate why certain studies can dominate Draco&#39;s recommendations, whereas others may have little influence. Given our findings, we discuss the potential for mutually reinforcing advancements in graphical perception and visualization recommendation research.},
  archive      = {J_TVCG},
  author       = {Zehua Zeng and Junran Yang and Dominik Moritz and Jeffrey Heer and Leilani Battle},
  doi          = {10.1109/TVCG.2023.3326527},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1063-1073},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Too many cooks: Exploring how graphical perception studies influence visualization recommendations in draco},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Perceptually uniform construction of illustrative textures.
<em>TVCG</em>, <em>30</em>(1), 1052–1062. (<a
href="https://doi.org/10.1109/TVCG.2023.3326574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Illustrative textures, such as stippling or hatching, were predominantly used as an alternative to conventional Phong rendering. Recently, the potential of encoding information on surfaces or maps using different densities has also been recognized. This has the significant advantage that additional color can be used as another visual channel and the illustrative textures can then be overlaid. Effectively, it is thus possible to display multiple information, such as two different scalar fields on surfaces simultaneously. In previous work, these textures were manually generated and the choice of density was unempirically determined. Here, we first want to determine and understand the perceptual space of illustrative textures. We chose a succession of simplices with increasing dimensions as primitives for our textures: Dots, lines, and triangles. Thus, we explore the texture types of stippling, hatching, and triangles. We create a range of textures by sampling the density space uniformly. Then, we conduct three perceptual studies in which the participants performed pairwise comparisons for each texture type. We use multidimensional scaling (MDS) to analyze the perceptual spaces per category. The perception of stippling and triangles seems relatively similar. Both are adequately described by a 1D manifold in 2D space. The perceptual space of hatching consists of two main clusters: Crosshatched textures, and textures with only one hatching direction. However, the perception of hatching textures with only one hatching direction is similar to the perception of stippling and triangles. Based on our findings, we construct perceptually uniform illustrative textures. Afterwards, we provide concrete application examples for the constructed textures.},
  archive      = {J_TVCG},
  author       = {Anna Sterzik and Monique Meuschke and Douglas W. Cunningham and Kai Lawonn},
  doi          = {10.1109/TVCG.2023.3326574},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1052-1062},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Perceptually uniform construction of illustrative textures},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Perception of line attributes for visualization.
<em>TVCG</em>, <em>30</em>(1), 1041–1051. (<a
href="https://doi.org/10.1109/TVCG.2023.3326523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Line attributes such as width and dashing are commonly used to encode information. However, many questions on the perception of line attributes remain, such as how many levels of attribute variation can be distinguished or which line attributes are the preferred choices for which tasks. We conducted three studies to develop guidelines for using stylized lines to encode scalar data. In our first study, participants drew stylized lines to encode uncertainty information. Uncertainty is usually visualized alongside other data. Therefore, alternative visual channels are important for the visualization of uncertainty. Additionally, uncertainty—e.g., in weather forecasts—is a familiar topic to most people. Thus, we picked it for our visualization scenarios in study 1. We used the results of our study to determine the most common line attributes for drawing uncertainty: Dashing, luminance, wave amplitude, and width. While those line attributes were especially common for drawing uncertainty, they are also commonly used in other areas. In studies 2 and 3, we investigated the discriminability of the line attributes determined in study 1. Studies 2 and 3 did not require specific application areas; thus, their results apply to visualizing any scalar data in line attributes. We evaluated the just-noticeable differences (JND) and derived recommendations for perceptually distinct line levels. We found that participants could discriminate considerably more levels for the line attribute width than for wave amplitude, dashing, or luminance.},
  archive      = {J_TVCG},
  author       = {Anna Sterzik and Nils Lichtenberg and Jana Wilms and Michael Krone and Douglas W. Cunningham and Kai Lawonn},
  doi          = {10.1109/TVCG.2023.3326523},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1041-1051},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Perception of line attributes for visualization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image or information? Examining the nature and impact of
visualization perceptual classification. <em>TVCG</em>, <em>30</em>(1),
1030–1040. (<a href="https://doi.org/10.1109/TVCG.2023.3326919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How do people internalize visualizations: as images or information ? In this study, we investigate the nature of internalization for visualizations (i.e., how the mind encodes visualizations in memory) and how memory encoding affects its retrieval. This exploratory work examines the influence of various design elements on a user&#39;s perception of a chart. Specifically, which design elements lead to perceptions of visualization as an image (aims to provide visual references, evoke emotions, express creativity, and inspire philosophic thought) or as information (aims to present complex data, information, or ideas concisely and promote analytical thinking)? Understanding how design elements contribute to viewers perceiving a visualization more as an image or information will help designers decide which elements to include to achieve their communication goals. For this study, we annotated 500 visualizations and analyzed the responses of 250 online participants, who rated the visualizations on a bilinear scale as ‘image’ or ‘information.’ We then conducted an in-person study ( $n = 101$ ) using a free recall task to examine how the image/information ratings and design elements impacted memory. The results revealed several interesting findings: Image-rated visualizations were perceived as more aesthetically ‘appealing,’ ‘enjoyable,’ and ‘pleasing.’ Information-rated visualizations were perceived as less ‘difficult to understand’ and more aesthetically ‘likable’ and ‘nice,’ though participants expressed higher ‘positive’ sentiment when viewing image-rated visualizations and felt less ‘guided to a conclusion.’ The presence of axes and text annotations heavily influenced the likelihood of participants rating the visualization as ‘information.’ We also found different patterns among participants that were older. Importantly, we show that visualizations internalized as ‘images’ are less effective in conveying trends and messages, though they elicit a more positive emotional judgment, while ‘informative’ visualizations exhibit annotation focused recall and elicit a more positive design judgment. We discuss the implications of this dissociation between aesthetic pleasure and perceived ease of use in visualization design.},
  archive      = {J_TVCG},
  author       = {Anjana Arunkumar and Lace Padilla and Gi-Yeul Bae and Chris Bryan},
  doi          = {10.1109/TVCG.2023.3326919},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1030-1040},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Image or information? examining the nature and impact of visualization perceptual classification},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design characterization for black-and-white textures in
visualization. <em>TVCG</em>, <em>30</em>(1), 1019–1029. (<a
href="https://doi.org/10.1109/TVCG.2023.3326941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the use of 2D black-and-white textures for the visualization of categorical data and contribute a summary of texture attributes, and the results of three experiments that elicited design strategies as well as aesthetic and effectiveness measures. Black-and-white textures are useful, for instance, as a visual channel for categorical data on low-color displays, in 2D/3D print, to achieve the aesthetic of historic visualizations, or to retain the color hue channel for other visual mappings. We specifically study how to use what we call geometric and iconic textures. Geometric textures use patterns of repeated abstract geometric shapes, while iconic textures use repeated icons that may stand for data categories. We parameterized both types of textures and developed a tool for designers to create textures on simple charts by adjusting texture parameters. 30 visualization experts used our tool and designed 66 textured bar charts, pie charts, and maps. We then had 150 participants rate these designs for aesthetics. Finally, with the top-rated geometric and iconic textures, our perceptual assessment experiment with 150 participants revealed that textured charts perform about equally well as non-textured charts, and that there are some differences depending on the type of chart.},
  archive      = {J_TVCG},
  author       = {Tingying He and Yuanyang Zhong and Petra Isenberg and Tobias Isenberg},
  doi          = {10.1109/TVCG.2023.3326941},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1019-1029},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Design characterization for black-and-white textures in visualization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The arrangement of marks impacts afforded messages:
Ordering, partitioning, spacing, and coloring in bar charts.
<em>TVCG</em>, <em>30</em>(1), 1008–1018. (<a
href="https://doi.org/10.1109/TVCG.2023.3326590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data visualizations present a massive number of potential messages to an observer. One might notice that one group&#39;s average is larger than another&#39;s, or that a difference in values is smaller than a difference between two others, or any of a combinatorial explosion of other possibilities. The message that a viewer tends to notice – the message that a visualization ‘affords’ – is strongly affected by how values are arranged in a chart, e.g., how the values are colored or positioned. Although understanding the mapping between a chart&#39;s arrangement and what viewers tend to notice is critical for creating guidelines and recommendation systems, current empirical work is insufficient to lay out clear rules. We present a set of empirical evaluations of how different messages-including ranking, grouping, and part-to-whole relationships–are afforded by variations in ordering, partitioning, spacing, and coloring of values, within the ubiquitous case study of bar graphs. In doing so, we introduce a quantitative method that is easily scalable, reviewable, and replicable, laying groundwork for further investigation of the effects of arrangement on message affordances across other visualizations and tasks. Pre-registration and all supplemental materials are available at https://osf.io/np3q7 and https://osf.io/bvy95 , respectively.},
  archive      = {J_TVCG},
  author       = {Racquel Fygenson and Steven Franconeri and Enrico Bertini},
  doi          = {10.1109/TVCG.2023.3326590},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1008-1018},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The arrangement of marks impacts afforded messages: Ordering, partitioning, spacing, and coloring in bar charts},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A heuristic approach for dual expert/end-user evaluation of
guidance in visual analytics. <em>TVCG</em>, <em>30</em>(1), 997–1007.
(<a href="https://doi.org/10.1109/TVCG.2023.3327152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Guidance can support users during the exploration and analysis of complex data. Previous research focused on characterizing the theoretical aspects of guidance in visual analytics and implementing guidance in different scenarios. However, the evaluation of guidance-enhanced visual analytics solutions remains an open research question. We tackle this question by introducing and validating a practical evaluation methodology for guidance in visual analytics. We identify eight quality criteria to be fulfilled and collect expert feedback on their validity. To facilitate actual evaluation studies, we derive two sets of heuristics. The first set targets heuristic evaluations conducted by expert evaluators. The second set facilitates end-user studies where participants actually use a guidance-enhanced system. By following such a dual approach, the different quality criteria of guidance can be examined from two different perspectives, enhancing the overall value of evaluation studies. To test the practical utility of our methodology, we employ it in two studies to gain insight into the quality of two guidance-enhanced visual analytics solutions, one being a work-in-progress research prototype, and the other being a publicly available visualization recommender system. Based on these two evaluations, we derive good practices for conducting evaluations of guidance in visual analytics and identify pitfalls to be avoided during such studies.},
  archive      = {J_TVCG},
  author       = {Davide Ceneda and Christopher Collins and Mennatallah El-Assady and Silvia Miksch and Christian Tominski and Alessio Arleo},
  doi          = {10.1109/TVCG.2023.3327152},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {997-1007},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A heuristic approach for dual Expert/End-user evaluation of guidance in visual analytics},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PSRFlow: Probabilistic super resolution with flow-based
models for scientific data. <em>TVCG</em>, <em>30</em>(1), 986–996. (<a
href="https://doi.org/10.1109/TVCG.2023.3327171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although many deep-learning-based super-resolution approaches have been proposed in recent years, because no ground truth is available in the inference stage, few can quantify the errors and uncertainties of the super-resolved results. For scientific visualization applications, however, conveying uncertainties of the results to scientists is crucial to avoid generating misleading or incorrect information. In this paper, we propose PSRFlow, a novel normalizing flow-based generative model for scientific data super-resolution that incorporates uncertainty quantification into the super-resolution process. PSRFlow learns the conditional distribution of the high-resolution data based on the low-resolution counterpart. By sampling from a Gaussian latent space that captures the missing information in the high-resolution data, one can generate different plausible super-resolution outputs. The efficient sampling in the Gaussian latent space allows our model to perform uncertainty quantification for the super-resolved results. During model training, we augment the training data with samples across various scales to make the model adaptable to data of different scales, achieving flexible super-resolution for a given input. Our results demonstrate superior performance and robust uncertainty quantification compared with existing methods such as interpolation and GAN-based super-resolution networks.},
  archive      = {J_TVCG},
  author       = {Jingyi Shen and Han-Wei Shen},
  doi          = {10.1109/TVCG.2023.3327171},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {986-996},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PSRFlow: Probabilistic super resolution with flow-based models for scientific data},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Photon field networks for dynamic real-time volumetric
global illumination. <em>TVCG</em>, <em>30</em>(1), 975–985. (<a
href="https://doi.org/10.1109/TVCG.2023.3327107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Volume data is commonly found in many scientific disciplines, like medicine, physics, and biology. Experts rely on robust scientific visualization techniques to extract valuable insights from the data. Recent years have shown path tracing to be the preferred approach for volumetric rendering, given its high levels of realism. However, real-time volumetric path tracing often suffers from stochastic noise and long convergence times, limiting interactive exploration. In this paper, we present a novel method to enable real-time global illumination for volume data visualization. We develop Photon Field Networks —a phase-function-aware, multi-light neural representation of indirect volumetric global illumination. The fields are trained on multi-phase photon caches that we compute a priori. Training can be done within seconds, after which the fields can be used in various rendering tasks. To showcase their potential, we develop a custom neural path tracer, with which our photon fields achieve interactive framerates even on large datasets. We conduct in-depth evaluations of the method&#39;s performance, including visual quality, stochastic noise, inference and rendering speeds, and accuracy regarding illumination and phase function awareness. Results are compared to ray marching, path tracing and photon mapping. Our findings show that Photon Field Networks can faithfully represent indirect global illumination within the boundaries of the trained phase spectrum while exhibiting less stochastic noise and rendering at a significantly faster rate than traditional methods.},
  archive      = {J_TVCG},
  author       = {David Bauer and Qi Wu and Kwan-Liu Ma},
  doi          = {10.1109/TVCG.2023.3327107},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {975-985},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Photon field networks for dynamic real-time volumetric global illumination},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptively placed multi-grid scene representation networks
for large-scale data visualization. <em>TVCG</em>, <em>30</em>(1),
965–974. (<a href="https://doi.org/10.1109/TVCG.2023.3327194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene representation networks (SRNs) have been recently proposed for compression and visualization of scientific data. However, state-of-the-art SRNs do not adapt the allocation of available network parameters to the complex features found in scientific data, leading to a loss in reconstruction quality. We address this shortcoming with an adaptively placed multi-grid SRN (APMGSRN) and propose a domain decomposition training and inference technique for accelerated parallel training on multi-GPU systems. We also release an open-source neural volume rendering application that allows plug-and-play rendering with any PyTorch-based SRN. Our proposed APMGSRN architecture uses multiple spatially adaptive feature grids that learn where to be placed within the domain to dynamically allocate more neural network resources where error is high in the volume, improving state-of-the-art reconstruction accuracy of SRNs for scientific data without requiring expensive octree refining, pruning, and traversal like previous adaptive models. In our domain decomposition approach for representing large-scale data, we train an set of APMGSRNs in parallel on separate bricks of the volume to reduce training time while avoiding overhead necessary for an out-of-core solution for volumes too large to fit in GPU memory. After training, the lightweight SRNs are used for realtime neural volume rendering in our open-source renderer, where arbitrary view angles and transfer functions can be explored. A copy of this paper, all code, all models used in our experiments, and all supplemental materials and videos are available at https://github.com/skywolf829/APMGSRN .},
  archive      = {J_TVCG},
  author       = {Skylar W. Wurster and Tianyu Xiong and Han-Wei Shen and Hanqi Guo and Tom Peterka},
  doi          = {10.1109/TVCG.2023.3327194},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {965-974},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Adaptively placed multi-grid scene representation networks for large-scale data visualization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Why change my design: Explaining poorly constructed
visualization designs with explorable explanations. <em>TVCG</em>,
<em>30</em>(1), 955–964. (<a
href="https://doi.org/10.1109/TVCG.2023.3327155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although visualization tools are widely available and accessible, not everyone knows the best practices and guidelines for creating accurate and honest visual representations of data. Numerous books and articles have been written to expose the misleading potential of poorly constructed charts and teach people how to avoid being deceived by them or making their own mistakes. These readings use various rhetorical devices to explain the concepts to their readers. In our analysis of a collection of books, online materials, and a design workshop, we identified six common explanation methods. To assess the effectiveness of these methods, we conducted two crowdsourced studies (each with $N=125$ ) to evaluate their ability to teach and persuade people to make design changes. In addition to these existing methods, we brought in the idea of Explorable Explanations, which allows readers to experiment with different chart settings and observe how the changes are reflected in the visualization. While we did not find significant differences across explanation methods, the results of our experiments indicate that, following the exposure to the explanations, the participants showed improved proficiency in identifying deceptive charts and were more receptive to proposed alterations of the visualization design. We discovered that participants were willing to accept more than 60% of the proposed adjustments in the persuasiveness assessment. Nevertheless, we found no significant differences among different explanation methods in convincing participants to accept the modifications.},
  archive      = {J_TVCG},
  author       = {Leo Yu-Ho Lo and Yifan Cao and Leni Yang and Huamin Qu},
  doi          = {10.1109/TVCG.2023.3327155},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {955-964},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Why change my design: Explaining poorly constructed visualization designs with explorable explanations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). InkSight: Leveraging sketch interaction for documenting
chart findings in computational notebooks. <em>TVCG</em>,
<em>30</em>(1), 944–954. (<a
href="https://doi.org/10.1109/TVCG.2023.3327170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational notebooks have become increasingly popular for exploratory data analysis due to their ability to support data exploration and explanation within a single document. Effective documentation for explaining chart findings during the exploration process is essential as it helps recall and share data analysis. However, documenting chart findings remains a challenge due to its time-consuming and tedious nature. While existing automatic methods alleviate some of the burden on users, they often fail to cater to users&#39; specific interests. In response to these limitations, we present InkSight, a mixed-initiative computational notebook plugin that generates finding documentation based on the user&#39;s intent. InkSight allows users to express their intent in specific data subsets through sketching atop visualizations intuitively. To facilitate this, we designed two types of sketches, i.e., open-path and closed-path sketch. Upon receiving a user&#39;s sketch, InkSight identifies the sketch type and corresponding selected data items. Subsequently, it filters data fact types based on the sketch and selected data items before employing existing automatic data fact recommendation algorithms to infer data facts. Using large language models (GPT-3.5), InkSight converts data facts into effective natural language documentation. Users can conveniently fine-tune the generated documentation within InkSight. A user study with 12 participants demonstrated the usability and effectiveness of InkSight in expressing user intent and facilitating chart finding documentation.},
  archive      = {J_TVCG},
  author       = {Yanna Lin and Haotian Li and Leni Yang and Aoyu Wu and Huamin Qu},
  doi          = {10.1109/TVCG.2023.3327170},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {944-954},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {InkSight: Leveraging sketch interaction for documenting chart findings in computational notebooks},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dupo: A mixed-initiative authoring tool for responsive
visualization. <em>TVCG</em>, <em>30</em>(1), 934–943. (<a
href="https://doi.org/10.1109/TVCG.2023.3326583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing responsive visualizations for various screen types can be tedious as authors must manage multiple chart versions across design iterations. Automated approaches for responsive visualization must take into account the user&#39;s need for agency in exploring possible design ideas and applying customizations based on their own goals. We design and implement Dupo, a mixedinitiative approach to creating responsive visualizations that combines the agency afforded by a manual interface with automation provided by a recommender system. Given an initial design, users can browse automated design suggestions for a different screen type and make edits to a chosen design, thereby supporting quick prototyping and customizability. Dupo employs a two-step recommender pipeline that first suggests significant design changes ( Exploration ) followed by more subtle changes ( Alteration ). We evaluated Dupo with six expert responsive visualization authors. While creating responsive versions of a source design in Dupo, participants could reason about different design suggestions without having to manually prototype them, and thus avoid prematurely fixating on a particular design. This process led participants to create designs that they were satisfied with but which they had previously overlooked.},
  archive      = {J_TVCG},
  author       = {Hyeok Kim and Ryan Rossi and Jessica Hullman and Jane Hoffswell},
  doi          = {10.1109/TVCG.2023.3326583},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {934-943},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dupo: A mixed-initiative authoring tool for responsive visualization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Designing for ambiguity in visual analytics: Lessons from
risk assessment and prediction. <em>TVCG</em>, <em>30</em>(1), 924–933.
(<a href="https://doi.org/10.1109/TVCG.2023.3326571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ambiguity is pervasive in the complex sensemaking domains of risk assessment and prediction but there remains little research on how to design visual analytics tools to accommodate it. We report on findings from a qualitative study based on a conceptual framework of sensemaking processes to investigate how both new visual analytics designs and existing tools, primarily data tables, support the cognitive work demanded in avalanche forecasting. While both systems yielded similar analytic outcomes we observed differences in ambiguous sensemaking and the analytic actions either afforded. Our findings challenge conventional visualization design guidance in both perceptual and interaction design, highlighting the need for data interfaces that encourage reflection, provoke alternative interpretations, and support the inherently ambiguous nature of sensemaking in this critical application. We review how different visual and interactive forms support or impede analytic processes and introduce “gisting” as a significant yet unexplored analytic action for visual analytics research. We conclude with design implications for enabling ambiguity in visual analytics tools to scaffold sensemaking in risk assessment.},
  archive      = {J_TVCG},
  author       = {Stan Nowak and Lyn Bartram},
  doi          = {10.1109/TVCG.2023.3326571},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {924-933},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Designing for ambiguity in visual analytics: Lessons from risk assessment and prediction},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A computational design pipeline to fabricate sensing network
physicalizations. <em>TVCG</em>, <em>30</em>(1), 913–923. (<a
href="https://doi.org/10.1109/TVCG.2023.3327198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interaction is critical for data analysis and sensemaking. However, designing interactive physicalizations is challenging as it requires cross-disciplinary knowledge in visualization, fabrication, and electronics. Interactive physicalizations are typically produced in an unstructured manner, resulting in unique solutions for a specific dataset, problem, or interaction that cannot be easily extended or adapted to new scenarios or future physicalizations. To mitigate these challenges, we introduce a computational design pipeline to 3D print network physicalizations with integrated sensing capabilities. Networks are ubiquitous, yet their complex geometry also requires significant engineering considerations to provide intuitive, effective interactions for exploration. Using our pipeline, designers can readily produce network physicalizations supporting selection —the most critical atomic operation for interaction—by touch through capacitive sensing and computational inference. Our computational design pipeline introduces a new design paradigm by concurrently considering the form and interactivity of a physicalization into one cohesive fabrication workflow. We evaluate our approach using (i) computational evaluations, (ii) three usage scenarios focusing on general visualization tasks, and (iii) expert interviews. The design paradigm introduced by our pipeline can lower barriers to physicalization research, creation, and adoption.},
  archive      = {J_TVCG},
  author       = {S. Sandra Bae and Takanori Fujiwara and Anders Ynnerman and Ellen Yi-Luen Do and Michael L. Rivera and Danielle Albers Szafir},
  doi          = {10.1109/TVCG.2023.3327198},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {913-923},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A computational design pipeline to fabricate sensing network physicalizations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large-scale evaluation of topic models and dimensionality
reduction methods for 2D text spatialization. <em>TVCG</em>,
<em>30</em>(1), 902–912. (<a
href="https://doi.org/10.1109/TVCG.2023.3326569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topic models are a class of unsupervised learning algorithms for detecting the semantic structure within a text corpus. Together with a subsequent dimensionality reduction algorithm, topic models can be used for deriving spatializations for text corpora as two-dimensional scatter plots, reflecting semantic similarity between the documents and supporting corpus analysis. Although the choice of the topic model, the dimensionality reduction, and their underlying hyperparameters significantly impact the resulting layout, it is unknown which particular combinations result in high-quality layouts with respect to accuracy and perception metrics. To investigate the effectiveness of topic models and dimensionality reduction methods for the spatialization of corpora as two-dimensional scatter plots (or basis for landscape-type visualizations), we present a large-scale, benchmark-based computational evaluation. Our evaluation consists of (1) a set of corpora, (2) a set of layout algorithms that are combinations of topic models and dimensionality reductions, and (3) quality metrics for quantifying the resulting layout. The corpora are given as document-term matrices, and each document is assigned to a thematic class. The chosen metrics quantify the preservation of local and global properties and the perceptual effectiveness of the two-dimensional scatter plots. By evaluating the benchmark on a computing cluster, we derived a multivariate dataset with over 45 000 individual layouts and corresponding quality metrics. Based on the results, we propose guidelines for the effective design of text spatializations that are based on topic models and dimensionality reductions. As a main result, we show that interpretable topic models are beneficial for capturing the structure of text corpora. We furthermore recommend the use of t-SNE as a subsequent dimensionality reduction.},
  archive      = {J_TVCG},
  author       = {Daniel Atzberger and Tim Cech and Matthias Trapp and Rico Richter and Willy Scheibel and Jürgen Döllner and Tobias Schreck},
  doi          = {10.1109/TVCG.2023.3326569},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {902-912},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Large-scale evaluation of topic models and dimensionality reduction methods for 2D text spatialization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TransforLearn: Interactive visual tutorial for the
transformer model. <em>TVCG</em>, <em>30</em>(1), 891–901. (<a
href="https://doi.org/10.1109/TVCG.2023.3327353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread adoption of Transformers in deep learning, serving as the core framework for numerous large-scale language models, has sparked significant interest in understanding their underlying mechanisms. However, beginners face difficulties in comprehending and learning Transformers due to its complex structure and abstract data representation. We present TransforLearn, the first interactive visual tutorial designed for deep learning beginners and non-experts to comprehensively learn about Transformers. TransforLearn supports interactions for architecture-driven exploration and task-driven exploration, providing insight into different levels of model details and their working processes. It accommodates interactive views of each layer&#39;s operation and mathematical formula, helping users to understand the data flow of long text sequences. By altering the current decoder-based recursive prediction results and combining the downstream task abstractions, users can deeply explore model processes. Our user study revealed that the interactions of TransforLearn are positively received. We observe that TransforLearn facilitates users&#39; accomplishment of study tasks and a grasp of key concepts in Transformer effectively.},
  archive      = {J_TVCG},
  author       = {Lin Gao and Zekai Shao and Ziqin Luo and Haibo Hu and Cagatay Turkay and Siming Chen},
  doi          = {10.1109/TVCG.2023.3327353},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {891-901},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TransforLearn: Interactive visual tutorial for the transformer model},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Action-evaluator: A visualization approach for player action
evaluation in soccer. <em>TVCG</em>, <em>30</em>(1), 880–890. (<a
href="https://doi.org/10.1109/TVCG.2023.3326524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In soccer, player action evaluation provides a fine-grained method to analyze player performance and plays an important role in improving winning chances in future matches. However, previous studies on action evaluation only provide a score for each action, and hardly support inspecting and comparing player actions integrated with complex match context information such as team tactics and player locations. In this work, we collaborate with soccer analysts and coaches to characterize the domain problems of evaluating player performance based on action scores. We design a tailored visualization of soccer player actions that places the action choice together with the tactic it belongs to as well as the player locations in the same view. Based on the design, we introduce a visual analytics system, Action-Evaluator, to facilitate a comprehensive player action evaluation through player navigation, action investigation, and action explanation. With the system, analysts can find players to be analyzed efficiently, learn how they performed under various match situations, and obtain valuable insights to improve their action choices. The usefulness and effectiveness of this work are demonstrated by two case studies on a real-world dataset and an expert interview.},
  archive      = {J_TVCG},
  author       = {Anqi Cao and Xiao Xie and Mingxu Zhou and Hui Zhang and Mingliang Xu and Yingcai Wu},
  doi          = {10.1109/TVCG.2023.3326524},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {880-890},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Action-evaluator: A visualization approach for player action evaluation in soccer},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SkiVis: Visual exploration and route planning in ski
resorts. <em>TVCG</em>, <em>30</em>(1), 869–879. (<a
href="https://doi.org/10.1109/TVCG.2023.3326940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal ski route selection is a challenge based on a multitude of factors, such as the steepness, compass direction, or crowdedness. The personal preferences of every skier towards these factors require individual adaptations, which aggravate this task. Current approaches within this domain do not combine automated routing capabilities with user preferences, missing out on the possibility of integrating domain knowledge in the analysis process. We introduce SkiVis, a visual analytics application to interactively explore ski slopes and provide routing recommendations based on user preferences. In collaboration with ski guides and enthusiasts, we elicited requirements and guidelines for such an application and propose different workflows depending on the skiers&#39; familiarity with the resort. In a case study on the resort of Ski Arlberg, we illustrate how to leverage volunteered geographic information to enable a numerical comparison between slopes. We evaluated our approach through a pair-analytics study and demonstrate how it supports skiers in discovering relevant and preference-based ski routes. Besides the tasks investigated in the study, we derive additional use cases from the interviews that showcase the further potential of SkiVis, and contribute directions for further research opportunities.},
  archive      = {J_TVCG},
  author       = {Julius Rauscher and Raphael Buchmüller and Daniel A. Keim and Matthias Miller},
  doi          = {10.1109/TVCG.2023.3326940},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {869-879},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SkiVis: Visual exploration and route planning in ski resorts},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). HoopInSight: Analyzing and comparing basketball shooting
performance through visualization. <em>TVCG</em>, <em>30</em>(1),
858–868. (<a href="https://doi.org/10.1109/TVCG.2023.3326910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data visualization has the power to revolutionize sports. For example, the rise of shot maps has changed basketball strategy by visually illustrating where “good/bad” shots are taken from. As a result, professional basketball teams today take shots from very different positions on the court than they did 20 years ago. Although the shot map has transformed many facets of the game, there is still much room for improvement to support richer and more complex analytical tasks. More specifically, we believe that the lack of sufficient interactivity to support various analytical queries and the inability to visually compare differences across situations are significant limitations of current shot maps. To address these limitations and showcase new possibilities, we designed and developed HoopInSight, an interactive visualization system that centers around a novel spatial comparison visual technique, enhancing the capabilities of shot maps in basketball analytics. This article presents the system, with a focus on our proposed visual technique and its accompanying interactions, all designed to promote comparison of two different scenarios. Furthermore, we provide reflections on and a discussion of relevant issues, including considerations for designing spatial comparison techniques, the scalability and transferability of this approach, and the benefits and pitfalls of designing as domain experts.},
  archive      = {J_TVCG},
  author       = {Yu Fu and John Stasko},
  doi          = {10.1109/TVCG.2023.3326910},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {858-868},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HoopInSight: Analyzing and comparing basketball shooting performance through visualization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FSLens: A visual analytics approach to evaluating and
optimizing the spatial layout of fire stations. <em>TVCG</em>,
<em>30</em>(1), 847–857. (<a
href="https://doi.org/10.1109/TVCG.2023.3327077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The provision of fire services plays a vital role in ensuring the safety of residents&#39; lives and property. The spatial layout of fire stations is closely linked to the efficiency of fire rescue operations. Traditional approaches have primarily relied on mathematical planning models to generate appropriate layouts by summarizing relevant evaluation criteria. However, this optimization process presents significant challenges due to the extensive decision space, inherent conflicts among criteria, and decision-makers&#39; preferences. To address these challenges, we propose FSLens , an interactive visual analytics system that enables in-depth evaluation and rational optimization of fire station layout. Our approach integrates fire records and correlation features to reveal fire occurrence patterns and influencing factors using spatiotemporal sequence forecasting. We design an interactive visualization method to explore areas within the city that are potentially under-resourced for fire service based on the fire distribution and existing fire station layout. Moreover, we develop a collaborative human-computer multi-criteria decision model that generates multiple candidate solutions for optimizing firefighting resources within these areas. We simulate and compare the impact of different solutions on the original layout through well-designed visualizations, providing decision-makers with the most satisfactory solution. We demonstrate the effectiveness of our approach through one case study with real-world datasets. The feedback from domain experts indicates that our system helps them to better identify and improve potential gaps in the current fire station layout.},
  archive      = {J_TVCG},
  author       = {Longfei Chen and He Wang and Yang Ouyang and Yang Zhou and Naiyu Wang and Quan Li},
  doi          = {10.1109/TVCG.2023.3327077},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {847-857},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FSLens: A visual analytics approach to evaluating and optimizing the spatial layout of fire stations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TactualPlot: Spatializing data as sound using sensory
substitution for touchscreen accessibility. <em>TVCG</em>,
<em>30</em>(1), 836–846. (<a
href="https://doi.org/10.1109/TVCG.2023.3326937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tactile graphics are one of the best ways for a blind person to perceive a chart using touch, but their fabrication is often costly, time-consuming, and does not lend itself to dynamic exploration. Refreshable haptic displays tend to be expensive and thus unavailable to most blind individuals. We propose T actual P lot , an approach to sensory substitution where touch interaction yields auditory (sonified) feedback. The technique relies on embodied cognition for spatial awareness—i.e., individuals can perceive 2D touch locations of their fingers with reference to other 2D locations such as the relative locations of other fingers or chart characteristics that are visualized on touchscreens. Combining touch and sound in this way yields a scalable data exploration method for scatterplots where the data density under the user&#39;s fingertips is sampled. The sample regions can optionally be scaled based on how quickly the user moves their hand. Our development of TactualPlot was informed by formative design sessions with a blind collaborator, whose practice while using tactile scatterplots caused us to expand the technique for multiple fingers. We present results from an evaluation comparing our TactualPlot interaction technique to tactile graphics printed on swell touch paper.},
  archive      = {J_TVCG},
  author       = {Pramod Chundury and Yasmin Reyazuddin and J. Bern Jordan and Jonathan Lazar and Niklas Elmqvist},
  doi          = {10.1109/TVCG.2023.3326937},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {836-846},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TactualPlot: Spatializing data as sound using sensory substitution for touchscreen accessibility},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reducing ambiguities in line-based density plots by
image-space colorization. <em>TVCG</em>, <em>30</em>(1), 825–835. (<a
href="https://doi.org/10.1109/TVCG.2023.3327149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Line-based density plots are used to reduce visual clutter in line charts with a multitude of individual lines. However, these traditional density plots are often perceived ambiguously, which obstructs the user&#39;s identification of underlying trends in complex datasets. Thus, we propose a novel image space coloring method for line-based density plots that enhances their interpretability. Our method employs color not only to visually communicate data density but also to highlight similar regions in the plot, allowing users to identify and distinguish trends easily. We achieve this by performing hierarchical clustering based on the lines passing through each region and mapping the identified clusters to the hue circle using circular MDS. Additionally, we propose a heuristic approach to assign each line to the most probable cluster, enabling users to analyze density and individual lines. We motivate our method by conducting a small-scale user study, demonstrating the effectiveness of our method using synthetic and real-world datasets, and providing an interactive online tool for generating colored line-based density plots.},
  archive      = {J_TVCG},
  author       = {Yumeng Xue and Patrick Paetzold and Rebecca Kehlbeck and Bin Chen and Kin Chung Kwan and Yunhai Wang and Oliver Deussen},
  doi          = {10.1109/TVCG.2023.3327149},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {825-835},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Reducing ambiguities in line-based density plots by image-space colorization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NL2Color: Refining color palettes for charts with natural
language. <em>TVCG</em>, <em>30</em>(1), 814–824. (<a
href="https://doi.org/10.1109/TVCG.2023.3326522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Choice of color is critical to creating effective charts with an engaging, enjoyable, and informative reading experience. However, designing a good color palette for a chart is a challenging task for novice users who lack related design expertise. For example, they often find it difficult to articulate their abstract intentions and translate these intentions into effective editing actions to achieve a desired outcome. In this work, we present NL2Color, a tool that allows novice users to refine chart color palettes using natural language expressions of their desired outcomes. We first collected and categorized a dataset of 131 triplets, each consisting of an original color palette of a chart, an editing intent, and a new color palette designed by human experts according to the intent. Our tool employs a large language model (LLM) to substitute the colors in original palettes and produce new color palettes by selecting some of the triplets as few-shot prompts. To evaluate our tool, we conducted a comprehensive two-stage evaluation, including a crowd-sourcing study ( $\mathrm{N}=71$ ) and a within-subjects user study ( $\mathrm{N}=12$ ). The results indicate that the quality of the color palettes revised by NL2Color has no significantly large difference from those designed by human experts. The participants who used NL2Color obtained revised color palettes to their satisfaction in a shorter period and with less effort.},
  archive      = {J_TVCG},
  author       = {Chuhan Shi and Weiwei Cui and Chengzhong Liu and Chengbo Zheng and Haidong Zhang and Qiong Luo and Xiaojuan Ma},
  doi          = {10.1109/TVCG.2023.3326522},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {814-824},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {NL2Color: Refining color palettes for charts with natural language},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data navigator: An accessibility-centered data navigation
toolkit. <em>TVCG</em>, <em>30</em>(1), 803–813. (<a
href="https://doi.org/10.1109/TVCG.2023.3327393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Making data visualizations accessible for people with disabilities remains a significant challenge in current practitioner efforts. Existing visualizations often lack an underlying navigable structure, fail to engage necessary input modalities, and rely heavily on visual-only rendering practices. These limitations exclude people with disabilities, especially users of assistive technologies. To address these challenges, we present Data Navigator: a system built on a dynamic graph structure, enabling developers to construct navigable lists, trees, graphs, and flows as well as spatial, diagrammatic, and geographic relations. Data Navigator supports a wide range of input modalities: screen reader, keyboard, speech, gesture detection, and even fabricated assistive devices. We present 3 case examples with Data Navigator, demonstrating we can provide accessible navigation structures on top of raster images, integrate with existing toolkits at scale, and rapidly develop novel prototypes. Data Navigator is a step towards making accessible data visualizations easier to design and implement.},
  archive      = {J_TVCG},
  author       = {Frank Elavsky and Lucas Nadolskis and Dominik Moritz},
  doi          = {10.1109/TVCG.2023.3327393},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {803-813},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Data navigator: An accessibility-centered data navigation toolkit},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Guaranteed visibility in scatterplots with tolerance.
<em>TVCG</em>, <em>30</em>(1), 792–802. (<a
href="https://doi.org/10.1109/TVCG.2023.3326596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2D visualizations, visibility of every datum&#39;s representation is crucial to ease the completion of visual tasks. Such a guarantee is barely respected in complex visualizations, mainly because of overdraws between datum representations that hide parts of the information (e.g., outliers). The literature proposes various Layout Adjustment algorithms to improve the readability of visualizations that suffer from this issue. Manipulating the data in high-dimensional, geometric or visual space; they rely on different strategies with their own strengths and weaknesses. Moreover, most of these algorithms are computationally expensive as they search for an exact solution in the geometric space and do not scale well to large datasets. This article proposes GIST, a layout adjustment algorithm that aims at optimizing three criteria: (i) node visibility guarantee (at least 1 pixel), (ii) node size maximization, and (iii) the original layout preservation. This is achieved by combining a search for the maximum node size that enables to draw all the data points without overlaps, with a limited budget of movements (i.e., limiting the distortions of the original layout). The method&#39;s basis relies on the idea that it is not necessary for two data representations to be strictly not overlapping in order to guarantee their visibility in visual space. Our algorithm therefore uses a tolerance in the geometric space to determine the overlaps between pairs of data. The tolerance is optimized such that the approximation computed in the geometric space can lead to visualization without noticeable overdraw after the data rendering rasterization. In addition, such an approximation helps to ease the algorithm&#39;s convergence as it reduces the number of constraints to resolve, enabling it to handle large datasets. We demonstrate the effectiveness of our approach by comparing its results to those of state-of-the-art methods on several large datasets.},
  archive      = {J_TVCG},
  author       = {Loann Giovannangeli and Frederic Lalanne and Romain Giot and Romain Bourqui},
  doi          = {10.1109/TVCG.2023.3326596},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {792-802},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Guaranteed visibility in scatterplots with tolerance},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Classes are not clusters: Improving label-based evaluation
of dimensionality reduction. <em>TVCG</em>, <em>30</em>(1), 781–791. (<a
href="https://doi.org/10.1109/TVCG.2023.3327187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common way to evaluate the reliability of dimensionality reduction (DR) embeddings is to quantify how well labeled classes form compact, mutually separated clusters in the embeddings. This approach is based on the assumption that the classes stay as clear clusters in the original high-dimensional space. However, in reality, this assumption can be violated; a single class can be fragmented into multiple separated clusters, and multiple classes can be merged into a single cluster. We thus cannot always assure the credibility of the evaluation using class labels. In this paper, we introduce two novel quality measures—Label-Trustworthiness and Label-Continuity (Label-T&amp;C)—advancing the process of DR evaluation based on class labels. Instead of assuming that classes are well-clustered in the original space, Label-T&amp;C work by (1) estimating the extent to which classes form clusters in the original and embedded spaces and (2) evaluating the difference between the two. A quantitative evaluation showed that Label-T &amp;amp;C outperform widely used DR evaluation measures (e.g., Trustworthiness and Continuity, Kullback-Leibler divergence) in terms of the accuracy in assessing how well DR embeddings preserve the cluster structure, and are also scalable. Moreover, we present case studies demonstrating that Label-T &amp;amp;C can be successfully used for revealing the intrinsic characteristics of DR techniques and their hyperparameters.},
  archive      = {J_TVCG},
  author       = {Hyeon Jeon and Yun-Hsin Kuo and Michaël Aupetit and Kwan-Liu Ma and Jinwook Seo},
  doi          = {10.1109/TVCG.2023.3327187},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {781-791},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Classes are not clusters: Improving label-based evaluation of dimensionality reduction},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CLAMS: A cluster ambiguity measure for estimating perceptual
variability in visual clustering. <em>TVCG</em>, <em>30</em>(1),
770–780. (<a href="https://doi.org/10.1109/TVCG.2023.3327201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual clustering is a common perceptual task in scatterplots that supports diverse analytics tasks (e.g., cluster identification). However, even with the same scatterplot, the ways of perceiving clusters (i.e., conducting visual clustering) can differ due to the differences among individuals and ambiguous cluster boundaries. Although such perceptual variability casts doubt on the reliability of data analysis based on visual clustering, we lack a systematic way to efficiently assess this variability. In this research, we study perceptual variability in conducting visual clustering, which we call Cluster Ambiguity . To this end, we introduce CLAMS , a data-driven visual quality measure for automatically predicting cluster ambiguity in monochrome scatterplots. We first conduct a qualitative study to identify key factors that affect the visual separation of clusters (e.g., proximity or size difference between clusters). Based on study findings, we deploy a regression module that estimates the human-judged separability of two clusters. Then, CLAMS predicts cluster ambiguity by analyzing the aggregated results of all pairwise separability between clusters that are generated by the module. CLAMS outperforms widely-used clustering techniques in predicting ground truth cluster ambiguity. Meanwhile, CLAMS exhibits performance on par with human annotators. We conclude our work by presenting two applications for optimizing and benchmarking data mining techniques using CLAMS. The interactive demo of CLAMS is available at clusterambiguity.dev.},
  archive      = {J_TVCG},
  author       = {Hyeon Jeon and Ghulam Jilani Quadri and Hyunwook Lee and Paul Rosen and Danielle Albers Szafir and Jinwook Seo},
  doi          = {10.1109/TVCG.2023.3327201},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {770-780},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CLAMS: A cluster ambiguity measure for estimating perceptual variability in visual clustering},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual analysis of displacement processes in porous media
using spatio-temporal flow graphs. <em>TVCG</em>, <em>30</em>(1),
759–769. (<a href="https://doi.org/10.1109/TVCG.2023.3326931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We developed a new approach comprised of different visualizations for the comparative spatio-temporal analysis of displacement processes in porous media. We aim to analyze and compare ensemble datasets from experiments to gain insight into the influence of different parameters on fluid flow. To capture the displacement of a defending fluid by an invading fluid, we first condense an input image series to a single time map. From this map, we generate a spatio-temporal flow graph covering the whole process. This graph is further simplified to only reflect topological changes in the movement of the invading fluid. Our interactive tools allow the visual analysis of these processes by visualizing the graph structure and the context of the experimental setup, as well as by providing charts for multiple metrics. We apply our approach to analyze and compare ensemble datasets jointly with domain experts, where we vary either fluid properties or the solid structure of the porous medium. We finally report the generated insights from the domain experts and discuss our contribution&#39;s advantages, generality, and limitations.},
  archive      = {J_TVCG},
  author       = {Alexander Straub and Nikolaos Karadimitriou and Guido Reina and Steffen Frey and Holger Steeb and Thomas Ertl},
  doi          = {10.1109/TVCG.2023.3326931},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {759-769},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual analysis of displacement processes in porous media using spatio-temporal flow graphs},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ViMO - visual analysis of neuronal connectivity motifs.
<em>TVCG</em>, <em>30</em>(1), 748–758. (<a
href="https://doi.org/10.1109/TVCG.2023.3327388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in high-resolution connectomics provide researchers with access to accurate petascale reconstructions of neuronal circuits and brain networks for the first time. Neuroscientists are analyzing these networks to better understand information processing in the brain. In particular, scientists are interested in identifying specific small network motifs, i.e., repeating subgraphs of the larger brain network that are believed to be neuronal building blocks. Although such motifs are typically small (e.g., 2–6 neurons), the vast data sizes and intricate data complexity present significant challenges to the search and analysis process. To analyze these motifs, it is crucial to review instances of a motif in the brain network and then map the graph structure to detailed 3D reconstructions of the involved neurons and synapses. We present Vimo , an interactive visual approach to analyze neuronal motifs and motif chains in large brain networks. Experts can sketch network motifs intuitively in a visual interface and specify structural properties of the involved neurons and synapses to query large connectomics datasets. Motif instances (MIs) can be explored in high-resolution 3D renderings. To simplify the analysis of MIs, we designed a continuous focus&amp;context metaphor inspired by visual abstractions. This allows users to transition from a highly-detailed rendering of the anatomical structure to views that emphasize the underlying motif structure and synaptic connectivity. Furthermore, Vimo supports the identification of motif chains where a motif is used repeatedly (e.g., 2–4 times) to form a larger network structure. We evaluate Vimo in a user study and an in-depth case study with seven domain experts on motifs in a large connectome of the fruit fly, including more than 21,000 neurons and 20 million synapses. We find that Vimo enables hypothesis generation and confirmation through fast analysis iterations and connectivity highlighting.},
  archive      = {J_TVCG},
  author       = {Jakob Troidl and Simon Warchol and Jinhan Choi and Jordan Matelsky and Nagaraju Dhanyasi and Xueying Wang and Brock Wester and Donglai Wei and Jeff W. Lichtman and Hanspeter Pfister and Johanna Beyer},
  doi          = {10.1109/TVCG.2023.3327388},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {748-758},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ViMO - Visual analysis of neuronal connectivity motifs},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PROWIS: A visual approach for building, managing, and
analyzing weather simulation ensembles at runtime. <em>TVCG</em>,
<em>30</em>(1), 738–747. (<a
href="https://doi.org/10.1109/TVCG.2023.3326514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weather forecasting is essential for decision-making and is usually performed using numerical modeling. Numerical weather models, in turn, are complex tools that require specialized training and laborious setup and are challenging even for weather experts. Moreover, weather simulations are data-intensive computations and may take hours to days to complete. When the simulation is finished, the experts face challenges analyzing its outputs, a large mass of spatiotemporal and multivariate data. From the simulation setup to the analysis of results, working with weather simulations involves several manual and error-prone steps. The complexity of the problem increases exponentially when the experts must deal with ensembles of simulations, a frequent task in their daily duties. To tackle these challenges, we propose ProWis: an interactive and provenance-oriented system to help weather experts build, manage, and analyze simulation ensembles at runtime. Our system follows a human-in-the-loop approach to enable the exploration of multiple atmospheric variables and weather scenarios. ProWis was built in close collaboration with weather experts, and we demonstrate its effectiveness by presenting two case studies of rainfall events in Brazil.},
  archive      = {J_TVCG},
  author       = {Carolina Veiga Ferreira de Souza and Suzanna Maria Bonnet and Daniel de Oliveira and Marcio Cataldi and Fabio Miranda and Marcos Lage},
  doi          = {10.1109/TVCG.2023.3326514},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {738-747},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PROWIS: A visual approach for building, managing, and analyzing weather simulation ensembles at runtime},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MolSieve: A progressive visual analytics system for
molecular dynamics simulations. <em>TVCG</em>, <em>30</em>(1), 727–737.
(<a href="https://doi.org/10.1109/TVCG.2023.3326584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular Dynamics (MD) simulations are ubiquitous in cutting-edge physio-chemical research. They provide critical insights into how a physical system evolves over time given a model of interatomic interactions. Understanding a system&#39;s evolution is key to selecting the best candidates for new drugs, materials for manufacturing, and countless other practical applications. With today&#39;s technology, these simulations can encompass millions of unit transitions between discrete molecular structures, spanning up to several milliseconds of real time. Attempting to perform a brute-force analysis with data-sets of this size is not only computationally impractical, but would not shed light on the physically-relevant features of the data. Moreover, there is a need to analyze simulation ensembles in order to compare similar processes in differing environments. These problems call for an approach that is analytically transparent, computationally efficient, and flexible enough to handle the variety found in materials-based research. In order to address these problems, we introduce MolSieve, a progressive visual analytics system that enables the comparison of multiple long-duration simulations. Using MolSieve, analysts are able to quickly identify and compare regions of interest within immense simulations through its combination of control charts, data-reduction techniques, and highly informative visual components. A simple programming interface is provided which allows experts to fit MolSieve to their needs. To demonstrate the efficacy of our approach, we present two case studies of MolSieve and report on findings from domain collaborators.},
  archive      = {J_TVCG},
  author       = {Rostyslav Hnatyshyn and Jieqiong Zhao and Danny Perez and James Ahrens and Ross Maciejewski},
  doi          = {10.1109/TVCG.2023.3326584},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {727-737},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MolSieve: A progressive visual analytics system for molecular dynamics simulations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extract and characterize hairpin vortices in turbulent
flows. <em>TVCG</em>, <em>30</em>(1), 716–726. (<a
href="https://doi.org/10.1109/TVCG.2023.3326603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hairpin vortices are one of the most important vortical structures in turbulent flows. Extracting and characterizing hairpin vortices provides useful insight into many behaviors in turbulent flows. However, hairpin vortices have complex configurations and might be entangled with other vortices, making their extraction difficult. In this work, we introduce a framework to extract and separate hairpin vortices in shear driven turbulent flows for their study. Our method first extracts general vortical regions with a region-growing strategy based on certain vortex criteria (e.g., $\lambda_{2}$ ) and then separates those vortices with the help of progressive extraction of ( $\lambda_{2}$ ) iso-surfaces in a top-down fashion. This leads to a hierarchical tree representing the spatial proximity and merging relation of vortices. After separating individual vortices, their shape and orientation information is extracted. Candidate hairpin vortices are identified based on their shape and orientation information as well as their physical characteristics. An interactive visualization system is developed to aid the exploration, classification, and analysis of hairpin vortices based on their geometric and physical attributes. We also present additional use cases of the proposed system for the analysis and study of general vortices in other types of flows.},
  archive      = {J_TVCG},
  author       = {Adeel Zafar and Di Yang and Guoning Chen},
  doi          = {10.1109/TVCG.2023.3326603},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {716-726},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Extract and characterize hairpin vortices in turbulent flows},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dr. KID: Direct remeshing and k-set isometric decomposition
for scalable physicalization of organic shapes. <em>TVCG</em>,
<em>30</em>(1), 705–715. (<a
href="https://doi.org/10.1109/TVCG.2023.3326595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dr. KID is an algorithm that uses isometric decomposition for the physicalization of potato-shaped organic models in a puzzle fashion. The algorithm begins with creating a simple, regular triangular surface mesh of organic shapes, followed by iterative K-means clustering and remeshing. For clustering, we need similarity between triangles (segments) which is defined as a distance function. The distance function maps each triangle&#39;s shape to a single point in the virtual 3D space. Thus, the distance between the triangles indicates their degree of dissimilarity. K-means clustering uses this distance and sorts segments into $k$ classes. After this, remeshing is applied to minimize the distance between triangles within the same cluster by making their shapes identical. Clustering and remeshing are repeated until the distance between triangles in the same cluster reaches an acceptable threshold. We adopt a curvature-aware strategy to determine the surface thickness and finalize puzzle pieces for 3D printing. Identical hinges and holes are created for assembling the puzzle components. For smoother outcomes, we use triangle subdivision along with curvature-aware clustering, generating curved triangular patches for 3D printing. Our algorithm was evaluated using various models, and the 3D-printed results were analyzed. Findings indicate that our algorithm performs reliably on target organic shapes with minimal loss of input geometry.},
  archive      = {J_TVCG},
  author       = {Dawar Khan and Ciril Bohak and Ivan Viola},
  doi          = {10.1109/TVCG.2023.3326595},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {705-715},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dr. KID: Direct remeshing and K-set isometric decomposition for scalable physicalization of organic shapes},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OW-adapter: Human-assisted open-world object detection with
a few examples. <em>TVCG</em>, <em>30</em>(1), 694–704. (<a
href="https://doi.org/10.1109/TVCG.2023.3326577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-world object detection (OWOD) is an emerging computer vision problem that involves not only the identification of predefined object classes, like what general object detectors do, but also detects new unknown objects simultaneously. Recently, several end-to-end deep learning models have been proposed to address the OWOD problem. However, these approaches face several challenges: a) significant changes in both network architecture and training procedure are required; b) they are trained from scratch, which can not leverage existing pre-trained general detectors; c) costly annotations for all unknown classes are needed. To overcome these challenges, we present a visual analytic framework called OW-Adapter. It acts as an adaptor to enable pre-trained general object detectors to handle the OWOD problem. Specifically, OW-Adapter is designed to identify, summarize, and annotate unknown examples with minimal human effort. Moreover, we introduce a lightweight classifier to learn newly annotated unknown classes and plug the classifier into pre-trained general detectors to detect unknown objects. We demonstrate the effectiveness of our framework through two case studies of different domains, including common object recognition and autonomous driving. The studies show that a simple yet powerful adaptor can extend the capability of pre-trained general detectors to detect unknown objects and improve the performance on known classes simultaneously.},
  archive      = {J_TVCG},
  author       = {Suphanut Jamonnak and Jiajing Guo and Wenbin He and Liang Gou and Liu Ren},
  doi          = {10.1109/TVCG.2023.3326577},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {694-704},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {OW-adapter: Human-assisted open-world object detection with a few examples},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explore your network in minutes: A rapid prototyping toolkit
for understanding neural networks with visual analytics. <em>TVCG</em>,
<em>30</em>(1), 683–693. (<a
href="https://doi.org/10.1109/TVCG.2023.3326575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks attract significant attention in almost every field due to their widespread applications in various tasks. However, developers often struggle with debugging due to the black-box nature of neural networks. Visual analytics provides an intuitive way for developers to understand the hidden states and underlying complex transformations in neural networks. Existing visual analytics tools for neural networks have been demonstrated to be effective in providing useful hints for debugging certain network architectures. However, these approaches are often architecture-specific with strong assumptions of how the network should be understood. This limits their use when the network architecture or the exploration goal changes. In this paper, we present a general model and a programming toolkit, Neural Network Visualization Builder (NNVisBuilder), for prototyping visual analytics systems to understand neural networks. NNVisBuilder covers the common data transformation and interaction model involved in existing tools for exploring neural networks. It enables developers to customize a visual analytics interface for answering their specific questions about networks. NNVisBuilder is compatible with PyTorch so that developers can integrate the visualization code into their learning code seamlessly. We demonstrate the applicability by reproducing several existing visual analytics systems for networks with NNVisBuilder. The source code and some example cases can be found at https://github.com/sysuvis/NVB .},
  archive      = {J_TVCG},
  author       = {Shaoxuan Lai and Wanna Luan and Jun Tao},
  doi          = {10.1109/TVCG.2023.3326575},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {683-693},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Explore your network in minutes: A rapid prototyping toolkit for understanding neural networks with visual analytics},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Are we closing the loop yet? Gaps in the generalizability of
VIS4ML research. <em>TVCG</em>, <em>30</em>(1), 672–682. (<a
href="https://doi.org/10.1109/TVCG.2023.3326591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualization for machine learning (VIS4ML) research aims to help experts apply their prior knowledge to develop, understand, and improve the performance of machine learning models. In conceiving VIS4ML systems, researchers characterize the nature of human knowledge to support human-in-the-loop tasks, design interactive visualizations to make ML components interpretable and elicit knowledge, and evaluate the effectiveness of human-model interchange. We survey recent VIS4ML papers to assess the generalizability of research contributions and claims in enabling human-in-the-loop ML. Our results show potential gaps between the current scope of VIS4ML research and aspirations for its use in practice. We find that while papers motivate that VIS4ML systems are applicable beyond the specific conditions studied, conclusions are often overfitted to non-representative scenarios, are based on interactions with a small set of ML experts and well-understood datasets, fail to acknowledge crucial dependencies, and hinge on decisions that lack justification. We discuss approaches to close the gap between aspirations and research claims and suggest documentation practices to report generality constraints that better acknowledge the exploratory nature of VIS4ML research.},
  archive      = {J_TVCG},
  author       = {Hariharan Subramonyam and Jessica Hullman},
  doi          = {10.1109/TVCG.2023.3326591},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {672-682},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Are we closing the loop yet? gaps in the generalizability of VIS4ML research},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comparative visual analytics framework for evaluating
evolutionary processes in multi-objective optimization. <em>TVCG</em>,
<em>30</em>(1), 661–671. (<a
href="https://doi.org/10.1109/TVCG.2023.3326921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary multi-objective optimization (EMO) algorithms have been demonstrated to be effective in solving multi-criteria decision-making problems. In real-world applications, analysts often employ several algorithms concurrently and compare their solution sets to gain insight into the characteristics of different algorithms and explore a broader range of feasible solutions. However, EMO algorithms are typically treated as black boxes, leading to difficulties in performing detailed analysis and comparisons between the internal evolutionary processes. Inspired by the successful application of visual analytics tools in explainable AI, we argue that interactive visualization can significantly enhance the comparative analysis between multiple EMO algorithms. In this paper, we present a visual analytics framework that enables the exploration and comparison of evolutionary processes in EMO algorithms. Guided by a literature review and expert interviews, the proposed framework addresses various analytical tasks and establishes a multi-faceted visualization design to support the comparative analysis of intermediate generations in the evolution as well as solution sets. We demonstrate the effectiveness of our framework through case studies on benchmarking and real-world multi-objective optimization problems to elucidate how analysts can leverage our framework to inspect and compare diverse algorithms.},
  archive      = {J_TVCG},
  author       = {Yansong Huang and Zherui Zhang and Ao Jiao and Yuxin Ma and Ran Cheng},
  doi          = {10.1109/TVCG.2023.3326921},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {661-671},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A comparative visual analytics framework for evaluating evolutionary processes in multi-objective optimization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Challenges and opportunities in data visualization
education: A call to action. <em>TVCG</em>, <em>30</em>(1), 649–660. (<a
href="https://doi.org/10.1109/TVCG.2023.3327378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is a call to action for research and discussion on data visualization education. As visualization evolves and spreads through our professional and personal lives, we need to understand how to support and empower a broad and diverse community of learners in visualization. Data Visualization is a diverse and dynamic discipline that combines knowledge from different fields, is tailored to suit diverse audiences and contexts, and frequently incorporates tacit knowledge. This complex nature leads to a series of interrelated challenges for data visualization education. Driven by a lack of consolidated knowledge, overview, and orientation for visualization education, the 21 authors of this paper—educators and researchers in data visualization—identify and describe 19 challenges informed by our collective practical experience. We organize these challenges around seven themes People, Goals &amp;amp; Assessment, Environment, Motivation, Methods, Materials , and Change . Across these themes, we formulate 43 research questions to address these challenges. As part of our call to action, we then conclude with 5 cross-cutting opportunities and respective action items: embrace DIVERSITY+INCLUSION, build COMMUNITIES, conduct RESEARCH, act AGILE, and relish RESPONSIBILITY. We aim to inspire researchers, educators and learners to drive visualization education forward and discuss why, how, who and where we educate, as we learn to use visualization to address challenges across many scales and many domains in a rapidly changing world: viseducationchallenges.github.io .},
  archive      = {J_TVCG},
  author       = {Benjamin Bach and Mandy Keck and Fateme Rajabiyazdi and Tatiana Losev and Isabel Meirelles and Jason Dykes and Robert S. Laramee and Mashael AlKadi and Christina Stoiber and Samuel Huron and Charles Perin and Luiz Morais and Wolfgang Aigner and Doris Kosminsky and Magdalena Boucher and Søren Knudsen and Areti Manataki and Jan Aerts and Uta Hinrichs and Jonathan C. Roberts and Sheelagh Carpendale},
  doi          = {10.1109/TVCG.2023.3327378},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {649-660},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Challenges and opportunities in data visualization education: A call to action},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causality-based visual analysis of questionnaire responses.
<em>TVCG</em>, <em>30</em>(1), 638–648. (<a
href="https://doi.org/10.1109/TVCG.2023.3327376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the final stage of questionnaire analysis, causal reasoning is the key to turning responses into valuable insights and actionable items for decision-makers. During the questionnaire analysis, classical statistical methods (e.g., Differences-in-Differences) have been widely exploited to evaluate causality between questions. However, due to the huge search space and complex causal structure in data, causal reasoning is still extremely challenging and time-consuming, and often conducted in a trial-and-error manner. On the other hand, existing visual methods of causal reasoning face the challenge of bringing scalability and expert knowledge together and can hardly be used in the questionnaire scenario. In this work, we present a systematic solution to help analysts effectively and efficiently explore questionnaire data and derive causality. Based on the association mining algorithm, we dig question combinations with potential inner causality and help analysts interactively explore the causal sub-graph of each question combination. Furthermore, leveraging the requirements collected from the experts, we built a visualization tool and conducted a comparative study with the state-of-the-art system to show the usability and efficiency of our system.},
  archive      = {J_TVCG},
  author       = {Renzhong Li and Weiwei Cui and Tianqi Song and Xiao Xie and Rui Ding and Yun Wang and Haidong Zhang and Hong Zhou and Yingcai Wu},
  doi          = {10.1109/TVCG.2023.3327376},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {638-648},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Causality-based visual analysis of questionnaire responses},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive assessment of visualization literacy.
<em>TVCG</em>, <em>30</em>(1), 628–637. (<a
href="https://doi.org/10.1109/TVCG.2023.3327165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualization literacy is an essential skill for accurately interpreting data to inform critical decisions. Consequently, it is vital to understand the evolution of this ability and devise targeted interventions to enhance it, requiring concise and repeatable assessments of visualization literacy for individuals. However, current assessments, such as the Visualization Literacy Assessment Test ( vlat ), are time-consuming due to their fixed, lengthy format. To address this limitation, we develop two streamlined computerized adaptive tests ( cats ) for visualization literacy, a-vlat and a-calvi , which measure the same set of skills as their original versions in half the number of questions. Specifically, we (1) employ item response theory (IRT) and non-psychometric constraints to construct adaptive versions of the assessments, (2) finalize the configurations of adaptation through simulation, (3) refine the composition of test items of a-calvi via a qualitative study, and (4) demonstrate the test-retest reliability (ICC: 0.98 and 0.98) and convergent validity (correlation: 0.81 and 0.66) of both CATS via four online studies. We discuss practical recommendations for using our CATS and opportunities for further customization to leverage the full potential of adaptive assessments. All supplemental materials are available at https://osf.io/a6258/ .},
  archive      = {J_TVCG},
  author       = {Yuan Cui and Lily W. Ge and Yiren Ding and Fumeng Yang and Lane Harrison and Matthew Kay},
  doi          = {10.1109/TVCG.2023.3327165},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {628-637},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Adaptive assessment of visualization literacy},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VISGRADER: Automatic grading of d3 visualizations.
<em>TVCG</em>, <em>30</em>(1), 617–627. (<a
href="https://doi.org/10.1109/TVCG.2023.3327181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manually grading D3 data visualizations is a challenging endeavor, and is especially difficult for large classes with hundreds of students. Grading an interactive visualization requires a combination of interactive, quantitative, and qualitative evaluation that are conventionally done manually and are difficult to scale up as the visualization complexity, data size, and number of students increase. We present V IS G RADER , a first-of-its kind automatic grading method for D3 visualizations that scalably and precisely evaluates the data bindings, visual encodings, interactions, and design specifications used in a visualization. Our method enhances students&#39; learning experience, enabling them to submit their code frequently and receive rapid feedback to better inform iteration and improvement to their code and visualization design. We have successfully deployed our method and auto-graded D3 submissions from more than 4000 students in a visualization course at Georgia Tech, and received positive feedback for expanding its adoption.},
  archive      = {J_TVCG},
  author       = {Matthew Hull and Vivian Pednekar and Hannah Murray and Nimisha Roy and Emmanuel Tung and Susanta Routray and Connor Guerin and Justin Chen and Zijie J. Wang and Seongmin Lee and Mahdi Roozbahani and Duen Horng Chau},
  doi          = {10.1109/TVCG.2023.3327181},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {617-627},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VISGRADER: Automatic grading of d3 visualizations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SpeechMirror: A multimodal visual analytics system for
personalized reflection of online public speaking effectiveness.
<em>TVCG</em>, <em>30</em>(1), 606–616. (<a
href="https://doi.org/10.1109/TVCG.2023.3326932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As communications are increasingly taking place virtually, the ability to present well online is becoming an indispensable skill. Online speakers are facing unique challenges in engaging with remote audiences. However, there has been a lack of evidence-based analytical systems for people to comprehensively evaluate online speeches and further discover possibilities for improvement. This paper introduces SpeechMirror, a visual analytics system facilitating reflection on a speech based on insights from a collection of online speeches. The system estimates the impact of different speech techniques on effectiveness and applies them to a speech to give users awareness of the performance of speech techniques. A similarity recommendation approach based on speech factors or script content supports guided exploration to expand knowledge of presentation evidence and accelerate the discovery of speech delivery possibilities. SpeechMirror provides intuitive visualizations and interactions for users to understand speech factors. Among them, SpeechTwin, a novel multimodal visual summary of speech, supports rapid understanding of critical speech factors and comparison of different speech samples, and SpeechPlayer augments the speech video by integrating visualization of the speaker&#39;s body language with interaction, for focused analysis. The system utilizes visualizations suited to the distinct nature of different speech factors for user comprehension. The proposed system and visualization techniques were evaluated with domain experts and amateurs, demonstrating usability for users with low visualization literacy and its efficacy in assisting users to develop insights for potential improvement.},
  archive      = {J_TVCG},
  author       = {Zeyuan Huang and Qiang He and Kevin Maher and Xiaoming Deng and Yu-Kun Lai and Cuixia Ma and Sheng-Feng Qin and Yong-Jin Liu and Hongan Wang},
  doi          = {10.1109/TVCG.2023.3326932},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {606-616},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SpeechMirror: A multimodal visual analytics system for personalized reflection of online public speaking effectiveness},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable hypergraph visualization. <em>TVCG</em>,
<em>30</em>(1), 595–605. (<a
href="https://doi.org/10.1109/TVCG.2023.3326599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypergraph visualization has many applications in network data analysis. Recently, a polygon-based representation for hypergraphs has been proposed with demonstrated benefits. However, the polygon-based layout often suffers from excessive self-intersections when the input dataset is relatively large. In this paper, we propose a framework in which the hypergraph is iteratively simplified through a set of atomic operations. Then, the layout of the simplest hypergraph is optimized and used as the foundation for a reverse process that brings the simplest hypergraph back to the original one, but with an improved layout. At the core of our approach is the set of atomic simplification operations and an operation priority measure to guide the simplification process. In addition, we introduce necessary definitions and conditions for hypergraph planarity within the polygon representation. We extend our approach to handle simultaneous simplification and layout optimization for both the hypergraph and its dual. We demonstrate the utility of our approach with datasets from a number of real-world applications.},
  archive      = {J_TVCG},
  author       = {Peter Oliver and Eugene Zhang and Yue Zhang},
  doi          = {10.1109/TVCG.2023.3326599},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {595-605},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Scalable hypergraph visualization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge graphs in practice: Characterizing their users,
challenges, and visualization opportunities. <em>TVCG</em>,
<em>30</em>(1), 584–594. (<a
href="https://doi.org/10.1109/TVCG.2023.3326904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents insights from interviews with nineteen Knowledge Graph (KG) practitioners who work in both enterprise and academic settings on a wide variety of use cases. Through this study, we identify critical challenges experienced by KG practitioners when creating, exploring, and analyzing KGs that could be alleviated through visualization design. Our findings reveal three major personas among KG practitioners – KG Builders, Analysts, and Consumers – each of whom have their own distinct expertise and needs. We discover that KG Builders would benefit from schema enforcers, while KG Analysts need customizable query builders that provide interim query results. For KG Consumers, we identify a lack of efficacy for node-link diagrams, and the need for tailored domain-specific visualizations to promote KG adoption and comprehension. Lastly, we find that implementing KGs effectively in practice requires both technical and social solutions that are not addressed with current tools, technologies, and collaborative workflows. From the analysis of our interviews, we distill several visualization research directions to improve KG usability, including knowledge cards that balance digestibility and discoverability, timeline views to track temporal changes, interfaces that support organic discovery, and semantic explanations for AI and machine learning predictions.},
  archive      = {J_TVCG},
  author       = {Harry Li and Gabriel Appleby and Camelia Daniela Brumar and Remco Chang and Ashley Suh},
  doi          = {10.1109/TVCG.2023.3326904},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {584-594},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Knowledge graphs in practice: Characterizing their users, challenges, and visualization opportunities},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantivine: A visualization approach for large-scale quantum
circuit representation and analysis. <em>TVCG</em>, <em>30</em>(1),
573–583. (<a href="https://doi.org/10.1109/TVCG.2023.3327148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum computing is a rapidly evolving field that enables exponential speed-up over classical algorithms. At the heart of this revolutionary technology are quantum circuits, which serve as vital tools for implementing, analyzing, and optimizing quantum algorithms. Recent advancements in quantum computing and the increasing capability of quantum devices have led to the development of more complex quantum circuits. However, traditional quantum circuit diagrams suffer from scalability and readability issues, which limit the efficiency of analysis and optimization processes. In this research, we propose a novel visualization approach for large-scale quantum circuits by adopting semantic analysis to facilitate the comprehension of quantum circuits. We first exploit meta-data and semantic information extracted from the underlying code of quantum circuits to create component segmentations and pattern abstractions, allowing for easier wrangling of massive circuit diagrams. We then develop Quantivine , an interactive system for exploring and understanding quantum circuits. A series of novel circuit visualizations is designed to uncover contextual details such as qubit provenance, parallelism, and entanglement. The effectiveness of Quantivine is demonstrated through two usage scenarios of quantum circuits with up to 100 qubits and a formal user evaluation with quantum experts. A free copy of this paper and all supplemental materials are available at https://osf.io/2m9yh/?view_only=0aa1618c97244f5093cd7ce15f1431f9 .},
  archive      = {J_TVCG},
  author       = {Zhen Wen and Yihan Liu and Siwei Tan and Jieyi Chen and Minfeng Zhu and Dongming Han and Jianwei Yin and Mingliang Xu and Wei Chen},
  doi          = {10.1109/TVCG.2023.3327148},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {573-583},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Quantivine: A visualization approach for large-scale quantum circuit representation and analysis},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Calliope-net: Automatic generation of graph data facts via
annotated node-link diagrams. <em>TVCG</em>, <em>30</em>(1), 562–572.
(<a href="https://doi.org/10.1109/TVCG.2023.3326925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph or network data are widely studied in both data mining and visualization communities to review the relationship among different entities and groups. The data facts derived from graph visual analysis are important to help understand the social structures of complex data, especially for data journalism. However, it is challenging for data journalists to discover graph data facts and manually organize correlated facts around a meaningful topic due to the complexity of graph data and the difficulty to interpret graph narratives. Therefore, we present an automatic graph facts generation system, Calliope-Net, which consists of a fact discovery module, a fact organization module, and a visualization module. It creates annotated node-link diagrams with facts automatically discovered and organized from network data. A novel layout algorithm is designed to present meaningful and visually appealing annotated graphs. We evaluate the proposed system with two case studies and an in-lab user study. The results show that Calliope-Net can benefit users in discovering and understanding graph data facts with visually pleasing annotated visualizations.},
  archive      = {J_TVCG},
  author       = {Qing Chen and Nan Chen and Wei Shuai and Guande Wu and Zhe Xu and Hanghang Tong and Nan Cao},
  doi          = {10.1109/TVCG.2023.3326925},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {562-572},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Calliope-net: Automatic generation of graph data facts via annotated node-link diagrams},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OldVisOnline: Curating a dataset of historical
visualizations. <em>TVCG</em>, <em>30</em>(1), 551–561. (<a
href="https://doi.org/10.1109/TVCG.2023.3326908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing adoption of digitization, more and more historical visualizations created hundreds of years ago are accessible in digital libraries online. It provides a unique opportunity for visualization and history research. Meanwhile, there is no large-scale digital collection dedicated to historical visualizations. The visualizations are scattered in various collections, which hinders retrieval. In this study, we curate the first large-scale dataset dedicated to historical visualizations. Our dataset comprises 13K historical visualization images with corresponding processed metadata from seven digital libraries. In curating the dataset, we propose a workflow to scrape and process heterogeneous metadata. We develop a semi-automatic labeling approach to distinguish visualizations from other artifacts. Our dataset can be accessed with OldVisOnline, a system we have built to browse and label historical visualizations. We discuss our vision of usage scenarios and research opportunities with our dataset, such as textual criticism for historical visualizations. Drawing upon our experience, we summarize recommendations for future efforts to improve our dataset.},
  archive      = {J_TVCG},
  author       = {Yu Zhang and Ruike Jiang and Liwenhan Xie and Yuheng Zhao and Can Liu and Tianhong Ding and Siming Chen and Xiaoru Yuan},
  doi          = {10.1109/TVCG.2023.3326908},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {551-561},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {OldVisOnline: Curating a dataset of historical visualizations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visualizing historical book trade data: An iterative design
study with close collaboration with domain experts. <em>TVCG</em>,
<em>30</em>(1), 540–550. (<a
href="https://doi.org/10.1109/TVCG.2023.3326923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The circulation of historical books has always been an area of interest for historians. However, the data used to represent the journey of a book across different places and times can be difficult for domain experts to digest due to buried geographical and chronological features within text-based presentations. This situation provides an opportunity for collaboration between visualization researchers and historians. This paper describes a design study where a variant of the Nine-Stage Framework [46] was employed to develop a Visual Analytics (VA) tool called DanteExploreVis . This tool was designed to aid domain experts in exploring, explaining, and presenting book trade data from multiple perspectives. We discuss the design choices made and how each panel in the interface meets the domain requirements. We also present the results of a qualitative evaluation conducted with domain experts. The main contributions of this paper include: 1) the development of a VA tool to support domain experts in exploring, explaining, and presenting book trade data; 2) a comprehensive documentation of the iterative design, development, and evaluation process following the variant Nine-Stage Framework; 3) a summary of the insights gained and lessons learned from this design study in the context of the humanities field; and 4) reflections on how our approach could be applied in a more generalizable way.},
  archive      = {J_TVCG},
  author       = {Yiwen Xing and Cristina Dondi and Rita Borgo and Alfie Abdul-Rahman},
  doi          = {10.1109/TVCG.2023.3326923},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {540-550},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualizing historical book trade data: An iterative design study with close collaboration with domain experts},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LiberRoad: Probing into the journey of chinese classics
through visual analytics. <em>TVCG</em>, <em>30</em>(1), 529–539. (<a
href="https://doi.org/10.1109/TVCG.2023.3326944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Books act as a crucial carrier of cultural dissemination in ancient times. This work involves joint efforts between visualization and humanities researchers, aiming at building a holistic view of the cultural exchange and integration between China and Japan brought about by the overseas circulation of Chinese classics. Book circulation data consist of uncertain spatiotemporal trajectories, with multiple dimensions, and movement across hierarchical spaces forms a compound network. LiberRoad visualizes the circulation of books collected in the Imperial Household Agency of Japan, and can be generalized to other book movement data. The LiberRoad system enables a smooth transition between three views (Location Graph, map, and timeline) according to the desired perspectives (spatial or temporal), as well as flexible filtering and selection. The Location Graph is a novel uncertainty-aware visualization method that employs improved circle packing to represent spatial hierarchy. The map view intuitively shows the overall circulation by clustering and allows zooming into single book trajectory with lenses magnifying local movements. The timeline view ranks dynamically in response to user interaction to facilitate the discovery of temporal events. The evaluation and feedback from the expert users demonstrate that LiberRoad is helpful in revealing movement patterns and comparing circulation characteristics of different times and spaces.},
  archive      = {J_TVCG},
  author       = {Yuhan Guo and Yuchu Luo and Keer Lu and Linfang Li and Haizheng Yang and Xiaoru Yuan},
  doi          = {10.1109/TVCG.2023.3326944},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {529-539},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LiberRoad: Probing into the journey of chinese classics through visual analytics},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). InnovationInsights: A visual analytics approach for
understanding the dual frontiers of science and technology.
<em>TVCG</em>, <em>30</em>(1), 518–528. (<a
href="https://doi.org/10.1109/TVCG.2023.3327387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Science has long been viewed as a key driver of economic growth and rising standards of living. Knowledge about how scientific advances support marketplace inventions is therefore essential for understanding the role of science in propelling real-world applications and technological progress. The increasing availability of large-scale datasets tracing scientific publications and patented inventions and the complex interactions among them offers us new opportunities to explore the evolving dual frontiers of science and technology at an unprecedented level of scale and detail. However, we lack suitable visual analytics approaches to analyze such complex interactions effectively. Here we introduce InnovationInsights , an interactive visual analysis system for researchers, research institutions, and policymakers to explore the complex linkages between science and technology, and to identify critical innovations, inventors, and potential partners. The system first identifies important associations between scientific papers and patented inventions through a set of statistical measures introduced by our experts from the field of the Science of Science. A series of visualization views are then used to present these associations in the data context. In particular, we introduce the Interplay Graph to visualize patterns and insights derived from the data, helping users effectively navigate citation relationships between papers and patents. This visualization thereby helps them identify the origins of technical inventions and the impact of scientific research. We evaluate the system through two case studies with experts followed by expert interviews. We further engage a premier research institution to test-run the system, helping its institution leaders to extract new insights for innovation. Through both the case studies and the engagement project, we find that our system not only meets our original goals of design, allowing users to better identify the sources of technical inventions and to understand the broad impact of scientific research; it also goes beyond these purposes to enable an array of new applications for researchers and research institutions, ranging from identifying untapped innovation potential within an institution to forging new collaboration opportunities between science and industry.},
  archive      = {J_TVCG},
  author       = {Yifang Wang and Yifan Qian and Xiaoyu Qi and Nan Cao and Dashun Wang},
  doi          = {10.1109/TVCG.2023.3327387},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {518-528},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {InnovationInsights: A visual analytics approach for understanding the dual frontiers of science and technology},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wizualization: A “hard magic” visualization system for
immersive and ubiquitous analytics. <em>TVCG</em>, <em>30</em>(1),
507–517. (<a href="https://doi.org/10.1109/TVCG.2023.3326580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {What if magic could be used as an effective metaphor to perform data visualization and analysis using speech and gestures while mobile and on-the-go? In this paper, we introduce W izualization , a visual analytics system for eXtended Reality (XR) that enables an analyst to author and interact with visualizations using such a magic system through gestures, speech commands, and touch interaction. Wizualization is a rendering system for current XR headsets that comprises several components: a cross-device (or A rcane F ocuses ) infrastructure for signalling and view control (W eave ), a code notebook (S pellbook ), and a grammar of graphics for XR (O ptomancy ). The system offers users three modes of input: gestures, spoken commands, and materials. We demonstrate Wizualization and its components using a motivating scenario on collaborative data analysis of pandemic data across time and space.},
  archive      = {J_TVCG},
  author       = {Andrea Batch and Peter W. S. Butcher and Panagiotis D. Ritsos and Niklas Elmqvist},
  doi          = {10.1109/TVCG.2023.3326580},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {507-517},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Wizualization: A “Hard magic” visualization system for immersive and ubiquitous analytics},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unraveling the design space of immersive analytics: A
systematic review. <em>TVCG</em>, <em>30</em>(1), 495–506. (<a
href="https://doi.org/10.1109/TVCG.2023.3327368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immersive analytics has emerged as a promising research area, leveraging advances in immersive display technologies and techniques, such as virtual and augmented reality, to facilitate data exploration and decision-making. This paper presents a systematic literature review of 73 studies published between 2013-2022 on immersive analytics systems and visualizations, aiming to identify and categorize the primary dimensions influencing their design. We identified five key dimensions: Academic Theory and Contribution, Immersive Technology, Data, Spatial Presentation, and Visual Presentation. Academic Theory and Contribution assess the motivations behind the works and their theoretical frameworks. Immersive Technology examines the display and input modalities, while Data dimension focuses on dataset types and generation. Spatial Presentation discusses the environment, space, embodiment, and collaboration aspects in IA, and Visual Presentation explores the visual elements, facet and position, and manipulation of views. By examining each dimension individually and cross-referencing them, this review uncovers trends and relationships that help inform the design of immersive systems visualizations. This analysis provides valuable insights for researchers and practitioners, offering guidance in designing future immersive analytics systems and shaping the trajectory of this rapidly evolving field. A free copy of this paper and all supplemental materials are available at osf.io/5ewaj.},
  archive      = {J_TVCG},
  author       = {David Saffo and Sara Di Bartolomeo and Tarik Crnovrsanin and Laura South and Justin Raynor and Caglar Yildirim and Cody Dunne},
  doi          = {10.1109/TVCG.2023.3327368},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {495-506},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Unraveling the design space of immersive analytics: A systematic review},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MeTACAST: Target- and context-aware spatial selection in VR.
<em>TVCG</em>, <em>30</em>(1), 480–494. (<a
href="https://doi.org/10.1109/TVCG.2023.3326517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose three novel spatial data selection techniques for particle data in VR visualization environments. They are designed to be target- and context-aware and be suitable for a wide range of data features and complex scenarios. Each technique is designed to be adjusted to particular selection intents: the selection of consecutive dense regions, the selection of filament-like structures, and the selection of clusters—with all of them facilitating post-selection threshold adjustment. These techniques allow users to precisely select those regions of space for further exploration—with simple and approximate 3D pointing, brushing, or drawing input—using flexible point- or path-based input and without being limited by 3D occlusions, non-homogeneous feature density, or complex data shapes. These new techniques are evaluated in a controlled experiment and compared with the Baseline method, a region-based 3D painting selection. Our results indicate that our techniques are effective in handling a wide range of scenarios and allow users to select data based on their comprehension of crucial features. Furthermore, we analyze the attributes, requirements, and strategies of our spatial selection methods and compare them with existing state-of-the-art selection methods to handle diverse data features and situations. Based on this analysis we provide guidelines for choosing the most suitable 3D spatial selection techniques based on the interaction environment, the given data characteristics, or the need for interactive post-selection threshold adjustment.},
  archive      = {J_TVCG},
  author       = {Lixiang Zhao and Tobias Isenberg and Fuqi Xie and Hai-Ning Liang and Lingyun Yu},
  doi          = {10.1109/TVCG.2023.3326517},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {480-494},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MeTACAST: Target- and context-aware spatial selection in VR},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 2D, 2.5D, or 3D? An exploratory study on multilayer network
visualisations in virtual reality. <em>TVCG</em>, <em>30</em>(1),
469–479. (<a href="https://doi.org/10.1109/TVCG.2023.3327402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relational information between different types of entities is often modelled by a multilayer network (MLN) – a network with subnetworks represented by layers. The layers of an MLN can be arranged in different ways in a visual representation, however, the impact of the arrangement on the readability of the network is an open question. Therefore, we studied this impact for several commonly occurring tasks related to MLN analysis. Additionally, layer arrangements with a dimensionality beyond 2D, which are common in this scenario, motivate the use of stereoscopic displays. We ran a human subject study utilising a Virtual Reality headset to evaluate 2D, 2.5D, and 3D layer arrangements. The study employs six analysis tasks that cover the spectrum of an MLN task taxonomy, from path finding and pattern identification to comparisons between and across layers. We found no clear overall winner. However, we explore the task-to-arrangement space and derive empirical-based recommendations on the effective use of 2D, 2.5D, and 3D layer arrangements for MLNs.},
  archive      = {J_TVCG},
  author       = {Stefan P. Feyer and Bruno Pinaud and Stephen Kobourov and Nicolas Brich and Michael Krone and Andreas Kerren and Michael Behrisch and Falk Schreiber and Karsten Klein},
  doi          = {10.1109/TVCG.2023.3327402},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {469-479},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {2D, 2.5D, or 3D? an exploratory study on multilayer network visualisations in virtual reality},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VIRD: Immersive match video analysis for high-performance
badminton coaching. <em>TVCG</em>, <em>30</em>(1), 458–468. (<a
href="https://doi.org/10.1109/TVCG.2023.3327161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Badminton is a fast-paced sport that requires a strategic combination of spatial, temporal, and technical tactics. To gain a competitive edge at high-level competitions, badminton professionals frequently analyze match videos to gain insights and develop game strategies. However, the current process for analyzing matches is time-consuming and relies heavily on manual note-taking, due to the lack of automatic data collection and appropriate visualization tools. As a result, there is a gap in effectively analyzing matches and communicating insights among badminton coaches and players. This work proposes an end-to-end immersive match analysis pipeline designed in close collaboration with badminton professionals, including Olympic and national coaches and players. We present VIRD , a VR Bird (i.e., shuttle) immersive analysis tool, that supports interactive badminton game analysis in an immersive environment based on 3D reconstructed game views of the match video. We propose a top-down analytic workflow that allows users to seamlessly move from a high-level match overview to a detailed game view of individual rallies and shots, using situated 3D visualizations and video. We collect 3D spatial and dynamic shot data and player poses with computer vision models and visualize them in VR. Through immersive visualizations, coaches can interactively analyze situated spatial data (player positions, poses, and shot trajectories) with flexible viewpoints while navigating between shots and rallies effectively with embodied interaction. We evaluated the usefulness of VIRD with Olympic and national-level coaches and players in real matches. Results show that immersive analytics supports effective badminton match analysis with reduced context-switching costs and enhances spatial understanding with a high sense of presence.},
  archive      = {J_TVCG},
  author       = {Tica Lin and Alexandre Aouididi and Chen Zhu-Tian and Johanna Beyer and Hanspeter Pfister and Jui-Hsien Wang},
  doi          = {10.1109/TVCG.2023.3327161},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {458-468},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VIRD: Immersive match video analysis for high-performance badminton coaching},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mystique: Deconstructing SVG charts for layout reuse.
<em>TVCG</em>, <em>30</em>(1), 447–457. (<a
href="https://doi.org/10.1109/TVCG.2023.3327354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To facilitate the reuse of existing charts, previous research has examined how to obtain a semantic understanding of a chart by deconstructing its visual representation into reusable components, such as encodings. However, existing deconstruction approaches primarily focus on chart styles, handling only basic layouts. In this paper, we investigate how to deconstruct chart layouts, focusing on rectangle-based ones, as they cover not only 17 chart types but also advanced layouts (e.g., small multiples, nested layouts). We develop an interactive tool, called Mystique, adopting a mixed-initiative approach to extract the axes and legend, and deconstruct a chart&#39;s layout into four semantic components: mark groups, spatial relationships, data encodings, and graphical constraints. Mystique employs a wizard interface that guides chart authors through a series of steps to specify how the deconstructed components map to their own data. On 150 rectangle-based SVG charts, Mystique achieves above 85% accuracy for axis and legend extraction and 96% accuracy for layout deconstruction. In a chart reproduction study, participants could easily reuse existing charts on new datasets. We discuss the current limitations of Mystique and future research directions.},
  archive      = {J_TVCG},
  author       = {Chen Chen and Bongshin Lee and Yunhai Wang and Yunjeong Chang and Zhicheng Liu},
  doi          = {10.1109/TVCG.2023.3327354},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {447-457},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Mystique: Deconstructing SVG charts for layout reuse},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mosaic: An architecture for scalable &amp; interoperable
data views. <em>TVCG</em>, <em>30</em>(1), 436–446. (<a
href="https://doi.org/10.1109/TVCG.2023.3327189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mosaic is an architecture for greater scalability, extensibility, and interoperability of interactive data views. Mosaic decouples data processing from specification logic: clients publish their data needs as declarative queries that are then managed and automatically optimized by a coordinator that proxies access to a scalable data store. Mosaic generalizes Vegalite&#39;s selection abstraction to enable rich integration and linking across visualizations and components such as menus, text search, and tables. We demonstrate Mosaic&#39;s expressiveness, extensibility, and interoperability through examples that compose diverse visualization, interaction, and optimization techniques—many constructed using vgplot , a grammar of interactive graphics in which graphical marks act as Mosaic clients. To evaluate scalability, we present benchmark studies with order-of-magnitude performance improvements over existing web-based visualization systems—enabling flexible, real-time visual exploration of billion+ record datasets. We conclude by discussing Mosaic&#39;s potential as an open platform that bridges visualization languages, scalable visualization, and interactive data systems more broadly.},
  archive      = {J_TVCG},
  author       = {Jeffrey Heer and Dominik Moritz},
  doi          = {10.1109/TVCG.2023.3327189},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {436-446},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Mosaic: An architecture for scalable &amp; interoperable data views},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Metrics-based evaluation and comparison of visualization
notations. <em>TVCG</em>, <em>30</em>(1), 425–435. (<a
href="https://doi.org/10.1109/TVCG.2023.3326907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A visualization notation is a recurring pattern of symbols used to author specifications of visualizations, from data transformation to visual mapping. Programmatic notations use symbols defined by grammars or domain-specific languages (e.g. ggplot2, dplyr, Vega-Lite) or libraries (e.g. Matplotlib, Pandas). Designers and prospective users of grammars and libraries often evaluate visualization notations by inspecting galleries of examples. While such collections demonstrate usage and expressiveness, their construction and evaluation are usually ad hoc, making comparisons of different notations difficult. More rarely, experts analyze notations via usability heuristics, such as the Cognitive Dimensions of Notations framework. These analyses, akin to structured close readings of text, can reveal design deficiencies, but place a burden on the expert to simultaneously consider many facets of often complex systems. To alleviate these issues, we introduce a metrics-based approach to usability evaluation and comparison of notations in which metrics are computed for a gallery of examples across a suite of notations. While applicable to any visualization domain, we explore the utility of our approach via a case study considering statistical graphics that explores 40 visualizations across 9 widely used notations. We facilitate the computation of appropriate metrics and analysis via a new tool called NotaScope. We gathered feedback via interviews with authors or maintainers of prominent charting libraries ( $n=6$ ). We find that this approach is a promising way to formalize, externalize, and extend evaluations and comparisons of visualization notations.},
  archive      = {J_TVCG},
  author       = {Nicolas Kruchten and Andrew M. McNutt and Michael J. McGuffin},
  doi          = {10.1109/TVCG.2023.3326907},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {425-435},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Metrics-based evaluation and comparison of visualization notations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ggdist: Visualizations of distributions and uncertainty in
the grammar of graphics. <em>TVCG</em>, <em>30</em>(1), 414–424. (<a
href="https://doi.org/10.1109/TVCG.2023.3327195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The grammar of graphics is ubiquitous, providing the foundation for a variety of popular visualization tools and toolkits. Yet support for uncertainty visualization in the grammar graphics—beyond simple variations of error bars, uncertainty bands, and density plots—remains rudimentary. Research in uncertainty visualization has developed a rich variety of improved uncertainty visualizations, most of which are difficult to create in existing grammar of graphics implementations. ggdist , an extension to the popular ggplot2 grammar of graphics toolkit, is an attempt to rectify this situation. ggdist unifies a variety of uncertainty visualization types through the lens of distributional visualization, allowing functions of distributions to be mapped to directly to visual channels (aesthetics), making it straightforward to express a variety of (sometimes weird!) uncertainty visualization types. This distributional lens also offers a way to unify Bayesian and frequentist uncertainty visualization by formalizing the latter with the help of confidence distributions. In this paper, I offer a description of this uncertainty visualization paradigm and lessons learned from its development and adoption: ggdist has existed in some form for about six years (originally as part of the tidybayes R package for post-processing Bayesian models), and it has evolved substantially over that time, with several rewrites and API re-organizations as it changed in response to user feedback and expanded to cover increasing varieties of uncertainty visualization types. Ultimately, given the huge expressive power of the grammar of graphics and the popularity of tools built on it, I hope a catalog of my experience with ggdist will provide a catalyst for further improvements to formalizations and implementations of uncertainty visualization in grammar of graphics ecosystems. A free copy of this paper is available at https://osf.io/2gsz6 . All supplemental materials are available at https://github.com/mjskay/ggdist-paper and are archived on Zenodo at doi:10.5281/zenodo.7770984 .},
  archive      = {J_TVCG},
  author       = {Matthew Kay},
  doi          = {10.1109/TVCG.2023.3327195},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {414-424},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Ggdist: Visualizations of distributions and uncertainty in the grammar of graphics},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DIVI: Dynamically interactive visualization. <em>TVCG</em>,
<em>30</em>(1), 403–413. (<a
href="https://doi.org/10.1109/TVCG.2023.3327172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamically Interactive Visualization (DIVI) is a novel approach for orchestrating interactions within and across static visualizations. DIVI deconstructs Scalable Vector Graphics charts at runtime to infer content and coordinate user input, decoupling interaction from specification logic. This decoupling allows interactions to extend and compose freely across different tools, chart types, and analysis goals. DIVI exploits positional relations of marks to detect chart components such as axes and legends, reconstruct scales and view encodings, and infer data fields. DIVI then enumerates candidate transformations across inferred data to perform linking between views. To support dynamic interaction without prior specification, we introduce a taxonomy that formalizes the space of standard interactions by chart element, interaction type, and input event. We demonstrate DIVI&#39;s usefulness for rapid data exploration and analysis through a usability study with 13 participants and a diverse gallery of dynamically interactive visualizations, including single chart, multi-view, and cross-tool configurations.},
  archive      = {J_TVCG},
  author       = {Luke S. Snyder and Jeffrey Heer},
  doi          = {10.1109/TVCG.2023.3327172},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {403-413},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DIVI: Dynamically interactive visualization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual analytics for understanding draco’s knowledge base.
<em>TVCG</em>, <em>30</em>(1), 392–402. (<a
href="https://doi.org/10.1109/TVCG.2023.3326912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Draco has been developed as an automated visualization recommendation system formalizing design knowledge as logical constraints in ASP (Answer-Set Programming). With an increasing set of constraints and incorporated design knowledge, even visualization experts lose overview in Draco and struggle to retrace the automated recommendation decisions made by the system. Our paper proposes an Visual Analytics (VA) approach to visualize and analyze Draco&#39;s constraints. Our VA approach is supposed to enable visualization experts to accomplish identified tasks regarding the knowledge base and support them in better understanding Draco. We extend the existing data extraction strategy of Draco with a data processing architecture capable of extracting features of interest from the knowledge base. A revised version of the ASP grammar provides the basis for this data processing strategy. The resulting incorporated and shared features of the constraints are then visualized using a hypergraph structure inside the radial-arranged constraints of the elaborated visualization. The hierarchical categories of the constraints are indicated by arcs surrounding the constraints. Our approach is supposed to enable visualization experts to interactively explore the design rules&#39; violations based on highlighting respective constraints or recommendations. A qualitative and quantitative evaluation of the prototype confirms the prototype&#39;s effectiveness and value in acquiring insights into Draco&#39;s recommendation process and design constraints.},
  archive      = {J_TVCG},
  author       = {Johanna Schmidt and Bernhard Pointner and Silvia Miksch},
  doi          = {10.1109/TVCG.2023.3326912},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {392-402},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual analytics for understanding draco&#39;s knowledge base},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transitioning to a commercial dashboarding system:
Socio-technical observations and opportunities. <em>TVCG</em>,
<em>30</em>(1), 381–391. (<a
href="https://doi.org/10.1109/TVCG.2023.3326525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many long-established, traditional manufacturing businesses are becoming more digital and data-driven to improve their production. These companies are embracing visual analytics in these transitions through their adoption of commercial dashboarding systems. Although a number of studies have looked at the technical challenges of adopting these systems, very few have focused on the socio-technical issues that arise. In this paper, we report on the results of an interview study with 17 participants working in a range of roles at a long-established, traditional manufacturing company as they adopted Microsoft Power BI. The results highlight a number of socio-technical challenges the employees faced, including difficulties in training, using and creating dashboards, and transitioning to a modern digital company. Based on these results, we propose a number of opportunities for both companies and visualization researchers to improve these difficult transitions, as well as opportunities for rethinking how we design dashboarding systems for real-world use.},
  archive      = {J_TVCG},
  author       = {Conny Walchshofer and Vaishal Dhanoa and Marc Streit and Miriah Meyer},
  doi          = {10.1109/TVCG.2023.3326525},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {381-391},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Transitioning to a commercial dashboarding system: Socio-technical observations and opportunities},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heuristics for supporting cooperative dashboard design.
<em>TVCG</em>, <em>30</em>(1), 370–380. (<a
href="https://doi.org/10.1109/TVCG.2023.3327158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dashboards are no longer mere static displays of metrics; through functionality such as interaction and storytelling, they have evolved to support analytic and communicative goals like monitoring and reporting. Existing dashboard design guidelines, however, are often unable to account for this expanded scope as they largely focus on best practices for visual design. In contrast, we frame dashboard design as facilitating an analytical conversation : a cooperative, interactive experience where a user may interact with, reason about, or freely query the underlying data. By drawing on established principles of conversational flow and communication, we define the concept of a cooperative dashboard as one that enables a fruitful and productive analytical conversation, and derive a set of 39 dashboard design heuristics to support effective analytical conversations. To assess the utility of this framing, we asked 52 computer science and engineering graduate students to apply our heuristics to critique and design dashboards as part of an ungraded, opt-in homework assignment. Feedback from participants demonstrates that our heuristics surface new reasons dashboards may fail, and encourage a more fluid, supportive, and responsive style of dashboard design. Our approach suggests several compelling directions for future work, including dashboard authoring tools that better anticipate conversational turn-taking, repair, and refinement and extending cooperative principles to other analytical workflows.},
  archive      = {J_TVCG},
  author       = {Vidya Setlur and Michael Correll and Arvind Satyanarayan and Melanie Tory},
  doi          = {10.1109/TVCG.2023.3327158},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {370-380},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Heuristics for supporting cooperative dashboard design},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From information to choice: A critical inquiry into
visualization tools for decision making. <em>TVCG</em>, <em>30</em>(1),
359–369. (<a href="https://doi.org/10.1109/TVCG.2023.3326593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the face of complex decisions, people often engage in a three-stage process that spans from (1) exploring and analyzing pertinent information (intelligence); (2) generating and exploring alternative options (design); and ultimately culminating in (3) selecting the optimal decision by evaluating discerning criteria (choice). We can fairly assume that all good visualizations aid in the “intelligence” stage by enabling data exploration and analysis. Yet, to what degree and how do visualization systems currently support the other decision making stages, namely “design” and “choice”? To further explore this question, we conducted a comprehensive review of decision-focused visualization tools by examining publications in major visualization journals and conferences, including VIS, EuroVis, and CHI, spanning all available years. We employed a deductive coding method and in-depth analysis to assess whether and how visualization tools support design and choice. Specifically, we examined each visualization tool by (i) its degree of visibility for displaying decision alternatives, criteria, and preferences, and (ii) its degree of flexibility for offering means to manipulate the decision alternatives, criteria, and preferences with interactions such as adding, modifying, changing mapping, and filtering. Our review highlights the opportunities and challenges that decision-focused visualization tools face in realizing their full potential to support all stages of the decision making process. It reveals a surprising scarcity of tools that support all stages, and while most tools excel in offering visibility for decision criteria and alternatives, the degree of flexibility to manipulate these elements is often limited, and the lack of tools that accommodate decision preferences and their elicitation is notable. Based on our findings, to better support the choice stage, future research could explore enhancing flexibility levels and variety, exploring novel visualization paradigms, increasing algorithmic support, and ensuring that this automation is user-controlled via the enhanced flexibility I evels. Our curated list of the 88 surveyed visualization tools is available in the OSF link ( https://osf.io/nrasz/?view_only=b92a90a34ae241449b5f2cd33383bfcb ).},
  archive      = {J_TVCG},
  author       = {Emre Oral and Ria Chawla and Michel Wijkstra and Narges Mahyar and Evanthia Dimara},
  doi          = {10.1109/TVCG.2023.3326593},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {359-369},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {From information to choice: A critical inquiry into visualization tools for decision making},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vistrust: A multidimensional framework and empirical study
of trust in data visualizations. <em>TVCG</em>, <em>30</em>(1), 348–358.
(<a href="https://doi.org/10.1109/TVCG.2023.3326579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trust is an essential aspect of data visualization, as it plays a crucial role in the interpretation and decision-making processes of users. While research in social sciences outlines the multi-dimensional factors that can play a role in trust formation, most data visualization trust researchers employ a single-item scale to measure trust. We address this gap by proposing a comprehensive, multidimensional conceptualization and operationalization of trust in visualization. We do this by applying general theories of trust from social sciences, as well as synthesizing and extending earlier work and factors identified by studies in the visualization field. We apply a two-dimensional approach to trust in visualization, to distinguish between cognitive and affective elements, as well as between visualization and data-specific trust antecedents. We use our framework to design and run a large crowd-sourced study to quantify the role of visual complexity in establishing trust in science visualizations. Our study provides empirical evidence for several aspects of our proposed theoretical framework, most notably the impact of cognition, affective responses, and individual differences when establishing trust in visualizations.},
  archive      = {J_TVCG},
  author       = {Hamza Elhamdadi and Adam Stefkovics and Johanna Beyer and Eric Moerth and Hanspeter Pfister and Cindy Xiong Bearfield and Carolina Nobre},
  doi          = {10.1109/TVCG.2023.3326579},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {348-358},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Vistrust: A multidimensional framework and empirical study of trust in data visualizations},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The rational agent benchmark for data visualization.
<em>TVCG</em>, <em>30</em>(1), 338–347. (<a
href="https://doi.org/10.1109/TVCG.2023.3326513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding how helpful a visualization is from experimental results is difficult because the observed performance is confounded with aspects of the study design, such as how useful the information that is visualized is for the task. We develop a rational agent framework for designing and interpreting visualization experiments. Our framework conceives two experiments with the same setup: one with behavioral agents (human subjects), and the other one with a hypothetical rational agent. A visualization is evaluated by comparing the expected performance of behavioral agents to that of a rational agent under different assumptions. Using recent visualization decision studies from the literature, we demonstrate how the framework can be used to pre-experimentally evaluate the experiment design by bounding the expected improvement in performance from having access to visualizations, and post-experimentally to deconfound errors of information extraction from errors of optimization, among other analyses.},
  archive      = {J_TVCG},
  author       = {Yifan Wu and Ziyang Guo and Michalis Mamakos and Jason Hartline and Jessica Hullman},
  doi          = {10.1109/TVCG.2023.3326513},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {338-347},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The rational agent benchmark for data visualization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). My model is unfair, do people even care? Visual design
affects trust and perceived bias in machine learning. <em>TVCG</em>,
<em>30</em>(1), 327–337. (<a
href="https://doi.org/10.1109/TVCG.2023.3327192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning technology has become ubiquitous, but, unfortunately, often exhibits bias. As a consequence, disparate stakeholders need to interact with and make informed decisions about using machine learning models in everyday systems. Visualization technology can support stakeholders in understanding and evaluating trade-offs between, for example, accuracy and fairness of models. This paper aims to empirically answer “Can visualization design choices affect a stakeholder&#39;s perception of model bias, trust in a model, and willingness to adopt a model?” Through a series of controlled, crowd-sourced experiments with more than 1,500 participants, we identify a set of strategies people follow in deciding which models to trust. Our results show that men and women prioritize fairness and performance differently and that visual design choices significantly affect that prioritization. For example, women trust fairer models more often than men do, participants value fairness more when it is explained using text than as a bar chart, and being explicitly told a model is biased has a bigger impact than showing past biased performance. We test the generalizability of our results by comparing the effect of multiple textual and visual design choices and offer potential explanations of the cognitive mechanisms behind the difference in fairness perception and trust. Our research guides design considerations to support future work developing visualization systems for machine learning.},
  archive      = {J_TVCG},
  author       = {Aimen Gaba and Zhanna Kaufman and Jason Cheung and Marie Shvakel and Kyle Wm. Hall and Yuriy Brun and Cindy Xiong Bearfield},
  doi          = {10.1109/TVCG.2023.3327192},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {327-337},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {My model is unfair, do people even care? visual design affects trust and perceived bias in machine learning},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Eleven years of gender data visualization: A step towards
more inclusive gender representation. <em>TVCG</em>, <em>30</em>(1),
316–326. (<a href="https://doi.org/10.1109/TVCG.2023.3327369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an analysis of the representation of gender as a data dimension in data visualizations and propose a set of considerations around visual variables and annotations for gender-related data. Gender is a common demographic dimension of data collected from study or survey participants, passengers, or customers, as well as across academic studies, especially in certain disciplines like sociology. Our work contributes to multiple ongoing discussions on the ethical implications of data visualizations. By choosing specific data, visual variables, and text labels, visualization designers may, inadvertently or not, perpetuate stereotypes and biases. Here, our goal is to start an evolving discussion on how to represent data on gender in data visualizations and raise awareness of the subtleties of choosing visual variables and words in gender visualizations. In order to ground this discussion, we collected and coded gender visualizations and their captions from five different scientific communities (Biology, Politics, Social Studies, Visualisation, and Human-Computer Interaction), in addition to images from Tableau Public and the Information Is Beautiful awards showcase. Overall we found that representation types are community-specific, color hue is the dominant visual channel for gender data, and nonconforming gender is under-represented. We end our paper with a discussion of considerations for gender visualization derived from our coding and the literature and recommendations for large data collection bodies. A free copy of this paper and all supplemental materials are available at https://osf.io/v9ams/ .},
  archive      = {J_TVCG},
  author       = {Florent Cabric and Margrét Vilborg Bjarnadóttir and Meng Ling and Guðbjörg Linda Rafnsdóttir and Petra Isenberg},
  doi          = {10.1109/TVCG.2023.3327369},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {316-326},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Eleven years of gender data visualization: A step towards more inclusive gender representation},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Average estimates in line graphs are biased toward areas of
higher variability. <em>TVCG</em>, <em>30</em>(1), 306–315. (<a
href="https://doi.org/10.1109/TVCG.2023.3326589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate variability overweighting , a previously undocumented bias in line graphs, where estimates of average value are biased toward areas of higher variability in that line. We found this effect across two preregistered experiments with 140 and 420 participants. These experiments also show that the bias is reduced when using a dot encoding of the same series. We can model the bias with the average of the data series and the average of the points drawn along the line. This bias might arise because higher variability leads to stronger weighting in the average calculation, either due to the longer line segments (even though those segments contain the same number of data values) or line segments with higher variability being otherwise more visually salient. Understanding and predicting this bias is important for visualization design guidelines, recommendation systems, and tool builders, as the bias can adversely affect estimates of averages and trends.},
  archive      = {J_TVCG},
  author       = {Dominik Moritz and Lace M. Padilla and Francis Nguyen and Steven L. Franconeri},
  doi          = {10.1109/TVCG.2023.3326589},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {306-315},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Average estimates in line graphs are biased toward areas of higher variability},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PromptMagician: Interactive prompt engineering for
text-to-image creation. <em>TVCG</em>, <em>30</em>(1), 295–305. (<a
href="https://doi.org/10.1109/TVCG.2023.3327168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative text-to-image models have gained great popularity among the public for their powerful capability to generate high-quality images based on natural language prompts. However, developing effective prompts for desired images can be challenging due to the complexity and ambiguity of natural language. This research proposes PromptMagician , a visual analysis system that helps users explore the image results and refine the input prompts. The backbone of our system is a prompt recommendation model that takes user prompts as input, retrieves similar prompt-image pairs from DiffusionDB, and identifies special (important and relevant) prompt keywords. To facilitate interactive prompt refinement, PromptMagician introduces a multi-level visualization for the cross-modal embedding of the retrieved images and recommended keywords, and supports users in specifying multiple criteria for personalized exploration. Two usage scenarios, a user study, and expert interviews demonstrate the effectiveness and usability of our system, suggesting it facilitates prompt engineering and improves the creativity support of the generative text-to-image model.},
  archive      = {J_TVCG},
  author       = {Yingchaojie Feng and Xingbo Wang and Kam Kwai Wong and Sijia Wang and Yuhong Lu and Minfeng Zhu and Baicheng Wang and Wei Chen},
  doi          = {10.1109/TVCG.2023.3327168},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {295-305},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PromptMagician: Interactive prompt engineering for text-to-image creation},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Let the chart spark: Embedding semantic context into chart
with text-to-image generative model. <em>TVCG</em>, <em>30</em>(1),
284–294. (<a href="https://doi.org/10.1109/TVCG.2023.3326913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pictorial visualization seamlessly integrates data and semantic context into visual representation, conveying complex information in an engaging and informative manner. Extensive studies have been devoted to developing authoring tools to simplify the creation of pictorial visualizations. However, mainstream works follow a retrieving-and-editing pipeline that heavily relies on retrieved visual elements from a dedicated corpus, which often compromise data integrity. Text-guided generation methods are emerging, but may have limited applicability due to their predefined entities. In this work, we propose ChartSpark , a novel system that embeds semantic context into chart based on text-to-image generative models. ChartSpark generates pictorial visualizations conditioned on both semantic context conveyed in textual inputs and data information embedded in plain charts. The method is generic for both foreground and background pictorial generation, satisfying the design practices identified from empirical research into existing pictorial visualizations. We further develop an interactive visual interface that integrates a text analyzer, editing module, and evaluation module to enable users to generate, modify, and assess pictorial visualizations. We experimentally demonstrate the usability of our tool, and conclude with a discussion of the potential of using text-to-image generative models combined with an interactive interface for visualization design.},
  archive      = {J_TVCG},
  author       = {Shishi Xiao and Suizi Huang and Yue Lin and Yilin Ye and Wei Zeng},
  doi          = {10.1109/TVCG.2023.3326913},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {284-294},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Let the chart spark: Embedding semantic context into chart with text-to-image generative model},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CommonsenseVIS: Visualizing and understanding commonsense
reasoning capabilities of natural language models. <em>TVCG</em>,
<em>30</em>(1), 273–283. (<a
href="https://doi.org/10.1109/TVCG.2023.3327153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, large pretrained language models have achieved compelling performance on commonsense benchmarks. Nevertheless, it is unclear what commonsense knowledge the models learn and whether they solely exploit spurious patterns. Feature attributions are popular explainability techniques that identify important input concepts for model outputs. However, commonsense knowledge tends to be implicit and rarely explicitly presented in inputs. These methods cannot infer models&#39; implicit reasoning over mentioned concepts. We present CommonsenseVIS , a visual explanatory system that utilizes external commonsense knowledge bases to contextualize model behavior for commonsense question-answering. Specifically, we extract relevant commonsense knowledge in inputs as references to align model behavior with human knowledge. Our system features multi-level visualization and interactive model probing and editing for different concepts and their underlying relations. Through a user study, we show that CommonsenseVIS helps NLP experts conduct a systematic and scalable visual analysis of models&#39; relational reasoning over concepts in different situations.},
  archive      = {J_TVCG},
  author       = {Xingbo Wang and Renfei Huang and Zhihua Jin and Tianqing Fang and Huamin Qu},
  doi          = {10.1109/TVCG.2023.3327153},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {273-283},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CommonsenseVIS: Visualizing and understanding commonsense reasoning capabilities of natural language models},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AttentionViz: A global view of transformer attention.
<em>TVCG</em>, <em>30</em>(1), 262–272. (<a
href="https://doi.org/10.1109/TVCG.2023.3327163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer models are revolutionizing machine learning, but their inner workings remain mysterious. In this work, we present a new visualization technique designed to help researchers understand the self-attention mechanism in transformers that allows these models to learn rich, contextual relationships between elements of a sequence. The main idea behind our method is to visualize a joint embedding of the query and key vectors used by transformer models to compute attention. Unlike previous attention visualization techniques, our approach enables the analysis of global patterns across multiple input sequences. We create an interactive visualization tool, AttentionViz (demo: http://attentionviz.com ), based on these joint query-key embeddings, and use it to study attention mechanisms in both language and vision transformers. We demonstrate the utility of our approach in improving model understanding and offering new insights about query-key interactions through several application scenarios and expert feedback.},
  archive      = {J_TVCG},
  author       = {Catherine Yeh and Yida Chen and Aoyu Wu and Cynthia Chen and Fernanda Viégas and Martin Wattenberg},
  doi          = {10.1109/TVCG.2023.3327163},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {262-272},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {AttentionViz: A global view of transformer attention},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Radial icicle tree (RIT): Node separation and area
constancy. <em>TVCG</em>, <em>30</em>(1), 251–261. (<a
href="https://doi.org/10.1109/TVCG.2023.3327178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Icicles and sunbursts are two commonly-used visual representations of trees. While icicle trees can map data values faithfully to rectangles of different sizes, often some rectangles are too narrow to be noticed easily. When an icicle tree is transformed into a sunburst tree, the width of each rectangle becomes the length of an annular sector that is usually longer than the original width. While sunburst trees alleviate the problem of narrow rectangles in icicle trees, it no longer maintains the consistency of size encoding. At different tree depths, nodes of the same data values are displayed in annular sections of different sizes in a sunburst tree, though they are represented by rectangles of the same size in an icicle tree. Furthermore, two nodes from different subtrees could sometimes appear as a single node in both icicle trees and sunburst trees. In this paper, we propose a new visual representation, referred to as radial icicle tree (RIT), which transforms the rectangular bounding box of an icicle tree into a circle, circular sector, or annular sector while introducing gaps between nodes and maintaining area constancy for nodes of the same size. We applied the new visual design to several datasets. Both the analytical design process and user-centered evaluation have confirmed that this new design has improved the design of icicles and sunburst trees without introducing any relative demerit.},
  archive      = {J_TVCG},
  author       = {Yuanzhe Jin and Tim J. A. de Jong and Martijn Tennekes and Min Chen},
  doi          = {10.1109/TVCG.2023.3327178},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {251-261},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Radial icicle tree (RIT): Node separation and area constancy},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cluster-aware grid layout. <em>TVCG</em>, <em>30</em>(1),
240–250. (<a href="https://doi.org/10.1109/TVCG.2023.3326934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grid visualizations are widely used in many applications to visually explain a set of data and their proximity relationships. However, existing layout methods face difficulties when dealing with the inherent cluster structures within the data. To address this issue, we propose a cluster-aware grid layout method that aims to better preserve cluster structures by simultaneously considering proximity, compactness, and convexity in the optimization process. Our method utilizes a hybrid optimization strategy that consists of two phases. The global phase aims to balance proximity and compactness within each cluster, while the local phase ensures the convexity of cluster shapes. We evaluate the proposed grid layout method through a series of quantitative experiments and two use cases, demonstrating its effectiveness in preserving cluster structures and facilitating analysis tasks.},
  archive      = {J_TVCG},
  author       = {Yuxing Zhou and Weikai Yang and Jiashu Chen and Changjian Chen and Zhiyang Shen and Xiaonan Luo and Lingyun Yu and Shixia Liu},
  doi          = {10.1109/TVCG.2023.3326934},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {240-250},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Cluster-aware grid layout},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visualization according to statisticians: An interview study
on the role of visualization for inferential statistics. <em>TVCG</em>,
<em>30</em>(1), 230–239. (<a
href="https://doi.org/10.1109/TVCG.2023.3326521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statisticians are not only one of the earliest professional adopters of data visualization, but also some of its most prolific users. Understanding how these professionals utilize visual representations in their analytic process may shed light on best practices for visual sensemaking. We present results from an interview study involving 18 professional statisticians (19.7 years average in the profession) on three aspects: (1) their use of visualization in their daily analytic work; (2) their mental models of inferential statistical processes; and (3) their design recommendations for how to best represent statistical inferences. Interview sessions consisted of discussing inferential statistics, eliciting participant sketches of suitable visual designs, and finally, a design intervention with our proposed visual designs. We analyzed interview transcripts using thematic analysis and open coding, deriving thematic codes on statistical mindset, analytic process, and analytic toolkit. The key findings for each aspect are as follows: (1) statisticians make extensive use of visualization during all phases of their work (and not just when reporting results); (2) their mental models of inferential methods tend to be mostly visually based; and (3) many statisticians abhor dichotomous thinking. The latter suggests that a multi-faceted visual display of inferential statistics that includes a visual indicator of analytically important effect sizes may help to balance the attributed epistemic power of traditional statistical testing with an awareness of the uncertainty of sensemaking.},
  archive      = {J_TVCG},
  author       = {Eric Newburger and Niklas Elmqvist},
  doi          = {10.1109/TVCG.2023.3326521},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {230-239},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualization according to statisticians: An interview study on the role of visualization for inferential statistics},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VISPUR: Visual aids for identifying and interpreting
spurious associations in data-driven decisions. <em>TVCG</em>,
<em>30</em>(1), 219–229. (<a
href="https://doi.org/10.1109/TVCG.2023.3326587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data and machine learning tools have jointly empowered humans in making data-driven decisions. However, many of them capture empirical associations that might be spurious due to confounding factors and subgroup heterogeneity. The famous Simpson&#39;s paradox is such a phenomenon where aggregated and subgroup-level associations contradict with each other, causing cognitive confusions and difficulty in making adequate interpretations and decisions. Existing tools provide little insights for humans to locate, reason about, and prevent pitfalls of spurious association in practice. We propose V ISPUR , a visual analytic system that provides a causal analysis framework and a human-centric workflow for tackling spurious associations. These include a C ONFOUNDER D ASHBOARD , which can automatically identify possible confounding factors, and a S UBGROUP V IEWER , which allows for the visualization and comparison of diverse subgroup patterns that likely or potentially result in a misinterpretation of causality. Additionally, we propose a R EASONING S TORYBOARD , which uses a flow-based approach to illustrate paradoxical phenomena, as well as an interactive D ECISION D IAGNOSIS panel that helps ensure accountable decision-making. Through an expert interview and a controlled user experiment, our qualitative and quantitative results demonstrate that the proposed “de-paradox” workflow and the designed visual analytic system are effective in helping human users to identify and understand spurious associations, as well as to make accountable causal decisions.},
  archive      = {J_TVCG},
  author       = {Xian Teng and Yongsu Ahn and Yu-Ru Lin},
  doi          = {10.1109/TVCG.2023.3326587},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {219-229},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VISPUR: Visual aids for identifying and interpreting spurious associations in data-driven decisions},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EVM: Incorporating model checking into exploratory visual
analysis. <em>TVCG</em>, <em>30</em>(1), 208–218. (<a
href="https://doi.org/10.1109/TVCG.2023.3326516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual analytics (VA) tools support data exploration by helping analysts quickly and iteratively generate views of data which reveal interesting patterns. However, these tools seldom enable explicit checks of the resulting interpretations of data—e.g., whether patterns can be accounted for by a model that implies a particular structure in the relationships between variables. We present EVM, a data exploration tool that enables users to express and check provisional interpretations of data in the form of statistical models. EVM integrates support for visualization-based model checks by rendering distributions of model predictions alongside user-generated views of data. In a user study with data scientists practicing in the private and public sector, we evaluate how model checks influence analysts&#39; thinking during data exploration. Our analysis characterizes how participants use model checks to scrutinize expectations about data generating process and surfaces further opportunities to scaffold model exploration in VA tools.},
  archive      = {J_TVCG},
  author       = {Alex Kale and Ziyang Guo and Xiao Li Qiao and Jeffrey Heer and Jessica Hullman},
  doi          = {10.1109/TVCG.2023.3326516},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {208-218},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {EVM: Incorporating model checking into exploratory visual analysis},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dead or alive: Continuous data profiling for interactive
data science. <em>TVCG</em>, <em>30</em>(1), 197–207. (<a
href="https://doi.org/10.1109/TVCG.2023.3327367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Profiling data by plotting distributions and analyzing summary statistics is a critical step throughout data analysis. Currently, this process is manual and tedious since analysts must write extra code to examine their data after every transformation. This inefficiency may lead to data scientists profiling their data infrequently, rather than after each transformation, making it easy for them to miss important errors or insights. We propose continuous data profiling as a process that allows analysts to immediately see interactive visual summaries of their data throughout their data analysis to facilitate fast and thorough analysis. Our system, AutoProfiler , presents three ways to support continuous data profiling: (1) it automatically displays data distributions and summary statistics to facilitate data comprehension; (2) it is live, so visualizations are always accessible and update automatically as the data updates; (3) it supports follow up analysis and documentation by authoring code for the user in the notebook. In a user study with 16 participants, we evaluate two versions of our system that integrate different levels of automation: both automatically show data profiles and facilitate code authoring, however, one version updates reactively (“live”) and the other updates only on demand (“dead”). We find that both tools, dead or alive, facilitate insight discovery with 91% of user-generated insights originating from the tools rather than manual profiling code written by users. Participants found live updates intuitive and felt it helped them verify their transformations while those with on-demand profiles liked the ability to look at past visualizations. We also present a longitudinal case study on how AutoProfiler helped domain scientists find serendipitous insights about their data through automatic, live data profiles. Our results have implications for the design of future tools that offer automated data analysis support.},
  archive      = {J_TVCG},
  author       = {Will Epperson and Vaishnavi Gorantla and Dominik Moritz and Adam Perer},
  doi          = {10.1109/TVCG.2023.3327367},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {197-207},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dead or alive: Continuous data profiling for interactive data science},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dataopsy: Scalable and fluid visual exploration using
aggregate query sculpting. <em>TVCG</em>, <em>30</em>(1), 186–196. (<a
href="https://doi.org/10.1109/TVCG.2023.3326594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present aggregate query sculpting (AQS), a faceted visual query technique for large-scale multidimensional data. As a “born scalable” query technique, AQS starts visualization with a single visual mark representing an aggregation of the entire dataset. The user can then progressively explore the dataset through a sequence of operations abbreviated as $\mathbb{P}^{6}$ : pivot (facet an aggregate based on an attribute), partition (lay out a facet in space), peek (see inside a subset using an aggregate visual representation), pile (merge two or more subsets), project (extracting a subset into a new substrate), and prune (discard an aggregate not currently of interest). We validate AQS with D ataopsy , a prototype implementation of AQS that has been designed for fluid interaction on desktop and touch-based mobile devices. We demonstrate AQS and Dataopsy using two case studies and three application examples.},
  archive      = {J_TVCG},
  author       = {Md Naimul Hoque and Niklas Elmqvist},
  doi          = {10.1109/TVCG.2023.3326594},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {186-196},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dataopsy: Scalable and fluid visual exploration using aggregate query sculpting},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ManiVault: A flexible and extensible visual analytics
framework for high-dimensional data. <em>TVCG</em>, <em>30</em>(1),
175–185. (<a href="https://doi.org/10.1109/TVCG.2023.3326582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploration and analysis of high-dimensional data are important tasks in many fields that produce large and complex data, like the financial sector, systems biology, or cultural heritage. Tailor-made visual analytics software is developed for each specific application, limiting their applicability in other fields. However, as diverse as these fields are, their characteristics and requirements for data analysis are conceptually similar. Many applications share abstract tasks and data types and are often constructed with similar building blocks. Developing such applications, even when based mostly on existing building blocks, requires significant engineering efforts. We developed ManiVault, a flexible and extensible open-source visual analytics framework for analyzing high-dimensional data. The primary objective of ManiVault is to facilitate rapid prototyping of visual analytics workflows for visualization software developers and practitioners alike. ManiVault is built using a plugin-based architecture that offers easy extensibility. While our architecture deliberately keeps plugins self-contained, to guarantee maximum flexibility and re-usability, we have designed and implemented a messaging API for tight integration and linking of modules to support common visual analytics design patterns. We provide several visualization and analytics plugins, and ManiVault&#39;s API makes the integration of new plugins easy for developers. ManiVault facilitates the distribution of visualization and analysis pipelines and results for practitioners through saving and reproducing complete application states. As such, ManiVault can be used as a communication tool among researchers to discuss workflows and results. A copy of this paper and all supplemental material is available at osf.io/9k6jw, and source code at github.com/ManiVaultStudio .},
  archive      = {J_TVCG},
  author       = {Alexander Vieth and Thomas Kroes and Julian Thijssen and Baldur van Lew and Jeroen Eggermont and Soumyadeep Basu and Elmar Eisemann and Anna Vilanova and Thomas Höllt and Boudewijn Lelieveldt},
  doi          = {10.1109/TVCG.2023.3326582},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {175-185},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ManiVault: A flexible and extensible visual analytics framework for high-dimensional data},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Class-constrained t-SNE: Combining data features and class
probabilities. <em>TVCG</em>, <em>30</em>(1), 164–174. (<a
href="https://doi.org/10.1109/TVCG.2023.3326600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data features and class probabilities are two main perspectives when, e.g., evaluating model results and identifying problematic items. Class probabilities represent the likelihood that each instance belongs to a particular class, which can be produced by probabilistic classifiers or even human labeling with uncertainty. Since both perspectives are multi-dimensional data, dimensionality reduction (DR) techniques are commonly used to extract informative characteristics from them. However, existing methods either focus solely on the data feature perspective or rely on class probability estimates to guide the DR process. In contrast to previous work where separate views are linked to conduct the analysis, we propose a novel approach, class-constrained t-SNE, that combines data features and class probabilities in the same DR result. Specifically, we combine them by balancing two corresponding components in a cost function to optimize the positions of data points and iconic representation of classes – class landmarks. Furthermore, an interactive user-adjustable parameter balances these two components so that users can focus on the weighted perspectives of interest and also empowers a smooth visual transition between varying perspectives to preserve the mental map. We illustrate its application potential in model evaluation and visual-interactive labeling. A comparative analysis is performed to evaluate the DR results.},
  archive      = {J_TVCG},
  author       = {Linhao Meng and Stef van den Elzen and Nicola Pezzotti and Anna Vilanova},
  doi          = {10.1109/TVCG.2023.3326600},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {164-174},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Class-constrained t-SNE: Combining data features and class probabilities},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). QEVIS: Multi-grained visualization of distributed query
execution. <em>TVCG</em>, <em>30</em>(1), 153–163. (<a
href="https://doi.org/10.1109/TVCG.2023.3326930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed query processing systems such as Apache Hive and Spark are widely-used in many organizations for large-scale data analytics. Analyzing and understanding the query execution process of these systems are daily routines for engineers and crucial for identifying performance problems, optimizing system configurations, and rectifying errors. However, existing visualization tools for distributed query execution are insufficient because (i) most of them (if not all) do not provide fine-grained visualization (i.e., the atomic task level), which can be crucial for understanding query performance and reasoning about the underlying execution anomalies, and (ii) they do not support proper linkages between system status and query execution, which makes it difficult to identify the causes of execution problems. To tackle these limitations, we propose QEVIS, which visualizes distributed query execution process with multiple views that focus on different granularities and complement each other. Specifically, we first devise a query logical plan layout algorithm to visualize the overall query execution progress compactly and clearly. We then propose two novel scoring methods to summarize the anomaly degrees of the jobs and machines during query execution, and visualize the anomaly scores intuitively, which allow users to easily identify the components that are worth paying attention to. Moreover, we devise a scatter plot-based task view to show a massive number of atomic tasks, where task distribution patterns are informative for execution problems. We also equip QEVIS with a suite of auxiliary views and interaction methods to support easy and effective cross-view exploration, which makes it convenient to track the causes of execution problems. QEVIS has been used in the production environment of our industry partner, and we present three use cases from real-world applications and user interview to demonstrate its effectiveness. QEVIS is open-source at https://github.com/DBGroup-SUSTech/QEVIS .},
  archive      = {J_TVCG},
  author       = {Qiaomu Shen and Zhengxin You and Xiao Yan and Chaozu Zhang and Ke Xu and Dan Zeng and Jianbin Qin and Bo Tang},
  doi          = {10.1109/TVCG.2023.3326930},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {153-163},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {QEVIS: Multi-grained visualization of distributed query execution},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A parallel framework for streaming dimensionality reduction.
<em>TVCG</em>, <em>30</em>(1), 142–152. (<a
href="https://doi.org/10.1109/TVCG.2023.3326515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visualization of streaming high-dimensional data often needs to consider the speed in dimensionality reduction algorithms, the quality of visualized data patterns, and the stability of view graphs that usually change over time with new data. Existing methods of streaming high-dimensional data visualization primarily line up essential modules in a serial manner and often face challenges in satisfying all these design considerations. In this research, we propose a novel parallel framework for streaming high-dimensional data visualization to achieve high data processing speed, high quality in data patterns, and good stability in visual presentations. This framework arranges all essential modules in parallel to mitigate the delays caused by module waiting in serial setups. In addition, to facilitate the parallel pipeline, we redesign these modules with a parametric non-linear embedding method for new data embedding, an incremental learning method for online embedding function updating, and a hybrid strategy for optimized embedding updating. We also improve the coordination mechanism among these modules. Our experiments show that our method has advantages in embedding speed, quality, and stability over other existing methods to visualize streaming high-dimensional data.},
  archive      = {J_TVCG},
  author       = {Jiazhi Xia and Linquan Huang and Yiping Sun and Zhiwei Deng and Xiaolong Luke Zhang and Minfeng Zhu},
  doi          = {10.1109/TVCG.2023.3326515},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {142-152},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A parallel framework for streaming dimensionality reduction},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Socrates: Data story generation via adaptive machine-guided
elicitation of user feedback. <em>TVCG</em>, <em>30</em>(1), 131–141.
(<a href="https://doi.org/10.1109/TVCG.2023.3327363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual data stories can effectively convey insights from data, yet their creation often necessitates intricate data exploration, insight discovery, narrative organization, and customization to meet the communication objectives of the storyteller. Existing automated data storytelling techniques, however, tend to overlook the importance of user customization during the data story authoring process, limiting the system&#39;s ability to create tailored narratives that reflect the user&#39;s intentions. We present a novel data story generation workflow that leverages adaptive machine-guided elicitation of user feedback to customize the story. Our approach employs an adaptive plug-in module for existing story generation systems, which incorporates user feedback through interactive questioning based on the conversation history and dataset. This adaptability refines the system&#39;s understanding of the user&#39;s intentions, ensuring the final narrative aligns with their goals. We demonstrate the feasibility of our approach through the implementation of an interactive prototype: Socrates. Through a quantitative user study with 18 participants that compares our method to a state-of-the-art data story generation algorithm, we show that Socrates produces more relevant stories with a larger overlap of insights compared to human-generated stories. We also demonstrate the usability of Socrates via interviews with three data analysts and highlight areas of future work.},
  archive      = {J_TVCG},
  author       = {Guande Wu and Shunan Guo and Jane Hoffswell and Gromit Yeuk-Yin Chan and Ryan A. Rossi and Eunyee Koh},
  doi          = {10.1109/TVCG.2023.3327363},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {131-141},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Socrates: Data story generation via adaptive machine-guided elicitation of user feedback},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EMPHASISCHECKER: A tool for guiding chart and caption
emphasis. <em>TVCG</em>, <em>30</em>(1), 120–130. (<a
href="https://doi.org/10.1109/TVCG.2023.3327150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent work has shown that when both the chart and caption emphasize the same aspects of the data, readers tend to remember the doubly-emphasized features as takeaways; when there is a mismatch, readers rely on the chart to form takeaways and can miss information in the caption text. Through a survey of 280 chart-caption pairs in real-world sources (e.g., news media, poll reports, government reports, academic articles, and Tableau Public), we find that captions often do not emphasize the same information in practice, which could limit how effectively readers take away the authors&#39; intended messages. Motivated by the survey findings, we present E MPHASISCHECKER , an interactive tool that highlights visually prominent chart features as well as the features emphasized by the caption text along with any mismatches in the emphasis. The tool implements a time-series prominent feature detector based on the Ramer-Douglas-Peucker algorithm and a text reference extractor that identifies time references and data descriptions in the caption and matches them with chart data. This information enables authors to compare features emphasized by these two modalities, quickly see mismatches, and make necessary revisions. A user study confirms that our tool is both useful and easy to use when authoring charts and captions.},
  archive      = {J_TVCG},
  author       = {Dae Hyun Kim and Seulgi Choi and Juho Kim and Vidya Setlur and Maneesh Agrawala},
  doi          = {10.1109/TVCG.2023.3327150},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {120-130},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {EMPHASISCHECKER: A tool for guiding chart and caption emphasis},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data player: Automatic generation of data videos with
narration-animation interplay. <em>TVCG</em>, <em>30</em>(1), 109–119.
(<a href="https://doi.org/10.1109/TVCG.2023.3327197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data visualizations and narratives are often integrated to convey data stories effectively. Among various data storytelling formats, data videos have been garnering increasing attention. These videos provide an intuitive interpretation of data charts while vividly articulating the underlying data insights. However, the production of data videos demands a diverse set of professional skills and considerable manual labor, including understanding narratives, linking visual elements with narration segments, designing and crafting animations, recording audio narrations, and synchronizing audio with visual animations. To simplify this process, our paper introduces a novel method, referred to as Data Player, capable of automatically generating dynamic data videos with narration-animation interplay. This approach lowers the technical barriers associated with creating data videos rich in narration. To enable narration-animation interplay, Data Player constructs references between visualizations and text input. Specifically, it first extracts data into tables from the visualizations. Subsequently, it utilizes large language models to form semantic connections between text and visuals. Finally, Data Player encodes animation design knowledge as computational low-level constraints, allowing for the recommendation of suitable animation presets that align with the audio narration produced by text-to-speech technologies. We assessed Data Player&#39;s efficacy through an example gallery, a user study, and expert interviews. The evaluation results demonstrated that Data Player can generate high-quality data videos that are comparable to human-composed ones.},
  archive      = {J_TVCG},
  author       = {Leixian Shen and Yizhi Zhang and Haidong Zhang and Yun Wang},
  doi          = {10.1109/TVCG.2023.3327197},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {109-119},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Data player: Automatic generation of data videos with narration-animation interplay},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Character-oriented design for visual data storytelling.
<em>TVCG</em>, <em>30</em>(1), 98–108. (<a
href="https://doi.org/10.1109/TVCG.2023.3326578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When telling a data story, an author has an intention they seek to convey to an audience. This intention can be of many forms such as to persuade, to educate, to inform, or even to entertain. In addition to expressing their intention, the story plot must balance being consumable and enjoyable while preserving scientific integrity. In data stories, numerous methods have been identified for constructing and presenting a plot. However, there is an opportunity to expand how we think and create the visual elements that present the story. Stories are brought to life by characters; often they are what make a story captivating, enjoyable, memorable, and facilitate following the plot until the end. Through the analysis of 160 existing data stories, we systematically investigate and identify distinguishable features of characters in data stories, and we illustrate how they feed into the broader concept of “character-oriented design”. We identify the roles and visual representations data characters assume as well as the types of relationships these roles have with one another. We identify characteristics of antagonists as well as define conflict in data stories. We find the need for an identifiable central character that the audience latches on to in order to follow the narrative and identify their visual representations. We then illustrate “character-oriented design” by showing how to develop data characters with common data story plots. With this work, we present a framework for data characters derived from our analysis; we then offer our extension to the data storytelling process using character-oriented design. To access our supplemental materials please visit https://chaorientdesignds.github.io/ .},
  archive      = {J_TVCG},
  author       = {Keshav Dasu and Yun-Hsin Kuo and Kwan-Liu Ma},
  doi          = {10.1109/TVCG.2023.3326578},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {98-108},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Character-oriented design for visual data storytelling},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VideoPro: A visual analytics approach for interactive video
programming. <em>TVCG</em>, <em>30</em>(1), 87–97. (<a
href="https://doi.org/10.1109/TVCG.2023.3326586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constructing supervised machine learning models for real-world video analysis require substantial labeled data, which is costly to acquire due to scarce domain expertise and laborious manual inspection. While data programming shows promise in generating labeled data at scale with user-defined labeling functions, the high dimensional and complex temporal information in videos poses additional challenges for effectively composing and evaluating labeling functions. In this paper, we propose VideoPro , a visual analytics approach to support flexible and scalable video data programming for model steering with reduced human effort. We first extract human-understandable events from videos using computer vision techniques and treat them as atomic components of labeling functions. We further propose a two-stage template mining algorithm that characterizes the sequential patterns of these events to serve as labeling function templates for efficient data labeling. The visual interface of VideoPro facilitates multifaceted exploration, examination, and application of the labeling templates, allowing for effective programming of video data at scale. Moreover, users can monitor the impact of programming on model performance and make informed adjustments during the iterative programming process. We demonstrate the efficiency and effectiveness of our approach with two case studies and expert interviews.},
  archive      = {J_TVCG},
  author       = {Jianben He and Xingbo Wang and Kam Kwai Wong and Xijie Huang and Changjian Chen and Zixin Chen and Fengjie Wang and Min Zhu and Huamin Qu},
  doi          = {10.1109/TVCG.2023.3326586},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {87-97},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VideoPro: A visual analytics approach for interactive video programming},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified interactive model evaluation for classification,
object detection, and instance segmentation in computer vision.
<em>TVCG</em>, <em>30</em>(1), 76–86. (<a
href="https://doi.org/10.1109/TVCG.2023.3326588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing model evaluation tools mainly focus on evaluating classification models, leaving a gap in evaluating more complex models, such as object detection. In this paper, we develop an open-source visual analysis tool, Uni-Evaluator, to support a unified model evaluation for classification, object detection, and instance segmentation in computer vision. The key idea behind our method is to formulate both discrete and continuous predictions in different tasks as unified probability distributions. Based on these distributions, we develop 1) a matrix-based visualization to provide an overview of model performance; 2) a table visualization to identify the problematic data subsets where the model performs poorly; 3) a grid visualization to display the samples of interest. These visualizations work together to facilitate the model evaluation from a global overview to individual samples. Two case studies demonstrate the effectiveness of Uni-Evaluator in evaluating model performance and making informed improvements.},
  archive      = {J_TVCG},
  author       = {Changjian Chen and Yukai Guo and Fengyuan Tian and Shilong Liu and Weikai Yang and Zhaowei Wang and Jing Wu and Hang Su and Hanspeter Pfister and Shixia Liu},
  doi          = {10.1109/TVCG.2023.3326588},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {76-86},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A unified interactive model evaluation for classification, object detection, and instance segmentation in computer vision},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guided visual analytics for image selection in time and
space. <em>TVCG</em>, <em>30</em>(1), 66–75. (<a
href="https://doi.org/10.1109/TVCG.2023.3326572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unexploded Ordnance (UXO) detection, the identification of remnant active bombs buried underground from archival aerial images, implies a complex workflow involving decision-making at each stage. An essential phase in UXO detection is the task of image selection, where a small subset of images must be chosen from archives to reconstruct an area of interest (AOI) and identify craters. The selected image set must comply with good spatial and temporal coverage over the AOI, particularly in the temporal vicinity of recorded aerial attacks, and do so with minimal images for resource optimization. This paper presents a guidance-enhanced visual analytics prototype to select images for UXO detection. In close collaboration with domain experts, our design process involved analyzing user tasks, eliciting expert knowledge, modeling quality metrics, and choosing appropriate guidance. We report on a user study with two real-world scenarios of image selection performed with and without guidance. Our solution was well-received and deemed highly usable. Through the lens of our task-based design and developed quality measures, we observed guidance-driven changes in user behavior and improved quality of analysis results. An expert evaluation of the study allowed us to improve our guidance-enhanced prototype further and discuss new possibilities for user-adaptive guidance.},
  archive      = {J_TVCG},
  author       = {Ignacio Pérez-Messina and Davide Ceneda and Silvia Miksch},
  doi          = {10.1109/TVCG.2023.3326572},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {66-75},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Guided visual analytics for image selection in time and space},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vortex lens: Interactive vortex core line extraction using
observed line integral convolution. <em>TVCG</em>, <em>30</em>(1),
55–65. (<a href="https://doi.org/10.1109/TVCG.2023.3326915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a novel method for detecting and visualizing vortex structures in unsteady 2D fluid flows. The method is based on an interactive local reference frame estimation that minimizes the observed time derivative of the input flow field $\mathrm{v}(x, t)$ . A locally optimal reference frame $\mathrm{w}(x, t)$ assists the user in the identification of physically observable vortex structures in Observed Line Integral Convolution (LIC) visualizations. The observed LIC visualizations are interactively computed and displayed in a user-steered vortex lens region, embedded in the context of a conventional LIC visualization outside the lens. The locally optimal reference frame is then used to detect observed critical points, where $\mathrm{v}=\mathrm{w}$ , which are used to seed vortex core lines. Each vortex core line is computed as a solution of the ordinary differential equation (ODE) $\dot{w}(t)=\mathrm{w}(w(t), t)$ , with an observed critical point as initial condition $(w(t_{0}), t_{0})$ . During integration, we enforce a strict error bound on the difference between the extracted core line and the integration of a path line of the input vector field, i.e., a solution to the ODE $\dot{v}(t)=\mathrm{v}(v(t), t)$ . We experimentally verify that this error depends on the step size of the core line integration. This ensures that our method extracts Lagrangian vortex core lines that are the simultaneous solution of both ODEs with a numerical error that is controllable by the integration step size. We show the usability of our method in the context of an interactive system using a lens metaphor, and evaluate the results in comparison to state-of-the-art vortex core line extraction methods.},
  archive      = {J_TVCG},
  author       = {Peter Rautek and Xingdi Zhang and Bernhard Woschizka and Thomas Theußl and Markus Hadwiger},
  doi          = {10.1109/TVCG.2023.3326915},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {55-65},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Vortex lens: Interactive vortex core line extraction using observed line integral convolution},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visualization of discontinuous vector field topology.
<em>TVCG</em>, <em>30</em>(1), 45–54. (<a
href="https://doi.org/10.1109/TVCG.2023.3326519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper extends the concept and the visualization of vector field topology to vector fields with discontinuities. We address the non-uniqueness of flow in such fields by introduction of a time-reversible concept of equivalence. This concept generalizes streamlines to streamsets and thus vector field topology to discontinuous vector fields in terms of invariant streamsets. We identify respective novel critical structures as well as their manifolds, investigate their interplay with traditional vector field topology, and detail the application and interpretation of our approach using specifically designed synthetic cases and a simulated case from physics.},
  archive      = {J_TVCG},
  author       = {Egzon Miftari and Daniel Durstewitz and Filip Sadlo},
  doi          = {10.1109/TVCG.2023.3326519},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {45-54},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualization of discontinuous vector field topology},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TimeSplines: Sketch-based authoring of flexible and
idiosyncratic timelines. <em>TVCG</em>, <em>30</em>(1), 34–44. (<a
href="https://doi.org/10.1109/TVCG.2023.3326520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Timelines are essential for visually communicating chronological narratives and reflecting on the personal and cultural significance of historical events. Existing visualization tools tend to support conventional linear representations, but fail to capture personal idiosyncratic conceptualizations of time. In response, we built TimeSplines, a visualization authoring tool that allows people to sketch multiple free-form temporal axes and populate them with heterogeneous, time-oriented data via incremental and lazy data binding. Authors can bend, compress, and expand temporal axes to emphasize or de-emphasize intervals based on their personal importance; they can also annotate the axes with text and figurative elements to convey contextual information. The results of two user studies show how people appropriate the concepts in TimeSplines to express their own conceptualization of time, while our curated gallery of images demonstrates the expressive potential of our approach.},
  archive      = {J_TVCG},
  author       = {Anna Offenwanger and Matthew Brehmer and Fanny Chevalier and Theophanis Tsandilas},
  doi          = {10.1109/TVCG.2023.3326520},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {34-44},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TimeSplines: Sketch-based authoring of flexible and idiosyncratic timelines},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Swaying the public? Impacts of election forecast
visualizations on emotion, trust, and intention in the 2022 u.s.
midterms. <em>TVCG</em>, <em>30</em>(1), 23–33. (<a
href="https://doi.org/10.1109/TVCG.2023.3327356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We conducted a longitudinal study during the 2022 U.S. midterm elections, investigating the real-world impacts of uncertainty visualizations. Using our forecast model of the governor elections in 33 states, we created a website and deployed four uncertainty visualizations for the election forecasts: single quantile dotplot (1-Dotplot), dual quantile dotplots (2-Dotplot), dual histogram intervals (2-Interval), and Plinko quantile dotplot (Plinko), an animated design with a physical and probabilistic analogy. Our online experiment ran from Oct. 18, 2022, to Nov. 23, 2022, involving 1,327 participants from 15 states. We use Bayesian multilevel modeling and post-stratification to produce demographically-representative estimates of people&#39;s emotions, trust in forecasts, and political participation intention. We find that election forecast visualizations can heighten emotions, increase trust, and slightly affect people&#39;s intentions to participate in elections. 2-Interval shows the strongest effects across all measures; 1-Dotplot increases trust the most after elections. Both visualizations create emotional and trust gaps between different partisan identities, especially when a Republican candidate is predicted to win. Our qualitative analysis uncovers the complex political and social contexts of election forecast visualizations, showcasing that visualizations may provoke polarization. This intriguing interplay between visualization types, partisanship, and trust exemplifies the fundamental challenge of disentangling visualization from its context, underscoring a need for deeper investigation into the real-world impacts of visualizations. Our preprint and supplements are available at https://doi.org/osf.io/ajq8f .},
  archive      = {J_TVCG},
  author       = {Fumeng Yang and Mandi Cai and Chloe Mortenson and Hoda Fakhari and Ayse D. Lokmanoglu and Jessica Hullman and Steven Franconeri and Nicholas Diakopoulos and Erik C. Nisbet and Matthew Kay},
  doi          = {10.1109/TVCG.2023.3327356},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {23-33},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Swaying the public? impacts of election forecast visualizations on emotion, trust, and intention in the 2022 U.S. midterms},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast compressed segmentation volumes for scientific
visualization. <em>TVCG</em>, <em>30</em>(1), 12–22. (<a
href="https://doi.org/10.1109/TVCG.2023.3326573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Voxel-based segmentation volumes often store a large number of labels and voxels, and the resulting amount of data can make storage, transfer, and interactive visualization difficult. We present a lossless compression technique which addresses these challenges. It processes individual small bricks of a segmentation volume and compactly encodes the labelled regions and their boundaries by an iterative refinement scheme. The result for each brick is a list of labels, and a sequence of operations to reconstruct the brick which is further compressed using rANS-entropy coding. As the relative frequencies of operations are very similar across bricks, the entropy coding can use global frequency tables for an entire data set which enables efficient and effective parallel (de)compression. Our technique achieves high throughput (up to gigabytes per second both for compression and decompression) and strong compression ratios of about 1% to 3% of the original data set size while being applicable to GPU-based rendering. We evaluate our method for various data sets from different fields and demonstrate GPU-based volume visualization with on-the-fly decompression, level-of-detail rendering (with optional on-demand streaming of detail coefficients to the GPU), and a caching strategy for decompressed bricks for further performance improvement.},
  archive      = {J_TVCG},
  author       = {Max Piochowiak and Carsten Dachsbacher},
  doi          = {10.1109/TVCG.2023.3326573},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {12-22},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Fast compressed segmentation volumes for scientific visualization},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Affective visualization design: Leveraging the emotional
impact of data. <em>TVCG</em>, <em>30</em>(1), 1–11. (<a
href="https://doi.org/10.1109/TVCG.2023.3327385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, more and more researchers have reflected on the undervaluation of emotion in data visualization and highlighted the importance of considering human emotion in visualization design. Meanwhile, an increasing number of studies have been conducted to explore emotion-related factors. However, so far, this research area is still in its early stages and faces a set of challenges, such as the unclear definition of key concepts, the insufficient justification of why emotion is important in visualization design, and the lack of characterization of the design space of affective visualization design. To address these challenges, first, we conducted a literature review and identified three research lines that examined both emotion and data visualization. We clarified the differences between these research lines and kept 109 papers that studied or discussed how data visualization communicates and influences emotion. Then, we coded the 109 papers in terms of how they justified the legitimacy of considering emotion in visualization design (i.e., why emotion is important) and identified five argumentative perspectives. Based on these papers, we also identified 61 projects that practiced affective visualization design. We coded these design projects in three dimensions, including design fields ( where ), design tasks ( what ), and design methods ( how ), to explore the design space of affective visualization design.},
  archive      = {J_TVCG},
  author       = {Xingyu Lan and Yanqiu Wu and Nan Cao},
  doi          = {10.1109/TVCG.2023.3327385},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {1},
  number       = {1},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Affective visualization design: Leveraging the emotional impact of data},
  volume       = {30},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
