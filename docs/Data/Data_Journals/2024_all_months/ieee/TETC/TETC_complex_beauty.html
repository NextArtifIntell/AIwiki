<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TETC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tetc---92">TETC - 92</h2>
<ul>
<li><details>
<summary>
(2024). Unsupervised domain adaptation via contrastive adversarial
domain mixup: A case study on COVID-19. <em>TETC</em>, <em>12</em>(4),
1105–1116. (<a href="https://doi.org/10.1109/TETC.2024.3354419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training large deep learning (DL) models with high performance for natural language downstream tasks usually requires rich-labeled data. However, in a real-world application of COVID-19 information service (e.g., misinformation detection, question answering), a fundamental challenge is the lack of the labeled COVID data to enable supervised end-to-end training of the models for different downstream tasks, especially at the early stage of the pandemic. To address this challenge, we propose an unsupervised domain adaptation framework using contrastive learning and adversarial domain mixup to transfer the knowledge from an existing source data domain to the target COVID-19 data domain. In particular, to bridge the gap between the source domain and the target domain, our method reduces a radial basis function (RBF) based discrepancy between these two domains. Moreover, we leverage the power of domain adversarial examples to establish an intermediate domain mixup, where the latent representations of the input text from both domains could be mixed during the training process. In this paper, we focus on two prevailing downstream tasks in mining COVID-19 text data: COVID-19 misinformation detection and COVID-19 news question answering. Extensive domain adaptation experiments on multiple real-world datasets suggest that our method can effectively adapt misinformation detection and question answering systems to the unseen COVID-19 target domain with significant improvements compared to the state-of-the-art baselines.},
  archive      = {J_TETC},
  author       = {Huimin Zeng and Zhenrui Yue and Lanyu Shang and Yang Zhang and Dong Wang},
  doi          = {10.1109/TETC.2024.3354419},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1105-1116},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Unsupervised domain adaptation via contrastive adversarial domain mixup: A case study on COVID-19},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Engravings, secrets, and interpretability of neural
networks. <em>TETC</em>, <em>12</em>(4), 1093–1104. (<a
href="https://doi.org/10.1109/TETC.2024.3358759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a definition and examines the problem of undetectably engraving special input/output information into a Neural Network (NN). Investigation of this problem is significant given the ubiquity of neural networks and society&#39;s reliance on their proper training and use. We systematically study this question and provide (1) definitions of security for secret engravings, (2) machine learning methods for the construction of an engraved network, (3) a threat model that is instantiated with state-of-the-art interpretability methods to devise distinguishers/attackers. In this work, there are two kinds of algorithms. First, the constructions of engravings through machine learning training methods. Second, the distinguishers associated with the threat model. The weakest of our engraved NN constructions are insecure and can be broken by our distinguishers, whereas other, more systematic engravings are resilient to each of our distinguishing attacks on three prototypical image classification datasets. Our threat model is of independent interest, as it provides a concrete quantification/benchmark for the “goodness” of interpretability methods.},
  archive      = {J_TETC},
  author       = {Nathaniel Hobbs and Periklis A. Papakonstantinou and Jaideep Vaidya},
  doi          = {10.1109/TETC.2024.3358759},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1093-1104},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Engravings, secrets, and interpretability of neural networks},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hardware-aware DNN compression via diverse pruning and
mixed-precision quantization. <em>TETC</em>, <em>12</em>(4), 1079–1092.
(<a href="https://doi.org/10.1109/TETC.2023.3346944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have shown significant advantages in a wide variety of domains. However, DNNs are becoming computationally intensive and energy hungry at an exponential pace, while at the same time, there is a vast demand for running sophisticated DNN-based services on resource constrained embedded devices. In this paper, we target energy-efficient inference on embedded DNN accelerators. To that end, we propose an automated framework to compress DNNs in a hardware-aware manner by jointly employing pruning and quantization. We explore, for the first time, per-layer fine- and coarse-grained pruning, in the same DNN architecture, in addition to low bit-width mixed-precision quantization for weights and activations. Reinforcement Learning (RL) is used to explore the associated design space and identify the pruning-quantization configuration so that the energy consumption is minimized whilst the prediction accuracy loss is retained at acceptable levels. Using our novel composite RL agent we are able to extract energy-efficient solutions without requiring retraining and/or fine tuning. Our extensive experimental evaluation over widely used DNNs and the CIFAR-10/100 and ImageNet datasets demonstrates that our framework achieves 39% average energy reduction for 1.7% average accuracy loss and outperforms significantly the state-of-the-art approaches.},
  archive      = {J_TETC},
  author       = {Konstantinos Balaskas and Andreas Karatzas and Christos Sad and Kostas Siozios and Iraklis Anagnostopoulos and Georgios Zervakis and Jörg Henkel},
  doi          = {10.1109/TETC.2023.3346944},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1079-1092},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Hardware-aware DNN compression via diverse pruning and mixed-precision quantization},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combining trust graphs and keystroke dynamics to counter
fake identities in social networks. <em>TETC</em>, <em>12</em>(4),
1066–1078. (<a href="https://doi.org/10.1109/TETC.2023.3346691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fake identity in social networks is a phenomenon that is strongly increasing, and it is used for discovering personal information, identity theft, influencing people, spreading fake news, fraud, and so on. In this article, we face this problem by introducing the concept of certified social profiles and by propagating this property through a collaborative approach that exploits keystroke-dynamic-recognition techniques to identify illegal access to certified profiles. We propose a decentralized approach to compute the trust level of a social profile, and we show the robustness of the proposal by analyzing the security of the trust mechanism through experimental validation.},
  archive      = {J_TETC},
  author       = {Francesco Buccafurri and Gianluca Lax and Denis Migdal and Lorenzo Musarella and Christophe Rosenberger},
  doi          = {10.1109/TETC.2023.3346691},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1066-1078},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Combining trust graphs and keystroke dynamics to counter fake identities in social networks},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the privacy of the count-min sketch: Extracting the top-k
elements. <em>TETC</em>, <em>12</em>(4), 1056–1065. (<a
href="https://doi.org/10.1109/TETC.2024.3383321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the frequency of elements in a data stream and identifying the elements that appear many times (also known as heavy hitters) are needed in many applications such as traffic monitoring in networks or popularity estimate in web and social networks. The Count-Min Sketch (CMS) is probably one of the most widely used algorithms for frequency estimate. The CMS uses a sub-linear space to provide queries for data streams and retrieve an approximate value for the frequency of events. It has been used in many different applications and scenarios, making its security and privacy a matter of interest. This paper considers the privacy of the CMS and presents an algorithm to extract the most frequent elements (also known as top-K) and their estimate from a CMS. This is possible for universes of a limited size; when the attacker has access to the sketch, its hash functions and the counters at a specific point of time. The algorithm is tested using CAIDA traces showing that it is able to retrieve the group of top-K elements with an acceptable percentage of false positives and negatives. The results improve with the size of the sketch and for smaller values of K, indicating that in some practical settings an attacker can extract substantial information about the top-K elements from the sketch. The code used in the simulation is provided in a public open-source repository to facilitate reproducing our results and extending the ideas presented in this paper.},
  archive      = {J_TETC},
  author       = {Alfonso Sánchez-Macián and Jorge Martínez and Pedro Reviriego and Shanshan Liu and Fabrizio Lombardi},
  doi          = {10.1109/TETC.2024.3383321},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1056-1065},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {On the privacy of the count-min sketch: Extracting the top-K elements},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MiniFloats on RISC-v cores: ISA extensions with
mixed-precision short dot products. <em>TETC</em>, <em>12</em>(4),
1040–1055. (<a href="https://doi.org/10.1109/TETC.2024.3365354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-precision floating-point (FP) formats have recently been intensely investigated in the context of machine learning inference and training applications. While 16-bit formats are already widely used, 8-bit FP data types have lately emerged as a viable option for neural network training when employed in a mixed-precision scenario and combined with rounding methods increasing the precision in compound additions, such as stochastic rounding. So far, hardware implementations supporting FP8 are mostly implemented within domain-specific accelerators. We propose two RISC-V instruction set architecture (ISA) extensions, enhancing respectively scalar and vector general-purpose cores with low and mixed-precision capabilities. The extensions support two 8-bit and two 16-bit FP formats and are based on dot-product instructions accumulating at higher precision. We develop a hardware unit supporting mixed-precision dot products and stochastic rounding and integrate it into an open-source floating-point unit (FPU). Finally, we integrate the enhanced FPU into a cluster of scalar cores, as well as a cluster of vector cores, and implement them in a 12 nm FinFET technology. The former achieves 575 GFLOPS/W on FP8-to-FP16 matrix multiplications at 0.8 V, 1.26 GHz; the latter reaches 860 GFLOPS/W at 0.8 V, 1.08 GHz, 1.93x higher efficiency than computing on FP16-to-FP32.},
  archive      = {J_TETC},
  author       = {Luca Bertaccini and Gianna Paulin and Matheus Cavalcante and Tim Fischer and Stefan Mach and Luca Benini},
  doi          = {10.1109/TETC.2024.3365354},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1040-1055},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {MiniFloats on RISC-V cores: ISA extensions with mixed-precision short dot products},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive task migration in multiplex networked industrial
chains. <em>TETC</em>, <em>12</em>(4), 1025–1039. (<a
href="https://doi.org/10.1109/TETC.2024.3364703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the cooperation structures of industrial chains have evolved into multiplex networks, in which product agents are connected through various types of links. Due to the constraints of the multi-coupled interaction structure of the multiplex networked industrial chains, the load imbalances generated by the industrial production processes will cascade in and between different network layers, thus affecting the load balance of the whole system. The challenges that arise when attempting such load balancing among multiplex networked industrial chains are twofold: 1) The multiplex networked interaction structure adds new constraints to traditional multiagent task migration problems, which increases the solution space dimension, and 2) The cascaded load imbalances require tasks to be migrated adaptively, which complicates the solution space structure, and it is proven $\mathcal {NP}$ -hard to achieve such load balancing. Then, a hierarchical cascade-triggered task migration algorithm is designed, where key agents are selected to cooperate with each other in a hierarchical control form to achieve load balancing between network layers, and appropriate agents are cascade-triggered to migrate tasks adaptively to achieve load balancing in network layers. Finally, the algorithm is extensively evaluated in experiments, concluding that it can significantly increase the resulting utility and task completion proportion, while efficiently reducing the task completion cost. In particular, the algorithm does not appear to be statistically different in the resulting optimization objectives from the optimal result computed by the CPLEX solver, but it may consume less runtime.},
  archive      = {J_TETC},
  author       = {Kai Di and Fulin Chen and Yuanshuang Jiang and Pan Li and Tianyi Liu and Yichuan Jiang},
  doi          = {10.1109/TETC.2024.3364703},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1025-1039},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Adaptive task migration in multiplex networked industrial chains},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Personalized privacy-preserving framework for cross-silo
federated learning. <em>TETC</em>, <em>12</em>(4), 1014–1024. (<a
href="https://doi.org/10.1109/TETC.2024.3356068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is recently surging as a promising decentralized deep learning (DL) framework that enables DL-based approaches trained collaboratively across clients without sharing private data. However, in the context of the central party being active and dishonest, the data of individual clients might be perfectly reconstructed, leading to the high possibility of sensitive information being leaked. Moreover, FL also suffers from the nonindependent and identically distributed (non-IID) data among clients, resulting in the degradation in the inference performance on local clients’ data. In this paper, we propose a novel framework, namely Personalized Privacy-Preserving Federated Learning (PPPFL), with a concentration on cross-silo FL to overcome these challenges. Specifically, we introduce a stabilized variant of the Model-Agnostic Meta-Learning (MAML) algorithm to collaboratively train a global initialization from clients’ synthetic data generated by Differential Private Generative Adversarial Networks (DP-GANs). After reaching convergence, the global initialization will be locally adapted by the clients to their private data. Through extensive experiments, we empirically show that our proposed framework outperforms multiple FL baselines on different datasets, including MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100.},
  archive      = {J_TETC},
  author       = {Van-Tuan Tran and Huy-Hieu Pham and Kok-Seng Wong},
  doi          = {10.1109/TETC.2024.3356068},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1014-1024},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Personalized privacy-preserving framework for cross-silo federated learning},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reliability evaluation and fault tolerant design for KLL
sketches. <em>TETC</em>, <em>12</em>(4), 1002–1013. (<a
href="https://doi.org/10.1109/TETC.2023.3324331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantile estimation is a fundamental task in Big Data analysis. In order to achieve high-speed estimation with low memory consumption, especially for streaming Big Data processing, data sketches which provide approximate estimates at low overhead are commonly used, and the Karnin-Lang-Liberty (KLL) sketch is one of the most popular options. However, soft errors in KLL memory may significantly degrade estimation performance. In this article, the influence of soft errors on the KLL sketch is considered for the first time. First, the reliability of KLL to soft errors is studied through theoretical analysis and fault injection experiments. The evaluation results show that the errors in the KLL construction phase may cause a large deviation in the estimated value. Then, two protection schemes are proposed based on a single parity check (SPC) and on the incremental property (IP) of the KLL memory. Further evaluation shows that the proposed schemes can significantly improve the reliability of KLL, and even remove the effect SEUs on the highest bits. In particular, the SPC scheme that requires additional memory, provides better protection for middle bit positions than the IP scheme which does not introduce any memory overhead.},
  archive      = {J_TETC},
  author       = {Zhen Gao and Jinhua Zhu and Pedro Reviriego},
  doi          = {10.1109/TETC.2023.3324331},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1002-1013},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Reliability evaluation and fault tolerant design for KLL sketches},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A design framework for hardware-efficient logarithmic
floating-point multipliers. <em>TETC</em>, <em>12</em>(4), 991–1001. (<a
href="https://doi.org/10.1109/TETC.2024.3365650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The symbiotic use of logarithmic approximation in floating-point (FP) multiplication can significantly reduce the hardware complexity of a multiplier. However, it is difficult for a limited number of logarithmic FP multipliers (LFPMs) to fit in a specific error-tolerant application, such as neural networks (NNs) and digital signal processing, due to their unique error characteristics. This article proposes a design framework for generating LFPMs. We consider two FP representation formats with different ranges of mantissas, the IEEE 754 Standard FP Format and the Nearest Power of Two FP Format. For both logarithm and anti-logarithm computation, the applicable regions of inputs are first evenly divided into several intervals, and then approximation methods with negative or positive errors are developed for each sub-region. By using piece-wise functions, different configurations of approximation methods throughout applicable regions are created, leading to LFPMs with various trade-offs between accuracy and hardware cost. The variety of error characteristics of LFPMs is discussed and the generic hardware implementation is illustrated. As case studies, two LFPM designs are presented and evaluated in applications of JPEG compression and NNs. They do not only increase the classification accuracy, but also achieve smaller PDPs compared to the exact FP multiplier, while being more accurate than a recent logarithmic FP design.},
  archive      = {J_TETC},
  author       = {Tingting Zhang and Zijing Niu and Jie Han},
  doi          = {10.1109/TETC.2024.3365650},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10-12},
  number       = {4},
  pages        = {991-1001},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A design framework for hardware-efficient logarithmic floating-point multipliers},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CiM-BNN: Computing-in-MRAM architecture for stochastic
computing based bayesian neural network. <em>TETC</em>, <em>12</em>(4),
980–990. (<a href="https://doi.org/10.1109/TETC.2023.3317136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian neural network (BNN) has gradually attracted researchers’ attention with its uncertainty representation and high robustness. However, high computational complexity, large number of sampling operations, and the von-Neumann architecture make a great limitation for the further deployment of BNN on edge devices. In this article, a new computing-in-MRAM BNN architecture (CiM-BNN) is proposed for stochastic computing (SC)-based BNN to alleviate these problems. In SC domain, neural network parameters are represented in bitstream format. In order to leverage the characteristics of bitstreams, CiM-BNN redesigns the computing-in-memory architecture without complex peripheral circuit requirements and MRAM state flipping. Additionally, real-time Gaussian random number generators are designed using MRAM&#39;s stochastic property to further improve energy efficiency. Cadence Virtuoso is used to evaluate the proposed architecture. Simulation results show that energy consumption is reduced more than 93.6% with slight accuracy decrease compared to FPGA implementation with von-Neumann architecture in SC domain.},
  archive      = {J_TETC},
  author       = {Huiyi Gu and Xiaotao Jia and Yuhao Liu and Jianlei Yang and Xueyan Wang and Youguang Zhang and Sorin Dan Cotofana and Weisheng Zhao},
  doi          = {10.1109/TETC.2023.3317136},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10-12},
  number       = {4},
  pages        = {980-990},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {CiM-BNN: Computing-in-MRAM architecture for stochastic computing based bayesian neural network},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Approximate MAC unit using static segmentation.
<em>TETC</em>, <em>12</em>(4), 968–979. (<a
href="https://doi.org/10.1109/TETC.2023.3315301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we investigate a novel approximate multiply-and-accumulate (MAC) unit, that computes Y = A × B + C using static segmentation. The proposed architecture uses a unique carry-propagate adder and performs segmentation on the three operands A , B , and C , to reduce hardware cost. The circuit can be configured at design-time by two parameters. The first one controls the segmentation on A and B , while the second one controls the segmentation on C and the adder length. An error compensation technique is also employed, to reduce the approximation error. Error analysis and implementation results in 28nm CMOS for 8-bits multiplier with 20-bits and 24-bits addition are presented. The proposed approximate MACs outperform the state of the art, showing the largest power saving when the mean relative error distance ( MRED ) is larger than 2 × 10 −3 and 4 × 10 −5 for 20 and 24-bits addition, respectively. For MRED of about 6 × 10 −3 the proposed approximate MAC with 20-bits addition exhibits a power reduction larger than 60% compared to the exact MAC and larger than 27% compared to the state-of-the-art approximate MACs. Application examples to image filtering and template matching show that proposed approximate circuits are good candidates in applications where their error performances are acceptable.},
  archive      = {J_TETC},
  author       = {Gennaro Di Meo and Gerardo Saggese and Antonio G. M. Strollo and Davide De Caro},
  doi          = {10.1109/TETC.2023.3315301},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10-12},
  number       = {4},
  pages        = {968-979},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Approximate MAC unit using static segmentation},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extremely energy-efficient non-linear function approximation
framework using stochastic superconductor devices. <em>TETC</em>,
<em>12</em>(4), 956–967. (<a
href="https://doi.org/10.1109/TETC.2023.3330979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently developed adiabatic quantum-flux-parametron (AQFP) superconducting technology achieves the highest energy efficiency among various superconducting logic families, potentially 10 4 -10 5 gain compared with state-of-the-art CMOS. Besides ultra-high energy efficiency, AQFP exhibits two unique characteristics: the deep pipelining nature as all logic gates are clocked and the potential of building stochastic number generators (SNGs) using a single AQFP gate, far more efficient than SNGs implemented in conventional CMOS. These unique characteristics indicate that the AQFP technology is highly compatible with stochastic computing (SC) implementations, where operands are represented by a time-independent bit sequence utilizing the deep pipelining structure of AQFP. To shed some light on the SC-based design methodology on novel superconducting technologies, we propose an AQFP-based non-linear function approximation framework with the fashion of Bernstein polynomials, achieving a general hardware architecture to perform multiple non-linear functions without any extra hardware overhead. Experimental results of 9 common non-linear functions widely used in pattern recognition, signal processing, and neural networks reveal that our work provides outstanding energy efficiency with sufficient computing accuracy. The energy-delay-error-product (EDE MAE P) of the proposed design, in terms of the polynomial degree of 3, 5 and 7, are 3.47 × 10 −25 J $\cdot$ s, 3.63 × 10 −25 J $\cdot$ s and 6.79 × 10 −25 J $\cdot$ s on average, respectively, achieving 5-6 orders better performance than its CMOS counterpart. Further discussions on the measurement results of trial-fabricated AQFP comparators reveal the future research directions of AQFP-based SC implementations.},
  archive      = {J_TETC},
  author       = {Olivia Chen and Renyuan Zhang and Wenhui Luo and Yanzhi Wang and Nobuyuki Yoshikawa},
  doi          = {10.1109/TETC.2023.3330979},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10-12},
  number       = {4},
  pages        = {956-967},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Extremely energy-efficient non-linear function approximation framework using stochastic superconductor devices},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guest editorial: Special section on “approximate data
processing: Computing, storage and applications.” <em>TETC</em>,
<em>12</em>(4), 954–955. (<a
href="https://doi.org/10.1109/TETC.2024.3488452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TETC},
  author       = {Ke Chen and Shanshan Liu and Weiqiang Liu and Fabrizio Lombardi and Nader Bagherzadeh},
  doi          = {10.1109/TETC.2024.3488452},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {10-12},
  number       = {4},
  pages        = {954-955},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Guest editorial: special section on “Approximate data processing: computing, storage and applications”},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Near-memory computing with compressed embedding table for
personalized recommendation. <em>TETC</em>, <em>12</em>(3), 938–951. (<a
href="https://doi.org/10.1109/TETC.2023.3345870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL)-based recommendation models play an important role in many real-world applications. However, an embedding layer, which is a key part of the DL-based recommendation models, requires sparse memory accesses to a very large memory space followed by the pooling operations (i.e., reduction operations). It makes the system overprovision memory capacity for model deployment. Moreover, with conventional CPU-based architecture, it is difficult to exploit the locality, causing a huge burden for data transfer between the CPU and memory. To resolve this problem, we propose an embedding vector element quantization and compression method to reduce the memory footprint (capacity) required by the embedding tables. In addition, to reduce the amount of data transfer and memory access, we propose near-memory acceleration hardware with an SRAM buffer that stores the frequently accessed embedding vectors. Our quantization and compression method results in compression ratios of 3.95–4.14 for embedding tables in widely used datasets while negligibly affecting the inference accuracy. Our acceleration technique with 3D stacked DRAM memories, which facilitates the near-memory processing in the logic die with high DRAM bandwidth, leads to 4.9 × –5.4 × embedding layer speedup as compared to the 8-core CPU-based execution while reducing the memory energy consumption by 5.9 × −12.1 ×, on average.},
  archive      = {J_TETC},
  author       = {Jeongmin Lim and Young Geun Kim and Sung Woo Chung and Farinaz Koushanfar and Joonho Kong},
  doi          = {10.1109/TETC.2023.3345870},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {938-951},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Near-memory computing with compressed embedding table for personalized recommendation},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrated edge computing and blockchain: A general medical
data sharing framework. <em>TETC</em>, <em>12</em>(3), 924–937. (<a
href="https://doi.org/10.1109/TETC.2023.3344655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical data sharing is crucial to enhance diagnostic efficiency and improve the quality of medical data analysis. However, related endeavors face obstacles due to insufficient collaboration among medical institutions, and traditional cloud-based sharing platforms lead to concerns regarding security and privacy. To overcome these challenges, the paper introduces MSNET, a novel framework that seamlessly combines blockchain and edge computing. Data traceability and access control are ensured by employing blockchain as a security layer. The blockchain stores only data summaries instead of complete medical data, thus enhancing scalability and transaction efficiency. The raw medical data are securely processed on edge servers within each institution, with data standardization and keyword extraction. To facilitate data access and sharing among institutions, smart contracts are designed to promote transparency and data accuracy. Moreover, a supervision mechanism is established to maintain a trusted environment, provide reliable evidence against dubious data-sharing practices, and encourage institutions to share data voluntarily. This novel framework effectively overcomes the limitations of traditional blockchain solutions, offering an efficient and secure method for medical data sharing and thereby fostering collaboration and innovation in the healthcare industry.},
  archive      = {J_TETC},
  author       = {Zongjin Li and Jie Zhang and Jian Zhang and Ya Zheng and Xunjie Zong},
  doi          = {10.1109/TETC.2023.3344655},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {924-937},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Integrated edge computing and blockchain: A general medical data sharing framework},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint partial offloading and resource allocation for parked
vehicle-assisted multi-access edge computing. <em>TETC</em>,
<em>12</em>(3), 918–923. (<a
href="https://doi.org/10.1109/TETC.2023.3344133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, parked vehicle-assisted multi-access edge computing (PVMEC) has emerged to expand the computational power of MEC networks by utilizing the opportunistic resources of parked vehicles (PVs) for computation offloading. In this article, we study a joint optimization problem of partial offloading and resource allocation in a PVMEC paradigm that enables each mobile device (MD) to offload its task partially to either the MEC server or nearby PVs. The problem is first formulated as a mixed-integer nonlinear programming problem with the aim of maximizing the total offloading utility of all MDs in terms of the benefit of reducing latency through offloading and the overall cost of using computing and networking resources. We then propose a partial offloading scheme, which employs a differentiation method to derive the optimal offloading ratio and resource allocation while optimizing the task assignment using a metaheuristic solution based on the whale optimization algorithm. Finally, evaluation results justify the superior system utility of our proposal compared with existing baselines.},
  archive      = {J_TETC},
  author       = {Xuan-Qui Pham and Thien Huynh-The and Dong-Seong Kim},
  doi          = {10.1109/TETC.2023.3344133},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {918-923},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Joint partial offloading and resource allocation for parked vehicle-assisted multi-access edge computing},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward designing high-speed cost-efficient quantum
reversible carry select adders. <em>TETC</em>, <em>12</em>(3), 905–917.
(<a href="https://doi.org/10.1109/TETC.2023.3332426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to classical computing implementations, reversible arithmetic adders offer a valuable platform for implementing quantum computation models in digital systems and specific applications, such as cryptography and natural language processing. Reversible logic efficiently prevents energy wastage through thermal dissipation. This study presents a comprehensive exploration introducing new carry-select adders (CSLA) based on quantum and reversible logic. Five reversible CSLA designs are proposed and compared, evaluating various criteria, including speed, quantum cost, and area, compared to previously published schemes. These comparative metrics are formulated for arbitrary n-bit size blocks. Each design type is described generically, capable of implementing carry-select adders of any size. As the best outcome, this study proposes an optimized reversible adder circuit that addresses quantum propagation delay, achieving an acceptable trade-off with quantum cost compared to its counterparts. This article reduces calculation delay by 66%, 73%, 82%, and 87% for 16, 32, 64, and 128 bits, respectively, while maintaining a lower quantum cost in all cases.},
  archive      = {J_TETC},
  author       = {Shekoofeh Moghimi and Mohammad Reza Reshadinezhad and Antonio Rubio},
  doi          = {10.1109/TETC.2023.3332426},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {905-917},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Toward designing high-speed cost-efficient quantum reversible carry select adders},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TCAM-GNN: A TCAM-based data processing strategy for GNN over
sparse graphs. <em>TETC</em>, <em>12</em>(3), 891–904. (<a
href="https://doi.org/10.1109/TETC.2023.3328008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The graph neural network (GNN) has recently become an emerging research topic for processing non-euclidean data structures since the data used in various popular application domains are usually modeled as a graph, such as social networks, recommendation systems, and computer vision. Previous GNN accelerators commonly utilize the hybrid architecture to resolve the issue of “hybrid computing pattern” in GNN training. Nevertheless, the hybrid architecture suffers from poor utilization of hardware resources mainly due to the dynamic workloads between different phases in GNN. To address these issues, existing GNN accelerators adopt a unified structure with numerous processing elements and high bandwidth memory. However, the large amount of data movement between the processor and memory could heavily downgrade the performance of such accelerators in real-world graphs. As a result, the processing-in-memory architecture, such as the ReRAM-based crossbar, becomes a promising solution to reduce the memory overhead of GNN training. In this work, we present the TCAM-GNN, a novel TCAM-based data processing strategy, to enable high-throughput and energy-efficient GNN training over ReRAM-based crossbar architecture. Several hardware co-designed data structures and placement methods are proposed to fully exploit the parallelism in GNN during training. In addition, we propose a dynamic fixed-point formatting approach to resolve the precision issue. An adaptive data reusing policy is also proposed to enhance the data locality of graph features by the bootstrapping batch sampling approach. Overall, TCAM-GNN could enhance computing performance by 4.25× and energy efficiency by 9.11× on average compared to the neural network accelerators.},
  archive      = {J_TETC},
  author       = {Yu-Pang Wang and Wei-Chen Wang and Yuan-Hao Chang and Chieh-Lin Tsai and Tei-Wei Kuo and Chun-Feng Wu and Chien-Chung Ho and Han-Wen Hu},
  doi          = {10.1109/TETC.2023.3328008},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {891-904},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {TCAM-GNN: A TCAM-based data processing strategy for GNN over sparse graphs},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed indexing schemes for k-dominant skyline
analytics on uncertain edge-IoT data. <em>TETC</em>, <em>12</em>(3),
878–890. (<a href="https://doi.org/10.1109/TETC.2023.3326295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skyline queries typically search a Pareto-optimal set from a given data set to solve the corresponding multiobjective optimization problem. As the number of criteria increases, the skyline presumes excessive data items, which yield a meaningless result. To address this curse of dimensionality, we proposed a $k$ -dominant skyline in which the number of skyline members was reduced by relaxing the restriction on the number of dimensions, considering the uncertainty of data. Specifically, each data item was associated with a probability of appearance, which represented the probability of becoming a member of the $k$ -dominant skyline. As data items appear continuously in data streams, the corresponding $k$ -dominant skyline may vary with time. Therefore, an effective and rapid mechanism of updating the $k$ -dominant skyline becomes crucial. Herein, we proposed two time-efficient schemes, Middle Indexing (MI) and All Indexing (AI), for $k$ -dominant skyline in distributed edge-computing environments, where irrelevant data items can be effectively excluded from the compute to reduce the processing duration. Furthermore, the proposed schemes were validated with extensive experimental simulations. The experimental results demonstrated that the proposed MI and AI schemes reduced the computation time by approximately 13% and 56%, respectively, compared with the existing method.},
  archive      = {J_TETC},
  author       = {Chuan-Chi Lai and Hsuan-Yu Lin and Chuan-Ming Liu},
  doi          = {10.1109/TETC.2023.3326295},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {878-890},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Distributed indexing schemes for K-dominant skyline analytics on uncertain edge-IoT data},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Memristive crossbar array-based adversarial defense using
compression. <em>TETC</em>, <em>12</em>(3), 864–877. (<a
href="https://doi.org/10.1109/TETC.2023.3319659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article shows that Memristive Crossbar Array (MCA)-based neuromorphic architectures provide a robust defense against adversarial attacks due to the stochastic behavior of memristors. Furthermore, it shows that adversarial robustness can be further improved by compression-based preprocessing steps that can be implemented on MCAs. It also evaluates the effect of inter-chip process variations on adversarial robustness using the proposed MCA implementation and studies the effect of on-chip training. It shows that adversarial attacks do not uniformly affect the classification accuracy of different chips. Experimental evidence using a variety of datasets and attack models supports the impact of MCA-based neuromorphic architectures and compression-based preprocessing implemented using MCA on defending against adversarial attacks. It is also experimentally shown that the on-chip training results in high resiliency to adversarial attacks in all chips.},
  archive      = {J_TETC},
  author       = {Bijay Raj Paudel and Spyros Tragoudas},
  doi          = {10.1109/TETC.2023.3319659},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {864-877},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Memristive crossbar array-based adversarial defense using compression},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A privacy enforcing framework for data streams on the edge.
<em>TETC</em>, <em>12</em>(3), 852–863. (<a
href="https://doi.org/10.1109/TETC.2023.3315131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent developments in machine learning (ML) allow for efficient data stream processing and also help in meeting various privacy requirements. Traditionally, predefined privacy policies are enforced in resource-rich and homogeneous environments such as in the cloud to protect sensitive information from being exposed. However, large amounts of data streams generated from heterogeneous IoT devices often result in high computational costs, cause network latency, and increase the chance of data interruption as data travels away from the source. Therefore, this article proposes a novel privacy-enforcing framework for transforming data streams by executing various privacy policies close to the data source. To achieve our proposed framework, we enable domain experts to specify high-level privacy policies in a human-readable form. Then, the edge-based runtime system analyzes data streams (i.e., generated from nearby IoT devices), interprets privacy policies (i.e., deployed on edge devices), and transforms data streams if privacy violations occur. Our proposed runtime mechanism uses a Deep Neural Networks (DNN) technique to detect privacy violations within the streamed data. Furthermore, we discuss the framework, processes of the approach, and the experiments carried out on a real-world testbed to validate its feasibility and applicability.},
  archive      = {J_TETC},
  author       = {Boris Sedlak and Ilir Murturi and Praveen Kumar Donta and Schahram Dustdar},
  doi          = {10.1109/TETC.2023.3315131},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {852-863},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A privacy enforcing framework for data streams on the edge},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Geometric deep learning strategies for the characterization
of academic collaboration networks. <em>TETC</em>, <em>12</em>(3),
840–851. (<a href="https://doi.org/10.1109/TETC.2023.3315954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper examines how geometric deep learning techniques may be employed to analyze academic collaboration networks (ACNs) and how using textual information drawn from publications improves the overall performance of the system. The proposed experimental pipeline was used to analyze the collaboration network of the Machine Learning Genoa Center (MaLGa) research group. First, we find the optimal method for embedding the input data graph and extracting meaningful keywords for the available publications. We then use Graph Neural Networks (GNN) for node type and research topic classification. Finally, we explore how the resulting corpus can be used to create a recommender system for optimal navigation of the ACN. Our results show that the GNN-based recommender system achieved high accuracy in suggesting unexplored nodes to users. Overall, this study demonstrates the potential for using geometric deep learning and Natural Language Processing (NLP) to best represent the scientific production of ACNs. In the future, we plan to incorporate the temporal nature of the data and navigation statistics of users exploring the graph as additional input for the recommender system.},
  archive      = {J_TETC},
  author       = {Daniele Pretolesi and Davide Garbarino and Daniele Giampaoli and Andrea Vian and Annalisa Barla},
  doi          = {10.1109/TETC.2023.3315954},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {840-851},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Geometric deep learning strategies for the characterization of academic collaboration networks},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient ternary logic circuits optimized by ternary
arithmetic algorithms. <em>TETC</em>, <em>12</em>(3), 826–839. (<a
href="https://doi.org/10.1109/TETC.2023.3321050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-valued logic (MVL) circuits, especially the ternary logic circuits, have attracted great attention in recent years due to their higher information density than binary logic systems. However, the basic construction method for MVL circuit standard cells and the CMOS fabrication possibility/compatibility issues are still to be addressed. In this work, we propose various ternary arithmetic circuits (adders and multipliers) with embedded ternary arithmetic algorithms to improve the efficiency. First, ternary cycling gates are designed to optimize both the arithmetic algorithms and logic circuits of ternary adders. Second, optimized ternary Boolean truth table is used to simplify the circuit complexity. Third, high-speed ternary Wallace tree multipliers are implemented with task dividing policy. Significant improvements in propagation delay and power-delay-product (PDP) have been achieved as compared with previous works. In particular, the ternary full adder shows 11 aJ PDP at 0.5 GHz, which is the best result among all the reported works using the same simulation platform. And an average PDP improvement of 36.8% in the ternary multiplier is also achieved. Furthermore, the proposed methods have been successfully explored using standard CMOS 180nm silicon devices, indicating its great potential for the practical application of ternary computing in the near future.},
  archive      = {J_TETC},
  author       = {Guangchao Zhao and Zhiwei Zeng and Xingli Wang and Abdelrahman G. Qoutb and Philippe Coquet and Eby G. Friedman and Beng Kang Tay and Mingqiang Huang},
  doi          = {10.1109/TETC.2023.3321050},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {826-839},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Efficient ternary logic circuits optimized by ternary arithmetic algorithms},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using diversities to model the reliability of two-version
machine learning systems. <em>TETC</em>, <em>12</em>(3), 810–825. (<a
href="https://doi.org/10.1109/TETC.2023.3322563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The N-version machine learning system (MLS) is an architectural approach to reduce error outputs from a system by redundant configuration using multiple machine learning (ML) modules. Improved system reliability achieved by N-version MLSs inherently depends on how diverse ML models are employed and how diverse input data sets are given. However, neither error input spaces of individual ML models nor input data distributions are obtainable in practice, which is a fundamental barrier to understanding the reliability improvement by N-version architectures. In this paper, we introduce two diversity measures quantifying the similarities of ML models’ capabilities and the interdependence of input data sets causing errors, respectively. The defined measures are used to formulate the reliability of an elemental N-version MLS called dependent double-modules double-inputs MLS. The system is assumed to fail when two ML modules output errors simultaneously for the same classification task. The reliabilities of different architecture options for this MLS are comprehensively analyzed through a compact matrix representation form of the proposed reliability model. The theoretical analysis and numerical results show that the architecture exploiting two diversities achieves preferable reliability under reasonable assumptions. Intuitive relations between diversity parameters and architecture reliabilities are also demonstrated through numerical examples.},
  archive      = {J_TETC},
  author       = {Fumio Machida},
  doi          = {10.1109/TETC.2023.3322563},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {810-825},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Using diversities to model the reliability of two-version machine learning systems},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non-invasive reverse engineering of one-hot finite state
machines using scan dump data. <em>TETC</em>, <em>12</em>(3), 795–809.
(<a href="https://doi.org/10.1109/TETC.2023.3322299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finite-state machine (FSM) always works as a core control unit of a chip or a system. As a high level design, FSM has also been exploited to build multiple secure designs as it is deemed hard to discern FSM structure from the netlist or physical design. However, these secure designs can never sustain once the FSM structure is reversed. Reverse engineering FSM not only indicates the access of the control scheme of a design, but also poses a severe threat to those FSM-based secure designs. As the one-hot encoding FSM is widely adopted in various circuit designs, this paper proposes a non-invasive method to reverse engineer the one-hot encoding FSM. The data dumped from the scan chain during chip operation is first collected. The scan data is then used to identify all the candidate sets of state registers which satisfy two necessary conditions for one-hot state registers. Association relationship between the candidate registers and data registers are further evaluated to identify the unique target set of state registers. The transitions among FSM states are finally retrieved based on the scan dump data from those identified state registers. The experimental results on the benchmark circuits of different size show that this proposed method can identify all one-hot state registers exactly and the transitions can be retrieved at a high accuracy while the existing methods cannot achieve a satisfactory correct detection rate for one-hot encoding FSM.},
  archive      = {J_TETC},
  author       = {Zhaoxuan Dong and Aijiao Cui and Hao Lu},
  doi          = {10.1109/TETC.2023.3322299},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {795-809},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Non-invasive reverse engineering of one-hot finite state machines using scan dump data},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing neural architecture search with multiple hardware
constraints for deep learning model deployment on tiny IoT devices.
<em>TETC</em>, <em>12</em>(3), 780–794. (<a
href="https://doi.org/10.1109/TETC.2023.3322033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid proliferation of computing domains relying on Internet of Things (IoT) devices has created a pressing need for efficient and accurate deep-learning (DL) models that can run on low-power devices. However, traditional DL models tend to be too complex and computationally intensive for typical IoT end-nodes. To address this challenge, Neural Architecture Search (NAS) has emerged as a popular design automation technique for co-optimizing the accuracy and complexity of deep neural networks. Nevertheless, existing NAS techniques require many iterations to produce a network that adheres to specific hardware constraints, such as the maximum memory available on the hardware or the maximum latency allowed by the target application. In this work, we propose a novel approach to incorporate multiple constraints into so-called Differentiable NAS optimization methods, which allows the generation, in a single shot, of a model that respects user-defined constraints on both memory and latency in a time comparable to a single standard training. The proposed approach is evaluated on five IoT-relevant benchmarks, including the MLPerf Tiny suite and Tiny ImageNet, demonstrating that, with a single search, it is possible to reduce memory and latency by 87.4% and 54.2%, respectively (as defined by our targets), while ensuring non-inferior accuracy on state-of-the-art hand-tuned deep neural networks for TinyML.},
  archive      = {J_TETC},
  author       = {Alessio Burrello and Matteo Risso and Beatrice Alessandra Motetti and Enrico Macii and Luca Benini and Daniele Jahier Pagliari},
  doi          = {10.1109/TETC.2023.3322033},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {780-794},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Enhancing neural architecture search with multiple hardware constraints for deep learning model deployment on tiny IoT devices},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Memristive crossbar array-based computing framework via DWT
for biomedical image enhancement. <em>TETC</em>, <em>12</em>(3),
766–779. (<a href="https://doi.org/10.1109/TETC.2023.3318303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Here, we report the fabrication of Y 2 O 3 -based memristive crossbar array (MCA) by utilizing dual ion beam sputtering system, which shows high cyclic stability in the resistive switching behavior. Further, the obtained experimental results are validated with an analytical MCA based model, which exhibits extremely well fitting with the corresponding experimental data. Moreover, the experimentally validated analytical model is further used for biomedical image analysis, specifically computed tomography (CT) scan and magnetic resonance imaging (MRI) images by utilizing the 2-dimensional image decomposition technique. The different levels of decomposition are used for different threshold values which help to analyze the quality of the reconstructed image in terms of peak signal-to-noise ratio, structural similarity index and mean square error. For the MRI and CT scan images, at the first decomposition level, the data compression ratio of 21.01%, and 47.81% with Haar and 18.82%, and 46.05% with biorthogonal wavelet are obtained. Furthermore, the impact of brightness is also analyzed which shows a sufficient increment in the quality of output image by 103.72% and 18.59% for CT scan and MRI image, respectively for Haar wavelet. The proposed MCA based model for image processing is a novel approach to reduce the computation time and storage for biomedical engineering.},
  archive      = {J_TETC},
  author       = {Kumari Jyoti and Mohit Kumar Gautam and Sanjay Kumar and Sai Sushma and Ram Bilas Pachori and Shaibal Mukherjee},
  doi          = {10.1109/TETC.2023.3318303},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {766-779},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Memristive crossbar array-based computing framework via DWT for biomedical image enhancement},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MissII: Missing information imputation for traffic data.
<em>TETC</em>, <em>12</em>(3), 752–765. (<a
href="https://doi.org/10.1109/TETC.2023.3280481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber-Physical-Social Systems (CPSS) offer a new perspective for applying advanced information technology to improve urban transportation. However, real-world traffic datasets collected from sensing devices like loop sensors often contain corrupted or missing values. The incompleteness of traffic data poses great challenges to downstream data analysis tasks and applications. Most existing data-driven methods only impute missing values based on observed data or hypothetical models, thus ignoring the incorporation of social world information into traffic data imputation. The connection between real-world social activities and CPSS is crucial. In this paper, a novel theory-guided traffic data imputation framework, namely MissII, is proposed. In MissII, we first estimate the traffic flow between two PoIs (Points of Interest) according to spatial interaction theory by considering the physical environment information (e.g., population distributions) and human social interactions (e.g., destination choice game). Moreover, we further refine the estimated traffic flow by considering the effects of road interactions and PoIs. Then, the estimated traffic flow is input into the non-parametric GAN model as real samples to guide the training process. Extensive experiments are conducted on real-world traffic dataset to demonstrate the effectiveness of the proposed framework.},
  archive      = {J_TETC},
  author       = {Mingliang Hou and Tao Tang and Feng Xia and Ibrahim Sultan and Roopdeep Kaur and Xiangjie Kong},
  doi          = {10.1109/TETC.2023.3280481},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {752-765},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {MissII: Missing information imputation for traffic data},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Designing mobile technologies to encourage civic engagement:
The role of situated motivational affordances. <em>TETC</em>,
<em>12</em>(3), 739–751. (<a
href="https://doi.org/10.1109/TETC.2023.3296772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social and ubiquitous computing opens up many opportunities to engage citizens in activities that benefit their communities. Technology is ready and available, but there are still open issues concerning how to engage people in activities that are not extrinsically rewarding or whose impact is not immediately perceived. In this paper, we explore the role that situated motivational affordances can play in encouraging citizens in one of such activities, early warning. With this purpose, we designed and implemented a gamified app, IWarn that was iteratively designed following an action-research process to align the needs and capabilities of two types of stakeholders: emergency managers and citizens. The situated motivational affordances framework was used to lead the evaluation considering the motivational affordances enabled by the app and the situation in which it was used. The IWarn app was evaluated in an in-the-wild deployment where 4 emergency workers and 17 citizens took part in a real exercise for one week. Our results suggest that the gamified elements helped to improve intrinsic and extrinsic motivation and user engagement. This work contributes to the social computing domain by illustrating a use case where carefully designed gamification can help in engaging citizens in participatory processes},
  archive      = {J_TETC},
  author       = {Mónica Sánchez de Francisco and Paloma Díaz and Teresa Onorati and Álvaro Monteron and Ignacio Aedo},
  doi          = {10.1109/TETC.2023.3296772},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {739-751},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Designing mobile technologies to encourage civic engagement: The role of situated motivational affordances},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). D<span class="math inline"><em>i</em></span>e<span
class="math inline"><em>v</em></span>d: Disruptive event detection from
dynamic datastreams using continual machine learning: A case study with
twitter. <em>TETC</em>, <em>12</em>(3), 727–738. (<a
href="https://doi.org/10.1109/TETC.2023.3272973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying disruptive events (riots, protests, natural calamities) from social media is important for maintaining social order and addressing geopolitical concerns. Existing works on identifying disruptive events use classical machine learning (ML) models on static datasets. However, social networks are dynamic entities and cannot be practically modeled using static techniques. A viable alternative is the emerging Continual Machine Learning (CML) approach which applies the knowledge acquired from the past to learn future tasks. However, existing CML techniques are trained and tested on static data and are incapable of handling real-time data obtained from dynamic environments. This paper presents a novel D $i$ E $v$ D framework for disruptive event detection using Continual Machine Learning (CML) specifically for dynamic data streams. We have used Twitter social media as a case study of the real-time and dynamic data provider. To the best of our knowledge, this is the first attempt to use CML for socially disruptive event detection. Comprehensive performance analysis show that our framework effectively identifies disruptive events with 98% accuracy and can classify them with an average incremental accuracy of 76.8%. Moreover, computational analysis is performed to establish the effectiveness of D $i$ E $v$ D by applying various language models and statistical tests.},
  archive      = {J_TETC},
  author       = {Aditi Seetha and Satyendra Singh Chouhan and Emmanuel S. Pilli and Vaskar Raychoudhury},
  doi          = {10.1109/TETC.2023.3272973},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {727-738},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {D$i$E$v$D: disruptive event detection from dynamic datastreams using continual machine learning: a case study with twitter},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Concept stability entropy: A novel group cohesion measure in
social networks. <em>TETC</em>, <em>12</em>(3), 715–726. (<a
href="https://doi.org/10.1109/TETC.2023.3315335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group cohesion is regarded as a central group property across both social psychology and sociology. It facilities the understanding of the organizational behavior of users, and in turn guides the users to work well together in order to achieve goals within a social network. Therefore, group cohesion assessment is a crucial research issue for social network analysis. Group cohesion is often viewed as network density in the current state-of-the-art. Due to the advantages of characterizing the cohesion of a network with concept stability, this article presents a novel group cohesion measure, called concept stability entropy inspired by Shannon Entropy. Particularly, the scale of concept stability entropy is investigated. Considering the dynamic nature of social networks, an incremental algorithm for concept stability entropy computation is devised. In addition, we explore the correlation between concept stability entropy and other related metrics, i.e., network density, average degree, and average clustering coefficient. Extensive experimental results first validate that the concept stability entropy falls into the range of $[0, log(k)]$ ( $k$ is the number of formal concepts), and then demonstrate that the concept stability entropy has a positive correlation with the average degree and a negative correlation with the network density and average clustering coefficient. Practically, a case study on the COVID-2019 virus network is conducted for illustrating the usefulness of our proposed group cohesion assessment approach.},
  archive      = {J_TETC},
  author       = {Fei Hao and Jie Gao and Yaguang Lin and Yulei Wu and Jiaxing Shang},
  doi          = {10.1109/TETC.2023.3315335},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {715-726},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Concept stability entropy: A novel group cohesion measure in social networks},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ARIS: Efficient admitted influence maximizing in large-scale
networks based on valid path reverse influence sampling. <em>TETC</em>,
<em>12</em>(3), 700–714. (<a
href="https://doi.org/10.1109/TETC.2022.3230933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization problem has been extensively studied in recent years. It aims at finding a seed set consisting of $k$ vertices from a network, so that their collective influence spread can be maximized. However, existing works have neglected the user admission behavior where the successfully influenced individuals do not always disseminate the messages to their neighbors, and we show that ignoring admission behavior may incur significant bias to existing methods. Based on this observation, a new admitted influence maximization (AIM) problem is proposed by taking the admission behavior into consideration. Specifically, we first propose two new diffusion models, i.e., AIC and ALT, by extending traditional independent cascade and linear threshold models. Then, a novel edge coloring scheme is proposed to theoretically prove the NP-hardness of the AIM problem and the submodularity of its objective function. Based on the theoretical findings, a greedy algorithm is given to find a $(1-1/e-\epsilon)$ approximate solution. To further improve the algorithm efficiency and handle large-scale networks, an efficient algorithm ARIS based on valid path reverse influence sampling is proposed, which could also ensure a $(1-1/e-\epsilon)$ approximate solution. Experimental results on several large-scale real networks exhibit the effectiveness and efficiency of our proposed algorithm.},
  archive      = {J_TETC},
  author       = {Xiaojuan Yang and Jiaxing Shang and Qinghong Hu and Dajiang Liu},
  doi          = {10.1109/TETC.2022.3230933},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {700-714},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {ARIS: Efficient admitted influence maximizing in large-scale networks based on valid path reverse influence sampling},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Affective region recognition and fusion network for
target-level multimodal sentiment classification. <em>TETC</em>,
<em>12</em>(3), 688–699. (<a
href="https://doi.org/10.1109/TETC.2022.3231746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of multimodal sentiment analysis tasks, target-level/aspect-level multimodal sentiment analysis has received more attention, aiming to intelligently judge the sentiment orientation of target words using visual and textual information. Most existing methods mainly rely on combining the whole image and text while ignoring the implicit affective regions in the image. We introduce a novel affective region recognition and fusion network (ARFN) for target-level multimodal sentiment classification, which focuses more on the alignment of multimodal fusion of visual and textual. First, to produce a visual representation with sentiment elements, ARFN employs the Yolov5 algorithm to extract the object region of the image and selects the emotional area according to the strategy. Next, this method learns target-sensitive visual representations and text semantic representations through a multi-head attention mechanism and pre-trained models BERT, respectively. Moreover, ARFN fuses textual and visual representations through a multimodal interaction method to perform target-level multimodal sentiment classification tasks. We achieve state-of-the-art performance on two available multimodal Twitter datasets, and experimental results show the effectiveness of our approach.},
  archive      = {J_TETC},
  author       = {Li Jia and Tinghuai Ma and Huan Rong and Najla Al-Nabhan},
  doi          = {10.1109/TETC.2022.3231746},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {688-699},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Affective region recognition and fusion network for target-level multimodal sentiment classification},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Special section on emerging social computing. <em>TETC</em>,
<em>12</em>(3), 686–687. (<a
href="https://doi.org/10.1109/TETC.2024.3447428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TETC},
  author       = {Yuan-Hao Chang and Paloma Díaz and Yunpeng Xiao},
  doi          = {10.1109/TETC.2024.3447428},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {686-687},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Special section on emerging social computing},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multiplier-free RNS-based CNN accelerator exploiting
bit-level sparsity. <em>TETC</em>, <em>12</em>(2), 667–683. (<a
href="https://doi.org/10.1109/TETC.2023.3301590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, a Residue Numbering System (RNS)-based Convolutional Neural Network (CNN) accelerator utilizing a multiplier-free distributed-arithmetic Processing Element (PE) is proposed. A method for maximizing the utilization of the arithmetic hardware resources is presented. It leads to an increase of the system&#39;s throughput, by exploiting bit-level sparsity within the weight vectors. The proposed PE design takes advantage of the properties of RNS and Canonical Signed Digit (CSD) encoding to achieve higher energy efficiency and effective processing rate, without requiring any compression mechanism or introducing any approximation. An extensive design space exploration for various parameters (RNS base, PE micro-architecture, encoding) using analytical models as well as experimental results from CNN benchmarks is conducted and the various trade-offs are analyzed. A complete end-to-end RNS accelerator is developed based on the proposed PE. The introduced accelerator is compared to traditional binary and RNS counterparts as well as to other state-of-the-art systems. Implementation results in a 22-nm process show that the proposed PE can lead to $1.85\times$ and $1.54\times$ more energy-efficient processing compared to binary and conventional RNS, respectively, with a $1.88\times$ maximum increase of effective throughput for the employed benchmarks. Compared to a state-of-the-art, all-digital, RNS-based system, the proposed accelerator is $8.87\times$ and $1.11\times$ more energy- and area-efficient, respectively.},
  archive      = {J_TETC},
  author       = {Vasilis Sakellariou and Vassilis Paliouras and Ioannis Kouretas and Hani Saleh and Thanos Stouraitis},
  doi          = {10.1109/TETC.2023.3301590},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {667-683},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A multiplier-free RNS-based CNN accelerator exploiting bit-level sparsity},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Error in ulps of the multiplication or division by a
correctly-rounded function or constant in binary floating-point
arithmetic. <em>TETC</em>, <em>12</em>(2), 656–666. (<a
href="https://doi.org/10.1109/TETC.2023.3294986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assume we use a binary floating-point arithmetic and that $\operatorname{RN}$ is the round-to-nearest function. Also assume that $c$ is a constant or a real function of one or more variables, and that we have at our disposal a correctly rounded implementation of $c$ , say $\hat{c}= \operatorname{RN}(c)$ . For evaluating $x \cdot c$ (resp. $ x / c$ or $c / x$ ), the natural way is to replace it by $\operatorname{RN}(x \cdot \hat{c})$ (resp. $ \operatorname{RN}(x / \hat{c})$ or $\operatorname{RN}(\hat{c}/ x)$ ), that is, to call function $\hat{c}$ and to perform a floating-point multiplication or division. This can be generalized to the approximation of $n/d$ by $\operatorname{RN}(\hat{n}/\hat{d})$ and the approximation of $n \cdot d$ by $\operatorname{RN}(\hat{n} \cdot \hat{d})$ , where $\hat{n} = \operatorname{RN}(n)$ and $\hat{d} = \operatorname{RN}(d)$ , and $n$ and $d$ are functions for which we have at our disposal a correctly rounded implementation. We discuss tight error bounds in ulps of such approximations. From our results, one immediately obtains tight error bounds for calculations such as $\mathtt {x * pi}$ , $\mathtt {ln(2)/x}$ , $\mathtt {x/(y+z)}$ , $\mathtt {(x+y)*z}$ , $\mathtt {x/sqrt(y)}$ , $\mathtt {sqrt(x)/{y}}$ , $\mathtt {(x+y)(z+t)}$ , $\mathtt {(x+y)/(z+t)}$ , $\mathtt {(x+y)/(zt)}$ , etc. in floating-point arithmetic.},
  archive      = {J_TETC},
  author       = {Nicolas Brisebarre and Jean-Michel Muller and Joris Picot},
  doi          = {10.1109/TETC.2023.3294986},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {656-666},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Error in ulps of the multiplication or division by a correctly-rounded function or constant in binary floating-point arithmetic},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Strategic honeypot deployment in ultra-dense beyond 5G
networks: A reinforcement learning approach. <em>TETC</em>,
<em>12</em>(2), 643–655. (<a
href="https://doi.org/10.1109/TETC.2022.3184112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The progression of Software Defined Networking (SDN) and the virtualisation technologies lead to the beyond 5G era, providing multiple benefits in the smart economies. However, despite the advantages, security issues still remain. In particular, SDN/NFV and cloud/edge computing are related to various security issues. Moreover, due to the wireless nature of the entities, they are prone to a wide range of cyberthreats. Therefore, the presence of appropriate intrusion detection mechanisms is critical. Although both Machine Learning (ML) and Deep Learning (DL) have optimised the typical rule-based detection systems, the use of ML and DL requires labelled pre-existing datasets. However, this kind of data varies based on the nature of the respective environment. Another smart solution for detecting intrusions is to use honeypots. A honeypot acts as a decoy with the goal to mislead the cyberatatcker and protect the real assets. In this paper, we focus on Wireless Honeypots (WHs) in ultra-dense networks. In particular, we introduce a strategic honeypot deployment method, using two Reinforcement Learning (RL) techniques: (a) $e-Greedy$ and (b) $Q-Learning$ . Both methods aim to identify the optimal number of honeypots that can be deployed for protecting the actual entities. The experimental results demonstrate the efficacy of both methods.},
  archive      = {J_TETC},
  author       = {Panagiotis Radoglou-Grammatikis and Panagiotis Sarigiannidis and Panagiotis Diamantoulakis and Thomas Lagkas and Theocharis Saoulidis and Eleftherios Fountoukidis and George Karagiannidis},
  doi          = {10.1109/TETC.2022.3184112},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {643-655},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Strategic honeypot deployment in ultra-dense beyond 5G networks: A reinforcement learning approach},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Designing secure and resilient cyber-physical systems: A
model-based moving target defense approach. <em>TETC</em>,
<em>12</em>(2), 631–642. (<a
href="https://doi.org/10.1109/TETC.2022.3197464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber-physical systems (CPSs) rely upon the deep integration of computation and physical processes/systems, enabled by Internet of Things (IoT), edge computing, and cloud technologies. Noticeably, cybersecurity is a major concern in CPSs, since attacks may exploit both cyber and physical vulnerabilities and damage significantly physical equipment, compromise operational safety, and impact negatively on product quality and performance. In this context, CPS design should take both security and resilience requirements into account, by identifying the needed measures not only to prevent but also to withstand, recover from, and adapt to adverse conditions and attacks. The approach proposed in this article aims at improving the security and resilience of a CPS deployment through a model-based design methodology leveraging security-by-design principles and Moving Target Defense (MTD) techniques, consisting in continually shifting a system configuration to reduce the attack success probability and survive attacks. Our methodology, in particular, is meant to support the threat modeling process of a CPS and the identification, based on spotted threats and on the properties of involved assets and data, of the security controls to include within the design to mitigate existing threats and of the MTD techniques to integrate in order to increase resilience.},
  archive      = {J_TETC},
  author       = {Valentina Casola and Alessandra De Benedictis and Carlo Mazzocca and Rebecca Montanari},
  doi          = {10.1109/TETC.2022.3197464},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {631-642},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Designing secure and resilient cyber-physical systems: A model-based moving target defense approach},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intent-based security for functional safety in
cyber-physical systems. <em>TETC</em>, <em>12</em>(2), 615–630. (<a
href="https://doi.org/10.1109/TETC.2023.3251031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel intent-based method to prevent security attacks on the safety functions in cyber-physical systems used in smart manufacturing. Context information about a cyber-physical system is collected from various sensors including non-safety sensors measuring device temperature, motor rotation speed, or instantaneous power consumption of machines. Such contextual information along with operational and business intent of the system under consideration are then used to check whether the current situation is indeed an emergency situation or a normal situation. Unlike the conventional safety systems that only rely on raw sensor data and safety protocol status packets from safety sensors, which might be spoofed and/or modified, decision on the safety situation in our method is intelligently made by comparing aggregated sensor information from the cyber-physical system and its environment for compliance with pre-configured operational intents that define the normal safe and secure operation of the system. We also show how to integrate Machine Learning (ML) and Artificial Intelligence (AI) into the proposed method for efficient and automated analysis of both intents and aggregated context information to make more intelligent decisions in execution of functional safety protocols. Our proposed AI/ML integration approach also enables the prediction of safety critical situations before they occur. The proposed method aims to prevent unnecessary switching to fail-safe mode causing insecure system states such as emergency doors opening or system halting in normal situations. In addition, it prevents sticking in non-safe states and not switching to fail-safe mode in real emergencies which could cause hazard on device and/or people.},
  archive      = {J_TETC},
  author       = {Emrah Tomur and Zeki Bilgin and Utku Gülen and Elif Ustundag Soykan and Leyli Karaçay and Ferhat Karakoç},
  doi          = {10.1109/TETC.2023.3251031},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {615-630},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Intent-based security for functional safety in cyber-physical systems},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards trustworthy autonomous systems: Taxonomies and
future perspectives. <em>TETC</em>, <em>12</em>(2), 601–614. (<a
href="https://doi.org/10.1109/TETC.2022.3227113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The class of Trustworthy Autonomous Systems (TAS) includes cyber-physical systems leveraging on self-x technologies that make them capable to learn, adapt to changes, and reason under uncertainties in possibly critical applications and evolving environments. In the last decade, there has been a growing interest in enabling artificial intelligence technologies, such as advanced machine learning, new threats, such as adversarial attacks, and certification challenges, due to the lack of sufficient explainability. However, in order to be trustworthy, those systems also need to be dependable, secure, and resilient according to well-established taxonomies, methodologies, and tools. Therefore, several aspects need to be addressed for TAS, ranging from proper taxonomic classification to the identification of research opportunities and challenges. Given such a context, in this paper address relevant taxonomies and research perspectives in the field of TAS. We start from basic definitions and move towards future perspectives, regulations, and emerging technologies supporting development and operation of TAS.},
  archive      = {J_TETC},
  author       = {Francesco Flammini and Cristina Alcaraz and Emanuele Bellini and Stefano Marrone and Javier Lopez and Andrea Bondavalli},
  doi          = {10.1109/TETC.2022.3227113},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {601-614},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Towards trustworthy autonomous systems: Taxonomies and future perspectives},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cyber resilience for the internet of things: Implementations
with resilience engines and attack classifications. <em>TETC</em>,
<em>12</em>(2), 583–600. (<a
href="https://doi.org/10.1109/TETC.2022.3231692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the number of publicized attacks on IoT devices has noticeably grown. This is in part due to the increasing deployment of embedded systems into various domains, including critical infrastructure, which makes them a valuable asset and a compromise can cause significant damages. In this case, it is often required to send an engineer to manually recover the devices, as the attack leaves them out of reach of standard remote management solutions. To avoid this costly process, the concept of cyber resilience has gained traction in recent years in both academia and industry. Its core idea is to enable compromised devices to recover themselves to a trusted state without human intervention. Initial guidelines and architectures to realize cyber resilience have been published by standardization entities like NIST and TCG, and in multiple academic article. While the initial works focused on guaranteed recovery, recent proposals included attack detection to speed up the recovery process. In this work, we build on top of these ideas and present an extended resilience architecture. We present new implementations of resilience engines with a focus on secure and reliable data acquisition for attack detection and classification. Our attack classification engine enables tailored, more efficient recovery responses.},
  archive      = {J_TETC},
  author       = {Eduardo Alvarenga and Jan R. Brands and Peter Doliwa and Jerry den Hartog and Erik Kraft and Marcel Medwed and Ventzislav Nikov and Joost Renes and Martin Rosso and Tobias Schneider and Nikita Veshchikov},
  doi          = {10.1109/TETC.2022.3231692},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {583-600},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Cyber resilience for the internet of things: Implementations with resilience engines and attack classifications},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Medical systems data security and biometric authentication
in public cloud servers. <em>TETC</em>, <em>12</em>(2), 572–582. (<a
href="https://doi.org/10.1109/TETC.2023.3271957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in distributed computing and virtualization allowed cloud computing to establish itself as a popular data management and storage option for organizations. However, unclear safeguards, practices, as well as the evolution of legislation around privacy and data protection, contribute to data security being one of the main concerns in adopting this paradigm. Another important aspect hindering the absolute success of cloud computing is the ability to ensure the digital identity of users and protect the virtual environment through logical access controls while avoiding the compromise of its authentication mechanism or storage medium. Therefore, this paper proposes a system that addresses data security wherein unauthorized access to data stored in a public cloud is prevented by applying a fragmentation technique and a NoSQL database. Moreover, a system for managing and authenticating users with multimodal biometrics is also suggested along with a mechanism to ensure the protection of biometric features. When compared with encryption, the proposed fragmentation method indicates better latency performance, highlighting its strong potential use-case in environments with lower latency requirements such as the healthcare IT infrastructure.},
  archive      = {J_TETC},
  author       = {Nelson Santos and Bogdan Ghita and Giovanni Luca Masala},
  doi          = {10.1109/TETC.2023.3271957},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {572-582},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Medical systems data security and biometric authentication in public cloud servers},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic reconfiguration for resilient state estimation
against cyber attacks. <em>TETC</em>, <em>12</em>(2), 559–571. (<a
href="https://doi.org/10.1109/TETC.2023.3266303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing complexity and connectivity of critical infrastructures makes it increasingly likely that they will be subject to malicious attacks that compromise operation. Recent studies have shown that these systems are vulnerable to a wide range of cyber-attacks, including False Data Injection (FDI) attacks that bypasses conventional detection techniques. Conventional security monitoring, protection, and control tools are based primarily on passive defense strategies. In this paper, we propose an approach for active defense that improves system security and the detection rate of FDI attacks. The key insight for this approach is that emerging micro-grids can utilize distributed energy resources to dynamically reconfigure the system (e.g., power flow paths), across multiple acceptable configurations. Instead of using information from only a single static configuration to detect FDI attacks, our proposed approach uses dynamic reconfiguration to compare measured and estimated states under multiple configurations to accurately detect FDI attacks. We describe an implementation and supporting infrastructure for a secure reconfigurable microgrid. Dynamic reconfiguration also makes it more difficult for attackers to gain knowledge on the system&#39;s parameters, which increases the difficulty for attackers to construct hidden attack vectors. We evaluate our approach in the specific scenario of emerging micro-grids. We develop a novel technique for state estimation using multiple configurations and demonstrate that this approach significantly improves FDI detection accuracy.},
  archive      = {J_TETC},
  author       = {Joseph Callenes and Majid Poshtan},
  doi          = {10.1109/TETC.2023.3266303},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {559-571},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Dynamic reconfiguration for resilient state estimation against cyber attacks},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guest editorial navigating the nexus of cyber security and
resilience. <em>TETC</em>, <em>12</em>(2), 558. (<a
href="https://doi.org/10.1109/TETC.2024.3402450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Welcome to this special issue of the IEEE Transactions on Emerging Topics in Computing, dedicated to exploring the dynamic landscape of Cyber Security and Resilience. In an era where digital advancements are driving unprecedented connectivity and innovation, the imperative for robust cyber defenses and resilient systems has never been more pressing.},
  archive      = {J_TETC},
  author       = {Francesco Flammini and Cristina Alcaraz},
  doi          = {10.1109/TETC.2024.3402450},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {558},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Guest editorial navigating the nexus of cyber security and resilience},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). I choose you: Automated hyperparameter tuning for deep
learning-based side-channel analysis. <em>TETC</em>, <em>12</em>(2),
546–557. (<a href="https://doi.org/10.1109/TETC.2022.3218372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, the deep learning-based side-channel analysis represents a widely researched topic, with numerous results indicating the advantages of such an approach. Indeed, breaking protected implementations while not requiring complex feature selection made deep learning a preferred option for profiling side-channel analysis. Still, this does not mean it is trivial to mount a successful deep learning-based side-channel analysis. One of the biggest challenges is to find optimal hyperparameters for neural networks resulting in powerful side-channel attacks. This work proposes an automated way for deep learning hyperparameter tuning based on Bayesian optimization. We build a custom framework denoted AutoSCA supporting machine learning and side-channel metrics. Our experimental analysis shows that our framework performs well regardless of the dataset, leakage model, or neural network type. We find several neural network architectures outperforming state-of-the-art attacks. Finally, while not considered a powerful option, we observe that neural networks obtained via random search can perform well, indicating that the publicly available datasets are relatively easy to break.},
  archive      = {J_TETC},
  author       = {Lichao Wu and Guilherme Perin and Stjepan Picek},
  doi          = {10.1109/TETC.2022.3218372},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {546-557},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {I choose you: Automated hyperparameter tuning for deep learning-based side-channel analysis},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PowerScout: Security-oriented power delivery network
modeling for side-channel vulnerability analysis. <em>TETC</em>,
<em>12</em>(2), 532–545. (<a
href="https://doi.org/10.1109/TETC.2023.3257826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing complexity of modern electronic systems leads to the design of more sophisticated power delivery networks (PDNs). Similar to other system-level shared hardware resources, the on-board PDN unintentionally introduces side channels across design layers and voltage domains which are not explicitly specified in the functional specification. Recent works have demonstrated that the exploitation of the side channel can compromise the system security such as information leakage and fault injection. In this work, we systematically investigate the PDN-based side channel as well as potential countermeasures. To facilitate this goal, we develop PowerScout, a security-oriented PDN simulation framework that unifies the modeling of different PDN-based side-channel attacks. PowerScout performs a fast nodal analysis of complex PDNs at the system level to quantitatively evaluate the severity of side-channel vulnerabilities. With the support of PowerScout, for the first time, we validate PDN side-channel attacks in the literature via simulation. Furthermore, we are able to quantitatively measure the security impact of PDN parameters and configurations. For example, towards information leakage, removing near-chip capacitors can increase intra-chip information leakage by a maximum of 23.23 dB at mid-frequency range and inter-chip leakage by an average of 31.68 dB at mid- and high-frequency range. Similarly, the optimal toggling frequency and duty cycle are derived to achieve fault injection attacks with higher success rate and more precise control. In addition, the vulnerabilities are evaluated when hiding-based countermeasures are implemented. Based on the evaluation, we can understand the optimal defense configuration and explore the trade-off between information leakage mitigation and power supply stability.},
  archive      = {J_TETC},
  author       = {Huifeng Zhu and Xiaolong Guo and Yier Jin and Xuan Zhang},
  doi          = {10.1109/TETC.2023.3257826},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {532-545},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {PowerScout: Security-oriented power delivery network modeling for side-channel vulnerability analysis},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CyFence: Securing cyber-physical controllers via trusted
execution environment. <em>TETC</em>, <em>12</em>(2), 521–531. (<a
href="https://doi.org/10.1109/TETC.2023.3268412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decades, Cyber-physical Systems (CPSs) have experienced a significant technological evolution and increased connectivity, at the cost of greater exposure to cyber-attacks. Since many CPS are used in safety-critical systems, such attacks entail high risks and potential safety harms. Although several defense strategies have been proposed, they rarely exploit the cyber-physical nature of the system. In this work, we exploit the nature of CPS by proposing CyFence, a novel architecture that improves the resilience of closed-loop control systems against cyber-attacks by adding a semantic check, used to confirm that the system is behaving as expected. To ensure the security of the semantic check code, we use the Trusted Execution Environment implemented by modern processors. We evaluate CyFence considering a real-world application, consisting of an active braking digital controller, demonstrating that it can mitigate different types of attacks with a negligible computation overhead.},
  archive      = {J_TETC},
  author       = {Stefano Longari and Alessandro Pozone and Jessica Leoni and Mario Polino and Michele Carminati and Mara Tanelli and Stefano Zanero},
  doi          = {10.1109/TETC.2023.3268412},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {521-531},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {CyFence: Securing cyber-physical controllers via trusted execution environment},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SenseHash: Computing on sensor values mystified at the
origin. <em>TETC</em>, <em>12</em>(2), 508–520. (<a
href="https://doi.org/10.1109/TETC.2022.3217488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose SenseHash, a novel design for the lightweight in-hardware mystification of the sensed data at the origin. The framework aims to ensure the privacy of sensitive sensor values while preserving their utility. The sensors are assumed to interface to various (potentially malicious) communication and computing components in the Internet-of-things (IoT) and other emerging pervasive computing scenarios. The primary security primitives of our work are Locality Sensitive Hashing (LSH) combined with Differential Privacy (DP) and secure construction of LSH. Our construction allows (i) sub-linear search in sensor readings while ensuring their security against triangulation attack, and (ii) differentially private statistics of the readings. SenseHash includes hardware architecture as well as accompanying protocols to efficiently utilize the secure readings in practical scenarios. Alongside these scenarios, we present an automated workflow to generalize the application of the mystified readings. Proof-of-concept FPGA implementation of the system demonstrates its practicability and low overhead in terms of hardware resources, energy consumption, and protocol execution time.},
  archive      = {J_TETC},
  author       = {Nojan Sheybani and Xinqiao Zhang and Siam Umar Hussain and Farinaz Koushanfar},
  doi          = {10.1109/TETC.2022.3217488},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {508-520},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {SenseHash: Computing on sensor values mystified at the origin},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gate-level side-channel leakage ranking with architecture
correlation analysis. <em>TETC</em>, <em>12</em>(2), 496–507. (<a
href="https://doi.org/10.1109/TETC.2023.3268303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While side-channel leakage is traditionally evaluated from a fabricated chip, it is more time-efficient and cost-effective to do so during the design phase of the chip. We present a methodology to rank the gates of a design according to their contribution to the side-channel leakage of the chip. The methodology relies on logic synthesis, logic simulation, gate-level power estimation, and gate leakage assessment to compute a ranking. The ranking metric can be defined as a specific test by correlating gate-level activity with a leakage model, or else as a non-specific test by evaluating gate-level activity in response to distinct test vector groups. Our results show that only a minority of the gates in a design contribute most of the side-channel leakage. We demonstrate this property for several designs, including a hardware AES coprocessor and a cryptographic hardware/software interface in a five-stage pipelined RISC processor.},
  archive      = {J_TETC},
  author       = {Pantea Kiaei and Yuan Yao and Zhenyuan Liu and Nicole Fern and Cees-Bart Breunesse and Jasper Van Woudenberg and Kate Gillis and Alex Dich and Peter Grossmann and Patrick Schaumont},
  doi          = {10.1109/TETC.2023.3268303},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {496-507},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Gate-level side-channel leakage ranking with architecture correlation analysis},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Digital contact tracing solutions: Promises, pitfalls and
challenges. <em>TETC</em>, <em>12</em>(2), 483–495. (<a
href="https://doi.org/10.1109/TETC.2022.3216473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic has caused many countries to deploy novel digital contact tracing (DCT) systems to boost the efficiency of manual tracing of infection chains. In this paper, we systematically analyze DCT solutions and categorize them based on their design approaches and architectures. We analyze them with regard to effectiveness, security, privacy and ethical aspects and compare prominent solutions based on these requirements. In particular, we discuss shortcomings of the Google and Apple Exposure Notification API (GAEN) that is currently widely adopted all over the world. We find that the security and privacy of GAEN has considerable deficiencies as it can be compromised by severe large-scale attacks. We also discuss other proposed approaches for contact tracing, including our proposal TraceCORONA , that are based on Diffie-Hellman (DH) key exchange and aim at tackling shortcomings of existing solutions. Our extensive analysis shows that TraceCORONA fulfills the above security requirements better than deployed state-of-the-art approaches. We have implemented TraceCORONA and its beta test version has been used by more than 2000 users without any major functional problems, 1 demonstrating that there are no technical reasons requiring to make compromises with regard to the requirements of DCT approaches.},
  archive      = {J_TETC},
  author       = {Thien Duc Nguyen and Markus Miettinen and Alexandra Dmitrienko and Ahmad-Reza Sadeghi and Ivan Visconti},
  doi          = {10.1109/TETC.2022.3216473},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {483-495},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Digital contact tracing solutions: Promises, pitfalls and challenges},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Special section on emerging topics in hardware computing
systems security. <em>TETC</em>, <em>12</em>(2), 482. (<a
href="https://doi.org/10.1109/TETC.2024.3394668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gang Qu received the BS degree in mathematics from the University of Science and Technology of China (USTC), China, and the PhD degree in computer science from the University of California, Los Angeles (UCLA), USA. He is currently a professor with the Department of Electrical and Computer Engineering, University of Maryland, College Park, MD, USA, where he leads the Maryland Embedded Systems and Hardware Security Lab (MeshSec Lab) and the Wireless Sensor Laboratory. His research interests include hardware security and trust, artificial intelligence, security in vehicular systems, and the Internet of Things. He is also known for his work on wireless sensor networks, low power and energy efficient embedded system design.},
  archive      = {J_TETC},
  author       = {Gang Qu and Debdeep Mukhopadhyay and Nele Mentens and Weiqiang Liu},
  doi          = {10.1109/TETC.2024.3394668},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {482},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Special section on emerging topics in hardware computing systems security},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep attentive interest collaborative filtering for
recommender systems. <em>TETC</em>, <em>12</em>(2), 467–481. (<a
href="https://doi.org/10.1109/TETC.2023.3286404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative filtering (CF) is a pivotal building block in commercial recommender systems due to its strength and utility in user interest modeling. Recently, many researchers have turned to deep learning as a way to capture richer collaborative signals from user-item feature interactions. However, most deep-based methods only consider nonlinear, high-order interactions while ignoring the explicit collaborative signals in low-order interactions. They also typically ignore the quality of the user and item profiles. These are cornerstones in item recommendation that, we argue, must be considered for high-quality recommendations. Hence, we propose Deep Attentive Interest Collaborative Filtering (DAICF) to overcome these limitations. DAICF profiles users based on their interactive items, i.e., user neighborhood information. Similarly, item profiles are based on users who had interacted with it, i.e., item neighborhood information. Given a user&#39;s profile varies over different items, DAICF accurately models his attentive interests based on the specific target item. Low-order collaborative signals are captured by a shallow component, and high-order collaborative signals are captured by a deep component. These two complementary collaborative signals are then fused to provide rich recommendations that cut through today&#39;s information overload. By designing a personalized feature extraction method based on bilateral neighborhood information to solve the data sparsity problem in recommender systems, DAICF can dynamically distinguish the importance of a user&#39;s historical interaction items for predicting user preferences for a specific target item. A set of experiments against four real-world datasets validate that DAICF outperforms the most recent state-of-the-art recommendation algorithms and justifies the effectiveness and interpretability of our method.},
  archive      = {J_TETC},
  author       = {Libing Wu and Youhua Xia and Shuwen Min and Zhenchang Xia},
  doi          = {10.1109/TETC.2023.3286404},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {467-481},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Deep attentive interest collaborative filtering for recommender systems},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LocalTGEP: A lightweight edge partitioner for time-varying
graph. <em>TETC</em>, <em>12</em>(2), 455–466. (<a
href="https://doi.org/10.1109/TETC.2023.3238333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph edge partitioning (GEP), the allocation of edges into different parts through cut vertices, is essential for the analytics of large-scale graphs. Most GEP models cannot be directly applied to a time-varying graph unless repartitioning the entire graph, which leads to a large consumption of resources. Although a few studies have focused on time-varying graph edge partitioning, they have ignored the memory consumption during the partitioning process. Therefore, a lightweight edge partitioner, referred to as LocalTGEP, broadening the application to time-varying graphs, is proposed herein. Three superiorities of LocalTGEP are highlighted as follows: 1) A satisfactory partitioning quality for a time-varying graph can be achieved without requiring global information owing to the local edge partitioning. 2) Memory consumption of the partitioner is significantly reduced using a novel storage framework of graph data in LocalTGEP. 3) The quality and efficiency of time-varying graph edge partitioning are optimized by designing the push and pop stages in LocalTGEP. Extensive experimental results obtained on 12 real-world graphs demonstrate that LocalTGEP outperforms rival algorithms in terms of memory consumption, partitioning quality, and efficiency.},
  archive      = {J_TETC},
  author       = {Shengwei Ji and Chenyang Bu and Lei Li and Xindong Wu},
  doi          = {10.1109/TETC.2023.3238333},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {455-466},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {LocalTGEP: A lightweight edge partitioner for time-varying graph},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CommGNAS: Unsupervised graph neural architecture search for
community detection. <em>TETC</em>, <em>12</em>(2), 444–454. (<a
href="https://doi.org/10.1109/TETC.2023.3270181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural architecture search (GNAS) has been successful in many supervised learning tasks, such as node classification, graph classification, and link prediction. GNAS uses a search algorithm to sample graph neural network (GNN) architectures from the search space and evaluates sampled GNN architectures based on estimation strategies to generate feedback for the search algorithm. In traditional GNAS, the typical estimation strategy requires using labeled graph data to generate feedback, which plays a fundamental and vital role in the search algorithm to sample a better GNN architecture during the search process. However, a large portion of real-world graph data is unlabeled. The estimation strategy in traditional GNAS cannot use unlabeled graph data to generate feedback for the search algorithm, so the traditional supervised GNAS fails to solve unsupervised problems, such as community detection tasks. To solve this challenge, this article proposed CommGNAS, an effective node representation learning method with unsupervised graph neural architecture search for community detection. In CommGNAS, we design an unsupervised evaluation strategy with self-supervised and self-representation learning. It represents the first research work in literature to solve the problems of unsupervised graph neural architecture search for community detection. The experimental results show that CommGNAS can obtain the best performance in community detection tasks on real-world graphs against the state-of-the-art baseline methods.},
  archive      = {J_TETC},
  author       = {Jianliang Gao and Jiamin Chen and Babatounde Moctard Oloulade and Raeed Al-Sabri and Tengfei Lyu and Ji Zhang and Zhao Li},
  doi          = {10.1109/TETC.2023.3270181},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {444-454},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {CommGNAS: Unsupervised graph neural architecture search for community detection},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sepformer-based models: More efficient models for long
sequence time-series forecasting. <em>TETC</em>, <em>12</em>(2),
432–443. (<a href="https://doi.org/10.1109/TETC.2022.3230920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting long sequence time series plays a crucial role in many applications such as anomaly detection and financial predictions. Achieving consistently good results requires a model that can precisely capture the long-range dependencies in input sequences. And very few current models can meet the requirements. Informer has recently demonstrated state-of-the-art accuracy in LSTF. Yet several other aspects of its performance leave much room for improvement. These include: 1) complexity - Informer has a relatively high computational complexity and a high memory overhead; 2) nuance - there is limited ability to capture the subtle features in a data stream; 3) interpretability - the inference procedure of Informer is not explainable; 4) extensibility - accuracy is poor with extra-long multivariate time series. To address these issues, we propose a suite of models under the banner Sepformer. The set comprises Sepformer and two variants SWformer and Mini-SWformer. Sepformer uses separate networks to extract data stream features in parallel. SWformer and Mini-SWformer dramatically separate high-frequency and low-frequency components to process the data stream and reduce the requirement for GPU memory by adopting a discrete wavelet transform. Extensive experiments show that the Sepformer models substantially outperform state-of-the-art methods in terms of accuracy, computational complexity and usage of GPU memory use.},
  archive      = {J_TETC},
  author       = {Jin Fan and Zehao Wang and Danfeng Sun and Huifeng Wu},
  doi          = {10.1109/TETC.2022.3230920},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {432-443},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Sepformer-based models: More efficient models for long sequence time-series forecasting},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Maximizing social influence with minimum information
alteration. <em>TETC</em>, <em>12</em>(2), 419–431. (<a
href="https://doi.org/10.1109/TETC.2023.3292384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of the Internet and social platforms, how to maximize the influence across popular online social networks has attracted great attention from both researchers and practitioners. Almost all the existing influence diffusion models assume that influence remains constant in the process of information spreading. However, in the real world, people tend to alternate information by attaching opinions or modifying the contents before spreading it. Namely, the meaning and idea of a message normally mutate in the process of influence diffusion. In this article, we investigate how to maximize the influence in online social platforms with a key consideration of suppressing the information alteration in the diffusion cascading process. We leverage deep learning models and knowledge graphs to present users’ personalised behaviours, i.e., actions after receiving a message. Furthermore, we investigate the information alteration in the process of influence diffusion. A novel seed selection algorithm is proposed to maximize the social influence without causing significant information alteration. Experimental results explicitly show the rationale of the proposed user behaviours deep learning model architecture and demonstrate the novel seeding algorithm&#39;s outstanding performance in both maximizing influence and retaining the influence originality.},
  archive      = {J_TETC},
  author       = {Guan Wang and Weihua Li and Quan Bai and Edmund M-K Lai},
  doi          = {10.1109/TETC.2023.3292384},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {419-431},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Maximizing social influence with minimum information alteration},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal dual-attributed network generation oriented
community detection model. <em>TETC</em>, <em>12</em>(2), 403–418. (<a
href="https://doi.org/10.1109/TETC.2022.3223058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection is a crucial task in the research field of network analysis. However, this task recently has become challenging due to the explosion of network in terms of the scale and the side information, e.g., temporal information and attribute information. Here we propose PGMTAN —a probabilistic generative model for overlapping community detection on temporal dual-attributed networks. PGMTAN aims to characterize four generation processes: 1) generation of occurrence of the links, 2) generation of node-community memberships via assortative attributes, 3) generation of generative attributes, and 4) generation of evolutionary dynamics of community structure. Particularly, we adopt a hidden Markov chain model to capture the network&#39;s dynamics on the evolution of community structure over time. Moreover, we seek to optimize a lower-bound of likelihood function to accelerate the model&#39;s parameter estimation. We carry out extensive experiments on several real-world and synthetic networks to test PGMTAN &#39;s performance and the results substantiate that it can outperform multiple baselines and give us promising performance in terms of detection accuracy and convergence.},
  archive      = {J_TETC},
  author       = {Yuyao Wang and Jie Cao and Zhan Bu and Mingming Leng},
  doi          = {10.1109/TETC.2022.3223058},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {403-418},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Temporal dual-attributed network generation oriented community detection model},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Special section on community detection in time-varying
information and computing networks: Theory, models, and applications.
<em>TETC</em>, <em>12</em>(2), 402. (<a
href="https://doi.org/10.1109/TETC.2024.3395072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Jia Wu received the PhD degree in computer science from the University of Technology Sydney, Ultimo, NSW, Australia. He is currently an ARC DECRA fellow with the Department of Computing, Macquarie University, Sydney, Australia. Prior to that, he was with the center for Artificial Intelligence, University of Technology Sydney. His current research interests include data mining and machine learning.},
  archive      = {J_TETC},
  author       = {Jia Wu and Jian Yang and Philip S. Yu and Carlo Condo},
  doi          = {10.1109/TETC.2024.3395072},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4},
  number       = {2},
  pages        = {402},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Special section on community detection in time-varying information and computing networks: Theory, models, and applications},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Private delegated computations using strong isolation.
<em>TETC</em>, <em>12</em>(1), 386–398. (<a
href="https://doi.org/10.1109/TETC.2023.3281738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computations are now routinely delegated to third-parties. In response, Confidential Computing technologies are being added to microprocessors offering a trusted execution environment ( TEE ) that provides confidentiality and integrity guarantees to code and data hosted within—even in the face of a privileged attacker. TEEs, along with an attestation protocol, permit remote third-parties to establish a trusted “beachhead” containing known code and data on an otherwise untrusted machine. Yet, they introduce many new problems, including: how to ease provisioning of computations safely into TEEs; how to develop distributed systems spanning multiple classes of TEE; and what to do about the billions of “legacy” devices without support for Confidential Computing? Tackling these problems, we introduce Veracruz , a pragmatic framework that eases the design and implementation of complex privacy-preserving, collaborative, delegated computations among a group of mutually mistrusting principals. Veracruz supports multiple isolation technologies and provides a common programming model and attestation protocol across all of them, smoothing deployment of delegated computations over supported technologies. We demonstrate Veracruz in operation, on private in-cloud object detection on encrypted video streaming from a video camera. In addition to supporting hardware-backed TEEs—like AWS Nitro Enclaves and Arm Confidential Computing Architecture Realms—Veracruz also provides pragmatic “software TEEs” on Armv8-A devices without hardware Confidential Computing capability, using the high-assurance seL4 microkernel and our IceCap framework.},
  archive      = {J_TETC},
  author       = {Mathias Brossard and Guilhem Bryant and Basma El Gaabouri and Xinxin Fan and Alexandre Ferreira and Edmund Grimley Evans and Christopher Haster and Evan Johnson and Derek Miller and Fan Mo and Dominic P. Mulligan and Nick Spinale and Eric van Hensbergen and Hugo J. M. Vincent and Shale Xiong},
  doi          = {10.1109/TETC.2023.3281738},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {386-398},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Private delegated computations using strong isolation},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards optimization of privacy-utility trade-off using
similarity and diversity based clustering. <em>TETC</em>,
<em>12</em>(1), 368–385. (<a
href="https://doi.org/10.1109/TETC.2023.3258528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most data owners publish personal data for information consumers, which is used for hidden knowledge discovery. But data publishing in its original form may be subjected to unwanted disclosure of subjects’ identities and their associated sensitive information, and therefore, data is usually anonymized before publication. Many anonymization techniques have been proposed, but most of them often sacrifice utility for privacy, or vice versa, and explicitly disclose sensitive information when original data have skewed distributions. To address these technical problems, we propose a novel anonymization method using similarity and diversity-based clustering that effectively preserves both the subjects’ privacy and anonymous-data utility. We identify influential attributes from the original data using a machine learning algorithm that assists in preserving a subject&#39;s privacy in imbalanced clusters, and that remained unexplored in previous research. The objective function of the clustering process considers both similarity and diversity in the attributes while assigning records to clusters, whereas most of the existing clustering-based anonymity techniques consider either similarity or diversity, thereby sacrificing either privacy or utility. Attribute values in each cluster set are minimally generalized to effectively achieve both competing goals. Extensive experiments were conducted on four real-world benchmark datasets to prove the feasibility of proposed method. The experimental results showed that the common and AI-based privacy risks were reduced by 13.01% and 24.3% respectively in contrast to existing methods. Data utility was augmented by 11.25% and 20.21% on two distinct metrics compared to its counterparts. The complications (e.g., # of iterations) of the clustering process were 2.25× lower than the state-of-the-art methods.},
  archive      = {J_TETC},
  author       = {Abdul Majeed and Safiullah Khan and Seong Oun Hwang},
  doi          = {10.1109/TETC.2023.3258528},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {368-385},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Towards optimization of privacy-utility trade-off using similarity and diversity based clustering},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A chaotic maps-based privacy-preserving distributed deep
learning for incomplete and non-IID datasets. <em>TETC</em>,
<em>12</em>(1), 357–367. (<a
href="https://doi.org/10.1109/TETC.2023.3320758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning is a machine learning approach that enables the training of a deep learning model among several participants with sensitive data that wish to share their own knowledge without compromising the privacy of their data. In this research, the authors employ a secured Federated Learning method with an additional layer of privacy and proposes a method for addressing the non-IID challenge. Moreover, differential privacy is compared with chaotic-based encryption as layer of privacy. The experimental approach assesses the performance of the federated deep learning model with differential privacy using both IID and non-IID data. In each experiment, the Federated Learning process improves the average performance metrics of the deep neural network, even in the case of non-IID data.},
  archive      = {J_TETC},
  author       = {Irina Arévalo and Jose L. Salmeron},
  doi          = {10.1109/TETC.2023.3320758},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {357-367},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A chaotic maps-based privacy-preserving distributed deep learning for incomplete and non-IID datasets},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fast RLWE-based IPFE library and its application to
privacy-preserving biometric authentication. <em>TETC</em>,
<em>12</em>(1), 344–356. (<a
href="https://doi.org/10.1109/TETC.2023.3268003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increased use of data and communication through the internet and the abundant misuse of personal data by many organizations, people are more sensitive about their privacy. Privacy-preserving computation is becoming increasingly important in this era. Functional encryption allows a user to evaluate a function on encrypted data without revealing sensitive information. Most implementations of functional encryption schemes are too time-consuming for practical use. Mera et al. first proposed an inner product functional encryption scheme based on ring learning with errors to improve efficiency. In this work, we optimize the implementation of their work and propose a fast inner product functional encryption library. Specifically, we identify the main performance bottleneck, which is the number theoretic transformation based polynomial multiplication used in the scheme. We also identify the micro and macro level parallel components of the scheme and propose novel techniques to improve the efficiency using open multi-processing and advanced vector extensions 2 vector processor. Compared to the original implementation, our optimization methods translate to 89.72%, 83.06%, 59.30%, and 53.80% improvements in the Setup , Encrypt , KeyGen , and Decrypt operations respectively, in the scheme for standard security level. Designing privacy-preserving applications using functional encryption is ongoing research. Therefore, as an additional contribution to this work, we design a privacy-preserving biometric authentication scheme using inner product functional encryption primitives.},
  archive      = {J_TETC},
  author       = {Supriya Adhikary and Angshuman Karmakar},
  doi          = {10.1109/TETC.2023.3268003},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {344-356},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A fast RLWE-based IPFE library and its application to privacy-preserving biometric authentication},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CuFE: High performance privacy preserving support vector
machine with inner-product functional encryption. <em>TETC</em>,
<em>12</em>(1), 328–343. (<a
href="https://doi.org/10.1109/TETC.2023.3261360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy preservation is a sensitive and important issue in this ever-growing and highly-connected digital era. Functional encryption is a computation on encrypted data paradigm that allows users to retrieve the evaluation of a function on encrypted data without revealing the data, effectively protecting user&#39;s privacy. However, existing functional encryption implementations are still very time-consuming for practical deployment, especially when applied to machine learning applications that involve huge amount of data. In this article, we present a high-performance implementation of inner-product functional encryption (IPFE) based on ring-learning with errors on graphics processing units. We execute a systematic investigation to select the best strategy for implementing number theoretic transform for different security levels, which is the most time-consuming operations in the IPFE scheme. We further propose novel techniques to parallelize the Gaussian sampling. Compared to the existing AVX2 implementation, our implementation on a RTX 2060 GPU achieves $34.24\times$ , $40.02\times$ , $156.30\times$ and $18.76\times$ speed-up for Setup , Encrypt , KeyGen and Decrypt respectively. Finally, we propose a fast privacy-preserving SVM to classify data securely using our GPU-accelerated IPFE scheme. On average, our implementation can classify one input with 591 support vectors in 688 ms ( $&amp;lt; 1$ second), which is $33.12\times$ faster than the AVX2 version.},
  archive      = {J_TETC},
  author       = {Kyung Hyun Han and Wai-Kong Lee and Angshuman Karmakar and Jose Maria Bermudo Mera and Seong Oun Hwang},
  doi          = {10.1109/TETC.2023.3261360},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {328-343},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {CuFE: High performance privacy preserving support vector machine with inner-product functional encryption},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Frequency estimation mechanisms under ϵδ-utility-optimized
local differential privacy. <em>TETC</em>, <em>12</em>(1), 316–327. (<a
href="https://doi.org/10.1109/TETC.2023.3238839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequency estimation mechanisms are widely applied in domains such as machine learning and cloud computing, where it is desirable to provide statistical information. As a fundamental operation in these domains, frequency estimation utilizes personal data which contains sensitive information while it is necessary to protect sensitive information from others. Motivated by this, we preserve user&#39;s privacy with local differential privacy by obfuscating personal data on the user side. In this paper, we propose frequency estimation mechanisms under utility-optimized local differential privacy (ULDP), which allow the data collector to obtain some non-sensitive values to improve data utility while protecting sensitive values from leaking sensitive information. We propose three frequency estimation mechanisms under $(\epsilon,\delta)$ -ULDP (uRFM-GRR, uRFM-RAPPOR, uRFM-OLH) to preserve user&#39;s sensitive information. Our proposed mechanisms protect sensitive data with the same privacy guarantee and they are suitable for different scenarios. Besides, in theory, we compare the estimation errors of our proposed mechanisms with existing LDP based mechanisms and show that ours are lower than theirs. Finally, we conduct experiments on synthetic and real-world datasets to evaluate the performance of the three mechanisms. The experimental results demonstrate that our proposed mechanisms are better than the existing LDP based solutions over the same privacy level, while uRFM-OLH frequently performs the best.},
  archive      = {J_TETC},
  author       = {Yue Zhang and Youwen Zhu and Yuqian Zhou and Jiabin Yuan},
  doi          = {10.1109/TETC.2023.3238839},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {316-327},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Frequency estimation mechanisms under ϵδ-utility-optimized local differential privacy},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Preserving differential privacy in deep learning based on
feature relevance region segmentation. <em>TETC</em>, <em>12</em>(1),
307–315. (<a href="https://doi.org/10.1109/TETC.2023.3244174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of Big Data, deep learning techniques provide intelligent solutions for various problems in real-life scenarios. However, deep neural networks depend on large-scale datasets including sensitive data, which causes the potential risk of privacy leakage. In addition, various constantly evolving attack methods are also threatening the data security in deep learning models. Protecting data privacy effectively at a lower cost has become an urgent challenge. This article proposes an Adaptive Feature Relevance Region Segmentation (AFRRS) mechanism to provide differential privacy preservation. The core idea is to divide the input features into different regions with different relevance according to the relevance between input features and the model output. Less noise is intentionally injected into the region with stronger relevance, and more noise is injected into the regions with weaker relevance. Furthermore, we perturb loss functions by injecting noise into the polynomial coefficients of the expansion of the objective function to protect the privacy of data labels. Theoretical analysis and experiments have shown that the proposed AFRRS mechanism can not only provide strong privacy preservation for the deep learning model, but also maintain the good utility of the model under a given moderate privacy budget compared with existing methods.},
  archive      = {J_TETC},
  author       = {Fangwei Wang and Meiyun Xie and Zhiyuan Tan and Qingru Li and Changguang Wang},
  doi          = {10.1109/TETC.2023.3244174},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {307-315},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Preserving differential privacy in deep learning based on feature relevance region segmentation},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Indoor geo-indistinguishability: Adopting differential
privacy for indoor location data protection. <em>TETC</em>,
<em>12</em>(1), 293–306. (<a
href="https://doi.org/10.1109/TETC.2023.3242166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the extensive applicability of Location-Based Services (LBSs) and the Global Navigation Satellite System (GNSS) failure in indoor environments, indoor positioning systems have been widely implemented. Location fingerprinting, in particular, collects the Received Signal Strength (RSS) from users’ devices, allowing Location Service Providers (LSPs) to precisely identify their locations. Therefore, LSPs and potential attackers have implicit access to this sensitive data, violating users’ privacy. This issue has been addressed in outdoor environments by introducing Geo-indistinguishability (GeoInd), an alternative representation of Differential Privacy (DP). In indoor environments, however, the user lacks their coordinates, posing a new difficulty. This article presents a novel framework for implementing GeoInd for indoor environments. The proposed framework introduces two distance calculation and RSS generation methods based solely on RSS values. Moreover, involving other participants or trusted third parties is not necessary to protect privacy, regardless of the attackers’ prior knowledge. The proposed framework is evaluated in a simulated environment and two experimental settings. The results validate the proposed framework&#39;s efficiency, effectiveness, and applicability in indoor environments under the GeoInd setting.},
  archive      = {J_TETC},
  author       = {Amir Fathalizadeh and Vahideh Moghtadaiee and Mina Alishahi},
  doi          = {10.1109/TETC.2023.3242166},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {293-306},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Indoor geo-indistinguishability: Adopting differential privacy for indoor location data protection},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Age of information optimization for privacy-preserving
mobile crowdsensing. <em>TETC</em>, <em>12</em>(1), 281–292. (<a
href="https://doi.org/10.1109/TETC.2023.3268234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile crowdsensing (MCS)-enabled data collection can be implemented in a cost-effective, scalable, and flexible manner. However, joint sensing data freshness and security assurance have not been fully investigated in the current research. To address these two concerns, the potential game and homomorphic encryption-based joint Age of Information (AoI) optimization and privacy-preservation scheme for MCS is put forward in this paper. At first, the AoI minimization and privacy preservation-oriented MCS system framework is established. Then, the AoI-based spectrum access strategies are derived by a potential game in detail, where the stochastic learning algorithm is used to reach the Nash Equilibrium (NE) solution. Next, based on the somewhat homomorphic encryption method, the encrypted sensing data can be submitted to the service provider (SP) for further processing, where the data content can only be known to mobile workers (MWs) and service requester (SR) with permission. Finally, the numerical results show that our proposed MCS system can simultaneously guarantee data freshness and system security at an acceptable cost.},
  archive      = {J_TETC},
  author       = {Yaoqi Yang and Bangning Zhang and Daoxing Guo and Renhui Xu and Chunhua Su and Weizheng Wang},
  doi          = {10.1109/TETC.2023.3268234},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {281-292},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Age of information optimization for privacy-preserving mobile crowdsensing},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Blockchain-based federated learning with SMPC model
verification against poisoning attack for healthcare systems.
<em>TETC</em>, <em>12</em>(1), 269–280. (<a
href="https://doi.org/10.1109/TETC.2023.3268186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the rising awareness of privacy and security in machine learning applications, federated learning (FL) has received widespread attention and applied to several areas, e.g., intelligence healthcare systems, IoT-based industries, and smart cities. FL enables clients to train a global model collaboratively without accessing their local training data. However, the current FL schemes are vulnerable to adversarial attacks. Its architecture makes detecting and defending against malicious model updates difficult. In addition, most recent studies to detect FL from malicious updates while maintaining the model&#39;s privacy have not been sufficiently explored. This article proposed blockchain-based federated learning with SMPC model verification against poisoning attacks for healthcare systems. First, we check the machine learning model from the FL participants through an encrypted inference process and remove the compromised model. Once the participants’ local models have been verified, the models are sent to the blockchain node to be securely aggregated. We conducted several experiments with different medical datasets to evaluate our proposed framework.},
  archive      = {J_TETC},
  author       = {Aditya Pribadi Kalapaaking and Ibrahim Khalil and Xun Yi},
  doi          = {10.1109/TETC.2023.3268186},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {269-280},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Blockchain-based federated learning with SMPC model verification against poisoning attack for healthcare systems},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guest editorial IEEE transactions on emerging topics in
computing special section on advances in emerging privacy-preserving
computing. <em>TETC</em>, <em>12</em>(1), 266–268. (<a
href="https://doi.org/10.1109/TETC.2024.3374568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning and cloud computing have dramatically increased the utility of data. These technologies facilitate our life and provide smart and intelligent services. Notably, machine learning algorithms need to learn from massive training data to improve accuracy. Hence, data is the core component of machine learning and plays an important role. Cloud computing is a new computing model that provides on-demand services, such as data storage, computing power, and infrastructure. Data owners are allowed to outsource their data to cloud servers, but will lose direct control of their data. The rising trend in data breach shows that privacy and security have been major issues in machine learning and cloud computing.},
  archive      = {J_TETC},
  author       = {Jinguang Han and Patrick Schaumont and Willy Susilo},
  doi          = {10.1109/TETC.2024.3374568},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {266-268},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Guest editorial IEEE transactions on emerging topics in computing special section on advances in emerging privacy-preserving computing},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MFDS-STGCN: Predicting the behaviors of college students
with fine-grained spatial-temporal activities data. <em>TETC</em>,
<em>12</em>(1), 254–265. (<a
href="https://doi.org/10.1109/TETC.2023.3344131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining and predicting college students behaviors from fine-grained spatial-temporal campus activity data play key roles in the academic success and personal development of college students. Most of the existing behavior prediction methods use shallow learning algorithms such as statistics, clustering, and correlation analysis approaches, which fail to mine the long-term spatial-temporal dependencies and semantic correlations from these fine-grained campus data. We propose a novel multi-fragment dynamic semantic spatial-temporal graph convolution network, named the MFDS-STGCN, on the basis of a spatial-temporal graph convolutional network (STGCN) for the automatic prediction of college students’ behaviors and abnormal behaviors. We construct a dataset including 7.6 million behavioral records derived from approximately 400 students over 140 days to evaluate the effectiveness of the prediction model. Extensive experimental results demonstrate that the proposed method outperforms multiple baseline prediction methods in terms of student behavior prediction and abnormal behavior prediction, with accuracies of 92.60% and 90.84%, respectively. To further enable behavior prediction, we establish an early warning management mechanism. Based on the predictions and analyses of Big Data, education administrators can detect undesirable abnormal behaviors in time and thus implement effective interventions to better guide students&#39; campus lives, ultimately helping them to more effectively develop and grow.},
  archive      = {J_TETC},
  author       = {Dongbo Zhou and Hongwei Yu and Jie Yu and Shuai Zhao and Wenhui Xu and Qianqian Li and Fengyin Cai},
  doi          = {10.1109/TETC.2023.3344131},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {254-265},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {MFDS-STGCN: Predicting the behaviors of college students with fine-grained spatial-temporal activities data},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coupled attention networks for multivariate time series
anomaly detection. <em>TETC</em>, <em>12</em>(1), 240–253. (<a
href="https://doi.org/10.1109/TETC.2023.3280577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series anomaly detection (MTAD) plays a vital role in a wide variety of real-world application domains. Over the past few years, MTAD has attracted rapidly increasing attention from both academia and industry. Many deep learning and graph learning models have been developed for effective anomaly detection in multivariate time series data, which enable advanced applications such as smart surveillance and risk management with unprecedented capabilities. Nevertheless, MTAD is facing critical challenges deriving from the dependencies among sensors and variables, which often change over time. To address this issue, we propose a coupled attention-based neural network framework (CAN) for anomaly detection in multivariate time series data featuring dynamic variable relationships. We combine adaptive graph learning methods with graph attention to generate a global-local graph that can represent both global correlations and dynamic local correlations among sensors. To capture inter-sensor relationships and temporal dependencies, a convolutional neural network based on the global-local graph is integrated with a temporal self-attention module to construct a coupled attention module. In addition, we develop a multilevel encoder-decoder architecture that accommodates reconstruction and prediction tasks to better characterize multivariate time series data. Extensive experiments on real-world datasets have been conducted to evaluate the performance of the proposed CAN approach, and the results show that CAN significantly outperforms state-of-the-art baselines.},
  archive      = {J_TETC},
  author       = {Feng Xia and Xin Chen and Shuo Yu and Mingliang Hou and Mujie Liu and Linlin You},
  doi          = {10.1109/TETC.2023.3280577},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {240-253},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Coupled attention networks for multivariate time series anomaly detection},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PresQ: Discovery of multidimensional equally-distributed
dependencies via quasi-cliques on hypergraphs. <em>TETC</em>,
<em>12</em>(1), 224–239. (<a
href="https://doi.org/10.1109/TETC.2022.3198252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-matching data stored on separate files is an everyday activity in the scientific domain. However, sometimes the relation between attributes may not be obvious. The discovery of foreign keys on relational databases is a similar problem. Thus techniques devised for this problem can be adapted. Nonetheless, when the data is numeric and subject to uncertainty, this adaptation is not trivial. This article first introduces the concept of Equally-Distributed Dependencies , which is similar to the Inclusion Dependencies from the relational domain. We describe a correspondence in order to bridge existing ideas. We then propose PresQ : a new algorithm based on the search of maximal quasi-cliques on hyper-graphs to make it more robust to the nature of uncertain numerical data. This algorithm has been tested on seven public datasets, showing promising results both in its capacity to find multidimensional equally-distributed sets of attributes and in run-time.},
  archive      = {J_TETC},
  author       = {Alejandro Álvarez-Ayllón and Manuel Palomo-Duarte and Juan-Manuel Dodero},
  doi          = {10.1109/TETC.2022.3198252},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {224-239},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {PresQ: Discovery of multidimensional equally-distributed dependencies via quasi-cliques on hypergraphs},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transformer-based graph neural networks for outfit
generation. <em>TETC</em>, <em>12</em>(1), 213–223. (<a
href="https://doi.org/10.1109/TETC.2023.3268363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Suggesting complementary clothing items to compose an outfit is a process of emerging interest, yet it involves a fine understanding of fashion trends and visual aesthetics. Previous works have mainly focused on recommendation by scoring visual appeal and representing garments as ordered sequences or as collections of pairwise-compatible items. This limits the full usage of relations among clothes. We attempt to bridge the gap between outfit recommendation and generation by leveraging a graph-based representation of items in a collection. The work carried out in this article, tries to build a bridge between outfit recommendation and generation, by discovering new appealing outfits starting from a collection of pre-existing ones. We propose a transformer-based architecture, named TGNN, which exploits multi-headed self attention to capture relations between clothing items in a graph as a message passing step in Convolutional Graph Neural Networks. Specifically, starting from a seed, i.e. one or more garments, outfit generation is performed by iteratively choosing the garment that is most compatible with the previously chosen ones. Extensive experimentations are conducted with two different datasets, demonstrating the capability of the model to perform seeded outfit generation as well as obtaining state of the art results on compatibility estimation tasks.},
  archive      = {J_TETC},
  author       = {Federico Becattini and Federico Maria Teotini and Alberto Del Bimbo},
  doi          = {10.1109/TETC.2023.3268363},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {213-223},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Transformer-based graph neural networks for outfit generation},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Skeleton-based action segmentation with multi-stage
spatial-temporal graph convolutional neural networks. <em>TETC</em>,
<em>12</em>(1), 202–212. (<a
href="https://doi.org/10.1109/TETC.2022.3230912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to identify and temporally segment fine-grained actions in motion capture sequences is crucial for applications in human movement analysis. Motion capture is typically performed with optical or inertial measurement systems, which encode human movement as a time series of human joint locations and orientations or their higher-order representations. State-of-the-art action segmentation approaches use multiple stages of temporal convolutions. The main idea is to generate an initial prediction with several layers of temporal convolutions and refine these predictions over multiple stages, also with temporal convolutions. Although these approaches capture long-term temporal patterns, the initial predictions do not adequately consider the spatial hierarchy among the human joints. To address this limitation, we recently introduced multi-stage spatial-temporal graph convolutional neural networks (MS-GCN). Our framework replaces the initial stage of temporal convolutions with spatial graph convolutions and dilated temporal convolutions, which better exploit the spatial configuration of the joints and their long-term temporal dynamics. Our framework was compared to four strong baselines on five tasks. Experimental results demonstrate that our framework is a strong baseline for skeleton-based action segmentation.},
  archive      = {J_TETC},
  author       = {Benjamin Filtjens and Bart Vanrumste and Peter Slaets},
  doi          = {10.1109/TETC.2022.3230912},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {202-212},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Skeleton-based action segmentation with multi-stage spatial-temporal graph convolutional neural networks},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph embedding techniques for predicting missing links in
biological networks: An empirical evaluation. <em>TETC</em>,
<em>12</em>(1), 190–201. (<a
href="https://doi.org/10.1109/TETC.2023.3282539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network science tries to understand the complex relationships among entities or actors of a system through graph formalism. For instance, biological networks represent macromolecules such as genes, proteins, or other small chemicals as nodes and the interactions among the molecules as links or edges. Often potential links are guessed computationally due to the expensive nature of wet lab experiments. Conventional link prediction techniques rely on local network topology and fail to incorporate the global structure fully. Graph representation learning (or embedding) aims to describe the properties of the entire graph by optimized, structure-preserving encoding of nodes or entire (sub) graphs into lower-dimensional vectors. Leveraging the encoded vectors as a feature improves the performance of the missing link identification task. Assessing the predictive quality of graph embedding techniques in missing link identification is essential. In this work, we evaluate the performance of ten (10) state-of-the-art graph embedding techniques in predicting missing links with special emphasis on homogeneous and heterogeneous biological networks. Most available graph embedding techniques cannot be used directly for link prediction. Hence, we use the latent representation of the network produced by the candidate techniques and reconstruct the network using various similarity and kernel functions. We evaluate nine (09) similarity functions in combination with candidate embedding techniques. We compare embedding techniques’ performance against five (05) traditional (non-embedding-based) link prediction techniques. Experimental results reveal that the quality of embedding-based link prediction is better than its counterpart. Among them, Neural Network-based embedding and attention-based techniques show consistent performance. We even observe that dot-product-based similarity is the best in inferring pair-wise edges among the nodes from their embedding. We report interesting findings that while predicting links in the heterogeneous graph, it predicts a good number of valid links between corresponding homogeneous nodes due to the possible indirect effect of homogeneous-heterogeneous interactions.},
  archive      = {J_TETC},
  author       = {Binon Teji and Swarup Roy and Devendra Singh Dhami and Dinabandhu Bhandari and Pietro Hiram Guzzi},
  doi          = {10.1109/TETC.2023.3282539},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {190-201},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Graph embedding techniques for predicting missing links in biological networks: An empirical evaluation},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep graph networks for drug repurposing with multi-protein
targets. <em>TETC</em>, <em>12</em>(1), 177–189. (<a
href="https://doi.org/10.1109/TETC.2023.3238963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the early phases of the COVID-19 pandemic, repurposing of drugs approved for use in other diseases helped counteract the aggressiveness of the virus. Therefore, the availability of effective and flexible methodologies to speed up and prioritize the repurposing process is fundamental to tackle present and future challenges to worldwide health. This work addresses the problem of drug repurposing through the lens of deep learning for graphs, by designing an architecture that exploits both structural and biological information to propose a reduced set of drugs that may be effective against an unknown disease. Our main contribution is a method to repurpose a drug against multiple proteins, rather than the most common single-drug/single-protein setting. The method leverages graph embeddings to encode the relevant proteins’ and drugs’ information based on gene ontology data and structural similarities. Finally, we publicly release a comprehensive and unified data repository for graph-based analysis to foster further studies on COVID-19 and drug repurposing. We empirically validate the proposed approach in a general drug repurposing setting, showing that it generalizes better than single protein repurposing schemes. We conclude the manuscript with an exemplified application of our method to the COVID-19 use case. All source code is publicly available.},
  archive      = {J_TETC},
  author       = {Davide Bacciu and Federico Errica and Alessio Gravina and Lorenzo Madeddu and Marco Podda and Giovanni Stilo},
  doi          = {10.1109/TETC.2023.3238963},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {177-189},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Deep graph networks for drug repurposing with multi-protein targets},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fusing higher and lower-order biological information for
drug repositioning via graph representation learning. <em>TETC</em>,
<em>12</em>(1), 163–176. (<a
href="https://doi.org/10.1109/TETC.2023.3239949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drug repositioning is a promising drug development technique to identify new indications for existing drugs. However, existing computational models only make use of lower-order biological information at the level of individual drugs, diseases and their associations, but few of them can take into account higher-order connectivity patterns presented in biological heterogeneous information networks (HINs). In this work, we propose a novel graph representation learning model, namely FuHLDR, for drug repositioning by fusing higher and lower-order biological information. Specifically, given a HIN, FuHLDR first learns the representations of drugs and diseases at a lower-order level by considering their biological attributes and drug-disease associations (DDAs) through a graph convolutional network model. Then, a meta-path-based strategy is designed to obtain their higher-order representations involving the associations among drugs, proteins and diseases. Their integrated representations are thus determined by fusing higher and lower-order representations, and finally a Random Vector Functional Link Network is employed by FuHLDR to identify novel DDAs. Experimental results on two benchmark datasets demonstrate that FuHLDR performs better than several state-of-the-art drug repositioning models. Furthermore, our case studies on Alzheimer&#39;s disease and Breast neoplasms indicate that the rich higher-order biological information gains new insight into drug repositioning with improved accuracy.},
  archive      = {J_TETC},
  author       = {Bo-Wei Zhao and Lei Wang and Peng-Wei Hu and Leon Wong and Xiao-Rui Su and Bao-Quan Wang and Zhu-Hong You and Lun Hu},
  doi          = {10.1109/TETC.2023.3239949},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {163-176},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Fusing higher and lower-order biological information for drug repositioning via graph representation learning},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Edgeless-GNN: Unsupervised representation learning for
edgeless nodes. <em>TETC</em>, <em>12</em>(1), 150–162. (<a
href="https://doi.org/10.1109/TETC.2023.3292240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of embedding edgeless nodes such as users who newly enter the underlying network, while using graph neural networks (GNNs) widely studied for effective representation learning of graphs. Our study is motivated by the fact that GNNs cannot be straightforwardly adopted for our problem since message passing to such edgeless nodes having no connections is impossible. To tackle this challenge, we propose $\mathsf{Edgeless-GNN}$ , a novel inductive framework that enables GNNs to generate node embeddings even for edgeless nodes through unsupervised learning . Specifically, we start by constructing a proxy graph based on the similarity of node attributes as the GNN&#39;s computation graph defined by the underlying network. The known network structure is used to train model parameters, whereas a topology-aware loss function is established such that our model judiciously learns the network structure by encoding positive, negative, and second-order relations between nodes. For the edgeless nodes, we inductively infer embeddings by expanding the computation graph. By evaluating the performance of various downstream machine learning tasks, we empirically demonstrate that $\mathsf{Edgeless-GNN}$ exhibits (a) superiority over state-of-the-art inductive network embedding methods for edgeless nodes, (b) effectiveness of our topology-aware loss function, (c) robustness to incomplete node attributes, and (d) a linear scaling with the graph size.},
  archive      = {J_TETC},
  author       = {Yong-Min Shin and Cong Tran and Won-Yong Shin and Xin Cao},
  doi          = {10.1109/TETC.2023.3292240},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {150-162},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Edgeless-GNN: Unsupervised representation learning for edgeless nodes},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph reconfigurable pooling for graph representation
learning. <em>TETC</em>, <em>12</em>(1), 139–149. (<a
href="https://doi.org/10.1109/TETC.2023.3268098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, graph neural networks have been widely used for tasks such as graph classification, link prediction, and node classification, and have achieved excellent results. In order to apply GNNs to graph classification tasks, recent works generate graph-level representations using node representations through a hierarchical pooling approach. Existing graph pooling methods such as DiffPool and EigenPool encourage adjacent nodes to be assigned to the same cluster, making the node assignment process similar to the graph partitioning process that ignores the role of nodes or some substructures (e.g., amino acids) in the process of composing a graph (e.g., proteins). In this article, we propose a new pooling operator RecPool to capture the role played by nodes in the process of composing a graph. Specifically, we probabilistically model the feature distribution of the coarsened graph, construct the feature distribution of each cluster, resample the features of the coarsened graph into the original nodes according to the soft assignment matrix, reconstruct the original graph, and optimize the soft assignment matrix to divide the nodes that play the same role in the reconstruction process into the same cluster. The excellent performance of Recpool is demonstrated through experiments on four public benchmark dataset.},
  archive      = {J_TETC},
  author       = {Xiaolin Li and Qikui Xu and Zhenyu Xu and Hongyan Zhang and Li Xu},
  doi          = {10.1109/TETC.2023.3268098},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {139-149},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Graph reconfigurable pooling for graph representation learning},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HDGCN: Dual-channel graph convolutional network with
higher-order information for robust feature learning. <em>TETC</em>,
<em>12</em>(1), 126–138. (<a
href="https://doi.org/10.1109/TETC.2023.3238046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional network (GCN) algorithms have been employed to learn graph embedding due to its inductive inference property, which is extended to GCN with higher-order information. However, the application of these higher-order information is confusing and not effectively distinguished. In addition, single channel GCN using higher-order information is weak for robust feature learning, and existing dual-channel GCNs rarely take higher-order information into account. To alleviate the above problems, we propose a dual-channel GCN with higher-order information for robust feature learning, denoted as HDGCN. First, features of positive and negative higher-order graphs are extracted that fully exploits the self-contained attributes and higher-order geometric information. Meantime, the features of original graph structure are extracted by a conventional GCN that utilizes the self-contained feature attributes. Then, node features are represented as edge features by a feature fusion function. For the selection of negative samples, a fractional staggered negative sampling method is applied, by which the trainable graph model gains better topological features. Finally, the performance on seven real-world datasets demonstrates that HDGCN obtains the state-of-the-art performance on pairwise link prediction, higher-order structure prediction, and node classification tasks. By changing the attributes of multiple tasks, it can be proved that HDGCN has good robustness.},
  archive      = {J_TETC},
  author       = {Meixia He and Jianrui Chen and Maoguo Gong and Zhongshi Shao},
  doi          = {10.1109/TETC.2023.3238046},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {126-138},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {HDGCN: Dual-channel graph convolutional network with higher-order information for robust feature learning},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guest editorial emerging trends and advances in graph-based
methods and applications. <em>TETC</em>, <em>12</em>(1), 122–125. (<a
href="https://doi.org/10.1109/TETC.2024.3374581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of graph structures in diverse domains has recently garnered substantial attention, presenting a paradigm shift from classical euclidean representations. This new trend is driven by the advent of novel algorithms that can capture complex relationships through a class of neural architectures: the Graph Neural Networks (GNNs) [1], [2]. These networks are adept at handling data that can be effectively modeled as graphs, introducing a new representation learning paradigm. The significance of GNNs extends to several domains, including computer vision [3], [4], natural language processing [5], chemistry/biology [6], physics [7], traffic networks [8], and recommendation systems [9].},
  archive      = {J_TETC},
  author       = {Alessandro D&#39;Amelio and Jianyi Lin and Jean-Yves Ramel and Raffaella Lanzarotti},
  doi          = {10.1109/TETC.2024.3374581},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {122-125},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Guest editorial emerging trends and advances in graph-based methods and applications},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Side-channel attack analysis on in-memory computing
architectures. <em>TETC</em>, <em>12</em>(1), 109–121. (<a
href="https://doi.org/10.1109/TETC.2023.3257684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-memory computing (IMC) systems have great potential for accelerating data-intensive tasks such as deep neural networks (DNNs). As DNN models are generally highly proprietary, the neural network architectures become valuable targets for attacks. In IMC systems, since the whole model is mapped on chip and weight memory read can be restricted, the pre-mapped DNN model acts as a “black box” for users. However, the localized and stationary weight and data patterns may subject IMC systems to other attacks. In this article, we propose a side-channel attack methodology on IMC architectures. We show that it is possible to extract model architectural information from power trace measurements without any prior knowledge of the neural network. We first developed a simulation framework that can emulate the dynamic power traces of the IMC macros. We then performed side-channel leakage analysis to reverse engineer model information such as the stored layer type, layer sequence, output channel/feature size and convolution kernel size from power traces of the IMC macros. Based on the extracted information, full networks can potentially be reconstructed without any knowledge of the neural network. Finally, we discuss potential countermeasures for building IMC systems that offer resistance to these model extraction attack.},
  archive      = {J_TETC},
  author       = {Ziyu Wang and Fan-Hsuan Meng and Yongmo Park and Jason K. Eshraghian and Wei D. Lu},
  doi          = {10.1109/TETC.2023.3257684},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {109-121},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Side-channel attack analysis on in-memory computing architectures},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparsity-oriented MRAM-centric computing for efficient
neural network inference. <em>TETC</em>, <em>12</em>(1), 97–108. (<a
href="https://doi.org/10.1109/TETC.2023.3326312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Near-memory computing (NMC) and in- memory computing (IMC) paradigms show great importance in non-von Neumann architecture. Spin-transfer torque magnetic random access memory (STT-MRAM) is considered as a promising candidate to realize both NMC and IMC for resource-constrained applications. In this work, two MRAM-centric computing frameworks are proposed: triple-skipping NMC (TS-NMC) and analog-multi-bit-sparsity IMC (AMS-IMC). The TS-NMC exploits the sparsity of activations and weights to implement a write-read-calculation triple skipping computing scheme by utilizing a sparse flag generator. The AMS-IMC with reconfigured computing bit-cell and flag generator accommodate bit-level activation sparsity in the computing. STT-MRAM array and its peripheral circuits are implemented with an industrial 28-nm CMOS design-kit and an MTJ compact model. The triple-skipping scheme can reduce memory access energy consumption by 51.5× when processing zero vectors, compared to processing non-zero vectors. The energy efficiency of AMS-IMC is improved by 5.9× and 1.5× (with 75% input sparsity) as compared to the conventional NMC framework and existing analog IMC framework. Verification results show that TS-NMC and AMS-IMC achieved 98.6% and 97.5% inference accuracy in MNIST classification, with energy consumption of 14.2 nJ/pattern and 12.7 nJ/pattern, respectively.},
  archive      = {J_TETC},
  author       = {Jia-Le Cui and Yanan Guo and Juntong Chen and Bo Liu and Hao Cai},
  doi          = {10.1109/TETC.2023.3326312},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {97-108},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Sparsity-oriented MRAM-centric computing for efficient neural network inference},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GRAPHIC: Gather and process harmoniously in the cache with
high parallelism and flexibility. <em>TETC</em>, <em>12</em>(1), 84–96.
(<a href="https://doi.org/10.1109/TETC.2023.3290683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-memory computing (IMC) has been proposed to overcome the von Neumann bottleneck in data-intensive applications. However, existing IMC solutions could not achieve both high parallelism and high flexibility, which limits their application in more general scenarios: As a highly parallel IMC design, the functionality of a MAC crossbar is limited to the matrix-vector multiplication; Another IMC method of logic-in-memory (LiM) is more flexible in supporting different logic functions, but has low parallelism. To improve the LiM parallelism, we are inspired by investigating how the single-instruction, multiple-data (SIMD) instruction set in conventional CPU could potentially help to expand the number of LiM operands in one cycle. The biggest challenge is the inefficiency in handling non-continuous data in parallel due to the SIMD limitation of (i) continuous address, (ii) limited cache bandwidth, and (iii) large full-resolution parallel computing overheads. This article presents GRAPHIC, the first reported in-memory SIMD architecture that solves the parallelism and irregular data access challenges in applying SIMD to LiM. GRAPHIC exploits content-addressable memory (CAM) and row-wise-accessible SRAM. By providing the in-situ, full-parallelism, and low-overhead operations of address search, cache read-compute-and-update, GRAPHIC accomplishes high-efficiency gather and aggregation with high parallelism, high energy efficiency, low latency, and low area overheads. Experiments in both continuous data access and irregular data pattern applications show an average speedup of 5x over iso-area AVX-like LiM, and 3-5x over the emerging CAM-based accelerators of CAPE and GaaS-X in advanced techniques.},
  archive      = {J_TETC},
  author       = {Yiming Chen and Mingyen Lee and Guohao Dai and Mufeng Zhou and Nagadastagiri Challapalle and Tianyi Wang and Yao Yu and Yongpan Liu and Yu Wang and Huazhong Yang and Vijaykrishnan Narayanan and Xueqing Li},
  doi          = {10.1109/TETC.2023.3290683},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {84-96},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {GRAPHIC: Gather and process harmoniously in the cache with high parallelism and flexibility},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A near-sensor processing accelerator for approximate local
binary pattern networks. <em>TETC</em>, <em>12</em>(1), 73–83. (<a
href="https://doi.org/10.1109/TETC.2023.3285493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, a high-speed and energy-efficient comparator-based N ear- S ensor L ocal B inary P attern accelerator architecture (NS-LBP) is proposed to execute a novel local binary pattern deep neural network. First, inspired by recent LBP networks, we design an approximate, hardware-oriented, and multiply-accumulate (MAC)-free network named Ap-LBP for efficient feature extraction, further reducing the computation complexity. Then, we develop NS-LBP as a processing-in-SRAM unit and a parallel in-memory LBP algorithm to process images near the sensor in a cache, remarkably reducing the power consumption of data transmission to an off-chip processor. Our circuit-to-application co-simulation results on MNIST and SVHN datasets demonstrate minor accuracy degradation compared to baseline CNN and LBP-network models, while NS-LBP achieves 1.25 GHz and an energy-efficiency of 37.4 TOPS/W. NS-LBP reduces energy consumption by 2.2× and execution time by a factor of 4× compared to the best recent LBP-based networks.},
  archive      = {J_TETC},
  author       = {Shaahin Angizi and Mehrdad Morsali and Sepehr Tabrizchi and Arman Roohi},
  doi          = {10.1109/TETC.2023.3285493},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {73-83},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A near-sensor processing accelerator for approximate local binary pattern networks},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3DL-PIM: A look-up table oriented programmable processing in
memory architecture based on the 3-d stacked memory for data-intensive
applications. <em>TETC</em>, <em>12</em>(1), 60–72. (<a
href="https://doi.org/10.1109/TETC.2023.3293140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memory-centric computing systems have demonstrated superior performance and efficiency in memory-intensive applications compared to state-of-the-art CPUs and GPUs. 3-D stacked DRAM architectures unlock higher I/O data bandwidth than the traditional 2-D memory architecture and therefore are better suited for incorporating memory-centric processors. However, merely integrating high-precision ALUs in the 3-D stacked memory does not ensure an optimized design since such a design can only achieve a limited utilization of the internal bandwidth of a memory chip and limited operational parallelization. To address this, we propose 3DL-PIM, a 3-D stacked memory-based Processing in Memory (PIM) architecture that locates a plurality of Look-up Table (LUT)-based low-footprint Processing Elements (PE) within the memory banks in order to achieve high parallel computing performance by maximizing data-bandwidth utilization. Instead of relying on the traditional logic-based ALUs, the PEs are formed by clustering a group of programmable LUTs and therefore can be programmed on-the-fly to perform various logic/arithmetic operations. Our simulations show that 3DL-PIM can achieve respectively up to 2.6× higher processing performance at 2.65× higher area efficiency compared to a state-of-the-art 3-D stacked memory-based accelerator.},
  archive      = {J_TETC},
  author       = {Purab Ranjan Sutradhar and Sathwika Bavikadi and Sai Manoj Pudukotai Dinakarrao and Mark A. Indovina and Amlan Ganguly},
  doi          = {10.1109/TETC.2023.3293140},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {60-72},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {3DL-PIM: A look-up table oriented programmable processing in memory architecture based on the 3-D stacked memory for data-intensive applications},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CRAM-based acceleration for intermittent computing of
parallelizable tasks. <em>TETC</em>, <em>12</em>(1), 48–59. (<a
href="https://doi.org/10.1109/TETC.2023.3293426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is an emerging requirement for performing data-intensive parallel computations, e.g., machine-learning inference, locally on batteryless sensors. These devices are resource-constrained and operate intermittently due to the irregular energy availability in the environment. Intermittent execution might lead to several side effects that might prevent the correct execution of computational tasks. Even though recent studies proposed methods to cope with these side effects and execute these tasks correctly, they overlooked the efficient intermittent execution of parallelizable data-intensive machine-learning tasks. In this article, we present PiMCo—a novel programmable CRAM-based in-memory coprocessor that exploits the Processing In-Memory (PIM) paradigm and facilitates the power-failure resilient execution of parallelizable computational loads. Contrary to existing PIM solutions for intermittent computing, PiMCo promotes better programmability to accelerate a variety of parallelizable tasks. Our performance evaluation demonstrates that PiMCo improves the performance of existing low-power accelerators for intermittent computing by up to 8× and energy efficiency by up to 150×.},
  archive      = {J_TETC},
  author       = {Khakim Akhunov and Kasım Sinan Yıldırım},
  doi          = {10.1109/TETC.2023.3293426},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {48-59},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {CRAM-based acceleration for intermittent computing of parallelizable tasks},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hardware/software co-design with ADC-less in-memory
computing hardware for spiking neural networks. <em>TETC</em>,
<em>12</em>(1), 35–47. (<a
href="https://doi.org/10.1109/TETC.2023.3316121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking Neural Networks (SNNs) are bio-plausible models that hold great potential for realizing energy-efficient implementations of sequential tasks on resource-constrained edge devices. However, commercial edge platforms based on standard GPUs are not optimized to deploy SNNs, resulting in high energy and latency. While analog In-Memory Computing (IMC) platforms can serve as energy-efficient inference engines, they are accursed by the immense energy, latency, and area requirements of high-precision ADCs (HP-ADC), overshadowing the benefits of in-memory computations. We propose a hardware/software co-design methodology to deploy SNNs into an ADC-Less IMC architecture using sense-amplifiers as 1-bit ADCs replacing conventional HP-ADCs and alleviating the above issues. Our proposed framework incurs minimal accuracy degradation by performing hardware-aware training and is able to scale beyond simple image classification tasks to more complex sequential regression tasks. Experiments on complex tasks of optical flow estimation and gesture recognition show that progressively increasing the hardware awareness during SNN training allows the model to adapt and learn the errors due to the non-idealities associated with ADC-Less IMC. Also, the proposed ADC-Less IMC offers significant energy and latency improvements, $2-7\times$ and $8.9-24.6\times$ , respectively, depending on the SNN model and the workload, compared to HP-ADC IMC.},
  archive      = {J_TETC},
  author       = {Marco Paul E. Apolinario and Adarsh Kumar Kosta and Utkarsh Saxena and Kaushik Roy},
  doi          = {10.1109/TETC.2023.3316121},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {35-47},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Hardware/Software co-design with ADC-less in-memory computing hardware for spiking neural networks},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A FeFET-based ADC offset robust compute-in-memory
architecture for streaming keyword spotting (KWS). <em>TETC</em>,
<em>12</em>(1), 23–34. (<a
href="https://doi.org/10.1109/TETC.2023.3345346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Keyword spotting (KWS) on edge devices requires low power consumption and real-time response. In this work, a ferroelectric field-effect transistor (FeFET)-based compute-in-memory (CIM) architecture is proposed for streaming KWS processing. Compared with the conventional sequential processing scheme, the inference latency is reduced by 7.7 × ∼17.6× without energy efficiency loss. To make the KWS models robust to hardware non-idealities such as analog-to-digital converter (ADC) offset, an offset-aware training scheme is proposed. It consists of ADC offset noise injection and frame-wise normalization. This scheme effectively improves the mean accuracy and chip yield by 1.5%∼5.2%, and 5%∼39%, for TC-ResNet and DS-TC-ResNet (with MatchboxNet configuration), respectively. The proposed CIM architecture is implemented with ferroelectric field-effect transistor technology, with simulated low energy consumption of 1.65 μJ/decision for 12-word keyword spotting using TC-ResNet8.},
  archive      = {J_TETC},
  author       = {Yandong Luo and Johan Vanderhaegen and Oleg Rybakov and Martin Kraemer and Niel Warren and Shimeng Yu},
  doi          = {10.1109/TETC.2023.3345346},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {23-34},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A FeFET-based ADC offset robust compute-in-memory architecture for streaming keyword spotting (KWS)},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding bulk-bitwise processing in-memory through
database analytics. <em>TETC</em>, <em>12</em>(1), 7–22. (<a
href="https://doi.org/10.1109/TETC.2023.3315189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bulk-bitwise processing-in-memory (PIM), where large bitwise operations are performed in parallel by the memory array itself, is an emerging form of computation with the potential to mitigate the memory wall problem. This article examines the capabilities of bulk-bitwise PIM by constructing PIMDB, a fully-digital system based on memristive stateful logic, utilizing and focusing on in-memory bulk-bitwise operations, designed to accelerate a real-life workload: analytical processing of relational databases. We introduce a host processor programming model to support bulk-bitwise PIM in virtual memory, develop techniques to efficiently perform in-memory filtering and aggregation operations, and adapt the application data set into the memory. To understand bulk-bitwise PIM, we compare it to an equivalent in-memory database on the same host system. We show that bulk-bitwise PIM substantially lowers the number of required memory read operations, thus accelerating TPC-H filter operations by 1.6×–18× and full queries by 56×–608×, while reducing the energy consumption by 1.7×–18.6× and 0.81×–12× for these benchmarks, respectively. Our extensive evaluation uses the gem5 full-system simulation environment. The simulations also evaluate cell endurance, showing that the required endurance is within the range of existing endurance of RRAM devices.},
  archive      = {J_TETC},
  author       = {Ben Perach and Ronny Ronen and Benny Kimelfeld and Shahar Kvatinsky},
  doi          = {10.1109/TETC.2023.3315189},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {7-22},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Understanding bulk-bitwise processing in-memory through database analytics},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guest editorial IEEE transactions on emerging topics in
special section on emerging in-memory computing architectures and
applications. <em>TETC</em>, <em>12</em>(1), 4–6. (<a
href="https://doi.org/10.1109/TETC.2024.3369288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer architecture stands at an important crossroad to surmount vital performance challenges. For more than four decades, the performance of general purpose computing systems has been improving by 20–50% per year [1]. In the last decade, this number has dropped to less than 7% per year. Most recently, that rate has slowed to only 3% per year. [1]. The demand for performance improvement, however, keeps increasing and diversifies within new application domains. This higher performance, however, often has to come at a lower power consumption cost too, adding to the complexity of the task of architectural design space optimization. Both today&#39;s computer architectures and device technologies (used to manufacture them) are facing major challenges to achieve the performance demands required by complex applications such as Artificial Intelligence (AI). The complexity stems from the extremely high number of operations to be computed and the involved amount of data.},
  archive      = {J_TETC},
  author       = {Alberto Bosio and Ronald F. DeMara and Deliang Fan and Nima TaheriNejad},
  doi          = {10.1109/TETC.2024.3369288},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  number       = {1},
  pages        = {4-6},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Guest editorial IEEE transactions on emerging topics in special section on emerging in-memory computing architectures and applications},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
