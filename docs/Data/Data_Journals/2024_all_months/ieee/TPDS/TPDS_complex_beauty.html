<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TPDS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tpds---175">TPDS - 175</h2>
<ul>
<li><details>
<summary>
(2024). DyLaClass: Dynamic labeling based classification for optimal
sparse matrix format selection in accelerating SpMV. <em>TPDS</em>,
<em>35</em>(12), 2624–2639. (<a
href="https://doi.org/10.1109/TPDS.2024.3488053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse matrix-vector multiplication (SpMV) is crucial in many scientific and engineering applications, particularly concerning the effectiveness of different sparse matrix storage formats for various architectures, no single format excels across all hardware. Previous research has focused on trying different algorithms to build predictors for the best format, yet it overlooked how to address the issue of the best format changing in the same hardware environment and how to reduce prediction overhead rather than merely considering the overhead in building predictors. This paper proposes a novel classification algorithm for optimizing sparse matrix storage formats, DyLaClass, based on dynamic labeling and flexible feature selection. Particularly, we introduce mixed labels and features with strong correlations, allowing us to achieve ultra-high prediction accuracy with minimal feature inputs, significantly reducing feature extraction overhead. For the first time, we propose the concept of the most suitable storage format rather than the best storage format, which can stably predict changes in the best format for the same matrix across multiple SpMV executions. We further demonstrate the proposed method on the University of Florida’s public sparse matrix collection dataset. Experimental results show that compared to existing work, our method achieves up to 91% classification accuracy. Using two different hardware platforms for verification, the proposed method outperforms existing methods by 1.26 to 1.43 times. Most importantly, the stability of the proposed prediction model is 25.5% higher than previous methods, greatly increasing the feasibility of the model in practical field applications.},
  archive      = {J_TPDS},
  author       = {Zheng Shi and Yi Zou and Xianfeng Song and Shupeng Li and Fangming Liu and Quan Xue},
  doi          = {10.1109/TPDS.2024.3488053},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {12},
  number       = {12},
  pages        = {2624-2639},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DyLaClass: Dynamic labeling based classification for optimal sparse matrix format selection in accelerating SpMV},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HybRAID: A high-performance hybrid RAID storage architecture
for write-intensive applications in all-flash storage systems.
<em>TPDS</em>, <em>35</em>(12), 2608–2623. (<a
href="https://doi.org/10.1109/TPDS.2024.3429336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the ever-increasing demand for higher I/O performance and reliability in data-intensive applications, solid-state drives (SSDs) typically configured as redundant array of independent disks (RAID) are broadly used in enterprise all-flash storage systems . While a mirrored RAID offers higher performance in random access workloads, parity-based RAIDs (e.g., RAID5) provide higher performance in sequential accesses with less cost overhead. Previous studies try to address the poor performance of parity-based RAIDs in small writes (i.e., writes into a single disk) by offering various schemes, including caching or logging small writes. However, such techniques impose a significant performance and/or reliability overheads and are seldom used in the industry. In addition, our empirical analysis shows that partial stripe writes, i.e., writing into a fraction of a full array in parity-based RAIDs, can significantly degrade the I/O performance, which has not been addressed in the previous work. In this paper, we first offer an empirical study which reveals partial stripe writes reduce the performance of parity-based RAIDs by up to 6.85× compared to full stripe writes (i.e., writes into entire disks). Then, we propose a high-performance hyb rid RAID storage architecture, called HybRAID , which is optimized for write-intensive applications. HybRAID exploits the advantages of mirror- and parity-based RAIDs to improve the write performance. HybRAID directs a) aligned full stripe writes to parity-based RAID tier and b) small/partial stripe writes to the RAID1 tier. We propose an online migration scheme, which aims to move small/partial writes from parity-based RAID to RAID1, based on access frequency of updates. As a complement, we further offer offline migration, whose aim is to make room in the fast tier for future references. Experimental results over enterprise SSDs show that HybRAID improves the performance of write-intensive applications by 3.3× and 2.6×, as well as enhancing performance per cost by 3.1× and 3.0× compared to parity-based RAID and RAID10, respectively, at equivalent costs.},
  archive      = {J_TPDS},
  author       = {Maryam Karimi and Reza Salkhordeh and André Brinkmann and Hossein Asadi},
  doi          = {10.1109/TPDS.2024.3429336},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {12},
  number       = {12},
  pages        = {2608-2623},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {HybRAID: A high-performance hybrid RAID storage architecture for write-intensive applications in all-flash storage systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and performance evaluation of linearly extensible
cube-triangle network for multicore systems. <em>TPDS</em>,
<em>35</em>(12), 2596–2607. (<a
href="https://doi.org/10.1109/TPDS.2024.3486219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-performance interconnection networks are currently being used to design Massively Parallel Computers. Selecting the set of nodes on which parallel tasks execute plays a vital role in the performance of such systems. These networks when deployed to run large parallel applications suffer from communication latencies which ultimately affect the system throughput. Mesh and Torus are primary examples of topologies used in such systems. However, these are being replaced with more efficient and complicated hybrid topologies such as ZMesh and x-Folded TM networks. This paper presents a new topology named as Linearly Extensible Cube-Triangle (LECΔ) which focuses on low latency, lesser average distance and improved throughput. It is symmetrical in nature and exhibits the desirable properties of similar networks with lesser complexity and cost. For N x N network, the LECΔ topology has lesser network latency than that of Mesh, ZMesh, Torus and x-Folded networks. The proposed LECΔ network produces reduced average distance, diameter and cost. It has a high value of bisection width and good scalability. The simulation results show that the performance of LECΔ network is similar to that of Mesh, ZMesh, Torus and x-Folded networks. The results verify the efficiency of the LECΔ network as evaluated and compared with similar networks.},
  archive      = {J_TPDS},
  author       = {Savita Gautam and Abdus Samad and Mohammad S. Umar},
  doi          = {10.1109/TPDS.2024.3486219},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {12},
  number       = {12},
  pages        = {2596-2607},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Design and performance evaluation of linearly extensible cube-triangle network for multicore systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PeakFS: An ultra-high performance parallel file system via
computing-network-storage co-optimization for HPC applications.
<em>TPDS</em>, <em>35</em>(12), 2578–2595. (<a
href="https://doi.org/10.1109/TPDS.2024.3485754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging high-performance computing (HPC) applications with diverse workload characteristics impose greater demands on parallel file systems (PFSs). PFSs also require more efficient software designs to fully utilize the performance of modern hardware, such as multi-core CPUs, Remote Direct Memory Access (RDMA), and NVMe SSDs. However, existing PFSs expose great limitations under these requirements due to limited multi-core scalability, unaware of HPC workloads, and disjointed network-storage optimizations. In this article, we present PeakFS, an ultra-high performance parallel file system via computing-network-storage co-optimization for HPC applications. PeakFS designs a shared-nothing scheduling system based on link-reduced task dispatching with lock-free queues to reduce concurrency overhead. Besides, PeakFS improves I/O performance with flexible distribution strategies, memory-efficient indexing, and metadata caching according to HPC I/O characteristics. Finally, PeakFS shortens the critical path of request processing through network-storage co-optimizations. Experimental results show that the metadata and data performance of PeakFS reaches more than 90% of the hardware limits. For metadata throughput, PeakFS achieves a 3.5–19× improvement over GekkoFS and outperforms BeeGFS by three orders of magnitude.},
  archive      = {J_TPDS},
  author       = {Yixiao Chen and Haomai Yang and Kai Lu and Wenlve Huang and Jibin Wang and Jiguang Wan and Jian Zhou and Fei Wu and Changsheng Xie},
  doi          = {10.1109/TPDS.2024.3485754},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {12},
  number       = {12},
  pages        = {2578-2595},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {PeakFS: An ultra-high performance parallel file system via computing-network-storage co-optimization for HPC applications},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient distributed edge computing for dependent
delay-sensitive tasks in multi-operator multi-access networks.
<em>TPDS</em>, <em>35</em>(12), 2559–2577. (<a
href="https://doi.org/10.1109/TPDS.2024.3468892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of distributed computing in the multi-operator multi-access edge computing (MEC) network for dependent tasks . Every task comprises several sub-tasks which are executed based on logical precedence modelled as a directed acyclic graph . In the graph, each vertex is a sub-task, each edge – precedence constraint, such that a sub-task can only be started after all its preceding sub-tasks are completed. Tasks are executed by MEC servers with the assistance of nearby edge devices, so that the MEC network can be viewed as a distributed “ primary-secondary node ” system where each MEC server acts as a primary node (PN) deciding on sub-tasks assigned to its secondary nodes (SNs), i.e., nearby edge devices. The PN&#39;s decision problem is complex, as its SNs can be associated with other neighboring PNs. In this case, the available processing resources of SNs depend on the sub-task assignment decisions of all neighboring PNs. Since PNs are controlled by different operators, they do not coordinate their decisions, and each PN is uncertain about the sub-task assignments of its neighbors (and, thus, the available resources of its SNs). To address this problem, we propose a novel framework based on a graphical Bayesian game , where PNs play under uncertainty about their neighbors’ decisions. We prove that the game has a perfect Bayesian equilibrium (PBE) yielding unique optimal values , and formulate new Bayesian reinforcement learning and Bayesian deep reinforcement learning algorithms enabling each PN to reach the PBE autonomously (without communicating with other PNs).},
  archive      = {J_TPDS},
  author       = {Alia Asheralieva and Dusit Niyato and Xuetao Wei},
  doi          = {10.1109/TPDS.2024.3468892},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {12},
  number       = {12},
  pages        = {2559-2577},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient distributed edge computing for dependent delay-sensitive tasks in multi-operator multi-access networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MoltDB: Accelerating blockchain via ancient state
segregation. <em>TPDS</em>, <em>35</em>(12), 2545–2558. (<a
href="https://doi.org/10.1109/TPDS.2024.3467927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain store states in Log-Structured Merge (LSM) tree-based database. Due to blockchain traceability, the growing ancient states are inevitably stored in the databases. Unfortunately, by default, this process mixes current and ancient states in the data layout, increasing unnecessary disk I/O access and slowing transaction execution. This paper proposes MoltDB, a scalable LSM-based database for efficient transaction execution through a novel idea of ancient state segregation , i.e., to segregate current and ancient states in the data layout. However, the frequently generated and uncertainly accessed characteristics of ancient states make the segregation challenging. Thus, we develop an “extract-compact” mechanism to batch extraction process for frequently generated ancient states and the LSM compaction process to relieve additional disk I/O overhead. Moreover, we design an adaptive LSM-based storage for the uncertainly accessed ancient states extracted for on-demand access. We implement MoltDB as a database engine compatible with many mainstream blockchains and integrate it into Ethereum for evaluation. Experimental results show that MoltDB achieves 1.3 × transaction throughput and 30% disk I/O latency savings over the state-of-the-art works.},
  archive      = {J_TPDS},
  author       = {Junyuan Liang and Wuhui Chen and Zicong Hong and Haogang Zhu and Wangjie Qiu and Zibin Zheng},
  doi          = {10.1109/TPDS.2024.3467927},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {12},
  number       = {12},
  pages        = {2545-2558},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {MoltDB: Accelerating blockchain via ancient state segregation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TARIS: Scalable incremental processing of time-respecting
algorithms on streaming graphs. <em>TPDS</em>, <em>35</em>(12),
2527–2544. (<a href="https://doi.org/10.1109/TPDS.2024.3471574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal graphs change with time and have a lifespan associated with each vertex and edge. These graphs are suitable to process time-respecting algorithms where the traversed edges must have monotonic timestamps. Interval-centric Computing Model (ICM) is a distributed programming abstraction to design such temporal algorithms. There has been little work on supporting time-respecting algorithms at large scales for streaming graphs, which are updated continuously at high rates (Millions/s), such as in financial and social networks. In this article, we extend the windowed-variant of ICM for incremental computing over streaming graph updates. We formalize the properties of temporal graph algorithms and prove that our model of incremental computing over streaming updates is equivalent to batch execution of ICM. We design TARIS, a novel distributed graph platform that implements these incremental computing features. We use efficient data structures to reduce memory access and enhance locality during graph updates. We also propose scheduling strategies to interleave updates with computing, and streaming strategies to adapt the execution window for incremental computing to the variable input rates. Our detailed and rigorous evaluation of temporal algorithms on large-scale graphs with up to $2\,\text{B}$ edges show that TARIS out-performs contemporary baselines, Tink and Gradoop, by 3–4 orders of magnitude, and handles a high input rate of $ 83k$ – $ 587\,\text{M}$ Mutations/s with latencies in the order of seconds–minutes.},
  archive      = {J_TPDS},
  author       = {Ruchi Bhoot and Suved Sanjay Ghanmode and Yogesh Simmhan},
  doi          = {10.1109/TPDS.2024.3471574},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {12},
  number       = {12},
  pages        = {2527-2544},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {TARIS: Scalable incremental processing of time-respecting algorithms on streaming graphs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Breaking the memory wall for heterogeneous federated
learning via model splitting. <em>TPDS</em>, <em>35</em>(12), 2513–2526.
(<a href="https://doi.org/10.1109/TPDS.2024.3480115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) enables multiple devices to collaboratively train a shared model while preserving data privacy. Ever-increasing model complexity coupled with limited memory resources on the participating devices severely bottlenecks the deployment of FL in real-world scenarios. Thus, a framework that can effectively break the memory wall while jointly taking into account the hardware and statistical heterogeneity in FL is urgently required. In this article, we propose SmartSplit a framework that effectively reduces the memory footprint on the device side while guaranteeing the training progress and model accuracy for heterogeneous FL through model splitting. Towards this end, SmartSplit employs a hierarchical structure to adaptively guide the overall training process. In each training round, the central manager, hosted on the server, dynamically selects the participating devices and sets the cutting layer by jointly considering the memory budget, training capacity, and data distribution of each device. The MEC manager, deployed within the edge server, proceeds to split the local model and perform training of the server-side portion. Meanwhile, it fine-tunes the splitting points based on the time-evolving statistical importance. The on-device manager, embedded inside each mobile device, continuously monitors the local training status while employing cost-aware checkpointing to match the runtime dynamic memory budget. Extensive experiments on representative datasets are conducted on both commercial off-the-shelf mobile device testbeds. The experimental results show that SmartSplit excels in FL training on highly memory-constrained mobile SoCs, offering up to a 94% peak latency reduction and 100-fold memory savings. It enhances accuracy performance by 1.49%-57.18% and adaptively adjusts to dynamic memory budgets through cost-aware recomputation},
  archive      = {J_TPDS},
  author       = {Chunlin Tian and Li Li and Kahou Tam and Yebo Wu and Cheng-Zhong Xu},
  doi          = {10.1109/TPDS.2024.3480115},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {12},
  number       = {12},
  pages        = {2513-2526},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Breaking the memory wall for heterogeneous federated learning via model splitting},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mitosis: A scalable sharding system featuring multiple
dynamic relay chains. <em>TPDS</em>, <em>35</em>(12), 2497–2512. (<a
href="https://doi.org/10.1109/TPDS.2024.3480223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sharding is a prevalent approach for addressing performance issues in blockchain. To reduce governance complexities and ensure system security, a common practice involves a relay chain to coordinate cross-shard transactions. However, with a growing number of shards and cross-shard transactions, the single relay chain usually first suffers from performance bottleneck and shows poor scalability, thus making the relay chain&#39;s scalability vital for sharding systems. To solve this, we propose Mitosis , the first multi-relay architecture to improve the relay chain&#39;s scalability by sharding the relay chain itself. Our proposed relay sharding algorithm dynamically adjusts the number of relays or optimizes the topology between relays and shards to adaptively scale up relay chain&#39;s performance. Furthermore, to guarantee the security of the multi-relay architecture, a new validator reconfiguration scheme is designed, accompanied by a comprehensive security analysis of Mitosis . Through simulation experiments on two mainstream relay chain paradigms, we demonstrate that Mitosis can achieve high scalability and outperform state-of-the-art baselines in terms of workload of relays, relay chain throughput, and transaction latency.},
  archive      = {J_TPDS},
  author       = {Keyuan Wang and Linpeng Jia and Zhaoxiong Song and Yi Sun},
  doi          = {10.1109/TPDS.2024.3480223},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {12},
  number       = {12},
  pages        = {2497-2512},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Mitosis: A scalable sharding system featuring multiple dynamic relay chains},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TrieKV: A high-performance key-value store design with
memory as its first-class citizen. <em>TPDS</em>, <em>35</em>(12),
2479–2496. (<a href="https://doi.org/10.1109/TPDS.2024.3473013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Key-value (KV) stores based on log-structured merge tree (LSM-tree) have been extensively studied and deployed in major information technology infrastructures. Because this type of systems is catered for KV store accessing disks, a limited disk bandwidth increases the difficulty of serving online data requests. One solution involves using a large DRAM such that frequent KV pairs are buffered and accessed from the main memory – and this solution exposes a major design drawback of the KV store: its lack of support for integrated data management in memory and on disks. For example, data in the most popular LSM-tree implementation – RocksDB – may reside in a small write buffer (MemTable) that organizes KV pairs for disk writes, a buffer cache for disk blocks, a write-ahead log on the disk for data persistence, and in various LSM levels on the disk. Without the integrated management of indexes, data, and their persistence in a hierarchical memory/disk architecture, memory is under-utilized along with missed performance optimization opportunities. We propose a KV store, TrieKV, which holistically incorporates DRAM, persistent memory (PMem), and disk with certain desired features: (1) fast in-memory access, (2) accurate identification of hot/cold data at an adaptable granularity, (3) customized memory space allocation for minimized fragmentation, (4) hotness-aware data placement across the storage hierarchy, (5) in-place data persistence in the PMem, and (6) hotness-aware LSM-tree compaction. TrieKV employs a single, integrated trie-structured index for all KV pairs in memory, where access hotness can be consistently discovered. Accordingly, the KV placement is dynamically determined according to the hotness and persistence needs of the storage hierarchy spanning the DRAM, PMem, and solid-state drive. In the experiment, we demonstrate that the 99th latency of RocksDB and NoveLSM is 38x and 6x higher than that of TrieKV, respectively. In addition, TrieKV outperforms RocksDB and NoveLSM by a factor of 5.6 and 1.7in terms of throughput, respectively.},
  archive      = {J_TPDS},
  author       = {Hui Sun and Deyan Kong and Song Jiang and Yinliang Yue and Xiao Qin},
  doi          = {10.1109/TPDS.2024.3473013},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {12},
  number       = {12},
  pages        = {2479-2496},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {TrieKV: A high-performance key-value store design with memory as its first-class citizen},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on performance modeling and prediction for
distributed DNN training. <em>TPDS</em>, <em>35</em>(12), 2463–2478. (<a
href="https://doi.org/10.1109/TPDS.2024.3476390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent breakthroughs in large-scale DNN attract significant attention from both academia and industry toward distributed DNN training techniques. Due to the time-consuming and expensive execution process of large-scale distributed DNN training, it is crucial to model and predict the performance of distributed DNN training before its actual deployment, in order to optimize the design of distributed DNN training at low cost. This paper analyzes and emphasizes the importance of modeling and predicting the performance of distributed DNN training, categorizes and analyses the related state-of-the-art works, and discusses future challenges and opportunities for this research field. The objectives of this paper are twofold: first, to assist researchers in understanding and choosing suitable modeling and prediction tools for large-scale distributed DNN training, and second, to encourage researchers to propose more valuable research about performance modeling and prediction for distributed DNN training in the future.},
  archive      = {J_TPDS},
  author       = {Zhenhua Guo and Yinan Tang and Jidong Zhai and Tongtong Yuan and Jian Jin and Li Wang and Yaqian Zhao and Rengang Li},
  doi          = {10.1109/TPDS.2024.3476390},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {12},
  number       = {12},
  pages        = {2463-2478},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A survey on performance modeling and prediction for distributed DNN training},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Competitive analysis of online elastic caching of transient
data in multi-tiered content delivery network. <em>TPDS</em>,
<em>35</em>(12), 2449–2462. (<a
href="https://doi.org/10.1109/TPDS.2024.3475412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the demand for faster and more reliable content delivery escalates, Content Delivery Networks (CDNs) face significant challenges in managing content placement across their increasingly complex, multi-tiered structures to balance performance, complexity, and scalability, while addressing the transient nature of data and the unpredictability of internet traffic. Addressing these challenges, this study introduces a novel multi-tier CDN caching strategy that navigates spatial and temporal trade-offs in cache placement, considering the cache placement cost diminishes with the content lifetime, and the uncertainty of future data demands. We design a distributed online algorithm that evaluates each incoming request and places new caches when the total content delivery cost exceeds a threshold. Our competitive analysis shows a tight and optimal $\mathtt {Tiers}+1$ competitive ratio. Additionally, our algorithm has low complexity by passing $O(\mathtt {Tiers})$ number of reference messages for each request, which enhances its practical applicability. Empirical validation through numerical simulations and trace-driven experiments confirms the superiority of our approach to existing benchmarks in real-world CDN settings.},
  archive      = {J_TPDS},
  author       = {Binghan Wu and Wei Bao and Bing Bing Zhou},
  doi          = {10.1109/TPDS.2024.3475412},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {12},
  number       = {12},
  pages        = {2449-2462},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Competitive analysis of online elastic caching of transient data in multi-tiered content delivery network},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BCB-SpTC: An efficient sparse high-dimensional tensor
contraction employing tensor core acceleration. <em>TPDS</em>,
<em>35</em>(12), 2435–2448. (<a
href="https://doi.org/10.1109/TPDS.2024.3477746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse tensor contraction (SpTC) is an important operator in tensor networks, which tends to generate a large amount of sparse high-dimensional data, placing higher demands on the computational performance and storage bandwidth of the processor. Using GPUs with powerful arithmetic characteristics is a reliable choice for accelerating SpTC, however, the high dimensionality and sparsity of tensor makes GPU-accelerated SpTC operators suffer from the difficulties of low computational intensity and high memory consumption. The recent introduction of Tensor Core Units (TCUs) on GPUs brings even more powerful arithmetic, which exacerbates the memory wall problem. To cope with the challenges, this paper proposes a new BCB format that linearizes the indices of multidimensional blocks to reduce block index accesses and uses a bitmap to store the distribution of non-zero elements in a block to reduce the storage overhead. A parallel blocking algorithm of BCB-SpTC is designed to divide the binary linear indices into free and contracted indexes to improve the pairing overhead of computational tasks. Then based on the characteristic computation method of TCUs, the proprietary filling method of TCUs is designed to overcome the inefficiency of parallel computation of sparse data on TCUs. Finally, experimental results on the A100 dataset show that BCB-SpTC improves the acceleration ratio by $1.1\times$ to $21.3\times$ over the existing SpTC GPU method.},
  archive      = {J_TPDS},
  author       = {Rong Hu and Haotian Wang and Wangdong Yang and Renqiu Ouyang and Keqin Li and Kenli Li},
  doi          = {10.1109/TPDS.2024.3477746},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {12},
  number       = {12},
  pages        = {2435-2448},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {BCB-SpTC: An efficient sparse high-dimensional tensor contraction employing tensor core acceleration},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FastLoad: Speeding up data loading of both sparse matrix and
vector for SpMV on GPUs. <em>TPDS</em>, <em>35</em>(12), 2423–2434. (<a
href="https://doi.org/10.1109/TPDS.2024.3477431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse Matrix-Vector Multiplication (SpMV) on GPUs has gained significant attention because of SpMV&#39;s importance in modern applications and the increasing computing power of GPUs in the last decade. Previous studies have emphasized the importance of data loading for the overall performance of SpMV and demonstrated the efficacy of coalesced memory access in enhancing data loading efficiency. However, existing approaches fall far short of reaching the full potential of data loading on modern GPUs. In this paper, we propose an efficient algorithm called FastLoad, that speeds up the loading of both sparse matrices and input vectors of SpMV on modern GPUs. Leveraging coalesced memory access, FastLoad achieves high loading efficiency and load balance by sorting both the columns of the sparse matrix and elements of the input vector based on the number of non-zero elements while organizing non-zero elements in blocks to avoid thread divergence. FastLoad takes the Compressed Sparse Column (CSC) format as an implementation case to prove the concept and gain insights. We conduct a comprehensive comparison of FastLoad with the CSC-based SpMV, cuSPARSE, CSR5, and TileSpMV, using the full SuiteSparse Matrix Collection as workload. The experimental results on RTX 3090 Ti demonstrate that our method outperforms the others in most matrices, with geometric speedup means over CSC-based, cuSPARSE, CSR5, and TileSpMV being 2.12×, 2.98×, 2.88×, and 1.22×, respectively.},
  archive      = {J_TPDS},
  author       = {Jinyu Hu and Huizhang Luo and Hong Jiang and Guoqing Xiao and Kenli Li},
  doi          = {10.1109/TPDS.2024.3477431},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {12},
  number       = {12},
  pages        = {2423-2434},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FastLoad: Speeding up data loading of both sparse matrix and vector for SpMV on GPUs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VisionAGILE: A versatile domain-specific accelerator for
computer vision tasks. <em>TPDS</em>, <em>35</em>(12), 2405–2422. (<a
href="https://doi.org/10.1109/TPDS.2024.3466891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of diverse machine learning (ML) models has led to groundbreaking revolutions in computer vision (CV). These ML models include convolutional neural networks (CNNs), graph neural networks (GNNs), and vision transformers (ViTs). However, existing hardware accelerators designed for CV lack the versatility to support various ML models, potentially limiting their applicability to real-world scenarios. To address this limitation, we introduce VisionAGILE, a domain-specific accelerator designed to be versatile and capable of accommodating a range of ML models, including CNNs, GNNs, and ViTs. VisionAGILE comprises a compiler, a runtime system, and a hardware accelerator. For the hardware accelerator, we develop a novel unified architecture with a flexible data path and memory organization to support the computation primitives in various ML models. Regarding the compiler design, we develop a unified compilation workflow that maps various ML models to the proposed hardware accelerator. The runtime system executes dynamic sparsity exploitation to reduce inference latency and dynamic task scheduling for workload balance. The compiler, the runtime system, and the hardware accelerator work synergistically to support a variety of ML models in CV, enabling low-latency inference. We deploy the hardware accelerator on a state-of-the-art data center FPGA (Xilinx Alveo U250). We evaluate VisionAGILE on diverse ML models for CV, including CNNs, GNNs, hybrid models (comprising both CNN and GNN), and ViTs. The experimental results indicate that, compared with state-of-the-art CPU (GPU) implementations, VisionAGILE achieves a speedup of $81.7\times$ ( $4.8\times$ ) in terms of latency. Evaluated on standalone CNNs, GNNs, and ViTs, VisionAGILE demonstrates comparable or higher performance with state-of-the-art CNN accelerators, GNN accelerators, and ViT accelerators, respectively.},
  archive      = {J_TPDS},
  author       = {Bingyi Zhang and Rajgopal Kannan and Carl Busart and Viktor K. Prasanna},
  doi          = {10.1109/TPDS.2024.3466891},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {12},
  number       = {12},
  pages        = {2405-2422},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {VisionAGILE: A versatile domain-specific accelerator for computer vision tasks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed task processing platform for infrastructure-less
IoT networks: A multi-dimensional optimization approach. <em>TPDS</em>,
<em>35</em>(12), 2392–2404. (<a
href="https://doi.org/10.1109/TPDS.2024.3469545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of artificial intelligence (AI) and the Internet of Things (IoT), intelligent information services have showcased unprecedented capabilities in acquiring and analysing information. The conventional task processing platforms rely on centralised Cloud processing, which encounters challenges in infrastructure-less environments with unstable or disrupted electrical grids and cellular networks. These challenges hinder the deployment of intelligent information services in such environments. To address these challenges, we propose a distributed task processing platform ( ${DTPP}$ ) designed to provide satisfactory performance for executing computationally intensive applications in infrastructure-less environments. This platform leverages numerous distributed homogeneous nodes to process the arriving task locally or collaboratively. Based on this platform, a distributed task allocation algorithm is developed to achieve high task processing performance with limited energy and bandwidth resources. To validate our approach, ${DTPP}$ has been tested in an experimental environment utilising real-world experimental data to simulate IoT network services in infrastructure-less environments. Extensive experiments demonstrate that our proposed solution surpasses comparative algorithms in key performance metrics, including task processing ratio, task processing accuracy, algorithm processing time, and energy consumption.},
  archive      = {J_TPDS},
  author       = {Qiushi Zheng and Jiong Jin and Zhishu Shen and Libing Wu and Iftekhar Ahmad and Yong Xiang},
  doi          = {10.1109/TPDS.2024.3469545},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {12},
  number       = {12},
  pages        = {2392-2404},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Distributed task processing platform for infrastructure-less IoT networks: A multi-dimensional optimization approach},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient schedule construction for distributed execution of
large DNN models. <em>TPDS</em>, <em>35</em>(12), 2375–2391. (<a
href="https://doi.org/10.1109/TPDS.2024.3466913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasingly complex and diverse deep neural network (DNN) models necessitate distributing the execution across multiple devices for training and inference tasks, and also require carefully planned schedules for performance. However, existing practices often rely on predefined schedules that may not fully exploit the benefits of emerging diverse model-aware operator placement strategies. Handcrafting high-efficiency schedules can be challenging due to the large and varying schedule space. This paper presents Tessel, an automated system that searches for efficient schedules for distributed DNN training and inference for diverse operator placement strategies. To reduce search costs, Tessel leverages the insight that the most efficient schedules often exhibit repetitive pattern ( repetend ) across different data inputs. This leads to a two-phase approach: repetend construction and schedule completion. By exploring schedules for various operator placement strategies, Tessel significantly improves both training and inference performance. Experiments with representative DNN models demonstrate that Tessel achieves up to 5.5× training performance speedup and up to 38% inference latency reduction.},
  archive      = {J_TPDS},
  author       = {Zhiqi Lin and Youshan Miao and Guanbin Xu and Cheng Li and Olli Saarikivi and Saeed Maleki and Fan Yang},
  doi          = {10.1109/TPDS.2024.3466913},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {12},
  number       = {12},
  pages        = {2375-2391},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient schedule construction for distributed execution of large DNN models},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GeoDeploy: Geo-distributed application deployment using
benchmarking. <em>TPDS</em>, <em>35</em>(12), 2361–2374. (<a
href="https://doi.org/10.1109/TPDS.2024.3470532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geo-distributed web-applications (GWA) can be deployed across multiple geographically separated datacenters to reduce the latency of access for users. Finding a suitable deployment for a GWA is challenging due to the requirement to consider a number of different parameters, such as host configurations across a federated infrastructure. The ability to evaluate multiple deployment configurations enables an efficient outcome to be determined, balancing resource usage while satisfying user requirements. We propose GeoDeploy , a framework designed for finding a deployment solution for GWA. We evaluate GeoDeploy using both a formal algorithmic model and a practical cloud-based deployment. We also compare our approach with other existing techniques.},
  archive      = {J_TPDS},
  author       = {Devki Nandan Jha and Yinhao Li and Zhenyu Wen and Graham Morgan and Prem Prakash Jayaraman and Maciej Koutny and Omer F. Rana and Rajiv Ranjan},
  doi          = {10.1109/TPDS.2024.3470532},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {12},
  number       = {12},
  pages        = {2361-2374},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {GeoDeploy: Geo-distributed application deployment using benchmarking},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fair coflow scheduling via controlled slowdown.
<em>TPDS</em>, <em>35</em>(12), 2347–2360. (<a
href="https://doi.org/10.1109/TPDS.2024.3446188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The average coflow completion time (CCT) is the standard performance metric in coflow scheduling. However, standard CCT minimization may introduce unfairness between the data transfer phase of different computing jobs. Thus, while progress guarantees have been introduced in the literature to mitigate this fairness issue, the trade-off between fairness and efficiency of data transfer is hard to control. This paper introduces a fairness framework for coflow scheduling based on the concept of slowdown, i.e., the performance loss of a coflow compared to isolation. By controlling the slowdown it is possible to enforce a target coflow progress while minimizing the average CCT. In the proposed framework, the minimum slowdown for a batch of coflows can be determined in polynomial time. By showing the equivalence with Gaussian elimination, slowdown constraints are introduced into primal-dual iterations of the CoFair algorithm. The algorithm extends the class of the $\sigma$ -order schedulers to solve the fair coflow scheduling problem in polynomial time. It provides a 4-approximation of the average CCT w.r.t. an optimal scheduler. Extensive numerical results demonstrate that this approach can trade off average CCT for slowdown more efficiently than existing state of the art schedulers.},
  archive      = {J_TPDS},
  author       = {Francesco De Pellegrini and Vaibhav Kumar Gupta and Rachid El Azouzi and Serigne Gueye and Cedric Richier and Jeremie Leguay},
  doi          = {10.1109/TPDS.2024.3446188},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {12},
  number       = {12},
  pages        = {2347-2360},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Fair coflow scheduling via controlled slowdown},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-timescale joint optimization of task scheduling and
resource scaling in multi-data center system based on multi-agent deep
reinforcement learning. <em>TPDS</em>, <em>35</em>(12), 2331–2346. (<a
href="https://doi.org/10.1109/TPDS.2024.3467212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a new computing paradigm, multi-data center computing enables service providers to deploy their applications close to the users. However, due to the spatio-temporal changes in workloads, it is challenging to coordinate multiple distributed data centers to provide high-quality services while reducing service operation costs. To address this challenge, this article studies the joint optimization problem of task scheduling and resource scaling in multi-data center systems. Since the task scheduling and the resource scaling are usually performed in different timescales, we decompose the joint optimization problem into two sub-problems and propose a two-timescale optimization framework. The short-timescale task scheduling can promptly relieve the bursty arrivals of computing tasks, and the long-timescale resource scaling can adapt well to the long-term changes in workloads. To address the distributed optimization problem, we propose a two-timescale multi-agent deep reinforcement learning algorithm. In order to characterize the graph-structured states of connected data centers, we develop a directed graph convolutional network based global state representation model. The evaluation indicates that the proposed algorithm is able to reduce both the task makespan and the task timeout while maintaining a reasonable cost.},
  archive      = {J_TPDS},
  author       = {Shuangwu Chen and Jiangming Li and Qifeng Yuan and Huasen He and Sen Li and Jian Yang},
  doi          = {10.1109/TPDS.2024.3467212},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {12},
  number       = {12},
  pages        = {2331-2346},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Two-timescale joint optimization of task scheduling and resource scaling in multi-data center system based on multi-agent deep reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing i/o performance through effective vCPU scheduling
interference management. <em>TPDS</em>, <em>35</em>(12), 2315–2330. (<a
href="https://doi.org/10.1109/TPDS.2023.3329298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual machines (VMs) heavily rely on virtual CPUs (vCPUs) scheduling to achieve efficient I/O performance. The vCPU scheduling interference can cause inconsistent scheduling latency and degraded I/O performance, potentially compromising the services provided by affected VMs. Existing solutions have limitations, such as inefficiency in diagnosing interference issues or imposing undesired side effects on cloud systems. To address these challenges, we present Otter, a holistic technique for optimizing I/O performance in the presence of vCPU scheduling interference. Otter employs innovative methods to enhance interference diagnosis efficiency. First, we propose lightweight methods to measure the dynamic changes in scheduling latencies for co-running vCPUs, ensuring both flexibility and accuracy. Second, we propose fine-grained quantification methods to timely determine the interference, with low false positive and false negative rates. Third, we identify interference patterns that aid in analyzing the root causes of interference and preventing similar issues from recurring. Otter has been operational for one year in the production cloud at the National Supercomputing Center (Wuxi). It diagnoses and helps fix more than 470 vCPU scheduling interference-related issues, resulting in a 19.6% improvement in cloud service I/O performance with negligible overhead in production.},
  archive      = {J_TPDS},
  author       = {Liang Wang and Jinzhe Yang and Jidong Zhai and Guangwen Yang},
  doi          = {10.1109/TPDS.2023.3329298},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {12},
  number       = {12},
  pages        = {2315-2330},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Optimizing I/O performance through effective vCPU scheduling interference management},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Acceleration of multi-body molecular dynamics with
customized parallel dataflow. <em>TPDS</em>, <em>35</em>(12), 2297–2314.
(<a href="https://doi.org/10.1109/TPDS.2024.3420441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {FPGAs are drawing increasing attention in resolving molecular dynamics (MD) problems, and have already been applied in problems such as two-body potentials, force fields composed of these potentials, etc. Competitive performance is obtained compared with traditional counterparts such as CPUs and GPUs. However, as far as we know, FPGA solutions for more complex and real-world MD problems, such as multi-body potentials, are seldom to be seen. This work explores the prospects of state-of-the-art FPGAs in accelerating multi-body potential. An FPGA-based accelerator with customized parallel dataflow that features multi-body potential computation, motion update, and internode communication is designed. Major contributions include: (1) parallelization applied at different levels of the accelerator; (2) an optimized dataflow mixing atom-level pipeline and cell-level pipeline to achieve high throughput; (3) a mixed-precision method using different precision at different stages of simulations; and (4) a communication-efficient method for internode communication. Experiments show that, our single-node accelerator is over 2.7× faster than an 8-core CPU design, performing 20.501 ns/day on a 55,296-atom system for the Tersoff simulation. Regarding power efficiency, our accelerator is 28.9× higher than I7-11700 and 4.8× higher than RTX 3090 when running the same test case.},
  archive      = {J_TPDS},
  author       = {Quan Deng and Qiang Liu and Ming Yuan and Xiaohui Duan and Lin Gan and Jinzhe Yang and Wenlai Zhao and Zhenxiang Zhang and Guiming Wu and Wayne Luk and Haohuan Fu and Guangwen Yang},
  doi          = {10.1109/TPDS.2024.3420441},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {12},
  number       = {12},
  pages        = {2297-2314},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Acceleration of multi-body molecular dynamics with customized parallel dataflow},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trusted model aggregation with zero-knowledge proofs in
federated learning. <em>TPDS</em>, <em>35</em>(11), 2284–2296. (<a
href="https://doi.org/10.1109/TPDS.2024.3455762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new global model aggregation method based on using zero-knowledge federated learning (ZKFL). The purpose is to secure horizontal or P2P federated machine learning systems with shorter aggregation times, higher model accuracy, and lower system costs. We use a model parameter-sharing Chord overlay network among all client hosts. The overlay guarantees a trusted sharing of zero-knowledge proofs for aggregation integrity, even under malicious Byzantine attacks. We tested over popular datasets, Fashion-MNIST and CIFAR10, to prove the new system protection concept. Our benchmark experiments validate the claimed advantages of the ZKFL scheme in all objective functions. Our aggregation method can be applied to secure both rank-based and similarity-based aggregation schemes. For a large system with over 200 clients, our system takes only 3 seconds to yield high-precision global machine models under the ALIE attacks with the Fashion-MNIST dataset. We have achieved up to 85% model accuracy, compared to only 3% $\sim$ 45% accuracy observed with federated schemes without protection. Moreover, our method demands a low memory overhead for handling zero-knowledge proofs as the system scales greatly to a larger number of client nodes.},
  archive      = {J_TPDS},
  author       = {Renwen Ma and Kai Hwang and Mo Li and Yiming Miao},
  doi          = {10.1109/TPDS.2024.3455762},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {2284-2296},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Trusted model aggregation with zero-knowledge proofs in federated learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Communication-efficient regret-optimal distributed online
convex optimization. <em>TPDS</em>, <em>35</em>(11), 2270–2283. (<a
href="https://doi.org/10.1109/TPDS.2024.3403883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online convex optimization in distributed systems has shown great promise in collaboratively learning on data streams with massive learners, such as in collaborative coordination in robot and IoT networks. When implemented in communication-constrained networks like robot and IoT networks, two critical yet distinct objectives in distributed online convex optimization (DOCO) are minimizing the overall regret and the communication cost. Achieving both objectives simultaneously is challenging, especially when the number of learners $n$ and learning time $T$ are prohibitively large. To address this challenge, we propose novel algorithms in typical adversarial and stochastic settings. Our algorithms significantly reduce the communication complexity of the algorithms with the state-of-the-art regret by a factor of $\mathcal {O}(n^{2})$ and $\tilde{\mathcal {O}}(\sqrt{nT})$ in adversarial and stochastic settings, respectively. We are the first to achieve nearly optimal regret and communication complexity simultaneously up to polylogarithmic factors. We validate our algorithms through experiments on real-world datasets in classification tasks. Our algorithms with appropriate parameters can achieve $90\%\sim 99\%$ communication saving with close accuracy over existing methods in most cases. The code is available at https://github.com/GGBOND121382/Communication-Efficient_Regret-Optimal_DOCO .},
  archive      = {J_TPDS},
  author       = {Jiandong Liu and Lan Zhang and Fengxiang He and Chi Zhang and Shanyang Jiang and Xiang-Yang Li},
  doi          = {10.1109/TPDS.2024.3403883},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {2270-2283},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Communication-efficient regret-optimal distributed online convex optimization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Freyr <span
class="math inline"><sup>+</sup></span>&lt;mml:math
xmlns:mml=“http://www.w3.org/1998/math/MathML”&gt;&lt;mml:msup&gt;&lt;mml:mrow/&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;:
Harvesting idle resources in serverless computing via deep reinforcement
learning. <em>TPDS</em>, <em>35</em>(11), 2254–2269. (<a
href="https://doi.org/10.1109/TPDS.2024.3462294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serverless computing has revolutionized online service development and deployment with ease-to-use operations, auto-scaling, fine-grained resource allocation, and pay-as-you-go pricing. However, a gap remains in configuring serverless functions—the actual resource consumption may vary due to function types, dependencies, and input data sizes, thus mismatching the static resource configuration by users. Dynamic resource consumption against static configuration may lead to either poor function execution performance or low utilization. This paper proposes Freyr $^+$ , a novel resource manager (RM) that dynamically harvests idle resources from over-provisioned functions to accelerate under-provisioned functions for serverless platforms. Freyr $^+$ monitors each function&#39;s resource utilization in real-time and detects the mismatches between user configuration and actual resource consumption. We design deep reinforcement learning (DRL) algorithms with attention-enhanced embedding, incremental learning, and safeguard mechanism for Freyr $^+$ to harvest idle resources safely and accelerate functions efficiently. We have implemented and deployed a Freyr $^+$ prototype in a 13-node Apache OpenWhisk cluster using AWS EC2. Freyr $^+$ is evaluated on both large-scale simulation and real-world testbed. Experimental results show that Freyr $^+$ harvests 38% of function invocations’ idle resources and accelerates 39% of invocations using harvested resources. Freyr $^+$ reduces the 99th-percentile function response latency by 26% compared to the baseline RMs.},
  archive      = {J_TPDS},
  author       = {Hanfei Yu and Hao Wang and Jian Li and Xu Yuan and Seung-Jong Park},
  doi          = {10.1109/TPDS.2024.3462294},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {2254-2269},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Freyr $^+$&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow/&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;: harvesting idle resources in serverless computing via deep reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accelerating communication-efficient federated multi-task
learning with personalization and fairness. <em>TPDS</em>,
<em>35</em>(11), 2239–2253. (<a
href="https://doi.org/10.1109/TPDS.2024.3411815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning techniques provide a promising framework for collaboratively training a machine learning model without sharing users’ data, and delivering a security solution to guarantee privacy during the model training of IoT devices. Nonetheless, challenges posed by data heterogeneity and communication resource constraints make it difficult to develop an efficient federated learning algorithm in terms of the low order of convergence rate. It could significantly deteriorate the quality of service for critical machine learning tasks, e.g., facial recognition, which requires an edge-ready, low-power, low-latency training algorithm. To address these challenges, a communication-efficient federated learning approach is proposed in this paper where the momentum technique is leveraged to accelerate the convergence rate while largely reducing the communication requirements. First, a federated multi-task learning framework by which the learning tasks are reformulated by the multi-objective optimization problem is introduced to address the data heterogeneity. The multiple gradient descent algorithm is harnessed to find the common gradient descending direction for all participants so that the common features can be learned and no sacrifice on each clients’ performance. Second, to reduce communication costs, a local momentum technique with global information is developed to speed up the convergence rate, where the convergence analysis of the proposed method under non-convex case is studied. It is proved that the proposed local momentum can actually achieve the same acceleration as the global momentum, whereas it is more robust than algorithms that solely rely on the acceleration by the global momentum. Third, the generalization of the proposed acceleration approach is investigated which is demonstrated by the accelerated variation of FedAvg. Finally, the performance of the proposed method on the learning model accuracy, convergence rate, and robustness to data heterogeneity, is investigated by empirical experiments on four public datasets, while a real-world IoT platform is constructed to demonstrate the communication efficiency of the proposed method.},
  archive      = {J_TPDS},
  author       = {Renyou Xie and Chaojie Li and Xiaojun Zhou and Zhaoyang Dong},
  doi          = {10.1109/TPDS.2024.3411815},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {2239-2253},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating communication-efficient federated multi-task learning with personalization and fairness},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient cross-cloud partial reduce with CREW.
<em>TPDS</em>, <em>35</em>(11), 2224–2238. (<a
href="https://doi.org/10.1109/TPDS.2024.3460185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By allowing $p$ out of $n$ workers to conduct all reduce operations among them for a round of synchronization, partial reduce , a promising partially-asynchronous variant of all reduce , has shown its power in alleviating the impacts of stragglers for iterative distributed machine learning (DML). Current partial reduce solutions are mainly designed for intra-cluster DML, in which workers are networked with high-bandwidth LAN links. Yet no prior work has looked into the problem of how to achieve efficient partial reduce for cross-cloud DML, where inter-worker connections are with scarcely-available capacities. To fill the gap, in this paper, we propose CREW , a flexible and efficient implementation of partial reduce for cross-cloud DML. At the high level, CREW is built upon the novel design of employing all active workers along with their internal connection capacities to execute the involved communication and computation tasks; and at the low level, CREW employs a suite of algorithms to distribute the tasks among workers in a load-balanced way, and deal with possible outages of workers/connections, and bandwidth contention. Detailed performance studies confirm that, CREW not only shortens the execution of each partial reduce operation, outperforming existing communication schemes such as PS, Ring, TopoAdopt , and BLINK greatly, but also significantly accelerates the training of large models, up to $15\times$ and $9\times$ , respectively, when compared with the all-to-all direct communication scheme and original partial reduce design.},
  archive      = {J_TPDS},
  author       = {Shouxi Luo and Renyi Wang and Ke Li and Huanlai Xing},
  doi          = {10.1109/TPDS.2024.3460185},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {2224-2238},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient cross-cloud partial reduce with CREW},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient inference for pruned CNN models on mobile devices
with holistic sparsity alignment. <em>TPDS</em>, <em>35</em>(11),
2208–2223. (<a href="https://doi.org/10.1109/TPDS.2024.3462092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many artificial intelligence applications based on convolutional neural networks are directly deployed on mobile devices to avoid network unavailability and user privacy leakage. However, the significant increase in model parameter volumes makes it difficult to achieve high-performance convolutional neural network inference on these mobile devices with limited computing power. Weight pruning is one of the main approaches to compress models by reducing model parameters and computational operations, which also introduces irregular sparsity of neural networks, leading to inefficient computation and memory access during inference. This work proposes an end-to-end framework, namely MCPruner, for efficient inference of pruned convolutional neural networks on mobile devices by aligning the sparse patterns with hardware execution features in computation, memory access, and parallelism. It first co-designs pruning methods and code generation optimizations for the alignment of non-zero weight count and vector width, to improve computational efficiency while ensuring accuracy. During the code generation, it applies a sparse pattern-aware format to reduce inefficient memory accesses. Besides, convolution computations are reordered for alignment, and then mapped to parallel threads on accelerated units to achieve high parallelism. Experimental results using several commonly used models and datasets on the ARM-based Hikey970 demonstrate that our work outperforms state-of-the-art methods in inference efficiency, with no accuracy degradation.},
  archive      = {J_TPDS},
  author       = {Yuyang Jin and Runxin Zhong and Saiqin Long and Jidong Zhai},
  doi          = {10.1109/TPDS.2024.3462092},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {2208-2223},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient inference for pruned CNN models on mobile devices with holistic sparsity alignment},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BIRD+: Design of a lightweight communication compressor for
resource-constrained distribution learning platforms. <em>TPDS</em>,
<em>35</em>(11), 2193–2207. (<a
href="https://doi.org/10.1109/TPDS.2024.3447221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Top-K sparsification-based compression framework is extensively explored for reducing communication costs in distributed learning. However, we identified several issues with existing Top-K sparsification-based compression methods: ( i ) The limited compressibility of the Top-K parameter&#39;s indexes critically restricts the overall communication compression ratio; ( ii ) Several time-consuming compression operations significantly offset the benefits of communication compression; ( iii ) The use of error feedback techniques to maintain model quality results in a high memory footprint consumption. To solve these issues, we propose BIRD, a lightweight tensor-wise Bi-Random sampling strategy with an expectation invariance property. Specifically, BIRD applies a tensor-wise index sharing mechanism that reduces the index proportion by allowing multiple tensor elements to share a single index, thus improving the overall compression ratio. Additionally, BIRD replaces the time-consuming Top-K sorting with a faster Bi-Random sampling strategy based on the aforementioned index sharing mechanism, significantly reducing compression overheads; Moreover, BIRD establishes an expectation invariance property into the Bi-Random sampling to ensure an approximate unbiased representation for the $L_1$ -norm of the sampled tensors, effectively maintaining the model quality without incurring extra memory costs. We further optimize BIRD to BIRD+ by introducing the uniform distribution-based sampling and Gamma correction on the tensor-wise sampling process, achieving a more flexibly adjustment of the sparsity with better convergence performance. Experimental evaluations across multiple conventional distributed learning tasks demonstrate that compared to state-of-the-art approaches, BIRD+ achieves higher communication compression ratios up to 36.2 $\times$ and higher computation throughput up to 149.6 $\times$ while maintaining the model quality without incurring extra memory costs.},
  archive      = {J_TPDS},
  author       = {Donglei Wu and Weihao Yang and Xiangyu Zou and Hao Feng and Dingwen Tao and Shiyi Li and Wen Xia and Binxing Fang},
  doi          = {10.1109/TPDS.2024.3447221},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {2193-2207},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {BIRD+: Design of a lightweight communication compressor for resource-constrained distribution learning platforms},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sophisticated orchestrating concurrent DLRM training on
CPU/GPU platform. <em>TPDS</em>, <em>35</em>(11), 2177–2192. (<a
href="https://doi.org/10.1109/TPDS.2024.3432620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems are essential to the operation of the majority of internet services, with Deep Learning Recommendation Models (DLRMs) serving as a crucial component. However, due to distinct computation, data access, and memory usage characteristics of recommendation models, the trainning of DLRMs may suffer from low resource utilization on prevalent heterogeneous CPU-GPU hardware platforms. Furthermore, as the majority of high-performance computing systems presently depend on multi-GPU computing nodes, the challenge of addressing low resource utilization becomes even more pronounced. Existing concurrent training solutions cannot be straightforwardly applied to DLRM due to various factors, such as insufficient fine-grained memory management and the lack of collaborative CPU-GPU scheduling. In this paper, we introduce RMixer, a scheduling framework that addresses these challenges by providing an efficient job management and scheduling mechanism for DLRM training jobs on heterogeneous CPU-GPU platforms. To facilitate training co-location, we first estimate the peak memory consumption of each job. Additionally, we track and collect resource utilization for DLRM training jobs. Based on the information of computational patterns, a batched job dispatcher with dynamic resource-complementary scheduling policy is proposed to co-locate DLRM training jobs on CPU-GPU platform. Scheduling strategies for both intra-GPU and inter-GPU scenarios were meticulously devised, with a focus on thoroughly examining individual GPU resource utilization and achieving a balanced state across multiple GPUs. Experimental results demonstrate that our implementation achieved up to 5.3× and 7.5× higher throughput on single GPU and 4 GPU respectively for training jobs involving various recommendation models.},
  archive      = {J_TPDS},
  author       = {Rui Tian and Jiazhi Jiang and Jiangsu Du and Dan Huang and Yutong Lu},
  doi          = {10.1109/TPDS.2024.3432620},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {2177-2192},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Sophisticated orchestrating concurrent DLRM training on CPU/GPU platform},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An evaluation framework for dynamic thermal management
strategies in 3D MultiProcessor system-on-chip co-design. <em>TPDS</em>,
<em>35</em>(11), 2161–2176. (<a
href="https://doi.org/10.1109/TPDS.2024.3459414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic thermal management (DTM) has been widely adopted to improve the energy efficiency, reliability, and performance of modern Multi-Processor SoCs (MPSoCs). However, the evolving industry trends and heterogeneous architecture designs have introduced significant challenges in state-of-the-art DTM methods. Specifically, the emergence of heterogeneous design has led to increased localized and non-uniform hotspots, necessitating accurate and responsive DTM strategies. Additionally, the increased number of cores to be managed requires the DTM to optimize and coordinate the whole system. However, existing methodologies fail in both precise thermal modeling in localized hotspots and fast architecture simulation. To tackle these existing challenges, we first introduce the latest version of 3D-ICE 3.1, with a novel non-uniform thermal modeling technique to support customized discretization levels of thermal grids. 3D-ICE 3.1 improves the accuracy of thermal analysis and reduces simulation overhead. Then, in conjunction with an efficient and fast offline application profiling strategy utilizing the architecture simulator gem5-X, we propose a novel DTM evaluation framework. This framework enables us to explore novel DTM methods to optimize the energy efficiency, reliability, and performance of contemporary 3D MPSoCs. The experimental results demonstrate that 3D-ICE 3.1 achieves high accuracy, with only 0.3K mean temperature error. Subsequently, we evaluate various DTM methods and propose a Multi-Agent Reinforcement Learning (MARL) control to address the demanding thermal challenges of 3D MPSoCs. Our experimental results show that the proposed DTM method based on MARL can reduce power consumption by 13% while maintaining a similar performance level to the comparison methods.},
  archive      = {J_TPDS},
  author       = {Darong Huang and Luis Costero and David Atienza},
  doi          = {10.1109/TPDS.2024.3459414},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {2161-2176},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An evaluation framework for dynamic thermal management strategies in 3D MultiProcessor system-on-chip co-design},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards efficient graph processing in geo-distributed data
centers. <em>TPDS</em>, <em>35</em>(11), 2147–2160. (<a
href="https://doi.org/10.1109/TPDS.2024.3453872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Iterative graph processing is widely used as a significant paradigm for large-scale data analysis. In many global businesses of multinational enterprises, graph-structure data is usually geographically distributed in different regions to support low-latency services. Geo-distributed graph processing suffers from the Wide Area Networks (WANs) with scarce and heterogeneous bandwidth, thus essentially differs from traditional distributed graph processing. In this paper, we propose RAGraph, a Region-Aware framework for geo-distributed graph processing . At the core of RAGraph, we design a region-aware graph processing framework that allows advancing inefficient global updates locally and enables sensible coordination-free message interactions and flexible replaceable communication module. In terms of graph data preprocessing, RAGraph introduces a contribution-driven edge migration algorithm to effectively utilize network resources. RAGraph also contains an adaptive hierarchical message interaction engine to switch interaction modes adaptively based on network heterogeneity and fluctuation, and a discrepancy-aware message filtering strategy to filter important messages. Experimental results show that RAGraph can achieve an average speedup of 9.7× (up to 98×) and an average WAN cost reduction of 78.5 $\%$ (up to 97.3 $\%$ ) compared with state-of-the-art systems.},
  archive      = {J_TPDS},
  author       = {Feng Yao and Qian Tao and Shengyuan Lin and Yanfeng Zhang and Wenyuan Yu and Shufeng Gong and Qiange Wang and Ge Yu and Jingren Zhou},
  doi          = {10.1109/TPDS.2024.3453872},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {2147-2160},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Towards efficient graph processing in geo-distributed data centers},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gamora: Learning-based buffer-aware preloading for adaptive
short video streaming. <em>TPDS</em>, <em>35</em>(11), 2132–2146. (<a
href="https://doi.org/10.1109/TPDS.2024.3456567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the emerging short video streaming applications have gained substantial attention. With the rapidly burgeoning demand for short video streaming services, maximizing their Quality of Experience (QoE) is an onerous challenge. Current video preloading algorithms cannot determine video preloading sequence decisions appropriately due to the impact of users’ swipes and bandwidth fluctuations. As a result, it is still ambiguous how to improve the overall QoE while mitigating bandwidth wastage to optimize short video streaming services. In this article, we devise Gamora, a buffer-aware short video streaming system to provide a high QoE of users. In Gamora, we first propose an unordered preloading algorithm that utilizes a Deep Reinforcement Learning (DRL) algorithm to make video preloading decisions. Then, we further devise an Asymmetric Imitation Learning (AIL) algorithm to guide the DRL-based preloading algorithm, which enables the agent to learn from expert demonstrations for fast convergence. Finally, we implement our proposed short video streaming system prototype and evaluate the performance of Gamora on various real-world network datasets. Our results demonstrate that Gamora significantly achieves QoE improvement by 28.7%–51.4% compared to state-of-the-art algorithms, while mitigating bandwidth wastage by 40.7%–83.2% without sacrificing video quality.},
  archive      = {J_TPDS},
  author       = {Biao Hou and Song Yang and Fan Li and Liehuang Zhu and Lei Jiao and Xu Chen and Xiaoming Fu},
  doi          = {10.1109/TPDS.2024.3456567},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {2132-2146},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Gamora: Learning-based buffer-aware preloading for adaptive short video streaming},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DeepCAT+: A low-cost and transferrable online configuration
auto-tuning approach for big data frameworks. <em>TPDS</em>,
<em>35</em>(11), 2114–2131. (<a
href="https://doi.org/10.1109/TPDS.2024.3459889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data frameworks usually provide a large number of performance-related parameters. Online auto-tuning these parameters based on deep reinforcement learning (DRL) to achieve a better performance has shown their advantages over search-based and machine learning-based approaches. Unfortunately, the time cost during the online tuning phase of conventional DRL-based methods is still heavy, especially for Big Data applications. Therefore, in this paper, we propose DeepCAT $^+$ , a low-cost and transferrable deep reinforcement learning-based approach to achieve online configuration auto-tuning for Big Data frameworks. To reduce the total online tuning cost and increase the adaptability: 1) DeepCAT $^+$ utilizes the TD3 algorithm instead of DDPG to alleviate value overestimation; 2) DeepCAT $^+$ modifies the conventional experience replay to fully utilize the rare but valuable transitions via a novel reward-driven prioritized experience replay mechanism; 3) DeepCAT $^+$ designs a Twin-Q Optimizer to estimate the execution time of each action without the costly configuration evaluation and optimize the sub-optimal ones to achieve a low-cost exploration-exploitation tradeoff; 4) Furthermore, DeepCAT $^+$ also implements an Online Continual Learner module based on Progressive Neural Networks to transfer knowledge from historical tuning experiences. Experimental results based on a lab Spark cluster with HiBench benchmark applications show that DeepCAT $^+$ is able to speed up the best execution time by a factor of 1.49×, 1.63× and 1.65× on average respectively over the baselines, while consuming up to 50.08%, 53.39% and 70.79% less total tuning time. In addition, DeepCAT $^+$ also has a strong adaptability to the time-varying environment of Big Data frameworks.},
  archive      = {J_TPDS},
  author       = {Hui Dou and Yilun Wang and Yiwen Zhang and Pengfei Chen and Zibin Zheng},
  doi          = {10.1109/TPDS.2024.3459889},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {2114-2131},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DeepCAT+: A low-cost and transferrable online configuration auto-tuning approach for big data frameworks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FedVeca: Federated vectorized averaging on non-IID data with
adaptive bi-directional global objective. <em>TPDS</em>,
<em>35</em>(11), 2102–2113. (<a
href="https://doi.org/10.1109/TPDS.2024.3454203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is a distributed machine learning framework in parallel and distributed systems. However, the systems’ Non-Independent and Identically Distributed (Non-IID) data negatively affect the communication efficiency, since clients with different datasets may cause significant gaps to the local gradients in each communication round. In this article, we propose a Federated Vectorized Averaging (FedVeca) method to optimize the FL communication system on Non-IID data. Specifically, we set a novel objective for the global model which is related to the local gradients. The local gradient is defined as a bi-directional vector with step size and direction, where the step size is the number of local updates and the direction is divided into positive and negative according to our definition. In FedVeca, the direction is influenced by the step size, thus we average the bi-directional vectors to reduce the effect of different step sizes. Then, we theoretically analyze the relationship between the step sizes and the global objective, and obtain upper bounds on the step sizes per communication round. Based on the upper bounds, we design an algorithm for the server and the client to adaptively adjusts the step sizes that make the objective close to the optimum. Finally, we conduct experiments on different datasets, models and scenarios by building a prototype system, and the experimental results demonstrate the effectiveness and efficiency of the FedVeca method.},
  archive      = {J_TPDS},
  author       = {Ping Luo and Jieren Cheng and N. Xiong and Zhenhao Liu and Jie Wu},
  doi          = {10.1109/TPDS.2024.3454203},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {2102-2113},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FedVeca: Federated vectorized averaging on non-IID data with adaptive bi-directional global objective},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed evolution strategies with multi-level learning
for large-scale black-box optimization. <em>TPDS</em>, <em>35</em>(11),
2087–2101. (<a href="https://doi.org/10.1109/TPDS.2024.3437688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the post-Moore era, main performance gains of black-box optimizers are increasingly depending on parallelism, especially for large-scale optimization (LSO). Here we propose to parallelize the well-established covariance matrix adaptation evolution strategy (CMA-ES) and in particular its one latest LSO variant called limited-memory CMA-ES (LM-CMA). To achieve efficiency while approximating its powerful invariance property, we present a multilevel learning-based meta-framework for distributed LM-CMA. Owing to its hierarchically organized structure, Meta-ES is well-suited to implement our distributed meta-framework, wherein the outer-ES controls strategy parameters while all parallel inner-ESs run the serial LM-CMA with different settings. For the distribution mean update of the outer-ES, both the elitist and multi-recombination strategy are used in parallel to avoid stagnation and regression, respectively. To exploit spatiotemporal information, the global step-size adaptation combines Meta-ES with the parallel cumulative step-size adaptation. After each isolation time, our meta-framework employs both the structure and parameter learning strategy to combine aligned evolution paths for CMA reconstruction. Experiments on a set of large-scale benchmarking functions with memory-intensive evaluations, arguably reflecting many data-driven optimization problems, validate the benefits (e.g., effectiveness w.r.t. solution quality, and adaptability w.r.t. second-order learning) and costs of our meta-framework.},
  archive      = {J_TPDS},
  author       = {Qiqi Duan and Chang Shao and Guochen Zhou and Minghan Zhang and Qi Zhao and Yuhui Shi},
  doi          = {10.1109/TPDS.2024.3437688},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {2087-2101},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Distributed evolution strategies with multi-level learning for large-scale black-box optimization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CPLNS: Cooperative parallel large neighborhood search for
large-scale multi-agent path finding. <em>TPDS</em>, <em>35</em>(11),
2069–2086. (<a href="https://doi.org/10.1109/TPDS.2024.3408030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The large-scale Multi-Agent Path Finding (MAPF) problem presents a significant challenge in combinatorial optimization. Currently, one of the advanced, near-optimal algorithms is Large Neighborhood Search (LNS), which can handle instances with thousands of agents. Although a basic portfolio parallel search based on multiple independent LNS solvers enhances speed and robustness, it encounters scalability issues with increasing CPU cores. To address this limitation, we propose the Cooperative Parallel LNS (CPLNS) algorithm, aimed at boosting parallel efficiency. The main challenge in cooperative parallel search lies in designing suitable portfolio and cooperative strategies that balance search diversification and intensification. To address this, we first analyze the characteristics of LNS. We then introduce a flexible group-based cooperative parallel strategy, where the current best solution is shared within each group to aid intensification, while maintaining diversification through independent group computations. Furthermore, we augment search diversification by integrating a simulated annealing-based LNS and bounded suboptimal single-agent pathfinding. We also introduce a rule-based methodology for portfolio construction to simplify parameter settings and improve search efficiency. Finally, we enhance communication and memory efficiency through a shared data filtering technique and optimized data structures. In benchmarks on 33 maps with 825 instances, CPLNS achieved a median speedup of 21.95 on a 32-core machine, solving 96.97% of cases within five minutes and reducing the average suboptimality score from 1.728 to 1.456. Additionally, tests with up to 10,000 agents verify CPLNS&#39;s scalability for large-scale MAPF problems.},
  archive      = {J_TPDS},
  author       = {Kai Chen and Qingjun Qu and Feng Zhu and Zhengming Yi and Wenjie Tang},
  doi          = {10.1109/TPDS.2024.3408030},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {2069-2086},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CPLNS: Cooperative parallel large neighborhood search for large-scale multi-agent path finding},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy-preserving data selection for horizontal and
vertical federated learning. <em>TPDS</em>, <em>35</em>(11), 2054–2068.
(<a href="https://doi.org/10.1109/TPDS.2024.3439709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) enables distributed participants to collaboratively train a machine learning model without accessing to their local data. In FL systems, the selection of training samples has a significant impact on model performances, e.g., selecting participants whose datasets have low-quality samples, features would result in low accuracy, unstable models. In this work, we aim to solve the problem that selects a collection of high-quality training samples for a given FL task under a monetary budget. We propose a holistic design to efficiently select high-quality samples while preserve the privacy of participants’ local data, the server’s label set. We propose an efficient hierarchical sample selection mechanism to select relevant clients, their samples before training for horizontal federated learning (HFL). It uses the determinantal point process (DPP) to select both the statistical homogenous, content diverse clients, samples. Besides, we propose a private set intersection (PSI) based scheme to filter relevant features for the target VFL task. Finally, during training, an erroneous-aware importance based selection is proposed to dynamically select important clients, samples to accelerate model convergence. We verify the merits of our proposed solution with extensive experiments on a real AIoT system with 50 clients. The experimental results validate that our solution achieves accurate, efficient selection of high-quality data, consequently an FL model with a faster convergence speed, higher accuracy.},
  archive      = {J_TPDS},
  author       = {Lan Zhang and Anran Li and Hongyi Peng and Feng Han and Fan Huang and Xiang-Yang Li},
  doi          = {10.1109/TPDS.2024.3439709},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {2054-2068},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Privacy-preserving data selection for horizontal and vertical federated learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy preserving task push in spatial crowdsourcing with
unknown popularity. <em>TPDS</em>, <em>35</em>(11), 2039–2053. (<a
href="https://doi.org/10.1109/TPDS.2024.3434978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the privacy-preserving task push problem with unknown popularity in Spatial Crowdsourcing (SC), where the platform needs to select some tasks with unknown popularity and push them to workers. Meanwhile, the preferences of workers and the popularity values of tasks might involve some sensitive information, which should be protected from disclosure. To address these concerns, we propose a Privacy Preserving Auction-based Bandit scheme, termed PPAB. Specifically, on the basis of the Combinatorial Multi-armed Bandit (CMAB) game, we first construct a Differentially Private Auction-based CMAB (DPA-CMAB) model. Under the DPA-CMAB model, we design a privacy-preserving arm-pulling policy based on Diffie-Hellman (DH), Differential Privacy (DP), and upper confidence bound, which includes the DH-based encryption mechanism and the hybrid DP-based protection mechanism. The policy not only can learn the popularity of tasks and make online task push decisions, but also can protect the popularity as well as workers’ preferences from being revealed. Meanwhile, we design an auction-based incentive mechanism to determine the payment for each selected task. Furthermore, we conduct an in-depth analysis of the security and online performance of PPAB, and prove that PPAB satisfies some desired properties (i.e., truthfulness, individual rationality, and computational efficiency). Finally, the significant performance of PPAB is confirmed through extensive simulations on the real-world dataset.},
  archive      = {J_TPDS},
  author       = {Yin Xu and Mingjun Xiao and Jie Wu and He Sun},
  doi          = {10.1109/TPDS.2024.3434978},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {2039-2053},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Privacy preserving task push in spatial crowdsourcing with unknown popularity},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SC-CGRA: An energy-efficient CGRA using stochastic
computing. <em>TPDS</em>, <em>35</em>(11), 2023–2038. (<a
href="https://doi.org/10.1109/TPDS.2024.3453310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic Computing (SC) offers a promising computing paradigm for low-power and cost-effective applications, with the added advantage of high error tolerance. In parallel, Coarse-Grained Reconfigurable Arrays (CGRA) prove to be a highly promising platform for domain-specific applications due to their combination of energy efficiency and flexibility. Intuitively, introducing SC to CGRA would significantly reinforce the strengths of both paradigms. However, existing SC-based architectures often encounter inherent computation errors, while the stochastic number generators employed in SC result in exponentially growing latency, which is deemed unacceptable in CGRA. In this work, we propose an SC-based CGRA by replacing the exact multiplication in traditional CGRA with an SC-based multiplication. To improve the accuracy of SC and shorten the latency of Stochastic Number Generators (SNG), we introduce the leading zero shifting and comparator truncation, while keeping the length of bitstream fixed. In addition, due to the flexible interconnections among PEs, we propose a quality scaling strategy that combines neighbor PEs to achieve high-accuracy operations without switching costs like power-gating. Compared to the state-of-the-art approximate computing design of CGRA, our proposed CGRA can averagely achieve a 65.3% reduction in output error while having a 21.2% reduction in energy consumption and a noteworthy 28.37% area savings.},
  archive      = {J_TPDS},
  author       = {Di Mou and Bo Wang and Dajiang Liu},
  doi          = {10.1109/TPDS.2024.3453310},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {2023-2038},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SC-CGRA: An energy-efficient CGRA using stochastic computing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CODE<span
class="math inline"><sup>+</sup></span>&lt;mml:math
xmlns:mml=“http://www.w3.org/1998/math/MathML”&gt;&lt;mml:msup&gt;&lt;mml:mrow/&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;:
Fast and accurate inference for compact distributed IoT data collection.
<em>TPDS</em>, <em>35</em>(11), 2006–2022. (<a
href="https://doi.org/10.1109/TPDS.2024.3453607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In distributed IoT data systems, full-size data collection is impractical due to the energy constraints and large system scales. Our previous work has investigated the advantages of integrating matrix sampling and inference for compact distributed IoT data collection, to minimize the data collection cost while guaranteeing the data benefits. This paper further advances the technology by boosting fast and accurate inference for those distributed IoT data systems that are sensitive to computation time, training stability, and inference accuracy. Particularly, we propose CODE$^{+}$&lt;mml:math&gt;&lt;mml:msup&gt;&lt;mml:mrow/&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt; , i.e., C ompact Distributed I O T D ata Coll E ction Plus, which features a cluster-based sampling module and a Convolutional Neural Network (CNN)-Transformer Autoencoders-based inference module, to reduce cost and guarantee the data benefits. The sampling component employs a cluster-based matrix sampling approach, in which data clustering is first conducted and then a two-step sampling is performed in accordance with the number of clusters and clustering errors. The inference component integrates a CNN-Transformer Autoencoders-based matrix inference model to estimate the full-size spatio-temporal data matrix, which consists of a CNN-Transformer encoder that extracts the underlying features from the sampled data matrix and a lightweight decoder that maps the learned latent features back to the original full-size data matrix. We implement CODE$^{+}$&lt;mml:math&gt;&lt;mml:msup&gt;&lt;mml:mrow/&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt; under three operational large-scale IoT systems and one synthetic Gaussian distribution dataset, and extensive experiments are provided to demonstrate its efficiency and robustness. With a 20% sampling ratio, CODE$^{+}$&lt;mml:math&gt;&lt;mml:msup&gt;&lt;mml:mrow/&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt; achieves an average data reconstruction accuracy of 94% across four datasets, outperforming our previous version of 87% and state-of-the-art baseline of 71%.},
  archive      = {J_TPDS},
  author       = {Huali Lu and Feng Lyu and Ju Ren and Huaqing Wu and Conghao Zhou and Zhongyuan Liu and Yaoxue Zhang and Xuemin Shen},
  doi          = {10.1109/TPDS.2024.3453607},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {2006-2022},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CODE$^{+}$&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow/&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;: fast and accurate inference for compact distributed IoT data collection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ComboFunc: Joint resource combination and container
placement for serverless function scaling with heterogeneous container.
<em>TPDS</em>, <em>35</em>(11), 1989–2005. (<a
href="https://doi.org/10.1109/TPDS.2024.3454071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serverless computing provides developers with a maintenance-free approach to resource usage, but it also transfers resource management responsibility to the cloud platform. However, the fine granularity of serverless function resources can lead to performance bottlenecks and resource fragmentation on nodes when creating many function containers. This poses challenges in effectively scaling function resources and optimizing node resource allocation, hindering overall agility. To address these challenges, we have introduced ComboFunc, an innovative resource scaling system for serverless platforms. ComboFunc associates function with heterogeneous containers of varying specifications and optimizes their resource combination and placement. This approach not only selects appropriate nodes for container creation, but also leverages the new feature of Kubernetes In-place Pod Vertical Scaling to enhance resource scaling agility and efficiency. By allowing a single function to correspond to heterogeneous containers with varying resource specifications and providing the ability to modify the resource specifications of existing containers in place, ComboFunc effectively utilizes fragmented resources on nodes. This, in turn, enhances the overall resource utilization of the entire cluster and improves scaling agility. We also model the problem of combining and placing heterogeneous containers as an NP-hard problem and design a heuristic solution based on a greedy algorithm that solves it in polynomial time. We implemented a prototype of ComboFunc on the Kubernetes platform and conducted experiments using real traces on a local cluster. The results demonstrate that, compared to existing strategies, ComboFunc achieves up to 3.01 × faster function resource scaling and reduces resource costs by up to 42.6%.},
  archive      = {J_TPDS},
  author       = {Zhaojie Wen and Qiong Chen and Quanfeng Deng and Yipei Niu and Zhen Song and Fangming Liu},
  doi          = {10.1109/TPDS.2024.3454071},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {1989-2005},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ComboFunc: Joint resource combination and container placement for serverless function scaling with heterogeneous container},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring the design space of distributed parallel sparse
matrix-multiple vector multiplication. <em>TPDS</em>, <em>35</em>(11),
1977–1988. (<a href="https://doi.org/10.1109/TPDS.2024.3452478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the distributed memory parallel multiplication of a sparse matrix by a dense matrix (SpMM). The dense matrix is often a collection of dense vectors. Standard implementations will multiply the sparse matrix by multiple dense vectors at the same time, to exploit the computational efficiencies therein. But such approaches generally utilize the same sparse matrix partitioning as if multiplying by a single vector. This article explores the design space of parallelizing SpMM and shows that a coarser grain partitioning of the matrix combined with a column-wise partitioning of the block of vectors can often require less communication volume and achieve higher SpMM performance. An algorithm is presented that chooses a process grid geometry for a given number of processes to optimize the performance of parallel SpMM. The algorithm can augment existing graph partitioners by utilizing the additional concurrency available when multiplying by multiple dense vectors to further reduce communication.},
  archive      = {J_TPDS},
  author       = {Hua Huang and Edmond Chow},
  doi          = {10.1109/TPDS.2024.3452478},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {1977-1988},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Exploring the design space of distributed parallel sparse matrix-multiple vector multiplication},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-throughput GPU implementation of dilithium post-quantum
digital signature. <em>TPDS</em>, <em>35</em>(11), 1964–1976. (<a
href="https://doi.org/10.1109/TPDS.2024.3453289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital signatures are fundamental building blocks in various protocols to provide integrity and authenticity. The development of the quantum computing has raised concerns about the security guarantees afforded by classical signature schemes. CRYSTALS-Dilithium is an efficient post-quantum digital signature scheme based on lattice cryptography and has been selected as the primary algorithm for standardization by the National Institute of Standards and Technology. In this work, we present a high-throughput GPU implementation of Dilithium. For individual operations, we employ a range of computational and memory optimizations to overcome sequential constraints, reduce memory usage and IO latency, address bank conflicts, and mitigate pipeline stalls. This results in high and balanced compute throughput and memory throughput for each operation. In terms of concurrent task processing, we leverage task-level batching to fully utilize parallelism and implement a memory pool mechanism for rapid memory access. We propose a dynamic task scheduling mechanism to improve multiprocessor occupancy and significantly reduce execution time. Furthermore, we apply asynchronous computing and launch multiple streams to hide data transfer latencies and maximize the computing capabilities of both CPU and GPU. Across all three security levels, our GPU implementation achieves over 160× speedups for signing and over 80× speedups for verification on both commercial and server-grade GPUs. This achieves microsecond-level amortized execution times for each task, offering a high-throughput and quantum-resistant solution suitable for a wide array of applications in real systems.},
  archive      = {J_TPDS},
  author       = {Shiyu Shen and Hao Yang and Wangchen Dai and Hong Zhang and Zhe Liu and Yunlei Zhao},
  doi          = {10.1109/TPDS.2024.3453289},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {1964-1976},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {High-throughput GPU implementation of dilithium post-quantum digital signature},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beyond belady to attain a seemingly unattainable byte miss
ratio for content delivery networks. <em>TPDS</em>, <em>35</em>(11),
1949–1963. (<a href="https://doi.org/10.1109/TPDS.2024.3452096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reducing the byte miss ratio (BMR) in the Content Delivery Network (CDN) caches can help providers save on the cost of paying for traffic. When evicting objects or files of different sizes in the caches of CDNs, it is no longer sufficient to pursue an optimal object miss ratio (OMR) by approximating Belady to ensure an optimal BMR. Our experimental observations suggest that there are multiple request sequence windows. In these windows, a replacement policy prioritizes the eviction of objects with large sizes and ultimately evicts the object with the longest reuse distance, lowering the BMR without increasing the OMR. To accurately capture those windows, we monitor the changes in OMR and BMR using a deep reinforcement learning (RL) model and then implement a BMR-friendly replacement algorithm in these windows. Based on this policy, we propose a Belady and Size Eviction (LRU-BaSE) algorithm that reduces BMR while maintaining OMR. To make LRU-BaSE efficient and practical, we address the feedback delay problem of RL with a two-pronged approach. On the one hand, we shorten the LRU-base decision region based on the observation that the rear section of the cache queue contains most of the eviction candidates. On the other hand, the request distribution on CDNs makes it feasible to divide the learning region into multiple sub-regions that are each learned with reduced time and increased accuracy. In real CDN systems, LRU-BaSE outperforms LRU by reducing “backing to OS” traffic and access latency by 30.05% and 17.07%, respectively, on average. In simulator tests, LRU-BaSE outperforms state-of-the-art cache replacement policies. On average, LRU-BaSE&#39;s BMR is 0.63% and 0.33% less than that of Belady and Practical Flow-based Offline Optimal (PFOO), respectively. In addition, compared to Learning Relaxed Belady (LRB), LRU-BaSE can yield relatively stable performance when facing workload drift.},
  archive      = {J_TPDS},
  author       = {Peng Wang and Hong Jiang and Yu Liu and Zhelong Zhao and Ke Zhou and Zhihai Huang},
  doi          = {10.1109/TPDS.2024.3452096},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {1949-1963},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Beyond belady to attain a seemingly unattainable byte miss ratio for content delivery networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Logical synchrony and the bittide mechanism. <em>TPDS</em>,
<em>35</em>(11), 1936–1948. (<a
href="https://doi.org/10.1109/TPDS.2024.3444739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce logical synchrony, a framework that allows distributed computing to be coordinated as tightly as in synchronous systems without the distribution of a global clock or any reference to universal time. We develop a model of events called a logical synchrony network, in which nodes correspond to processors and every node has an associated local clock which generates the events. We construct a measure of logical latency and develop its properties. A further model, called a multiclock network, is then analyzed and shown to be a refinement of the logical synchrony network. We present the bittide mechanism as an instantiation of multiclock networks, and discuss the clock control mechanism that ensures that buffers do not overflow or underflow. Finally we give conditions under which a logical synchrony network has an equivalent synchronous realization.},
  archive      = {J_TPDS},
  author       = {Sanjay Lall and Călin Caşcaval and Martin Izzard and Tammo Spalink},
  doi          = {10.1109/TPDS.2024.3444739},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {1936-1948},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Logical synchrony and the bittide mechanism},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DeepTM: Efficient tensor management in heterogeneous memory
for DNN training. <em>TPDS</em>, <em>35</em>(11), 1920–1935. (<a
href="https://doi.org/10.1109/TPDS.2024.3431910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have gained widespread adoption in diverse fields, including image classification, object detection, and natural language processing. However, training large-scale DNN models often encounters significant memory bottlenecks, which ask for efficient management of extensive tensors. Heterogeneous memory system, which combines persistent memory (PM) modules with traditional DRAM, offers an economically viable solution to address tensor management challenges during DNN training. However, existing memory management methods on heterogeneous memory systems often lead to low PM access efficiency, low bandwidth utilization, and incomplete analysis of model characteristics. To overcome these hurdles, we introduce an efficient tensor management approach, DeepTM, tailored for heterogeneous memory to alleviate memory bottlenecks during DNN training. DeepTM employs page-level tensor aggregation to enhance PM read and write performance and executes contiguous page migration to increase memory bandwidth. Through an analysis of tensor access patterns and model characteristics, we quantify the overall performance and transform the performance optimization problem into the framework of Integer Linear Programming. Additionally, we achieve tensor heat recognition by dynamically adjusting the weights of four key tensor characteristics and develop a global optimization strategy using Deep Reinforcement Learning. To validate the efficacy of our approach, we implement and evaluate DeepTM, utilizing the TensorFlow framework running on a PM-based heterogeneous memory system. The experimental results demonstrate that DeepTM achieves performance improvements of up to 36% and 49% compared to the current state-of-the-art memory management strategies AutoTM and Sentinel, respectively. Furthermore, our solution reduces the overhead by 18 times and achieves up to 29% cost reduction compared to AutoTM.},
  archive      = {J_TPDS},
  author       = {Haoran Zhou and Wei Rang and Hongyang Chen and Xiaobo Zhou and Dazhao Cheng},
  doi          = {10.1109/TPDS.2024.3431910},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {1920-1935},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DeepTM: Efficient tensor management in heterogeneous memory for DNN training},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Redundancy-free and load-balanced TGNN training with
hierarchical pipeline parallelism. <em>TPDS</em>, <em>35</em>(11),
1904–1919. (<a href="https://doi.org/10.1109/TPDS.2024.3432855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Temporal Graph Neural Networks (TGNNs), as an extension of Graph Neural Networks, have demonstrated remarkable effectiveness in handling dynamic graph data. Distributed TGNN training requires efficiently tackling temporal dependency, which often leads to excessive cross-device communication that generates significant redundant data. However, existing systems are unable to remove the redundancy in data reuse and transfer, and suffer from severe communication overhead in a distributed setting. This work introduces Sven, a co-designed algorithm-system library aimed at accelerating TGNN training on a multi-GPU platform. Exploiting dependency patterns of TGNN models, we develop a redundancy-free graph organization to mitigate redundant data transfer. Additionally, we investigate communication imbalance issues among devices and formulate the graph partitioning problem as minimizing the maximum communication balance cost, which is proved to be an NP-hard problem. We propose an approximation algorithm called Re-FlexBiCut to tackle this problem. Furthermore, we incorporate prefetching, adaptive micro-batch pipelining, and asynchronous pipelining to present a hierarchical pipelining mechanism that mitigates the communication overhead. Sven represents the first comprehensive optimization solution for scaling memory-based TGNN training. Through extensive experiments conducted on a 64-GPU cluster, Sven demonstrates impressive speedup, ranging from 1.9x to 3.5x, compared to State-of-the-Art approaches. Additionally, Sven achieves up to 5.26x higher communication efficiency and reduces communication imbalance by up to 59.2%.},
  archive      = {J_TPDS},
  author       = {Yaqi Xia and Zheng Zhang and Donglin Yang and Chuang Hu and Xiaobo Zhou and Hongyang Chen and Qianlong Sang and Dazhao Cheng},
  doi          = {10.1109/TPDS.2024.3432855},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {1904-1919},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Redundancy-free and load-balanced TGNN training with hierarchical pipeline parallelism},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Opca: Enabling optimistic concurrent access for multiple
users in oblivious data storage. <em>TPDS</em>, <em>35</em>(11),
1891–1903. (<a href="https://doi.org/10.1109/TPDS.2024.3441623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenges of data privacy and security posed by data outsourcing are becoming increasingly prevalent. Oblivious RAM (ORAM)-based oblivious data storage guarantees data confidentiality through data encryption and access pattern obfuscation. However, it suffers from performance degradation and low throughput. To address these issues, the concurrency of ORAM in a multi-user scenario has been explored. We investigate several existing concurrent oblivious data storage solutions and discover that a trusted proxy is used to serve concurrent accesses between users and storage, with processing locks involved in the proxy to ensure correctness and prevent conflicts. The proxy-based system is inherently prone to pessimistic concurrency control, and as the number of users grows, a proxy might become a performance bottleneck, causing significant delays. In this study, we propose Opca, a novel oblivious data storage framework that enables optimistic concurrent access. Opca refines the proxy design by temporally storing multiple versions of modified data with labeled timestamps, committing only the latest version to the storage during a separate processing period. Opca is implemented and evaluated in different real-world storage backends with a scalable number of users, and its performance is compared to alternative schemes. Opca outperforms the state-of-the-art concurrent oblivious storage system TaoStore, which relies on a similar system setting. Our results show that Opca can improve 3.77x throughput and reduce 73.5% response time.},
  archive      = {J_TPDS},
  author       = {Yuezhi Che and Dazhao Cheng and Xiao Wang and Rujia Wang},
  doi          = {10.1109/TPDS.2024.3441623},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {1891-1903},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Opca: Enabling optimistic concurrent access for multiple users in oblivious data storage},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SR-FDIL: Synergistic replay for federated domain-incremental
learning. <em>TPDS</em>, <em>35</em>(11), 1879–1890. (<a
href="https://doi.org/10.1109/TPDS.2024.3436874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is to allow multiple clients to collaboratively train a model while keeping their data locally. However, existing FL approaches typically assume that the data in each client is static and fixed, which cannot account for incremental data with domain shift, leading to catastrophic forgetting on previous domains, particularly when clients are common edge devices that may lack enough storage to retain full samples of each domain. To tackle this challenge, we propose F ederated D omain- I ncremental L earning via S ynergistic R eplay (SR-FDIL), which alleviates catastrophic forgetting by coordinating all clients to cache samples and replay them. More specifically, when new data arrives, each client selects the cached samples based not only on their importance in the local dataset but also on their correlation with the global dataset. Moreover, to achieve a balance between learning new data and memorizing old data, we propose a novel client selection mechanism by jointly considering the importance of both old and new data. We conducted extensive experiments on several datasets of which the results demonstrate that SR-FDIL outperforms state-of-the-art methods by up to 4.05% in terms of average accuracy of all domains.},
  archive      = {J_TPDS},
  author       = {Yichen Li and Wenchao Xu and Yining Qi and Haozhao Wang and Ruixuan Li and Song Guo},
  doi          = {10.1109/TPDS.2024.3436874},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {11},
  number       = {11},
  pages        = {1879-1890},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SR-FDIL: Synergistic replay for federated domain-incremental learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Proteus: Simulating the performance of distributed DNN
training. <em>TPDS</em>, <em>35</em>(10), 1867–1878. (<a
href="https://doi.org/10.1109/TPDS.2024.3443255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DNN models are becoming increasingly larger to achieve unprecedented accuracy, and the accompanying increased computation and memory requirements necessitate the employment of massive clusters and elaborate parallelization strategies to accelerate DNN training. In order to better optimize the performance and analyze the cost, it is indispensable to model the training throughput of distributed DNN training. However, complex parallelization strategies and the resulting complex runtime behaviors make it challenging to construct an accurate performance model. In this article, we present Proteus, the first standalone simulator to model the performance of complex parallelization strategies through simulation execution. Proteus first models complex parallelization strategies with a unified representation named Strategy Tree . Then, it compiles the strategy tree into a distributed execution graph and simulates the complex runtime behaviors, comp-comm overlap and bandwidth sharing , with a H ierarchical T opo- A ware E xecutor ( HTAE ). We finally evaluate Proteus across a wide variety of DNNs on three hardware configurations. Experimental results show that Proteus achieves 3.0% average prediction error and preserves order for training throughput of various parallelization strategies. Compared to state-of-the-art approaches, Proteus reduces prediction error by up to 133.8%.},
  archive      = {J_TPDS},
  author       = {Jiangfei Duan and Xiuhong Li and Ping Xu and Xingcheng Zhang and Shengen Yan and Yun Liang and Dahua Lin},
  doi          = {10.1109/TPDS.2024.3443255},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  number       = {10},
  pages        = {1867-1878},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Proteus: Simulating the performance of distributed DNN training},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Paired many-to-many 2-disjoint path covers in meshes.
<em>TPDS</em>, <em>35</em>(10), 1854–1866. (<a
href="https://doi.org/10.1109/TPDS.2024.3445283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the paired many-to-many $k$ -disjoint path cover ( $k$ -DPC) problem, given a set of $k$ pairs of vertices $(s_{i},t_{i})$ , $1\leqslant i\leqslant k$ , of a graph $G$ we want to find $k$ simple vertex-disjoint paths whose end-vertices are these $k$ pairs, such that each vertex of $G$ is covered by a path. This problem is a well-known problem in parallel processing and is a generalization of the well-known Hamiltonian $(s,t)$ -path problem, which is equal to 1-DPC. In this paper, we consider the paired many-to-many 2-disjoint path cover problem (2-DPC) in meshes (rectangular grids). We give the necessary conditions for existence of such covers, and present a linear-time algorithm to compute them. Although the paired many-to-many $k$ -disjoint path cover problem is well-known in parallel processing, our motivation to study this problem is its application in solving the Hamiltonian path problem in solid grid graphs. We consider the case where the pairs of vertices are on the outer face of the graph.},
  archive      = {J_TPDS},
  author       = {Fatemeh Keshavarz-Kohjerdi},
  doi          = {10.1109/TPDS.2024.3445283},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  number       = {10},
  pages        = {1854-1866},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Paired many-to-many 2-disjoint path covers in meshes},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SSRAID: A stripe-queued and stripe-threaded merging i/o
strategy to improve write performance of serial interface SSD RAID.
<em>TPDS</em>, <em>35</em>(10), 1841–1853. (<a
href="https://doi.org/10.1109/TPDS.2024.3443083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RAID (Redundant Array of Independent Disks) has been widely used to enhance read and write performance of existing storage systems. Existing software RAID do not fully utilize write performance of Serial interface SSDs (Solid State Drive). The most popular software RAID currently is Linux Multiple-Disks (MD), and the latest software RAID is StRAID. We observe that both of these software RAID methods lead to thread contention in multi-threaded mode, especially when applied to Serial interface SSDs. Multiple threads writing to same address can limit write performance. In this paper, we propose a stripe-queued and stripe-threaded merging I/O strategy. First, SSRAID segregates write requests across different stripes using a set of stripe-queues and stripe-threads to prevent interference between them. As a result, write thread contention in SSRAID is eliminated, allowing stripe-threads to maintain the highest efficiency of parallelism. Secondly, SSRAID can merge write requests from the same stripe-queue multiple times through stripe-thread, effectively reducing the number of additional write I/Os. Finally, SSRAID presents a stage buffer based on data merging. During partial stripe-write, write-induced read I/Os on the SSD are transformed into direct access to the stage buffer, effectively reducing write-induced read I/Os. Compared to StRAID, SSRAID improves average sequential write throughput by 86% and reduces average sequential write latency by 61% in the optimal case.},
  archive      = {J_TPDS},
  author       = {Peixuan Li and Ping Xie and Qiang Cao},
  doi          = {10.1109/TPDS.2024.3443083},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  number       = {10},
  pages        = {1841-1853},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SSRAID: A stripe-queued and stripe-threaded merging I/O strategy to improve write performance of serial interface SSD RAID},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FlexRaft: Exploiting flexible erasure coding for
minimum-cost consensus and fast recovery. <em>TPDS</em>,
<em>35</em>(10), 1826–1840. (<a
href="https://doi.org/10.1109/TPDS.2024.3443424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consensus protocols like Paxos and Raft provide data consistency and fault tolerance for distributed services. Log replication in these protocols can be supported by erasure coding, which incurs lower redundancy than full-copy replication and significantly saves network and storage costs for overall performance improvements. However, existing consensus protocols with erasure coding cannot achieve the minimum network and storage costs during log replication. We propose FlexRaft, which dynamically varies the coding scheme used in Raft based on the server status to always achieve the theoretically minimum redundancy ratio, while maintaining the same liveness as in Raft. To address the issue of an inconsistent coding scheme between the leader and its followers, we specify the prerequisite of overwriting a log entry and also allow the leader and its followers to exactly track the coding scheme being used. We further extend FlexRaft into FlexRaft+, which provides a different storage layout to vary the coding scheme through a novel technique called re-encoding-free replication, so as to enable fast server recovery. We prove that both FlexRaft and FlexRaft+ maintain Raft safety. We implement a prototype of FlexRaft and FlexRaft+, atop which we build a distributed key-value store to show its efficacy. Experiments on Alibaba Cloud show that FlexRaft achieves the theoretically minimum network and storage costs in practice, and reduces the commit latency by 44.51% and 19.37% compared with state-of-the-art CRaft and HRaft, respectively. FlexRaft+ further reduces the commit latency when the coding scheme is being varied and improves the server recovery performance.},
  archive      = {J_TPDS},
  author       = {Mi Zhang and Qihan Kang and Patrick P. C. Lee},
  doi          = {10.1109/TPDS.2024.3443424},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  number       = {10},
  pages        = {1826-1840},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FlexRaft: Exploiting flexible erasure coding for minimum-cost consensus and fast recovery},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Locality-preserving graph traversal with split live
migration. <em>TPDS</em>, <em>35</em>(10), 1810–1825. (<a
href="https://doi.org/10.1109/TPDS.2024.3436828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph models many real-world data like social, transportation, biology, and communication data. Hence, graph traversal including multi-hop or graph-walking queries has been the key operation atop graph stores. However, since different graph traversals may touch different sets of vertices, it is hard or even impossible to have a one-size-fits-all graph partitioning algorithm that preserves access locality for various graph traversal workloads. Meanwhile, prior shard-based migration faces a dilemma such that coarse-grained migration may incur more migration overhead over increased locality benefits, while fine-grained migration usually requires excessive metadata and incurs non-trivial maintenance costs. We present Pragh, an efficient locality-preserving live graph migration scheme for graph stores in the form of key-value pairs. The key idea of Pragh is a split migration model that only migrates values physically while retaining keys in the initial location. This allows fine-grained migration while avoiding the need to maintain excessive metadata. Pragh integrates an RDMA-friendly location cache from DrTM-KV to provide fully-localized access to migrated data and further makes a novel reuse of the cache replacement policy for lightweight monitoring. Pragh further supports evolving graphs through a check-and-forward mechanism to resolve the conflict between updates and migration of graph data. Evaluations on an 8-node RDMA-capable cluster (100 Gbps) using a representative graph traversal benchmark show that Pragh can increase the throughput by up to 19× and decrease the median latency by up to 94%, thanks to split live migration that eliminates 97% remote accesses. A port of split live migration to Wukong shows up to 2.53× throughput improvement on representative workloads like LUBM-10240, thanks to a reduction of 88% remote accesses. This further confirms the effectiveness and generality of Pragh. Finally, though Pragh focuses on RDMA-based graph traversal, we show its generality by extending it to support graph traversals under traditional networking. Evaluations on the graph traversal benchmarks and graph query workloads on the same cluster but with 10 Gbps TCP/IP network further confirm its effectiveness without RDMA. Specifically, when evaluating on the LUBM-10240, Wukong-TCP with Pragh can achieve up to 1.87× throughput improvement with a 56% decrease in remote accesses.},
  archive      = {J_TPDS},
  author       = {Rong Chen and Xingda Wei and Xiating Xie and Haibo Chen},
  doi          = {10.1109/TPDS.2024.3436828},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  number       = {10},
  pages        = {1810-1825},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Locality-preserving graph traversal with split live migration},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IRIS: A performance-portable framework for cross-platform
heterogeneous computing. <em>TPDS</em>, <em>35</em>(10), 1796–1809. (<a
href="https://doi.org/10.1109/TPDS.2024.3429010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From edge to exascale, computer architectures are becoming more heterogeneous and complex. The systems typically have fat nodes, with multicore CPUs and multiple hardware accelerators such as GPUs, FPGAs, and DSPs. This complexity is causing a crisis in programming systems and performance portability. Several programming systems are working to address these challenges, but the increasing architectural diversity is forcing software stacks and applications to be specialized for each architecture. As we show, all of these approaches critically depend on their software framework for discovery, execution, scheduling, and data orchestration. To address this challenge, we believe that a more agile and proactive software framework is essential to increase performance portability and improve user productivity. To this end, we have designed and implemented IRIS: a performance-portable framework for cross-platform heterogeneous computing. IRIS can discover available resources, manage multiple diverse programming platforms (e.g., CUDA, Hexagon, HIP, Level Zero, OpenCL, OpenMP) simultaneously in the same execution, respect data dependencies, orchestrate data movement proactively, and provide for user-configurable scheduling. To simplify data movement, IRIS introduces a shared virtual device memory with relaxed consistency among different heterogeneous devices. IRIS also adds an automatic kernel workload partitioning technique using the polyhedral model so that it can resize kernels for a wide range of devices. Our evaluation on three architectures, ranging from Qualcomm Snapdragon to a Summit supercomputer node, shows that IRIS improves portability across a wide range of diverse heterogeneous architectures with negligible overhead.},
  archive      = {J_TPDS},
  author       = {Jungwon Kim and Seyong Lee and Beau Johnston and Jeffrey S. Vetter},
  doi          = {10.1109/TPDS.2024.3429010},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  number       = {10},
  pages        = {1796-1809},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {IRIS: A performance-portable framework for cross-platform heterogeneous computing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Which coupled is best coupled? An exploration of AIMC tile
interfaces and load balancing for CNNs. <em>TPDS</em>, <em>35</em>(10),
1780–1795. (<a href="https://doi.org/10.1109/TPDS.2024.3437657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to stringent energy and performance constraints, edge AI computing often employs heterogeneous systems that utilize both general-purpose CPUs and accelerators. Analog in-memory computing (AIMC) is a well-known AI inference solution that overcomes computational bottlenecks by performing matrix-vector multiplication operations (MVMs) in constant time. However, the tiles of AIMC-based accelerators are limited by the number of weights they can hold. State-of-the-art research often sizes neural networks to AIMC tiles (or vice-versa), but does not consider cases where AIMC tiles cannot cover the whole network due to lack of tile resources or the network size. In this work, we study the trade-offs of available AIMC tile resources, neural network coverage, AIMC tile proximity to compute resources, and multi-core load balancing techniques. We first perform a study of single-layer performance and energy scalability of AIMC tiles in the two most typical AIMC acceleration targets: dense/fully-connected layers and convolutional layers. This study guides the methodology with which we approach parameter allocation to AIMC tiles in the context of large edge neural networks, both where AIMC tiles are close to the CPU (tightly-coupled) and cannot share resources across the system, and where AIMC tiles are far from the CPU (loosely-coupled) and can employ workload stealing. We explore the performance and energy trends of six modern CNNs using different methods of load balancing for differently-coupled system configurations with variable AIMC tile resources. We show that, by properly distributing workloads, AIMC acceleration can be made highly effective even on under-provisioned systems. As an example, 5.9x speedup and 5.6x energy gains were measured on an 8-core system, for a 41% coverage of neural network parameters.},
  archive      = {J_TPDS},
  author       = {Joshua Klein and Irem Boybat and Giovanni Ansaloni and Marina Zapater and David Atienza},
  doi          = {10.1109/TPDS.2024.3437657},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  number       = {10},
  pages        = {1780-1795},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Which coupled is best coupled? an exploration of AIMC tile interfaces and load balancing for CNNs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cost-effective and robust service provisioning in
multi-access edge computing. <em>TPDS</em>, <em>35</em>(10), 1765–1779.
(<a href="https://doi.org/10.1109/TPDS.2024.3435929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of multiaccess edge computing (MEC) technology, an increasing number of researchers and developers are deploying their computation-intensive and IO-intensive services (especially AI services) on edge devices. These devices, being close to end users, provide better performance in mobile environments. By constructing a service provisioning system at the network edge, latency is significantly reduced due to short-distance communication with edge servers. However, since the MEC-based service provisioning system is resource-sensitive and the network may be unstable, careful resource allocation and traffic scheduling strategies are essential. This paper investigates and quantifies the cost-effectiveness and robustness of the MEC-based service provisioning system with the applied resource allocation and traffic scheduling strategies. Based on this analysis, a c ost- e ffective and r obust service provisioning a lgorithm, termed CERA , is proposed to minimize deployment costs while maintaining system robustness. Extensive experiments are conducted to compare the proposed approach with well-known baseline algorithms and evaluate factors impacting the results. The findings demonstrate that CERA achieves at least 15.9% better performance than other baseline algorithms across various instances.},
  archive      = {J_TPDS},
  author       = {Zhengzhe Xiang and Yuhang Zheng and Dongjing Wang and Javid Taheri and Zengwei Zheng and Minyi Guo},
  doi          = {10.1109/TPDS.2024.3435929},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  number       = {10},
  pages        = {1765-1779},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Cost-effective and robust service provisioning in multi-access edge computing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploiting temporal-unrolled parallelism for
energy-efficient SNN acceleration. <em>TPDS</em>, <em>35</em>(10),
1749–1764. (<a href="https://doi.org/10.1109/TPDS.2024.3415712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event-driven spiking neural networks (SNNs) have demonstrated significant potential for achieving high energy and area efficiency. However, existing SNN accelerators suffer from issues such as high latency and energy consumption due to serial accumulation-comparison operations. This is mainly because SNN neurons integrate spikes, accumulate membrane potential, and generate output spikes when the potential exceeds a threshold. To address this, one approach is to leverage the sparsity of SNN spikes to reduce the number of time steps. However, this method can result in imbalanced workloads among neurons and limit the utilization of processing elements (PEs). In this paper, we present SATO, a temporal-parallel SNN accelerator that enables parallel accumulation of membrane potential for all time steps. SATO adopts a two-stage pipeline methodology, effectively decoupling neuron computations. This not only maintains accuracy but also unveils opportunities for fine-grained parallelism. By dividing the neuron computation into distinct stages, SATO enables the concurrent execution of spike accumulation for each time step, leveraging the parallel processing capabilities of modern hardware architectures. This not only enhances the overall efficiency of the accelerator but also reduces latency by exploiting parallelism at a granular level. The architecture of SATO includes a novel binary adder-search tree for generating the output spike train, effectively decoupling the chronological dependence in the accumulation-comparison operation. Furthermore, SATO employs a bucket-sort-based method to evenly distribute compressed workloads to all PEs, maximizing data locality of input spike trains. Experimental results on various SNN models demonstrate that SATO outperforms the well-known accelerator, the 8-bit version of “Eyeriss” by $20.7\times$ in terms of speedup and $6.0\times$ energy-saving, on average. Compared to the state-of-the-art SNN accelerator “SpinalFlow”, SATO can also achieve $4.6\times$ performance gain and $3.1\times$ energy reduction on average, which is quite impressive for inference.},
  archive      = {J_TPDS},
  author       = {Fangxin Liu and Zongwu Wang and Wenbo Zhao and Ning Yang and Yongbiao Chen and Shiyuan Huang and Haomin Li and Tao Yang and Songwen Pei and Xiaoyao Liang and Li Jiang},
  doi          = {10.1109/TPDS.2024.3415712},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  number       = {10},
  pages        = {1749-1764},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Exploiting temporal-unrolled parallelism for energy-efficient SNN acceleration},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). InSS: An intelligent scheduling orchestrator for multi-GPU
inference with spatio-temporal sharing. <em>TPDS</em>, <em>35</em>(10),
1735–1748. (<a href="https://doi.org/10.1109/TPDS.2024.3430063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the applications of AI proliferate, it is critical to increase the throughput of online DNN inference services. Multi-process service (MPS) improves the utilization rate of GPU resources by spatial-sharing, but it also brings unique challenges. First, interference between co-located DNN models deployed on the same GPU must be accurately modeled. Second, inference tasks arrive dynamically online, and each task needs to be served within a bounded time to meet the service-level objective (SLO). Third, the problem of fragments has become more serious. To address the above three challenges, we propose an In telligent S cheduling orchestrator for multi-GPU inference servers with spatio-temporal S haring ( InSS ), aiming to maximize the system throughput. InSS exploits two key innovations: i) An interference-aware latency analytical model which estimates the task latency. ii) A two-stage intelligent scheduler is tailored to jointly optimize the model placement, GPU resource allocation and adaptively decides batch size by coupling the latency analytical model. Our prototype implementation on four NVIDIA A100 GPUs shows that InSS can improve the throughput by up to 86% compared to the state-of-the-art GPU schedulers, while satisfying SLOs. We further show the scalability of InSS on 64 GPUs.},
  archive      = {J_TPDS},
  author       = {Ziyi Han and Ruiting Zhou and Chengzhong Xu and Yifan Zeng and Renli Zhang},
  doi          = {10.1109/TPDS.2024.3430063},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  number       = {10},
  pages        = {1735-1748},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {InSS: An intelligent scheduling orchestrator for multi-GPU inference with spatio-temporal sharing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SSA: A uniformly recursive bidirection-sequence systolic
sorter array. <em>TPDS</em>, <em>35</em>(10), 1721–1734. (<a
href="https://doi.org/10.1109/TPDS.2024.3434332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of reconfigurable circuits with parallel computing capabilities has been explored to enhance sorting performance and reduce power consumption. Nonetheless, most sorting algorithms utilizing dedicated processors are designed solely based on the parallelization of the algorithm, lacking considerations of specialized hardware structures. This leads to problems, including but not limited to the consumption of excessive I/O interface resources, on-chip storage resources, and complex layout wiring. In this paper, we propose a Systolic Sorter Array, implemented by a Uniform Recurrence Equation (URE) with highly parameterised in terms of data size, bit width and type. Leveraging this uniformly recursive structure, the sorter can simultaneously sort two independent sequences. In addition, we implemented global and local control modes on the FPGA to achieve higher computational frequencies. In our experiments, we have demonstrated the speed-up ratio of SSA relative to other state of the art (SOTA) sorting algorithms using C++ $std$ :: $sort()$ as benchmark. Inheriting the benefits from the Systolic Array architecture, the SSA reaches up to 810 Mhz computing frequency on the U200. The results of our study show that SSA outperforms other sorting algorithms in terms of throughput, speed-up ratio, and computation frequency.},
  archive      = {J_TPDS},
  author       = {Teng Gao and Lan Huang and Shang Gao and Kangping Wang},
  doi          = {10.1109/TPDS.2024.3434332},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  number       = {10},
  pages        = {1721-1734},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SSA: A uniformly recursive bidirection-sequence systolic sorter array},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ElasticBatch: A learning-augmented elastic scheduling system
for batch inference on MIG. <em>TPDS</em>, <em>35</em>(10), 1708–1720.
(<a href="https://doi.org/10.1109/TPDS.2024.3431189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As deep learning (DL) technologies become ubiquitous, GPU clusters are deployed for inference tasks with consistent service level objectives (SLOs). Efficiently utilizing multiple GPUs is crucial for throughput and cost-effectiveness. This article addresses the challenges posed by dynamic input and NVIDIA MIG in scheduling DL workloads. We present ElasticBatch, a scheduling system that simplifies configuration through bucketization and employs a machine learning-based pipeline to optimize settings. Our experiments demonstrate that ElasticBatch achieves a 50% reduction in GPU instances compared to MIG disablement, increases GPU utilization by 1.4% to 6.5% over an ideal scheduler and significantly reduces profiling time. This research contributes to the discourse on efficient utilization of GPU clusters. ElasticBatch&#39;s effectiveness in mitigating challenges posed by dynamic inputs and NVIDIA MIG underscores its potential to optimize GPU cluster performance, providing tangible benefits in terms of reduced instances, increased utilization, and significant time savings in real-world deployment scenarios.},
  archive      = {J_TPDS},
  author       = {Jiaxing Qi and Wencong Xiao and Mingzhen Li and Chaojie Yang and Yong Li and Wei Lin and Hailong Yang and Zhongzhi Luan and Depei Qian},
  doi          = {10.1109/TPDS.2024.3431189},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  number       = {10},
  pages        = {1708-1720},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ElasticBatch: A learning-augmented elastic scheduling system for batch inference on MIG},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Long-range MD electrostatics force computation on FPGAs.
<em>TPDS</em>, <em>35</em>(10), 1690–1707. (<a
href="https://doi.org/10.1109/TPDS.2024.3434347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strong scaling of long-range electrostatic force computation, which is a central concern of long timescale molecular dynamics simulations, is challenging for CPUs and GPUs due to its complex communication structure and global communication requirements. The scalability challenge is seen especially in small simulations of tens to hundreds of thousands of atoms that are of interest to many important applications such as physics-driven drug discovery. FPGA clusters, with their direct, tightly coupled, low-latency interconnects, are able to address these requirements. For FPGA MD clusters to be effective, however, single device performance must also be competitive. In this work, we leverage the inherent benefits of FPGAs to implement a long-range electrostatic force computation architecture. We present an overall framework with numerous algorithmic, mapping, and architecture innovations, including a unified interleaved memory, a spatial scheduling algorithm, and a design for seamless integration with the larger MD system. We examine a number of alternative configurations based on different resource allocation strategies and user parameters. We show that the best configuration of this architecture, implemented on an Intel Agilex FPGA, can achieve $2124 ns$ and $287 ns$ of simulated time per day of wall-clock time for the two molecular dynamics benchmarks DHFR and ApoA1; simulating 23K and 92K particles, respectively.},
  archive      = {J_TPDS},
  author       = {Sahan Bandara and Anthony Ducimo and Chunshu Wu and Martin Herbordt},
  doi          = {10.1109/TPDS.2024.3434347},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {10},
  number       = {10},
  pages        = {1690-1707},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Long-range MD electrostatics force computation on FPGAs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IrGEMM: An input-aware tuning framework for irregular GEMM
on ARM and x86 CPUs. <em>TPDS</em>, <em>35</em>(9), 1672–1689. (<a
href="https://doi.org/10.1109/TPDS.2024.3432579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The matrix multiplication algorithm is a fundamental numerical technique in linear algebra and plays a crucial role in many scientific computing applications. Despite the high performance of mainstream basic linear algebra libraries for large-scale dense matrix multiplications, they exhibit poor performance when applied to matrix multiplication with irregular input. This paper proposes an input-aware tuning framework that accounts for application scenarios and computer architectures to provide high-performance irregular matrix multiplication on ARMv8 and X86 CPUs. The framework comprises two stages: the install-time stage and the run-time stage. The install-time stage utilizes our proposed computational template to generate high-performance kernels for general data layout and SIMD-friendly data layout. The run-time stage utilizes a tiling algorithm suitable for irregular GEMM to select the optimal kernel and link as an execution plan. Additionally, load-balanced multi-threaded optimization algorithms are defined to exploit the multi-threading capability of modern processors. Experiments demonstrate that the proposed IrGEMM framework can achieve significant performance improvements for irregular GEMM on both ARMv8 and X86 CPUs compared to other mainstream BLAS libraries.},
  archive      = {J_TPDS},
  author       = {Cunyang Wei and Haipeng Jia and Yunquan Zhang and Jianyu Yao and Chendi Li and Wenxuan Cao},
  doi          = {10.1109/TPDS.2024.3432579},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  number       = {9},
  pages        = {1672-1689},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {IrGEMM: An input-aware tuning framework for irregular GEMM on ARM and x86 CPUs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Springald: GPU-accelerated window-based aggregates over
out-of-order data streams. <em>TPDS</em>, <em>35</em>(9), 1657–1671. (<a
href="https://doi.org/10.1109/TPDS.2024.3431611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An increasing number of application domains require high-throughput processing to extract insights from massive data streams. The Data Stream Processing (DSP) paradigm provides formal approaches to analyze structured data streams considered as special, unbounded relations. The most used class of stateful operators in DSP are the ones running sliding-window aggregation, which continuously extracts insights from the most recent portion of the stream. This article presents Springald , an efficient sliding-window operator leveraging GPU devices. Springald , incorporated in the WindFlow parallel library, processes out-of-order data streams with watermarks propagation. These two features—GPU processing and out-of-orderliness—make Springald a novel contribution to this research area. This article describes the methodology behind Springald , its design and implementation. We also provide an extensive experimental evaluation to understand the behavior of Springald deeply, and we showcase its superior performance against state-of-the-art competitors.},
  archive      = {J_TPDS},
  author       = {Gabriele Mencagli and Patrizio Dazzi and Massimo Coppola},
  doi          = {10.1109/TPDS.2024.3431611},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  number       = {9},
  pages        = {1657-1671},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Springald: GPU-accelerated window-based aggregates over out-of-order data streams},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Swift: Expedited failure recovery for large-scale DNN
training. <em>TPDS</em>, <em>35</em>(9), 1644–1656. (<a
href="https://doi.org/10.1109/TPDS.2024.3429625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the size of deep learning models gets larger and larger, training takes longer time and more resources, making fault tolerance more and more critical. Existing state-of-the-art methods like CheckFreq and Elastic Horovod need to back up a copy of the model state (i.e., parameters and optimizer states) in memory, which is costly for large models and leads to non-trivial overhead. This article presents Swift , a novel recovery design for distributed deep neural network training that significantly reduces the failure recovery overhead without affecting training throughput and model accuracy. Instead of making an additional copy of the model state, Swift resolves the inconsistencies of the model state caused by the failure and exploits the replicas of the model state in data parallelism for failure recovery. We propose a logging-based approach when replicas are unavailable, which records intermediate data and replays the computation to recover the lost state upon a failure. The re-computation is distributed across multiple machines to accelerate failure recovery further. We also log intermediate data selectively, exploring the trade-off between recovery time and intermediate data storage overhead. Evaluations show that Swift significantly reduces the failure recovery time and achieves similar or better training throughput during failure-free execution compared to state-of-the-art methods without degrading final model accuracy. Swift can also achieve up to 1.16x speedup in total training time compared to state-of-the-art methods.},
  archive      = {J_TPDS},
  author       = {Yuchen Zhong and Guangming Sheng and Juncheng Liu and Jinhui Yuan and Chuan Wu},
  doi          = {10.1109/TPDS.2024.3429625},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  number       = {9},
  pages        = {1644-1656},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Swift: Expedited failure recovery for large-scale DNN training},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward materials genome big-data: A blockchain-based secure
storage and efficient retrieval method. <em>TPDS</em>, <em>35</em>(9),
1630–1643. (<a href="https://doi.org/10.1109/TPDS.2024.3426275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of the era of data-driven material R&amp;D, more and more countries have begun to build material Big Data sharing platforms to support the design and R&amp;D of new materials. In the application process of material Big Data sharing platforms, storage and retrieval are the basis of resource mining and analysis. However, achieving efficient storage and recovery is not accessible due to the multimodality, isomerization, discrete and other characteristics of material data. At the same time, due to the lack of security mechanisms, how to ensure the integrity and reliability of the original data is also a significant problem faced by researchers. Given these issues, this paper proposes a blockchain-based secure storage and efficient retrieval scheme. Introducing the Improved Merkle Tree (MMT) structure into the block, the transaction data on the chain and the original data in the off-chain cloud are mapped through the material data template. Experimental results show that our proposed MMT structure has no significant impact on the block creation efficiency while improving the retrieval efficiency. At the same time, MMT is superior to state-of-the-art retrieval methods in terms of efficiency, especially regarding range retrieval. The method proposed in this paper is more suitable for the application needs of the material Big Data sharing platform, and the retrieval efficiency has also been significantly improved.},
  archive      = {J_TPDS},
  author       = {Ran Wang and Cheng Xu and Xiaotong Zhang},
  doi          = {10.1109/TPDS.2024.3426275},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  number       = {9},
  pages        = {1630-1643},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Toward materials genome big-data: A blockchain-based secure storage and efficient retrieval method},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STT-RAM-based hierarchical in-memory computing.
<em>TPDS</em>, <em>35</em>(9), 1615–1629. (<a
href="https://doi.org/10.1109/TPDS.2024.3430853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-memory computing promises to overcome the von Neumann bottleneck in computer systems by performing computations directly within the memory. Previous research has suggested using Spin-Transfer Torque RAM (STT-RAM) for in-memory computing due to its non-volatility, low leakage power, high density, endurance, and commercial viability. This paper explores hierarchical in-memory computing , where different levels of the memory hierarchy are augmented with processing elements to optimize workload execution. The paper investigates processing in memory (PiM) using non-volatile STT-RAM and processing in cache (PiC) using volatile STT-RAM with relaxed retention, which helps mitigate STT-RAM&#39;s write latency and energy overheads. We analyze tradeoffs and overheads associated with data movement for PiC versus write overheads for PiM using STT-RAMs for various workloads. We examine workload characteristics, such as computational intensity and CPU-dependent workloads with limited instruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using these workloads, we evaluate computing in STT-RAM versus SRAM at different cache hierarchy levels and explore the potential of heterogeneous STT-RAM cache architectures with various retention times for PiC and CPU-based computing. Our experiments reveal significant advantages of STT-RAM-based PiC over PiM for specific workloads. Finally, we describe open research problems in hierarchical in-memory computing architectures to further enhance this paradigm.},
  archive      = {J_TPDS},
  author       = {Dhruv Gajaria and Kevin Antony Gomez and Tosiron Adegbija},
  doi          = {10.1109/TPDS.2024.3430853},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  number       = {9},
  pages        = {1615-1629},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {STT-RAM-based hierarchical in-memory computing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RADAR: A skew-resistant and hotness-aware ordered index
design for processing-in-memory systems. <em>TPDS</em>, <em>35</em>(9),
1598–1614. (<a href="https://doi.org/10.1109/TPDS.2024.3424853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pointer chasing becomes the performance bottleneck for today&#39;s in-memory indexes due to the memory wall. Emerging processing-in-memory (PIM) technologies are promising to mitigate this bottleneck, by enabling low-latency memory access and aggregated memory bandwidth scaling with the number of PIM modules. Prior PIM-based indexes adopt a fixed granularity to partition the key space and maintain static heights of skiplist nodes among PIM modules to accelerate index operations on skiplist, neglecting the changes in skewness and hotness of data access patterns during runtime. In this article, we present RADAR, an innovative PIM-friendly skiplist that dynamically partitions the key space among PIM modules to adapt to varying skewness. An offline learning-based model is employed to catch hotness changes to adjust the heights of skiplist nodes. In multiple datasets, RADAR achieves up to 198.2x performance improvement and consumes 47.4% less memory than state-of-the-art designs on real PIM hardware.},
  archive      = {J_TPDS},
  author       = {Yifan Hua and Shengan Zheng and Weihan Kong and Cong Zhou and Kaixin Huang and Ruoyan Ma and Linpeng Huang},
  doi          = {10.1109/TPDS.2024.3424853},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  number       = {9},
  pages        = {1598-1614},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {RADAR: A skew-resistant and hotness-aware ordered index design for processing-in-memory systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cost-effective server deployment for multi-access edge
networks: A cooperative scheme. <em>TPDS</em>, <em>35</em>(9),
1583–1597. (<a href="https://doi.org/10.1109/TPDS.2024.3426523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The combination of 5G/6G and edge computing has been envisioned as a promising paradigm to empower pervasive and intensive computing for the Internet-of-Things (IoT). High deployment cost is one of the major obstacles for realizing 5G/6G edge computing. Most existing works tried to deploy the minimum number of edge servers to cover a target area by avoiding coverage overlaps. However, following this framework, the resource requirement per server will be drastically increased by the peak requirement during workload variations. Even worse, most resources will be left under-utilized for most of the time. To address this problem, we propose CoopEdge, a cost-effective server deployment scheme for cooperative multi-access edge computing. The key idea of CoopEdge is to allow deploying overlapped servers to handle variable requested workloads in a cooperative manner. In this way, the peak demands can be dispersed into multiple servers, and the resource requirement for each server can be greatly reduced. We propose a Two-step Incremental Deployment (TID) algorithm to jointly decide the server deployment and cooperation policies. For the scenarios involving multiple network operators that are unwilling to cooperate with each other, we further extend the TID algorithm to a distributed TID algorithm based on the game theory. Extensive evaluation experiments are conducted based on the measurement results of seven real-world edge applications. The results show that compared with the state-of-the-art work, CoopEdge significantly reduces the deployment cost by 38.7% and improves resource utilization by 36.2%, and the proposed distributed algorithm can achieve a comparable deployment cost with CoopEdge, especially for small-coverage servers.},
  archive      = {J_TPDS},
  author       = {Rong Cong and Zhiwei Zhao and Linyuanqi Zhang and Geyong Min},
  doi          = {10.1109/TPDS.2024.3426523},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  number       = {9},
  pages        = {1583-1597},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Cost-effective server deployment for multi-access edge networks: A cooperative scheme},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive QoS-aware microservice deployment with excessive
loads via intra- and inter-datacenter scheduling. <em>TPDS</em>,
<em>35</em>(9), 1565–1582. (<a
href="https://doi.org/10.1109/TPDS.2024.3425931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User-facing applications often experience excessive loads and are shifting towards the microservice architecture. To fully utilize heterogeneous resources, current datacenters have adopted the disaggregated storage and compute architecture, where the storage and compute clusters are suitable to deploy the stateful and stateless microservices, respectively. Moreover, when the local datacenter has insufficient resources to host excessive loads, a reasonable solution is moving some microservices to remote datacenters. However, it is nontrivial to decide the appropriate microservice deployment inside the local datacenter and identify the appropriate migration decision to remote datacenters, as microservices show different characteristics, and the local datacenter shows different resource contention situations. We therefore propose ELIS, an intra- and inter-datacenter scheduling system that ensures the Quality-of-Service (QoS) of the microservice application, while minimizing the network bandwidth usage and computational resource usage. ELIS comprises a resource manager , a cross-cluster microservice deployer , and a reward-based microservice migrator . The resource manager allocates near-optimal resources for microservices while ensuring QoS. The microservice deployer deploys the microservices between the storage and compute clusters in the local datacenter, to minimize the network bandwidth usage while satisfying the microservice resource demand. The microservice migrator migrates some microservices to remote datacenters when local resources cannot afford the excessive loads. Experimental results show that ELIS ensures the QoS of user-facing applications. Meanwhile, it reduces the public network bandwidth usage, the remote computational resource usage, and the local network bandwidth usage by 49.6%, 48.5%, and 60.7% on average, respectively.},
  archive      = {J_TPDS},
  author       = {Jiuchen Shi and Kaihua Fu and Jiawen Wang and Quan Chen and Deze Zeng and Minyi Guo},
  doi          = {10.1109/TPDS.2024.3425931},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  number       = {9},
  pages        = {1565-1582},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Adaptive QoS-aware microservice deployment with excessive loads via intra- and inter-datacenter scheduling},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Malleability in modern HPC systems: Current experiences,
challenges, and future opportunities. <em>TPDS</em>, <em>35</em>(9),
1551–1564. (<a href="https://doi.org/10.1109/TPDS.2024.3406764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increase of complex scientific simulations driven by workflows and heterogeneous workload profiles, managing system resources effectively is essential for improving performance and system throughput, especially due to trends like heterogeneous HPC and deeply integrated systems with on-chip accelerators. For optimal resource utilization, dynamic resource allocation can improve productivity across all system and application levels, by adapting the applications’ configurations to the system&#39;s resources. In this context, malleable jobs, which can change resources at runtime, can increase the system throughput and resource utilization while bringing various advantages for HPC users (e.g., shorter waiting time). Malleability has received much attention recently, even though it has been an active research area for more than two decades. This article presents the state-of-the-art of malleable implementations in HPC systems, targeting mainly malleability in compute and I/O resources. Based on our experiences, we state our current concerns and list future opportunities for research.},
  archive      = {J_TPDS},
  author       = {Ahmad Tarraf and Martin Schreiber and Alberto Cascajo and Jean-Baptiste Besnard and Marc-André Vef and Dominik Huber and Sonja Happ and André Brinkmann and David E. Singh and Hans-Christian Hoppe and Alberto Miranda and Antonio J. Peña and Rui Machado and Marta Garcia-Gasulla and Martin Schulz and Paul Carpenter and Simon Pickartz and Tiberiu Rotaru and Sergio Iserte and Victor Lopez and Jorge Ejarque and Heena Sirwani and Jesus Carretero and Felix Wolf},
  doi          = {10.1109/TPDS.2024.3406764},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  number       = {9},
  pages        = {1551-1564},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Malleability in modern HPC systems: Current experiences, challenges, and future opportunities},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pyxis: Scheduling mixed tasks in disaggregated datacenters.
<em>TPDS</em>, <em>35</em>(9), 1536–1550. (<a
href="https://doi.org/10.1109/TPDS.2024.3418620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disaggregating compute from storage is an emerging trend in cloud computing. Effectively utilizing resources in both compute and storage pool is the key to high performance. The state-of-the-art scheduler provides optimal scheduling decisions for workloads with homogeneous tasks. However, cloud applications often generate a mix of tasks with diverse compute and IO characteristics, resulting in sub-optimal performance for existing solutions. We present Pyxis, a system that provides optimal scheduling decisions for mixed workloads in disaggregated datacenters with theoretical guarantees. Pyxis is capable of maximizing overall throughput while meeting latency SLOs. Pyxis decouples the scheduling of different tasks. Our insight is that the optimal solution has an “all-or-nothing” structure that can be captured by a single turning point in the spectrum of tasks. Based on task characteristics, the turning point partitions the tasks either all to storage nodes or all to compute nodes (none to storage nodes). We theoretically prove that the optimal solution has such a structure, and design an online algorithm with sub-second convergence. We implement a prototype of Pyxis. Experiments on CloudLab with various synthetic and application workloads show that Pyxis improves the throughput by 3–21× over the state-of-the-art solution.},
  archive      = {J_TPDS},
  author       = {Sheng Qi and Chao Jin and Mosharaf Chowdhury and Zhenming Liu and Xuanzhe Liu and Xin Jin},
  doi          = {10.1109/TPDS.2024.3418620},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  number       = {9},
  pages        = {1536-1550},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Pyxis: Scheduling mixed tasks in disaggregated datacenters},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). KLNK: Expanding page boundaries in a distributed shared
memory system. <em>TPDS</em>, <em>35</em>(9), 1524–1535. (<a
href="https://doi.org/10.1109/TPDS.2024.3409882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software-based distributed shared memory (DSM) allows multiple processes to access shared data without the need for specialized hardware. However, this flexibility comes at a significant cost due to the need for data synchronization. One approach to mitigate these costs is to relax the consistency model, which can lead to delayed updates to the shared data. This approach typically requires the use of explicit synchronization primitives to regulate access to the shared memory and determine the timing of data synchronization. To circumvent the need for explicit synchronization, an alternative approach is to manage shared memory transparently using the underlying system. While this can simplify programming, it often imposes a fixed granularity for data sharing, which can limit the expansion of the coherence domain and increase the synchronization requirements. To overcome this limitation, we propose an abstraction called the elastic coherence domain, which dynamically adjusts the scope of data synchronization and is supported by the underlying system for transparent management of shared memory. The experimental results show that this approach can improve the efficiency of memory sharing in distributed environments.},
  archive      = {J_TPDS},
  author       = {Yi-Wei Ci and Michael R. Lyu and Zhan Zhang and De-Cheng Zuo and Xiao-Zong Yang},
  doi          = {10.1109/TPDS.2024.3409882},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {9},
  number       = {9},
  pages        = {1524-1535},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {KLNK: Expanding page boundaries in a distributed shared memory system},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-performance hardware acceleration architecture for
cross-silo federated learning. <em>TPDS</em>, <em>35</em>(8), 1506–1523.
(<a href="https://doi.org/10.1109/TPDS.2024.3413718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-silo federated learning (FL) adopts various cryptographic operations to preserve data privacy, which introduces significant performance overhead. In this paper, we identify nine widely-used cryptographic operations and design an efficient hardware architecture to accelerate them. However, directly offloading them on hardware statically leads to (1) inadequate hardware acceleration due to the limited resources allocated to each operation; (2) insufficient resource utilization, since different operations are used at different times. To address these challenges, we propose FLASH, a high-performance hardware acceleration architecture for cross-silo FL systems. At its heart, FLASH extracts two basic operators—modular exponentiation and multiplication—behind the nine cryptographic operations and implements them as highly-performant engines to achieve adequate acceleration. Furthermore, it leverages a dataflow scheduling scheme to dynamically compose different cryptographic operations based on these basic engines to obtain sufficient resource utilization. We have implemented a fully-functional FLASH prototype with Xilinx VU13P FPGA and integrated it with FATE, the most widely-adopted cross-silo FL framework. Experimental results show that, for the nine cryptographic operations, FLASH achieves up to $14.0\times$ and $3.4\times$ acceleration over CPU and GPU, translating to up to $6.8\times$ and $2.0\times$ speedup for realistic FL applications, respectively. We finally evaluate the FLASH design as an ASIC, and it achieves $23.6\times$ performance improvement upon the FPGA prototype.},
  archive      = {J_TPDS},
  author       = {Junxue Zhang and Xiaodian Cheng and Liu Yang and Jinbin Hu and Han Tian and Kai Chen},
  doi          = {10.1109/TPDS.2024.3413718},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1506-1523},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {High-performance hardware acceleration architecture for cross-silo federated learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RR-compound: RDMA-fused gRPC for low latency, high
throughput, and easy interface. <em>TPDS</em>, <em>35</em>(8),
1488–1505. (<a href="https://doi.org/10.1109/TPDS.2024.3404394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced data centers strive for high performance and throughput, which can be achieved through the desirable merits of Remote Procedure Call (RPC) programming model and the low latency of Remote Direct Memory Access (RDMA). However, despite the widespread availability of these software and hardware utilities, they have been utilized separately for their own applications in existing production systems for many years. Although researchers have attempted to develop RDMA-enabled RPC prototypes, they often face challenges such as API discrepancies and a lack of specific features for effective integration with major production software, rendering them incompatible. This industry R&amp;D project aims to enhance the performance of gRPC, a widely utilized RPC framework in major companies, by integrating RDMA as an internal component. Our system solution, called RR-Compound, combines the simple user interface and other merits of gRPC with low latency for remote data accesses. RR-Compound is fully compatible with gRPC and can serve as a seamless replacement without altering existing applications. However, to achieve low latency, high throughput, and scalability for RR-Compound, several technical challenges in managing network connections and memory space utilization must be effectively addressed. To overcome the limitations of existing connection methods, we have developed a new method called BPEV that is independent of gRPC and applicable to all RDMA systems. We have also retained the asynchronous framework of gRPC, albeit with limited buffer space in RDMA memory management. In micro-benchmarks, RR-Compound outperforms mRPC - the state-of-the-art RPC framework for a large number of connections, achieving a 14.77% increase in throughput and a 42.55% reduction in latency. Subsequently, we compare RR-Compound with gRPC over IPoIB using two real-world applications: KV-Store and TensorFlow. RR-Compound achieves up to a 2.35x increase in throughput and reduces the average latency by 46.92%.},
  archive      = {J_TPDS},
  author       = {Liang Geng and Hao Wang and Jingsong Meng and Dayi Fan and Sami Ben-Romdhane and Hari Kadayam Pichumani and Vinay Phegade and Xiaodong Zhang},
  doi          = {10.1109/TPDS.2024.3404394},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1488-1505},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {RR-compound: RDMA-fused gRPC for low latency, high throughput, and easy interface},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TeGraph+: Scalable temporal graph processing enabling
flexible edge modifications. <em>TPDS</em>, <em>35</em>(8), 1469–1487.
(<a href="https://doi.org/10.1109/TPDS.2024.3393914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal graphs are widely used for time-critical applications, which enable the extraction of graph structural information with temporal features but cannot be efficiently supported by static graph computing systems. However, the current state-of-the-art solutions for temporal graph problems are not only ad-hoc and suboptimal, but they also exhibit poor scalability, particularly in terms of their inability to scale to evolving graphs with flexible edge modifications (including insertions and deletions) and diverse execution environments. In this article, we present two key observations. First, temporal path problems can be characterized as topological-optimum problems, which can be efficiently resolved using a universal single-scan execution model. Second, data redundancy in transformed temporal graphs can be mitigated by merging superfluous vertices. Building upon these fundamental insights, we propose TeGraph+, a versatile temporal graph computing engine that makes the following contributions: (1) a unified optimization strategy and execution model for temporal graph problems; (2) a novel graph transformation model with graph redundancy reduction strategy; (3) a spanning tree decomposition (STD) based distributed execution model which uses an efficient transformed graph decomposition strategy to partition the transformed graph into different spanning trees for distributed execution; (4) an efficient mixed imperative and lazy graph update strategy that offers support for evolving graphs with flexible edge modifications; (5) a general system framework with user-friendly APIs and the support of various execution environments, including in-memory, out-of-core, and distributed execution environments. Our extensive evaluation reveals that TeGraph+ can achieve up to $241\times$ speedups over the state-of-the-art counterparts.},
  archive      = {J_TPDS},
  author       = {Chengying Huan and Yongchao Liu and Heng Zhang and Hang Liu and Shiyang Chen and Shuaiwen Leon Song and Yanjun Wu},
  doi          = {10.1109/TPDS.2024.3393914},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1469-1487},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {TeGraph+: Scalable temporal graph processing enabling flexible edge modifications},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint participant and learning topology selection for
federated learning in edge clouds. <em>TPDS</em>, <em>35</em>(8),
1456–1468. (<a href="https://doi.org/10.1109/TPDS.2024.3413751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying federated learning (FL) in edge clouds poses challenges, especially when multiple models are concurrently trained in resource-constrained edge environments. Existing research on federated edge learning has predominantly focused on client selection for training a single FL model, typically with a fixed learning topology. Preliminary experiments indicate that FL models with adaptable topologies exhibit lower learning costs compared to those with fixed topologies. This paper delves into the intricacies of jointly selecting participants and learning topologies for multiple FL models simultaneously trained in the edge cloud. The problem is formulated as an integer non-linear programming problem, aiming to minimize total learning costs associated with all FL models while adhering to edge resource constraints. To tackle this challenging optimization problem, we introduce a two-stage algorithm that decouples the original problem into two sub-problems and iteratively addresses them separately with efficient heuristics. Our method enhances resource competition and load balancing in edge clouds by allowing FL models to choose participants and learning topologies independently. Extensive experiments conducted with real-world networks and FL datasets affirm the better performance of our algorithm, demonstrating lower average total costs with up to 33.5% and 39.6% compared to previous methods designed for multi-model FL.},
  archive      = {J_TPDS},
  author       = {Xinliang Wei and Kejiang Ye and Xinghua Shi and Cheng-Zhong Xu and Yu Wang},
  doi          = {10.1109/TPDS.2024.3413751},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1456-1468},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Joint participant and learning topology selection for federated learning in edge clouds},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Faster-BNI: Fast parallel exact inference on bayesian
networks. <em>TPDS</em>, <em>35</em>(8), 1444–1455. (<a
href="https://doi.org/10.1109/TPDS.2024.3414177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian networks (BNs) have recently attracted more attention, because they are interpretable machine learning models and enable a direct representation of causal relations between variables. However, exact inference on BNs is time-consuming, especially for complex problems, which hinders the widespread adoption of BNs. To improve the efficiency, we propose a fast BN exact inference named Faster-BNI on multi-core CPUs. Faster-BNI enhances the efficiency of a well-known BN exact inference algorithm, namely the junction tree algorithm, through hybrid parallelism that tightly integrates coarse- and fine-grained parallelism. Moreover, we identify that the bottleneck of BN exact inference methods lies in recursively updating the potential tables of the network. To reduce the table update cost, Faster-BNI employs novel optimizations, including the reduction of potential tables and re-organizing the potential table storage, to avoid unnecessary memory consumption and simplify potential table operations. Comprehensive experiments on real-world BNs show that the sequential version of Faster-BNI outperforms existing sequential implementation by 9 to 22 times, and the parallel version of Faster-BNI achieves up to 11 times faster inference than its parallel counterparts.},
  archive      = {J_TPDS},
  author       = {Jiantong Jiang and Zeyi Wen and Atif Mansoor and Ajmal Mian},
  doi          = {10.1109/TPDS.2024.3414177},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1444-1455},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Faster-BNI: Fast parallel exact inference on bayesian networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FEUAGame: Fairness-aware edge user allocation for app
vendors. <em>TPDS</em>, <em>35</em>(8), 1429–1443. (<a
href="https://doi.org/10.1109/TPDS.2024.3409548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) offers a new computing paradigm that turns computing and storage resources to the network edge to provide minimal service latency compared to cloud computing. Many research works have attempted to help app vendors allocate users to appropriate edge servers for high-performance service provisioning. However, existing edge user allocation (EUA) approaches have ignored fairness in users’ data rates caused by interference, which is crucial in service provisioning in the MEC environment. To pursue fairness in EUA, edge users need to be assigned to edge servers so their quality of experience can be ensured at minimum costs without significant service performance differences among them. In this paper, we make the first attempt to address this fair edge user allocation (FEUA) problem. Specifically, we formulate the FEUA problem, prove its $\mathcal {NP}$ -hardness, and propose an optimal approach to solve small-scale FEUA problems. To accommodate large-scale FEUA scenarios, we propose a game-theoretic approach called FEUAGame that transforms the FEUA problem into a potential game that admits a Nash equilibrium. FEUA employs a decentralized algorithm to find the Nash equilibrium in the potential game as the solution to the FEUA problem. A widely-used real-world data set is utilised to experimentally compare the performance of FEUAGame to four representative approaches. The numerical outcomes show the effectiveness and efficiency of the proposed approaches in solving the FEUA problem.},
  archive      = {J_TPDS},
  author       = {Jingwen Zhou and Feifei Chen and Guangming Cui and Yong Xiang and Qiang He},
  doi          = {10.1109/TPDS.2024.3409548},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1429-1443},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FEUAGame: Fairness-aware edge user allocation for app vendors},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multidimensional communication scheduling method for
hybrid parallel DNN training. <em>TPDS</em>, <em>35</em>(8), 1415–1428.
(<a href="https://doi.org/10.1109/TPDS.2024.3406420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TPDS},
  author       = {Shengwei Li and Kai Lu and Zhiquan Lai and Weijie Liu and Keshi Ge and Dongsheng Li},
  doi          = {10.1109/TPDS.2024.3406420},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1415-1428},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A multidimensional communication scheduling method for hybrid parallel DNN training},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WASP: Efficient power management enabling workload-aware,
self-powered AIoT devices. <em>TPDS</em>, <em>35</em>(8), 1400–1414. (<a
href="https://doi.org/10.1109/TPDS.2024.3408167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The wide adoption of edge AI has heightened the demand for various battery-less and maintenance-free smart systems. Nevertheless, emerging Artificial Intelligence of Things (AIoT) are complex workloads showing increased power demand, diversified power usage patterns, and unique sensitivity to power management (PM) approaches. Existing AIoT devices cannot select the most appropriate PM tuning knob, and therefore they often make sub-optimal decisions. In addition, these PM solutions always assume traditional power regulation circuit which incurs non-negligible power loss and control overhead. This can greatly compromise the potential of AIoT efficiency. In this paper, we explore power management (PM) optimization for emerging self-powered AIoT devices. We propose WASP, a highly efficient power management scheme for workload-aware, self-powered AIoT devices. The novelty of WASP is two fold. First, it combines offline profiling and light-weight online control to select the most appropriate PM tuning knobs for the given DNN models. Second, it is well tailored to a reconfigurable voltage regulation module that can make the best use of the limited power budget. Our results show that WASP allows AIoT devices to accomplish 65.6% more inference tasks under a stringent power budget without any performance degradation compared with other existing approaches.},
  archive      = {J_TPDS},
  author       = {Xiaofeng Hou and Xuehan Tang and Jiacheng Liu and Chao Li and Luhong Liang and Kwang-Ting Cheng},
  doi          = {10.1109/TPDS.2024.3408167},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1400-1414},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {WASP: Efficient power management enabling workload-aware, self-powered AIoT devices},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Proactive caching with distributed deep reinforcement
learning in 6G cloud-edge collaboration computing. <em>TPDS</em>,
<em>35</em>(8), 1387–1399. (<a
href="https://doi.org/10.1109/TPDS.2024.3406027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TPDS},
  author       = {Changmao Wu and Zhengwei Xu and Xiaoming He and Qi Lou and Yuanyuan Xia and Shuman Huang},
  doi          = {10.1109/TPDS.2024.3406027},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1387-1399},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Proactive caching with distributed deep reinforcement learning in 6G cloud-edge collaboration computing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The static allocation is not a static: Optimizing SSD
address allocation through boosting static policy. <em>TPDS</em>,
<em>35</em>(8), 1373–1386. (<a
href="https://doi.org/10.1109/TPDS.2024.3407367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The address allocation policy in SSD aims to translate the logical address of I/O requests into a physical address, and the static address allocation is widely used in modern SSD. Through extensive experiments, we find that there are significant differences in the utilization of SSD parallelism among different static address allocation policies. We also observe that the fixed address allocation design prevents SSDs from continuing to meet the challenges posed by cloud workloads and misses the possibility of further optimization. These situations stem from our excessive reliance on SSD parallelism over time. In this paper, we propose HsaP , a h ybrid s tatic address a llocation p olicy, that adaptively chooses the best static allocation policy to meet the SSD performance at runtime. HsaP is a dynamic scheduling scheme based on static address allocation policy. The static policy ensures that HsaP has stable performance and light-weight overhead, while dynamic scheduling can effectively combine different allocation policies, selecting the best-performing static mapping mode for a given SSD state. Meanwhile, HsaP can further improve the read and write performance of SSDs simultaneously through plane reallocation and data rewrite. Experimental results show that HsaP achieves significant read and write performance gain of a wide range of the latest cloud block storage traces compared to several state-of-the-art address allocation approaches.},
  archive      = {J_TPDS},
  author       = {Yang Zhou and Fang Wang and Zhan Shi and Dan Feng},
  doi          = {10.1109/TPDS.2024.3407367},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1373-1386},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {The static allocation is not a static: Optimizing SSD address allocation through boosting static policy},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Age-of-event aware: Sampling period optimization in a
three-stage wireless cyber-physical system with diverse parallelisms.
<em>TPDS</em>, <em>35</em>(8), 1360–1372. (<a
href="https://doi.org/10.1109/TPDS.2024.3405790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of parallel computing systems and distributed time-sensitive applications, it is urgent to provide statistical guarantees for age of information (AoI) in wireless cyber-physical systems (WCPS) with diverse parallelisms. However, most of the existing research on AoI have tended to focus on serial transmission, and the AoI performance of multi-stage parallel systems remains unclear. To help address these research gaps, in this work, we set out to investigate the age of event (AoE) violation probability in a three-stage WCPS with diverse parallelisms such as fork-join and split-merge. We analyze both transient and steady-state characteristics of AoE violation probability (AoEVP). Using these characteristics, we transform the AoEVP minimization problem into an equivalent minimization problem. Moreover, we develop a queuing model to capture the queue dynamics under the max-plus theory of stochastic network calculus (SNC) approach. Based on the max-plus model, we derive a closed-form Chernoff upper bound for the equivalent problem by applying the union bound and the Chernoff inequality. Furthermore, we characterize the service process for different parallelisms applicable to each stage. By solving the Chernoff upper bound with the service moment generation functions (MGFs), we obtain heuristic update period solutions for minimizing the AoEVP of three-stage WCPS. Simulation results validate our analysis and demonstrate that our heuristic update period solutions are near optimal for minimizing the AoEVP of three-stage WCPS with diverse parallelisms.},
  archive      = {J_TPDS},
  author       = {Yanxi Zhang and Muyu Mei and Dongqi Yan and Xu Zhang and Qinghai Yang and Mingwu Yao},
  doi          = {10.1109/TPDS.2024.3405790},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1360-1372},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Age-of-event aware: Sampling period optimization in a three-stage wireless cyber-physical system with diverse parallelisms},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On off-chaining smart contract runtime protection: A queuing
model approach. <em>TPDS</em>, <em>35</em>(8), 1345–1359. (<a
href="https://doi.org/10.1109/TPDS.2024.3389153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vulnerability of smart contracts has been demonstrated by an increasing number of multi-million exploitation incidents in public blockchains. Several works propose applying runtime verification to protect smart contracts post-deployment. However, none discuss the induced onchain overhead that may preclude its deployment, leaving smart contracts unprotected. A prominent solution to the onchain overhead is outsourcing the analysis off-chain. In this work, we analytically study the potential efficiency of off-chain smart contract runtime verification. We present a generic queueing network model of the off-chain runtime verification and the block generation process. The queuing model approach allows us to efficiently and flexibly capture the non-deterministic behavior of blockchain, estimating the number of transactions in the pool and their corresponding waiting times. We analyze the onchain overhead and evaluate off-chain RV, providing numerical indicators of transaction processing latency and throughput.},
  archive      = {J_TPDS},
  author       = {Isra M. Ali and Mohamed M. Abdallah},
  doi          = {10.1109/TPDS.2024.3389153},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1345-1359},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {On off-chaining smart contract runtime protection: A queuing model approach},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AutoDDL: Automatic distributed deep learning with
near-optimal bandwidth cost. <em>TPDS</em>, <em>35</em>(8), 1331–1344.
(<a href="https://doi.org/10.1109/TPDS.2024.3397800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep learning are driven by the growing scale of computation, data, and models. However, efficiently training large-scale models on distributed systems requires an intricate combination of data, operator, and pipeline parallelism, which exerts heavy burden on machine learning practitioners. To this end, we propose AutoDDL, a distributed training framework that automatically explores and exploits new parallelization schemes with near-optimal bandwidth cost. AutoDDL facilitates the description and implementation of different schemes by utilizing OneFlow&#39;s Split , Broadcast , and Partial Sum (SBP) abstraction. AutoDDL is equipped with an analytical performance model combined with a customized Coordinate Descent algorithm, which significantly reduces the scheme searching overhead. We conduct evaluations on Multi-Node-Single-GPU and Multi-Node-Multi-GPU machines using different models, including VGG and Transformer. Compared to the expert-optimized implementations, AutoDDL reduces the end-to-end training time by up to 31.1% and 10% for Transformer and up to 17.7% and 71.5% for VGG on the two parallel systems, respectively.},
  archive      = {J_TPDS},
  author       = {Jinfan Chen and Shigang Li and Ran Guo and Jinhui Yuan and Torsten Hoefler},
  doi          = {10.1109/TPDS.2024.3397800},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {8},
  number       = {8},
  pages        = {1331-1344},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {AutoDDL: Automatic distributed deep learning with near-optimal bandwidth cost},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive neural control for a network of parabolic PDEs with
event-triggered mechanism. <em>TPDS</em>, <em>35</em>(7), 1320–1330. (<a
href="https://doi.org/10.1109/TPDS.2024.3401164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the finite-time consensus problem for nonlinear parabolic networks by designing a new tracking controller. For undirected topology, the newly designed controller allows to optimize the consensus time by adjusting the parameter $\beta (0&amp;lt; \beta &amp;lt; 1 )$ . First, the neural network approximation property is utilized to counteract the uncertain nonlinear dynamics of agents, and the event-triggered mechanism is designed to save energy and reduce the communication burden. Second, a tracking control protocol is proposed based on event-triggered mechanism, which drives the multi-agent system to reach leader-follower consensus in finite time. Then, by considering appropriate Lyapunov generalization functions and using some important inequalities, the sufficient condition for achieving finite-time consensus in the multi-agent system is obtained. Finally, the effectiveness of the presented method is verified by simulation.},
  archive      = {J_TPDS},
  author       = {Sai Zhang and Li Tang and Yan-Jun Liu},
  doi          = {10.1109/TPDS.2024.3401164},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  number       = {7},
  pages        = {1320-1330},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Adaptive neural control for a network of parabolic PDEs with event-triggered mechanism},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rollback-free recovery for a high performance dense linear
solver with reduced memory footprint. <em>TPDS</em>, <em>35</em>(7),
1307–1319. (<a href="https://doi.org/10.1109/TPDS.2024.3400365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scale of nowadays High Performance Computing (HPC) systems is the key element that determines the achievement of impressive performance, as well as the reason for their relatively limited reliability. Over the last decade, specific areas of the High Performance Computing (HPC) research field have addressed the issue at different levels, by enriching the infrastructure, the platforms, or the algorithms with fault tolerance features. In this work, we focus on the rather-pervasive task of computing the solution of a dense, unstructured linear system and we propose an algorithm-based technique to obtain fault tolerance to multiple anywhere-located faults during the parallel computation. We particularly study the ways to boost the performance of the rollback-free recovery, and we provide an extensive evaluation of our technique w.r.t. to other state-of-the-art algorithm-based methods.},
  archive      = {J_TPDS},
  author       = {Daniela Loreti and Marcello Artioli and Anna Ciampolini},
  doi          = {10.1109/TPDS.2024.3400365},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  number       = {7},
  pages        = {1307-1319},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Rollback-free recovery for a high performance dense linear solver with reduced memory footprint},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CREPE: Concurrent reverse-modulo-scheduling and placement
for CGRAs. <em>TPDS</em>, <em>35</em>(7), 1293–1306. (<a
href="https://doi.org/10.1109/TPDS.2024.3402098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coarse-Grained Reconfigurable Array (CGRA) architectures are popular as high-performance and energy-efficient computing devices. Compute-intensive loop constructs of complex applications are mapped onto CGRAs by modulo-scheduling the innermost loop dataflow graph (DFG). In the state-of-the-art approaches, mapping quality is typically determined by initiation interval ( II ), while schedule length for one iteration is neglected. However, for nested loops, schedule length becomes important. In this article, we propose CREPE, a C oncurrent Re verse-modulo-scheduling and P lac e ment technique for CGRAs that minimizes both II and schedule length . CREPE performs simultaneous modulo-scheduling and placement coupled with dynamic graph transformations, generating good-quality mappings with high success rates. Furthermore, we introduce a compilation flow that maps nested loops onto the CGRA and modulo-schedules the innermost loop using CREPE. Experiments show that the proposed solution outperforms the conventional approaches in mapping success rate and total execution time with no impact on the compilation time. CREPE maps all kernels considered while state-of-the-art techniques Crimson and Epimap failed to find a mapping or mapped at very high II s. On a 2×4 CGRA, CREPE reports a 100% success rate and a speed-up up to 5.9× and 1.4× over Crimson with 78.5% and Epimap with 46.4% success rates respectively.},
  archive      = {J_TPDS},
  author       = {Chilankamol Sunny and Satyajit Das and Kevin J. M. Martin and Philippe Coussy},
  doi          = {10.1109/TPDS.2024.3402098},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  number       = {7},
  pages        = {1293-1306},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CREPE: Concurrent reverse-modulo-scheduling and placement for CGRAs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AdaptChain: Adaptive data sharing and synchronization for
NFV systems on heterogeneous architectures. <em>TPDS</em>,
<em>35</em>(7), 1281–1292. (<a
href="https://doi.org/10.1109/TPDS.2024.3400594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a Network Function Virtualization (NFV) system, network functions (NFs) are implemented on general-purpose hardware, including CPU, GPU, and FPGA. Studies have shown that there is no one-size-fits-all processor, as each processor demonstrates performance advantages to implement certain types of NFs. With more general-purpose processors such as GPUs being deployed in data center servers, the best practice to build a high-performance NFV service chain should employ available heterogeneous processors. However, current NFV systems fail to utilize these processors for acceleration. This is because, due to separate memory spaces, data synchronization is demanded to guarantee correctness, which can incur non-trivial overhead and result in low performance. This paper proposes AdaptChain, a data management facility that enables adaptive data sharing and synchronization for hybrid NFV systems on heterogeneous architectures. AdaptChain shares the host and device memory among NFs in a service chain. With adaptive synchronization plan generation and NF code adaptation, AdaptChain exploits three classes of opportunities to reduce the amount of synchronized data while guaranteeing correctness. Experimental results show that AdaptChain improves the overall throughput by up to 3.2× and reduces the latency by up to 52%.},
  archive      = {J_TPDS},
  author       = {Kai Zhang and Jiahui Hong and Zhengying He and Yinan Jing and X. Sean Wang},
  doi          = {10.1109/TPDS.2024.3400594},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  number       = {7},
  pages        = {1281-1292},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {AdaptChain: Adaptive data sharing and synchronization for NFV systems on heterogeneous architectures},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Availability-aware revenue-effective application deployment
in multi-access edge computing. <em>TPDS</em>, <em>35</em>(7),
1268–1280. (<a href="https://doi.org/10.1109/TPDS.2024.3399840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-access edge computing (MEC) has emerged as a promising computing paradigm to push computing resources and services to the network edge. It allows applications/services to be deployed on edge servers for provisioning low-latency services to nearby users. However, in the MEC environment, edge servers may suffer from failures while the app vendor has to guarantee continuously available services to its users, thereby securing its revenue for application instances deployed. In this paper, we focus on available service provisioning when cost-effectively deploying application instances on edge servers. We first formulate a novel A vailability-aware R evenue-effective A pplication D eployment (ARAD) problem in the MEC environment with the aim to maximize the overall revenue by considering both service availability benefit and deployment cost. We prove that the ARAD problem is $\mathcal {NP}$ -hard. Then, we propose an approximation algorithm named ARAD-A to find the ARAD solution efficiently with a constant approximation ratio of $\frac{1}{2}$ . We extensively evaluate the performance of ARAD-A against five representative approaches. Experimental results demonstrate that our ARAD-A can achieve the best performance in securing the app vendor&#39;s overall revenue.},
  archive      = {J_TPDS},
  author       = {Lu Zhao and Fu Xiao and Bo Li and Jian Zhou and Xiaolong Xu and Yun Yang},
  doi          = {10.1109/TPDS.2024.3399840},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  number       = {7},
  pages        = {1268-1280},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Availability-aware revenue-effective application deployment in multi-access edge computing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Bayesian-driven automated scaling in stream computing with
multiple QoS targets. <em>TPDS</em>, <em>35</em>(7), 1251–1267. (<a
href="https://doi.org/10.1109/TPDS.2024.3399834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stream processing systems commonly work with auto-scaling to ensure resource efficiency and quality of service (QoS). Existing auto-scaling solutions lack accuracy in resource allocation because they rely on static QoS-resource models that fail to account for high workload variability and use indirect metrics with much distractive information. Moreover, different types of QoS metrics present different characteristics and thus need individual auto-scaling methods. In this paper, we propose a versatile auto-scaling solution for operator-level parallelism configuration, called AuTraScale+, to meet the throughput, processing-time latency, and event-time latency targets. AuTraScale+ follows the Bayesian optimization framework to make scaling decisions. First, it uses the Gaussian process model to eliminate the negative influence of uncertain factors on the performance model accuracy. Second, it leverages the expected improvement-based (EI-based) acquisition function to search and recommend the optimal configuration quickly. Besides, to make a more accurate scaling decision when the new model is not ready, AuTraScale+ proposes a transfer learning algorithm to estimate the benefits of all configurations at a new rate based on existing models and then recommend the optimal one. We implement and evaluate AuTraScale+ on the Flink platform. The experimental results on three representative workloads demonstrate that compared with the state-of-the-art methods, AuTraScale+ can reduce 66.6% and 36.7% resource consumption, respectively, in the scale-down and scale-up scenarios while achieving their throughput and processing-time latency targets. Compared with other methods of optimizing event-time latency, AuTraScale+ saves 26.9% of resources on average.},
  archive      = {J_TPDS},
  author       = {Liang Zhang and Wenli Zheng and Kuangyu Zheng and Hongzi Zhu and Chao Li and Minyi Guo},
  doi          = {10.1109/TPDS.2024.3399834},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  number       = {7},
  pages        = {1251-1267},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Bayesian-driven automated scaling in stream computing with multiple QoS targets},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spiking neural p systems with microglia. <em>TPDS</em>,
<em>35</em>(7), 1239–1250. (<a
href="https://doi.org/10.1109/TPDS.2024.3399755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural P systems (SNP systems), one of the parallel and distributed computing models with biological interpretability, have been a hot research topic in bio-inspired computational models in recent years. To improve the stability of the models, this study introduces microglia in the biological nervous system into SNP systems and proposes SNP systems with microglia (MSNP systems). In MSNP systems, besides neurons, another cell type named microglia is introduced. Microglia can help neurons in the range of action maintain homeostasis and prevent excitotoxicity, i.e., excessive excitability. Specifically, microglia use a new microglial maintenance rule to lower the number of spikes in neurons within their range of action when it is too high. The computational capability and efficiency of MSNP systems are also proved. This study makes SNP systems more stable and avoids data overflow or data explosion problems to some degree.},
  archive      = {J_TPDS},
  author       = {Yuzhen Zhao and Xiyu Liu},
  doi          = {10.1109/TPDS.2024.3399755},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  number       = {7},
  pages        = {1239-1250},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Spiking neural p systems with microglia},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph-centric performance analysis for large-scale parallel
applications. <em>TPDS</em>, <em>35</em>(7), 1221–1238. (<a
href="https://doi.org/10.1109/TPDS.2024.3396849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance analysis is essential for understanding the performance behaviors of parallel programs and detecting performance bottlenecks. Whereas, complex interconnections across several types of performance bugs, as well as inter-process communications and data dependence, make efficient performance analysis even more difficult. Despite the fact that many performance tools have been developed, accurately identifying underlying performance bottlenecks for such complex scenarios requires specific in-depth analysis. Significant human efforts and analysis knowledge are often required to implement each specific analytic task. To alleviate the complexity of developing specific performance analytic tasks, we present a programmable performance analysis tool, called PerFlow . In PerFlow , a step-by-step performance analysis process is represented as an Analysis Flow Diagram, which is constructed with several performance analysis sub-tasks, namely passes, that can be defined by developers or provided by PerFlow ’s built-in analysis pass library. Furthermore, we define a Performance Abstraction Graph to describe the performance behavior of a parallel program, where the edges indicate the interactions between parallel units, therefore the analytic sub-tasks are converted to graph analysis tasks. PerFlow provides plentiful Python APIs for developing analytic tasks. Several case studies of real-world applications with up to 700 K lines of code are used to demonstrate the effectiveness of PerFlow . The results indicate that PerFlow makes it much easier to implement specific performance analytic tasks, and these tasks are performed automatically and efficiently to detect underlying performance bottlenecks.},
  archive      = {J_TPDS},
  author       = {Yuyang Jin and Haojie Wang and Runxin Zhong and Chen Zhang and Xia Liao and Feng Zhang and Jidong Zhai},
  doi          = {10.1109/TPDS.2024.3396849},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  number       = {7},
  pages        = {1221-1238},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Graph-centric performance analysis for large-scale parallel applications},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fed-RAC: Resource-aware clustering for tackling
heterogeneity of participants in federated learning. <em>TPDS</em>,
<em>35</em>(7), 1207–1220. (<a
href="https://doi.org/10.1109/TPDS.2024.3379933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning is a training framework that enables multiple participants to collaboratively train a shared model while preserving data privacy. The heterogeneity of devices and networking resources of the participants delay the training and aggregation. The paper introduces a novel approach to federated learning by incorporating resource-aware clustering. This method addresses the challenges posed by the diverse devices and networking resources among participants. Unlike static clustering approaches, this paper proposes a dynamic method to determine the optimal number of clusters using Dunn Indices. It enables adaptability to the varying heterogeneity levels among participants, ensuring a responsive and customized approach to clustering. Next, the paper goes beyond empirical observations by providing a mathematical derivation of the communication rounds for convergence within each cluster. Further, the participant assignment mechanism adds a layer of sophistication and ensures that devices and networking resources are allocated optimally. Afterwards, we incorporate a leader-follower technique, particularly through knowledge distillation, which improves the performance of lightweight models within clusters. Finally, experiments are conducted to validate the approach and to compare it with state-of-the-art. The results demonstrated an accuracy improvement of over 3% compared to its closest competitor and a reduction in communication rounds of around 10%.},
  archive      = {J_TPDS},
  author       = {Rahul Mishra and Hari Prabhat Gupta and Garvit Banga and Sajal K. Das},
  doi          = {10.1109/TPDS.2024.3379933},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  number       = {7},
  pages        = {1207-1220},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Fed-RAC: Resource-aware clustering for tackling heterogeneity of participants in federated learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FedREM: Guided federated learning in the presence of dynamic
device unpredictability. <em>TPDS</em>, <em>35</em>(7), 1189–1206. (<a
href="https://doi.org/10.1109/TPDS.2024.3396133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a promising distributed machine learning scheme where multiple clients collaborate by sharing a common learning model while maintaining their private data locally. It can be applied to a lot of applications, e.g., training an automatic driving system by the perception of multiple vehicles. However, some clients may join the training system dynamically, which affects the stability and accuracy of the learning system a IoT. Meanwhile, data heterogeneity in the FL system exacerbates the above problem further due to imbalanced data distribution. To solve the above problems, we propose a novel FL framework named FedREM (Retain-Expansion and Matching), which guides clients training models by two mechanisms. They are 1) a Retain-Expansion mechanism that can let clients perform local training and extract data characteristics automatically during the training and 2) a Matching mechanism that can ensure new clients quickly adapt to the global model based on matching their data characteristics and adjusting the model accordingly. Results of extensive experiments verify that our FedREM outperforms various baselines in terms of model accuracy, communication efficiency, and system robustness.},
  archive      = {J_TPDS},
  author       = {Linsi Lan and Junbo Wang and Zhi Li and Krishna Kant and Wanquan Liu},
  doi          = {10.1109/TPDS.2024.3396133},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  number       = {7},
  pages        = {1189-1206},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FedREM: Guided federated learning in the presence of dynamic device unpredictability},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FastTuning: Enabling fast and efficient hyper-parameter
tuning with partitioning and parallelism of search space. <em>TPDS</em>,
<em>35</em>(7), 1174–1188. (<a
href="https://doi.org/10.1109/TPDS.2024.3386939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyper-parameter tuning (HPT) for deep learning (DL) models is prohibitively expensive. Sequential model-based optimization (SMBO) emerges as the state-of-the-art (SOTA) approach to automatically optimize HPT performance due to its heuristic advantages. Unfortunately, focusing on algorithm optimization rather than a large-scale parallel HPT system, existing SMBO-based approaches still cannot effectively remove their strong sequential nature, posing two performance problems: (1) extremely low tuning speed and (2) sub-optimal model quality . In this paper, we propose FastTuning, a fast, scalable, and generic system aiming at parallelly accelerating SMBO-based HPT for large DL/ML models. The key is to partition the highly complex search space into multiple smaller sub-spaces, each of which is assigned to and optimized by a different tuning worker in parallel. However, determining the right level of resource allocation to strike a balance between quality and cost remains a challenge. To address this, we further propose NIMBLE, a dynamic scheduling strategy that is specially designed for FastTuning, including (1) Dynamic Elimination Algorithm, (2) Sub-space Re-division, and (3) Posterior Information Sharing. Finally, we incorporate 6 SOTAs (i.e., 3 tuning algorithms and 3 parallel tuning tools) into FastTuning. Experimental results, on ResNet18, VGG19, ResNet50, and ResNet152, show that FastTuning can consistently offer much faster tuning speed (up to $80\times$ ) with better accuracy (up to 4.7% improvement), thereby enabling the application of automatic HPT to real-life DL models.},
  archive      = {J_TPDS},
  author       = {Xiaqing Li and Qi Guo and Guangyan Zhang and Siwei Ye and Guanhua He and Yiheng Yao and Rui Zhang and Yifan Hao and Zidong Du and Weimin Zheng},
  doi          = {10.1109/TPDS.2024.3386939},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  number       = {7},
  pages        = {1174-1188},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FastTuning: Enabling fast and efficient hyper-parameter tuning with partitioning and parallelism of search space},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synchronize only the immature parameters:
Communication-efficient federated learning by freezing parameters
adaptively. <em>TPDS</em>, <em>35</em>(7), 1155–1173. (<a
href="https://doi.org/10.1109/TPDS.2023.3241965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning allows edge devices to collaboratively train a global model without sharing their local private data. Yet, with limited network bandwidth at the edge, communication often becomes a severe bottleneck. In this article, we find that it is unnecessary to always synchronize the full model in the entire training process, because many parameters already become mature (i.e., stable) prior to model convergence, and can thus be excluded from later synchronizations. This allows us to reduce the communication overhead without compromising the model accuracy. However, challenges are that the local parameters excluded from global synchronization may diverge on different clients, and meanwhile some parameters may stabilize only temporally. To address these challenges, we propose a novel scheme called Adaptive Parameter Freezing (APF), which fixes (freezes) the non-synchronized stable parameters in intermittent periods. Specifically, the freezing periods are tentatively adjusted in an additively-increase and multiplicatively-decrease manner—depending on whether the previously-frozen parameters remain stable in subsequent iterations. We also extend APF into APF# and APF++, which freeze parameters in a more aggressive manner to achieve larger performance benefit for large complex models. We implemented APF and its variants as Python modules with PyTorch, and extensive experiments show that APF can reduce data transfer amount by over $60\%$ .},
  archive      = {J_TPDS},
  author       = {Chen Chen and Hong Xu and Wei Wang and Baochun Li and Bo Li and Li Chen and Gong Zhang},
  doi          = {10.1109/TPDS.2023.3241965},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  number       = {7},
  pages        = {1155-1173},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Synchronize only the immature parameters: Communication-efficient federated learning by freezing parameters adaptively},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A 3D hybrid optical-electrical NoC using novel mapping
strategy based DCNN dataflow acceleration. <em>TPDS</em>,
<em>35</em>(7), 1139–1154. (<a
href="https://doi.org/10.1109/TPDS.2024.3394747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large number of multiply-accumulate operations and memory accesses required in deep convolutional neural networks (DCNN) leads to high latency and energy consumption (EC), that hinder their further applications. Dataflow-based acceleration schemes reduce memory accesses by leveraging reusable data in DCNNs. Row Stationary (RS) dataflow is a more advanced dataflow. In the convolutional layer acceleration of RS dataflow, the flexibility of mapping from logical processing element (LPE) sets to physical PE sets is relatively poor. The utilization of processing elements (PEs) is low. In this article, a novel mapping strategy based on genetic algorithm (GAMS) with the goal of optimizing EC is proposed. GAMS is designed to address the energy inefficiencies faced when mapping RS dataflow. A 3D hybrid optical-electrical Network-on-Chip (3DHOENoC) is proposed to further improve the communication efficiency, energy efficiency and the processing speed of DCNN. Simulation and evaluation results show that GAMS can achieve better mapping flexibility, higher PEs utilization and 15.9% improvement of execution speed on average. In addition, the execution time (ET) performance of processing the DCNN can be further improved by adopting the 3DHOENoC architecture with better communication parallelism.},
  archive      = {J_TPDS},
  author       = {Bowen Zhang and Huaxi Gu and Grace Li Zhang and Yintang Yang and Ziteng Ma and Ulf Schlichtmann},
  doi          = {10.1109/TPDS.2024.3394747},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  number       = {7},
  pages        = {1139-1154},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A 3D hybrid optical-electrical NoC using novel mapping strategy based DCNN dataflow acceleration},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HiHGNN: Accelerating HGNNs through parallelism and data
reusability exploitation. <em>TPDS</em>, <em>35</em>(7), 1122–1138. (<a
href="https://doi.org/10.1109/TPDS.2024.3394841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous graph neural networks (HGNNs) have emerged as powerful algorithms for processing heterogeneous graphs (HetGs), widely used in many critical fields. To capture both structural and semantic information in HetGs, HGNNs first aggregate the neighboring feature vectors for each vertex in each semantic graph and then fuse the aggregated results across all semantic graphs for each vertex. Unfortunately, existing graph neural network accelerators are ill-suited to accelerate HGNNs. This is because they fail to efficiently tackle the specific execution patterns and exploit the high-degree parallelism as well as data reusability inside and across the processing of semantic graphs in HGNNs. In this work, we first quantitatively characterize a set of representative HGNN models on GPU to disclose the execution bound of each stage, inter-semantic-graph parallelism, and inter-semantic-graph data reusability in HGNNs. Guided by our findings, we propose a high-performance HGNN accelerator, HiHGNN, to alleviate the execution bound and exploit the newfound parallelism and data reusability in HGNNs. Specifically, we first propose a bound-aware stage-fusion methodology that tailors to HGNN acceleration, to fuse and pipeline the execution stages being aware of their execution bounds. Second, we design an independency-aware parallel execution design to exploit the inter-semantic-graph parallelism. Finally, we present a similarity-aware execution scheduling to exploit the inter-semantic-graph data reusability. Compared to the state-of-the-art software framework running on NVIDIA GPU T4 and GPU A100, HiHGNN respectively achieves an average 40.0× and 8.3× speedup as well as 99.59% and 99.74% energy reduction with quintile the memory bandwidth of GPU A100.},
  archive      = {J_TPDS},
  author       = {Runzhen Xue and Dengke Han and Mingyu Yan and Mo Zou and Xiaocheng Yang and Duo Wang and Wenming Li and Zhimin Tang and John Kim and Xiaochun Ye and Dongrui Fan},
  doi          = {10.1109/TPDS.2024.3394841},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {7},
  number       = {7},
  pages        = {1122-1138},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {HiHGNN: Accelerating HGNNs through parallelism and data reusability exploitation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FedICT: Federated multi-task distillation for multi-access
edge computing. <em>TPDS</em>, <em>35</em>(6), 1107–1121. (<a
href="https://doi.org/10.1109/TPDS.2023.3289444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing interest in intelligent services and privacy protection for mobile devices has given rise to the widespread application of federated learning in Multi-access Edge Computing (MEC). Diverse user behaviors call for personalized services with heterogeneous Machine Learning (ML) models on different devices. Federated Multi-task Learning (FMTL) is proposed to train related but personalized ML models for different devices, whereas previous works suffer from excessive communication overhead during training and neglect the model heterogeneity among devices in MEC. Introducing knowledge distillation into FMTL can simultaneously enable efficient communication and model heterogeneity among clients, whereas existing methods rely on a public dataset, which is impractical in reality. To tackle this dilemma, Fed erated Mult I -task Distillation for Multi-access Edge C ompu T ing (FedICT) is proposed. FedICT direct local-global knowledge aloof during bi-directional distillation processes between clients and the server, aiming to enable multi-task clients while alleviating client drift derived from divergent optimization directions of client-side local models. Specifically, FedICT includes Federated Prior Knowledge Distillation (FPKD) and Local Knowledge Adjustment (LKA). FPKD is proposed to reinforce the clients’ fitting of local data by introducing prior knowledge of local data distributions. Moreover, LKA is proposed to correct the distillation loss of the server, making the transferred local knowledge better match the generalized representation. Extensive experiments on three datasets demonstrate that FedICT significantly outperforms all compared benchmarks in various data heterogeneous and model architecture settings, achieving improved accuracy with less than 1.2% training communication overhead compared with FedAvg and no more than 75% training communication round compared with FedGKT in all considered scenarios.},
  archive      = {J_TPDS},
  author       = {Zhiyuan Wu and Sheng Sun and Yuwei Wang and Min Liu and Quyang Pan and Xuefeng Jiang and Bo Gao},
  doi          = {10.1109/TPDS.2023.3289444},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {1107-1121},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FedICT: Federated multi-task distillation for multi-access edge computing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Formal definitions and performance comparison of consistency
models for parallel file systems. <em>TPDS</em>, <em>35</em>(6),
937–951. (<a href="https://doi.org/10.1109/TPDS.2024.3391058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The semantics of HPC storage systems are defined by the consistency models to which they abide. Storage consistency models have been less studied than their counterparts in memory systems, with the exception of the POSIX standard and its strict consistency model. The use of POSIX consistency imposes a performance penalty that becomes more significant as the scale of parallel file systems increases and the access time to storage devices, such as node-local solid storage devices, decreases. While some efforts have been made to adopt relaxed storage consistency models, these models are often defined informally and ambiguously as by-products of a particular implementation. In this work, we establish a connection between memory consistency models and storage consistency models and revisit the key design choices of storage consistency models from a high-level perspective. Further, we propose a formal and unified framework for defining storage consistency models and a layered implementation that can be used to easily evaluate their relative performance for different I/O workloads. Finally, we conduct a comprehensive performance comparison of two relaxed consistency models on a range of commonly seen parallel I/O workloads, such as checkpoint/restart of scientific applications and random reads of deep learning applications. We demonstrate that for certain I/O scenarios, a weaker consistency model can significantly improve the I/O performance. For instance, in small random reads that are typically found in deep learning applications, session consistency achieved a 5x improvement in I/O bandwidth compared to commit consistency, even at small scales.},
  archive      = {J_TPDS},
  author       = {Chen Wang and Kathryn Mohror and Marc Snir},
  doi          = {10.1109/TPDS.2024.3391058},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {937-951},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Formal definitions and performance comparison of consistency models for parallel file systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SLO-aware function placement for serverless workflows with
layer-wise memory sharing. <em>TPDS</em>, <em>35</em>(6), 919–936. (<a
href="https://doi.org/10.1109/TPDS.2024.3391858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Function-as-a-Service (FaaS) is a promising cloud computing model known for its scalability and elasticity. In various application domains, FaaS workflows have been widely adopted to manage user requests and complete computational tasks efficiently. Motivated by the fact that function containers collaboratively use the image layer&#39;s memory, co-placing functions would leverage memory sharing to reduce cluster memory footprint, this article studies layer-wise memory sharing for serverless functions. We find that overwhelming memory sharing by placing containers in the same cluster machine may lead to performance deterioration and Service Level Objective (SLO) violations due to the increased CPU pressure. We investigate how to maximally reduce cluster memory footprint via layer-wise memory sharing for serverless workflows while guaranteeing their SLO. First, we study the container memory sharing problem under serverless workflows with a static Directed Acyclic Graph (DAG) structure. We prove it is NP-Hard and propose a 2-approximation algorithm, namely MDP. Then we consider workflows with dynamic DAG structure scenarios, where the memory sharing problem is also NP-Hard. We design a Greedy-based algorithm called GSP to address this issue. We implement a carefully designed prototype on the OpenWhisk platform, and our evaluation results demonstrate that both MDP and GSP achieve a balanced and satisfying state, effectively reducing up to 63 $\%$ of cache memory usage while guaranteeing serverless workflow SLO.},
  archive      = {J_TPDS},
  author       = {Dazhao Cheng and Kai Yan and Xinquan Cai and Yili Gong and Chuang Hu},
  doi          = {10.1109/TPDS.2024.3391858},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {919-936},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SLO-aware function placement for serverless workflows with layer-wise memory sharing},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HRCM: A hierarchical regularizing mechanism for sparse and
imbalanced communication in whole human brain simulations.
<em>TPDS</em>, <em>35</em>(6), 901–918. (<a
href="https://doi.org/10.1109/TPDS.2024.3387720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain simulation is one of the most important measures to understand how information is represented and processed in the brain, which usually needs to be realized in supercomputers with a large number of interconnected graphical processing units (GPUs). For the whole human brain simulation, tens of thousands of GPUs are utilized to simulate tens of billions of neurons and tens of trillions of synapses for the living brain to reveal functional connectivity patterns. However, as an application of the irregular spares communication problem on a large-scale system, the sparse and imbalanced communication patterns of the human brain make it particularly challenging to design a communication system for supporting large-scale brain simulations. To face this challenge, this paper proposes a hierarchical regularized communication mechanism, HRCM. The HRCM maintains a hierarchical virtual communication topology (HVCT) with a merge-forward algorithm that exploits the sparsity of neuron interactions to regularize inter-process communications in brain simulations. HRCM also provides a neuron-level partition scheme for assigning neurons to simulation processes to balance the communication load while improving resource utilization. In HRCM, neuron partition is formulated as a k-way graph partition problem and solved efficiently by the proposed hybrid multi-constraint greedy (HMCG) algorithm. HRCM has been implemented in human brain simulations at the scale of up to 86 billion neurons running on 10000 GPUs. Results obtained from extensive simulation experiments verify the effectiveness of HRCM in significantly reducing communication delay, increasing resource usage, and shortening simulation time for large-scale human brain models.},
  archive      = {J_TPDS},
  author       = {Xin Du and Minglong Wang and Zhihui Lu and Qiang Duan and Yuhao Liu and Jianfeng Feng and Huarui Wang},
  doi          = {10.1109/TPDS.2024.3387720},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {901-918},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {HRCM: A hierarchical regularizing mechanism for sparse and imbalanced communication in whole human brain simulations},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient utilization of multi-threading parallelism on
heterogeneous systems for sparse tensor contraction. <em>TPDS</em>,
<em>35</em>(6), 889–900. (<a
href="https://doi.org/10.1109/TPDS.2024.3391254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many fields of scientific simulation, such as chemistry and condensed matter physics, are increasingly eschewing dense tensor contraction in favor of sparse tensor contraction. In this work, we center around binary sparse tensor contraction (SpTC) which has the challenges of index matching and accumulation. To address these difficulties, we present GSpTC, an efficient element-wise SpTC framework on CPU-GPU heterogeneous systems. GSpTC first introduces a fine-grained partitioning strategy based on element-wise tensor contraction. By analyzing and selecting appropriate dimension partitioning strategies, we can efficiently utilize the multi-threading parallelism on GPUs and optimize the overall performance of GSpTC. In particular, GSpTC leverages multi-threading parallelism on GPUs for the contraction phase and merging phase, which greatly accelerates the computation phase in sparse tensor contraction computations. Furthermore, GSpTC employs parallel pipeline technology to hide the data transmission time between the host and the device, further enhancing its performance. As a result, GSpTC achieves an average performance improvement of 267% compared to the previous state-of-the-art framework Sparta.},
  archive      = {J_TPDS},
  author       = {Guoqing Xiao and Chuanghui Yin and Yuedan Chen and Mingxing Duan and Kenli Li},
  doi          = {10.1109/TPDS.2024.3391254},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {889-900},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient utilization of multi-threading parallelism on heterogeneous systems for sparse tensor contraction},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sampling-based multi-job placement for heterogeneous deep
learning clusters. <em>TPDS</em>, <em>35</em>(6), 874–888. (<a
href="https://doi.org/10.1109/TPDS.2024.3390109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous deep learning clusters commonly host a variety of distributed learning jobs. In such scenarios, the training efficiency of learning models is negatively affected by the slowest worker. To accelerate the training process, multiple learning jobs may compete for limited computational resources, posing significant challenges to multi-job placement among heterogeneous workers. This article presents a heterogeneity-aware scheduler to solve the multi-job placement problem while taking into account job sizing and load balancing, minimizing the average Job Completion Time (JCT) of deep learning jobs. A novel scheme based on proportional training workload assignment, feasible solution categorization, and matching markets is proposed with theoretical guarantees. To further reduce the computational complexity for low latency decision-making and improve scheduling fairness, we propose to construct the sparsification of feasible solution categories through sampling, which has negligible performance loss in JCT. We evaluate the performance of our design with real-world deep neural network benchmarks on heterogeneous computing clusters. Experimental results show that, compared to existing solutions, the proposed sampling-based scheme can achieve 1) results within 2.04% of the optimal JCT with orders-of-magnitude improvements in algorithm running time, and 2) high scheduling fairness among learning jobs.},
  archive      = {J_TPDS},
  author       = {Kaiyang Liu and Jingrong Wang and Zhiming Huang and Jianping Pan},
  doi          = {10.1109/TPDS.2024.3390109},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {874-888},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Sampling-based multi-job placement for heterogeneous deep learning clusters},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HPETC: History priority enhanced tensor completion for
network distance measurement. <em>TPDS</em>, <em>35</em>(6), 857–873.
(<a href="https://doi.org/10.1109/TPDS.2023.3274305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In network distance measurement, how to estimate the whole network distance data from partially observed samples has attracted lots of attention because of its significance for network performance evaluation. Matrix completion becomes the most effective approach. However, the two-dimension matrix can only capture the spatial features in the network distance data while ignoring the temporal features. To conquer the problem, few recent studies begin to model the network distance data as a three-dimension tensor and propose tensor completion approaches for distance estimation. Although promising, existing tensor completion approaches still suffer the problem of low recovery accuracy and high measurement cost because they ignore the history priority information. To fully utilize both spatial and temporal features hidden in the distance data, this paper formulates a novel History Priority Enhanced Tensor Completion (HPETC) for distance estimation as a weighted tensor nuclear norm minimization problem where the weight is defined based on the history subspaces information. To solve the weighted tensor nuclear norm minimization problem, we first transform it into a factorization-based Frobenius norm minimization problem to avoid costly T-SVD computations, and then propose an iterative algorithm to solve the transformed problem. We further derive a theoretical sampling bound that is lower than the existing sampling bound, thus leads a lower measurement cost. We demonstrate the effectiveness of the proposed algorithm by conducting extensive experiments using two real network distance datasets. The result shows that the proposed algorithm can not only improve the estimation accuracy but also reduce the sampling complexity compared to the state-of-the-art approaches.},
  archive      = {J_TPDS},
  author       = {Cheng Wang and Kun Xie and Jiazheng Tian and Jigang Wen and Xiaocan Li and Gaogang Xie and Kenli Li},
  doi          = {10.1109/TPDS.2023.3274305},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {857-873},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {HPETC: History priority enhanced tensor completion for network distance measurement},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MPMoE: Memory efficient MoE for pre-trained models with
adaptive pipeline parallelism. <em>TPDS</em>, <em>35</em>(6), 843–856.
(<a href="https://doi.org/10.1109/TPDS.2024.3385639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the Mixture-of-Experts (MoE) technique has gained widespread popularity as a means to scale pre-trained models to exceptionally large sizes. Dynamic activation of experts allows for conditional computation, increasing the number of parameters of neural networks, which is critical for absorbing the vast amounts of knowledge available in many deep learning areas. However, despite the existing system and algorithm optimizations, there are significant challenges to be tackled when it comes to the inefficiencies of communication and memory consumption. In this paper, we present the design and implementation of MPMoE, a high-performance library that accelerates MoE training with adaptive and memory-efficient pipeline parallelism. Inspired by that the MoE training procedure can be divided into multiple independent sub-stages. We design a pipeline parallelism method for reducing communication latency by overlapping with computation operations. Further, we analyze the memory footprint breakdown of MoE training and identify that activations and temporary buffers are the primary contributors to the overall memory footprint. Toward memory efficiency, we propose memory reuse strategies to reduce memory requirements by eliminating memory redundancies. Finally, to optimize pipeline granularity and memory reuse strategies jointly, we propose a profile-based algorithm and a performance model to determine the configurations of MPMoE at runtime. We implement MPMoE upon PyTorch and evaluate it with common MoE models in two physical clusters, including 64 NVIDIA A100 GPU cards and 16 NVIDIA V100 GPU cards. Compared with the state-of-art approach, MPMoE achieves up to 2.3× speedup while reducing more than 30% memory footprint for training large models.},
  archive      = {J_TPDS},
  author       = {Zheng Zhang and Yaqi Xia and Hulin Wang and Donglin Yang and Chuang Hu and Xiaobo Zhou and Dazhao Cheng},
  doi          = {10.1109/TPDS.2024.3385639},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {843-856},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {MPMoE: Memory efficient MoE for pre-trained models with adaptive pipeline parallelism},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analytical modeling and throughput computation of blockchain
sharding. <em>TPDS</em>, <em>35</em>(6), 828–842. (<a
href="https://doi.org/10.1109/TPDS.2024.3376452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sharding has shown great potential to scale out blockchains. It divides nodes into smaller groups which allow for partial transaction processing, relaying and storage. Hence, instead of running one blockchain, we will run multiple blockchains in parallel, and call each one a shard. Sharding can be applied to address shortcomings due to compulsory duplication of three resources in blockchains, i.e., computation, communication and storage. The most pressing issue in blockchains today is throughput. In this paper, we propose new queueing-theoretic models to derive the maximum throughput of sharded blockchains. We consider two cases, a fully sharded blockchain and a computation sharding. We model each with a queueing network that exploits signals to account for block production as well as multi-destination cross-shard transactions. We make sure quasi-reversibility for every queue in our models is satisfied so that they fall into the category of product-form queueing networks. We then obtain a closed-form solution for the maximum stable throughput of these systems with respect to block size, block rate, number of destinations in transactions and the number of shards. Comparing the results obtained from the two introduced sharding systems, we conclude that the extent of sharding in different domains plays a significant role in scalability.},
  archive      = {J_TPDS},
  author       = {Pourya Soltani and Farid Ashtiani},
  doi          = {10.1109/TPDS.2024.3376452},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {828-842},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Analytical modeling and throughput computation of blockchain sharding},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HybridChain: Fast, accurate, and secure transaction
processing with distributed learning. <em>TPDS</em>, <em>35</em>(6),
813–827. (<a href="https://doi.org/10.1109/TPDS.2024.3381593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to fully unlock the transformative power of distributed ledgers and blockchains, it is crucial to develop innovative consensus algorithms that can overcome the obstacles of security, scalability, and interoperability, which currently hinder their widespread adoption. This paper introduces HybridChain that combines the advantages of sharded blockchain and DAG distributed ledger, and a consensus algorithm that leverages decentralized learning. Our approach involves validators exchanging perceptions as votes to assess potential conflicts between transactions and the witness set, representing input transactions in the UTXO model. These perceptions collectively contribute to an intermediate belief regarding the validity of transactions. By integrating their beliefs with those of other validators, localized decisions are made to determine validity. Ultimately, a final consensus is achieved through a majority vote, ensuring precise and efficient validation of transactions. Our proposed approach is compared to the existing DAG-based scheme IOTA and the sharded blockchain Omniledger through extensive simulations. The results show that IOTA has high throughput and low latency but sacrifices accuracy and is vulnerable to orphanage attacks especially with low transaction rates. Omniledger achieves stable accuracy by increasing shards but has increased latency. In contrast, the proposed HybridChain exhibits fast, accurate, and secure transaction processing, and excellent scalability.},
  archive      = {J_TPDS},
  author       = {Amirhossein Taherpour and Xiaodong Wang},
  doi          = {10.1109/TPDS.2024.3381593},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {813-827},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {HybridChain: Fast, accurate, and secure transaction processing with distributed learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). G-learned index: Enabling efficient learned index on GPU.
<em>TPDS</em>, <em>35</em>(6), 795–812. (<a
href="https://doi.org/10.1109/TPDS.2024.3381214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AI and GPU technologies have been widely applied to solve Big Data problems. The total data volume worldwide reaches 200 zettabytes in 2022. How to efficiently index the required content among massive data becomes serious. Recently, a promising learned index has been proposed to address this challenge: It has extremely high efficiency while retaining marginal space overhead. However, we notice that previous learned indexes have mainly focused on CPU architecture, while ignoring the advantages of GPU. Because traditional indexes like B-Tree, LSM, and bitmap have greatly benefited from GPU acceleration, a combination of a learned index and GPU has great potentials to reach tremendous speedups. In this paper, we propose a GPU-based learned index, called G-Learned Index, to significantly improve the performance of learned index structures. The primary challenges in developing G-Learned Index lie in the use of thousands of GPU cores including minimization of synchronization and branch divergence, data structure design for parallel operations, and usage of memory bandwidth including limited memory transactions and multi-memory hierarchy. To overcome these challenges, a series of novel technologies are developed, including efficient thread organization, succinct data structures, and heterogeneous memory hierarchy utilization. Compared to the state-of-the-art learned index, the proposed G-Learned Index achieves an average of 174× speedup (and 107× of its parallel version). Meanwhile, we attain 2× less query time over the state-of-the-art GPU B-Tree. Our further exploration of range queries shows that G-Learned Index is $17\times$ faster than CPU multi-dimensional learned index.},
  archive      = {J_TPDS},
  author       = {Jiesong Liu and Feng Zhang and Lv Lu and Chang Qi and Xiaoguang Guo and Dong Deng and Guoliang Li and Huanchen Zhang and Jidong Zhai and Hechen Zhang and Yuxing Chen and Anqun Pan and Xiaoyong Du},
  doi          = {10.1109/TPDS.2024.3381214},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {795-812},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {G-learned index: Enabling efficient learned index on GPU},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BurstBalancer: Do less, better balance for large-scale data
center traffic. <em>TPDS</em>, <em>35</em>(6), 777–794. (<a
href="https://doi.org/10.1109/TPDS.2023.3295454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Layer-3 load balancing is a key topic in the networking field. It is well acknowledged that flowlet is the most promising solution because of its good trade-off between load balance and packet reordering. However, we find its one significant limitation: it makes the forwarding paths of flows unpredictable. To address this limitation, this article presents BurstBalancer, a simple yet efficient load balancing system with a sketch, named BalanceSketch. Our design philosophy is doing less changes to keep the forwarding path of most flows fixed, which guides the design of BalanceSketch and our balance operations. We have fully implemented BurstBalancer in a small-scale testbed built with Tofino switches, and conducted both large-scale event-level (NS-2) and ESL (electronic system level) simulations. Our results show that BurstBalancer achieves 5% $\sim$ 35% smaller FCT than LetFlow in symmetric topology and up to 30× smaller FCT in asymmetric topology, while 58× fewer flows suffer from path changing. All related codes are open-sourced at GitHub.},
  archive      = {J_TPDS},
  author       = {Zirui Liu and Yikai Zhao and Zhuochen Fan and Tong Yang and Xiaodong Li and Ruwen Zhang and Kaicheng Yang and Zihan Jiang and Zheng Zhong and Yi Huang and Cong Liu and Jing Hu and Gaogang Xie and Bin Cui},
  doi          = {10.1109/TPDS.2023.3295454},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {777-794},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {BurstBalancer: Do less, better balance for large-scale data center traffic},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parallel computation of dominance scores for
multidimensional datasets on GPUs. <em>TPDS</em>, <em>35</em>(6),
764–776. (<a href="https://doi.org/10.1109/TPDS.2024.3382119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dominance scoring problem in a multidimensional dataset is to return the number of points dominated by a given point, which is a common metric for evaluating the quality of a data point. Dominance scoring is an elementary operator for variations of the skyline operator, including top- $k$ dominating and $k$ -skyband queries. This study proposes query processing for dominance scores that operates primarily on the graphics processing unit (GPU) to fully utilize its massive processing resources and restricted memory space while reducing the transfer overhead between the central processing unit (CPU) and GPU. We introduce a heap-based multidimensional data structure with complete and well-balanced characteristics. Using our preprocessed data, we can construct a complete R-tree with the non-overlapping property, ensuring that the bounding boxes of internal nodes of the same level do not overlap, thereby reducing redundant operations. In addition, we propose two algorithms based on depth-first and breadth-first traversals to accumulate the dominance score on GPUs in parallel. Both take full advantage of the GPU&#39;s computing resources and memory space supported by the non-overlapping tree structures. Experiments on synthetic and real-world datasets demonstrate that the proposed algorithms implemented on GPUs dramatically improve the efficiency of dominance scoring.},
  archive      = {J_TPDS},
  author       = {Wei-Mei Chen and Hsin-Hung Tsai and Joon Fong Ling},
  doi          = {10.1109/TPDS.2024.3382119},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {764-776},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Parallel computation of dominance scores for multidimensional datasets on GPUs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AtRec: Accelerating recommendation model training on CPUs.
<em>TPDS</em>, <em>35</em>(6), 750–763. (<a
href="https://doi.org/10.1109/TPDS.2024.3381186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of recommendation models and the enhanced AI processing capability of CPUs have provided massive performance opportunities to deliver satisfactory experiences to a large number of users. Unfortunately, existing recommendation model training methods fail to achieve high efficiency due to unique challenges such as dynamic shape and high parallelism. To address the above limitations, we comprehensively study the distinctive characteristics of recommendation models and discover several unexploited optimization opportunities. To exploit such opportunities, we propose AtRec , a high-performant recommendation model training engine that significantly accelerates the training process on CPUs. Specifically, AtRec presents comprehensive approach of training that employs operator-level and graph-level joint optimizations and runtime optimization. At the operator-level, AtRec identifies and optimizes the time-consuming operators, which enables further efficient graph-level optimizations. At the graph-level, AtRec conducts an in-depth analysis of the inefficiencies in several frequently used subgraphs, enables further performance improvement via eliminating redundant computations and memory accesses. In addition, to achieve better runtime performance, AtRec also identifies inefficiencies prevalent in the current scheduling and proposes runtime batching. The experiment results demonstrate that AtRec can significantly outperform state-of-the-art recommendation model training engines. We have open sourced the implementation and corresponding data of AtRec to boost research in this direction.},
  archive      = {J_TPDS},
  author       = {Siqi Wang and Tianyu Feng and Hailong Yang and Xin You and Bangduo Chen and Tongxuan Liu and Zhongzhi Luan and Depei Qian},
  doi          = {10.1109/TPDS.2024.3381186},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {750-763},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {AtRec: Accelerating recommendation model training on CPUs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Taking RNA-RNA interaction to machine peak. <em>TPDS</em>,
<em>35</em>(6), 737–749. (<a
href="https://doi.org/10.1109/TPDS.2024.3380443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RNA-RNA interactions (RRIs) are essential in many biological processes, including gene transcription, translation, and localization. They play a critical role in diseases such as cancer and Alzheimer’s. Algorithms to model RRI typically use dynamic programming and have the complexity $\Theta (N^{3} \, M^{3})$ in time and $\Theta (N^{2} \, M^{2})$ in space where $N$ and $M$ are the lengths of the two RNA sequences. This makes it both essential and challenging to parallelize them. Previous efforts to do so have been hand-optimized, which is prone to human error and costly to develop and maintain. This paper presents a multi-core CPU parallelization of BPMax, one of the simpler RRI algorithms, generated by a user-guided polyhedral code generation tool, AlphaZ . The user starts with a mathematical specification of the dynamic programming algorithm and provides the choice of polyhedral program transformations such as schedules, memory-maps, and multi-level tiling. AlphaZ automatically generates highly optimized code. At the lowest level, we implemented a small hand-optimized register-tiled “matrix max-plus” kernel and integrated it with our tool-generated optimized code. Our final optimized program version is about $400\times$ faster than the base program, translating to around 312 GFLOPS, more than half of our platform&#39;s Roofline Machine Peak ( RMP ) performance. On a single core, we attain 80% of RMP . The main kernel in the algorithm, whose complexity is $\Theta (N^{3} \, M^{3})$ , attains 58 GFLOPS on a single-core and 344 GFLOPS on multi-core (90% and 58% of RMP , respectively).},
  archive      = {J_TPDS},
  author       = {Chiranjeb Mondal and Sanjay Rajopadhye},
  doi          = {10.1109/TPDS.2024.3380443},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {737-749},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Taking RNA-RNA interaction to machine peak},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HI-kyber: A novel high-performance implementation scheme of
kyber based on GPU. <em>TPDS</em>, <em>35</em>(6), 722–736. (<a
href="https://doi.org/10.1109/TPDS.2024.3379734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {CRYSTALS-Kyber, as the only public key encryption (PKE) algorithm selected by the National Institute of Standards and Technology (NIST) in the third round, is considered one of the most promising post-quantum cryptography (PQC) schemes. Lattice-based cryptography uses complex discrete algorithm problems on lattices to build secure encryption and decryption systems to resist attacks from quantum computing. Performance is an important bottleneck affecting the promotion of post quantum cryptography. In this paper, we present a High-performance Implementation of Kyber (named HI-Kyber) on the NVIDIA GPUs, which can increase the key-exchange performance of Kyber to the million-level. Firstly, we propose a lattice-based PQC implementation architecture based on kernel fusion, which can avoid redundant global-memory access operations. Secondly, We optimize and implement the core operations of CRYSTALS-Kyber, including Number Theoretic Transform (NTT), inverse NTT (INTT), pointwise multiplication, etc. Especially for the calculation bottleneck NTT operation, three novel methods are proposed to explore extreme performance: the sliced layer merging (SLM), the sliced depth-first search (SDFS-NTT) and the entire depth-first search (EDFS-NTT), which achieve a speedup of 7.5%, 28.5%, and 41.6% compared to the native implementation. Thirdly, we conduct comprehensive performance experiments with different parallel dimensions based on the above optimization. Finally, our key exchange performance reaches 1,664 kops/s. Specifically, based on the same platform, our HI-Kyber is 3.52× that of the GPU implementation based on the same instruction set and 1.78× that of the state-of-the-art one based on AI-accelerated tensor core.},
  archive      = {J_TPDS},
  author       = {Xinyi Ji and Jiankuo Dong and Tonggui Deng and Pinchang Zhang and Jiafeng Hua and Fu Xiao},
  doi          = {10.1109/TPDS.2024.3379734},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {722-736},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {HI-kyber: A novel high-performance implementation scheme of kyber based on GPU},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Taking advantage of the mistakes: Rethinking clustered
federated learning for IoT anomaly detection. <em>TPDS</em>,
<em>35</em>(6), 707–721. (<a
href="https://doi.org/10.1109/TPDS.2024.3379905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustered federated learning (CFL) is a promising solution to address the non-IID problem in the spatial domain for federated learning (FL). However, existing CFL solutions overlook the non-IID issue in the temporal domain and lack consideration of time efficiency. In this work, we propose a novel approach, called ClusterFLADS , which takes advantage of the false predictions of the inappropriate global models, together with knowledge of temperature scaling and catastrophic forgetting to reveal distributional similarities between the training data (of different clusters) and the test data. Additionally, we design an efficient feature extraction scheme by exploiting the role of each layer in a neural network&#39;s learning process. By strategically selecting model parameters and using PCA for dimensionality reduction, ClusterFLADS effectively improves clustering speed. We evaluate ClusterFLADS using real-world IoT trace data in various scenarios. Our results show that ClusterFLADS accurately and efficiently clusters clients, achieving a 100% true positive rate and low false positives across various data distributions in both the spatial and temporal domains.},
  archive      = {J_TPDS},
  author       = {Jiamin Fan and Kui Wu and Guoming Tang and Yang Zhou and Shengqiang Huang},
  doi          = {10.1109/TPDS.2024.3379905},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {6},
  number       = {6},
  pages        = {707-721},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Taking advantage of the mistakes: Rethinking clustered federated learning for IoT anomaly detection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PROV-IO<span class="math inline"><sup>+</sup></span>+: A
cross-platform provenance framework for scientific data on HPC systems.
<em>TPDS</em>, <em>35</em>(5), 844–861. (<a
href="https://doi.org/10.1109/TPDS.2024.3374555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data provenance, or data lineage, describes the life cycle of data. In scientific workflows on HPC systems, scientists often seek diverse provenance (e.g., origins of data products, usage patterns of datasets). Unfortunately, existing provenance solutions cannot address the challenges due to their incompatible provenance models and/or system implementations. In this paper, we analyze four representative scientific workflows in collaboration with the domain scientists to identify concrete provenance needs. Based on the first-hand analysis, we propose a provenance framework called PROV-IO $^+$ , which includes an I/O-centric provenance model for describing scientific data and the associated I/O operations and environments precisely. Moreover, we build a prototype of PROV-IO $^+$ to enable end-to-end provenance support on real HPC systems with little manual effort. The PROV-IO $^+$ framework can support both containerized and non-containerized workflows on different HPC platforms with flexibility in selecting various classes of provenance. Our experiments with realistic workflows show that PROV-IO $^+$ can address the provenance needs of the domain scientists effectively with reasonable performance (e.g., less than 3.5% tracking overhead for most experiments). Moreover, PROV-IO $^+$ outperforms a state-of-the-art system (i.e., ProvLake) in our experiments.},
  archive      = {J_TPDS},
  author       = {Runzhou Han and Mai Zheng and Suren Byna and Houjun Tang and Bin Dong and Dong Dai and Yong Chen and Dongkyun Kim and Joseph Hassoun and David Thorsley},
  doi          = {10.1109/TPDS.2024.3374555},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {844-861},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {PROV-IO$^+$+: A cross-platform provenance framework for scientific data on HPC systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DMA-assisted i/o for persistent memory. <em>TPDS</em>,
<em>35</em>(5), 829–843. (<a
href="https://doi.org/10.1109/TPDS.2024.3373003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern local persistent memory (PM) file systems often rely on CPU-based memory copying for data transfer between DRAM and PM, resulting in significant CPU resource consumption. While some nascent systems explore DMA (direct memory access) as an alternative for improved efficiency, the intricacies and trade-offs remain obscure. This paper investigates the feasibility of DMA for PM I/O and argues that it is not a straightforward replacement for CPU-based methods. Two key limitations hinder the direct adoption: poor performance for small data and limited bandwidth. To relieve these issues, we propose PM-DMA, a novel I/O mechanism that leverages the strengths of both CPU and DMA. It incorporates three key components: (1) L-Switch, seamlessly switches between CPU and DMA modes based on workload characteristics, maximizing performance; (2) D-Pool, reduces DMA setup overhead, improving responsiveness; (3) P-Mode, allows servicing requests through multiple channels, even hybrid CPU-DMA ones, for enhanced throughput. We implemented PM-DMA on two well-known PM file systems, NOVA and WineFS, utilizing Intel I/OAT technology. Our experimental results demonstrate substantial CPU consumption reductions across diverse workloads. Notably, under heavy load, PM-DMA delivers up to a $10.4\times$ performance improvement.},
  archive      = {J_TPDS},
  author       = {Dingding Li and Weijie Zhang and Mianxiong Dong and Kaoru Ota},
  doi          = {10.1109/TPDS.2024.3373003},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {829-843},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DMA-assisted I/O for persistent memory},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Byzantine-tolerant causal ordering for unicasts, multicasts,
and broadcasts. <em>TPDS</em>, <em>35</em>(5), 814–828. (<a
href="https://doi.org/10.1109/TPDS.2024.3368280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Byzantine fault-tolerant causal ordering of messages is useful to many applications. Causal ordering requires a property that we term strong safety, and liveness. In this paper, we use execution histories to prove that it is impossible to solve causal ordering – strong safety and liveness – in a deterministic manner for unicasts, multicasts, and broadcasts in an asynchronous system with one or more Byzantine processes. We also define a weaker version of strong safety termed weak safety. We prove that it is impossible to solve causal ordering – weak safety and liveness – in a deterministic manner for unicasts and multicasts, in an asynchronous system with one or more Byzantine processes. In view of these impossibility results, we propose the Sender-Inhibition algorithm and the Channel Sync algorithm to provide causal ordering – weak safety and liveness – of unicasts under the Byzantine failure model in synchronous systems, which have a known upper bound on message latency. The algorithms operate under the synchronous system model, but are inherently asynchronous and offer a high degree of concurrency as lock-step communication is not assumed. The two algorithms provide different trade-offs. We also indicate how the algorithms can be extended to multicasts.},
  archive      = {J_TPDS},
  author       = {Anshuman Misra and Ajay D. Kshemkalyani},
  doi          = {10.1109/TPDS.2024.3368280},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {814-828},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Byzantine-tolerant causal ordering for unicasts, multicasts, and broadcasts},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Revisiting PM-based b<span
class="math inline"><sup>+</sup></span>+-tree with persistent CPU cache.
<em>TPDS</em>, <em>35</em>(5), 796–813. (<a
href="https://doi.org/10.1109/TPDS.2024.3372621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Persistent memory (PM) promises near-DRAM performance as well as data persistence. Recently, a new feature called eADR is available for PM-equipped platforms to guarantee the persistence of CPU cache. The emergence of eADR presents unique opportunities to build lock-free data structures and unleash the full potential of PM. In this paper, we propose NBTree, a lock-free PM-friendly B $^+$ -Tree, to deliver high scalability and low PM overhead. To our knowledge, NBTree is the first persistent index designed for PM systems with persistent CPU cache. To achieve lock-free, NBTree uses atomic primitives to serialize index operations. Moreover, NBTree proposes five novel techniques to enable lock-free accesses during structural modification operations (SMO), including three-phase SMO , sync-on-write , sync-on-read , cooperative SMO , and shift-aware search . To reduce PM access overhead, NBTree employs a decoupled leaf node design to absorb the metadata accesses in DRAM. Moreover, NBTree devises a cache-crafty persistent allocator and adopts log-structured insert and in-place update/delete to enhance the access locality of write operations, absorbing a substantial amount of PM writes in persistent CPU cache. Our evaluation shows that NBTree achieves up to 11× higher throughput and 43× lower 99% tail latency than state-of-the-art persistent B $^+$ -Trees under YCSB workloads.},
  archive      = {J_TPDS},
  author       = {Bowen Zhang and Shengan Zheng and Liangxu Nie and Zhenlin Qi and Hongyi Chen and Linpeng Huang and Hong Mei},
  doi          = {10.1109/TPDS.2024.3372621},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {796-813},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Revisiting PM-based b$^{+}$+-tree with persistent CPU cache},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FHVAC: Feature-level hybrid video adaptive configuration for
machine-centric live streaming. <em>TPDS</em>, <em>35</em>(5), 780–795.
(<a href="https://doi.org/10.1109/TPDS.2024.3372046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread deployment of edge computing, the focus has shifted to machine-centric live video streaming, where endpoint-collected videos are transmitted over networks to edge servers for analysis. Unlike maximizing user&#39;s Quality of Experience (QoE), machine-centric video streaming optimizes the machine&#39;s Quality of Inference (QoI) by balancing the inference accuracy, inference delay, and transmission latency with video adaptive configuration. Traditional heuristic configuration adaption methods are reliable but unable to respond to erratic network fluctuations. Reinforcement learning (RL) based algorithms exhibit superior flexibility but suffer from exploration mechanisms, resulting in long-tail effects on upload latency. In this paper, we propose FHVAC, which dynamically selects video encoding parameters for live streaming by coherently fusing rule-based and RL-based agent at the feature level. We initially develop a robust rule-based approach for ensuring the low latency in transmission, and employ imitation learning to convert it into a neural network equivalently. Subsequently, we design a novel module to combine the two approaches and assess various fusion mechanisms. Our evaluation of FHVAC across two vision tasks (pose estimation and semantic segmentation) in two scenarios (trace-driven simulation and testbed-based experiment) shows that FHVAC enhances the average QoI, and reduces 10.61%-65.27% latency tail performance compared to prior work.},
  archive      = {J_TPDS},
  author       = {Yuanhong Zhang and Weizhan Zhang and Haipeng Du and Caixia Yan and Li Liu and Qinghua Zheng},
  doi          = {10.1109/TPDS.2024.3372046},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {780-795},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FHVAC: Feature-level hybrid video adaptive configuration for machine-centric live streaming},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing multi-grid preconditioned conjugate gradient
method on multi-cores. <em>TPDS</em>, <em>35</em>(5), 768–779. (<a
href="https://doi.org/10.1109/TPDS.2024.3372473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multigrid preconditioned conjugate gradient (MGPCG) is commonly used in high-performance computing (HPC) workloads. However, MGPCG is notoriously challenging to optimize since most of its computation kernels are memory-bounded with low arithmetic intensity and non-trivial communication patterns among parallel processes. This article presents new techniques to improve the data locality and reduce the communication overhead of MGPCG by first merging the kernels of multigrid (MG). We then develop an asynchronous neighboring communication algorithm to reduce the data communications across parallel processes. We demonstrated the benefits of our approach by applying it to the high-performance conjugate gradient (HPCG) benchmark and integrating it with a real-life algebraic multigrid package. We test the resulting software implementations on three ARMv8 and one Intel Xeon system. Experimental results show that our approach leads to a 1.62x-2.54x speedup over the engineer- and vendor-tuned HPCG implementations across various workloads and platforms.},
  archive      = {J_TPDS},
  author       = {Fan Yuan and Xiaojian Yang and Shengguo Li and Dezun Dong and Chun Huang and Zheng Wang},
  doi          = {10.1109/TPDS.2024.3372473},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {768-779},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Optimizing multi-grid preconditioned conjugate gradient method on multi-cores},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). INT-label: Lightweight in-band network-wide telemetry via
distributed labeling. <em>TPDS</em>, <em>35</em>(5), 751–767. (<a
href="https://doi.org/10.1109/TPDS.2024.3367933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-band Network Telemetry (INT) enables hop-by-hop device-internal state exposure for maintaining and troubleshooting data center networks. To achieve network-wide telemetry coverage, orchestration on top of the INT primitive is required. A straightforward solution would flood the network with INT probe packets for maximum measurement coverage, which leads to a huge bandwidth overhead. A refined solution leverages the SDN controller to collect the network topology information and carry out centralized probing path planning, which, however, is inefficient in reacting to topology changes. To tackle the above problems, we propose INT-label , a Lightweight In-band Network-Wide Telemetry architecture via the Distributed Labeling approach. INT-label periodically labels the sampled packets with device-internal states. It is cost-effective with a minor bandwidth overhead and able to seamlessly adapt to topology changes. In order to reduce the number of labeled packets, we introduce a times-based probabilistic labeling algorithm, which allows fewer packets to carry more INT information than the interval-based algorithm. In addition, to counteract the degradation of telemetry resolution due to loss of labeled packets, we design a feedback mechanism which can adaptively change the instant labeling frequency. We provide theoretical proof that INT-label can achieve network-wide telemetry. We analyze the impact of transmission delay on coverage rate and labeling times distribution under the INT-label architecture. Evaluation on software P4 switches suggests that INT-label can achieve 99.72% measurement coverage under the labeling frequency of 20 times per second. With the adaptive labeling enabled, even if 60% of the packets are lost, the coverage can still reach 92%.},
  archive      = {J_TPDS},
  author       = {Enge Song and Tian Pan and Haoyu Song and Qiang Fu and Yingjiang Liu and Chenhao Jia and Chuanying Yuan and Minglan Gao and Jiao Zhang and Tao Huang and Yunjie Liu},
  doi          = {10.1109/TPDS.2024.3367933},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {751-767},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {INT-label: Lightweight in-band network-wide telemetry via distributed labeling},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Suppressing the interference within a datacenter: Theorems,
metric and strategy. <em>TPDS</em>, <em>35</em>(5), 732–750. (<a
href="https://doi.org/10.1109/TPDS.2024.3354418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the paradigm of cloud computing, a datacenter accommodates many co-running applications sharing system resources. Although highly concurrent applications improve resource utilization, the resulting resource contention can increase the uncertainty of quality of services (QoS). Previous studies have shown that achieving high resource utilization and high QoS simultaneously is challenging. Moreover, quantifying the intensity of interference across multiple concurrent applications in a datacenter, where applications can be either latency-critical (LC) or best-effort (BE), poses a significant challenge. To address these issues, we propose Ah-Q, which comprises two theorems, a metric, and a scheduling strategy. First, we present the necessary and sufficient conditions to precisely test whether a datacenter is both QoS guaranteed and high-throughput. We also present a theorem that reveals the relationship between tail latency and throughput. Our theoretical results are insightful and useful for building datacenters that have desirable performance. Second, we propose the “System Entropy” (E ${_{S}}$ ) to quantitatively measure the interference within a datacenter. Interference arises due to resource scarcity or irrational scheduling, and effective scheduling can alleviate resource scarcity. To assess the effectiveness of a resource scheduling strategy, we introduce the concept of “resource equivalence”. We evaluate various resource scheduling strategies to demonstrate the correctness and effectiveness of the proposed theory. Third, we introduce a new resource scheduling strategy, ARQ, that leverages both isolation and sharing of resources. Our evaluations show that ARQ significantly outperforms state-of-the-art strategies PARTIES and CLITE in reducing the tail latency of LC applications and increasing the IPC of BE applications.},
  archive      = {J_TPDS},
  author       = {Yuhang Liu and Xin Deng and Jiapeng Zhou and Mingyu Chen and Yungang Bao},
  doi          = {10.1109/TPDS.2024.3354418},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {732-750},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Suppressing the interference within a datacenter: Theorems, metric and strategy},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An offline-transfer-online framework for cloud-edge
collaborative distributed reinforcement learning. <em>TPDS</em>,
<em>35</em>(5), 720–731. (<a
href="https://doi.org/10.1109/TPDS.2024.3360438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep reinforcement learning (DRL) have made it possible to train various powerful agents to perform complex tasks in real-time environments. With the next-generation communication technologies, making cloud-edge collaborative artificial intelligence service with evolved DRL agents can be a significant scenario. However, agents with different algorithms and architectures in the same DRL scenario may not be compatible, and training them is either time-consuming or resource-demanding. In this article, we design a novel cloud-edge collaborative DRL training framework, named Offline-Transfer-Online, which is a new approach that can speed up the convergence of online DRL agents at the edge by interacting with offline agents in the cloud, with the minimum data interchanged and without relying on high-quality offline datasets. Therein, we propose a novel algorithm-independent knowledge distillation algorithm for online RL agents, by leveraging pre-trained models and the interface between agents and the environment to transfer distilled knowledge among multiple heterogeneous agents efficiently. Extensive experiments show that our algorithm can accelerate the convergence of various online agents in a double to decuple speed, with comparable reward achieved in different environments.},
  archive      = {J_TPDS},
  author       = {Tianyu Zeng and Xiaoxi Zhang and Jingpu Duan and Chao Yu and Chuan Wu and Xu Chen},
  doi          = {10.1109/TPDS.2024.3360438},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {720-731},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An offline-transfer-online framework for cloud-edge collaborative distributed reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HitGNN: High-throughput GNN training framework on
CPU+multi-FPGA heterogeneous platform. <em>TPDS</em>, <em>35</em>(5),
707–719. (<a href="https://doi.org/10.1109/TPDS.2024.3371332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the size of real-world graphs increases, training Graph Neural Networks (GNNs) has become time-consuming and requires acceleration. While previous works have demonstrated the potential of utilizing FPGA for accelerating GNN training, few works have been carried out to accelerate GNN training with multiple FPGAs due to the necessity of hardware expertise and substantial development effort. To this end, we propose HitGNN, a framework that enables users to effortlessly map GNN training workloads onto a CPU+Multi-FPGA platform for acceleration. In particular, HitGNN takes the user-defined synchronous GNN training algorithm, GNN model, and platform metadata as input, determines the design parameters based on the platform metadata, and performs hardware mapping onto the CPU+Multi-FPGA platform, automatically. HitGNN consists of the following building blocks: (1) high-level application programming interfaces (APIs) that allow users to specify various synchronous GNN training algorithms and GNN models with only a handful of lines of code; (2) a software generator that generates a host program that performs mini-batch sampling, manages CPU-FPGA communication, and handles workload balancing among the FPGAs; (3) an accelerator generator that generates GNN kernels with optimized datapath and memory organization. We show that existing synchronous GNN training algorithms such as DistDGL and PaGraph can be easily deployed on a CPU+Multi-FPGA platform using our framework, while achieving high training throughput. Compared with the state-of-the-art frameworks that accelerate synchronous GNN training on a multi-GPU platform, HitGNN achieves up to 27.21× bandwidth efficiency, and up to 4.26× speedup using much less compute power and memory bandwidth than GPUs. In addition, HitGNN demonstrates good scalability to 16 FPGAs on a CPU+Multi-FPGA platform.},
  archive      = {J_TPDS},
  author       = {Yi-Chien Lin and Bingyi Zhang and Viktor K. Prasanna},
  doi          = {10.1109/TPDS.2024.3371332},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {5},
  number       = {5},
  pages        = {707-719},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {HitGNN: High-throughput GNN training framework on CPU+Multi-FPGA heterogeneous platform},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Runtime performance anomaly diagnosis in production HPC
systems using active learning. <em>TPDS</em>, <em>35</em>(4), 693–706.
(<a href="https://doi.org/10.1109/TPDS.2024.3365462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing scale and complexity of High-Performance Computing (HPC) systems, performance variations in applications caused by anomalies have become significant bottlenecks in system health and operational efficiency. As we move towards exascale systems, these variations become more prominent due to the increased sharing of resources. Such variations lead to lower energy efficiency and higher operational costs. To mitigate these problems, one must quickly and accurately diagnose the root cause of the anomalies at scale. One way to evaluate system health and identify the underlying causes is by manually examining certain performance metrics in telemetry data or using rule-based methods. Due to the daily size of telemetry data reaching terabytes and the fact that the numeric telemetry data contains thousands of metrics, manual analysis of telemetry to diagnose problems becomes challenging. Given these limitations, Machine Learning (ML)-based approaches have been gaining popularity as they have been shown to be effective and practical in diagnosing previously encountered performance anomalies. One primary challenge for supervised ML models is that they require a significant amount of labeled samples during training. However, obtaining many labels for anomalies is extremely difficult and costly, considering anomalies occur infrequently and real-world numeric system telemetry data is hard to label since it contains thousands of metrics. This paper proposes a novel active learning-based framework that diagnoses performance anomalies (i.e., identifying the type of an anomaly) in HPC systems at runtime using significantly fewer labeled samples compared to state-of-the-art ML-based approaches. We show that the proposed framework achieves the same F1-score compared to a supervised approach using much fewer labeled samples (i.e., 16x fewer samples for achieving a 0.78 F1-score, 11x fewer samples for achieving a 0.82 F1-score), even when there are previously unseen applications and application inputs in the test dataset.},
  archive      = {J_TPDS},
  author       = {Burak Aksar and Efe Sencan and Benjamin Schwaller and Omar Aaziz and Vitus J. Leung and Jim Brandt and Brian Kulis and Manuel Egele and Ayse K. Coskun},
  doi          = {10.1109/TPDS.2024.3365462},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {693-706},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Runtime performance anomaly diagnosis in production HPC systems using active learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High throughput lattice-based signatures on GPUs: Comparing
falcon and mitaka. <em>TPDS</em>, <em>35</em>(4), 675–692. (<a
href="https://doi.org/10.1109/TPDS.2024.3367319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The US National Institute of Standards and Technology initiated a standardization process for post-quantum cryptography in 2017, with the aim of selecting key encapsulation mechanisms and signature schemes that can withstand the threat from emerging quantum computers. In 2022, Falcon was selected as one of the standard signature schemes, eventually attracting effort to optimize the implementation of Falcon on various hardware architectures for practical applications. Recently, Mitaka was proposed as an alternative to Falcon, allowing parallel execution of most of its operations. These recent advancements motivate us to develop high throughput implementations of Falcon and Mitaka signature schemes on Graphics Processing Units (GPUs), a massively parallel architecture widely available on cloud service platforms. In this article, we propose the first parallel implementation of Falcon on various GPUs. We develop an iterative version of the sampling process in Falcon, which is also the most time-consuming Falcon operation. This allows us to implement Falcon signature generation without relying on expensive recursive function calls on GPUs. In addition, we propose a parallel random samples generation approach to accelerate the performance of Mitaka on GPUs. We evaluate our implementation techniques on state-of-the-art GPU architectures (RTX 3080, A100, T4 and V100). Experimental results show that our Falcon-512 implementation achieves 58,595 signatures/second and 2,721,562 verifications/second on an A100 GPU, which is $20.03\times$ and $29.51\times$ faster than the highly optimized AVX2 implementation on CPU. Our Mitaka implementation achieves 161,985 signatures/second and 1,421,046 verifications/second on the same GPU. Due to the adoption of a parallelizable sampling process, Mitaka signature generation enjoys $\approx 2$ – $20 \times$ higher throughput than Falcon on various GPUs. The high throughput signature generation and verification achieved by this work can be very useful in various emerging applications, including the Internet of Things.},
  archive      = {J_TPDS},
  author       = {Wai-Kong Lee and Raymond K. Zhao and Ron Steinfeld and Amin Sakzad and Seong Oun Hwang},
  doi          = {10.1109/TPDS.2024.3367319},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {675-692},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {High throughput lattice-based signatures on GPUs: Comparing falcon and mitaka},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Agile cache replacement in edge computing via offline-online
deep reinforcement learning. <em>TPDS</em>, <em>35</em>(4), 663–674. (<a
href="https://doi.org/10.1109/TPDS.2024.3368763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One fundamental problem of content caching in edge computing is how to replace contents in edge servers with limited capacities to meet the dynamic requirements of users without knowing their preferences in advance. Recently, online deep reinforcement learning (DRL)-based caching methods have been developed to address this problem by learning an edge cache replacement policy using samples collected from continuous interactions (trial and error) with the environment. However, in practice, the online data collection phase is often expensive and time-consuming, thus hindering the practical deployment of online DRL-based methods. To bridge this gap, we propose a novel Agile edge Cache replacement method based on Offline-online deep Reinforcement learNing (ACORN), which can efficiently learn an edge cache replacement policy offline from a training dataset collected by a behavior policy (e.g., Least Recently Used) and then improve it with fast online fine-tuning. We also design a specific convolutional neural network structure with multiple branches to effectively extract content popularity knowledge from the dataset. Experimental results show that the offline policy generated by ACORN outperforms the behavior policy by up to 38%. Through online fine-tuning, ACORN also achieves the number of cache hits as good as that of several advanced DRL-based methods while significantly reducing the number of training epochs by up to 40%.},
  archive      = {J_TPDS},
  author       = {Zhe Wang and Jia Hu and Geyong Min and Zhiwei Zhao and Zi Wang},
  doi          = {10.1109/TPDS.2024.3368763},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {663-674},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Agile cache replacement in edge computing via offline-online deep reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GeoScale: Microservice autoscaling with cost budget in
geo-distributed edge clouds. <em>TPDS</em>, <em>35</em>(4), 646–662. (<a
href="https://doi.org/10.1109/TPDS.2024.3366533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying microservice instances in geo-distributed edge clouds which are located at the network edge and in proximity to end-users can provide on-site processing, thereby improving the quality of service (QoS). To accommodate the time-varying request arrival rate of each edge cloud, the deployment scheme of microservice instances is dynamically adapted, which is called microservice autoscaling. However, existing studies on microservice autoscaling at the edge either only optimize the QoS without considering the cost of deploying microservice instances or simply focus on the cost per individual timeslot, and thus always severely violate the long-term budget constraint. To solve this problem, in this article, we propose GeoScale, a novel method that aims to optimize the average request response time under the long-term cost budget constraint. GeoScale first utilizes the Lyapunov optimization framework to decompose the long-term optimization problem into a series of per-timeslot sub-problems and then applies a signomial geometric programming (SGP)-based algorithm to obtain a near-optimal solution to each NP-hard sub-problem. Through extensive trace-driven experiments, we validate the superiority of GeoScale. The experimental results show that compared with existing strategies and designed baselines, GeoScale can improve QoS by reducing the average request response time up to 87.8% while significantly mitigating the violation of the long-term cost budget constraint.},
  archive      = {J_TPDS},
  author       = {Ke Cheng and Sheng Zhang and Meizhao Liu and Yingcheng Gu and Liu Wei and Huanyu Cheng and Kai Liu and Yu Song and Xiaohang Shi and Andong Zhu and Lei Tang},
  doi          = {10.1109/TPDS.2024.3366533},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {646-662},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {GeoScale: Microservice autoscaling with cost budget in geo-distributed edge clouds},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). End-to-end bayesian networks exact learning in shared
memory. <em>TPDS</em>, <em>35</em>(4), 634–645. (<a
href="https://doi.org/10.1109/TPDS.2024.3366471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian networks are important Machine Learning models with many practical applications in, e.g., biomedicine and bioinformatics. The problem of Bayesian networks learning is $\mathcal {NP}$ -hard and computationally challenging. In this article, we propose practical parallel exact algorithms to learn Bayesian networks from data. Our approach uses shared-memory task parallelism to realize exploration of dynamic programming lattices emerging in Bayesian networks structure learning, and introduces several optimization techniques to constraint and partition the underlying search space. Through extensive experimental testing we show that the resulting method is highly scalable, and it can be used to efficiently learn large globally optimal networks.},
  archive      = {J_TPDS},
  author       = {Subhadeep Karan and Zainul Abideen Sayed and Jaroslaw Zola},
  doi          = {10.1109/TPDS.2024.3366471},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {634-645},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {End-to-end bayesian networks exact learning in shared memory},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CloudSentry: Two-stage heavy hitter detection for
cloud-scale gateway overload protection. <em>TPDS</em>, <em>35</em>(4),
616–633. (<a href="https://doi.org/10.1109/TPDS.2023.3301852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cloud vendors provide sharing resources for millions of tenants across the world to achieve economies of scale. At the same time, the cloud network keeps the performance isolation between different tenants as if they use their private dedicated resources. However, heavy hitters caused by a single tenant at cloud gateways will break such isolation, undermining the predictable performance expected by other cloud tenants. To prevent it, heavy hitter detection becomes a key concern at the performance-critical cloud gateways but faces the dilemma between fine granularity and low overhead. In this work, we present CloudSentry , a scalable two-stage heavy hitter detection system dedicated to multi-tenant cloud gateways against such a dilemma. CloudSentry uses CPU utilization as an indicator of heavy hitters and conducts a lightweight coarse-grained detection running 24/7 to detect such CPU spikes. Then it invokes a fine-grained detection to precisely dump and analyze the potential heavy-hitter packets at the CPU spikes. After that, a more comprehensive analysis is conducted to associate heavy hitters with the cloud service scenarios and invoke a corresponding backpressure procedure. CloudSentry significantly reduces memory, computation and storage overhead compared with existing approaches. In a gateway cluster under an average traffic throughput of 251 Gbps, CloudSentry consumes only a fraction of 2%–5% CPU utilization with 8 KB run-time memory, producing only 10 MB heavy hitter logs during one month. Additionally, as it has been deployed in Alibaba Cloud for over two years, we share case studies and a lot of deployment experiences in this article.},
  archive      = {J_TPDS},
  author       = {Jianyuan Lu and Tian Pan and Shan He and Mao Miao and Guangzhe Zhou and Yining Qi and Shize Zhang and Enge Song and Xiaoqing Sun and Huaiyi Zhao and Biao Lyu and Shunmin Zhu},
  doi          = {10.1109/TPDS.2023.3301852},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {616-633},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CloudSentry: Two-stage heavy hitter detection for cloud-scale gateway overload protection},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-agent deep reinforcement learning framework for
renewable energy-aware workflow scheduling on distributed cloud data
centers. <em>TPDS</em>, <em>35</em>(4), 604–615. (<a
href="https://doi.org/10.1109/TPDS.2024.3360448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ever-increasing demand for the cloud computing paradigm has resulted in the widespread deployment of multiple datacenters, the operations of which consume very high levels of energy. The carbon footprint resulting from these operations threatens environmental sustainability while the increased energy costs have a direct impact on the profitability of cloud providers. Using renewable energy sources to satisfy the energy demands of datacenters has emerged as a viable approach to overcome the aforementioned issues. The problem of scheduling workflows across multi-cloud environments powered through a combination of brown and green energy sources includes multiple levels of complexities. First, the general case of workflow scheduling in a distributed system itself is NP-hard. The need to schedule workflows across geo-distributed cloud datacenters adds a further layer of complexity atop the general problem. The problem becomes further challenging when the datacenters are powered through renewable sources which are inherently intermittent in nature. Consequently, traditional workflow scheduling algorithms and single-agent reinforcement learning algorithms are incapable of efficiently meeting the decentralized and adaptive control required for addressing these challenges. To this end, we have leveraged the recent advancements in the paradigm of MARL (Multi-Agent Reinforcement Learning) for designing and developing a multi-agent RL framework for optimizing the green energy utilization of workflow executions across multi-cloud environments. The results of extensive simulations demonstrate that the proposed approach outperforms the comparison algorithms with respect to minimizing energy consumption of workflow executions by 47% while also keeping the makespan of workflows in par with comparison algorithms. Furthermore, with the proposed optimizations, the multi-agent technique learnt 5 times faster than a generic multi-agent algorithm.},
  archive      = {J_TPDS},
  author       = {Amanda Jayanetti and Saman Halgamuge and Rajkumar Buyya},
  doi          = {10.1109/TPDS.2024.3360448},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {604-615},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Multi-agent deep reinforcement learning framework for renewable energy-aware workflow scheduling on distributed cloud data centers},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FedHAP: Federated hashing with global prototypes for
cross-silo retrieval. <em>TPDS</em>, <em>35</em>(4), 592–603. (<a
href="https://doi.org/10.1109/TPDS.2023.3324426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep hashing has been widely applied in large-scale data retrieval due to its superior retrieval efficiency and low storage cost. However, data are often scattered in data silos with privacy concerns, so performing centralized data storage and retrieval is not always possible. Leveraging the approach of federated learning (FL) to perform deep hashing is a recent research trend. However, existing frameworks mostly rely on the aggregation of the local deep hashing models, which are trained by performing similarity learning with local skewed data only. Therefore, they cannot work well for non-IID clients in a real federated environment. To overcome these challenges, we propose a novel federated hashing framework that enables participating clients to jointly train a shared deep hashing model by leveraging the class-wise prototypical hash codes. Globally, sharing global prototypes with only one prototypical hash code per class assists in learning consistent code distributions across clients while minimizing the cost of communication and privacy. Locally, the use of the global prototypical hash codes are maximized by jointly training a discriminator network and the local hashing network. Extensive experiments on benchmark datasets are conducted to demonstrate that our method can significantly improve the performance of the deep hashing model in the federated environments with non-IID data distributions.},
  archive      = {J_TPDS},
  author       = {Meilin Yang and Jian Xu and Wenbo Ding and Yang Liu},
  doi          = {10.1109/TPDS.2023.3324426},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {592-603},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FedHAP: Federated hashing with global prototypes for cross-silo retrieval},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A memory-efficient hybrid parallel framework for deep neural
network training. <em>TPDS</em>, <em>35</em>(4), 577–591. (<a
href="https://doi.org/10.1109/TPDS.2023.3343570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing volumes of data samples and deep neural network (DNN) models, efficiently scaling the training of DNN models has become a significant challenge for server clusters with AI accelerators in terms of memory and computing efficiency. Existing parallelism schemes can be broadly classified into three categories: data parallelism (splitting data samples), model parallelism (splitting model parameters), and pipeline model parallelism (splitting model layers). Hybrid approaches split data and models, offering a comprehensive solution for parallel training. However, these methods encounter limitations in efficiently scaling larger models across more computing nodes, as they incur substantial memory constraints that affect training efficiency and overall throughput. In this paper, we propose HIPPIE , a hybrid parallel training framework designed to enhance memory efficiency and scalability of large DNN training. First, to evaluate the optimization effect more reasonably, we propose an index of Memory Efficiency (ME) to quantify the tradeoff between throughput and memory overhead. Second, driven by the informed ME optimization objective, we automatically partition the pipeline to balance the throughput and memory. Third, we optimize the model training process via a novel hybrid parallel scheduler that improves the throughput and scalability by informed pipeline scheduling and communication scheduling with gradient-hidden optimization. Experiments on various models show that HIPPIE achieves above 90% scaling efficiency on a 16-GPU platform. Moreover, HIPPIE increases throughput by up to 80%, while saving 57% of memory overhead and achieving 4.18× memory-efficiency improvement.},
  archive      = {J_TPDS},
  author       = {Dongsheng Li and Shengwei Li and Zhiquan Lai and Yongquan Fu and Xiangyu Ye and Lei Cai and Linbo Qiao},
  doi          = {10.1109/TPDS.2023.3343570},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {577-591},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A memory-efficient hybrid parallel framework for deep neural network training},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint optimization of parallelism and resource configuration
for serverless function steps. <em>TPDS</em>, <em>35</em>(4), 560–576.
(<a href="https://doi.org/10.1109/TPDS.2024.3365134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Function-as-a-Service (FaaS) offers a fine-grained resource provision model, enabling developers to build highly elastic cloud applications. User requests are handled by a series of serverless functions step by step, which forms a multi-step workflow. The developers are required to set proper configurations for functions to meet service level objectives (SLOs) and save costs. However, developing the configuration strategy is challenging. This is mainly because the execution of serverless functions often suffers from cold starts and performance fluctuation, which requires a dynamic configuration strategy to guarantee the SLOs. In this article, we present StepConf, a framework that automates the configuration as the workflow runs. StepConf optimizes memory size for each function step in the workflow and takes inter and intra-function parallelism into consideration, which has been overlooked by existing work. StepConf intelligently predicts the potential configurations for subsequent function steps, and proactively prewarms function instances in a configuration-aware manner to reduce the cold start overheads. We evaluate StepConf on AWS and Knative. Compared to existing work, StepConf improves performance by up to 5.6× under the same cost budget and achieves up to a 40% cost reduction while maintaining the same level of performance.},
  archive      = {J_TPDS},
  author       = {Zhaojie Wen and Qiong Chen and Yipei Niu and Zhen Song and Quanfeng Deng and Fangming Liu},
  doi          = {10.1109/TPDS.2024.3365134},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {560-576},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Joint optimization of parallelism and resource configuration for serverless function steps},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). X-shard: Optimistic cross-shard transaction processing for
sharding-based blockchains. <em>TPDS</em>, <em>35</em>(4), 548–559. (<a
href="https://doi.org/10.1109/TPDS.2024.3361180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in cryptocurrencies have sparked significant interest in blockchain technology. However, scalability issues remain a major challenge for wide adoption of blockchains. Sharding is a promising approach to scale blockchains, but existing sharding-based blockchains fail to achieve expected performance gains due to limitations in cross-shard transaction processing. In this paper, we propose X-shard, a blockchain system that optimizes cross-shard transaction processing, achieving high effective throughput and low processing latency. First, we allocate transactions to shards based on historical transaction patterns to minimize cross-shard transactions. Second, we take an optimistic strategy to process cross-shard transactions in parallel as sub-transactions within input shards, thereby accelerating transaction processing. Finally, we employ a cross-shard commit protocol with threshold signatures to reduce communication overhead. We implement and deploy X-shard on Amazon EC2 clusters. Experimental results validate our theoretical analysis and show that as the number of shards increases, X-shard achieves nearly linear scaling in effective throughput and decreases in transaction processing latency.},
  archive      = {J_TPDS},
  author       = {Jie Xu and Yulong Ming and Zihan Wu and Cong Wang and Xiaohua Jia},
  doi          = {10.1109/TPDS.2024.3361180},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {548-559},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {X-shard: Optimistic cross-shard transaction processing for sharding-based blockchains},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CloudSimPer: Simulating geo-distributed datacenters powered
by renewable energy mix. <em>TPDS</em>, <em>35</em>(4), 531–547. (<a
href="https://doi.org/10.1109/TPDS.2024.3357532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, studies on energy-efficient datacenters, especially the DataCenters powered by Renewable Energy mix (DCRE), have gained great attention. DCREs are large-scale, geo-distributed, and equipped with on-site renewable energy generators. For these features, it is expensive to perform empirical evaluations of proposed algorithms and solutions on the real-world DCREs, while the state-of-the-art datacenter simulators are not applicable for DCREs. In this paper, we present CloudSimPer (CLOUD SIMulator hybrid-Powered by rEnewable eneRgy), a general-purpose simulator that comprehensively supports the simulation of DCREs. Besides the functions such as renewable energy, geo-distribution, and long-term simulation, we also design evaluation metrics and an integrated simulation case for experimental studies in the future. The main challenge of CloudSimPer lies in designing a new model and software layer upon CloudSim, to solve the complexity of traceable and comparable simulations which connect renewable energies, datacenters, workloads, regions, and schedulers. We use the term schedulers broadly, encompassing any optimization approaches on DCREs for energy saving. We prove CloudSimPer and integrated case to be valid, so that simulation results are scientifically sound, by examining the expectation and the simulation results, and comparing the simulation results with selected competitors. CloudSimPer offers simulation services to datacenter designers, datacenter administrators, and academics.},
  archive      = {J_TPDS},
  author       = {Jie Song and Peimeng Zhu and Yanfeng Zhang and Ge Yu},
  doi          = {10.1109/TPDS.2024.3357532},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {531-547},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CloudSimPer: Simulating geo-distributed datacenters powered by renewable energy mix},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parallel and distributed bayesian network structure
learning. <em>TPDS</em>, <em>35</em>(4), 517–530. (<a
href="https://doi.org/10.1109/TPDS.2023.3326832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian networks (BNs) are graphical models representing uncertainty in causal discovery, and have been widely used in medical diagnosis and gene analysis due to their effectiveness and good interpretability. However, mainstream BN structure learning methods are computationally expensive, as they must perform numerous conditional independence (CI) tests to decide the existence of edges. Some researchers attempt to accelerate the learning process by parallelism, but face issues including load unbalancing, costly dominant parallelism overhead. We propose a multi-thread method, namely Fast-BNS version 1 (Fast-BNS-v1 for short), on multi-core CPUs to enhance the efficiency of the BN structure learning. Fast-BNS-v1 incorporates a series of efficiency optimizations, including a dynamic work pool for better scheduling, grouping CI tests to avoid unnecessary operations, a cache-friendly data storage to improve memory efficiency, and on-the-fly conditioning sets generation to avoid extra memory consumption. To further boost learning performance, we develop a two-level parallel method Fast-BNS-v2 by integrating edge-level parallelism with multi-processes and CI-level parallelism with multi-threads. Fast-BNS-v2 is equipped with careful optimizations including dynamic work stealing for load balancing, SIMD edge list deletion for list updating, and effective communication policies for synchronization. Comprehensive experiments show that our Fast-BNS achieves 9 to 235 times speedup over the state-of-the-art multi-threaded method on a single machine. When running on multi-machines, it further reduces the execution time of the single-machine implementation by 80%.},
  archive      = {J_TPDS},
  author       = {Jian Yang and Jiantong Jiang and Zeyi Wen and Ajmal Mian},
  doi          = {10.1109/TPDS.2023.3326832},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {4},
  number       = {4},
  pages        = {517-530},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Parallel and distributed bayesian network structure learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EvoGWP: Predicting long-term changes in cloud workloads
using deep graph-evolution learning. <em>TPDS</em>, <em>35</em>(3),
499–516. (<a href="https://doi.org/10.1109/TPDS.2024.3357715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Workload prediction plays a crucial role in resource management of large scale cloud datacenters. Although quite a number of methods/algorithms have been proposed, long-term changes have not been explicitly identified and considered. Due to shifty user demands, workload re-locations, or other reasons, the “resource usage pattern” of a workload, which is usually quite stable in a short-term view, may change dynamically in a long-term range. Such long-term dynamic changes may cause significant accuracy degradation for prediction algorithms. How to handle such long-term dynamic changes is an open and challenging issue. In this article, we propose Evolution Graph for Workload Prediction (EvoGWP), a novel method that can predict long-term dynamic changes using a delicately designed graph-based evolution learning algorithm. EvoGWP automatically extracts shapelets to explicitly identify resource usage patterns of workloads in a fine-grained level, and predicts workload changes by considering factors in both temporal and spatial dimensions. We design a two-level importance based shapelet extraction mechanism to mine new usage pattern changes in temporal dimension, and design a novel evolution graph model to fuse the interference among resource usage patterns of different workloads in spatial dimension. By combining temporal extraction of shapelets from each single workload and spatial interference of shapelets among different workloads, we then design a spatio-temporal GNN-based encoder-decoder model to predict the long-term dynamic changes of workloads. Experiments using real trace data from Alibaba, Tencent and Google show that EvoGWP improves the prediction accuracy by up to 58.6% over the state-of-the-art prediction methods. Moreover, EvoGWP can outperform the state-of-the-art prediction methods in terms of model convergence. To the best of our knowledge, this is the first work that explicitly identifies fine-grained workload resource usage patterns to accurately predict long-term dynamic changes of workloads.},
  archive      = {J_TPDS},
  author       = {Jialun Li and Jieqian Yao and Danyang Xiao and Diying Yang and Weigang Wu},
  doi          = {10.1109/TPDS.2024.3357715},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {499-516},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {EvoGWP: Predicting long-term changes in cloud workloads using deep graph-evolution learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synergistically rebalancing the EDP of container-based
parallel applications. <em>TPDS</em>, <em>35</em>(3), 484–498. (<a
href="https://doi.org/10.1109/TPDS.2024.3357353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of containers has become standard in cloud environments. However, many parallel applications in containers will not present gains proportional to the extra available hardware. This inefficient use of hardware naturally leads to energy consumption waste. With that in mind, we propose TT-Autoscaling . It works at two different levels: a) in the container, by automatically and transparently tuning the number of threads at runtime of the application, in a way to optimize the trade-off between energy and performance; b) in the cloud infrastructure, by smartly transferring the released resources to other containers that may run in parallel, making better use of the available resources. We compare TT-Autoscaling to the default execution of containers (serial execution with the maximum number of threads), showing 55.8% of performance improvements, 53.6% of energy reductions, and 79.5% of EDP improvements. We also show that TT-Autoscaling outperforms strategies that apply vertical autoscalers proposed by orchestrator tools.},
  archive      = {J_TPDS},
  author       = {Vinicius S. da Silva and Everton C. de Lima and Janaína Schwarzrock and Fábio D. Rossi and Marcelo C. Luizelli and Antonio Carlos S. Beck and Arthur F. Lorenzon},
  doi          = {10.1109/TPDS.2024.3357353},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {484-498},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Synergistically rebalancing the EDP of container-based parallel applications},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parallel fractional stochastic gradient descent with
adaptive learning for recommender systems. <em>TPDS</em>,
<em>35</em>(3), 470–483. (<a
href="https://doi.org/10.1109/TPDS.2022.3185212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The structural change toward the digital transformation of online sales elevates the importance of parallel processing techniques in recommender systems, particularly in the pandemic and post-pandemic era. Matrix factorization (MF) is a popular and scalable approach in collaborative filtering (CF) to predict user preferences in recommender systems. Researchers apply Stochastic Gradient Descent (SGD) as one of the most famous optimization techniques for MF. Paralleling SGD methods help address big data challenges due to the wide range of products and the sparsity in user ratings. However, these methods’ convergence rate and accuracy are affected by the dependency between the user and item latent factors, specifically in large-scale problems. Besides, the performance is sensitive to the applied learning rates. This article proposes a new parallel method to remove dependencies and boost speed-up by using fractional calculus to improve accuracy and convergence rate. We also apply adaptive learning rates to enhance the performance of our proposed method. The proposed method is based on Compute Unified Device Architecture (CUDA) platform. We evaluate the performance of our proposed method using real-world data and compare the results with the close baselines. The results show that our method can obtain high accuracy and convergence rate in addition to high parallelism.},
  archive      = {J_TPDS},
  author       = {Fatemeh Elahi and Mahmood Fazlali and Hadi Tabatabaee Malazi and Mehdi Elahi},
  doi          = {10.1109/TPDS.2022.3185212},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {470-483},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Parallel fractional stochastic gradient descent with adaptive learning for recommender systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collaboration in federated learning with differential
privacy: A stackelberg game analysis. <em>TPDS</em>, <em>35</em>(3),
455–469. (<a href="https://doi.org/10.1109/TPDS.2024.3354713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a privacy-preserving distributed learning paradigm, federated learning (FL) enables multiple client devices to train a shared model without uploading their local data. To further enhance the privacy protection performance of FL, differential privacy (DP) has been successfully incorporated into FL systems to defend against privacy attacks from adversaries. In FL with DP, how to stimulate efficient client collaboration is vital for the FL server due to the privacy-preserving nature of DP and the heterogeneity of various costs (e.g., computation cost) of the participating clients. However, this kind of collaboration remains largely unexplored in existing works. To fill in this gap, we propose a novel analytical framework based on Stackelberg game to model the collaboration behaviors among clients and the server with reward allocation as incentive in FL with DP. We first conduct rigorous convergence analysis of FL with DP and reveal how clients’ multidimensional attributes would affect the convergence performance of FL model. Accordingly, we solve the Stackelberg game and derive the collaboration strategies for both clients and the server. We further devise an approximately optimal algorithm for the server to efficiently conduct the joint optimization of the client set selection, the number of global iterations, and the reward payment for the clients. Numerical evaluations using real-world datasets validate our theoretical analysis and corroborate the superior performance of the proposed solution.},
  archive      = {J_TPDS},
  author       = {Guangjing Huang and Qiong Wu and Peng Sun and Qian Ma and Xu Chen},
  doi          = {10.1109/TPDS.2024.3354713},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {455-469},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Collaboration in federated learning with differential privacy: A stackelberg game analysis},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing full-spectrum matrix multiplications on ARMv8
multi-core CPUs. <em>TPDS</em>, <em>35</em>(3), 439–454. (<a
href="https://doi.org/10.1109/TPDS.2024.3350368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {General Matrix Multiplication (GEMM) is a key subroutine in high-performance computing. While the mainstream Basic Linear Algebra Subprograms (BLAS) libraries can deliver good performance on large and regular-shaped GEMMs, they are inadequate for optimizing small and irregular-shaped GEMMs, which are commonly seen in emerging HPC applications. Recent research has focused on improving GEMM performance on GPUs, but there is still significant room for improvement on emerging HPC hardware based on multi-core CPUs. We present LibShalom2 , an open-source library to optimize full-spectrum GEMMs, taking small, irregular-shaped, and large-scale regular-shaped matrices. LibShalom2 explicitly targets the ARMv8 architecture, which is becoming common in HPC systems. LibShalom2 is designed to minimize the expensive memory accessing overhead for data packing and processing small matrices. It uses analytic methods to determine GEMM kernel optimization parameters, enhancing the computation and parallelization efficiency of the GEMM kernels. We evaluate LibShalom2 by applying it to three ARMv8 multi-core architectures and comparing it against five mainstream linear algebra libraries. Experimental results show that LibShalom2 consistently outperforms existing solutions across full-spectrum GEMM workloads and hardware architectures. We also show that LibShalom2 delivers an average speedup of 2.2x for real-life neural network workloads.},
  archive      = {J_TPDS},
  author       = {Weiling Yang and Jianbin Fang and Dezun Dong and Xing Su and Zheng Wang},
  doi          = {10.1109/TPDS.2024.3350368},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {439-454},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Optimizing full-spectrum matrix multiplications on ARMv8 multi-core CPUs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TAC+: Optimizing error-bounded lossy compression for 3D AMR
simulations. <em>TPDS</em>, <em>35</em>(3), 421–438. (<a
href="https://doi.org/10.1109/TPDS.2023.3339474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today&#39;s scientific simulations require significant data volume reduction because of the enormous amounts of data produced and the limited I/O bandwidth and storage space. Error-bounded lossy compression has been considered one of the most effective solutions to the above problem. However, little work has been done to improve error-bounded lossy compression for Adaptive Mesh Refinement (AMR) simulation data. Unlike the previous work that only leverages 1D compression, in this work, we propose an approach ( TAC ) to leverage high-dimensional SZ compression for each refinement level of AMR data. To remove the data redundancy across different levels, we propose several pre-process strategies and adaptively use them based on the data features. We further optimize TAC to TAC+ by improving the lossless encoding stage of SZ compression to handle many small AMR data blocks after the pre-processing efficiently. Experiments on 10 AMR datasets from three real-world large-scale AMR simulations demonstrate that TAC+ can improve the compression ratio by up to 4.9× under the same data distortion, compared to the state-of-the-art method. In addition, we leverage the flexibility of our approach to tune the error bound for each level, which achieves much lower data distortion on two application-specific metrics.},
  archive      = {J_TPDS},
  author       = {Daoce Wang and Jesus Pulido and Pascal Grosset and Sian Jin and Jiannan Tian and Kai Zhao and James Ahrens and Dingwen Tao},
  doi          = {10.1109/TPDS.2023.3339474},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {421-438},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {TAC+: Optimizing error-bounded lossy compression for 3D AMR simulations},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estuary: A low cross-shard blockchain sharding protocol
based on state splitting. <em>TPDS</em>, <em>35</em>(3), 405–420. (<a
href="https://doi.org/10.1109/TPDS.2024.3351632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sharding is one of the most promising technologies for significantly increasing blockchain transaction throughput. However, as the number of shards increases, the ratio of cross-shard transactions in existing blockchain sharding protocols gradually approaches 100%. Since cross-shard transactions consume many times more resources than intra-shard transactions, the processing overhead of cross-shard transactions already accounts for the majority of the total overhead of the sharding system. There is a very large gap between the transaction throughput of the sharding system and its theoretical upper limit. In this article, we propose Estuary, a novel low cross-shard blockchain sharding protocol. Taking the state model as an entry point, Estuary designs a multi-level state model and state splitting and aggregation mechanism. It decouples the identity and quantity of state units, enabling transactions between users to be completed within one shard. Only when the state quantity for all shards of a user is insufficient a small number of cross-shard transactions are required. On this basis, we propose a community overlap propagation algorithm for sharding. It defines the users’ belonging coefficients of each shard and optimizes the state distribution so that the state distribution can better match the transaction characteristics between users. Finally, we develop an analysis framework for the sharding protocol and experiment with real Bitcoin transactions. The evaluation results show that compared to the state-of-the-art sharding protocol, Estuary reduces the ratio of cross-shard transactions by 88.54% and achieves more than 1.85 times the throughput improvement (92.98% of the theoretical upper limit).},
  archive      = {J_TPDS},
  author       = {Linpeng Jia and Yanxiu Liu and Keyuan Wang and Yi Sun},
  doi          = {10.1109/TPDS.2024.3351632},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {405-420},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Estuary: A low cross-shard blockchain sharding protocol based on state splitting},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time offloading for dependent and parallel tasks in
cloud-edge environments using deep reinforcement learning.
<em>TPDS</em>, <em>35</em>(3), 391–404. (<a
href="https://doi.org/10.1109/TPDS.2023.3349177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an effective technique to relieve the problem of resource constraints on mobile devices (MDs), the computation offloading utilizes powerful cloud and edge resources to process the computation-intensive tasks of mobile applications uploaded from MDs. In cloud-edge computing, the resources (e.g., cloud and edge servers) that can be accessed by mobile applications may change dynamically. Meanwhile, the parallel tasks in mobile applications may lead to the huge solution space of offloading decisions. Therefore, it is challenging to determine proper offloading plans in response to such high dynamics and complexity in cloud-edge environments. The existing studies often preset the priority of parallel tasks to simplify the solution space of offloading decisions, and thus the proper offloading plans cannot be found in many cases. To address this challenge, we propose a novel real-time and Dependency-aware task Offloading method with Deep Q-networks (DODQ) in cloud-edge computing. In DODQ, mobile applications are first modeled as Directed Acyclic Graphs (DAGs). Next, the Deep Q-Networks (DQN) is customized to train the decision-making model of task offloading, aiming to quickly complete the decision-making process and generate new offloading plans when the environments change, which considers the parallelism of tasks without presetting the task priority when scheduling tasks. Simulation results show that the DODQ can well adapt to different environments and efficiently make offloading decisions. Moreover, the DODQ outperforms the state-of-art methods and quickly reaches the optimal/near-optimal performance.},
  archive      = {J_TPDS},
  author       = {Xing Chen and Shengxi Hu and Chujia Yu and Zheyi Chen and Geyong Min},
  doi          = {10.1109/TPDS.2023.3349177},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {391-404},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Real-time offloading for dependent and parallel tasks in cloud-edge environments using deep reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EcoFed: Efficient communication for DNN partitioning-based
federated learning. <em>TPDS</em>, <em>35</em>(3), 377–390. (<a
href="https://doi.org/10.1109/TPDS.2024.3349617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficiently running federated learning (FL) on resource-constrained devices is challenging since they are required to train computationally intensive deep neural networks (DNN) independently. DNN partitioning-based FL (DPFL) has been proposed as one mechanism to accelerate training where the layers of a DNN (or computation) are offloaded from the device to the server. However, this creates significant communication overheads since the intermediate activation and gradient need to be transferred between the device and the server during training. While current research reduces the communication introduced by DNN partitioning using local loss-based methods, we demonstrate that these methods are ineffective in improving the overall efficiency (communication overhead and training speed) of a DPFL system. This is because they suffer from accuracy degradation and ignore the communication costs incurred when transferring the activation from the device to the server. This article proposes EcoFed – a communication efficient framework for DPFL systems. EcoFed eliminates the transmission of the gradient by developing pre-trained initialization of the DNN model on the device for the first time. This reduces the accuracy degradation seen in local loss-based methods. In addition, EcoFed proposes a novel replay buffer mechanism and implements a quantization-based compression technique to reduce the transmission of the activation. It is experimentally demonstrated that EcoFed can reduce the communication cost by up to 133× and accelerate training by up to 21× when compared to classic FL. Compared to vanilla DPFL, EcoFed achieves a 16× communication reduction and 2.86× training time speed-up.},
  archive      = {J_TPDS},
  author       = {Di Wu and Rehmat Ullah and Philip Rodgers and Peter Kilpatrick and Ivor Spence and Blesson Varghese},
  doi          = {10.1109/TPDS.2024.3349617},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {3},
  number       = {3},
  pages        = {377-390},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {EcoFed: Efficient communication for DNN partitioning-based federated learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Power demand reshaping using energy storage for distributed
edge clouds. <em>TPDS</em>, <em>35</em>(2), 362–376. (<a
href="https://doi.org/10.1109/TPDS.2023.3347774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The booming edge computing market that is supported by the edge cloud (EC) infrastructure has brought huge operating costs, mainly the energy cost, to edge service providers. The energy cost in form of electricity bills usually consists of energy charge and demand charge, and the demand charge based on peak power may account for a large proportion of the energy cost given a significant fluctuating power curve. In this work, we investigate the backup battery characteristics and electricity charge tariffs at ECs and explore the corresponding cost-saving potential. Specifically, we transform the backup battery group into distributed battery energy storage system (BESS) and strategically schedule the BESS to minimize the energy cost of service providers. We then propose a deep reinforcement learning (DRL) based approach to BESS charging/discharging in coping with the dynamic power demand and BESS state at each EC. To enable better decision-making and speed up agent training, we further design the customized invalid action masking (IAM) method and apply the prioritized experience replay (PER) scheme. The experiment results based on real-world EC power traces show that the proposed approach can reduce the demand charge and overall electricity bill by up to 27% and 13%, respectively.},
  archive      = {J_TPDS},
  author       = {Dongyu Zheng and Lei Liu and Guoming Tang and Yi Wang and Weichao Li},
  doi          = {10.1109/TPDS.2023.3347774},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {362-376},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Power demand reshaping using energy storage for distributed edge clouds},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Versa-DNN: A versatile architecture enabling
high-performance and energy-efficient multi-DNN acceleration.
<em>TPDS</em>, <em>35</em>(2), 349–361. (<a
href="https://doi.org/10.1109/TPDS.2023.3340953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging applications utilize numerous Deep Neural Networks (DNNs) to address multiple tasks simultaneously. As these applications continue to expand, there is a growing need for off-chip memory access optimization and innovative architectures that can adapt to diverse computation, memory, and communication requirements of various DNN models. To address these challenges, we propose Versa-DNN, a versatile DNN accelerator that can provide efficient computation, memory, and communication support for the simultaneous execution of multiple DNNs. Versa-DNN features three unique designs: a flexible off-chip memory access optimization strategy, adaptable communication fabrics, and a communication and computational aware scheduling algorithm. The proposed off-chip memory optimization strategy can improve performance and energy efficiency by increasing hardware utilization, eliminating excess data duplication, and reducing off-chip memory accesses. The adaptable communication fabrics consist of distributed buffers, processing elements, and a flexible Network-on-Chip (NoC), which can dynamically morph and fission to support distinct communication and computation needs for simultaneously running DNN models. Furthermore, the proposed scheduling policy manages the simultaneous execution of multiple DNN models with improved performance and energy efficiency. Simulation results using several DNN models, show that the proposed Versa-DNN architecture achieves 41%, 238%, 392% throughput speedup and 30%, 59%, 63% energy reduction on average for different workloads when compared to state-of-the-art accelerators such as Planaria, Herald, and AI-MT, respectively.},
  archive      = {J_TPDS},
  author       = {Jiaqi Yang and Hao Zheng and Ahmed Louri},
  doi          = {10.1109/TPDS.2023.3340953},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {349-361},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Versa-DNN: A versatile architecture enabling high-performance and energy-efficient multi-DNN acceleration},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding the impact of arbitration in MZI-based beneš
switching fabrics. <em>TPDS</em>, <em>35</em>(2), 338–348. (<a
href="https://doi.org/10.1109/TPDS.2023.3336703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Top-of-rack switches based on photonic switching fabrics (PSF) could provide higher bandwidth and energy efficiency for datacenters (DC) and high-performance computers (HPC) than these with traditional electronic crossbars. However, because of their bufferless nature, PFS are affected by contention much more drastically than traditional packet-switched electronic networks where traffic can advance towards its destination, getting buffered upon encountering contention and resuming transmission once resources are freed. In contrast, PSFs stop the injection of all traffic that generate contention. Consequently, it is important to understand how the order in which flows are serviced affects performance metrics. Our contribution is to quantify this impact through a comprehensive simulation-based evaluation focusing on a recently fabricated PSF prototype. Our experiments include configurations with three routing algorithms, two switching methods, three ToR switch sizes and 9 representative workloads from the DC and HPC domains. We found that the effect of arbitration on raw throughput is negligible but, when considering more realistic loads, selecting an appropriate arbitration policy can improve communication time and energy efficiency. Indeed, the communication time can be reduced by between 10% and 30% by employing appropriate arbitration. Switching energy efficiency can also be improved between 4% and 13%. Finally, insertion loss is barely affected, with differences below 2%. LFU and ARR were found to obtain the best results. LFU is very good with regular workloads but one of the worse with irregular workloads. ARR obtains good results regardless of the type of workload.},
  archive      = {J_TPDS},
  author       = {Javier Navaridas and Markos Kynigos and Jose A. Pascual and Mikel Luján and Jose Miguel-Alonso and John Goodacre},
  doi          = {10.1109/TPDS.2023.3336703},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {338-348},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Understanding the impact of arbitration in MZI-based beneš switching fabrics},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SunwayLB: Enabling extreme-scale lattice boltzmann method
based computing fluid dynamics simulations on advanced heterogeneous
supercomputers. <em>TPDS</em>, <em>35</em>(2), 324–337. (<a
href="https://doi.org/10.1109/TPDS.2023.3343706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Lattice Boltzmann Method (LBM) is a class of Computational Fluid Dynamics methods which models the fluid as fictive particles. In this paper, we report our work on SunwayLB, which enables LBM based solutions aiming for industrial applications using advanced heterogeneous systems such as the Sunway supercomputers. We propose several techniques to boost the simulation speed and improve the scalability of SunwayLB, including a customized multi-level domain decomposition and data sharing scheme, a carefully orchestrated strategy to fuse kernels with different performance constraints for a more balanced workload, and optimization strategies for assembly code. Based on these optimization schemes, we manage to scale SunwayLB on three advanced supercomputers: Sunway TaihuLight, the new Sunway Supercomputer and a GPU cluster. On Sunway TaihuLight, our largest simulation involves up to 5.6 trillion lattice cells, achieving 11,245 billion cell updates per second (GLUPS), 77% memory bandwidth utilization and a sustained performance of 4.7 PFlops. We further improve the memory bandwidth utilization and computational efficiency using the unique features of a new generation of Sunway supercomputer. On the new Sunway Supercomputer, the largest simulation contains over 4.2 trillion lattice cells, resulting in 6,583 GLUPS, 81% memory bandwidth utilization and a sustained performance of 2.76 PFlops. To evaluate the portability of our code, we also adapt our code to a GPU cluster with tailored optimization techniques, resulting in 191x speedup and 83.8% memory bandwidth utilization. We demonstrate a series of computational experiments for extreme-large scale fluid flow, as examples of real-world applications, to check the validity and performance of our work. The results show that our implementation is competent to be a highly scalable and efficient solution for large-scale CFD problems on heterogeneous systems.},
  archive      = {J_TPDS},
  author       = {Zhao Liu and Xuesen Chu and Xiaojing Lv and Hongsong Meng and Hanyue Liu and Guanghui Zhu and Haohuan Fu and Guangwen Yang},
  doi          = {10.1109/TPDS.2023.3343706},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {324-337},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SunwayLB: Enabling extreme-scale lattice boltzmann method based computing fluid dynamics simulations on advanced heterogeneous supercomputers},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed semi-supervised learning with consensus
consistency on edge devices. <em>TPDS</em>, <em>35</em>(2), 310–323. (<a
href="https://doi.org/10.1109/TPDS.2023.3340707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed learning has been increasingly studied in edge computing, enabling edge devices to learn a model collaboratively without exchanging their private data. However, existing approaches assume the private data owned by edge devices are all labeled while the reality is that massive private data are unlabeled and remain to be utilized, which leads to suboptimal performance. To overcome this limitation, we study a new practical problem, Distributed Semi-Supervised Learning (DSSL), to learn models collaboratively with mixed private labeled and unlabeled data on each device. We also propose a novel method DistMatch that exploits private unlabeled data by self-training on each device with the help of models from neighboring devices. DistMatch generates pseudo-labels for unlabeled data by properly averaging the predictions of these received models. Furthermore, to avoid self-training with wrong pseudo-labels, DistMatch proposes a consensus consistency loss to filter pseudo-labels with high consensus and force the output of the trained model to be consistent with these pseudo-labels. Extensive evaluation results via our self-developed testbed indicate the proposed method outperforms all baselines on commonly used image classification benchmark datasets.},
  archive      = {J_TPDS},
  author       = {Hao-Rui Chen and Lei Yang and Xinglin Zhang and Jiaxing Shen and Jiannong Cao},
  doi          = {10.1109/TPDS.2023.3340707},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {310-323},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Distributed semi-supervised learning with consensus consistency on edge devices},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TCSA: Efficient localization of busy-wait synchronization
bugs for latency-critical applications. <em>TPDS</em>, <em>35</em>(2),
297–309. (<a href="https://doi.org/10.1109/TPDS.2023.3342573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Busy-wait synchronization is often used for latency-critical applications to ensure low latency. Unfortunately, its performance bugs due to thread contention may lead to request failures or even system crashes. Localizing the performance bugs of busy-wait synchronization is not trivial because we have to pinpoint the exact moment of occurrence from a relatively long measurement period and simultaneously identify candidate busy-wait threads from numerous concurrent threads. Existing methods often rely on hotspot-driven analysis of lock-related functions, but they still need extensive manual work to localize busy-wait threads. This paper proposes timing call stack analysis (TCSA), an efficient approach to localizing busy-wait synchronization bugs. The key idea is to time-serialize the function call stacks of applications and identify consecutive identical call stacks to catch busy-wait threads. TCSA can handle any application regardless of its programming language and identify various busy-wait patterns, including spinlocks, chaining spinlocks, futexes, and safepoint checks within the Java Virtual Machine. Compared to the state-of-the-art, TCSA can effectively diminish the quantity of examined records (e.g., threads and functions) by 1 to 3 orders of magnitude. TCSA has been deployed to a large cloud service provider, demonstrating its effectiveness, efficiency, and practicality in four real latency-critical applications.},
  archive      = {J_TPDS},
  author       = {Ning Li and Jianmei Guo and Bo Huang and Yuyang Li and Yilei Zhang and Chengdong Li and Wenxin Huang},
  doi          = {10.1109/TPDS.2023.3342573},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {297-309},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {TCSA: Efficient localization of busy-wait synchronization bugs for latency-critical applications},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graft: Efficient inference serving for hybrid deep learning
with SLO guarantees via DNN re-alignment. <em>TPDS</em>, <em>35</em>(2),
280–296. (<a href="https://doi.org/10.1109/TPDS.2023.3340518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have been widely adopted for various mobile inference tasks, yet their ever-increasing computational demands are hindering their deployment on resource-constrained mobile devices. Hybrid deep learning partitions a DNN into two parts and deploys them across the mobile device and a server, aiming to reduce inference latency or prolong battery life of mobile devices. However, such partitioning produces (non-uniform) DNN fragments which are hard to serve efficiently on the server. This article presents Graft—an efficient inference serving system for hybrid deep learning with latency service-level objective (SLO) guarantees. Our main insight is to mitigate the non-uniformity by a core concept called DNN re-alignment, allowing multiple heterogeneous DNN fragments to be restructured to share layers. To fully exploit the potential of DNN re-alignment, Graft employs fine-grained GPU resource sharing. Based on that, we propose efficient algorithms for merging, grouping, and re-aligning DNN fragments to maximize request batching opportunities, minimizing resource consumption while guaranteeing the inference latency SLO. We implement a Graft prototype and perform extensive experiments with five types of widely used DNNs and real-world network traces. Our results show that Graft improves resource efficiency by up to 70% compared with the state-of-the-art inference serving systems.},
  archive      = {J_TPDS},
  author       = {Jing Wu and Lin Wang and Qirui Jin and Fangming Liu},
  doi          = {10.1109/TPDS.2023.3340518},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {280-296},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Graft: Efficient inference serving for hybrid deep learning with SLO guarantees via DNN re-alignment},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Faster and scalable MPI applications launching.
<em>TPDS</em>, <em>35</em>(2), 264–279. (<a
href="https://doi.org/10.1109/TPDS.2022.3218077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed parallel MPI applications are the dominant workload in many high-performance computing systems. While optimizing MPI application execution is a well-studied field, little work has considered optimizing the initial MPI application launching phase, which incurs extensive cross-machine communications and synchronization. The overhead of MPI application launching can be expensive, accounting for more than million core hours per 10K nodes annually on the production Tianhe-2A supercomputer, which will increase as the number of parallel machines used grows. Therefore, it is critical to optimize the MPI application launching process. This paper presents a novel approach to optimizing the MPI application launch. Our approach adopts a location-aware address generation rule to eliminate the need for address exchange and a topology-aware global communication scheme to optimize cross-machine synchronization. We then design a new application launch procedure to support the proposed optimizations to further reduce the pressure of the shared I/O system. Our techniques have been deployed to production in the Tianhe-2A supercomputer and the Next Generation Tianhe Supercomputer. Experimental results show that our approach scales well and outperforms alternative schemes, reducing the MPI application launching time by over 29% with 320K MPI processes.},
  archive      = {J_TPDS},
  author       = {Yong Dong and Yiqin Dai and Min Xie and Kai Lu and Ruibo Wang and Juan Chen and Mingtian Shao and Zheng Wang},
  doi          = {10.1109/TPDS.2022.3218077},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {264-279},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Faster and scalable MPI applications launching},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient bottleneck planes exclusion method for
reconfiguring 3D VLSI arrays. <em>TPDS</em>, <em>35</em>(2), 250–263.
(<a href="https://doi.org/10.1109/TPDS.2023.3339961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the ever-increasing integration and parallel computing capabilities of 3D processor arrays, the occurrence of processor elements (PEs) failures caused by various factors has become more prevalent. Therefore, the implementation of a fault-tolerant mechanism that uses the remaining fault-free PEs to reconfigure sub-array becomes critical. In this paper, we study the problem of reconfiguring a 3D subarray with as many fault-free PEs as possible, which has been shown to be NP-complete in previous work. Although prior algorithms have been effective under low fault densities, they are severely limited when faced with high fault densities. To address this, we first define the bottleneck of the 3D processor array, proposed a novel method to identify the physical bottleneck plane that restricts the reconfigurable size of the logical sub-array and prove its correctness. Then, we propose an effective compensation strategy that can fully utilize the fault-free PEs in the bottleneck plane. Under this strategy, a sliding-window weight calculation method is proposed to determine the priority of compensation. Finally, we proposed a heuristic algorithm, which can construct the maximum target array from different dimensions in polynomial time. Experimental results demonstrate that the proposed algorithm exhibits favorable performance in terms of harvest and degradation. For the random-failure model, the improvement in the harvest for fault-free PEs is up to 32.03% on a $32 \times 32 \times 32$ host array with a 20% fault density. And for the clustered fault model, the improvement in harvest is up to 70.63% on a $32 \times 32 \times 32$ host array distributed with 12 cluster failures of size $6 \times 6 \times 6$ .},
  archive      = {J_TPDS},
  author       = {Junyan Qian and Kunzhu Qiu and Hao Ding and Huimin Zhang and Zhongyi Zhai},
  doi          = {10.1109/TPDS.2023.3339961},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {250-263},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An efficient bottleneck planes exclusion method for reconfiguring 3D VLSI arrays},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A high-performance genomic accelerator for accurate
sequence-to-graph alignment using dynamic programming algorithm.
<em>TPDS</em>, <em>35</em>(2), 237–249. (<a
href="https://doi.org/10.1109/TPDS.2023.3325137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid mutation of viruses, such as SARS-CoV-2, highlights the urgent need for fast and precise genomic sequencing. The traditional sequencing technique maps the DNA fragments collected from an individual to a known linear reference genome sequence. The linear reference cannot express the genetic diversity of the population, which leads to mapping bias. Therefore, researchers proposed to use a graph reference together with long reads for sequence mapping so that the mapping bias can be avoided to the greatest extent. However, the graph reference introduces irregular edges making memory access of alignment a bottleneck and meanwhile the long read quadratically increases the storage pressure in the alignment process. Therefore, there is a pressing need for a high-performance hardware accelerator for accurate sequence-to-graph alignment. To our best knowledge, this paper presents ASGDP, the first hardware accelerator designed for aligning sequences of arbitrary length reads to a graph. It is based on the traditional dynamic programming algorithm and supports flexible penalty scoring strategies. ASGDP has proposed an efficient memory access pattern in hardware and a hierarchical prediction pruning strategy in algorithm. This combined software-hardware strategy effectively alleviates the storage bottleneck of multi-edge access and improves the accuracy of pruning strategies. We demonstrate that ASGDP provides significant improvements for long reads of the sequence-to-graph alignment. For a typical 10 K long read, a single ASGDP accelerator outperforms state-of-the-art S2G mapping tools by 70.8×, 168.1×.},
  archive      = {J_TPDS},
  author       = {Gang Zeng and Jianfeng Zhu and Yichi Zhang and Ganhui Chen and Zhenhai Yuan and Shaojun Wei and Leibo Liu},
  doi          = {10.1109/TPDS.2023.3325137},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {237-249},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A high-performance genomic accelerator for accurate sequence-to-graph alignment using dynamic programming algorithm},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FAST: Enhancing federated learning through adaptive data
sampling and local training. <em>TPDS</em>, <em>35</em>(2), 221–236. (<a
href="https://doi.org/10.1109/TPDS.2023.3334398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emerging paradigm of federated learning (FL) strives to enable devices to cooperatively train models without exposing their raw data. In most cases, the data across devices are non-independently and identically distributed in FL. Thus, the local models trained over different data distributions will inevitably deviate from the global optima, which induces optimization inconsistency and even hurts global convergence. Moreover, the resource-constrained devices with heterogeneous training capacities (e.g., computing and communication) further slow down the convergence rate. To this end, we introduce an F L framework with a daptive data s ampling and local t raining, namely FAST. Specifically, even without devices’ private data distributions, FAST enables each device to sample different rates of data points from each of its local classes to rebuild a dataset for training, thus adjusting the convergence direction of the aggregated global model to be closer to the global optima. The theoretical analysis shows that the convergence bound depends on the sampling rates as well as the number of local iterations executed on the sampled data. To achieve resource-effective and convergence-guaranteed FL, we then design an online learning algorithm that jointly optimizes the data sampling and local training strategies so as to encourage the decrease of global loss under the given time budget. Extensive experiments on physical and simulated environments show that, FAST improves the model accuracy by about 1.55%-6.78% given the same time budget, and accelerates training by about 1.39-5.89× with the same target accuracy, compared with the baselines.},
  archive      = {J_TPDS},
  author       = {Zhiyuan Wang and Hongli Xu and Yang Xu and Zhida Jiang and Jianchun Liu and Suo Chen},
  doi          = {10.1109/TPDS.2023.3334398},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {221-236},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FAST: Enhancing federated learning through adaptive data sampling and local training},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simple, fast and widely applicable concurrent memory
reclamation via neutralization. <em>TPDS</em>, <em>35</em>(2), 203–220.
(<a href="https://doi.org/10.1109/TPDS.2023.3335671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reclaiming memory in non-blocking dynamic data structures in unmanaged languages like C/C++ presents a unique challenge due to the risk of use-after-free errors caused by concurrent accesses. Existing safe memory reclamation (SMR) algorithms fall short of satisfying five key properties: high performance, bounded garbage, usability, consistency, and applicability. In particular, bounded garbage and high performance are quite difficult to achieve simultaneously. In this paper, we address this limitation by proposing a new, provably correct technique called neutralization based reclamation (NBR) that neutralizes threads using POSIX signals to provide the synchronization required for safe memory reclamation. NBR uses atomic reads and writes and achieves bounded garbage and high performance without imposing significant overhead on concurrent readers and writers. An extensive experimental evaluation serves to demonstrate the efficiency of our technique across various data structures, reclamation algorithms, and workloads. A detailed survey of popular concurrent data structures suggests NBR is applicable to a wide range of data structures, many of which could not be used with prior SMR algorithms that guarantee bounded garbage.},
  archive      = {J_TPDS},
  author       = {Ajay Singh and Trevor Alexander Brown and Ali José Mashtizadeh},
  doi          = {10.1109/TPDS.2023.3335671},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {2},
  number       = {2},
  pages        = {203-220},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Simple, fast and widely applicable concurrent memory reclamation via neutralization},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PaVM: A parallel virtual machine for smart contract
execution and validation. <em>TPDS</em>, <em>35</em>(1), 186–202. (<a
href="https://doi.org/10.1109/TPDS.2023.3334208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance bottleneck of blockchain has shifted from consensus to serial smart contract execution in transaction validation. Previous works predominantly focus on inter-contract parallel execution, but they fail to address the inherent limitations of each smart contract execution performance. In this paper, we propose PaVM, the first smart contract virtual machine that supports both inter-contract and intra-contract parallel execution to accelerate the validation process. PaVM consists of (1) key instructions for precisely recording entire runtime information at the instruction level, (2) a runtime system with a re-designed machine state and thread management to facilitate parallel execution, and (3) a read/write-operation-based receipt generation method to ensure both the correctness of operations and the consistency of blockchain data. We evaluate PaVM on the Ethereum testnet, demonstrating that it can outperform the mainstream blockchain client Geth. Our evaluation results reveal that PaVM speeds up overall validation performance by 33.4×, and enhances validation throughput by up to 46×.},
  archive      = {J_TPDS},
  author       = {Yaozheng Fang and Zhiyuan Zhou and Surong Dai and Jinni Yang and Hui Zhang and Ye Lu},
  doi          = {10.1109/TPDS.2023.3334208},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {1},
  number       = {1},
  pages        = {186-202},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {PaVM: A parallel virtual machine for smart contract execution and validation},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Batch jobs load balancing scheduling in cloud computing
using distributional reinforcement learning. <em>TPDS</em>,
<em>35</em>(1), 169–185. (<a
href="https://doi.org/10.1109/TPDS.2023.3334519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud computing, how to reasonably allocate computing resources for batch jobs to ensure the load balance of dynamic clusters and meet user requests is an important and challenging task. Most existing studies are based on deep Q network, which utilizes neural networks to estimate the expected value of cumulative return in the scheduling process. The value-based DQN algorithms ignore the complete information contained in the value distribution and lack strong adaptability to time-varying batch jobs and dynamic cluster resources. Therefore, to capture the inherent stochasticity of the scheduling process caused by environmental stochasticity, we utilize Distributional Reinforcement Learning to model the value distribution of the cumulative return. Specifically, we formalize the load balancing scheduling as a multi-objective optimization problem and construct a Distributional Reinforcement Learning model. Then we introduce quantile regression to learn the value distribution of the cumulative return during scheduling and propose a dynamic load balancing scheduling algorithm based on Distributional Reinforcement Learning. In addition, we develop a cluster environment for real-time processing of batch jobs to simulate the arrival of batch jobs and train the Distributional Reinforcement Learning-based scheduling agent. We conduct empirical experiments and detailed analysis by using the real Alibaba Cluster cluster traces v2018 and v2020. The results show that compared to the baseline algorithms, the proposed algorithm performs better in terms of cluster load balancing, success rate of instance creation and average completion time of the tasks. The experimental results on different trace datasets also indicate that the propsoed algorithm exhibits excellent scalability.},
  archive      = {J_TPDS},
  author       = {Tiangang Li and Shi Ying and Yishi Zhao and Jianga Shang},
  doi          = {10.1109/TPDS.2023.3334519},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {1},
  number       = {1},
  pages        = {169-185},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Batch jobs load balancing scheduling in cloud computing using distributional reinforcement learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enabling efficient erasure coding in disaggregated memory
systems. <em>TPDS</em>, <em>35</em>(1), 154–168. (<a
href="https://doi.org/10.1109/TPDS.2023.3332782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disaggregated memory (DM) separates compute and memory resources to build a huge memory pool. Erasure coding (EC) is expected to provide fault tolerance in DM with low memory cost. In DM with EC, objects are first coded in compute servers, then directly written to memory servers via high-speed networks like one-sided RDMA. However, as the one-sided RDMA latency goes down to the microsecond level, coding overhead degrades the performance in DM with EC. To enable efficient EC in DM, we thoroughly analyze the coding stack from the perspective of cache efficiency and RDMA transmission. We develop MicroEC, which optimizes the coding workflow by reusing the auxiliary coding data and coordinates the coding and RDMA transmission with an exponential pipeline, as well as carefully adjusting the coding and transmission threads to minimize the latency. We implement a prototype supporting common basic operations, such as write/read/degraded read/recovery. Experiments show that MicroEC reduces the write latency by up to 44.35% and 42.14% and achieves up to $1.80\times$ and $1.73\times$ write throughput, compared with the state-of-the-art DM systems with EC and 3-way replication for objects not smaller than 1 MB, respectively. For small objects, MicroEC also evidently reduces the variation of latency, e.g., it reduces the P99 latency of writing 1 KB objects by 27.81%.},
  archive      = {J_TPDS},
  author       = {Qiliang Li and Liangliang Xu and Yongkun Li and Min Lyu and Wei Wang and Pengfei Zuo and Yinlong Xu},
  doi          = {10.1109/TPDS.2023.3332782},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {1},
  number       = {1},
  pages        = {154-168},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Enabling efficient erasure coding in disaggregated memory systems},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible and efficient memory swapping across mobile devices
with LegoSwap. <em>TPDS</em>, <em>35</em>(1), 140–153. (<a
href="https://doi.org/10.1109/TPDS.2023.3331703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents LegoSwap, a cross-device memory swapping mechanism for mobile devices. It exploits the unbalanced utilization of memory resources across devices. With LegoSwap, remote memory is utilized in a seamless plug-and-play manner. It achieves comparable-to-local swapping performance based on existing network infrastructure. In addition, LegoSwap frees from the effect of remote I/O disconnection and minimizes the effect on remote devices. This is realized by three novel approaches: resource-dedicated swapping for fast swapping among devices, app-aware swapping for network connectivity considerations, and elastic swap area management for inter-device interference relieving. LegoSwap is implemented on real-life mobile devices. Experimental results show that LegoSwap can enhance app caching capability by 2x compared with no swapping, and improve performance by 2.3x compared with state-of-the-art remote swapping. More importantly, local swapping induced read-write conflicts are largely removed.},
  archive      = {J_TPDS},
  author       = {Changlong Li and Yu Liang and Liang Shi and Chao Wang and Chun Jason Xue and Xuehai Zhou},
  doi          = {10.1109/TPDS.2023.3331703},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {1},
  number       = {1},
  pages        = {140-153},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Flexible and efficient memory swapping across mobile devices with LegoSwap},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). US-byte: An efficient communication framework for scheduling
unequal-sized tensor blocks in distributed deep learning. <em>TPDS</em>,
<em>35</em>(1), 123–139. (<a
href="https://doi.org/10.1109/TPDS.2023.3331372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The communication bottleneck severely constrains the scalability of distributed deep learning, and efficient communication scheduling accelerates distributed DNN training by overlapping computation and communication tasks. However, existing approaches based on tensor partitioning are not efficient and suffer from two challenges: 1) the fixed number of tensor blocks transferred in parallel can not necessarily minimize the communication overheads; 2) although the scheduling order that preferentially transmits tensor blocks close to the input layer can start forward propagation in the next iteration earlier, the shortest per-iteration time is not obtained. In this paper, we propose an efficient communication framework called US-Byte. It can schedule unequal-sized tensor blocks in a near-optimal order to minimize the training time. We build the mathematical model of US-Byte by two phases: 1) the overlap of gradient communication and backward propagation, and 2) the overlap of gradient communication and forward propagation. We theoretically derive the optimal solution for the second phase and efficiently solve the first phase with a low-complexity algorithm. We implement the US-Byte architecture on PyTorch framework. Extensive experiments on two different 8-node GPU clusters demonstrate that US-Byte can achieve up to 1.26x and 1.56x speedup compared to ByteScheduler and WFBP, respectively. We further exploit simulations of 128 GPUs to verify the potential scaling performance of US-Byte. Simulation results show that US-Byte can achieve up to 1.69x speedup compared to the state-of-the-art communication framework.},
  archive      = {J_TPDS},
  author       = {Yunqi Gao and Bing Hu and Mahdi Boloursaz Mashhadi and A-Long Jin and Pei Xiao and Chunming Wu},
  doi          = {10.1109/TPDS.2023.3331372},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {1},
  number       = {1},
  pages        = {123-139},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {US-byte: An efficient communication framework for scheduling unequal-sized tensor blocks in distributed deep learning},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enabling streaming analytics in satellite edge computing via
timely evaluation of big data queries. <em>TPDS</em>, <em>35</em>(1),
105–122. (<a href="https://doi.org/10.1109/TPDS.2023.3332333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet-of-Things (IoT) applications from many industries, such as transportation (maritime, road, rail, air) and fleet management, offshore monitoring, and farming are located in remote areas without cellular connectivity. Such IoT applications continuously generate stream data with hidden values that need to unveiled in real time. Streaming analytics is emerging as a popular type of Big Data analytics to process large volume of stream data for IoT applications in remote regions. Built upon terrestrial-satellite integrated networks, Satellite Edge Computing (SEC) equipped with computing resource in satellites has been envisioning as a key enabling technology to timely analyze stream data of IoT applications in remote regions on the Earth. Considering the dynamically-moving property of satellites in an SEC network, it is vital to optimize the responsiveness of each Big Data analytical query, such that none of such queries takes much longer time to wait for available satellites with sufficient computing resource. Furthermore, the uncertain data volumes of Big Data queries in SEC networks make the resources in satellites fragmented, particularly when the collaboration among satellites is intermittent. Therefore, directly application of existing methods may not guarantee the timelineness of streaming analytics in an SEC network. To address the afore-mentioned unique challenges of streaming analytics in SEC networks, it is urgent to design new algorithms and methods for timely Big Data processing. Specifically, we use the flow time to capture the responsiveness of streaming analytics in satellite edge computing, which is the time between the generation of the first unit of a dataset and the finish time of the data processing. We consider the flow time minimization problem for query evaluation of Big Data analytics in an SEC network with the aim of minimizing the average flow time of Big Data analytical queries, under an assumption of uncertain volumes of datasets. To this end, we first propose an approximation algorithm with a provable approximation ratio for the offline version of the flow time minimization problem. We then devise an online learning algorithm, referred to the customized Lipschitz bandit learning algorithm, with a bounded regret for the online version of the problem. We finally evaluate the performance of the proposed algorithms in a real SEC network topology. Experiment results show that the performance of the proposed algorithm outperforms its counterparts by at least 13% in terms of flow time.},
  archive      = {J_TPDS},
  author       = {Zichuan Xu and Guangyuan Xu and Hao Wang and Weifa Liang and Qiufen Xia and Shangguang Wang},
  doi          = {10.1109/TPDS.2023.3332333},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {1},
  number       = {1},
  pages        = {105-122},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Enabling streaming analytics in satellite edge computing via timely evaluation of big data queries},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hopscotch: A hardware-software co-design for efficient cache
resizing on multi-core SoCs. <em>TPDS</em>, <em>35</em>(1), 89–104. (<a
href="https://doi.org/10.1109/TPDS.2023.3332711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Following the trend of increasing autonomy in real-time systems, multi-core System-on-Chips (SoCs) have enabled devices to better handle the large streams of data and intensive computation required by such autonomous systems. In modern multi-core SoCs, each L1 cache is designed to be tied to an individual processor, and a processor can only access its own L1 cache. This design method ensures the system&#39;s average throughput, but also limits the possibility of parallelism, significantly reducing the system&#39;s real-time schedulability. To overcome this problem, we present a new system framework for highly-parallel multi-core systems, Hopscotch . Hopscotch introduces re-sizable L1 cache which is shared between processors in the same computing cluster. At execution, Hopscotch dynamically allocates L1 cache capacity to the tasks executed by the processors, unblocking the available parallelism in the system. Based on the new hardware architecture, we also present a new theoretical model and schedulability analysis providing cache size selection methods and corresponding timing guarantees for the system. As demonstrated in the evaluations, Hopscotch effectively improves system-level schedulability with negligible extra overhead.},
  archive      = {J_TPDS},
  author       = {Zhe Jiang and Kecheng Yang and Nathan Fisher and Nan Guan and Neil C. Audsley and Zheng Dong},
  doi          = {10.1109/TPDS.2023.3332711},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {1},
  number       = {1},
  pages        = {89-104},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Hopscotch: A hardware-software co-design for efficient cache resizing on multi-core SoCs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SpatialSSJP: QoS-aware adaptive approximate stream-static
spatial join processor. <em>TPDS</em>, <em>35</em>(1), 73–88. (<a
href="https://doi.org/10.1109/TPDS.2023.3330669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread adoption of Internet of Things (IoT) motivated the emergence of mixed workloads in smart cities, where fast arriving geo-referenced big data streams are joined with archive tables, aiming at enriching streams with descriptive attributes that enable insightful analytics. Applications are now relying on finding, in real-time, to which geographical regions data streaming tuples belong. This problem requires a computationally intensive stream-static join for joining a dynamic stream with a disk-resident static table. In addition, the time-varying nature of fluctuation in geospatial data arriving online calls for an approximate solution that can trade-off QoS constraints while ensuring that the system survives sudden spikes in data loads. In this paper, we present SpatialSSJP, an adaptive spatial-aware approximate query processing system that specifically focuses on stream-static joins in a way that guarantees achieving an agreed set of Quality-of-Service goals and maintains geo-statistics of stateful online aggregations over stream-static join results. SpatialSSJP employs a state-of-art stratified-like sampling design to select well-balanced representative geospatial data stream samples and serve them to a stream-static geospatial join operator downstream. We implemented a prototype atop Spark Structured Streaming. Our extensive evaluations on big real datasets show that our system can survive and mitigate harsh join workloads and outperform state-of-art baselines by significant magnitudes, without risking rigorous error bounds in terms of the accuracy of the output results. SpatialSSJP achieves a relative accuracy gain against plain Spark joins of approximately 10% in worst cases but reaching up to 50% in best case scenarios.},
  archive      = {J_TPDS},
  author       = {Isam Mashhour Al Jawarneh and Paolo Bellavista and Antonio Corradi and Luca Foschini and Rebecca Montanari},
  doi          = {10.1109/TPDS.2023.3330669},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {1},
  number       = {1},
  pages        = {73-88},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SpatialSSJP: QoS-aware adaptive approximate stream-static spatial join processor},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Demystifying the cost of serverless computing: Towards a
win-win deal. <em>TPDS</em>, <em>35</em>(1), 59–72. (<a
href="https://doi.org/10.1109/TPDS.2023.3330849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serverless is an emerging computing paradigm that greatly simplifies the development, deployment, and maintenance of cloud applications. However, due to potential cost issues brought by the widely adopted pricing, it is difficult to answer how to use and operate serverless computing services from the perspectives of users and providers. To demystify the cost of serverless computing, we present one of the first studies that develops an analytical model for serverless cost from the perspectives of users and providers, by comparing it to Infrastructure-as-a-Service. Based on the model, driven by real-world traces, extensive simulation results verify the following cost issues: 1) For the users, serverless is not always cost-saving, even possibly leading to expense explosion; 2) The serverless providers are in urgent need of widening use scenarios to improve resource utilization and raise revenue; 3) The prevailing pricing fails to neither reduce the risk of expense explosion nor meet the need of attracting more workloads. To remove the cost barrier, we propose future function , auction-based pricing for serverless, to offer discounts to the users as well as boost profit for the providers. Experimental results show the duration price of functions can be reduced by 57.5% on average for 13.5% of users yet without harming the revenue of providers.},
  archive      = {J_TPDS},
  author       = {Fangming Liu and Yipei Niu},
  doi          = {10.1109/TPDS.2023.3330849},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {1},
  number       = {1},
  pages        = {59-72},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Demystifying the cost of serverless computing: Towards a win-win deal},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A high-performance and energy-efficient photonic
architecture for multi-DNN acceleration. <em>TPDS</em>, <em>35</em>(1),
46–58. (<a href="https://doi.org/10.1109/TPDS.2023.3327535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale deep neural network (DNN) accelerators are poised to facilitate the concurrent processing of diverse DNNs, imposing demanding challenges on the interconnection fabric. These challenges encompass overcoming performance degradation and energy increase associated with system scaling while also necessitating flexibility to support dynamic partitioning and adaptable organization of compute resources. Nevertheless, conventional metallic-based interconnects frequently confront inherent limitations in scalability and flexibility. In this paper, we leverage silicon photonic interconnects and adopt an algorithm-architecture co-design approach to develop MDA, a DNN accelerator meticulously crafted to empower high-performance and energy-efficient concurrent processing of diverse DNNs. Specifically, MDA consists of three novel components: 1) a resource allocation algorithm that assigns compute resources to concurrent DNNs based on their computational demands and priorities; 2) a dataflow selection algorithm that determines off-chip and on-chip dataflows for each DNN, with the objectives of minimizing off-chip and on-chip memory accesses, respectively; 3) a flexible silicon photonic network that can be dynamically segmented into sub-networks, each interconnecting the assigned compute resources of a certain DNN while adapting to the communication patterns dictated by the selected on-chip dataflow. Simulation results show that the proposed MDA accelerator outperforms other state-of-the-art multi-DNN accelerators, including PREMA, AI-MT, Planaria, and HDA. MDA accelerator achieves a speedup of 3.6, accompanied by substantial improvements of 7.3×, 12.7×, and 9.2× in energy efficiency, service-level agreement (SLA) satisfaction rate, and fairness, respectively.},
  archive      = {J_TPDS},
  author       = {Yuan Li and Ahmed Louri and Avinash Karanth},
  doi          = {10.1109/TPDS.2023.3327535},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {1},
  number       = {1},
  pages        = {46-58},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A high-performance and energy-efficient photonic architecture for multi-DNN acceleration},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CLIC: An extensible and efficient cross-platform data
analytics system. <em>TPDS</em>, <em>35</em>(1), 34–45. (<a
href="https://doi.org/10.1109/TPDS.2023.3298038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the ever-increasing data volume and application diversity, a modern data analytics job is generally built as a workflow consisting of multiple tasks. For either specific functionalities or higher performance, tasks in a workflow may need to be deployed on different data processing platforms. This article proposes CLIC, a highly extensible system for efficient cross-platform data analytics. To leverage the advantage of diverse platforms while alleviating development efforts, we propose an embedding-based operator encoding scheme and a Graph Convolutional Network model for efficient platform selection. Aiming at flexibly integrating new operators and platforms, CLIC is designed with a highly extensible system architecture that decouples the core functionalities from backend platforms. Experiments show that CLIC can significantly improve the performance of modern data analysis workflows with fast platform selection.},
  archive      = {J_TPDS},
  author       = {Qixiang Chen and Zhijun Chen and Kai Zhang and X. Sean Wang},
  doi          = {10.1109/TPDS.2023.3298038},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {1},
  number       = {1},
  pages        = {34-45},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CLIC: An extensible and efficient cross-platform data analytics system},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive auto-tuning framework for global exploration of
stencil optimization on GPUs. <em>TPDS</em>, <em>35</em>(1), 20–33. (<a
href="https://doi.org/10.1109/TPDS.2023.3325630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stencil computations are widely used in high performance computing (HPC) applications. Many HPC platforms utilize the high computation capability of GPUs to accelerate stencil computations. In recent years, stencils have become more diverse in terms of stencil order, memory accesses and computation patterns. To adapt diverse stencils to GPUs, a variety of optimization techniques have been proposed. Due to the diversity of stencil patterns and GPU architectures, no single optimization technique fits all stencils. Therefore, stencil auto-tuning mechanisms have been proposed to conduct parameter search for a given combination of optimization techniques. However, parameter search for an inappropriate optimization combination (OC) misses the globally optimal solution. To address the above problems, we propose GSTuner , an adaptive auto-tuning framework that efficiently determines the optimal parameter setting of the global optimization space for stencils on GPUs. Specifically, GSTuner represents stencil patterns as neighboring features and unifies feature vectors of OCs through data pre-processing. In addition, GSTuner samples parameter settings from superior OCs via the quota-based reward policy and regression mechanisms. After that, GSTuner employs the genetic algorithm that considers sub-population similarity to reduce the cost of evolutionary search. The experiment results show that GSTuner can identify better performing settings with higher auto-tuning speed compared to the state-of-the-art works.},
  archive      = {J_TPDS},
  author       = {Qingxiao Sun and Yi Liu and Hailong Yang and Zhonghui Jiang and Zhongzhi Luan and Depei Qian},
  doi          = {10.1109/TPDS.2023.3325630},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {1},
  number       = {1},
  pages        = {20-33},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Adaptive auto-tuning framework for global exploration of stencil optimization on GPUs},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online learning algorithms for context-aware video caching
in D2D edge networks. <em>TPDS</em>, <em>35</em>(1), 1–19. (<a
href="https://doi.org/10.1109/TPDS.2023.3326187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of various short video platforms such as TikTok and Instagram, coupled with the accelerated pace of people&#39;s lives, people are spending more time sharing and watching online videos than ever before, and they gradually turn their attention to short videos with short duration and novel content. Browsing and watching short videos by users with their energy-capacitated devices, such as phones and tablets, have become one of the main ways for users to entertain online. Timely response and high quality online video delivery are of the utmost importance to guarantee the quality of service (QoS) experienced by users. Caching videos at locations close to the video demanders can significantly improve the QoS by reducing the access delay of high quality videos. Together with an explosive growth of mobile devices and great demand for bandwidth, Device-to-Device (D2D) network is emerging as a promising technology to enable ultra-low latency communications, by allowing mobile devices to communicate with each other with or without the involvement of network infrastructures. Caching videos in D2D networks can further reduce video response delays of high quality videos, thereby improving the QoS experienced by users when watching short videos. However, how to cache the most appropriate videos at strategic mobile devices is crucial to satisfy the QoS requirements of users. Specifically, the QoS experienced by users depends on many intertwining factors from both users and the D2D network, such as videos characteristics, various users’ demands for different videos, different communities of users, and energy levels of devices. Motivated by these facts, we investigate the video caching problems in a D2D network. The novelty of our study is to jointly consider the intertwining factors from both users and the D2D network when users access short videos. Specifically, we first formulate an optimization problem of video caching with the objective to minimize the average delay experienced by mobile devices, subject to the cache storage capacity and energy budget of each mobile device. We then propose an approximation algorithm with an approximation ratio for the offline video caching problem. We further devise an online learning algorithm for the online context-aware video caching problem. We finally conduct extensive experiments based on real datasets compared with existing similar studies. Experimental results demonstrate that our algorithms can achieve better average performance with confidence levels. For instance, our algorithms achieve 86% lower average delay experienced by users and 20% average energy consumption of each device, as well as 7% higher average hit ratio and 1.3 times more residual cache storage resource, than their counterparts.},
  archive      = {J_TPDS},
  author       = {Qiufen Xia and Zhiwei Jiao and Zichuan Xu},
  doi          = {10.1109/TPDS.2023.3326187},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  month        = {1},
  number       = {1},
  pages        = {1-19},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Online learning algorithms for context-aware video caching in D2D edge networks},
  volume       = {35},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
