<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAFFC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="taffc---163">TAFFC - 163</h2>
<ul>
<li><details>
<summary>
(2024). A wide evaluation of ChatGPT on affective computing tasks.
<em>IEEE Transactions on Affective Computing</em>, <em>15</em>(4),
2204–2212. (<a
href="https://doi.org/10.1109/TAFFC.2024.3419593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the rise of foundation models, a new artificial intelligence paradigm has emerged, by simply using general purpose foundation models with prompting to solve problems instead of training a separate machine learning model for each problem. Such models have been shown to have emergent properties of solving problems that they were not initially trained on. The studies for the effectiveness of such models are still quite limited. In this work, we widely study the capabilities of the ChatGPT models, namely GPT-4 and GPT-3.5, on 13 affective computing problems, namely aspect extraction, aspect polarity classification, opinion extraction, sentiment analysis, sentiment intensity ranking, emotions intensity ranking, suicide tendency detection, toxicity detection, well-being assessment, engagement measurement, personality assessment, sarcasm detection, and subjectivity detection. We introduce a framework to evaluate the ChatGPT models on regression-based problems, such as intensity ranking problems, by modelling them as pairwise ranking classification. We compare ChatGPT against more traditional NLP methods, such as end-to-end recurrent neural networks and transformers. The results demonstrate the emergent abilities of the ChatGPT models on a wide range of affective computing problems, where GPT-3.5 and especially GPT-4 have shown strong performance on many problems, particularly the ones related to sentiment, emotions, or toxicity. The ChatGPT models fell short for problems with implicit signals, such as engagement measurement and subjectivity detection.},
  archive  = {J},
  author   = {Mostafa M. Amin and Rui Mao and Erik Cambria and Björn W. Schuller},
  doi      = {10.1109/TAFFC.2024.3419593},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {2204-2212},
  title    = {A wide evaluation of ChatGPT on affective computing tasks},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards generalised and incremental bias mitigation in
personality computing. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(4), 2192–2203. (<a
href="https://doi.org/10.1109/TAFFC.2024.3409830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Building systems for predicting human socio-emotional states has promising applications; however, if trained on biased data, such systems could inadvertently yield biased decisions. Bias mitigation remains an open problem, which tackles the correction of a model&#39;s disparate performance over different groups defined by particular sensitive attributes (e.g., gender, age, and race). In this work, we design a novel fairness loss function named Multi-Group Parity (MGP) to provide a generalised approach for bias mitigation in personality computing. In contrast to existing works in the literature, MGP is generalised as it features four ‘multiple’ properties (4Mul): multiple tasks, multiple modalities, multiple sensitive attributes, and multi-valued attributes. Moreover, we explore how to incrementally mitigate the biases when more sensitive attributes are taken into consideration sequentially. Towards this problem, we introduce a novel algorithm that utilises an incremental learning framework to mitigate bias against one attribute data at a time without compromising past fairness. Extensive experiments on two large-scale multi-modal personality recognition datasets validate the effectiveness of our approach in achieving superior bias mitigation under the proposed four properties and incremental debiasing settings.},
  archive  = {J},
  author   = {Jian Jiang and Viswonathan Manoranjan and Hanan Salam and Oya Celiktutan},
  doi      = {10.1109/TAFFC.2024.3409830},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {2192-2203},
  title    = {Towards generalised and incremental bias mitigation in personality computing},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). U-shaped distribution guided sign language emotion
recognition with semantic and movement features. <em>IEEE Transactions
on Affective Computing</em>, <em>15</em>(4), 2180–2191. (<a
href="https://doi.org/10.1109/TAFFC.2024.3409357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotional expression is a bridge to human communication, especially for the hearing impaired. This paper proposes a sign language emotion recognition method based on semantic and movement features by exploring the relationship between emotion valence and arousal in-depth, called SeMER. The SeMER framework includes a semantic extractor, a movement feature extractor, and an emotion classifier. The contextual relations obtained from the sign language recognition task are added to the semantic extractor as prior knowledge using a transfer learning approach to better acquire the affective polarity of semantics. In the movement feature extractor based on graph convolutional networks, a spatial-temporal adjacency matrix of gestures and node attention matrix are developed to aggregate the emotion-related movement features of intra- and inter-gestures. The proposed emotion classifier maps semantic and movement features to the emotion space. The validated U-shaped distributions of valance and arousal are then used to guide the relationship between them, and improve the accuracy of emotion prediction. In addition, a sign language emotion dataset containing 5 emotions from 18 participants, SE-Sentence, is collected through armbands with built-in surface electromyograph and inertial measurement unit sensors. Experimental results showed that SeMER achieved an accuracy and f1 value of 88% on SE-Sentence.},
  archive  = {J},
  author   = {Jiangtao Zhang and Qingshan Wang and Qi Wang},
  doi      = {10.1109/TAFFC.2024.3409357},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {2180-2191},
  title    = {U-shaped distribution guided sign language emotion recognition with semantic and movement features},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Affect-conditioned image generation. <em>IEEE Transactions
on Affective Computing</em>, <em>15</em>(4), 2169–2179. (<a
href="https://doi.org/10.1109/TAFFC.2024.3406726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In creativity support and computational co-creativity contexts, the task of discovering appropriate prompts for use with text-to-image generative models remains difficult. In many cases the creator wishes to evoke a certain impression with the image, but the task of conferring that succinctly in a text prompt poses a challenge: affective language is nuanced, complex, and very much influenced by the training trajectory of each specific AI model. In this work we introduce a method for generating images conditioned on desired affect, quantified using a psychometrically validated three-component approach, that can be combined with conditioning on text descriptions. We first train a neural network for estimating the affect content of text and images from semantic embeddings, and then demonstrate how this can be used to exert control over a variety of generative models. We show examples of how affect modifies the outputs, provide quantitative and qualitative analysis of its capabilities, and discuss possible extensions and use cases. We also show the capacity of our affect-guided generation to output images which re-frame or extend ideas in original ways that may not have been immediately apparent to the human prompt-writer.},
  archive  = {J},
  author   = {Francisco Ibarrola and Rohan Lulham and Kazjon Grace},
  doi      = {10.1109/TAFFC.2024.3406726},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {2169-2179},
  title    = {Affect-conditioned image generation},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rethinking inconsistent context and imbalanced regression in
depression severity prediction. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(4), 2154–2168. (<a
href="https://doi.org/10.1109/TAFFC.2024.3405584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As one of the world&#39;s most prevalent mental illnesses, depression is not easy to detect since it affects different people in different ways. Recently, linguistic features extracted from transcribed texts have been widely explored in depression detection because they contain a variety of cues about psychological activities. However, the detection performance is limited due to the following two reasons: 1) the dialogue structure is ignored, which causes the Inconsistent Context problem; and 2) Imbalanced Regression occurs due to the long-tailed distribution of depression datasets. To this end, in this paper we investigate the relationship between the local topic and global context in interview transcripts, and bridge the gap between depression symptoms and depression severity. In particular, we propose a model called Conditional Variational Topic-enriched Auto-Encoder (CVTAE), which can capture the spatial features from local topics via variational inference, and the temporal features from the global context with attention mechanism. Besides, we apply the re-weighting strategies to assigning weights to the depression labels with different values. Extensive experiments on the DAIC-WOZ dataset in English and a self-constructed database NCUDID in Chinese demonstrate the effectiveness and robustness of CVTAE, while the comprehensive ablation study and case study show its interpretability.},
  archive  = {J},
  author   = {Guanhe Huang and Jing Li and Heli Lu and Ming Guo and Shengyong Chen},
  doi      = {10.1109/TAFFC.2024.3405584},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {2154-2168},
  title    = {Rethinking inconsistent context and imbalanced regression in depression severity prediction},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VyaktitvaNirdharan: Multimodal assessment of personality and
trait emotional intelligence. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(4), 2139–2153. (<a
href="https://doi.org/10.1109/TAFFC.2024.3404243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Automatic personality assessment (APA) has immense potential to improve decision-making and human-machine interaction. Numerous techniques for APA have been proposed in existing literature, with prior psychological studies demonstrating convergent validity between personality and trait emotional intelligence (EI). However, to the best of our knowledge, none in APA literature has leveraged this relationship. Further, the primary language for most APA studies is English. To this end, we propose VyaktitvaNirdharan , a multi-modal-multitask learning (MM-MTL) system for predicting personality and EI traits using dyadic conversations in a non-intrusive and near real-time fashion. While most of the previous research has focused on APA for the English language, VyaktitvaNirdharan is also amongst the first to use peer-to-peer conversations in Hindi, a low-resource language. We also show the convergent validity between the predictions of the developed system, confirming that the personality and EI traits estimated by our system preserve the properties of the original traits. Our system can identify the Big-Five and EI traits with average F1-scores of 0.88, and 0.82, respectively. The validity of our findings is further supported by a user study involving 7 stakeholders, including recruiters and clinical psychologists, conducted through a prototype. The proposed system demonstrates promising results and has the potential to aid in improving decision-making in recruitment, mental health wellness and enhance human-machine interaction.},
  archive  = {J},
  author   = {Maitree Leekha and Shahid Nawaz Khan and Harshita Srinivas and Rajiv Ratn Shah and Jainendra Shukla},
  doi      = {10.1109/TAFFC.2024.3404243},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {2139-2153},
  title    = {VyaktitvaNirdharan: Multimodal assessment of personality and trait emotional intelligence},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EEG microstates and fNIRS metrics reveal the spatiotemporal
joint neural processing features of human emotions. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(4), 2128–2138. (<a
href="https://doi.org/10.1109/TAFFC.2024.3399729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotions deeply influence human behavior and decision-making. Currently, the spatiotemporal joint neural processing pattern of human emotions remains largely unclear. This study employed EEG-fNIRS simultaneous recordings to capture the spatiotemporal neural processing characteristics of human emotions by presenting Chinese emotional video stimuli. 1) EEG microstates’ temporal dynamic: Compared to low emotional arousal, microstate C and D&#39;s activities and C $\rightleftharpoons$ D&#39;s transition probability significantly increased. 2) fNIRS spatial patterns: Compared to low arousal, the dorsolateral prefrontal cortex (DLPFC), inferior frontal gyrus (IFG), and temporoparietal junction (TPJ)&#39;s oxygenated hemoglobin (HbO) concentrations significantly increased, along with the significant increase of DLPFC-IFG&amp;TPJ&#39;s functional connectivity during high arousal. Compared to high valence, the DLPFC was significantly activated during low valence. 3) EEG-fNIRS spatiotemporal joint features: Compared to low arousal, there was a significant positive correlation between the occurrence of microstate D (corresponded to dorsal attention network, DAN) and HbO concentrations of DLPFC (DAN&#39;s key node) during high arousal, which consistently revealed the DAN&#39;s involvement for human emotional arousal processing. These results could provide not only multimodal complementary features for promoting the development of affective brain-computer interface, but also potential objective spatiotemporal neural markers for emotional disorders.},
  archive  = {J},
  author   = {Xiaopeng Si and He Huang and Jiayue Yu and Dong Ming},
  doi      = {10.1109/TAFFC.2024.3399729},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {2128-2138},
  title    = {EEG microstates and fNIRS metrics reveal the spatiotemporal joint neural processing features of human emotions},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EmoTake: Exploring drivers’ emotion for takeover behavior
prediction. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(4), 2112–2127. (<a
href="https://doi.org/10.1109/TAFFC.2024.3399328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The blossoming semi-automated vehicles allow drivers to engage in various non-driving-related tasks, which may stimulate diverse emotions, thus affecting takeover safety. Though the effects of emotion on takeover behavior have recently been examined, how to effectively obtain and utilize drivers’ emotions for predicting takeover behavior remains largely unexplored. We propose EmoTake, a deep learning-empowered system that explores drivers’ emotional and physical states to predict takeover readiness, reaction time, and quality. The key enabler is a deep neural framework that extracts drivers’ fine-grained body movements from a camera and interprets them into drivers’ multi-channel emotional and physical information (e.g., facial expression, and head pose) for prediction. Our study (N = 26) verifies the efficiency of EmoTake and shows that: 1) facial expression benefits prediction; 2) emotions have diverse impacts on takeovers. Our findings provide insights into takeover prediction and in-vehicle emotion regulation.},
  archive  = {J},
  author   = {Yu Gu and Yibing Weng and Yantong Wang and Meng Wang and Guohang Zhuang and Jinyang Huang and Xiaolan Peng and Liang Luo and Fuji Ren},
  doi      = {10.1109/TAFFC.2024.3399328},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {2112-2127},
  title    = {EmoTake: Exploring drivers’ emotion for takeover behavior prediction},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SynSem-ASTE: An enhanced multi-encoder network for aspect
sentiment triplet extraction with syntax and semantics. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(4), 2097–2111. (<a
href="https://doi.org/10.1109/TAFFC.2024.3397961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Aspect Sentiment Triplet Extraction (ASTE) is an essential task in fine-grained opinion mining and sentiment analysis that involves extracting triplets consisting of aspect terms, opinion terms, and their associated sentiment polarities from texts. While prevailing approaches primarily adopt pipeline frameworks or unified tagging schemes for this task, these methods tend to either overlook syntactic structural information and inherent semantic features, or lack explicit mechanisms for integration of syntax and semantics among the triplets’ elements. To overcome these shortcomings, we propose an Enhanced Multi-Encoder Network for ASTE with Syntax and Semantics (SynSem-ASTE). Our model innovatively incorporates syntactic information and semantic features derived from syntactic structures and attention weights, which is achieved through the design of a syntax encoder and a semantics encoder. Furthermore, we adopt a grid tagging scheme and an effective inference strategy to extract triplets simultaneously. Extensive evaluations on four benchmark datasets reveal that SynSem-ASTE not only achieves superior performance in terms of the primary metric F1-score, but also exhibits enhanced robustness against variations in model architecture.},
  archive  = {J},
  author   = {Lulin Liu and Tao Qin and Yuankun Zhou and Chenxu Wang and Xiaohong Guan},
  doi      = {10.1109/TAFFC.2024.3397961},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {2097-2111},
  title    = {SynSem-ASTE: An enhanced multi-encoder network for aspect sentiment triplet extraction with syntax and semantics},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Boosting micro-expression recognition via self-expression
reconstruction and memory contrastive learning. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(4), 2083–2096. (<a
href="https://doi.org/10.1109/TAFFC.2024.3397701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Micro-expression (ME) is an instinctive reaction that is not controlled by thoughts. It reveals one&#39;s inner feelings, which is significant in sentiment analysis and lie detection. Since micro-expression is expressed as subtle facial changes within particular facial action units, learning discriminative and generalized features for Micro-expression Recognition (MER) is challenging. To achieve the purpose, this paper proposes a novel MER framework that simultaneously integrates supervised Prototype-based Memory Contrastive Learning (PMCL) for discriminative feature mining and adds Self-expression Reconstruction (SER) as an auxiliary task and regularization for better generalization. In particular, the proposed SER module is forced as a regularization by reconstructing input ME from the randomly dropped patch-wise features in the bottleneck. And, the PMCL module globally compares historical and current cluster agents learned from training instances to enhance intra-class compactness and inter-class separability. Extensive experiments are conducted on three benchmarks, e.g., SMIC, CASME II, and SAMM, under evaluation criteria of both Composite Database Evaluation (CDE) and Single Database Evaluation (SDE) protocols. The results show our method surpasses other state-of-the-art approaches under various evaluation metrics, achieving overall 86.30% unweighed F1-score and 88.30% unweighed average recall on the composite dataset. Furthermore, the ablation studies verify the effectiveness of our SER for better generalization and PMCL for better discrimination in learning feature representation from limited micro-expression samples.},
  archive  = {J},
  author   = {Yongtang Bao and Chenxi Wu and Peng Zhang and Caifeng Shan and Yue Qi and Xianye Ben},
  doi      = {10.1109/TAFFC.2024.3397701},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {2083-2096},
  title    = {Boosting micro-expression recognition via self-expression reconstruction and memory contrastive learning},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weakly correlated multimodal sentiment analysis: New dataset
and topic-oriented model. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(4), 2070–2082. (<a
href="https://doi.org/10.1109/TAFFC.2024.3396144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Existing multimodal sentiment analysis models focus more on fusing highly correlated image-text pairs, and thus achieves unsatisfactory performance on multimodal social media data which usually manifests weak correlations between different modalities. To address this issue, we first build a large multimodal social media sentiment analysis dataset RU-Senti which contains more than 100,000 image-text pairs with sentiment labels. Then, we proposed a topic-oriented model (TOM) which assumes that text is usually related to a certain portion of the image contents and significant variances exist in sentiment distribution across diverse topics. TOM learns the topic information from textual content and designs a topic-oriented feature alignment module to extract textual semantics correlated information from images, thus achieving the alignment between two modalities. Then, TOM utilizes a transformer encoder initialized with the parameters from a pre-trained vision-language model to fuse the multimodal features for sentiment prediction. According to the experiments over the public MVSA-Multiple dataset and our RU-Senti dataset, RU-Senti is of high suitability for studying weakly correlated multimodal sentiment analysis, and the proposed TOM model also largely outperforms the SOTA mulitimodal sentiment analysis methods and pre-trained vision-language models.},
  archive  = {J},
  author   = {Wuchao Liu and Wengen Li and Yu-Ping Ruan and Yulou Shu and Juntao Chen and Yina Li and Caili Yu and Yichao Zhang and Jihong Guan and Shuigeng Zhou},
  doi      = {10.1109/TAFFC.2024.3396144},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {2070-2082},
  title    = {Weakly correlated multimodal sentiment analysis: New dataset and topic-oriented model},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A weighted co-training framework for emotion recognition
based on EEG data generation using frequency-spatial diffusion
transformer. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(4), 2055–2069. (<a
href="https://doi.org/10.1109/TAFFC.2024.3395359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion recognition based on EEG signals has been a challenging task. The acquisition of EEG signals is complex, time-consuming, and has a high overhead. Artificial Intelligence Generated Content technology has been developing rapidly in image and sound generation, but effective generative models for EEG signal generation have rarely appeared. To address the problem, we propose a weighted co-training framework for emotion recognition using a frequency-spatial diffusion transformer. We propose the EEG signal generation model by utilizing frequency-spatial correlation. In the first step, we use forward diffusion to add noise to the real samples and then use the proposed model to denoise and restore the real EEG signals. It ensures that the trained generative model can generate real EEG signals. Then, we use the denoising process of the proposed model to generate a large amount of data and then use the pseudo-data weighting module to evaluate the generated samples further. Finally, the actual samples and weighted pseudo-data are used to train the classifier for better generalization jointly. We conducted comprehensive experiments on three EEG emotion recognition benchmark datasets. The experimental results show that our method improves by 0.86%-1.91% compared to state-of-the-art methods. In addition, quantitative and qualitative analysis proves the effectiveness of our proposed method.},
  archive  = {J},
  author   = {Yufan Yi and Yiping Xu and Bo Yang and Yan Tian},
  doi      = {10.1109/TAFFC.2024.3395359},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {2055-2069},
  title    = {A weighted co-training framework for emotion recognition based on EEG data generation using frequency-spatial diffusion transformer},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bridge graph attention based graph convolution network with
multi-scale transformer for EEG emotion recognition. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(4), 2042–2054. (<a
href="https://doi.org/10.1109/TAFFC.2024.3394873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In multichannel electroencephalograph (EEG) emotion recognition, most graph-based studies employ shallow graph model for spatial characteristics learning due to node over-smoothing caused by an increase in network depth. To address over-smoothing, we propose the bridge graph attention-based graph convolution network (BGAGCN). It bridges previous graph convolution layers to attention coefficients of the final layer by adaptively combining each graph convolution output based on the graph attention network, thereby enhancing feature distinctiveness. Considering that graph-based network primarily focus on local EEG channel relationships, we introduce a transformer for global dependency. Inspired by the neuroscience finding that neural activities of different timescales reflect distinct spatial connectivities, we modify the transformer to a multi-scale transformer (MT) by applying multi-head attention to multichannel EEG signals after 1D convolutions at different scales. MT learns spatial features more elaborately to enhance feature representation ability. By combining BGAGCN and MT, our model BGAGCN-MT achieves state-of-the-art accuracy under subject-dependent and subject-independent protocols across three benchmark EEG emotion datasets (SEED, SEED-IV and DREAMER). Notably, our model effectively addresses over-smoothing in graph neural networks and provides an efficient solution to learning spatial relationships of EEG features at different scales.},
  archive  = {J},
  author   = {Huachao Yan and Kailing Guo and Xiaofen Xing and Xiangmin Xu},
  doi      = {10.1109/TAFFC.2024.3394873},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {2042-2054},
  title    = {Bridge graph attention based graph convolution network with multi-scale transformer for EEG emotion recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal prediction of obsessive-compulsive disorder and
comorbid depression severity and energy delivered by deep brain
electrodes. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(4), 2025–2041. (<a
href="https://doi.org/10.1109/TAFFC.2024.3395117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {To develop reliable, valid, and efficient measures of obsessive-compulsive disorder (OCD) severity, comorbid depression severity, and total electrical energy delivered (TEED) by deep brain stimulation (DBS), we trained and compared random forests regression models in a clinical trial of participants receiving DBS for refractory OCD. Six participants were recorded during open-ended interviews at pre- and post-surgery baselines and then at 3-month intervals following DBS activation. Ground-truth severity was assessed by clinical interview and self-report. Visual and auditory modalities included facial action units, head and facial landmarks, speech behavior and content, and voice acoustics. Mixed-effects random forest regression with Shapley feature reduction strongly predicted severity of OCD, comorbid depression, and total electrical energy delivered by the DBS electrodes (intraclass correlation, ICC, = 0.83, 0.87, and 0.81, respectively. When random effects were omitted from the regression, predictive power decreased to moderate for severity of OCD and comorbid depression and remained comparable for total electrical energy delivered (ICC = 0.60, 0.68, and 0.83, respectively). Multimodal measures of behavior outperformed ones from single modalities. Feature selection achieved large decreases in features and corresponding increases in prediction. The approach could contribute to closed-loop DBS that would automatically titrate DBS based on affect measures.},
  archive  = {J},
  author   = {Saurabh Hinduja and Ali Darzi and Itir Onal Ertugrul and Nicole Provenza and Ron Gadot and Eric A. Storch and Sameer A. Sheth and Wayne K. Goodman and Jeffrey F. Cohn},
  doi      = {10.1109/TAFFC.2024.3395117},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {2025-2041},
  title    = {Multimodal prediction of obsessive-compulsive disorder and comorbid depression severity and energy delivered by deep brain electrodes},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spectral-spatial attention alignment for multi-source domain
adaptation in EEG-based emotion recognition. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(4), 2012–2024. (<a
href="https://doi.org/10.1109/TAFFC.2024.3394436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In electroencephalographic-based (EEG-based) emotion recognition, high non-stationarity and individual differences in EEG signals could lead to significant discrepancies between sessions/subjects, making generalization to a new session/subject very difficult. Most existing domain adaptation (DA) and multi-source domain adaptation (MSDA) techniques aim to mitigate this discrepancy by aligning feature distributions. However, when confronted with many diverse domain distributions, learning domain-invariant features via aligning pairwise feature distributions between domains can be hard or even counterproductive. To address this issue, this article proposes an attention alignment approach to learning abundant domain-invariant features. The motivation is simple: despite individual differences causing significant differences in feature distributions in EEG-based emotion recognition, shared affective cognitive attributes (attention) of spectral and spatial domains can be observed within the same emotion categories. The proposed spectral-spatial attention alignment multi-source domain adaptation (S 2 A 2 -MSDA) constructs domain attention to represent affective cognition attributes in spatial and spectral domains and utilizes domain consistent loss to align them between domains. Furthermore, to facilitate discriminative feature learning on the target classes, S 2 A 2 -MSDA learns the conditional semantic information of the target domain using a pseudo-labeling method. This algorithm has been validated on the SEED and SEED-IV datasets in cross-session and cross-subject scenarios, respectively. Experimental results demonstrate that S 2 A 2 -MSDA outperforms existing representative DA and MSDA methods, achieving state-of-the-art performance.},
  archive  = {J},
  author   = {Yi Yang and Ze Wang and Wei Tao and Xucheng Liu and Ziyu Jia and Boyu Wang and Feng Wan},
  doi      = {10.1109/TAFFC.2024.3394436},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {2012-2024},
  title    = {Spectral-spatial attention alignment for multi-source domain adaptation in EEG-based emotion recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving representation with hierarchical contrastive
learning for emotion-cause pair extraction. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(4), 1997–2011. (<a
href="https://doi.org/10.1109/TAFFC.2024.3391854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion-cause pair extraction (ECPE) aims to extract emotions and their corresponding cause from a document. The previous works have made great progress. However, there exist two major issues in existing works. First, most existing works mainly focus on the semantic relation between the emotion clause and cause clause, ignoring their inner statistical relation in representation space. Second, the existing works are sensitive to the relative position between the emotion clause and cause clause, which damages the model&#39;s robustness. To address the two issues, we propose a hierarchical contrastive learning framework (HCL-ECPE), which hierarchically performs contrastive learning on representation from two levels. The first level is inter-clause contrastive learning (ICCL), which performs between emotion clause and cause clause through mutual information maximization. The second level is intra-pair contrastive learning (IPCL), which performs between clause representation and pair representation through contrastive predictive coding (CPC). HCL-ECPE integrates ICCL and IPCL modules to explore the statistical relations between the emotion clause, cause clause, and their constructed emotion-cause pair from the perspective of mutual information, thereby improving the model performance and robustness. Experimental results on two public datasets, ECPED and RECCON, demonstrate that HCL-ECPE outperforms the most competitive baselines. Furthermore, ICCL and IPCL are orthogonal to the existing model, and introducing them into the current models updates state-of-the-art performance.},
  archive  = {J},
  author   = {Guimin Hu and Yi Zhao and Guangming Lu},
  doi      = {10.1109/TAFFC.2024.3391854},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {1997-2011},
  title    = {Improving representation with hierarchical contrastive learning for emotion-cause pair extraction},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MAST-GCN: Multi-scale adaptive spatial-temporal graph
convolutional network for EEG-based depression recognition. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(4), 1985–1996. (<a
href="https://doi.org/10.1109/TAFFC.2024.3392904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, depression recognition through EEG has gained significant attention. However, two challenges have not been properly addressed in prior automated depression recognition and classification studies: 1) EEG data lacks an explicit topological structure. 2) Capturing spatio-temporal features of EEG signals is difficult. In this paper, we propose Multi-scale Adaptive Spatial-Temporal Graph Convolutional Network (MAST-GCN) for mining latent topological structure among EEG channels and capturing discriminative spatio-temporal features. First, we integrate Adaptive Graph Convolution (AGC) that merges the inherent graph construction method with a data-driven graph reconstruction method. The model uses attention mechanism to learn an adaptive topological structure and semantic information from different layers and classes. Second, we propose Multi-Scale Time Convolutional Layer (MS-TCL), which captures long-term dependence from EEG data. Since Graph Convolution is weak for aggregating the spatio-temporal information, we have implemented a 3D Graph Convolution (G3D) to directly capture the spatio-temporal dependencies by reconstructing the spatio-temporal graph. The experimental results demonstrate that MAST-GCN consistently outperforms state-of-the-art methods on two datasets. Furthermore, we use the gradient-based saliency maps for interpretability analysis, discovering the active brain regions and important electrode pairs related to depression.},
  archive  = {J},
  author   = {Haifeng Lu and Zhiyang You and Yi Guo and Xiping Hu},
  doi      = {10.1109/TAFFC.2024.3392904},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {1985-1996},
  title    = {MAST-GCN: Multi-scale adaptive spatial-temporal graph convolutional network for EEG-based depression recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CiABL: Completeness-induced adaptative broad learning for
cross-subject emotion recognition with EEG and eye movement signals.
<em>IEEE Transactions on Affective Computing</em>, <em>15</em>(4),
1970–1984. (<a
href="https://doi.org/10.1109/TAFFC.2024.3392791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Although multimodal physiological data from the central and peripheral nervous systems can objectively respond to human emotional states, the individual differences caused by non-stationary and low signal-to-noise properties bring several challenges to cross-subject emotion recognition tasks. Many previous studies usually focused on learning high correlation information between different modalities, which easily leads to incomplete descriptions of different physiological signals and difficulties in aligning critical emotional information. To tackle these challenges, this paper proposes a novel multimodal emotion recognition model for improving the generalization performance to unseen target domain subjects, termed Completeness-induced Adaptative Broad Learning (CiABL). The proposed CiABL can gradely explore the completeness modality representation that encompasses both modality-relevant and modality-independent information, avoiding the loss of performance due to spurious correlations from different modalities. Subsequently, a well-designed weighted representation distribution alignment mechanism of CiABL can appropriately align the marginal and conditional distributions to reduce the influences of individual differences greatly. Extensive experiments on the SEED and SEED-FRA datasets demonstrate the effectiveness and generalization of the proposed CiABL, which outperforms current state-of-the-art methods. In addition, CiABL can precisely quantify the importance of global features to properly explain the modality contribution and averaged activation patterns of the brain under cross-subject emotion recognition tasks.},
  archive  = {J},
  author   = {Xinrong Gong and C. L. Philip Chen and Bin Hu and Tong Zhang},
  doi      = {10.1109/TAFFC.2024.3392791},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {1970-1984},
  title    = {CiABL: Completeness-induced adaptative broad learning for cross-subject emotion recognition with EEG and eye movement signals},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling category semantic and sentiment knowledge for
aspect-level sentiment analysis. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(4), 1962–1969. (<a
href="https://doi.org/10.1109/TAFFC.2024.3391337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {To classify the sentiment polarity of the aspect entity in a sentence, most existing research evaluates the semantic knowledge among a certain aspect of a sentence and corresponding context as significant clues for the task. However, available accompanying information has not been completely exploited, especially the coarse-grained category-level knowledge in contexts. Such knowledge can help to alleviate polysemy and ambivalence problems. In this article, we propose a multi-task learning framework Co-interactive Attention Network(CoAN) to jointly learn and handle multiple granularity features at both target and category levels. In order to leverage the fine-grained and coarse-grained knowledge in contexts and get multi-granularity sentiment related sentence representations, we introduce two co-interactive attention layers to conduct accompanying semantic interactions at the word-level and the feature-level. The experimental results on three restaurant review datasets prove that CoAN is superior to the baselines by 1.41% in accuracy and 2.81% in F1-score. Furthermore, ablation studies and attention visualizations show that the multi-task framework and novel co-interactive mechanisms can distinguish and fuse multi-granularity knowledge, which benefits the two subtasks in aspect based sentiment analysis.},
  archive  = {J},
  author   = {Yuan Wang and Peng Huo and Lingyan Tang and Ning Xiong and Mengting Hu and Qi Yu and Jucheng Yang},
  doi      = {10.1109/TAFFC.2024.3391337},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {1962-1969},
  title    = {Modeling category semantic and sentiment knowledge for aspect-level sentiment analysis},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluation of virtual agents’ hostility in video games.
<em>IEEE Transactions on Affective Computing</em>, <em>15</em>(4),
1949–1961. (<a
href="https://doi.org/10.1109/TAFFC.2024.3390400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Non-Playable Characters (NPCs) are a subtype of virtual agents that populate video games by endorsing social roles in the narrative. To infer NPCs’ roles, players evaluate NPCs’ appearance and behaviors, usually by ascribing human traits to NPCs, such as intelligence, likability and morality. In particular, hostile NPCs in video games are essential to build the games’ inherent challenges. The three experiments reported here investigated the extent to which the perception of hostility in a military shooter game (including both threat of appearance and aggressiveness in behaviors) is influenced by the appearance and the behaviors of NPCs thanks to perceived intelligence, likability and morality-related questionnaires. Our results first show that hostility is efficiently conveyed through NPCs’ behaviors, but not significantly by their appearance. Second, our study allows identifying the main predictors of hostility perception, namely unfriendliness, knowledge and harmfulness.},
  archive  = {J},
  author   = {Remi Poivet and Alexandra de Lagarde and Catherine Pelachaud and Malika Auvray},
  doi      = {10.1109/TAFFC.2024.3390400},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {1949-1961},
  title    = {Evaluation of virtual agents’ hostility in video games},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical shared encoder with task-specific transformer
layer selection for emotion-cause pair extraction. <em>IEEE Transactions
on Affective Computing</em>, <em>15</em>(4), 1934–1948. (<a
href="https://doi.org/10.1109/TAFFC.2024.3390223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion Cause Pair Extraction (ECPE) aims to extract emotions and their causes from a document. Powerful emotion and cause extraction abilities have proven essential in achieving accurate ECPE. However, most existing methods employ shared feature learning of emotion extraction and cause extraction, which can harm the abilities of both tasks as they focus on different information (i.e., task-specific features). Moreover, shared feature learning of the two tasks also leads to the label imbalance problem. To address these issues, this paper proposes a multi-task learning framework named Hierarchical Shared Encoder with Task-specific Transformer Layer Selection (HSE-TTLS). The model achieves ECPE via two subtasks: Emotion Extraction (EE) and Emotion Cause Extraction (ECE). The design of two subtasks for ECPE corresponds to the fact that cause clauses are emotion-dependent and significantly alleviates the label imbalance problem. To effectively extract task-specific features for EE and ECE, we employ BERT as the token-level encoder and select task-specific optimal layers for the two subtasks. Focal loss is used as the objective function for EE to further alleviate the label imbalance problem. Extensive experiments on benchmark ECPE corpus demonstrate the effectiveness of HSE-TTLS, which outperforms state-of-the-art baseline methods by at least 1.56% on the F1 score.},
  archive  = {J},
  author   = {Xinxin Su and Zhen Huang and Yixin Su and Bayu Distiawan Trisedya and Yong Dou and Yunxiang Zhao},
  doi      = {10.1109/TAFFC.2024.3390223},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {1934-1948},
  title    = {Hierarchical shared encoder with task-specific transformer layer selection for emotion-cause pair extraction},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CFN-ESA: A cross-modal fusion network with emotion-shift
awareness for dialogue emotion recognition. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(4), 1919–1933. (<a
href="https://doi.org/10.1109/TAFFC.2024.3389453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multimodal emotion recognition in conversation (ERC) has garnered growing attention from research communities in various fields. In this paper, we propose a Cross-modal Fusion Network with Emotion-Shift Awareness (CFN-ESA) for ERC. Extant approaches employ each modality equally without distinguishing the amount of emotional information in these modalities, rendering it hard to adequately extract complementary information from multimodal data. To cope with this problem, in CFN-ESA, we treat textual modality as the primary source of emotional information, while visual and acoustic modalities are taken as the secondary sources. Besides, most multimodal ERC models ignore emotion-shift information and overfocus on contextual information, leading to the failure of emotion recognition under emotion-shift scenario. We elaborate an emotion-shift module to address this challenge. CFN-ESA mainly consists of unimodal encoder (RUME), cross-modal encoder (ACME), and emotion-shift module (LESM). RUME is applied to extract conversation-level contextual emotional cues while pulling together data distributions between modalities; ACME is utilized to perform multimodal interaction centered on textual modality; LESM is used to model emotion shift and capture emotion-shift information, thereby guiding the learning of the main task. Experimental results demonstrate that CFN-ESA can effectively promote performance for ERC and remarkably outperform state-of-the-art models.},
  archive  = {J},
  author   = {Jiang Li and Xiaoping Wang and Yingjian Liu and Zhigang Zeng},
  doi      = {10.1109/TAFFC.2024.3389453},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {1919-1933},
  title    = {CFN-ESA: A cross-modal fusion network with emotion-shift awareness for dialogue emotion recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FBSTCNet: A spatio-temporal convolutional network
integrating power and connectivity features for EEG-based emotion
decoding. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(4), 1906–1918. (<a
href="https://doi.org/10.1109/TAFFC.2024.3385651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Electroencephalography (EEG)-based emotion recognition plays a key role in the development of affective brain-computer interfaces (BCIs). However, emotions are complex and extracting salient EEG features underlying distinct emotional states is inherently limited by low signal-to-noise ratio (SNR) and low spatial resolution of practical EEG data, which is further compounded by the lack of effective spatio-temporal filter optimization approaches for generic EEG features. To address these challenges, this study proposes a set of neural networks termed the Filter-Bank Spatio-Temporal Convolutional Networks (FBSTCNets) for performing end-to-end multi-class emotion recognition via robust extraction of power and/or connectivity features from EEG. First, a filter bank is employed to construct a multiview spectral representation of EEG data. Next, a temporal convolutional layer, followed by a depth-wise spatial convolutional layer, performs spatio-temporal filtering, transforming EEG into latent signals with higher SNR. A feature extraction layer then extracts power and/or connectivity features from the latent signals. Finally, a fully connected layer with a cropped decoding strategy predicts the emotional state. Experimental results on two public emotion EEG datasets, SEED and SEED-IV, demonstrate that FBSTCNets outperform previous benchmark methods in decoding accuracy. Our approach provides a principled emotion decoding framework for designing high-performance spatio-temporal filtering networks tailored to specific EEG feature types.},
  archive  = {J},
  author   = {Weichen Huang and Wenlong Wang and Yuanqing Li and Wei Wu},
  doi      = {10.1109/TAFFC.2024.3385651},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {1906-1918},
  title    = {FBSTCNet: A spatio-temporal convolutional network integrating power and connectivity features for EEG-based emotion decoding},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VAD: A video affective dataset with danmu. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(4), 1889–1905. (<a
href="https://doi.org/10.1109/TAFFC.2024.3382503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Although video affective content analysis has great potential in many applications, it has not been thoroughly studied due to limited datasets. In this article, we construct a large-scale video affective dataset with danmu (VAD). It consists of 19,267 elaborately segmented video clips from user-generated videos. The VAD dataset is annotated by the crowdsourcing platform with discrete valence, arousal, and primary emotions, as well as the comparison of valence and arousal between two consecutive video clips. Unlike previous datasets, including only video clips, our proposed dataset also provides danmu, which is the real-time comment from users as they watch a video. Danmu provides extra information for video affective content analysis. As a preliminary assessment of the usability of our dataset, an analysis of inter-annotator consistency for each label is conducted using weighted Fleiss’ Kappa, regular Fleiss’ Kappa, intraclass correlation coefficient, and percent consensus. Besides, we also perform a statistical analysis of labels and danmu. Finally, video affective content analysis is conducted on our dataset and three typical methods (i.e., TFN, MulT, and MISA) are leveraged to provide benchmarks. We also demonstrate that danmu can significantly improve the performance of the video affective content analysis task on some labels. Our dataset is available for research purposes.},
  archive  = {J},
  author   = {Shangfei Wang and Xin Li and Feiyi Zheng and Jicai Pan and Xuewei Li and Yanan Chang and Zhou’an Zhu and Qiong Li and Jiahe Wang and Yufei Xiao},
  doi      = {10.1109/TAFFC.2024.3382503},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {1889-1905},
  title    = {VAD: A video affective dataset with danmu},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fusion and discrimination: A multimodal graph contrastive
learning framework for multimodal sarcasm detection. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(4), 1874–1888. (<a
href="https://doi.org/10.1109/TAFFC.2024.3380375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Identifying sarcastic clues from both textual and visual information has become an important research issue, called Multimodal Sarcasm Detection. In this article, we investigate multimodal sarcasm detection from a novel perspective, where a multimodal graph contrastive learning strategy is proposed to fuse and distinguish the sarcastic clues for textual modality and visual modality. Specifically, we first utilize object detection to derive the crucial visual regions accompanied by their captions of the images, which allows better learning of the key visual regions of visual modality. In addition, to make full use of the semantic information of the visual modality, we employ optical character recognition to extract the textual content in the images. Then, based on image regions, the textual content of visual modality, and the context of the textual modality, we build a multimodal graph for each sample to model the intricate sarcastic relations between modalities. Furthermore, we devise a graph-oriented contrastive learning strategy to leverage the correlations in the same label and differences between different labels, so as to capture better multimodal representations for multimodal sarcasm detection. Extensive experiments show that our method outperforms the previous best baseline models (with a 2.47% improvement in Accuracy, a 1.99% improvement in F-score, and a 2.20% improvement in Macro F-score). The ablation study shows that both multimodal graph structure and graph-oriented contrastive learning are important to our framework. Further, the experiments of using different pre-trained methods show that the proposed multimodal graph contrastive learning framework can directly work with various pre-trained models and achieve outstanding performance in multimodal sarcasm detection.},
  archive  = {J},
  author   = {Bin Liang and Lin Gui and Yulan He and Erik Cambria and Ruifeng Xu},
  doi      = {10.1109/TAFFC.2024.3380375},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {1874-1888},
  title    = {Fusion and discrimination: A multimodal graph contrastive learning framework for multimodal sarcasm detection},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contrastive learning based modality-invariant feature
acquisition for robust multimodal emotion recognition with missing
modalities. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(4), 1856–1873. (<a
href="https://doi.org/10.1109/TAFFC.2024.3378570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multimodal emotion recognition (MER) aims to understand the way that humans express their emotions by exploring complementary information across modalities. However, it is hard to guarantee that full-modality data is always available in real-world scenarios. To deal with missing modalities, researchers focused on meaningful joint multimodal representation learning during cross-modal missing modality imagination. However, the cross-modal imagination mechanism is highly susceptible to errors due to the “modality gap” issue, which affects the imagination accuracy, thus, the final recognition performance. To this end, we introduce the concept of a modality-invariant feature into the missing modality imagination network, which contains two key modules: 1) a novel contrastive learning-based module to extract modality-invariant features under full modalities and 2) a robust imagination module based on imagined invariant features to reconstruct missing information under missing conditions. Finally, we incorporate imagined and available modalities for emotion recognition. Experimental results on benchmark datasets demonstrate that our proposed method outperforms existing state-of-the-art strategies. Compared with our previous work, our extended version is more effective on multimodal emotion recognition with missing modalities.},
  archive  = {J},
  author   = {Rui Liu and Haolin Zuo and Zheng Lian and Björn W. Schuller and Haizhou Li},
  doi      = {10.1109/TAFFC.2024.3378570},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {1856-1873},
  title    = {Contrastive learning based modality-invariant feature acquisition for robust multimodal emotion recognition with missing modalities},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GenSumm: A joint framework for multi-task tweet
classification and summarization using sentiment analysis and generative
modelling. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(4), 1838–1855. (<a
href="https://doi.org/10.1109/TAFFC.2021.3131516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Social media platforms like Twitter act as the medium for communication among people, government agencies, NGOs, and other relief providing agencies in widespread humanitarian havoc during a disaster outbreak when other communication means might not be available. Various agencies leverage Twitter&#39;s open and public features to get timely and reliable updates, thus support agencies in communicating with the people on rescue and provide immediate relief. As situational updates are mixed in millions of other tweets, an efficient system is required to extract and summarize these tweets. We have developed a novel framework that uses a deep learning-based classification model to separate the informational tweets from others and summarizes them in the current paper. Non-situational tweets mostly comprise sentiments like grief, anger, sorrow, etc. Motivated by this observation, we have solved sentiment classification and informative tweet selection tasks simultaneously using a multi-task learning (MTL) in a deep-learning framework. Our summarization approach generates clustering solutions using various existing approaches and then ensembles cluster solutions using generative modelling. A summary is formulated by extracting tweets from different clusters. The proposed approach&#39;s superior performance on four disaster-related events indicates the developed framework&#39;s efficiency over the state-of-the-art techniques.},
  archive  = {J},
  author   = {Diksha Bansal and Rahul Grover and Naveen Saini and Sriparna Saha},
  doi      = {10.1109/TAFFC.2021.3131516},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10-12},
  number   = {4},
  pages    = {1838-1855},
  title    = {GenSumm: A joint framework for multi-task tweet classification and summarization using sentiment analysis and generative modelling},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical encoding and fusion of brain functions for
depression subtype classification. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 1826–1837. (<a
href="https://doi.org/10.1109/TAFFC.2024.3401251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Depression is a serious mental disorder with complex etiology, exhibiting strong heterogeneity in clinical manifestations such as various subtypes. Research on depression subtypes may deepen the understanding of the disease, contributing to the diagnosis and prognosis. While brain functional network and graph neural networks (GNNs) provide such a means, the task is still challenged by limited feature encoding from the informative fMRI data, ineffective information fusion of brain functional network, and small size of the recruited subjects. Therefore, we propose a hierarchical encoding and fusion framework of brain functions. First, we pre-train a model to extract the features from individual brain regions, which signify nodes in the brain functional network. Then, distinct graphs are constructed to link the nodes within each subject, resulting in multi-view graphs of the brain functional network. We further develop a graph fusion strategy to integrate the multi-view information, by referring to the local encoding of the nodes and their interactions across multiple graph instances. Finally, we attain the classification of depression subtypes based on the fused graph representation. The experimental results demonstrate that our method can superiorly distinguish major depression subtypes and outperform the state-of-the-art methods.},
  archive  = {J},
  author   = {Mengjun Liu and Huifeng Zhang and Mianxin Liu and Dongdong Chen and Rubai Zhou and Wenxian Lu and Lichi Zhang and Dinggang Shen and Qian Wang and Daihui Peng},
  doi      = {10.1109/TAFFC.2024.3401251},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1826-1837},
  title    = {Hierarchical encoding and fusion of brain functions for depression subtype classification},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint training on multiple datasets with inconsistent
labeling criteria for facial expression recognition. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(3), 1812–1825. (<a
href="https://doi.org/10.1109/TAFFC.2024.3382618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {One potential way to enhance the performance of facial expression recognition (FER) is to augment the training set by increasing the number of samples. By incorporating multiple FER datasets, deep learning models can extract more discriminative features. However, the inconsistent labeling criteria and subjective biases found in annotated FER datasets can significantly hinder the recognition accuracy of deep learning models when handling mixed datasets. Effectively perform joint training on multiple datasets remains a challenging task. In this study, we propose a joint training method for training an FER model using multiple FER datasets. Our method consists of four steps: (1) selecting a subset from the additional dataset, (2) generating pseudo-continuous labels for the target dataset, (3) refining the labels of different datasets using continuous label mapping and discrete label relabeling according to the labeling criteria of the target dataset, and (4) jointly training the model using multi-task learning. We conduct joint training experiments on two popular in-the-wild FER benchmark databases, RAF-DB and CAER-S, while utilizing the AffectNet dataset as an additional dataset. The experimental results demonstrate that our proposed method outperforms the direct merging of different FER datasets into a single training set and achieves state-of-the-art performance on RAF-DB and CAER-S with accuracies of 92.24% and 94.57%, respectively.},
  archive  = {J},
  author   = {Chengyan Yu and Dong Zhang and Wei Zou and Ming Li},
  doi      = {10.1109/TAFFC.2024.3382618},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1812-1825},
  title    = {Joint training on multiple datasets with inconsistent labeling criteria for facial expression recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Emotion-aware multimodal fusion for meme emotion detection.
<em>IEEE Transactions on Affective Computing</em>, <em>15</em>(3),
1800–1811. (<a
href="https://doi.org/10.1109/TAFFC.2024.3378698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The ever-evolving social media discourse has witnessed an overwhelming use of memes to express opinions or dissent. Besides being misused for spreading malcontent, they are mined by corporations and political parties to glean the public&#39;s opinion. Therefore, memes predominantly offer affect-enriched insights towards ascertaining the societal psyche. However, the current approaches are yet to model the affective dimensions expressed in memes effectively. They rely extensively on large multimodal datasets for pre-training and do not generalize well due to constrained visual-linguistic grounding. In this paper, we introduce MOOD (Meme emOtiOns Dataset), which embodies six basic emotions. We then present ALFRED (emotion-Aware muLtimodal Fusion foR Emotion Detection), a novel multimodal neural framework that (i) explicitly models emotion-enriched visual cues, and (ii) employs an efficient cross-modal fusion via a gating mechanism. Our investigation establishes ALFRED &#39;s superiority over existing baselines by 4.94% F1. Additionally, ALFRED competes strongly with previous best approaches on the challenging Memotion task. We then discuss ALFRED &#39;s domain-agnostic generalizability by demonstrating its dominance on two recently-released datasets – HarMeme and Dank Memes , over other baselines. Further, we analyze ALFRED &#39;s interpretability using attention maps. Finally, we highlight the inherent challenges posed by the complex interplay of disparate modality-specific cues toward meme analysis.},
  archive  = {J},
  author   = {Shivam Sharma and Ramaneswaran S and Md. Shad Akhtar and Tanmoy Chakraborty},
  doi      = {10.1109/TAFFC.2024.3378698},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1800-1811},
  title    = {Emotion-aware multimodal fusion for meme emotion detection},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-stage visual perception approach for image emotion
analysis. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(3), 1786–1799. (<a
href="https://doi.org/10.1109/TAFFC.2024.3372090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Most current methods for image emotion analysis suffer from the affective gap, in which features directly extracted from images are supervised by a single emotional label, which may not align with users’ perceived emotions. To effectively address this limitation, this article introduces a novel multi-stage perception approach inspired by the human staged emotion perception process. The proposed approach comprises three perception modules: entity perception, attribute perception, and emotion perception. The entity perception module identifies entities in images, while the attribute perception module captures the attribute content associated with each entity. Finally, the emotion perception module combines entity and attribute information to extract emotion features. Pseudo-labels of entities and attributes are generated through image segmentation and vision-language models to provide auxiliary guidance for network learning. A progressive understanding of entities and attributes allows the network to hierarchically extract semantic-level features for emotion analysis. Comprehensive experiments on image emotion classification, regression, and distribution learning demonstrate the superior performance of our multi-stage perception network.},
  archive  = {J},
  author   = {Jicai Pan and Jinqiao Lu and Shangfei Wang},
  doi      = {10.1109/TAFFC.2024.3372090},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1786-1799},
  title    = {A multi-stage visual perception approach for image emotion analysis},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Can large language models assess personality from
asynchronous video interviews? A comprehensive evaluation of validity,
reliability, fairness, and rating patterns. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(3), 1769–1785. (<a
href="https://doi.org/10.1109/TAFFC.2024.3374875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The advent of Artificial Intelligence (AI) technologies has precipitated the rise of asynchronous video interviews (AVIs) as an alternative to conventional job interviews. These one-way video interviews are conducted online and can be analyzed using AI algorithms to automate and speed up the selection procedure. In particular, the swift advancement of Large Language Models (LLMs) has significantly decreased the cost and technical barrier to developing AI systems for automatic personality and interview performance evaluation. However, the generative and task-unspecific nature of LLMs might pose potential risks and biases when evaluating humans based on their AVI responses. In this study, we conducted a comprehensive evaluation of the validity, reliability, fairness, and rating patterns of two widely-used LLMs, GPT-3.5 and GPT-4, in assessing personality and interview performance from an AVI. We compared the personality and interview performance ratings of the LLMs with the ratings from a task-specific AI model and human annotators using simulated AVI responses of 685 participants. The results show that LLMs can achieve similar or even better zero-shot validity compared with the task-specific AI model when predicting personality traits. The verbal explanations for predicting personality traits generated by LLMs are interpretable by the personality items that are designed according to psychological theories. However, LLMs also suffered from uneven performance across different traits, insufficient test-retest reliability, and the emergence of certain biases. Thus, it is necessary to exercise caution when applying LLMs for human-related application scenarios, especially for significant decisions such as employment.},
  archive  = {J},
  author   = {Tianyi Zhang and Antonis Koutsoumpis and Janneke K. Oostrom and Djurre Holtrop and Sina Ghassemi and Reinout E. de Vries},
  doi      = {10.1109/TAFFC.2024.3374875},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1769-1785},
  title    = {Can large language models assess personality from asynchronous video interviews? a comprehensive evaluation of validity, reliability, fairness, and rating patterns},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analyzing continuous-time and sentence-level annotations for
speech emotion recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 1754–1768. (<a
href="https://doi.org/10.1109/TAFFC.2024.3372380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The emotional content of several databases are annotated with continuous-time (CT) annotations, providing traces with frame-by-frame scores describing the instantaneous value of an emotional attribute. However, having a single score describing the global emotion of a short segment is more convenient for several emotion recognition formulations. A common approach is to derive sentence-level (SL) labels from CT annotations by aggregating the values of the emotional traces across time and annotators. How similar are these aggregated SL labels from labels originally collected at the sentence level? The release of the MSP-Podcast (SL annotations) and MSP-Conversation (CT annotations) corpora provides the resources to explore the validity of aggregating SL labels from CT annotations. There are 2,884 speech segments that belong to both corpora. Using this set, this study (1) compares both types of annotations using statistical metrics, (2) evaluates their inter-evaluator agreements, and (3) explores the effect of these SL labels on speech emotion recognition (SER) tasks. The analysis reveals benefits of using SL labels derived from CT annotations in the estimation of valence. This analysis also provides insights on how the two types of labels differ and how that could affect a model.},
  archive  = {J},
  author   = {Luz Martinez-Lucas and Wei-Cheng Lin and Carlos Busso},
  doi      = {10.1109/TAFFC.2024.3372380},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1754-1768},
  title    = {Analyzing continuous-time and sentence-level annotations for speech emotion recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GDDN: Graph domain disentanglement network for generalizable
EEG emotion recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 1739–1753. (<a
href="https://doi.org/10.1109/TAFFC.2024.3371540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Cross-subject EEG emotion recognition suffers a major setback due to high inter-subject variability in emotional responses. Many prior studies have endeavored to alleviate the inter-subject discrepancies of EEG feature distributions, ignoring the variable EEG connectivity and prediction deviation caused by individual differences, which may cause poor generalization to the unseen subject. This article proposes a graph domain disentanglement network (GDDN) to generalize EEG emotion recognition across subjects in terms of EEG connectivity, representation, and prediction. More specifically, a graph domain disentanglement module is proposed to extract common-specific characteristics on both EEG graph connectivity and graph representation, enabling a more comprehensive network transferability to the unseen individual. Meanwhile, to strengthen stable emotion prediction capability, a domain-adaptive classifier aggregation module is developed to facilitate adaptive emotional prediction for the unseen individual conditioned on the domain weights of the input individuals. Finally, an auxiliary supervision module is imposed to alleviate the domain discrepancy and reduce information loss during the disentanglement learning. Extensive experiments on three public EEG emotion datasets, i.e., SEED, SEED-IV, and MPED, validate the superior generalizability of GDDN compared with the state-of-the-art methods.},
  archive  = {J},
  author   = {Bianna Chen and C. L. Philip Chen and Tong Zhang},
  doi      = {10.1109/TAFFC.2024.3371540},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1739-1753},
  title    = {GDDN: Graph domain disentanglement network for generalizable EEG emotion recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dep-FER: Facial expression recognition in depressed patients
based on voluntary facial expression mimicry. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(3), 1725–1738. (<a
href="https://doi.org/10.1109/TAFFC.2024.3370103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Facial expressions are important nonverbal behaviors that humans use to express their feelings. Clinical research have shown that depressed patients have poor facial expressiveness and mimicry. As a result, we propose a VFEM experiment with seven expressions to explore variations in facial expression features between depressed patients and normal people, including anger, disgust, fear, happiness, neutrality, sadness, and surprise. It has been discovered through VFEM experiments that depressed patients frequently exhibit negative facial expressions. Meanwhile, we propose a depression facial expression recognition (Dep-FER) model in this research. Dep-FER involves three innovative and crucial components: Mask Multi-head Self-Attention (MMSA), facial action unit similarity loss function (AUs Loss), and case-control loss function (CC Loss). MMSA can filter out disturbing samples and force to learn the relationship between different samples. AUs Loss utilizes the similarity between each expression AU and the model output to improve the generalization ability of the model. CC Loss addresses the intrinsic link between the depressed and normal patient categories. Dep-FER achieves excellent performance in VFEM and outperforms existing comparative models.},
  archive  = {J},
  author   = {Jiayu Ye and Yanhong Yu and Yunshao Zheng and Yang Liu and Qingxiang Wang},
  doi      = {10.1109/TAFFC.2024.3370103},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1725-1738},
  title    = {Dep-FER: Facial expression recognition in depressed patients based on voluntary facial expression mimicry},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vesper: A compact and effective pretrained model for speech
emotion recognition. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(3), 1711–1724. (<a
href="https://doi.org/10.1109/TAFFC.2024.3369726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article presents a paradigm that adapts general large-scale pretrained models (PTMs) to speech emotion recognition task. Although PTMs shed new light on artificial general intelligence, they are constructed with general tasks in mind, and thus, their efficacy for specific tasks can be further improved. Additionally, employing PTMs in practical applications can be challenging due to their considerable size. Above limitations spawn another research direction, namely, optimizing large-scale PTMs for specific tasks to generate task-specific PTMs that are both compact and effective. In this paper, we focus on the speech emotion recognition task and propose an impro V ed e motion- s pecific p retrained encod er called Vesper. Vesper is pretrained on a speech dataset based on WavLM and takes into account emotional characteristics. To enhance sensitivity to emotional information, Vesper employs an emotion-guided masking strategy to identify the regions that need masking. Subsequently, Vesper employs hierarchical and cross-layer self-supervision to improve its ability to capture acoustic and semantic representations, both of which are crucial for emotion recognition. Experimental results on the IEMOCAP, MELD, and CREMA-D datasets demonstrate that Vesper with 4 layers outperforms WavLM Base with 12 layers, and the performance of Vesper with 12 layers surpasses that of WavLM Large with 24 layers.},
  archive  = {J},
  author   = {Weidong Chen and Xiaofen Xing and Peihao Chen and Xiangmin Xu},
  doi      = {10.1109/TAFFC.2024.3369726},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1711-1724},
  title    = {Vesper: A compact and effective pretrained model for speech emotion recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An analysis of physiological and psychological responses in
virtual reality and flat screen gaming. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(3), 1696–1710. (<a
href="https://doi.org/10.1109/TAFFC.2024.3368703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent research has focused on the effectiveness of Virtual Reality (VR) in games as a more immersive method of interaction. However, there is a lack of robust analysis of the physiological effects between VR and flatscreen (FS) gaming. This paper introduces the first systematic comparison and analysis of emotional and physiological responses to commercially available games in VR and FS environments. To elicit these responses, we first selected four games through a pilot study of 6 participants to cover all four quadrants of the valence-arousal space. Using these games, we recorded the physiological activity, including Blood Volume Pulse and Electrodermal Activity, and self-reported emotions of 33 participants in a user study. Our data analysis revealed that VR gaming elicited more pronounced emotions, higher arousal, increased cognitive load and stress, and lower dominance than FS gaming. The Virtual Reality and Flat Screen (VRFS) dataset, containing over 15 hours of multimodal data comparing FS and VR gaming across different games, is also made publicly available for research purposes. Our analysis provides valuable insights for further investigations into the physiological and emotional effects of VR and FS gaming.},
  archive  = {J},
  author   = {Ritik Vatsal and Shrivatsa Mishra and Rushil Thareja and Mrinmoy Chakrabarty and Ojaswa Sharma and Jainendra Shukla},
  doi      = {10.1109/TAFFC.2024.3368703},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1696-1710},
  title    = {An analysis of physiological and psychological responses in virtual reality and flat screen gaming},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continuous emotion ambiguity prediction: Modeling with beta
distributions. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(3), 1684–1695. (<a
href="https://doi.org/10.1109/TAFFC.2024.3367371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Conventional continuous emotion prediction systems are typically trained to predict the ‘average’ of affect ratings obtained from multiple human annotators. These systems, however, ignore the ambiguity inherent in the perceived emotions, which is not captured by the ‘average rating’. This paper presents a novel ambiguity-aware continuous emotion prediction system that predicts the time-varying emotion state as a series of beta distributions. Our recent work has shown beta distributions to be an effective parametric model of a collection of affect ratings. This work develops an appropriate cost function that enables neural networks to be trained to predict beta distributions. It also investigates the choice of parameterization of the beta distribution, the choice of activation functions of the output layer, and the tractability of gradient definitions in combination with the loss function. The proposed framework is implemented using a Bag-of-Audio-Words front-end and an LSTM-based back-end and evaluated on the RECOLA dataset. In addition to comparison with baseline systems that only predict the ‘average rating’, the effectiveness with which the predictions represent ambiguity in perceived emotions is also evaluated. Experimental results reveal that the proposed approach outperforms other ambiguity-aware systems, especially when predicting valence.},
  archive  = {J},
  author   = {Deboshree Bose and Vidhyasaharan Sethu and Eliathamby Ambikairajah},
  doi      = {10.1109/TAFFC.2024.3367371},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1684-1695},
  title    = {Continuous emotion ambiguity prediction: Modeling with beta distributions},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Facial action unit detection and intensity estimation from
self-supervised representation. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 1669–1683. (<a
href="https://doi.org/10.1109/TAFFC.2024.3367015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As a fine-grained and local expression behavior measurement, facial action unit (FAU) analysis (e.g., detection and intensity estimation) has been documented for its time-consuming, labor-intensive, and error-prone annotation. Thus a long-standing challenge of FAU analysis arises from the data scarcity of manual annotations, limiting the generalization ability of trained models to a large extent. Amounts of previous works have made efforts to alleviate this issue via semi/weakly supervised methods and extra auxiliary information. However, these methods still require domain knowledge and have not yet avoided the high dependency on data annotation. This article introduces a robust facial representation model MAE-Face for AU analysis. Using masked autoencoding as the self-supervised pre-training approach, MAE-Face first learns a high-capacity model from a feasible collection of face images without additional data annotations. Then after being fine-tuned on AU datasets, MAE-Face exhibits convincing performance for both AU detection and AU intensity estimation, achieving a new state-of-the-art on nearly all the evaluation results. Further investigation shows that MAE-Face achieves decent performance even when fine-tuned on only 1% of the AU training set, strongly proving its robustness and generalization performance. The pre-trained model is available at our GitHub repository.},
  archive  = {J},
  author   = {Bowen Ma and Rudong An and Wei Zhang and Yu Ding and Zeng Zhao and Rongsheng Zhang and Tangjie Lv and Changjie Fan and Zhipeng Hu},
  doi      = {10.1109/TAFFC.2024.3367015},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1669-1683},
  title    = {Facial action unit detection and intensity estimation from self-supervised representation},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-task inconsistency based active learning (CTIAL) for
emotion recognition. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(3), 1659–1668. (<a
href="https://doi.org/10.1109/TAFFC.2024.3366767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion recognition is a critical component of affective computing. Training accurate machine learning models for emotion recognition typically requires a large amount of labeled data. Due to the subtleness and complexity of emotions, multiple evaluators are usually needed for each affective sample to obtain its ground-truth label, which is expensive. To save the labeling cost, this paper proposes an inconsistency-based active learning approach for cross-task transfer between emotion classification and estimation. Affective norms are utilized as prior knowledge to connect the label spaces of categorical and dimensional emotions. Then, the prediction inconsistency on the two tasks for the unlabeled samples is used to guide sample selection in active learning for the target task. Experiments on within-corpus and cross-corpus transfers demonstrated that cross-task inconsistency could be a very valuable metric in active learning. To our knowledge, this is the first work that utilizes prior knowledge on affective norms and data in a different task to facilitate active learning for a new task, even the two tasks are from different datasets.},
  archive  = {J},
  author   = {Yifan Xu and Xue Jiang and Dongrui Wu},
  doi      = {10.1109/TAFFC.2024.3366767},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1659-1668},
  title    = {Cross-task inconsistency based active learning (CTIAL) for emotion recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bodily sensation map vs. Bodily motion map: Visualizing and
analyzing emotional body motions. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 1649–1658. (<a
href="https://doi.org/10.1109/TAFFC.2024.3365895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion detection using features presented in the body has been comparatively understudied compared to other emotional modalities. This study investigated and compared how emotions are revealed through bodily sensations and body movement information. We propose a novel visualization method for addressing body part activation or deactivation associated with different emotions using motion capture data (bodily motion maps; BMMs) and aim to compare its emotional features with existing methods of addressing sensation towards body activation or deactivation using the computerised self-report method (bodily sensation maps; BSMs). A user study is conducted with twenty-nine participants to gather BSMs and BMMs under seven emotional statuses (happy, sad, surprise, angry, disgust, fearful, and neutral). The results indicate that people&#39;s BMMs as well as their BSMs contain shared features that can discriminate between emotions. The asynchronies between bodily sensation and body motion existed. Furthermore, convolutional neural network (CNN)-based automatic emotion recognition outperforms humans’ emotion recognition abilities. The implications of the current study&#39;s method of comparing emotional features between bodily sensation and body motion for future emotion-related studies for mental disorders are discussed.},
  archive  = {J},
  author   = {Myeongul Jung and Youngwug Cho and Jejoong Kim and Hyungsook Kim and Kwanguk Kim},
  doi      = {10.1109/TAFFC.2024.3365895},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1649-1658},
  title    = {Bodily sensation map vs. bodily motion map: Visualizing and analyzing emotional body motions},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Looking into gait for perceiving emotions via bilateral
posture and movement graph convolutional networks. <em>IEEE Transactions
on Affective Computing</em>, <em>15</em>(3), 1634–1648. (<a
href="https://doi.org/10.1109/TAFFC.2024.3365694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotions can be perceived from a person&#39;s gait, i.e., their walking style. Existing methods on gait emotion recognition mainly leverage the posture information as input, but ignore the body movement, which contains complementary information for recognizing emotions evoked in the gait. In this paper, we propose a Bilateral Posture and Movement Graph Convolutional Network (BPM-GCN) that consists of two parallel streams, namely posture stream and movement stream, to recognize emotions from two views. The posture stream aims to explicitly analyse the emotional state of the person. Specifically, we design a novel regression constraint based on the hand-engineered features to distill the prior affective knowledge into the network and boost the representation learning. The movement stream is designed to describe the intensity of the emotion, which is an implicitly cue for recognizing emotions. To achieve this goal, we employ a higher-order velocity-acceleration pair to construct graphs, in which the informative movement features are utilized. Besides, we design a PM-Interacted feature fusion mechanism to adaptively integrate the features from the two streams. Therefore, the two streams collaboratively contribute to the performance from two complementary views. Extensive experiments on the largest benchmark dataset Emotion-Gait show that BPM-GCN performs favorably against the state-of-the-art approaches (with at least 4.59% performance improvement).},
  archive  = {J},
  author   = {Yingjie Zhai and Guoli Jia and Yu-Kun Lai and Jing Zhang and Jufeng Yang and Dacheng Tao},
  doi      = {10.1109/TAFFC.2024.3365694},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1634-1648},
  title    = {Looking into gait for perceiving emotions via bilateral posture and movement graph convolutional networks},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-modal hierarchical empathetic framework for social
robots with affective body control. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 1621–1633. (<a
href="https://doi.org/10.1109/TAFFC.2024.3356511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Social robots require the ability to understand human emotions and provide affective and behavioral responses during human-robot interactions. However, current social robots lack empathy capabilities. In this work, we propose a novel Multi-modal Hierarchical Empathetic (MHE) framework for generating empathetic responses for social robots. MHE is composed of a multi-modal fusion and emotion recognition module, an empathetic dialogue generation module, and an expression generation module. By fusing the sensor signals of different modalities, the robot can recognize human emotions and generate affective responses. Multiple experiments are conducted on a real robot, Pepper, to evaluate the proposed framework. The experiments are conducted to discriminate between MHE-generated text and human responses in complete ignorance, and most experimenters agree that MHE can effectively generate human-like and empathetic responses. To better evaluate the similarity between human-robot and human-human interactions, a period eye movement map (PEM) captured by an eye tracker is proposed. The experimental results demonstrate the improvement in the MHE in human-robot interactions by comparing different PEMs.},
  archive  = {J},
  author   = {Yue Gao and Yangqing Fu and Ming Sun and Feng Gao},
  doi      = {10.1109/TAFFC.2024.3356511},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1621-1633},
  title    = {Multi-modal hierarchical empathetic framework for social robots with affective body control},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Avatar-based feedback in job interview training impacts
action identities and anxiety. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 1608–1620. (<a
href="https://doi.org/10.1109/TAFFC.2024.3363835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This study examined the use of avatars to provide feedback to influence action identities, anxiety, mood, and performance during job interview training. We recruited 36 university students for the experiment and divided them into two groups. The first group received avatar-based feedback whereas the other group received self-feedback after the first interview session. Results showed that the avatar-based feedback group experienced significantly higher levels of action identities, reduced anxiety, and happier mood in the second interview after the feedback session. Additionally, compared to the self-feedback group, the avatar-based feedback group rated their performance better in the second interview. Furthermore, the effect of avatar feedback on mood and performance varied depending on the participants’ initial anxiety levels. For those with low initial anxiety, avatar feedback resulted in higher levels of action identification, a marginally significant increase in positive mood, higher pitch, better word usage, and better self-assessment. In contrast, for those with high initial anxiety, avatar feedback did not cause any significant changes in the action identities or moods, however, it reduced anxiety and the use of weak words. We believe that our findings have implications not only for improving job interview performance but also for designing future communication coaching systems.},
  archive  = {J},
  author   = {Sarinasadat Hosseini and Jingyu Quan and Xiaoqi Deng and Yoshihiro Miyake and Takayuki Nozawa},
  doi      = {10.1109/TAFFC.2024.3363835},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1608-1620},
  title    = {Avatar-based feedback in job interview training impacts action identities and anxiety},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An open-source benchmark of deep learning models for
audio-visual apparent and self-reported personality recognition.
<em>IEEE Transactions on Affective Computing</em>, <em>15</em>(3),
1590–1607. (<a
href="https://doi.org/10.1109/TAFFC.2024.3363710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Personality determines various human daily and working behaviours. Recently, a large number of automatic personality computing approaches have been developed to predict either the apparent or self-reported personality of the subject based on non-verbal audio-visual behaviours. However, most of them suffer from complex and dataset-specific pre-processing steps and model training tricks. In the absence of a standardized benchmark with consistent experimental settings, it is not only impossible to fairly compare the real performances of these personality computing models but also makes them difficult to be reproduced. This paper presents the first reproducible audio-visual benchmark to provide a fair and consistent evaluation of eight existing personality computing models (e.g., audio, visual and audio-visual) and seven standard deep learning models on both self-reported and apparent personality recognition tasks. Building upon a set of benchmarked models, we also investigate the impact of two previously-used long-term modelling strategies for summarising short-term/frame-level predictions on personality computing results. We comprehensively investigate all benchmarked models on two publicly available datasets, ChaLearn First Impression and UDIVA self-reported personality datasets, and conclude: (i) apparent personality traits, inferred from facial behaviours by most benchmarked deep learning models, show more reliability than self-reported ones; (ii) visual models frequently achieved superior performances than audio models on personality recognition; (iii) non-verbal behaviours contribute differently in predicting different personality traits; and (iv) our reproduced personality computing models generally achieved worse performances than their original reported results.},
  archive  = {J},
  author   = {Rongfan Liao and Siyang Song and Hatice Gunes},
  doi      = {10.1109/TAFFC.2024.3363710},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1590-1607},
  title    = {An open-source benchmark of deep learning models for audio-visual apparent and self-reported personality recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Novel VR-based biofeedback systems: A comparison between
heart rate variability- and electrodermal activity-driven approaches.
<em>IEEE Transactions on Affective Computing</em>, <em>15</em>(3),
1580–1589. (<a
href="https://doi.org/10.1109/TAFFC.2024.3352424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Anxiety symptoms are important contributors to the global health-related burden. Low-intensity interventions have been proposed to reduce anxiety symptoms in the population. Among these, biofeedback (BF) offers an effective approach to reducing anxiety. In the present study, BF was integrated into a novel virtual reality (VR) architecture to enhance BF&#39;s effectiveness to 1) evaluate the feasibility of a VR-based single-session BF in teaching participants to self-regulate; 2) compare the BF aiming at reducing sympathetic (measured though the tonic level of skin conductance, SCL) versus increasing cardiac vagal (i.e., normalized high frequency of heart rate variability, HFnu-HRV) activation, and 3) evaluate which of the two VR-BF single-sessions was most effective in reducing perceived state anxiety. 20 healthy participants underwent both SCL- and HFnu-based in a single session VR-BF. Results showed the feasibility of a short single-session VR-BF and the effectiveness of both VR-BF sessions in reducing perceived state anxiety. Moreover, SCL-based VR-BF determined a significant reduction in sympathetic activation and in sympathovagal balance as well as a greater reduction in perceived state anxiety compared to HFnu-based VR-BF. SCL-based VR-BF represents a safe and effective intervention in reducing anxiety while enhancing adaptive psychophysiological activation.},
  archive  = {J},
  author   = {Andrea Baldini and Elisabetta Patron and Claudio Gentili and Enzo Pasquale Scilingo and Alberto Greco},
  doi      = {10.1109/TAFFC.2024.3352424},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1580-1589},
  title    = {Novel VR-based biofeedback systems: A comparison between heart rate variability- and electrodermal activity-driven approaches},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Emotion recognition in conversation based on a dynamic
complementary graph convolutional network. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(3), 1567–1579. (<a
href="https://doi.org/10.1109/TAFFC.2024.3360979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion recognition in conversation (ERC) is a widely used technology in both affective dialogue bots and dialogue recommendation scenarios, where motivating a system to correctly recognize human emotions is crucial. Uncovering as much contextual information as possible with a limited amount of dialogue information is essential for eventually identifying the correct emotion of each sentence. The integration of contextual information using the existing approaches often results in inadequate access to information or information redundancy. Deeply integrating the different knowledge behind utterances is also difficult. Therefore, to address these problems, we propose a dynamic complementary graph convolutional network (DCGCN) for conversational emotion recognition. Our approach uses commonsense knowledge to complement the contextual information contained in utterances and enrich the extracted conversation information. We creatively propose the concept of utterance density to prevent redundancy and the loss of utterance information in context-dependent contextual information modeling cases. An utterance dependency structure is dynamically determined by the utterance density, and the contextual information is fully integrated into each sentence representation. We evaluate our proposed model in extensive experiments conducted on four public benchmark datasets that are commonly used for ERC. The results demonstrate the effectiveness of the DCGCN, which achieves competitive results in terms of well-known evaluation metrics.},
  archive  = {J},
  author   = {Zhenyu Yang and Xiaoyang Li and Yuhu Cheng and Tong Zhang and Xuesong Wang},
  doi      = {10.1109/TAFFC.2024.3360979},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1567-1579},
  title    = {Emotion recognition in conversation based on a dynamic complementary graph convolutional network},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-level alignment and cross-modal unified semantic
graph refinement network for conversational emotion recognition.
<em>IEEE Transactions on Affective Computing</em>, <em>15</em>(3),
1553–1566. (<a
href="https://doi.org/10.1109/TAFFC.2024.3354382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion recognition in conversation (ERC) based on multiple modalities has attracted enormous attention. However, most research simply concatenated multimodal representations, generally neglecting the impact of cross-modal correspondences and uncertain factors, and leading to the cross-modal misalignment problems. Furthermore, recent methods only considered simple contextual features, commonly ignoring semantic clues and resulting in an insufficient capture of the semantic consistency. To address these limitations, we propose a novel multi-level alignment and cross-modal unified semantic graph refinement network (MA-CMU-SGRNet) for ERC task. Specifically, a multi-level alignment (MA) is first designed to bridge the gap between acoustic and lexical modalities, which can effectively contrast both the instance-level and prototype-level relationships, separating the multimodal features in the latent space. Second, a cross-modal uncertainty-aware unification (CMU) is adopted to generate a unified representation in joint space considering the ambiguity of emotion. Finally, a dual-encoding semantic graph refinement network (SGRNet) is investigated, which includes a syntactic encoder to aggregate information from near neighbors and a semantic encoder to focus on useful semantically close neighbors. Extensive experiments on three multimodal public datasets show the effectiveness of our proposed method compared with the state-of-the-art methods, indicating its potential application in conversational emotion recognition.},
  archive  = {J},
  author   = {Xiaoheng Zhang and Weigang Cui and Bin Hu and Yang Li},
  doi      = {10.1109/TAFFC.2024.3354382},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1553-1566},
  title    = {A multi-level alignment and cross-modal unified semantic graph refinement network for conversational emotion recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning with rater-expanded label space to improve speech
emotion recognition. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(3), 1539–1552. (<a
href="https://doi.org/10.1109/TAFFC.2024.3360428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Automatic sensing of emotional information in speech is important for numerous everyday applications. Conventional Speech Emotion Recognition (SER) models rely on averaging or consensus of human annotations for training, but emotions and raters’ interpretations are subjective in nature, leading to diverse variations in perceptions. To address this, our proposed approach integrates the rater&#39;s subjectivity by forming the Perception-Coherent Clusters (PCC) of raters to be used to derive expanded label space for learning to improve SER. We evaluate our method on the IEMOCAP and the MSP-Podcast corpora, considering scenarios of fixed and variable raters, respectively. The proposed architecture, Rater Perception Coherency (RPC)-based SER surpasses single-task models with consensus labels by achieving UAR improvements of 3.39% for the IEMOCAP and 2.03% for the MSP-Podcast. Further analysis provides comprehensive insights into the contributions of these perception consistency clusters in SER learning.},
  archive  = {J},
  author   = {Shreya G. Upadhyay and Woan-Shiuan Chien and Bo-Hao Su and Chi-Chun Lee},
  doi      = {10.1109/TAFFC.2024.3360428},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1539-1552},
  title    = {Learning with rater-expanded label space to improve speech emotion recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling the interplay between cohesion dimensions: A
challenge for group affective emergent states. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(3), 1526–1538. (<a
href="https://doi.org/10.1109/TAFFC.2024.3349910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emergent states are temporal group phenomena that arise from collective affective, behavioral, and cognitive processes shared among the group&#39;s members during their interactions. Cohesion is one such state, mainly conceptualized by scholars as affective in nature, and frequently distinguished into the two dimensions social and task cohesion. Whereas social cohesion is related to the need of belonging to a group, task cohesion is related to the group&#39;s goals and tasks. In this article, we emphasize the importance of behavioral interaction dynamics to predict cohesion&#39;s dynamics. Drawing from Social Science insights, we investigate the interplay between social and task cohesion to predict their dynamics across group tasks from nonverbal behavioral features. Three computational architectures exploiting transfer learning are presented. Transfer learning capitalizes on information learnt by a model for a specific dimension to predict the dynamics of the other dimension. Results show that integrating the influence of social cohesion to predict dynamics of task cohesion outperforms state-of-the-art. To predict dynamics of social cohesion, a model integrating the reciprocal impact of social and task cohesion significantly improves performance with respect to the state-of-the-art and a model only integrating the impact of task cohesion on dynamics of social cohesion.},
  archive  = {J},
  author   = {Lucien Maman and Nale Lehmann-Willenbrock and Mohamed Chetouani and Laurence Likforman-Sulem and Giovanna Varni},
  doi      = {10.1109/TAFFC.2024.3349910},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1526-1538},
  title    = {Modeling the interplay between cohesion dimensions: A challenge for group affective emergent states},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring retrospective annotation in long-videos for
emotion recognition. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(3), 1514–1525. (<a
href="https://doi.org/10.1109/TAFFC.2024.3359706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion recognition systems are typically trained to classify a given psychophysiological state into emotion categories. Current platforms for emotion ground-truth collection show limitations for real-world scenarios of long-duration content (e.g. $&amp;gt; $ 10 minutes), namely: 1) Real-time annotation tools are distracting and become exhausting; 2) Perform retrospective annotation of the whole content in bulk (providing highly coarse annotations); or 3) Are used by external experts (depending on the number of annotators and their subjective experience). We explore a novel approach, the EmotiphAI Annotator, that allows undisturbed content visualisation and simplifies the annotation process by using segmentation algorithms that select brief clips for emotional annotation retrospectively. We compare three methods for content segmentation based on physiological data (Electrodermal Activity (EDA), emotion-based), scene (time-based), and random (control) selection. The EmotiphAI Annotator attained a B+ System Usability Scale score and low-average mental workload as per the NASA Task Load Index (40%). The reliability of the self-report was analysed by the inter-rater agreement (STD $&amp;lt; $ 0.75), coherence across time segmentation methods (STD $&amp;lt; $ 0.17), comparison against the state-of-the-art ground truth (STD $&amp;lt; $ 0.7), and correlation to EDA ( $&amp;gt; $ 0.3 to 0.8), where the EDA-based method obtained the overall best performance.},
  archive  = {J},
  author   = {Patrícia Bota and Pablo Cesar and Ana Fred and Hugo Plácido da Silva},
  doi      = {10.1109/TAFFC.2024.3359706},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1514-1525},
  title    = {Exploring retrospective annotation in long-videos for emotion recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CFDA-CSF: A multi-modal domain adaptation method for
cross-subject emotion recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 1502–1513. (<a
href="https://doi.org/10.1109/TAFFC.2024.3357656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multi-modal classifiers for emotion recognition have become prominent, as the emotional states of subjects can be more comprehensively inferred from Electroencephalogram (EEG) signals and eye movements. However, existing classifiers experience a decrease in performance due to the distribution shift when applied to new users. Unsupervised domain adaptation (UDA) emerges as a solution to address the distribution shift between subjects by learning a shared latent feature space. Nevertheless, most UDA approaches focus on a single modality, while existing multi-modal approaches do not consider that fine-grained structures should also be explicitly aligned and the learned feature space must be discriminative. In this paper, we propose Coarse and Fine-grained Distribution Alignment with Correlated and Separable Features (CFDA-CSF), which performs a coarse alignment over the global feature space, and a fine-grained alignment between modalities from each domain distribution. At the same time, the model learns intra-domain correlated features, while a separable feature space is encouraged on new subjects. We conduct an extensive experimental study across the available sessions on three public datasets for multi-modal emotion recognition: SEED, SEED-IV, and SEED-V. Our proposal effectively improves the recognition performance in every session, achieving an average accuracy of 93.05%, 85.87% and 91.20% for SEED; 85.72%, 89.60%, and 86.88% for SEED-IV; and 88.49%, 91.37% and 91.57% for SEED-V.},
  archive  = {J},
  author   = {Magdiel Jiménez-Guarneros and Gibran Fuentes-Pineda},
  doi      = {10.1109/TAFFC.2024.3357656},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1502-1513},
  title    = {CFDA-CSF: A multi-modal domain adaptation method for cross-subject emotion recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Show me how you use your mouse and i tell you how you feel?
Sensing affect with the computer mouse. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(3), 1490–1501. (<a
href="https://doi.org/10.1109/TAFFC.2024.3357733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Computer mouse tracking is a simple and cost-efficient way to gather continuous behavioral data. As theory suggests a relationship between affect and sensorimotor processes, the computer mouse might be usable for affect sensing. However, the processes underlying a connection between mouse usage and affect are complex, hitherto empirical evidence is ambiguous, and the research area lacks longitudinal studies. The present work brings forward a longitudinal field study in which 179 participants hourly self-reported their affect while their mouse usage was tracked both during their self-directed, contextless as well as task-bound computer use over the course of 14 days, resulting in a dataset comprising 10,760 instances of data collection. Extensive statistical analysis using null hypothesis significance testing and machine learning reveal weak and sporadic relationships between mouse usage and longitudinal self-reported affect at best. The results of this study challenge the use of computer mouse tracking for longitudinal affect sensing and point to a necessity for more research.},
  archive  = {J},
  author   = {Paul Freihaut and Anja S. Göritz},
  doi      = {10.1109/TAFFC.2024.3357733},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1490-1501},
  title    = {Show me how you use your mouse and i tell you how you feel? sensing affect with the computer mouse},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How virtual reality therapy affects refugees from ukraine -
acute stress reduction pilot study. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 1475–1489. (<a
href="https://doi.org/10.1109/TAFFC.2024.3352401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article extends and builds upon our previous research concerning Virtual Reality (VR) with bilateral stimulation as an automated stress-reduction therapy tool. The study coincided with Russia&#39;s invasion of Ukraine, thus the software was tailored to reduce the stress of war refugees. We created a 28 minutes relaxation training program in a virtual, relaxing environment in the form of a cozy apartment in the mountains. An integral part of this tool is a set of sensors, which collects and records objective physiological measures to evaluate the system&#39;s effectiveness. A pilot treatment programme, incorporating the VR system mentioned above was carried out on the experimental group of 55 Ukrainian refugees who participated in up to five relaxation training sessions. Before starting the session, baseline features such as subjectively perceived stress, mood, galvanic skin response, and EEG were registered. The monitoring of physiological indicators was continued during the training session. Before and after the session, volunteers were asked to fill in questionnaires regarding their current stress level and mood. The obtained results were analyzed in terms of variability over time: before, during, and after the session.},
  archive  = {J},
  author   = {Dorota Kamińska and Grzegorz Zwoliński and Dorota Merecz-Kot},
  doi      = {10.1109/TAFFC.2024.3352401},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1475-1489},
  title    = {How virtual reality therapy affects refugees from ukraine - acute stress reduction pilot study},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anthropomorphism and affective perception: Dimensions,
measurements, and interdependencies in aerial robotics. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(3), 1463–1474. (<a
href="https://doi.org/10.1109/TAFFC.2024.3349858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Assigning lifelike qualities to robotic agents (Anthropomorphism) is associated with complex affective interpretations of their behavior. These anthropomorphized perceptions are traditionally elicited through robots’ designs. Yet, aerial robots (or drones) present a special case due to their – traditionally – non-anthropomorphic design, and prior research shows conflicting evidence on their perception as either person-like, animal-like, or machine-like. In this article, we explore how people perceive drones in a cross-dimensional space between these three dimensions by varying the affective state presented on the drone. To capture these perceptions, we developed a novel measurement instrument AnZoMa . We describe the design, use, and deployment of the instrument in an online study ( $N$ =98). The study results suggest that different drone emotions triggered people to attribute various characteristics to the drone (e.g., interaction metaphors, traits, and features) and variations in acceptability of drone affective states. These results demonstrate the interdependencies between affective perceptions and anthropomorphism of drones. We conclude by discussing the necessity to integrate cross-dimensional perception of anthropomorphism in human-drone interaction and affective computing. This work contributes a novel tool to measure the dimensions and gravity of anthropomorphism and insights into interdependencies between different affective states displayed on drones and their anthropomorphized perception.},
  archive  = {J},
  author   = {Viviane Herdel and Anastasia Kuzminykh and Yisrael Parmet and Jessica R. Cauchard},
  doi      = {10.1109/TAFFC.2024.3349858},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1463-1474},
  title    = {Anthropomorphism and affective perception: Dimensions, measurements, and interdependencies in aerial robotics},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gusa: Graph-based unsupervised subdomain adaptation for
cross-subject EEG emotion recognition. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(3), 1451–1462. (<a
href="https://doi.org/10.1109/TAFFC.2024.3349770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {EEG emotion recognition has been hampered by the clear individual differences in the electroencephalogram (EEG). Nowadays, domain adaptation is a good way to deal with this issue because it aligns the distribution of data across subjects. However, the performance for EEG emotion recognition is limited by the existing research, which mainly focuses on the global alignment between the source domain and the target domain and ignores much fine-grained information. In this study, we propose a method called Graph-based Unsupervised Subdomain Adaptation (Gusa), which simultaneously aligns the distribution between the source and target domains in a fine-grained way from both the channel and emotion subdomains. Gusa employs three modules, such as the Node-wise Domain Constraints Module to align each EEG channel and obtain a domain-variant representation, the Class-level Distribution Constraints Module, and the Emotion-wise Domain Constraints Module, to collect more fine-grained information, create more discriminative representations for each emotion, and lessen the impact of noisy emotion labels. The studies on the SEED, SEED-IV, and MPED datasets demonstrate that Gusa significantly improves the ability of EEG to recognize emotions and can extract more granular and discriminative representations for EEG.},
  archive  = {J},
  author   = {Xiaojun Li and C. L. Philip Chen and Bianna Chen and Tong Zhang},
  doi      = {10.1109/TAFFC.2024.3349770},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1451-1462},
  title    = {Gusa: Graph-based unsupervised subdomain adaptation for cross-subject EEG emotion recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MASANet: Multi-aspect semantic auxiliary network for visual
sentiment analysis. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(3), 1439–1450. (<a
href="https://doi.org/10.1109/TAFFC.2023.3331776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, multi-modal affective computing has demonstrated that introducing multi-modal information can enhance performance. However, multi-modal research faces significant challenges due to its high requirements regarding data acquisition, modal integrity, and feature alignment. The widespread use of multi-modal pre-training methods offers the possibility of aiding visual sentiment analysis by introducing cross-domain knowledge. This paper proposes a Multi-Aspect Semantic Auxiliary Network (MASANet) for visual sentiment analysis. Specifically, MASANet achieves modality expansion through cross-modal generation, making it possible to introduce cross-domain semantic assistance. Then, a cross-modal gating module and an adaptive modal fusion module are proposed for aspect-level and cross-modal interaction, respectively. In addition, a designed semantic polarity constraint loss is presented to improve sentiment multi-classification performance. Evaluations of eight widely-used affective image datasets demonstrate that our proposed method outperforms the state-of-the-art methods. Further ablation experiments and visualization results also confirm the effectiveness of the proposed method and its modules.},
  archive  = {J},
  author   = {Jinglun Cen and Chunmei Qing and Haochun Ou and Xiangmin Xu and Junpeng Tan},
  doi      = {10.1109/TAFFC.2023.3331776},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1439-1450},
  title    = {MASANet: Multi-aspect semantic auxiliary network for visual sentiment analysis},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interaction between dynamic affection and arithmetic
cognitive ability: A practical investigation with EEG measurement.
<em>IEEE Transactions on Affective Computing</em>, <em>15</em>(3),
1427–1438. (<a
href="https://doi.org/10.1109/TAFFC.2023.3347391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotions play an essential role in affecting the performance of cognitive abilities in continuous cognitive tasks. Most previous studies share a common issue in that the evoked emotions are simply presumed to be real emotions, without taking into account the observation that emotions may be changed when carrying out cognitive activities. This may lead to the inaccurate detection of true emotions, which further adversely affects the investigation of interactions between emotion and cognition. To address this challenging problem, the present work develops an innovative study using EEG measurement to investigate the interaction between dynamic affection and cognitive ability. In particular, a real-time emotion detection model by the use of physiological signals (i.e., EEG) is constructed, to dynamically monitor the current emotional state. Given the observed emotion, the analysis of the interaction between cognitive abilities and dynamic emotions is undertaken from the perspectives of both behavioral performance and brain mechanisms. Research outcomes indicate that emotions are not stable, and are indeed dynamically changed by cognitive performance. Meanwhile, cognitive activities also influence the brain activation pattern revealed under different emotions, which validates the necessity of introducing the dynamic emotion monitoring model. In addition, the best performance has been found when the emotional state is neutral in terms of accuracy and response time. The results of this study provide a potential basis for assessing the cognitive abilities of individuals with different emotions in a variety of applications of cognitive scenarios.},
  archive  = {J},
  author   = {Xiaonan Yang and Yilu Peng and Yuyang Han and Fangyi Li and Qin Zhang and Shuo Wu and Xia Wu},
  doi      = {10.1109/TAFFC.2023.3347391},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1427-1438},
  title    = {Interaction between dynamic affection and arithmetic cognitive ability: A practical investigation with EEG measurement},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research on the association mechanism and evaluation model
between fNIRS data and aesthetic quality in product aesthetic quality
evaluation. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(3), 1414–1426. (<a
href="https://doi.org/10.1109/TAFFC.2023.3344189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Aesthetic quality evaluation has been an important research question in the field of user experience in product design. However, the feasibility and accuracy of using fNIRS data for product aesthetic quality evaluation are unknown. In this article, we analyze the correlation and association between fNIRS data and aesthetic quality and designed a product aesthetic quality evaluation model to answer this question. We find that HBO 2 data in the prefrontal (S19-D11), frontal (S4-D3), temporal (S3-D1), and parietal (S8-D8) regions of the brain have significant correlations and logistic relationships with high visual product aesthetic quality, whereas HBO 2 data in the prefrontal (S19-D11) and parietal (S8-D8) regions of the brain have significant correlations and association relationships. These data can be used for products aesthetic quality evaluation. Importantly, the overall prediction accuracy of the model to evaluate products’ aesthetic quality is 84.1%. The model is therefore able to better distinguish and evaluate the aesthetic quality of products. This study demonstrates the feasibility of using fNIRS data to evaluate the aesthetic quality of products and shows that the product aesthetic quality evaluation model can provide an objective and accurate decision-making reference to help designers evaluate and improve the aesthetic quality of products.},
  archive  = {J},
  author   = {Yong Wang and Fanghao Song and Yan Liu and Yaying Li and Weihao Wang and Qiqi Huang and Yang Hu},
  doi      = {10.1109/TAFFC.2023.3344189},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1414-1426},
  title    = {Research on the association mechanism and evaluation model between fNIRS data and aesthetic quality in product aesthetic quality evaluation},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continuously controllable facial expression editing in
talking face videos. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(3), 1400–1413. (<a
href="https://doi.org/10.1109/TAFFC.2023.3334511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently audio-driven talking face video generation has attracted considerable attention. However, very few researches address the issue of emotional editing of these talking face videos with continuously controllable expressions, which is a strong demand in the industry. The challenge is that speech-related expressions and emotion-related expressions are often highly coupled. Meanwhile, traditional image-to-image translation methods cannot work well in our application due to the coupling of expressions with other attributes such as poses, i.e., translating the expression of the character in each frame may simultaneously change the head pose due to the bias of the training data distribution. In this paper, we propose a high-quality facial expression editing method for talking face videos, allowing the user to control the target emotion in the edited video continuously. We present a new perspective for this task as a special case of motion information editing, where we use a 3DMM to capture major facial movements and an associated texture map modeled by a StyleGAN to capture appearance details. Both representations (3DMM and texture map) contain emotional information and can be continuously modified by neural networks and easily smoothed by averaging in coefficient/latent spaces, making our method simple yet effective. We also introduce a mouth shape preservation loss to control the trade-off between lip synchronization and the degree of exaggeration of the edited expression. Extensive experiments and a user study show that our method achieves state-of-the-art performance across various evaluation criteria.},
  archive  = {J},
  author   = {Zhiyao Sun and Yu-Hui Wen and Tian Lv and Yanan Sun and Ziyang Zhang and Yaoyuan Wang and Yong-Jin Liu},
  doi      = {10.1109/TAFFC.2023.3334511},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1400-1413},
  title    = {Continuously controllable facial expression editing in talking face videos},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A classification framework for depressive episode using r-r
intervals from smartwatch. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 1387–1399. (<a
href="https://doi.org/10.1109/TAFFC.2023.3343463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Depressive episode is key symptom collection of mood disorders. Early intervention can prevent it from happening or reduce its impact, and close monitoring can greatly improve medical management. However, most current monitoring methods are ex post facto, coarse in time granularity and resource consuming. In this article, we aimed to develop a cost-friendly and high usability depressive episode detection framework. In Phase I, we fitted instantaneous affective state models by using R-R intervals collected with photoplethysmogram sensors in smartwatches from laboratory experiments of 1107 participants. In Phase II we utilized the models from Phase I to record long-term affective experience of 2192 participants. Depressive episode models were fitted with affective experience time series. The best instantaneous affective states models achieved overall accuracies of 91% with 2 classes (neutral/ aroused) and 82% with 3 classes (joy/ neutral/ sadness), and the depressive episode models (less severe/ more severe) achieved an overall accuracy of 76% and a best accuracy of 88%. We investigated and discussed the performance differences of the models with multiple settings. We found person-based feature normalization is effective in improving model performance for subjective affect experience. We also found identification of diurnal mood variation may be critical in depressive episode detection.},
  archive  = {J},
  author   = {Fenghua Li and Guoxiong Liu and Zhiling Zou and Yang Yan and Xin Huang and Xuanang Liu and Zhengkui Liu},
  doi      = {10.1109/TAFFC.2023.3343463},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1387-1399},
  title    = {A classification framework for depressive episode using R-R intervals from smartwatch},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DFME: A new benchmark for dynamic facial micro-expression
recognition. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(3), 1371–1386. (<a
href="https://doi.org/10.1109/TAFFC.2023.3341918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {One of the most important subconscious reactions, micro-expression (ME), is a spontaneous, subtle, and transient facial expression that reveals human beings’ genuine emotion. Therefore, automatically recognizing ME (MER) is becoming increasingly crucial in the field of affective computing, providing essential technical support for lie detection, clinical psychological diagnosis, and public safety. However, the ME data scarcity has severely hindered the development of advanced data-driven MER models. Despite the recent efforts by several spontaneous ME databases to alleviate this problem, there is still a lack of sufficient data. Hence, in this paper, we overcome the ME data scarcity problem by collecting and annotating a dynamic spontaneous ME database with the largest current ME data scale called DFME (Dynamic Facial Micro-expressions). Specifically, the DFME database contains 7,526 well-labeled ME videos spanning multiple high frame rates, elicited by 671 participants and annotated by more than 20 professional annotators over three years. Furthermore, we comprehensively verify the created DFME, including using influential spatiotemporal video feature learning models and MER models as baselines, and conduct emotion classification and ME action unit classification experiments. The experimental results demonstrate that the DFME database can facilitate research in automatic MER, and provide a new benchmark for this field.},
  archive  = {J},
  author   = {Sirui Zhao and Huaying Tang and Xinglong Mao and Shifeng Liu and Yiming Zhang and Hao Wang and Tong Xu and Enhong Chen},
  doi      = {10.1109/TAFFC.2023.3341918},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1371-1386},
  title    = {DFME: A new benchmark for dynamic facial micro-expression recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic confidence-aware multi-modal emotion recognition.
<em>IEEE Transactions on Affective Computing</em>, <em>15</em>(3),
1358–1370. (<a
href="https://doi.org/10.1109/TAFFC.2023.3340924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multi-modal emotion recognition has attracted increasing attention in human-computer interaction, as it extracts complementary information from physiological and behavioral features. Compared to single modal approaches, multi-modal fusion methods are more susceptible to uncertainty in emotion recognition, such as heterogeneity and inconsistent predictions across different modalities. Previous multi-modal approaches ignore systematic modeling of uncertainty in fusion and revelation of dynamic variations in emotion process. In this article, we propose a dynamic confidence-aware fusion network for robust recognition of heterogeneous emotion features, including electroencephalogram (EEG) and facial expression. First, we develop a self-attention based multi-channel LSTM network to preliminarily align the heterogeneous emotion features. Second, we propose a confidence regression network to estimate true class probability (TCP) on each modality, which helps explore the uncertainty at modality level. Then, different modalities are weighted fused according to above two types of uncertainty. Finally, we adopt self-paced learning (SPL) mechanism to further improve the model robustness by alleviating negative effect from the hard learning samples. The experimental results on several multi-modal emotion datasets demonstrate the proposed method outperforms the state-of-the-art methods in emotion recognition performance and explicitly reveals the dynamic variation of emotion with uncertainty estimation.},
  archive  = {J},
  author   = {Qi Zhu and Chuhang Zheng and Zheng Zhang and Wei Shao and Daoqiang Zhang},
  doi      = {10.1109/TAFFC.2023.3340924},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1358-1370},
  title    = {Dynamic confidence-aware multi-modal emotion recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Geometric graph representation with learnable graph
structure and adaptive AU constraint for micro-expression recognition.
<em>IEEE Transactions on Affective Computing</em>, <em>15</em>(3),
1343–1357. (<a
href="https://doi.org/10.1109/TAFFC.2023.3340016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Micro-expression recognition (MER) holds significance in uncovering hidden emotions. Most works take image sequences as input and cannot effectively explore ME information because subtle ME-related motions are easily submerged in unrelated information. Instead, the facial landmark is a low-dimensional and compact modality, which achieves lower computational cost and potentially concentrates on ME-related movement features. However, the discriminability of facial landmarks for MER is unclear. Thus, this article investigates the contribution of facial landmarks and proposes a novel framework to efficiently recognize MEs with facial landmarks. First, a geometric two-stream graph network is constructed to aggregate the low-order and high-order geometric movement information from facial landmarks to obtain discriminative ME representation. Second, a self-learning fashion is introduced to automatically model the dynamic relationship between nodes even long-distance nodes. Furthermore, an adaptive action unit loss is proposed to reasonably build a strong correlation between landmarks, facial action units and MEs. Notably, this work provides a novel idea with much higher efficiency to promote MER, only utilizing graph-based geometric features. The experimental results demonstrate that the proposed method achieves competitive performance with a significantly reduced computational cost. Furthermore, facial landmarks significantly contribute to MER and are worth further study for high-efficient ME analysis.},
  archive  = {J},
  author   = {Jinsheng Wei and Wei Peng and Guanming Lu and Yante Li and Jingjie Yan and Guoying Zhao},
  doi      = {10.1109/TAFFC.2023.3340016},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1343-1357},
  title    = {Geometric graph representation with learnable graph structure and adaptive AU constraint for micro-expression recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Olfactory-enhanced VR: What’s the difference in brain
activation compared to traditional VR for emotion induction? <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(3), 1331–1342. (<a
href="https://doi.org/10.1109/TAFFC.2023.3337745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Olfactory-enhanced virtual reality (OVR) creates a complex and rich emotional experience, thus promoting a new generation of human-computer interaction experiences in real-world scenarios. However, with the rise of virtual reality (VR) as a mood induction procedure (MIP), few studies have incorporated olfactory stimuli into emotion induction in three-dimensional (3D) environments. Considering the differences in electroencephalography (EEG) dynamics between sensory stimuli, all previous two-dimensional (2D) and 3D emotional studies have been less effective in reality because they only use visual and audio senses. To overcome these limitations, we developed a novel EEG signal dataset based on OVR. We systematically analyzed the influence of olfactory stimuli on emotion induction in a VR environment from a neurophysiological perspective. Specifically, synchronous EEG signals were collected from 65 participants as they watched positive and negative videos in traditional VR and OVR. Their power spectral densities (PSDs) were then calculated to compare the differences in brain activation between their VR and OVR modes during the induction of positive and negative emotions, while their brain states were classified after feature selection. The results showed that olfactory stimuli enhanced EEG responses for positive emotions, but the opposite was true for negative emotions. Additionally, the recognition rate of brain emotional states was more than 90% under both positive and negative emotions, while the high-frequency β and γ bands could effectively distinguish VR and OVR modes. This study introduced the olfaction into the field of human-computer interaction, which could promote research on emotion induction and recognition in real-world environments.},
  archive  = {J},
  author   = {Xinyue Zhong and Wanqing Liu and Jialan Xie and Yun Gu and Guangyuan Liu},
  doi      = {10.1109/TAFFC.2023.3337745},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1331-1342},
  title    = {Olfactory-enhanced VR: What&#39;s the difference in brain activation compared to traditional VR for emotion induction?},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Emotion recognition from few-channel EEG signals by
integrating deep feature aggregation and transfer learning. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(3), 1315–1330. (<a
href="https://doi.org/10.1109/TAFFC.2023.3336531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Electroencephalogram (EEG) signals have been widely studied in human emotion recognition. The majority of existing EEG emotion recognition algorithms utilize dozens or hundreds of electrodes covering the whole scalp region (denoted as full-channel EEG devices in this paper). Nowadays, more and more portable and miniature EEG devices with only a few electrodes (denoted as few-channel EEG devices in this paper) are emerging. However, emotion recognition from few-channel EEG data is challenging because the device can only capture EEG signals from a portion of the brain area. Moreover, existing full-channel algorithms cannot be directly adapted to few-channel EEG signals due to the significant inter-variation between full-channel and few-channel EEG devices. To address these challenges, we propose a novel few-channel EEG emotion recognition framework from the perspective of knowledge transfer. We leverage full-channel EEG signals to provide supplementary information, available online, for few-channel signals via a transfer learning-based model CD-EmotionNet , which consists of a base emotion model for efficient emotional feature extraction and a cross-device transfer learning strategy. This strategy helps to enhance emotion recognition performance on few-channel EEG data by utilizing knowledge learned from full-channel EEG data. To evaluate our cross-device EEG emotion transfer learning framework, we construct an emotion dataset containing paired 18-channel and 5-channel EEG signals from 25 subjects, as well as 5-channel EEG signals from 13 other subjects. Extensive experiments show that our framework outperforms state-of-the-art EEG emotion recognition methods by a large margin.},
  archive  = {J},
  author   = {Fang Liu and Pei Yang and Yezhi Shu and Niqi Liu and Jenny Sheng and Junwen Luo and Xiaoan Wang and Yong-Jin Liu},
  doi      = {10.1109/TAFFC.2023.3336531},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1315-1330},
  title    = {Emotion recognition from few-channel EEG signals by integrating deep feature aggregation and transfer learning},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Movement representation learning for pain level
classification. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(3), 1303–1314. (<a
href="https://doi.org/10.1109/TAFFC.2023.3334522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Self-supervised learning has shown value for uncovering informative movement features for human activity recognition. However, there has been minimal exploration of this approach for affect recognition where availability of large labelled datasets is particularly limited. In this paper, we propose a P-STEMR (Parallel Space-Time Encoding Movement Representation) architecture with the aim of addressing this gap and specifically leveraging the higher availability of human activity recognition datasets for pain-level classification. We evaluated and analyzed the architecture using three different datasets across four sets of experiments. We found statistically significant increase in average F1 score to 0.84 for pain level classification with two classes based on the architecture compared with the use of hand-crafted features. This suggests that it is capable of learning movement representations and transferring these from activity recognition based on data captured in lab settings to classification of pain levels with messier real-world data. We further found that the efficacy of transfer between datasets can be undermined by dissimilarities in population groups due to impairments that affect movement behaviour and in motion primitives (e.g. rotation versus flexion). Future work should investigate how the effect of these differences could be minimized so that data from healthy people can be more valuable for transfer learning.},
  archive  = {J},
  author   = {Temitayo Olugbade and Amanda C de C Williams and Nicolas Gold and Nadia Bianchi-Berthouze},
  doi      = {10.1109/TAFFC.2023.3334522},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1303-1314},
  title    = {Movement representation learning for pain level classification},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Emotion dictionary learning with modality attentions for
mixed emotion exploration. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 1289–1302. (<a
href="https://doi.org/10.1109/TAFFC.2023.3334520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Most existing multi-modal emotion recognition studies are targeted at a classification task that aims to assign a specific emotion category to a combination of several heterogeneous input data, including multimedia signals and physiological signals. A growing number of recent psychological evidence suggests that different discrete emotions may co-exist at the same time, which promotes the development of mixed-emotion recognition to identify a mixture of basic emotions. In this work, we focus on a challenging situation where both positive and negative emotions are presented simultaneously, and propose a multi-modal mixed emotion recognition framework, namely EmotionDict. The key characteristics of our EmotionDict include the following. (1) Inspired by the psychological evidence that such a mixed state can be represented by combinations of basic emotions, we address mixed emotion recognition as a label distribution learning task. An emotion dictionary has been designed to disentangle the mixed emotion representations into a weighted sum of a set of basic emotion elements in a shared latent space and their corresponding weights. (2) We incorporate physiological and overt behavioral multi-modal signals, including electroencephalogram (EEG), peripheral physiological signals, and facial videos, which directly display the subjective emotions. These modalities have diverse characteristics given that they are related to the central or peripheral nervous system, and the motor cortex. (3) We further design auxiliary tasks to learn modality attentions for modality integration. Experiments on two datasets show that our method outperforms existing state-of-the-art approaches on mixed-emotion recognition.},
  archive  = {J},
  author   = {Fang Liu and Pei Yang and Yezhi Shu and Fei Yan and Guanhua Zhang and Yong-Jin Liu},
  doi      = {10.1109/TAFFC.2023.3334520},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1289-1302},
  title    = {Emotion dictionary learning with modality attentions for mixed emotion exploration},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving multi-label facial expression recognition with
consistent and distinct attentions. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 1279–1288. (<a
href="https://doi.org/10.1109/TAFFC.2023.3333874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Facial expression recognition (FER) attracts much attention in computer vision. Previous works mostly study the single-label FER problem. The more complex multi-label facial expression recognition task is underexplored. Multi-label FER is more challenging than single-label task due to two primary causes. On one hand, there are less available multi-label facial expression data for analysis. On the other hand, the entanglement of expressions makes recognition more difficult. In this work, we leverage class activation map (CAM) to improve the performance of multi-label FER. Firstly, considering the shortage of training data, an attention flipping consistency (AFC) loss equipped with random rotation augmentation is proposed. It restrains the network to produce consistent CAMs under horizontally flipping transformation and thus improves the stability of network without any extra data. Secondly, based on the prior knowledge that different emotions have different predominant activated facial regions, we propose a label-guided spatial attention dispersing (SAD) loss to enable the model to learn from distinct expression-related regions. By combining the widely used multi-label classification loss (i.e., binary cross-entropy loss) and proposed AFC loss and SAD loss, our method achieves state-of-the-art performance on multi-label FER databases and the model’s interpretability is improved.},
  archive  = {J},
  author   = {Jing Jiang and Weihong Deng},
  doi      = {10.1109/TAFFC.2023.3333874},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1279-1288},
  title    = {Improving multi-label facial expression recognition with consistent and distinct attentions},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image-to-text conversion and aspect-oriented filtration for
multimodal aspect-based sentiment analysis. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(3), 1264–1278. (<a
href="https://doi.org/10.1109/TAFFC.2023.3333200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multimodal aspect-based sentiment analysis (MABSA) aims to determine the sentiment polarity of each aspect mentioned in the text based on multimodal content. Various approaches have been proposed to model multimodal sentiment features for each aspect via modal interactions. However, most existing approaches have two shortcomings: (1) The representation gap between textual and visual modalities may increase the risk of misalignment in modal interactions; (2) In some examples where the image is not related to the text, the visual information may not enrich the textual modality when learning aspect-based sentiment features. In such cases, blindly leveraging visual information may introduce noises in reasoning the aspect-based sentiment expressions. To tackle these shortcomings, we propose an end-to-end MABSA framework with image conversion and noise filtration. Specifically, to bridge the representation gap in different modalities, we attempt to translate images into the input space of a pre-trained language model (PLM). To this end, we develop an image-to-text conversion module that can convert an image to an implicit sequence of token embedding. Moreover, an aspect-oriented filtration module is devised to alleviate the noise in the implicit token embeddings, which consists of two attention operations. After filtering the noise, we leverage a PLM to encode the text, aspect, and image prompt derived from filtered implicit token embeddings as sentiment features to perform aspect-based sentiment prediction. Experimental results on two MABSA datasets show that our framework achieves state-of-the-art performance. Furthermore, extensive experimental analysis demonstrates the proposed framework has superior robustness and efficiency.},
  archive  = {J},
  author   = {Qianlong Wang and Hongling Xu and Zhiyuan Wen and Bin Liang and Min Yang and Bing Qin and Ruifeng Xu},
  doi      = {10.1109/TAFFC.2023.3333200},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1264-1278},
  title    = {Image-to-text conversion and aspect-oriented filtration for multimodal aspect-based sentiment analysis},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Empathy by design: The influence of trembling AI voices on
prosocial behavior. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(3), 1253–1263. (<a
href="https://doi.org/10.1109/TAFFC.2023.3332742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent advances in artificial speech synthesis and machine learning equip AI-powered conversational agents, from voice assistants to social robots, with the ability to mimic human emotional expression during their interactions with users. One unexplored development is the ability to design machine-generated voices that induce varying levels of “shakiness” (i.e., trembling) in the agents’ voices. In the current work, we examine how the trembling voice of a conversational AI impacts users’ perceptions, affective experiences, and their subsequent behavior. Across three studies, we demonstrate that a trembling voice enhances the perceived psychological vulnerability of the agent, followed by a heightened sense of empathic concern, ultimately increasing people&#39;s willingness to donate in a prosocial charity context. We provide further evidence from a large-scale field experiment that conversational agents with a trembling voice lead to increased click-through rates and decreased costs-per-impression in an online charity advertising setting. These findings deepen our understanding of the nuanced impact of intentionally designed voices of conversational AI agents on humans and highlight the ethical and societal challenges that arise.},
  archive  = {J},
  author   = {Fotis Efthymiou and Christian Hildebrand},
  doi      = {10.1109/TAFFC.2023.3332742},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1253-1263},
  title    = {Empathy by design: The influence of trembling AI voices on prosocial behavior},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual learning for conversational emotion recognition and
emotional response generation. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 1241–1252. (<a
href="https://doi.org/10.1109/TAFFC.2023.3332631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion recognition in conversation (ERC) and emotional response generation (ERG) are two important NLP tasks. ERC aims to detect the utterance-level emotion from a dialogue, while ERG focuses on expressing a desired emotion. Essentially, ERC is a classification task, with its input and output domains being the utterance text and emotion labels, respectively. On the other hand, ERG is a generation task with its input and output domains being the opposite. These two tasks are highly related, but surprisingly, they are addressed independently without making use of their duality in prior works. Therefore, in this article, we propose to solve these two tasks in a dual learning framework. Our contributions are fourfold: (1) We propose a dual learning framework for ERC and ERG. (2) Within the proposed framework, two models can be trained jointly, so that the duality between them can be utilised. (3) Instead of a symmetric framework that deals with two tasks of the same data domain, we propose a dual learning framework that performs on a pair of asymmetric input and output spaces, i.e., the natural language space and the emotion labels. (4) Experiments are conducted on benchmark datasets to demonstrate the effectiveness of our framework.},
  archive  = {J},
  author   = {Shuhe Zhang and Haifeng Hu and Songlong Xing},
  doi      = {10.1109/TAFFC.2023.3332631},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1241-1252},
  title    = {Dual learning for conversational emotion recognition and emotional response generation},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing EEG-based decision-making performance prediction
by maximizing mutual information between emotion and decision-relevant
features. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(3), 1228–1240. (<a
href="https://doi.org/10.1109/TAFFC.2023.3329526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotions are important factors in decision-making. With the advent of brain-computer interface (BCI) techniques, researchers developed a strong interest in predicting decisions based on emotions, which is a challenging task. To predict decision-making performance using emotion, we have proposed the Maximizing Mutual Information between Emotion and Decision relevant features (MMI-ED) method, with three modules: (1) Temporal-spatial encoding module captures spatial correlation and temporal dependence from electroencephalogram (EEG) signals; (2) Relevant feature decomposition module extracts emotion-relevant features and decision-relevant features; (3) Relevant feature fusion module maximizes the mutual information to incorporate useful emotion-related feature information during the decision-making prediction process. To construct a dataset that uses emotions to predict decision-making performance, we designed an experiment involving emotion elicitation and decision-making tasks and collected EEG, behavioral, and subjective data. We performed a comparison of our model with several emotion recognition and motion imagery models using our dataset. The results demonstrate that our model achieved state-of-the-art performance, achieving a classification accuracy of 92.96 $\%$ . This accuracy is 6.83 $\%$ higher than the best-performing model. Furthermore, we conducted an ablation study to demonstrate the validity of each module and provided explanations for the brain regions associated with the relevant features.},
  archive  = {J},
  author   = {Xinyuan Wang and Danli Wang and Xuange Gao and Yanyan Zhao and Steve C. Chiu},
  doi      = {10.1109/TAFFC.2023.3329526},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1228-1240},
  title    = {Enhancing EEG-based decision-making performance prediction by maximizing mutual information between emotion and decision-relevant features},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Annotate smarter, not harder: Using active learning to
reduce emotional annotation effort. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 1213–1227. (<a
href="https://doi.org/10.1109/TAFFC.2023.3329563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The success of supervised models for emotion recognition on images heavily depends on the availability of images properly annotated. Although millions of images are presently available, only a few are annotated with reliable emotional information. Current emotion recognition solutions either use large amounts of weakly-labeled web images, which often contain noise that is unrelated to the emotions of the image, or transfer learning, which usually results in performance losses. Thus, it would be desirable to know which images would be useful to be annotated to avoid an extensive annotation effort. In this paper, we propose a novel approach based on active learning to choose which images are more relevant to be annotated. Our approach dynamically combines multiple active learning strategies and learns the best ones (without prior knowledge of the best ones). Experiments using nine benchmark datasets revealed that: (i) active learning allows to reduce the annotation effort, while reaching or surpassing the performance of a supervised baseline with as little as 3% to 18% of the baseline&#39;s training set, in classification tasks; (ii) our online combination of multiple strategies converges to the performance of the best individual strategies, while avoiding the experimentation overhead needed to identify them.},
  archive  = {J},
  author   = {Soraia M. Alarcão and Vânia Mendonça and Cláudia Sevivas and Carolina Maruta and Manuel J. Fonseca},
  doi      = {10.1109/TAFFC.2023.3329563},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1213-1227},
  title    = {Annotate smarter, not harder: Using active learning to reduce emotional annotation effort},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EmoStim: A database of emotional film clips with discrete
and componential assessment. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 1202–1212. (<a
href="https://doi.org/10.1109/TAFFC.2023.3328900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion elicitation using emotional film clips is one of the most common and ecologically valid methods in Affective Computing. However, selecting and validating appropriate materials that evoke a range of emotions is challenging. Here, we present EmoStim: A Database of Emotional Film Clips as a film library with rich and varied content. EmoStim is designed for researchers interested in studying emotions in relation to either discrete or componential models of emotion. To create the database, 139 film clips were selected from literature and then annotated by 638 participants through the CrowdFlower platform. We selected 99 film clips based on the distribution of subjective ratings that effectively distinguished between emotions defined by the discrete model. We show that the selected film clips reliably induce a range of specific emotions according to the discrete model. Further, we describe relationships between emotions, emotion organization in the componential space, and underlying dimensions representing emotional experience. The EmoStim database and participant annotations are freely available for research purposes. The database can be used to enrich our understanding of emotions further and serve as a guide to select or creating additional materials.},
  archive  = {J},
  author   = {Rukshani Somarathna and Patrik Vuilleumier and Gelareh Mohammadi},
  doi      = {10.1109/TAFFC.2023.3328900},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1202-1212},
  title    = {EmoStim: A database of emotional film clips with discrete and componential assessment},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Style-exprGAN: Diverse smile style image generation via
attention-guided adversarial networks. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(3), 1190–1201. (<a
href="https://doi.org/10.1109/TAFFC.2023.3327118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article proposes a data-driven approach for generating personalized smile style images for neutral expressions, which aims to produce diverse smile styles while preserving individual features. Unlike other generator models that require expensive manual facial attribute labeling, we designed an auxiliary expression attention Siamese network (EASN) to extract identity-irrelevant facial expression attention regions and guide the proposed two-stage style-expression generative adversarial network (style-exprGAN). The first generator stage generates the overall facial geometry and virtual smile features, while the second stage refines the image quality. Additionally, we introduced traditional geometry warping methods to include registered neutral expression images for consistent transformation and realistic texture fusion. Results show that the proposed method effectively synthesizes realistic and diverse smile styles while preserving individual features. Furthermore, we demonstrate the potential of our data-driven approach by applying the generated personalized smile style images to image augmentation tasks, improving the stability and robustness of facial recognition models.},
  archive  = {J},
  author   = {Ching-Ting Tu and Kuan-Lin Chen},
  doi      = {10.1109/TAFFC.2023.3327118},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1190-1201},
  title    = {Style-exprGAN: Diverse smile style image generation via attention-guided adversarial networks},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Chronic stress recognition based on time-slot analysis of
ambulatory electrocardiogram and tri-axial acceleration. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(3), 1178–1189. (<a
href="https://doi.org/10.1109/TAFFC.2023.3326747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Stress, especially chronic stress, is a high risk factor of many physical and mental health problems. This work acquired 702 days of full-day ambulatory electrocardiogram (ECG) and Tri-axial acceleration (T-ACC) data from 104 healthy college students and realized chronic stress recognition through signal processing, statistical test and machine learning. We divided the 24 hours of a day into 153 time slots, and calculated 30 features from ECG and T-ACC data in each time slot. Statistical test of the above 30 features of the subjects with chronic stress and no chronic stress labels showed that chronic stress altered the autonomic nervous control of the heart not only in the daily activity time but also in the rest time at night, leading to smaller heart rate variability, faster heart rate and less complexity of the heartbeat rhythm. More specifically, the parasympathetic nervous activity at night was weakened by chronic stress. We expressed the ECG and T-ACC data of a day as a 30 × 153 data matrix, applied a four-layer fully connected neural network to classify the data of 702 days with chronic stress and no chronic stress labels, and obtained 88.17% chronic stress detection accuracy in the leave-one-subject-out cross test.},
  archive  = {J},
  author   = {Jiayu Li and Manman Wang and Feifei Zhang and Guangyuan Liu and Wanhui Wen},
  doi      = {10.1109/TAFFC.2023.3326747},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1178-1189},
  title    = {Chronic stress recognition based on time-slot analysis of ambulatory electrocardiogram and tri-axial acceleration},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NEMO: A database for emotion analysis using functional
near-infrared spectroscopy. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 1166–1177. (<a
href="https://doi.org/10.1109/TAFFC.2023.3315971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present a dataset for the analysis of human affective states using functional near-infrared spectroscopy (fNIRS). Data were recorded from thirty-one participants who engaged in two tasks. In the emotional perception task the participants passively viewed images sampled from the standard international affective picture system database, which provided ground-truth valence and arousal annotation for the stimuli. In the affective imagery task the participants actively imagined emotional scenarios followed by rating these for subjective valence and arousal. Correlates between the fNIRS signal and the valence-arousal ratings were investigated to estimate the validity of the dataset. Source-code and summaries are provided for a processing pipeline, brain activity group analysis, and estimating baseline classification performance. For classification, prediction experiments are conducted for single-trial 4-class classification of arousal and valence as well as cross-participant classifications, and comparisons between high and low arousal variants of the valence prediction tasks. Finally, classification results are presented for subject-specific and cross-participant models. The dataset is made publicly available to encourage research on affective decoding and downstream applications using fNIRS data.},
  archive  = {J},
  author   = {Michiel Spapé and Kalle Mäkelä and Tuukka Ruotsalo},
  doi      = {10.1109/TAFFC.2023.3315971},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1166-1177},
  title    = {NEMO: A database for emotion analysis using functional near-infrared spectroscopy},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new perspective on stress detection: An automated approach
for detecting eustress and distress. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 1153–1165. (<a
href="https://doi.org/10.1109/TAFFC.2023.3324910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Previous studies have solely focused on establishing Machine Learning (ML) models for automated detection of stress arousal. However, these studies do not recognize stress appraisal and presume stress is a negative mental state. Yet, stress can be classified according to its influence on individuals; the way people perceive a stressor determines whether the stress reaction is considered as eustress (positive stress) or distress (negative stress). Thus, this study aims to assess the potential of using an ML approach to determine stress appraisal and identify eustress and distress instances using physiological and behavioral features. The results indicate that distress leads to higher perceived stress arousal compared to eustress. An XGBoost model that combined physiological and behavioral features using a 30 second time window had 83.38% and 78.79% F 1 -scores for predicting eustress and distress, respectively. Gender-based models resulted in an average increase of 2-4% in eustress and distress prediction accuracy. Finally, a model to predict the simultaneous assessment of eustress and distress, distinguishing between pure eustress, pure distress, eustress-distress coexistence, and the absence of stress achieved a moderate F 1 -score of 65.12%. The results of this study lay the foundation for work management interventions to maximize eustress and minimize distress in the workplace.},
  archive  = {J},
  author   = {Mohamad Awada and Burcin Becerik-Gerber and Gale Lucas and Shawn Roll and Ruying Liu},
  doi      = {10.1109/TAFFC.2023.3324910},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1153-1165},
  title    = {A new perspective on stress detection: An automated approach for detecting eustress and distress},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HICEM: A high-coverage emotion model for artificial
emotional intelligence. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 1136–1152. (<a
href="https://doi.org/10.1109/TAFFC.2023.3324902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As social robots and other intelligent machines enter the home, artificial emotional intelligence (AEI) is taking center stage to address users’ desire for deeper, more meaningful human-machine interaction. To accomplish such efficacious interaction, the next-generation AEI needs comprehensive human emotion models for training. Unlike theory of emotion, which has been the historical focus in psychology, emotion models are a descriptive tool. In practice, the strongest models need robust coverage, which means defining the smallest core set of emotions from which all others can be derived. To achieve the desired coverage, we turn to word embeddings from natural language processing. Using unsupervised clustering techniques, our experiments show that with as few as 15 discrete emotion categories, we can provide maximum coverage across six major languages–Arabic, Chinese, English, French, Spanish, and Russian. In support of our findings, we also examine annotations from two large-scale emotion recognition datasets to assess the validity of existing emotion models compared to human perception at scale. Because robust, comprehensive emotion models are foundational for developing real-world affective computing applications, this work has broad implications in social robotics, human-machine interaction, mental healthcare, computational psychology, and entertainment.},
  archive  = {J},
  author   = {Benjamin Wortman and James Z. Wang},
  doi      = {10.1109/TAFFC.2023.3324902},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1136-1152},
  title    = {HICEM: A high-coverage emotion model for artificial emotional intelligence},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Threat perception captured by emotion, motor and empathetic
system responses: A systematic review. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(3), 1116–1135. (<a
href="https://doi.org/10.1109/TAFFC.2023.3323043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The fight or flight phenomena is of evolutionary origin and responsible for the type of defensive behaviours enacted, when in the face of threat. This review attempts to draw the link between fear and aggression as motivational levers for fight or flight defensive behaviours. Furthermore, this review investigates whether human biological motion is modulated by the affective behaviours associated with the fight or flight phenomenon. It examines how threat informed emotion and motor systems have the potential to result in the modulation of empathetic appraisal. This is of interest to this systematic review, as empathetic modulation is crucial to prosocial drive, which has the potential to alleviate the perceived threat. Hence, this review investigates the role of affective computing in capturing the potential outcome of threat perception and associated empathetic responses. To gain a comprehensive understanding of the affective responses and biological motion evoked from threat scenarios, affective computing methods used to capture these neurophysiological and behavioural responses are discussed. A systematic review using Google Scholar and Web of Science was conducted as of 2023, and findings were supplemented by bibliographies of key articles. A total of 26 studies were analysed from initial web searches to explore the topics of empathy, threat perception, fight or flight, fear, aggression, and human motion. Relationships between affective behaviours (fear, aggression) and corresponding motor defensive behaviours (fight or flight) were examined within threat scenarios, and whether existing affective computing methods are effective in capturing these responses, identifying the varying consensus in the literature, challenges, and limitations of existing research.},
  archive  = {J},
  author   = {Elizabeth Michelle Jacobs and Fani Deligianni and Frank Pollick},
  doi      = {10.1109/TAFFC.2023.3323043},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1116-1135},
  title    = {Threat perception captured by emotion, motor and empathetic system responses: A systematic review},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Implementing the affective mechanism for group emotion
recognition with a new graph convolutional network architecture.
<em>IEEE Transactions on Affective Computing</em>, <em>15</em>(3),
1104–1115. (<a
href="https://doi.org/10.1109/TAFFC.2023.3320101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Research on social psychology has revealed the existence of an affective mechanism in a human group, which is the group members spread their emotions to one another, the emotions of the group members form the group emotion, and the group emotion as a powerful force shapes the group members’ emotions. Current group emotion recognition methods focus on how the emotions of the group members form the group-level emotion but rarely take into account how the group emotion feeds back to the group members instantaneously. This paper proposes a new graph convolutional network architecture to characterize this unique affective mechanism for group emotion recognition. We regard the group members as the nodes of the graph and introduce a pseudo node into the graph to represent the role of the group. This paper uses graph convolutional networks to model the emotional interactions within the group from a static image and constructs an effective emotional representation at the group level for recognition. Experiment results on three widely used datasets for group emotion recognition show that our proposed method achieved superior performance in terms of recognition accuracy compared to the state-of-the-art methods.},
  archive  = {J},
  author   = {Xingzhi Wang and Dong Zhang and Dah-Jye Lee},
  doi      = {10.1109/TAFFC.2023.3320101},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1104-1115},
  title    = {Implementing the affective mechanism for group emotion recognition with a new graph convolutional network architecture},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised time-aware sampling network with deep
reinforcement learning for EEG-based emotion recognition. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(3), 1090–1103. (<a
href="https://doi.org/10.1109/TAFFC.2023.3319397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recognizing human emotions from complex, multivariate, and non-stationary electroencephalography (EEG) time series is essential in affective brain-computer interface. However, because continuous labeling of ever-changing emotional states is not feasible in practice, existing methods can only assign a fixed label to all EEG timepoints in a continuous emotion-evoking trial, which overlooks the highly dynamic emotional states and highly non-stationary EEG signals. To solve the problems of high reliance on fixed labels and ignorance of time-changing information, in this paper we propose a time-aware sampling network ( TAS-Net ) using deep reinforcement learning (DRL) for unsupervised emotion recognition, which is able to detect key emotion fragments and disregard irrelevant and misleading parts. Specifically, we formulate the process of mining key emotion fragments from EEG time series as a Markov decision process and train a time-aware agent through DRL without label information. First, the time-aware agent takes deep features from a feature extractor as input and generates sample-wise importance scores reflecting the emotion-related information each sample contains. Then, based on the obtained sample-wise importance scores, our method preserves top- X continuous EEG fragments with relevant emotion and discards the rest. Finally, we treat these continuous fragments as key emotion fragments and feed them into a hypergraph decoding model for unsupervised clustering. Extensive experiments are conducted on three public datasets (SEED, DEAP, and MAHNOB-HCI) for emotion recognition using leave-one-subject-out cross-validation, and the results demonstrate the superiority of the proposed method against previous unsupervised emotion recognition methods. The proposed TAS-Net has great potential in achieving a more practical and accurate affective brain-computer interface in a dynamic and label-free circumstance.},
  archive  = {J},
  author   = {Yongtao Zhang and Yue Pan and Yulin Zhang and Min Zhang and Linling Li and Li Zhang and Gan Huang and Lei Su and Honghai Liu and Zhen Liang and Zhiguo Zhang},
  doi      = {10.1109/TAFFC.2023.3319397},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1090-1103},
  title    = {Unsupervised time-aware sampling network with deep reinforcement learning for EEG-based emotion recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MTDAN: A lightweight multi-scale temporal difference
attention networks for automated video depression detection. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(3), 1078–1089. (<a
href="https://doi.org/10.1109/TAFFC.2023.3312263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Deep learning based video depression analysis has been recently an interesting and challenging topic. Most of existing works focus on learning single-scale facial dynamics of participants for depression detection. Besides, they usually adopt expensive deep learning models with high computational complexity, resulting in difficulty in real-time clinical applications. To address these two issues, this work proposes a lightweight Multi-scale Temporal Difference Attention Networks (MTDAN) integrating the temporal difference and attention mechanism to model both short-term and long-term temporal facial behaviors for automated video depression detection. Initially, two simple yet effective sub-branches, i.e., a Short-term Temporal Difference Attention Network (ST-TDAN), and a Long-term Temporal Difference Attention Network (LT-TDAN), are designed to perform individually short-term and long-term depressive behavior modeling. Then, a simple Interactive Multi-head Attention Fusion (IMHAF) strategy is employed for integrating short-term and long-term spatiotemporal features, followed by a linear fully-collected layer for depression score prediction. Experiments on two public AVEC2013 and AVEC2014 datasets show that our proposed method not only achieves highly competitive performance to state-of-the-art methods, but also has much smaller computational complexity than them on video depression detection tasks.},
  archive  = {J},
  author   = {Shiqing Zhang and Xingnan Zhang and Xiaoming Zhao and Jiangxiong Fang and Mingyue Niu and Ziping Zhao and Jun Yu and Qi Tian},
  doi      = {10.1109/TAFFC.2023.3312263},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1078-1089},
  title    = {MTDAN: A lightweight multi-scale temporal difference attention networks for automated video depression detection},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AMDET: Attention based multiple dimensions EEG transformer
for emotion recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 1067–1077. (<a
href="https://doi.org/10.1109/TAFFC.2023.3318321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Affective computing is an important subfield of artificial intelligence, and with the rapid development of brain-computer interface technology, emotion recognition based on EEG signals has received broad attention. It is still a great challenge to effectively explore the multi-dimensional information in the EEG data in spite of a large number of deep learning methods. In this article, we propose a deep learning model called Attention-based Multiple Dimensions EEG Transformer (AMDET), which can leverage the complementarity among the spectral-spatial-temporal features of EEG data by employing the multi-dimensional global attention mechanism. We first transform the original EEG data into 3D temporal-spectral-spatial representations and then the AMDET would use spectral-spatial transformer blocks to extract effective features in the EEG signal and focus on the critical time frame with the temporal attention block. We conduct extensive experiments on the DEAP, SEED, and SEED-IV datasets to evaluate the performance of AMDET and the results outperform the state-of-the-art baseline on three datasets. Accuracy rates of 97.48%, 96.85%, 97.17%, 87.32% were achieved in the DEAP-Arousal, DEAP-Valence, SEED, and SEED-IV datasets, respectively. Based on AMDET, we can achieve over 90% accuracy with only eight channels, significantly improving the possibility of practical applications.},
  archive  = {J},
  author   = {Yongling Xu and Yang Du and Ling Li and Honghao Lai and Jing Zou and Tianying Zhou and Lushan Xiao and Li Liu and Pengcheng Ma},
  doi      = {10.1109/TAFFC.2023.3318321},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1067-1077},
  title    = {AMDET: Attention based multiple dimensions EEG transformer for emotion recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised multimodal learning for dependency-free
personality recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 1053–1066. (<a
href="https://doi.org/10.1109/TAFFC.2023.3318367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent advances in AI-based learning models have significantly increased the accuracy of Automatic Personality Recognition (APR). However, these methods either require training data from the same subject or the meta-information from the training set to learn the personality-related features (i.e., subject-dependency). The variance of feature extraction for different subjects compromises the possibility of designing a dependency-free system for APR. To address this problem, we present an unsupervised multimodal learning framework to infer personality traits from audio, visual, and verbal modalities. Our method both extracts the handcraft features and transfers deep-learning based embeddings from other tasks (e.g., emotion recognition) to recognize personality traits. Since these representations are extracted locally in the time domain, we present an unsupervised temporal aggregation method to aggregate the extracted features over the temporal dimension. We evaluate our method on the ChaLearn dataset, the most widely referenced dataset for APR, using a dependency-free split of the dataset. Our results show that the proposed feature extraction and temporal aggregation modules do not require personality annotations in training but still outperform other state-of-the-art baseline methods. We also address the problem of subject-dependency in the original split of the ChaLearn dataset. The newly proposed split (i.e., data for training, validation, and testing) of the dataset can benefit the community by providing a more accurate method to validate the subject-generalizability of APR algorithms.},
  archive  = {J},
  author   = {Sina Ghassemi and Tianyi Zhang and Ward van Breda and Antonis Koutsoumpis and Janneke K. Oostrom and Djurre Holtrop and Reinout E. de Vries},
  doi      = {10.1109/TAFFC.2023.3318367},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1053-1066},
  title    = {Unsupervised multimodal learning for dependency-free personality recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Managing emotional dialogue for a virtual cancer patient: A
schema-guided approach. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 1041–1052. (<a
href="https://doi.org/10.1109/TAFFC.2023.3317805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we describe a general-purpose dialogue management framework used to design SOPHIE (Standardized Online Patient for Healthcare Interaction Education). SOPHIE simulates a virtual standardized cancer patient that allows physicians to practice skills such as empathy and patient empowerment in end-of-life communication. To provide the user with an opportunity to practice these skills, SOPHIE must produce a natural, emotionally appropriate conversation, yet handle topic shifts and open-ended questions from the user. To accomplish this, our approach to dialogue management loosely follows schemas – explicit representations of the typical flows of dialogue in end-of-life communication – while also using flexible pattern-driven methods for interpretation and generation. We conduct a crowdsourced evaluation of conversations between medical students and SOPHIE. Our agent is judged to produce responses that are natural, emotionally appropriate, and consistent with her role as a cancer patient. Furthermore, it significantly outperforms an end-to-end neural model fine-tuned on a human standardized patient corpus, attesting to the advantages of a schema-guided approach in this domain. However, the system is currently limited in its ability to generate responses that are judged to demonstrate deep understanding of the user, suggesting that future work should place focus on integrating this framework with robust natural language understanding and commonsense reasoning methods.},
  archive  = {J},
  author   = {Benjamin Kane and Catherine Giugno and Lenhart Schubert and Kurtis Haut and Caleb Wohn and Ehsan Hoque},
  doi      = {10.1109/TAFFC.2023.3317805},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1041-1052},
  title    = {Managing emotional dialogue for a virtual cancer patient: A schema-guided approach},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved video emotion recognition with alignment of CNN and
human brain representations. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 1026–1040. (<a
href="https://doi.org/10.1109/TAFFC.2023.3316173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The ability to perceive emotions is an important criterion for judging whether a machine is intelligent. To this end, a large number of emotion recognition algorithms have been developed especially for visual information such as video. Most previous studies are based on hand-crafted features or CNN, in which the former fails to extract expressive features and the latter still faces the undesired affective gap. This drives us to think about what if we could incorporate the human emotional perception capability into CNN. In this paper, we attempt to address this question by exploring alignment between representations of neural networks and human brain activity. In particular, we employ a visually evoked emotional brain activity dataset to conduct a jointly training strategy for CNN. In the training phase, we introduce the representation similarity analysis (RSA) to align the CNN with human brain to obtain more brain-like features. Specifically, representation similarity matrices (RSMs) of multiple convolutional layers are averaged with learnable weights and related to the RSM of human brain. In order to obtain emotion-related brain activity, we conduct voxel selection and denoising with a banded ridge model before computing the RSM. Sufficient experiments on two challenging video emotion recognition datasets and multiple popular CNN architectures suggest that human brain activity is promising to provide an inductive bias for CNN towards better performance of emotion recognition.},
  archive  = {J},
  author   = {Kaicheng Fu and Changde Du and Shengpei Wang and Huiguang He},
  doi      = {10.1109/TAFFC.2023.3316173},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1026-1040},
  title    = {Improved video emotion recognition with alignment of CNN and human brain representations},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bodily electrodermal representations for affective
computing. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(3), 1018–1025. (<a
href="https://doi.org/10.1109/TAFFC.2023.3315973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The view of embodied emotion believes that emotions are the emotions of the body. While emotion-specific patterns of self-reported bodily sensation have been previously reported, the physiological bodily representation across emotions remains to be addressed. The present study aimed to investigate the effectiveness of multi-site bodily electrodermal representations of emotions. A multi-channel electrodermal measurement device was designed to record electrodermal activities from nine body sites (neck, back, chest, bilateral abdomen, bilateral wrist, and bilateral ankle) from thirty-six college students (all male), while they were presented with a series of emotional pictures. Using the integral skin conductance response feature and a random forest classification method, the classification of high and low arousal levels achieved an average classification accuracy of 80.4 ± 8.1%, and the classification of positive, neutral, and negative states reached an average classification accuracy of 76.4 ± 10.2%. The classification models for arousal and valence were found to rely on distinct bodily representations. Meanwhile, the classification results of multi-site measurement were significantly better than single-site results. Our findings for the first time illustrate the bodily electrodermal representations of emotion and suggest the feasibility of affective computing using bodily electrodermal signals.},
  archive  = {J},
  author   = {Xinyu Shui and Rongzan Lin and Ziyang Luo and Bingxin Lin and Xinxin Mao and Haojie Li and Ran Liu and Dan Zhang},
  doi      = {10.1109/TAFFC.2023.3315973},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1018-1025},
  title    = {Bodily electrodermal representations for affective computing},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MPEG: A multi-perspective enhanced graph attention network
for causal emotion entailment in conversations. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(3), 1004–1017. (<a
href="https://doi.org/10.1109/TAFFC.2023.3315752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion causes constitute a pivotal component in the comprehension of emotional conversations. Recently, a new task named Causal Emotion Entailment (CEE) has been proposed to identify the causal utterances for the target emotional utterance in a conversation. Although researchers have achieved some progress in solving this problem, they failed to adequately incorporate speaker characteristics and overlooked the effects of temporal relations in conversation structures. To fill such a research gap to some extent, we propose a novel causal emotion entailment framework, namely MPEG (Multi-Perspective Enhanced Graph attention network). The training of MPEG consists of three stages. First, we utilize a speaker-aware pre-trained model and two attention mechanisms to obtain the utterance representations that incorporate local contexts as well as the speaker and emotional information. Then, these representations are fed into a graph attention network to model the conversation structures and emotional dynamics from both local and global perspectives. Finally, a fully-connected network is implemented to predict the relationships between emotional utterances and causal utterances. Experimental results show that MPEG achieves state-of-the-art performance.},
  archive  = {J},
  author   = {Tiantian Chen and Ying Shen and Xuri Chen and Lin Zhang and Shengjie Zhao},
  doi      = {10.1109/TAFFC.2023.3315752},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {1004-1017},
  title    = {MPEG: A multi-perspective enhanced graph attention network for causal emotion entailment in conversations},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CFEW: A large-scale database for understanding child facial
expression in real world. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 990–1003. (<a
href="https://doi.org/10.1109/TAFFC.2023.3313782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Currently, much progress has been achieved on adult facial expressions recognition. Few attentions have been paid to child facial expression analysis. A lack of publicly available large-scale child facial expression databases hinders the development of automatic coding for child facial expression behaviors. In this work, we constructed a new face database for understanding C hild F acial E xpression in real W orld (CFEW). The database contains three novelties: (1) the largest publicly available child facial expression database (11,000+ images); (2) covering full developmental range of 0–18-year-old child subjects; (3) rich annotations for facial expression labels, including discrete expression categories aka happy, neutral, disgust, angry, sad, cry, fear, surprise, sleepy and others, intensity of arousal and valence, and several types of facial action units (AUs). In addition, the images in this database cover several challenging conditions in real world, including frontal and non-frontal head poses, facial occlusions, various illuminations and low image resolution. Three dominant deep convolutional neural networks (i.e., VGG11bn, ResNet18 and DenseNet121) were used to conduct extensive baseline experiments for discrete facial expression classification, arousal and valence estimation and facial action units detection within database, and cross-database seven facial expressions recognition.},
  archive  = {J},
  author   = {Chuangao Tang and Sunan Li and Wenming Zheng and Yuan Zong and Su Zhang and Cheng Lu and Yan Zhao},
  doi      = {10.1109/TAFFC.2023.3313782},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {990-1003},
  title    = {CFEW: A large-scale database for understanding child facial expression in real world},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CA-FER: Mitigating spurious correlation with counterfactual
attention in facial expression recognition. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(3), 977–989. (<a
href="https://doi.org/10.1109/TAFFC.2023.3312768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Although facial expression recognition based on deep learning has become a major trend, existing methods have been found to prefer learning spurious statistical correlations and non-robust features during training. This degenerates the model&#39;s generalizability in practical situations. One of the research fields mitigating such misperception of correlations as causality is causal reasoning. In this article, we propose a learnable counterfactual attention mechanism, CA-FER, that uses causal reasoning to simultaneously optimize feature discrimination and diversity to mitigate spurious correlations in expression datasets. To the best of our knowledge, this is the first work to study the spurious correlations in facial expression recognition from a counterfactual attention perspective. Extensive experiments on a synthetic dataset and four public datasets demonstrate that our method outperforms previous methods, which shows the effectiveness and generalizability of our learnable counterfactual attention mechanism for the expression recognition task.},
  archive  = {J},
  author   = {Pin-Jui Huang and Hongxia Xie and Hung-Cheng Huang and Hong-Han Shuai and Wen-Huang Cheng},
  doi      = {10.1109/TAFFC.2023.3312768},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {977-989},
  title    = {CA-FER: Mitigating spurious correlation with counterfactual attention in facial expression recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mental stress assessment in the workplace: A review.
<em>IEEE Transactions on Affective Computing</em>, <em>15</em>(3),
958–976. (<a href="https://doi.org/10.1109/TAFFC.2023.3312762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Workers with demanding jobs are at risk of experiencing mental stress, leading to decreased performance, mental illness, and disrupted sleep. To detect elevated stress levels in the workplace, studies have explored stress measurement from physiological, psychological, and behavioral perspectives. This paper reviews the assessment methods and strategies for mitigating mental stress in the workplace and provides recommendations for early detection and mitigation of mental stress. Among the modalities, Electroencephalography (EEG), Electrocardiography (ECG) and Galvanic Skin Response (GSR) were found to be the most used in assessing mental stress in the workplace. Nevertheless, these modalities are sensitive to motion artifacts and are difficult to be integrated into real work environments. To further improve stress level assessment in the workplace, multimodality integration with a reduced number of sensors such as EEG, GSR and Functional near infrared spectroscopy (fNIRS) can be utilized. This would lead to developing strategies for stress management in real-time. Furthermore, combining EEG with fNIRS would improve source localization of mental stress. To mitigate stress, we recommend developing a closed loop system that incorporates brain data acquisition systems and machine learning with physical stimulations such as audio Binaural Beats Stimulation and/or Transcranial Electric Stimulation.},
  archive  = {J},
  author   = {Ghinwa Masri and Fares Al-Shargie and Usman Tariq and Fadwa Almughairbi and Fabio Babiloni and Hasan Al-Nashash},
  doi      = {10.1109/TAFFC.2023.3312762},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {958-976},
  title    = {Mental stress assessment in the workplace: A review},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive interview strategy based on interviewees’ speaking
willingness recognition for interview robots. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(3), 942–957. (<a
href="https://doi.org/10.1109/TAFFC.2023.3309640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Social signal recognition techniques based on nonverbal behavioral sensing allow conversational robots to understand the user’s social signals, thereby enabling them to adopt interaction strategies based on internal states inferred from the social signals. This research investigates how the online social signal recognition and adaptive dialog strategy influences the dynamic change in a user’s inner state. For this purpose, we develop a semiautonomous interview robot system with an online speaker’s willingness recognition module and an adaptive question selection module based on the willingness level. The online recognition model of speaker willingness is trained from multimodal nonverbal features extracted using a novel interview corpus, which allows appropriate interview questions to be chosen based on the estimated willingness level of the user. We conduct the experiment using the system to evaluate the effectiveness of adaptive question selection based on the willingness recognition model. First, the multimodal willingness recognition model is evaluated using the interview corpus. The best recognition accuracy of willingness level (high or low) was $72. 8\%$ with the random forest classifier. Second, 27 interviewees were interviewed with the two interview robot systems: (I) with the adaptive question selection module based on willingness recognition and (II) with a random question selection strategy. The proposed adaptive question strategy significantly increased the number of utterances with high willingness compared with the baseline system (II); thus, adaptive question selection with online willingness recognition elicited the speaker’s willingness even though the model cannot be estimated with near-perfect accuracy.},
  archive  = {J},
  author   = {Fuminori Nagasawa and Shogo Okada and Takuya Ishihara and Katsumi Nitta},
  doi      = {10.1109/TAFFC.2023.3309640},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {942-957},
  title    = {Adaptive interview strategy based on interviewees’ speaking willingness recognition for interview robots},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An affective brain-computer interface based on a transfer
learning method. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(3), 929–941. (<a
href="https://doi.org/10.1109/TAFFC.2023.3305982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {An affective brain-computer interface (aBCI) can detect affective states based on brain signals and might assist people in improving their emotion regulation abilities. However, individual differences in emotional brain patterns make cross-subject emotion identification extremely challenging. Traditional supervised single-subject classification schemes require considerable calibration samples from new individuals to train subject-dependent models. Individuals are easily fatigued with long-term EEG collection processes, which may affect performance in subsequent online experiments. In this study, we propose a real-time aBCI system using domain-fusion-based multisource style transfer mapping (DF-MS-STM) to detect positive, neutral, and negative emotional states without the need for additional training sessions. Sixteen subjects participated in our online experiments to test the performance of our aBCI system and an average online prediction accuracy of 72.17±12.25% was obtained for three-class emotion recognition tasks in the last three experimental sessions. Our proposed algorithm significantly outperformed numerous baseline methods in terms of cross-subject emotion classification. In addition, we identified distinct brain patterns in response to different emotional stimuli based on the results of event-related spectral perturbation (ERSP) analyses. These neural patterns might provide new insights for emotional brain mechanistic studies and related aBCIs.},
  archive  = {J},
  author   = {Weichen Huang and Zijing Guan and Kendi Li and Yajun Zhou and Yuanqing Li},
  doi      = {10.1109/TAFFC.2023.3305982},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {929-941},
  title    = {An affective brain-computer interface based on a transfer learning method},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Focus on cooperation: A face-to-face VR serious game for
relationship enhancement. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 913–928. (<a
href="https://doi.org/10.1109/TAFFC.2023.3306198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Exploring effective approaches to enhance face-to-face interactions and interpersonal relationships is an important topic in the applications of affective computing. According to the co-actualization model, we propose a face-to-face co-participation serious game for relationship enhancement, with a focus on battling COVID-19. Moreover, a prototype system is developed using an immersive virtual environment and a low-cost brain-computer interface. Through this system, a dynamic flow experience enhancement tool is utilized to involve partners in the cooperative task. To evaluate the system performance, two studies are conducted with schoolmates as participants. Study 1 compares the cooperative and competitive modes, and demonstrates that the former elicited higher level of decision-making challenge and affections, which are beneficial for forming relationships. Study 2 further examines the effect of the dynamic flow enhancement tool in the cooperative task and the results show its effectiveness in promoting flow experience, perceived closeness, and intimacy in relationships. Given this short-term participation, participants felt a greater sense of closeness and intimacy than they had before the test. In conclusion, our proposed system is effective in enhancing schoolmate relationships.},
  archive  = {J},
  author   = {Yulong Bian and Chao Zhou and Yang Zhang and Juan Liu and Jenny Sheng and Yong-Jin Liu},
  doi      = {10.1109/TAFFC.2023.3306198},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {913-928},
  title    = {Focus on cooperation: A face-to-face VR serious game for relationship enhancement},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Emotion recognition from full-body motion using multiscale
spatio-temporal network. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 898–912. (<a
href="https://doi.org/10.1109/TAFFC.2023.3305197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Body motion is an important channel for human communication and plays a crucial role in automatic emotion recognition. This work proposes a multiscale spatio-temporal network, which captures the coarse-grained and fine-grained affective information conveyed by full-body motion and decodes the complex mapping between emotion and body movement. The proposed method consists of three main components. First, a scale selection algorithm based on the pseudo-energy model is presented, which guides our network to focus not only on long-term macroscopic body expressions, but also on short-term subtle posture changes. Second, we propose a hierarchical spatio-temporal network that can jointly process posture covariance matrices and 3D posture images with different time scales, and then hierarchically fuse them in a coarse-to-fine manner. Finally, a spatio-temporal iterative (ST-ITE) fusion algorithm is developed to jointly optimize the proposed network. The proposed approach is evaluated on five public datasets. The experimental results show that the introduction of the energy-based scale selection algorithm significantly enhances the learning capability of the network. The proposed ST-ITE fusion algorithm improves the generalization and convergence of our model. The average classification results of the proposed method exceed 86% on all datasets and outperform the state-of-the-art methods.},
  archive  = {J},
  author   = {Tao Wang and Shuang Liu and Feng He and Weina Dai and Minghao Du and Yufeng Ke and Dong Ming},
  doi      = {10.1109/TAFFC.2023.3305197},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {898-912},
  title    = {Emotion recognition from full-body motion using multiscale spatio-temporal network},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Digital phenotyping-based bipolar disorder assessment using
multiple correlation data imputation and lasso-MLP. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(3), 885–897. (<a
href="https://doi.org/10.1109/TAFFC.2023.3299607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Clinical rating scales can be used to assess the severity of bipolar disorder; however, their use involves clinician–patient interactions, which is labor-intensive. Therefore, this study proposes a digital-phenotyping-based system that provides clinical ratings of bipolar disorder severity using global positioning system, self-scale, daily mood, user emotion, sleep time, and multimedia data; these ratings are given on Hamilton Depression Rating Scale (HAM-D) and Young Mania Rating Scale (YMRS). A K-nearest-neighbor-based imputation method was used to handle missing data. In this method, missing data points are filled in with the multiple correlations between different features. Furthermore, the Least Absolute Shrinkage and Selection Operator (Lasso)-regression-based multilayer perceptron (Lasso-MLP) method was adopted to predict the total and factor scores on the HAM-D and YMRS. Five-fold cross-validation were used in evaluation experiments. When the designed data imputation method was used with Lasso-MLP, the mean square errors of the total score and average factor score on HAM-D (the YMRS) were 0.56 (0.38) and 1.88 (0.98), respectively, which were smaller than the corresponding values obtained through Lasso regression (by 0.12 and 0.05, respectively, for HAM-D and by 0.12 and 0.10, respectively, for the YMRS). The experimental results also indicated that the models trained with the imputed data outperformed those trained without imputed data. Thus, the developed approaches can eliminate the missing data problem and provide accurate clinical ratings.},
  archive  = {J},
  author   = {Jia-Hao Hsu and Chung-Hsien Wu and Wei-Kai Wang and Hung-Yi Su and Esther Ching-Lan Lin and Po See Chen},
  doi      = {10.1109/TAFFC.2023.3299607},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {885-897},
  title    = {Digital phenotyping-based bipolar disorder assessment using multiple correlation data imputation and lasso-MLP},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Teaching reverse appraisal to improve negotiation skills.
<em>IEEE Transactions on Affective Computing</em>, <em>15</em>(3),
872–884. (<a href="https://doi.org/10.1109/TAFFC.2023.3285931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Individual differences in preferences allow for integrative (win–win) solutions in negotiations. However, reaching an integrative solution is difficult as each party&#39;s preferences and limits are private and must be inferred. We hypothesized that teaching people to infer a generative model of how individuals appraise outcomes and express them as emotional expressions, i.e., an appraisal model, contributes to improving the capability of mental state inference and thus facilitates integrative solutions. In the present study, we compared participants’ performance in a 4-issue negotiation after training participants to infer appraisal model during three 2-issue negotiations with a visualized appraisal process and text feedback with the performance of those without inference learning. The results showed that training participants to infer appraisal model helped them better estimate their counterpart&#39;s preferences but did not lead them to negotiate more integrative solutions.},
  archive  = {J},
  author   = {Motoaki Sato and Kazunori Terada and Jonathan Gratch},
  doi      = {10.1109/TAFFC.2023.3285931},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {872-884},
  title    = {Teaching reverse appraisal to improve negotiation skills},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-rater consensus learning for modeling multiple sparse
ratings of affective behaviour. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(3), 859–871. (<a
href="https://doi.org/10.1109/TAFFC.2023.3297270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The use of multiple raters to label datasets is an established practice in affective computing. The principal goal is to reduce unwanted subjective bias in the labelling process. Unfortunately, this leads to the key problem of identifying a ground truth for training the affect recognition system. This problem becomes more relevant in a sparsely-crossed annotation where each rater only labels a portion of the full dataset to ensure a manageable workload per rater. In this article, we introduce a Multi-Rater Consensus Learning (MRCL) method which learns a representative affect recognition model that accounts for each rater&#39;s agreement with the other raters. MRCL combines a multitask learning (MTL) regularizer and a consensus loss. Unlike standard MTL, this approach allows the model to learn to predict each rater&#39;s label while explicitly accounting for the consensus among raters. We evaluated our approach on two different datasets based on spontaneous affective body movement expressions for pain behaviour detection and laughter type recognition respectively. The two naturalistic datasets were chosen for the different forms of labelling (different in affect, observation stimuli, and raters) that they together offer for evaluating our approach. Empirical results demonstrate that MRCL is effective for modelling affect from datasets with sparsely-crossed multi-rater annotation.},
  archive  = {J},
  author   = {Luca Romeo and Temitayo Olugbade and Massimiliano Pontil and Nadia Bianchi-Berthouze},
  doi      = {10.1109/TAFFC.2023.3297270},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {859-871},
  title    = {Multi-rater consensus learning for modeling multiple sparse ratings of affective behaviour},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep adaptation of adult-child facial expressions by fusing
landmark features. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(3), 847–858. (<a
href="https://doi.org/10.1109/TAFFC.2023.3297075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Imaging of facial affects may be used to measure psychophysiological attributes of children through their adulthood for applications in education, healthcare, and entertainment, among others. Deep convolutional neural networks show promising results in classifying facial expressions of adults. However, classifier models trained with adult benchmark data are unsuitable for learning child expressions due to discrepancies in psychophysical development. Similarly, models trained with child data perform poorly in adult expression classification. We propose domain adaptation to concurrently align distributions of adult and child expressions in a shared latent space for robust classification of either domain. Furthermore, age variations in facial images are studied in age-invariant face recognition yet remain unleveraged in adult-child expression classification. We take inspiration from multiple fields and propose deep adaptive FACial Expressions fusing BEtaMix SElected Landmark Features (FACE-BE-SELF) for adult-child expression classification. For the first time in the literature, a mixture of Beta distributions is used to decompose and select facial features based on correlations with expression, domain, and identity factors. We evaluate FACE-BE-SELF using 5-fold cross validation for two pairs of adult-child data sets. Our proposed FACE-BE-SELF approach outperforms transfer learning and other baseline domain adaptation methods in aligning latent representations of adult and child expressions.},
  archive  = {J},
  author   = {Megan A. Witherow and Manar D. Samad and Norou Diawara and Haim Y. Bar and Khan M. Iftekharuddin},
  doi      = {10.1109/TAFFC.2023.3297075},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {847-858},
  title    = {Deep adaptation of adult-child facial expressions by fusing landmark features},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sentiment analysis meets explainable artificial
intelligence: A survey on explainable sentiment analysis. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(3), 837–846. (<a
href="https://doi.org/10.1109/TAFFC.2023.3296373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Sentiment analysis can be used to derive knowledge that is connected to emotions and opinions from textual data generated by people. As computer power has grown, and the availability of benchmark datasets has increased, deep learning models based on deep neural networks have emerged as the dominant approach for sentiment analysis. While these models offer significant advantages, their lack of interpretability poses a major challenge in comprehending the rationale behind their reasoning and prediction processes, leading to complications in the models&#39; explainability. Further, only limited research has been carried out into developing deep learning models that describe their internal functionality and behaviors. In this timely study, we carry out a first of its kind overview of key sentiment analysis techniques and eXplainable artificial intelligence (XAI) methodologies that are currently in use. Furthermore, we provide a comprehensive review of sentiment analysis explainability.},
  archive  = {J},
  author   = {Arwa Diwali and Kawther Saeedi and Kia Dashtipour and Mandar Gogate and Erik Cambria and Amir Hussain},
  doi      = {10.1109/TAFFC.2023.3296373},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {837-846},
  title    = {Sentiment analysis meets explainable artificial intelligence: A survey on explainable sentiment analysis},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrating deep facial priors into landmarks for privacy
preserving multimodal depression recognition. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(3), 828–836. (<a
href="https://doi.org/10.1109/TAFFC.2023.3296318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Automatic depression diagnosis is a challenging problem, that requires integrating spatial-temporal information and extracting features from audio-visual signals. In terms of privacy protection, the development trend of recognition algorithms based on facial landmarks has created additional challenges and difficulties. In this article, we propose an audio-visual attention network (AVA-DepressNet) for depression recognition. It is a novel multimodal framework with facial privacy protection, and uses attention-based modules to enhance audio-visual spatial and temporal features. In addition, an adversarial multistage (AMS) training strategy is developed to optimize the encoder-decoder structure. Additionally, facial structure prior knowledge is creatively used in AMS training. Our AVA-DepressNet is evaluated on popular audio-visual depression datasets: AVEC 2013, AVEC 2014, and AVEC 2017. The results show that our approach reaches the state-of-the-art performance or competitive results for depression recognition.},
  archive  = {J},
  author   = {Yuchen Pan and Yuanyuan Shang and Zhuhong Shao and Tie Liu and Guodong Guo and Hui Ding},
  doi      = {10.1109/TAFFC.2023.3296318},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {828-836},
  title    = {Integrating deep facial priors into landmarks for privacy preserving multimodal depression recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fake news, real emotions: Emotion analysis of COVID-19
infodemic in weibo. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(3), 815–827. (<a
href="https://doi.org/10.1109/TAFFC.2023.3295806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The proliferation of COVID-19 fake news on social media poses a severe threat to the health information ecosystem. We show that affective computing can make significant contributions to combat this infodemic. Given that fake news is often presented with emotional appeals, we propose a new perspective on the role of emotion in the attitudes, perceptions, and behaviors of the dissemination of information. We study emotions in conjunction with fake news, and explore different aspects of their interaction. To process both emotion and ‘falsehood’ based on the same set of data, we auto-tag emotions on existing COVID-19 fake news datasets following an established emotion taxonomy. More specifically, based on the distribution of seven basic emotions (e.g., Happiness, Like, Fear, Sadness, Surprise, Disgust, Anger ), we find across domains and styles that COVID-19 fake news is dominated by emotions of Fear (e.g., of coronavirus), and Disgust (e.g., of social conflicts). In addition, the framing of fake news in terms of gain-versus-loss reveals a close correlation between emotions, perceptions, and collective human reactions. Our analysis confirms the significant role of emotion Fear in the spreading of the fake news, especially when contextualized in the loss frame. Our study points to a future direction of incorporating emotion footprints in models of automatic fake news detection, and establishes an affective computing approach to information quality in general and fake news detection in particular.},
  archive  = {J},
  author   = {Mingyu Wan and Yin Zhong and Xuefeng Gao and Yat Mei Lee and Chu-Ren Huang},
  doi      = {10.1109/TAFFC.2023.3295806},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {815-827},
  title    = {Fake news, real emotions: Emotion analysis of COVID-19 infodemic in weibo},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated scoring of asynchronous interview videos based on
multi-modal window-consistency fusion. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(3), 799–814. (<a
href="https://doi.org/10.1109/TAFFC.2023.3294335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Soft skills, such as personality characteristics, communication skills and leadership, affect personal career performance greatly. Therefore, predicting the soft skills of interviewees can provide interviewers with a strong reference for the decision of hiring. Nowadays, as asynchronous video interviews have gradually become a popular form of interviews, automatic interview evaluation of soft skills has attracted widespread attention from researchers. However, existing automatic evaluation methods have two significant drawbacks. First, most of them model the problem as multi-modal fusion of long-term sequences, while ignoring the consistency of multi-modal expression in short-time windows, which is a key attribute of the interview scene. Second, without embedding of professional knowledge in the interview field, the interpretability of the model is relatively weak. To address the above problems, we propose a novel Multi-modal Window-Consistency Fusion network, namely MWCF, to capture the expression consistency of different modalities in a short-time window and re-weight the language signals to enhance important portions in verbal clues. Meanwhile, in order to enhance the interpretability of the evaluation model, we introduce the professional knowledge of interviewers by proposing a topic generation module based on question attention, and embedding the most representative keywords under different soft skills into the model. Furthermore, a real-world interview dataset is built by developing an asynchronous interview platform, and extensive experiments are conducted to show the superior performance of our proposed model.},
  archive  = {J},
  author   = {Jianming Lv and Chujie Chen and Zequan Liang},
  doi      = {10.1109/TAFFC.2023.3294335},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {799-814},
  title    = {Automated scoring of asynchronous interview videos based on multi-modal window-consistency fusion},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Designing a mood-mediated multi-level reasoner. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(3), 787–798. (<a
href="https://doi.org/10.1109/TAFFC.2023.3293310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Psychology-oriented research extensively studied how affect influences human decision making. Particularly, the cognitive tuning assumption suggests that mood can serve to regulate between shallow and deliberative decisions – a more negative mood means higher deliberation. This work proposes a model that mimics the cognitive tuning assumption. To that end, the Collective Risk Dilemma (CRD) game For the Planet was designed and created, a process allowing for distinct levels of reasoning was defined and tested in that game, and mood was integrated to control the level of reasoning of such a process. Several distinct Artificial Intelligence (AI) profiles were created to verify that the developed model was able to dynamically change the level of reasoning and adjust the resulting AI behavior in our CRD game. Results revealed that the distinct AI profiles were influenced by their affective states and experienced circumstances, and that the emergent behaviors were consistent with the cognitive tuning assumption, thus demonstrating that we managed to construct an innovative and flexible model that can use mood to dynamically adjust the level of reasoning of an AI agent.},
  archive  = {J},
  author   = {Samuel Gomes and José Bernardo Rocha and João Dias and Carlos Martinho},
  doi      = {10.1109/TAFFC.2023.3293310},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {787-798},
  title    = {Designing a mood-mediated multi-level reasoner},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Investigating cardiovascular activation of young adults in
routine driving. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(3), 769–786. (<a
href="https://doi.org/10.1109/TAFFC.2023.3291330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We report on a naturalistic study investigating the effects of routine driving on cardiovascular activation. We recruited 21 healthy young adults from a broad geographic area in the Southwestern United States. Using the participants’ own smartphones and smartwatches, we monitored for a week both their driving and non-driving activities. Monitoring included the continuous recording of 1) heart rate throughout the day, 2) hand motion during driving as a proxy of persistent texting, and 3) contextualized driving data, complete with traffic and weather information. These high temporal resolution variables were complemented with the drivers’ biographic and psychometric profiles. Our analysis suggests that anxiety predisposition and high speeds are associated with significant cardiovascular activation on drivers, likely linked to sympathetic arousal. Surprisingly, these associations hold true under good weather, normal traffic, and with experienced drivers behind the wheel. The said findings call for attention to insidious effects of apparently benign drives even for people in their prime. Accordingly, our research contributes to intriguing new discourses on driving affect and personal health informatics.},
  archive  = {J},
  author   = {MD Tanim Hasan and Huda Alghamdi and Salah Taamneh and Mike Manser and Robert Wunderlich and Panagiotis Tsiamyrtzis and Ioannis Pavlidis},
  doi      = {10.1109/TAFFC.2023.3291330},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {769-786},
  title    = {Investigating cardiovascular activation of young adults in routine driving},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-party conversation modeling for emotion recognition.
<em>IEEE Transactions on Affective Computing</em>, <em>15</em>(3),
751–768. (<a href="https://doi.org/10.1109/TAFFC.2023.3273589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multi-party conversation modeling plays a vital role in emotion recognition in conversation (ERC). Aside from the intra- and inter-speaker dependencies between different speakers, the difficulty also lies in the fact that each conversation may contain several to many utterances that compose a long text sequence. In this article, we present two approaches to effective multi-party conversation modeling. First, to encode long sequences and capture long-range dependency between utterances, we introduce a dialog-oriented language model, DialogXL, with enhanced memory to store longer conversation sequences and dialog-aware self-attention to deal with multi-party dependencies. Second, we present a directed acyclic neural network, namely DAG-ERC, to encode the utterances with a directed acyclic graph (DAG) to better capture the intrinsic structure within a conversation. DAG-ERC combines the advantages of recurrent models and graph models and provides a more intuitive way to model information flow between sequential utterances. Extensive experiments are conducted on four ERC benchmarks with state-of-the-art models employed for comparison, and empirical results demonstrate the superiority of the two models in multi-party conversation modeling.},
  archive  = {J},
  author   = {Xiaojun Quan and Siyue Wu and Junqing Chen and Weizhou Shen and Jianxing Yu},
  doi      = {10.1109/TAFFC.2023.3273589},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7-9},
  number   = {3},
  pages    = {751-768},
  title    = {Multi-party conversation modeling for emotion recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Confidence-aware sentiment quantification via sentiment
perturbation modeling. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(2), 736–750. (<a
href="https://doi.org/10.1109/TAFFC.2023.3301956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Sentiment Quantification aims to detect the overall sentiment polarity of users from a set of reviews corresponding to a target. Existing methods equally treat and aggregate individual reviews’ sentiment to judge the overall sentiment polarity. However, the confidence of each review is not equal in sentiment quantification where sentiment perturbation arising from high- and low-confidence reviews may degrade the accuracy of Sentiment Quantification. Specifically, fake reviews with deceptive sentiments are low confidence, which perturbs the overall sentiment prediction. Whereas, some reviews generated by responsible users are high confidence. They contain authoritative suggestions so they should be emphasized in Sentiment Quantification. In this paper, we design and build COSE, a confidence-aware sentiment quantification framework, which can measure the confidence of individual reviews to eliminate sentiment perturbation and facilitate sentiment quantification. We design a Review Graph that achieves review confidence modeling in an unsupervised manner and obtains review confidence representations. Moreover, we develop a dynamic fusion attention mechanism, which produces sentiment “de-perturbation” vectors to eliminate the sentiment perturbation based on the confidence representations. Extensive experiments on large-scale review datasets validate the significant superiority of COSE over the state-of-the-art.},
  archive  = {J},
  author   = {Xiangyun Tang and Dongliang Liao and Meng Shen and Liehuang Zhu and Shen Huang and Gongfu Li and Hong Man and Jin Xu},
  doi      = {10.1109/TAFFC.2023.3301956},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {736-750},
  title    = {Confidence-aware sentiment quantification via sentiment perturbation modeling},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Encoding syntactic information into transformers for
aspect-based sentiment triplet extraction. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(2), 722–735. (<a
href="https://doi.org/10.1109/TAFFC.2023.3291730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Aspect-based sentiment triplet extraction (ASTE) aims to extract triplets consisting of aspect terms and their associated opinion terms and sentiment polarities from sentences, a relatively new and challenging subtask of aspect-based sentiment analysis (ABSA). Previous studies have used either pipeline models or unified tagging schema models. These models ignore the syntactic relationships between the aspect and its corresponding opinion words, which leads them to mistakenly focus on syntactically unrelated words. One feasible option is to use a graph convolution network (GCN) to exploit syntactic information by propagating the representation from the opinion words to the aspect. However, such a method considers all syntactic dependencies to be of the same type and thus may still incorrectly associate unrelated words to the target aspect through the iterations of graph convolutional propagation. Herein, a syntax-aware transformer (SA-Transformer) is proposed to extend the GCN strategy by fully exploiting the dependency types of edges to block inappropriate propagation. The proposed approach can obtain different representations and weights even for edges with the same dependency type according to their adjacent dependency type of edges. Instead of using a GCN layer, we used an L -layer SA transformer to encode syntactic information in the word-pair representation to improve performance. Experimental results on four benchmark datasets show that the proposed model outperforms various previous models for ASTE.},
  archive  = {J},
  author   = {Li Yuan and Jin Wang and Liang-Chih Yu and Xuejie Zhang},
  doi      = {10.1109/TAFFC.2023.3291730},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {722-735},
  title    = {Encoding syntactic information into transformers for aspect-based sentiment triplet extraction},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A knowledge-enhanced and topic-guided domain adaptation
model for aspect-based sentiment analysis. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(2), 709–721. (<a
href="https://doi.org/10.1109/TAFFC.2023.3292213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Cross-domain aspect-based sentiment analysis has recently attracted significant attention, which can effectively alleviate the problem of lacking large-scale labeled data for supervised learning based methods. Most of current methods mainly focus on extracting domain-shared syntactic features to conduct the domain adaptation. Due to the language and syntax are diverse between domains, these methods lack generalization and even lead to syntactic transfer errors. External knowledge graphs have rich domain commonsense and share the relational structures between source and target domains. The domain-shared relational structure can effectively bridge the gap across domains and solve the problem of syntactic transfer errors. Moreover, not all the introduced external knowledge is equally important for the cross-domain aspect-based sentiment analysis. Motivated by these, we propose a knowledge-enhanced and topic-guided cross domain aspect-based sentiment analysis model with the domain-shared commonsense relational structure learning module and the topic-guided knowledge attention module. Extensive experiments are conducted and the experimental results evaluate the effectiveness of our proposed model.},
  archive  = {J},
  author   = {Yushi Zeng and Guohua Wang and Haopeng Ren and Yi Cai and Ho-Fung Leung and Qing Li and Qingbao Huang},
  doi      = {10.1109/TAFFC.2023.3292213},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {709-721},
  title    = {A knowledge-enhanced and topic-guided domain adaptation model for aspect-based sentiment analysis},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial domain generalized transformer for cross-corpus
speech emotion recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(2), 697–708. (<a
href="https://doi.org/10.1109/TAFFC.2023.3290795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Speech emotion recognition (SER) promotes the development of intelligent devices, which enable natural and friendly human-computer interactions. However, the recognition performance of existing approaches is significantly reduced on unseen datasets, and the lack of sufficient training data limits the generalizability of deep learning models. In this article, we analyze the impact of the domain generalization method on cross-corpus SER and propose an adversarial domain generalized transformer (ADoGT), which is aimed at learning a shared feature distribution for the source and target domains. Specifically, we investigate the effect of domain adversarial learning by eliminating nonaffective information. We also combine the center loss with the softmax function as joint supervision to learn discriminative features. Moreover, we introduce unsupervised transfer learning to extract additional features, and incorporate a gated fusion model to learn the complementary information of the features learned by the supervised feature extractor and pretrained model. The proposed transformer based domain generalization method is evaluated using four emotional datasets. We also provide an ablation study of different domain adversarial model structures and feature fusion models. The results of comparative experiments demonstrate the effectiveness of the proposed ADoGT.},
  archive  = {J},
  author   = {Yuan Gao and Longbiao Wang and Jiaxing Liu and Jianwu Dang and Shogo Okada},
  doi      = {10.1109/TAFFC.2023.3290795},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {697-708},
  title    = {Adversarial domain generalized transformer for cross-corpus speech emotion recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic alignment and fusion of multimodal physiological
patterns for stress recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(2), 685–696. (<a
href="https://doi.org/10.1109/TAFFC.2023.3290177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Stress has been identified as one of major causes of health issues. To detect the stress levels with higher accuracy, fusion of multimodal physiological signals is a promising technique. However, there is an asynchrony between physiological signals observed from different perspectives. Exploring the temporal alignment relationship between modalities is helpful to improve the quality of multimodal fusion. This paper proposes an end-to-end multimodal stress detection model based on Bidirectional Cross- and Self-modal Attention (BCSA) mechanism. Specifically, we first construct different feature extractors based on the characteristics of Blood Volume Pulse (BVP) and Electrodermal Activity (EDA) to complete automated temporal feature extraction. Second, cross-modal attention is used to seek the alignment relationship between the two modalities and fully fuse cross-modal information. The self-modal attention is used to attenuate noise and redundant information, highlight important information and obtain salient stress representations. Finally, the stress representations of the two modalities are processed separately, and the mean square error (MSE) is used to narrow the gap between them. Experimental results on the UBFC-Phys dataset and WESAD dataset show that the proposed model can effectively improve the accuracy of stress recognition, and outperforms several state-of-the-art methods.},
  archive  = {J},
  author   = {Xiaowei Zhang and Xiangyu Wei and Zhongyi Zhou and Qiqi Zhao and Sipo Zhang and Yikun Yang and Rui Li and Bin Hu},
  doi      = {10.1109/TAFFC.2023.3290177},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {685-696},
  title    = {Dynamic alignment and fusion of multimodal physiological patterns for stress recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fine-grained interpretability for EEG emotion recognition:
Concat-aided grad-CAM and systematic brain functional network. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(2), 671–684. (<a
href="https://doi.org/10.1109/TAFFC.2023.3288885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {EEG emotion recognition plays a significant role in various mental health services. Deep learning-based methods perform excellently, but still suffer from interpretability. Although methods such as Gradient-weighted Class Activation Mapping(Grad-CAM) can cope with the above problem, their coarse granularity cannot accurately reveal the mechanism to promote emotional intelligence. In this paper, fine-grained interpretability is proposed, called Concat-aided Grad-CAM. Specifically, the multi-level feature mapping before the fully connected layer is concatenated to obtain the gradients of the target concept so that the discriminant information can be directly located in the high-precision area. Unlike coarse-grained interpretability methods applied in EEG emotion recognition, it can accurately highlight the EEG channels related to emotion rather than an obscure area. In addition, a systematic brain functional network is proposed to reveal the relationship between those channels and to further improve emotion recognition performance. The channels with greater contributions are connected, and those connections are learned by dynamic graph convolutional networks, while the others are independent to eliminate interference. Experiments on four EEG emotion recognition datasets manifest that Concat-aided Grad-CAM can be interpreted by the fine-grained. In addition, it has been shown that the learned brain functional network can improve the performance of the baselines. Significantly, the experiment results achieve state-of-the-art performance in multiple experiments.},
  archive  = {J},
  author   = {Bingxiu Liu and Jifeng Guo and C. L. Philip Chen and Xia Wu and Tong Zhang},
  doi      = {10.1109/TAFFC.2023.3288885},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {671-684},
  title    = {Fine-grained interpretability for EEG emotion recognition: Concat-aided grad-CAM and systematic brain functional network},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PR-PL: A novel prototypical representation based pairwise
learning framework for emotion recognition using EEG signals. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(2), 657–670. (<a
href="https://doi.org/10.1109/TAFFC.2023.3288118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Affective brain-computer interface based on electroencephalography (EEG) is an important branch in the field of affective computing. However, the individual differences in EEG emotional data and the noisy labeling problem in the subjective feedback seriously limit the effectiveness and generalizability of existing models. To tackle these two critical issues, we propose a novel transfer learning framework with Prototypical Representation based Pairwise Learning ( PR-PL ). The discriminative and generalized EEG features are learned for emotion revealing across individuals and the emotion recognition task is formulated as pairwise learning for improving the model tolerance to the noisy labels. More specifically, a prototypical learning is developed to encode the inherent emotion-related semantic structure of EEG data and align the individuals’ EEG features to a shared common feature space under consideration of the feature separability of both source and target domains. Based on the aligned feature representations, pairwise learning with an adaptive pseudo labeling method is introduced to encode the proximity relationships among samples and alleviate the label noises effect on modeling. Extensive results on two benchmark databases (SEED and SEED-IV) under four different cross-validation evaluation protocols validate the model reliability and stability across subjects and sessions. Compared to the literature, the average enhancement of emotion recognition across four different evaluation protocols is 2.04% (SEED) and 2.58% (SEED-IV).},
  archive  = {J},
  author   = {Rushuang Zhou and Zhiguo Zhang and Hong Fu and Li Zhang and Linling Li and Gan Huang and Fali Li and Xin Yang and Yining Dong and Yuan-Ting Zhang and Zhen Liang},
  doi      = {10.1109/TAFFC.2023.3288118},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {657-670},
  title    = {PR-PL: A novel prototypical representation based pairwise learning framework for emotion recognition using EEG signals},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RLWOA-SOFL: A new learning model-based reinforcement swarm
intelligence and self-organizing deep fuzzy rules for fMRI pain
decoding. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(2), 644–656. (<a
href="https://doi.org/10.1109/TAFFC.2023.3285997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Pain is highly subjective, so it is always desirable to develop objective pain assessment methods. Brain imaging techniques, such as functional magnetic resonance imaging (fMRI), have the potential to provide a physiological and quantitative pain assessment tool. However, the ultra-high-dimensional fMRI data and the nonlinear relationship between fMRI and pain greatly degrade the efficiency of fMRI-based pain decoding models. In this article, a novel pain decoding model is proposed based on the whale optimization algorithm (WOA), reinforcement learning (RL), and self-organizing fuzzy logic (SOFL), namely RLWOA-SOFL. The new non-linear WOA method incorporates RL and repository experiences (RE), which is based on a back-propagation neural network (BPNN) to map a set of agents states to appropriate actions, to extract and select features that are highly predictive of pain. More specifically, the proposed RLWOA is self-learning and self-optimizing so it can deal with the high-dimensional and complex fMRI data. On the other hand, to establish a fMRI-based pain decoding model, a novel SOFL method is proposed as a new type of deep fuzzy rule that can learn continuously from new data and identify prototypes to construct fuzzy rules. The proposed RLWOA-SOFL model is applied to real-world pain-evoked fMRI data, and the results show that the new model can decode pain intensity more accurately and can identify pain-related fMRI patterns more reliably. Therefore, the proposed RLWOA-SOFL model has great potential to evaluate the intensity of pain perception in clinical uses.},
  archive  = {J},
  author   = {Ahmed M. Anter and Zhiguo Zhang},
  doi      = {10.1109/TAFFC.2023.3285997},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {644-656},
  title    = {RLWOA-SOFL: A new learning model-based reinforcement swarm intelligence and self-organizing deep fuzzy rules for fMRI pain decoding},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Belief mining in persian texts based on deep learning and
users’ opinions. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(2), 632–643. (<a
href="https://doi.org/10.1109/TAFFC.2023.3288407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Belief mining and the study of public opinion provide valuable information. Analyzing the feelings and belief mining of social media data leads to understanding users&#39; opinions and has wide applications in decision making and policymaking. This article applies a new method based on deep learning to solve the problems of belief mining for Persian comments on the Twitter. In this method, first, the data is preprocessed with a deep neural network and then classified into political, cultural, economic, and sports classes, and the sentimental polarity is obtained. SentiPers is applied on four different datasets from Persian Twitter, Digikala store, Google translator, and synonym for evaluation. Then the results are compared with other machine learning and deep learning methods such as neural network, support vector machine, DNN, CNN, and LSTM. Python software has been used to implement this method. The accuracy of the proposed word embedding method for LSTM, CNN, DNN on the SentiPres dataset is 0.931, 0.923, 0.916 respectively. For the TF-IDF method, it is 0.837, 0.863, 0.883 respectively. that the accuracy of LSTM-WSD, CNN-WSD model has increased by 8% and 6% compared to TF-IDF. The results show that the LSTM and Word embedding methods work best.},
  archive  = {J},
  author   = {Hossein Alikarami and Amir Massoud Bidgoli and Hamid Haj Seyed Javadi},
  doi      = {10.1109/TAFFC.2023.3288407},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {632-643},
  title    = {Belief mining in persian texts based on deep learning and users&#39; opinions},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Disagreement matters: Exploring internal diversification for
redundant attention in generic facial action analysis. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(2), 620–631. (<a
href="https://doi.org/10.1109/TAFFC.2023.3286838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper demonstrates the effectiveness of a diversification mechanism for building a more robust multi-attention system in generic facial action analysis. While previous multi-attention (e.g., visual attention and self-attention) research on facial expression recognition (FER) and Action Unit (AU) detection have been thoroughly studied to focus on “external attention diversification”, where attention branches localize different facial areas, we delve into the realm of “internal attention diversification” and explore the impact of diverse attention patterns within the same Region of Interest (RoI). Our experiments reveal that variability in attention patterns significantly impacts model performance, indicating that unconstrained multi-attention plagued by redundancy and over-parameterization, leading to sub-optimal results. To tackle this issue, we propose a compact module that guides the model to achieve self-diversified multi-attention. Our method is applied to both CNN-based and Transformer-based models, benchmarked on popular databases such as BP4D and DISFA for AU detection, as well as CK+, MMI, BU-3DFE, and BP4D+ for facial expression recognition. We also evaluate the mechanism on Self-attention and Channel-wise attention designs for improving their adaptive capabilities in multi-modal feature fusion tasks. The multi-modal evaluation is conducted on BP4D, BP4D+, and our newly developed large-scale comprehensive emotion database BP4D++, which contains well-synchronized and aligned sensor modalities, addressing the scarcity of annotations and identities in human affective computing. We plan to release the new database to the research community, fostering further advancements in this field.},
  archive  = {J},
  author   = {Xiaotian Li and Zheng Zhang and Xiang Zhang and Taoyue Wang and Zhihua Li and Huiyuan Yang and Umur Ciftci and Qiang Ji and Jeffrey Cohn and Lijun Yin},
  doi      = {10.1109/TAFFC.2023.3286838},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {620-631},
  title    = {Disagreement matters: Exploring internal diversification for redundant attention in generic facial action analysis},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MGEED: A multimodal genuine emotion and expression detection
database. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(2), 606–619. (<a
href="https://doi.org/10.1109/TAFFC.2023.3286351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multimodal emotion recognition has attracted increasing interest from academia and industry in recent years, since it enables emotion detection using various modalities, such as facial expression images, speech and physiological signals. Although research in this field has grown rapidly, it is still challenging to create a multimodal database containing facial electrical information due to the difficulty in capturing natural and subtle facial expression signals, such as optomyography (OMG) signals. To this end, we present a newly developed Multimodal Genuine Emotion and Expression Detection (MGEED) database in this paper, which is the first publicly available database containing the facial OMG signals. MGEED consists of 17 subjects with over 150 K facial images, 140 K depth maps and different modalities of physiological signals including OMG, electroencephalography (EEG) and electrocardiography (ECG) signals. The emotions of the participants are evoked by video stimuli and the data are collected by a multimodal sensing system. With the collected data, an emotion recognition method is developed based on multimodal signal synchronisation, feature extraction, fusion and emotion prediction. The results show that superior performance can be achieved by fusing the visual, EEG and OMG features.},
  archive  = {J},
  author   = {Yiming Wang and Hui Yu and Weihong Gao and Yifan Xia and Charles Nduka},
  doi      = {10.1109/TAFFC.2023.3286351},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {606-619},
  title    = {MGEED: A multimodal genuine emotion and expression detection database},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transformer-augmented network with online label correction
for facial expression recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(2), 593–605. (<a
href="https://doi.org/10.1109/TAFFC.2023.3285231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Facial expression recognition (FER) in the wild is extremely challenging due to occlusions, variant head poses under unconstrained conditions and incorrect annotations (e.g., label noise). In this article, we aim to improve the performance of in-the-wild FER with Transformers and online label correction. Different from pure CNNs based methods, we propose a Transformer-augmented network (TAN) to dynamically capture the relationships within each facial patch and across the facial patches. Specifically, the TAN translates a number of facial patch images into a set of visual feature sequences by a backbone convolutional neural network. The intra-patch Transformer is subsequently utilized to capture the most discriminative features within each visual feature sequence. The position-disentangled attention mechanism of the intra-patch Transformer is proposed to better incorporate the positional information for feature sequences. Furthermore, we propose the inter-patch Transformer to model the dependencies across these feature sequences. More importantly, we present the online label correction (OLC) framework to correct suspicious hard labels and accumulate soft labels based on the predictions of the model, which strengthens the robustness of our model against label noise. We validate our method on several widely-used datasets (RAF-DB, FERPlus, AffectNet), realistic occlusion and pose variation datasets, and synthetic noisy datasets. Extensive experiments on these benchmarks demonstrate that the proposed method performs favorably against state-of-the-art methods. The source code will be made publicly available.},
  archive  = {J},
  author   = {Fuyan Ma and Bin Sun and Shutao Li},
  doi      = {10.1109/TAFFC.2023.3285231},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {593-605},
  title    = {Transformer-augmented network with online label correction for facial expression recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). End-to-end label uncertainty modeling in speech emotion
recognition using bayesian neural networks and label distribution
learning. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(2), 579–592. (<a
href="https://doi.org/10.1109/TAFFC.2023.3283595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {To train machine learning algorithms to predict emotional expressions in terms of arousal and valence, annotated datasets are needed. However, as different people perceive others’ emotional expressions differently, their annotations are subjective. To account for this, annotations are typically collected from multiple annotators and averaged to obtain ground-truth labels. However, when exclusively trained on this averaged ground-truth, the model is agnostic to the inherent subjectivity in emotional expressions. In this work, we therefore propose an end-to-end Bayesian neural network capable of being trained on a distribution of annotations to also capture the subjectivity-based label uncertainty. Instead of a Gaussian, we model the annotation distribution using Student&#39;s $t$ -distribution, which also accounts for the number of annotations available. We derive the corresponding Kullback-Leibler divergence loss and use it to train an estimator for the annotation distribution, from which the mean and uncertainty can be inferred. We validate the proposed method using two in-the-wild datasets. We show that the proposed $t$ -distribution based approach achieves state-of-the-art uncertainty modeling results in speech emotion recognition, and also consistent results in cross-corpora evaluations. Furthermore, analyses reveal that the advantage of a $t$ -distribution over a Gaussian grows with increasing inter-annotator correlation and a decreasing number of annotations available.},
  archive  = {J},
  author   = {Navin Raj Prabhu and Nale Lehmann-Willenbrock and Timo Gerkmann},
  doi      = {10.1109/TAFFC.2023.3283595},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {579-592},
  title    = {End-to-end label uncertainty modeling in speech emotion recognition using bayesian neural networks and label distribution learning},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generating multiple 4D expression transitions by learning
face landmark trajectories. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(2), 566–578. (<a
href="https://doi.org/10.1109/TAFFC.2023.3280671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, we address the problem of 4D facial expressions generation. This is usually addressed by animating a neutral 3D face to reach an expression peak, and then get back to the neutral state. In the real world though, people show more complex expressions, and switch from one expression to another. We thus propose a new model that generates transitions between different expressions, and synthesizes long and composed 4D expressions. This involves three sub-problems: (1) modeling the temporal dynamics of expressions, (2) learning transitions between them, and (3) deforming a generic mesh. We propose to encode the temporal evolution of expressions using the motion of a set of 3D landmarks, that we learn to generate by training a manifold-valued GAN (Motion3DGAN). To allow the generation of composed expressions, this model accepts two labels encoding the starting and the ending expressions. The final sequence of meshes is generated by a Sparse2Dense mesh Decoder (S2D-Dec) that maps the landmark displacements to a dense, per-vertex displacement of a known mesh topology. By explicitly working with motion trajectories, the model is totally independent from the identity. Extensive experiments on five public datasets show that our proposed approach brings significant improvements with respect to previous solutions, while retaining good generalization to unseen data.},
  archive  = {J},
  author   = {Naima Otberdout and Claudio Ferrari and Mohamed Daoudi and Stefano Berretti and Alberto Del Bimbo},
  doi      = {10.1109/TAFFC.2023.3280671},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {566-578},
  title    = {Generating multiple 4D expression transitions by learning face landmark trajectories},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-task momentum distillation for multimodal sentiment
analysis. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(2), 549–565. (<a
href="https://doi.org/10.1109/TAFFC.2023.3282410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the field of Multimodal Sentiment Analysis (MSA), the prevailing methods are devoted to developing intricate network architectures to capture the intra- and inter-modal dynamics, which necessitates numerous parameters and poses more difficulties in terms of interpretability in multimodal modeling. Besides, the heterogeneous nature of multiple modalities (text, audio, and vision) introduces significant modality gaps, thereby making multimodal representation learning an ongoing challenge. To address the aforementioned issues, by considering the learning process of modalities as multiple subtasks, we propose a novel approach named Multi-Task Momentum Distillation (MTMD) which succeeds in reducing the gap among different modalities. Specifically, according to the abundance of semantic information, we treat the subtasks of textual and multimodal representations as the teacher networks while the subtasks of acoustic and visual representations as the student ones to present knowledge distillation, which transfers the sentiment-related knowledge guided by the regression and classification subtasks. Additionally, we adopt unimodal momentum models to explore modality-specific knowledge deeply and employ adaptive momentum fusion factors to learn a robust multimodal representation. Furthermore, we provide a theoretical perspective of mutual information maximization by interpreting MTMD as generating sentiment-related views in various ways. Extensive experiments illustrate the superiority of our approach compared with the state-of-the-art methods in MSA.},
  archive  = {J},
  author   = {Ronghao Lin and Haifeng Hu},
  doi      = {10.1109/TAFFC.2023.3282410},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {549-565},
  title    = {Multi-task momentum distillation for multimodal sentiment analysis},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Are 3D face shapes expressive enough for recognising
continuous emotions and action unit intensities? <em>IEEE Transactions
on Affective Computing</em>, <em>15</em>(2), 535–548. (<a
href="https://doi.org/10.1109/TAFFC.2023.3280530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recognising continuous emotions and action unit (AU) intensities from face videos, requires a spatial and temporal understanding of expression dynamics. Existing works primarily rely on 2D face appearance features to extract such dynamics. This work focuses on a promising alternative based on parametric 3D face alignment models, which disentangle different factors of variation, including expression-induced shape variations. We aim to understand how expressive 3D face shapes are in estimating valence-arousal and AU intensities compared to the state-of-the-art 2D appearance-based models. We benchmark five recent 3D face models: ExpNet, 3DDFA-V2, RingNet, DECA, and EMOCA. In valence-arousal estimation, expression features of 3D face models consistently surpassed previous works and yielded an average concordance correlation of. 745 and. 574 on SEWA and AVEC 2019 CES corpora, respectively. We also study how 3D face shapes performed on AU intensity estimation on BP4D and DISFA datasets, and report that 3D face features were on par with 2D appearance features in recognising AUs 4, 6, 10, 12, and 25, but not the entire set of AUs. To understand this discrepancy, we conduct a correspondence analysis between valence-arousal and AUs, which points out that accurate prediction of valence-arousal may require the knowledge of only a few AUs.},
  archive  = {J},
  author   = {Mani Kumar Tellamekala and Ömer Sümer and Björn W. Schuller and Elisabeth André and Timo Giesbrecht and Michel Valstar},
  doi      = {10.1109/TAFFC.2023.3280530},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {535-548},
  title    = {Are 3D face shapes expressive enough for recognising continuous emotions and action unit intensities?},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Impact of annotation modality on label quality and model
performance in the automatic assessment of laughter in-the-wild.
<em>IEEE Transactions on Affective Computing</em>, <em>15</em>(2),
519–534. (<a href="https://doi.org/10.1109/TAFFC.2023.3269003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Although laughter is known to be a multimodal signal, it is primarily annotated from audio. It is unclear how laughter labels may differ when annotated from modalities like video, which capture body movements and are relevant in in-the-wild studies. In this work we ask whether annotations of laughter are congruent across modalities, and compare the effect that labeling modality has on machine learning model performance. We compare annotations and models for laughter detection, intensity estimation, and segmentation, using a challenging in-the-wild conversational dataset with a variety of camera angles, noise conditions and voices. Our study with 48 annotators revealed evidence for incongruity in the perception of laughter and its intensity between modalities, mainly due to lower recall in the video condition. Our machine learning experiments compared the performance of modern unimodal and multi-modal models for different combinations of input modalities, training, and testing label modalities. In addition to the same input modalities rated by annotators (audio and video), we trained models with body acceleration inputs, robust to cross-contamination, occlusion and perspective differences. Our results show that performance of models with body movement inputs does not suffer when trained with video-acquired labels, despite their lower inter-rater agreement.},
  archive  = {J},
  author   = {Jose David Vargas Quiros and Laura Cabrera-Quiros and Catharine Oertel and Hayley Hung},
  doi      = {10.1109/TAFFC.2023.3269003},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {519-534},
  title    = {Impact of annotation modality on label quality and model performance in the automatic assessment of laughter in-the-wild},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Disentangled variational autoencoder for emotion recognition
in conversations. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(2), 508–518. (<a
href="https://doi.org/10.1109/TAFFC.2023.3280038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In Emotion Recognition in Conversations (ERC), the emotions of target utterances are closely dependent on their context. Therefore, existing works train the model to generate the response of the target utterance, which aims to recognise emotions leveraging contextual information. However, adjacent response generation ignores long-range dependencies and provides limited affective information in many cases. In addition, most ERC models learn a unified distributed representation for each utterance, which lacks interpretability and robustness. To address these issues, we propose a VAD -disentangled V ariational A uto E ncoder (VAD-VAE), which first introduces a target utterance reconstruction task based on Variational Autoencoder, then disentangles three affect representations Valence-Arousal-Dominance (VAD) from the latent space. We also enhance the disentangled representations by introducing VAD supervision signals from a sentiment lexicon and minimising the mutual information between VAD distributions. Experiments show that VAD-VAE outperforms the state-of-the-art model on two datasets. Further analysis proves the effectiveness of each proposed module and the quality of disentangled VAD representations.},
  archive  = {J},
  author   = {Kailai Yang and Tianlin Zhang and Sophia Ananiadou},
  doi      = {10.1109/TAFFC.2023.3280038},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {508-518},
  title    = {Disentangled variational autoencoder for emotion recognition in conversations},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neuro or symbolic? Fine-tuned transformer with unsupervised
LDA topic clustering for text sentiment analysis. <em>IEEE Transactions
on Affective Computing</em>, <em>15</em>(2), 493–507. (<a
href="https://doi.org/10.1109/TAFFC.2023.3279318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {For text sentiment analysis, state-of-the-art neural language models have demonstrated promising performance. However, they lack interpretability, require vast volumes of annotated data, and are typically specialized for tasks. In this article, we explore a connection between fine-tuned Transformer models and unsupervised LDA approach to cope with text sentiment analysis tasks, inspired by the concept of Neuro-symbolic AI. The Transformer and LDA models are combined as a feature extractor to extract the hidden representations of the input text sequences. Subsequently, we employ a feedforward network to forecast various sentiment analysis tasks, such as multi-label emotion prediction, dialogue quality prediction, and nugget detection. Our proposed method obtains the best results in the NTCIR-16 dialogue evaluation (DialEval-2) task, as well as cutting-edge results in emotional intensity prediction using the Ren_CECps corpus. Extensive experiments show that our proposed method is highly explainable, cost-effective in training, and superior in terms of accuracy and robustness.},
  archive  = {J},
  author   = {Fei Ding and Xin Kang and Fuji Ren},
  doi      = {10.1109/TAFFC.2023.3279318},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {493-507},
  title    = {Neuro or symbolic? fine-tuned transformer with unsupervised LDA topic clustering for text sentiment analysis},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards human-compatible autonomous car: A study of
non-verbal turing test in automated driving with affective transition
modelling. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(2), 478–492. (<a
href="https://doi.org/10.1109/TAFFC.2023.3279311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Autonomous cars are indispensable when humans go further down the hands-free route. Although existing literature highlights that the acceptance of the autonomous car will increase if it drives in a human-like manner, sparse research offers the naturalistic experience from a passenger&#39;s seat perspective to examine the humanness of current autonomous cars. The present study tested whether the AI driver could create a human-like ride experience for passengers based on 69 participants’ feedback in a real-road scenario. We designed a ride experience-based version of the non-verbal Turing test for automated driving. Participants rode in autonomous cars (driven by either human or AI drivers) as a passenger and judged whether the driver was human or AI. The AI driver failed to pass our test because passengers detected the AI driver above chance. In contrast, when the human driver drove the car, the passengers’ judgement was around chance. We further investigated how human passengers ascribe humanness in our test. Based on Lewin&#39;s field theory, we advanced a computational model combining signal detection theory with pre-trained language models to predict passengers’ humanness rating behaviour. We employed affective transition between pre-study baseline emotions and corresponding post-stage emotions as the signal strength of our model. Results showed that the passengers’ ascription of humanness would increase with the greater affective transition. Our study suggested an important role of affective transition in passengers’ ascription of humanness, which might become a future direction for autonomous driving.},
  archive  = {J},
  author   = {Zhaoning Li and Qiaoli Jiang and Zhengming Wu and Anqi Liu and Haiyan Wu and Miner Huang and Kai Huang and Yixuan Ku},
  doi      = {10.1109/TAFFC.2023.3279311},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {478-492},
  title    = {Towards human-compatible autonomous car: A study of non-verbal turing test in automated driving with affective transition modelling},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From what you see to what we smell: Linking human emotions
to bio-markers in breath. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(2), 465–477. (<a
href="https://doi.org/10.1109/TAFFC.2023.3275216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Research has shown that the composition of breath can differ based on the human&#39;s behavioral patterns and mental and physical states immediately before being collected. These breath-collection techniques have also been extended to observe the general processes occurring in groups of humans and can link them to what those groups are collectively experiencing. In this research, we applied machine learning techniques to the breath data collected from cinema audiences. These techniques included XGBOOST Regression, Hierarchical Clustering, and Item Basket analyses created using the Apriori algorithm. They were conducted to find associations between the biomarkers in the crowd&#39;s breath and the movie&#39;s audio-visual stimuli and thematic events. This analysis enabled us to directly link what the group was experiencing and their biological response to that experience. We first extracted visual and auditory features from a movie to achieve this. We compared it to the biomarkers in the crowd&#39;s breath using regression and pattern mining techniques. Our results supported the theory that a crowd&#39;s collective experience directly correlates to the biomarkers in the crowd&#39;s breath. Consequently, these findings suggest that visual and auditory experiences have predictable effects on the human body that can be monitored without requiring expensive or invasive neuroimaging techniques.},
  archive  = {J},
  author   = {Joshua Bensemann and Hasnain Cheena and David Tse Jung Huang and Elizabeth Broadbent and Jonathan Williams and Jörg Wicker},
  doi      = {10.1109/TAFFC.2023.3275216},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {465-477},
  title    = {From what you see to what we smell: Linking human emotions to bio-markers in breath},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic context-aware inference of engagement in HMI: A
survey. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(2), 445–464. (<a
href="https://doi.org/10.1109/TAFFC.2023.3278707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Engagement is the process by which participants establish, maintain, and end their perceived connection. Automatic engagement inference is one of the tasks required to develop successful human-centered HMI applications. Engagement is a multi-faceted multimodal construct requiring high accuracy in interpretating contextual, verbal and non-verbal cues, making the development of an intelligent automated engagement inference system challenging. Existing surveys concentrate on specific application settings, and a comprehensive survey covering the different engagement facets, definition and inference across various contexts is lacking. Moreover, despite the importance of context-aware modeling, the literature lacks a systematic context-aware overview on the topic. This paper presents a comprehensive survey on previous work in engagement for HMI, entailing interdisciplinary definition, engagement components, publicly available datasets, ground truth assessment, and commonly used features and methods, serving as a guide for the development of future HMI interfaces with reliable context-aware engagement inference capability. An in-depth review across embodied and disembodied interaction modes, and an emphasis on the interaction context of which engagement is studied sets apart this survey from existing ones. Our findings suggest four important directions for future research: 1) context-aware computational modeling, 2) temporal dynamics, 3) personalised computing, and 4) bias and fairness of engagement inference systems.},
  archive  = {J},
  author   = {Hanan Salam and Oya Celiktutan and Hatice Gunes and Mohamed Chetouani},
  doi      = {10.1109/TAFFC.2023.3278707},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {445-464},
  title    = {Automatic context-aware inference of engagement in HMI: A survey},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards emotion-aware agents for improved user satisfaction
and partner perception in negotiation dialogues. <em>IEEE Transactions
on Affective Computing</em>, <em>15</em>(2), 433–444. (<a
href="https://doi.org/10.1109/TAFFC.2023.3238007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Negotiation is a complex social interaction that encapsulates emotional encounters in human decision-making. Virtual agents that can negotiate with humans by the means of language are useful in pedagogy and conversational AI. To advance the development of such agents, we explore the role of emotion in the prediction of two important subjective goals in a negotiation – outcome satisfaction and partner perception. We devise ways to measure and compare different degrees of emotion expression in negotiation dialogues, consisting of emoticon , lexical , and contextual variables. Through an extensive analysis of a large-scale dataset in chat-based negotiations, we find that incorporating emotion expression explains significantly more variance, above and beyond the demographics and personality traits of the participants. Further, our temporal analysis reveals that emotive information from both early and later stages of the negotiation contributes to this prediction, indicating the need for a continual learning model of capturing emotion for automated agents. Finally, we extend our analysis to another dataset, showing promise that our findings generalize to more complex scenarios. We conclude by discussing our insights, which will be helpful for designing adaptive negotiation agents that interact through realistic communication interfaces.},
  archive  = {J},
  author   = {Kushal Chawla and Rene Clever and Jaysa Ramirez and Gale M. Lucas and Jonathan Gratch},
  doi      = {10.1109/TAFFC.2023.3238007},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {433-444},
  title    = {Towards emotion-aware agents for improved user satisfaction and partner perception in negotiation dialogues},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The deep method: Towards computational modeling of the
social emotion shame driven by theory, introspection, and social
signals. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(2), 417–432. (<a
href="https://doi.org/10.1109/TAFFC.2023.3298062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Understanding emotions is key to Affective Computing. Emotion recognition focuses on the communicative component of emotions encoded in social signals. This view alone is insufficient for a deeper understanding and computational representation of the internal, subjectively experienced component of emotions. This article presents a cognition-based method called Deep as a starting point for deeper computational modeling of the internal component of emotions. Deep incorporates an approach to query individual internal emotional experiences and to represent such information computationally. It combines social signals, verbalized introspection information, context information, and theory-driven knowledge. We apply the Deep method to the emotion of shame as an example and compare it to a typical emotion recognition model, highlighting the differences and advantages.},
  archive  = {J},
  author   = {Tanja Schneeberger and Mirella Hladký and Ann-Kristin Thurner and Jana Volkert and Alexander Heimerl and Tobias Baur and Elisabeth André and Patrick Gebhard},
  doi      = {10.1109/TAFFC.2023.3298062},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {417-432},
  title    = {The deep method: Towards computational modeling of the social emotion shame driven by theory, introspection, and social signals},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Text-based fine-grained emotion prediction. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(2), 405–416. (<a
href="https://doi.org/10.1109/TAFFC.2023.3298405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Text-based emotion prediction is an important task in the field of affective computing. Most prior work has been restricted to predicting emotions corresponding to a few high-level emotion classes. This paper explores and experiments with various techniques for fine-grained (27 classes) emotion prediction $^\dagger$ . In particular, 1) we present a method to incorporate multiple annotations from different raters, 2) we analyze the model&#39;s performance on fused emotion classes and with sub-sampled training data, 3) we present a method to leverage the correlations among the emotion categories, and 4) we propose a new framework for text-based fine-grained emotion prediction through emotion definition modeling. The emotion definition-based model outperforms the existing state-of-the-art for fine-grained emotion dataset GoEmotions. The approach involves a multi-task learning framework that models definitions of emotions as an auxiliary task while being trained on the primary task of emotion prediction. We model definitions using masked language modeling and class definition prediction tasks. We show that this trained model can be used for transfer learning on other benchmark datasets in emotion prediction with varying emotion label sets, domains, and sizes. The proposed models outperform the baselines on transfer learning experiments demonstrating the model&#39;s generalization capability.},
  archive  = {J},
  author   = {Gargi Singh and Dhanajit Brahma and Piyush Rai and Ashutosh Modi},
  doi      = {10.1109/TAFFC.2023.3298405},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {405-416},
  title    = {Text-based fine-grained emotion prediction},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Smiling in the face and voice of avatars and robots:
Evidence for a “smiling McGurk effect.” <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(2), 393–404. (<a
href="https://doi.org/10.1109/TAFFC.2022.3213269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multisensory integration influences emotional perception, as the McGurk effect demonstrates for the communication between humans. Human physiology implicitly links the production of visual features with other modes like the audio channel: Face muscles responsible for a smiling face also stretch the vocal cords that result in a characteristic smiling voice. For artificial agents capable of multimodal expression, this linkage is modeled explicitly. In our studies, we observe the influence of visual and audio channels on the perception of the agents’ emotional expression. We created videos of virtual characters and social robots either with matching or mismatching emotional expressions in the audio and visual channels. In two online studies, we measured the agents’ perceived valence and arousal. Our results consistently lend support to the ‘emotional McGurk effect’ hypothesis, according to which face transmits valence information, and voice transmits arousal. When dealing with dynamic virtual characters, visual information is enough to convey both valence and arousal, and thus audio expressivity need not be congruent. When dealing with robots with fixed facial expressions, however, both visual and audio information need to be present to convey the intended expression.},
  archive  = {J},
  author   = {Ilaria Torre and Simon Holk and Elmira Yadollahi and Iolanda Leite and Rachel McDonnell and Naomi Harte},
  doi      = {10.1109/TAFFC.2022.3213269},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {393-404},
  title    = {Smiling in the face and voice of avatars and robots: Evidence for a ‘Smiling McGurk effect’},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From the lab to the wild: Affect modeling via privileged
information. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(2), 380–392. (<a
href="https://doi.org/10.1109/TAFFC.2023.3265072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {How can we reliably transfer affect models trained in controlled laboratory conditions ( in-vitro ) to uncontrolled real-world settings ( in-vivo )? The information gap between in-vitro and in-vivo applications defines a core challenge of affective computing. This gap is caused by limitations related to affect sensing including intrusiveness, hardware malfunctions and availability of sensors. As a response to these limitations, we introduce the concept of privileged information for operating affect models in real-world scenarios (in the wild). Privileged information enables affect models to be trained across multiple modalities available in a lab, and ignore, without significant performance drops, those modalities that are not available when they operate in the wild. Our approach is tested in two multimodal affect databases one of which is designed for testing models of affect in the wild. By training our affect models using all modalities and then using solely raw footage frames for testing the models, we reach the performance of models that fuse all available modalities for both training and testing. The results are robust across both classification and regression affect modeling tasks which are dominant paradigms in affective computing. Our findings make a decisive step towards realizing affect interaction in the wild.},
  archive  = {J},
  author   = {Konstantinos Makantasis and Kosmas Pinitas and Antonios Liapis and Georgios N. Yannakakis},
  doi      = {10.1109/TAFFC.2023.3265072},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {380-392},
  title    = {From the lab to the wild: Affect modeling via privileged information},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guest editorial best of ACII 2021. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(2), 376–379. (<a
href="https://doi.org/10.1109/TAFFC.2024.3389249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The 9TH AAAC Conference on Affective Computing and Intelligent Interaction 2021 was held in a virtual format in the fall of 2021. It was technically co-sponsored by the IEEE Computer Society and featured the recent work on Affective Computing. The six best papers from this conference were selected by the technical program chairs. They were invited to submit their extended version to be considered for this special section at the IEEE Transactions on Affective Computing. Each submission was reviewed by at least three expert reviewers and was evaluated in terms of overall contribution and the adequacy of the additional content to warrant a new article. This special section features five accepted submissions whose major contributions are summarized below.},
  archive  = {J},
  author   = {Mohammad Soleymani and Shiro Kumano and Emily Mower Provost and Nadia Bianchi-Berthouze and Akane Sano and Kenji Suzuki},
  doi      = {10.1109/TAFFC.2024.3389249},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {376-379},
  title    = {Guest editorial best of ACII 2021},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review of tools and methods for detection, analysis, and
prediction of allostatic load due to workplace stress. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(1), 357–375. (<a
href="https://doi.org/10.1109/TAFFC.2023.3273201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Chronic stress risks an individual&#39;s overall well-being. Chronic stress is associated with allostatic load, the body&#39;s wear-and-tear due to prolonged heightened physiological and psychological states. Increased allostatic load among workers increases their risk of injuries and the likelihood of diseases and illnesses. An allostatic load model could explain the basis of a stress response. Stress research in affective computing uses wearable devices, data processing algorithms, and machine learning methods to create models that could benefit from an allostatic load model of stress. We emphasize the need for the allostatic load model in affective computing to create disease and illness prediction models. Predictive models could enhance safeguards in the workplace by helping to create proactive mitigation strategies against chronic stress. First, we briefly introduce allostasis’ physiological and psychological basis. Next, we reviewed stress studies within affective computing that may benefit from an allostatic load model of stress. We focused our review on studies conducted in dynamic settings, such as the workplace, and those incorporating typical stress study elements in affective computing. We conclude our review by identifying gaps between affective computing and neuroscientific stress studies and provide recommendations for adopting the allostatic load model of stress.},
  archive  = {J},
  author   = {Karl Magtibay and Karthikeyan Umapathy},
  doi      = {10.1109/TAFFC.2023.3273201},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {357-375},
  title    = {A review of tools and methods for detection, analysis, and prediction of allostatic load due to workplace stress},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic deceit detection through multimodal analysis of
high-stake court-trials. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(1), 342–356. (<a
href="https://doi.org/10.1109/TAFFC.2023.3322331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article we propose the use of convolutional self-attention for attention-based representation learning, while replacing traditional vectorization methods with a transformer as the backbone of our speech model for transfer learning within our automatic deceit detection framework. This design performs a multimodal data analysis and applies fusion to merge visual, vocal, and speech(textual) channels; reporting deceit predictions. Our experimental results show that the proposed architecture improves the state-of-the-art on the popular Real-Life Trial (RLT) dataset in terms of correct classification rate. To further assess the generalizability of our design, we experiment on the low-stakes Box of Lies (BoL) dataset and achieve state-of-the-art performance as well as providing cross-corpus comparisons. Following our analysis, we report that (1) convolutional self-attention learns meaningful representations while performing joint attention computation for deception, (2) apparent deceptive intent is a continuous function of time and subjects can display varying levels of apparent deceptive intent throughout recordings, and (3), in support of criminal psychology findings, studying abnormal behavior out of context can be an unreliable way to predict deceptive intent.},
  archive  = {J},
  author   = {Berat Biçer and Hamdi Dibeklioğlu},
  doi      = {10.1109/TAFFC.2023.3322331},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {342-356},
  title    = {Automatic deceit detection through multimodal analysis of high-stake court-trials},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A quantum probability driven framework for joint multi-modal
sarcasm, sentiment and emotion analysis. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(1), 326–341. (<a
href="https://doi.org/10.1109/TAFFC.2023.3279145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Sarcasm, sentiment, and emotion are three typical kinds of spontaneous affective responses of humans to external events and they are tightly intertwined with each other. Such events may be expressed in multiple modalities (e.g., linguistic, visual and acoustic), e.g., multi-modal conversations. Joint analysis of humans’ multi-modal sarcasm, sentiment, and emotion is an important yet challenging topic, as it is a complex cognitive process involving both cross-modality interaction and cross-affection correlation. From the probability theory perspective, cross-affection correlation also means that the judgments on sarcasm, sentiment, and emotion are incompatible. However, this exposed phenomenon cannot be sufficiently modelled by classical probability theory due to its assumption of compatibility. Neither do the existing approaches take it into consideration. In view of the recent success of quantum probability (QP) in modeling human cognition, particularly contextual incompatible decision making, we take the first step towards introducing QP into joint multi-modal sarcasm, sentiment, and emotion analysis. Specifically, we propose a QU antum probab I lity driven multi-modal sarcasm, s E ntiment and emo T ion analysis framework, termed QUIET. Extensive experiments on two datasets and the results show that the effectiveness and advantages of QUIET in comparison with a wide range of the state-of-the-art baselines. We also show the great potential of QP in multi-affect analysis.},
  archive  = {J},
  author   = {Yaochen Liu and Yazhou Zhang and Dawei Song},
  doi      = {10.1109/TAFFC.2023.3279145},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {326-341},
  title    = {A quantum probability driven framework for joint multi-modal sarcasm, sentiment and emotion analysis},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient multimodal transformer with dual-level feature
restoration for robust multimodal sentiment analysis. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(1), 309–325. (<a
href="https://doi.org/10.1109/TAFFC.2023.3274829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the proliferation of user-generated online videos, Multimodal Sentiment Analysis (MSA) has attracted increasing attention recently. Despite significant progress, there are still two major challenges on the way towards robust MSA: 1) inefficiency when modeling cross-modal interactions in unaligned multimodal data; and 2) vulnerability to random modality feature missing which typically occurs in realistic settings. In this paper, we propose a generic and unified framework to address them, named Efficient Multimodal Transformer with Dual-Level Feature Restoration (EMT-DLFR). Concretely, EMT employs utterance-level representations from each modality as the global multimodal context to interact with local unimodal features and mutually promote each other. It not only avoids the quadratic scaling cost of previous local-local cross-modal interaction methods but also leads to better performance. To improve model robustness in the incomplete modality setting, on the one hand, DLFR performs low-level feature reconstruction to implicitly encourage the model to learn semantic information from incomplete data. On the other hand, it innovatively regards complete and incomplete data as two different views of one sample and utilizes siamese representation learning to explicitly attract their high-level representations. Comprehensive experiments on three popular datasets demonstrate that our method achieves superior performance in both complete and incomplete modality settings.},
  archive  = {J},
  author   = {Licai Sun and Zheng Lian and Bin Liu and Jianhua Tao},
  doi      = {10.1109/TAFFC.2023.3274829},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {309-325},
  title    = {Efficient multimodal transformer with dual-level feature restoration for robust multimodal sentiment analysis},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Crowdsourcing affective annotations via fNIRS-BCI. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(1), 297–308. (<a
href="https://doi.org/10.1109/TAFFC.2023.3273916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Affective annotation refers to the process of labeling media content based on the emotions they evoke. Since such experiences are inherently subjective and depend on individual differences, the central challenge is associating digital content with its affective, interindividual experience. Here, we present a first-of-its-kind methodology for affective annotation directly from brain signals by monitoring the affective experience of a crowd of individuals via functional near-infrared spectroscopy (fNIRS). An experiment is reported in which fNIRS was recorded from 31 participants to develop a brain-computer interface (BCI) for affective annotation. Brain signals evoked by images were used to draw predictions about the affective dimensions that characterize the stimuli. By combining annotations, the results show that monitoring crowd responses can draw accurate affective annotations, with performance improving significantly with increases in crowd size. Our methodology demonstrates a proof-of-concept to source affective annotations from a crowd of BCI users without requiring any auxiliary mental or physical interaction.},
  archive  = {J},
  author   = {Tuukka Ruotsalo and Kalle Mäkelä and Michiel Spapé},
  doi      = {10.1109/TAFFC.2023.3273916},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {297-308},
  title    = {Crowdsourcing affective annotations via fNIRS-BCI},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WavDepressionNet: Automatic depression level prediction via
raw speech signals. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(1), 285–296. (<a
href="https://doi.org/10.1109/TAFFC.2023.3272553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Physiological reports have confirmed that there are differences in speech signals between depressed and healthy individuals. Therefore, as an application in the field of affective computing, automatic depression level prediction through speech signals has received the attention of researchers, which often estimate the depression severity of individuals by the Fourier or Mel spectrograms of speech signals. However, some studies on speech emotion recognition suggest that directly modeling the raw speech signal is more helpful for extracting emotion-related information. Inspired by this fact, we develop a WavDepressionNet to model raw speech signals for the improvement of prediction accuracy. In our method, a representation block is proposed to find a set of basis vectors to construct the optimal transformation space and generate the transformation result (named Depression Feature Map, DFM) of speech signal for facilitating the perception of depression cues. We further propose an assessment block, which cannot only use the designed spatiotemporal self-calibration mechanism to calibrate the DFM and highlight the useful elements, but also aggregates the calibrated DFM across various temporal ranges with the dilated convolution. Experimental results on the AVEC 2013 and AVEC 2014 depression databases demonstrate the effectiveness of our approach over previous works.},
  archive  = {J},
  author   = {Mingyue Niu and Jianhua Tao and Yongwei Li and Yong Qin and Ya Li},
  doi      = {10.1109/TAFFC.2023.3272553},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {285-296},
  title    = {WavDepressionNet: Automatic depression level prediction via raw speech signals},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Empirical validation of an agent-based model of emotion
contagion. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(1), 273–284. (<a
href="https://doi.org/10.1109/TAFFC.2023.3272031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In recent years, many agent-based models of human groups have implemented a mechanism of emotion contagion, yet empirical validation is lagging behind. The aim of the present paper is to validate an agent-based model of emotion contagion at the level of group emotion, by comparing simulations against the emotional development of real people in small groups. To study the effect of emotion contagion, the participants interacted via a video call, where they were virtually placed in different social environments while they played a quiz. This allowed the exchange of emotion among all, some or none of the participants. The patterns of emotional development in the empirical results supported our hypotheses based on literature of emotion contagion and social norms. Further, the simulations with the complete model resembled many of these patterns. When emotion contagion was disabled in the model, the resemblance decreased. These results give a first indication that emotion contagion occurs in groups that meet via video calls, and can in-part be predicted by the proposed model of emotion contagion. Yet, further study with a larger and more diverse empirical sample is needed, as well as comparisons across contagion mechanisms, to draw stronger conclusions and ultimately justify societal application.},
  archive  = {J},
  author   = {Erik Stefan van Haeringen and Emmeke Anna Veltmeijer and Charlotte Gerritsen},
  doi      = {10.1109/TAFFC.2023.3272031},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {273-284},
  title    = {Empirical validation of an agent-based model of emotion contagion},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The role of preprocessing for word representation learning
in affective tasks. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(1), 254–272. (<a
href="https://doi.org/10.1109/TAFFC.2023.3270115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Affective tasks, including sentiment analysis, emotion classification, and sarcasm detection have drawn a lot of attention in recent years due to a broad range of useful applications in various domains. The main goal of affect detection tasks is to recognize states such as mood, sentiment, and emotions from textual data (e.g., news articles or product reviews). Despite the importance of utilizing preprocessing steps in different stages (i.e., word representation learning and building a classification model) of affect detection tasks, this topic has not been studied well. To that end, we explore whether applying various preprocessing methods (stemming, lemmatization, stopword removal, punctuation removal and so on) and their combinations in different stages of the affect detection pipeline can improve the model performance. The are many preprocessing approaches that can be utilized in affect detection tasks. However, their influence on the final performance depends on the type of preprocessing and the stages that they are applied. Moreover, the preprocessing impacts vary across different affective tasks. Our analysis provides thorough insights into how preprocessing steps can be applied in building an effect detection pipeline and their respective influence on performance.},
  archive  = {J},
  author   = {Nastaran Babanejad and Heidar Davoudi and Ameeta Agrawal and Aijun An and Manos Papagelis},
  doi      = {10.1109/TAFFC.2023.3270115},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {254-272},
  title    = {The role of preprocessing for word representation learning in affective tasks},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pose-aware facial expression recognition assisted by
expression descriptions. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(1), 241–253. (<a
href="https://doi.org/10.1109/TAFFC.2023.3267774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Although expression descriptions provide additional information about facial behaviors despite of different poses, and pose features are beneficial to adapt to pose variety, neither has been fully leveraged in facial expression recognition. This paper proposes a pose-aware text-assisted facial expression recognition method using cross-modality attention. Specifically, the method contains three components. The pose feature extractor extracts pose-related features from facial images, and then cooperates with a fully-connected layer for pose classification. When poses can be clearly discriminated and classified, features obtained from the extractor can represent the corresponding poses. To eliminate bias due to appearance and illumination, cluster centers are taken as the final pose features. The text feature extractor obtains embeddings from expression descriptions. These descriptions are first passed through Intra-Exp attention to obtain preliminary embeddings. To leverage the correlations among expressions, all expression embeddings are then concatenated and passed through Inter-Exp attention. The cross-modality module attempts to learn attention maps that distinguish the importance of facial regions by using prior knowledge about poses and expression descriptions. The image features weighted by the attention maps are utilized to recognize pose and expression jointly. Experiments on three benchmark datasets demonstrate the superiority of the proposed method.},
  archive  = {J},
  author   = {Shangfei Wang and Yi Wu and Yanan Chang and Guoming Li and Meng Mao},
  doi      = {10.1109/TAFFC.2023.3267774},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {241-253},
  title    = {Pose-aware facial expression recognition assisted by expression descriptions},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LGSNet: A two-stream network for micro- and macro-expression
spotting with background modeling. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(1), 223–240. (<a
href="https://doi.org/10.1109/TAFFC.2023.3266808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Micro- and macro-expression spotting in an untrimmed video is a challenging task, due to the mass generation of false positive samples. Most existing methods localize higher response areas by extracting hand-crafted features or cropping specific regions from all or some key raw images. However, these methods either neglect the continuous temporal information or model the inherent human motion paradigms (background) as foreground. Consequently, we propose a novel two-stream network, named Local suppression and Global enhancement Spotting Network (LGSNet), which takes segment-level features from optical flow and videos as input. LGSNet adopts anchors to encode expression intervals and selects the encoded deviations as the object of optimization. Furthermore, we introduce a Temporal Multi-Receptive Field Feature Fusion Module (TMRF $^{3}$ M) and a Local Suppression and Global Enhancement Module (LSGEM), which help spot short intervals more precisely and suppress background information. To further highlight the differences between positive and negative samples, we set up a large number of random pseudo ground truth intervals (background clips) on some discarded sliding windows to accomplish background clips modeling to counteract the effect of non-expressive face and head movements. Experimental results show that our proposed network achieves state-of-the-art performance on the CAS(ME) $^{2}$ , CAS(ME) $^{3}$ and SAMM-LV datasets.},
  archive  = {J},
  author   = {Wang-Wang Yu and Jingwen Jiang and Kai-Fu Yang and Hong-Mei Yan and Yong-Jie Li},
  doi      = {10.1109/TAFFC.2023.3266808},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {223-240},
  title    = {LGSNet: A two-stream network for micro- and macro-expression spotting with background modeling},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-day data diversity improves inter-individual emotion
commonality of spatio-spectral EEG signatures using independent
component analysis. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(1), 210–222. (<a
href="https://doi.org/10.1109/TAFFC.2023.3261867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Electroencephalogram (EEG) variability poses a great challenge to the affective brain-computer interface (aBCI) for practical applications. Most aBCI frameworks have been demonstrated successfully but deliberated on single-day data, which can be realistically susceptible to psychophysiological changes and further hinder the exploration of inter-individual EEG commonality. This study proposes a multiple-day scenario that learns exclusively from diverse EEG correlates of emotional responses on different days (i.e., enriched data diversity) by using a unified independent components analysis framework. Given an eight-day dataset of 10 subjects (i.e., 80 sessions), the results demonstrated that the multiple-day scenario intensified the inter-subject emotion commonality (i.e., the percentage of subjects with the same signature) to a certain extent when considering sufficient cross-day sessions, whereas the most commonly adopted single-day analysis (i.e., diversity-confined) led to session-dependent inferior outcomes. Given the best case, the emotional valence dimension was associated with relatively reproducible frontal beta, central midline gamma, and occipital beta modulations with 30%–40% subject commonality, whereas the arousal counterpart suffered more substantially from EEG variability and barely returned representative signatures. These results suggest that EEG signature representation may be substantially compromised by limited data diversity, impeding the efficacy and generalizability of the aBCI model in real-life settings.},
  archive  = {J},
  author   = {Yi-Wei Shen and Yuan-Pin Lin},
  doi      = {10.1109/TAFFC.2023.3261867},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {210-222},
  title    = {Cross-day data diversity improves inter-individual emotion commonality of spatio-spectral EEG signatures using independent component analysis},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling uncertainty for low-resolution facial expression
recognition. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(1), 198–209. (<a
href="https://doi.org/10.1109/TAFFC.2023.3264719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, facial expression recognition techniques have made significant progress on high-resolution web images. However, in real-world applications, the obtained images are often with low resolution since they are mostly captured in a wide range of public spaces. As a result, the ambiguity of the expression labels hinders recognition performance due to not only subjective emotion annotations but also ambiguous images. Existing approaches tend to perform poorly when the resolution of face images decreases. In this work, we aim to model the aleatoric uncertainty induced by low-image-resolution and label ambiguity for robust facial expression recognition. We propose probabilistic data uncertainty learning to capture the ambiguity induced by poor image resolution. Additionally, we introduce the emotion wheel to learn the label-uncertainty-aware embedding. Moreover, we exploit the ambiguous nature of neutrality and propose a neutral expression constraint to learn more robust features for facial expression recognition. To the best of our knowledge, this is the first work utilizing the intrinsic nature of neutrality as a regularization to benefit model training. Extensive experimental results show the effectiveness and robustness of our approach. Under low-resolution conditions, our proposed method outperforms the state-of-the-art approaches by 3.02% and 3.16% in terms of accuracy on RAF-DB and FERPlus, respectively.},
  archive  = {J},
  author   = {Ling Lo and Bo-Kai Ruan and Hong-Han Shuai and Wen-Huang Cheng},
  doi      = {10.1109/TAFFC.2023.3264719},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {198-209},
  title    = {Modeling uncertainty for low-resolution facial expression recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data leakage and evaluation issues in micro-expression
analysis. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(1), 186–197. (<a
href="https://doi.org/10.1109/TAFFC.2023.3265063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Micro-expressions have drawn increasing interest lately due to various potential applications. The task is, however, difficult as it incorporates many challenges from the fields of computer vision, machine learning and emotional sciences. Due to the spontaneous and subtle characteristics of micro-expressions, the available training and testing data are limited, which make evaluation complex. We show that data leakage and fragmented evaluation protocols are issues among the micro-expression literature. We find that fixing data leaks can drastically reduce model performance, in some cases even making the models perform similarly to a random classifier. To this end, we go through common pitfalls, propose a new standardized evaluation protocol using facial action units with over 2000 micro-expression samples, and provide an open source library that implements the evaluation protocols in a standardized manner. Code is publicly available in https://github.com/tvaranka/meb .},
  archive  = {J},
  author   = {Tuomas Varanka and Yante Li and Wei Peng and Guoying Zhao},
  doi      = {10.1109/TAFFC.2023.3265063},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {186-197},
  title    = {Data leakage and evaluation issues in micro-expression analysis},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unconstrained facial expression recognition with
no-reference de-elements learning. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(1), 173–185. (<a
href="https://doi.org/10.1109/TAFFC.2023.3263886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Most unconstrained facial expression recognition (FER) methods take original facial images as inputs to learn discriminative features by well-designed loss functions, which cannot reflect important visual information in faces. Although existing methods have explored the visual information of constrained facial expressions, there is no explicit modeling of what visual information is important for unconstrained FER. To find out valuable information of unconstrained facial expressions, we pose a new problem of no-reference de-elements learning: we decompose any unconstrained facial image into the facial expression element and a neutral face without the reference of corresponding neutral faces. Importantly, the element provides visualization results to understand important facial expression information and improves the discriminative power of features. Moreover, we propose a simple yet effective D e- E lements Net work (DENet) to learn the element and introduce appropriate constraints to overcome no ground truth of corresponding neutral faces during the de-elements learning. We extensively evaluate the proposed method on in-the-wild FER datasets including RAF-DB, AffectNet, SFEW and FERPlus. The comparable results show that our method is promising to improve classification performance and achieves equivalent performance compared with state-of-the-art methods. Also, we demonstrate the strong generalization performance on realistic occlusion and pose variation datasets and the cross-dataset evaluation.},
  archive  = {J},
  author   = {Hangyu Li and Nannan Wang and Xi Yang and Xiaoyu Wang and Xinbo Gao},
  doi      = {10.1109/TAFFC.2023.3263886},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {173-185},
  title    = {Unconstrained facial expression recognition with no-reference de-elements learning},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transformer-based self-supervised multimodal representation
learning for wearable emotion recognition. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(1), 157–172. (<a
href="https://doi.org/10.1109/TAFFC.2023.3263907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, wearable emotion recognition based on peripheral physiological signals has drawn massive attention due to its less invasive nature and its applicability in real-life scenarios. However, how to effectively fuse multimodal data remains a challenging problem. Moreover, traditional fully-supervised based approaches suffer from overfitting given limited labeled data. To address the above issues, we propose a novel self-supervised learning (SSL) framework for wearable emotion recognition, where efficient multimodal fusion is realized with temporal convolution-based modality-specific encoders and a transformer-based shared encoder, capturing both intra-modal and inter-modal correlations. Extensive unlabeled data is automatically assigned labels by five signal transforms, and the proposed SSL model is pre-trained with signal transformation recognition as a pretext task, allowing the extraction of generalized multimodal representations for emotion-related downstream tasks. For evaluation, the proposed SSL model was first pre-trained on a large-scale self-collected physiological dataset and the resulting encoder was subsequently frozen or fine-tuned on three public supervised emotion recognition datasets. Ultimately, our SSL-based method achieved state-of-the-art results in various emotion classification tasks. Meanwhile, the proposed model was proved to be more accurate and robust compared to fully-supervised methods on low data regimes.},
  archive  = {J},
  author   = {Yujin Wu and Mohamed Daoudi and Ali Amad},
  doi      = {10.1109/TAFFC.2023.3263907},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {157-172},
  title    = {Transformer-based self-supervised multimodal representation learning for wearable emotion recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Context-aware dynamic word embeddings for aspect term
extraction. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(1), 144–156. (<a
href="https://doi.org/10.1109/TAFFC.2023.3262941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The aspect term extraction (ATE) task aims to extract aspect terms describing a part or an attribute of a product from review sentences. Most existing works rely on either general or domain embedding to address this problem. Despite the promising results, the importance of general and domain embeddings is still ignored by most methods, resulting in degraded performances. Besides, word embedding is also related to downstream tasks, and how to regularize word embeddings to capture context-aware information is an unresolved problem. To solve these issues, we first propose context-aware dynamic word embedding (CDWE), which could simultaneously consider general meanings, domain-specific meanings, and the context information of words. Based on CDWE, we propose an attention-based convolution neural network, called ADWE-CNN for ATE, which could adaptively capture the previous meanings of words by utilizing an attention mechanism to assign different importance to the respective embeddings. The experimental results show that ADWE-CNN achieves a comparable performance with the state-of-the-art approaches. Various ablation studies have been conducted to explore the benefit of each component. Our code is publicly available at http://github.com/xiejiajia2018/ADWE-CNN .},
  archive  = {J},
  author   = {Jingyun Xu and Jiayuan Xie and Yi Cai and Zehang Lin and Ho-Fung Leung and Qing Li and Tat-Seng Chua},
  doi      = {10.1109/TAFFC.2023.3262941},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {144-156},
  title    = {Context-aware dynamic word embeddings for aspect term extraction},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GA2MIF: Graph and attention based two-stage multi-source
information fusion for conversational emotion detection. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(1), 130–143. (<a
href="https://doi.org/10.1109/TAFFC.2023.3261279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multimodal Emotion Recognition in Conversation (ERC) plays an influential role in the field of human-computer interaction and conversational robotics since it can motivate machines to provide empathetic services. Multimodal data modeling is an up-and-coming research area in recent years, which is inspired by human capability to integrate multiple senses. Several graph-based approaches claim to capture interactive information between modalities, but the heterogeneity of multimodal data makes these methods prohibit optimal solutions. In this article, we introduce a multimodal fusion approach named Graph and Attention based Two-stage Multi-source Information Fusion (GA2MIF) for emotion detection in conversation. Our proposed method circumvents the problem of taking heterogeneous graph as input to the model while eliminating complex redundant connections in the construction of graph. GA2MIF focuses on contextual modeling and cross-modal modeling through leveraging Multi-head Directed Graph ATtention networks (MDGATs) and Multi-head Pairwise Cross-modal ATtention networks (MPCATs), respectively. Extensive experiments on two public datasets (i.e., IEMOCAP and MELD) demonstrate that the proposed GA2MIF has the capacity to validly capture intra-modal long-range contextual information and inter-modal complementary information, as well as outperforms the prevalent State-Of-The-Art (SOTA) models by a remarkable margin.},
  archive  = {J},
  author   = {Jiang Li and Xiaoping Wang and Guoqing Lv and Zhigang Zeng},
  doi      = {10.1109/TAFFC.2023.3261279},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {130-143},
  title    = {GA2MIF: Graph and attention based two-stage multi-source information fusion for conversational emotion detection},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prompt consistency for multi-label textual emotion
detection. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(1), 121–129. (<a
href="https://doi.org/10.1109/TAFFC.2023.3254883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Textual emotion detection is playing an important role in the human-computer interaction domain. The mainstream methods of textual emotion detection are extracting semantic features and fine-tuning by language models. Due to the information redundancy in semantics, it is difficult for these methods to accurately detect all the emotions implied in the text. The prompting method has been shown to make the language models more purposeful in prediction by filling the cloze or prefix prompts defined. Therefore, we design a prompting method for multi-label classification. To stabilize the output, we design two consistency training strategies. We experiment on two multi-label emotion classification datasets: Ren-CECps and NLPCC2018. Our proposed prompting method with consistency training strategies for multi-label textual emotion detection (PC-MTED) model achieves state-of-the-art Macro F1 scores of 0.5432 and 0.5269, respectively. The experimental results indicate that our proposed method is effective in the multi-label textual emotion detection task.},
  archive  = {J},
  author   = {Yangyang Zhou and Xin Kang and Fuji Ren},
  doi      = {10.1109/TAFFC.2023.3254883},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {121-129},
  title    = {Prompt consistency for multi-label textual emotion detection},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ARAUS: A large-scale dataset and baseline models of
affective responses to augmented urban soundscapes. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(1), 105–120. (<a
href="https://doi.org/10.1109/TAFFC.2023.3247914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Choosing optimal maskers for existing soundscapes to effect a desired perceptual change via soundscape augmentation is non-trivial due to extensive varieties of maskers and a dearth of benchmark datasets with which to compare and develop soundscape augmentation models. To address this problem, we make publicly available the ARAUS (Affective Responses to Augmented Urban Soundscapes) dataset, which comprises a five-fold cross-validation set and independent test set totaling 25,440 unique subjective perceptual responses to augmented soundscapes presented as audio-visual stimuli. Each augmented soundscape is made by digitally adding “maskers” (bird, water, wind, traffic, construction, or silence) to urban soundscape recordings at fixed soundscape-to-masker ratios. Responses were then collected by asking participants to rate how pleasant, annoying, eventful, uneventful, vibrant, monotonous, chaotic, calm, and appropriate each augmented soundscape was, in accordance with ISO/TS 12913-2:2018. Participants also provided relevant demographic information and completed standard psychological questionnaires. We perform exploratory and statistical analysis of the responses obtained to verify internal consistency and agreement with known results in the literature. Finally, we demonstrate the benchmarking capability of the dataset by training and comparing four baseline models for urban soundscape pleasantness: a low-parameter regression model, a high-parameter convolutional neural network, and two attention-based networks in the literature.},
  archive  = {J},
  author   = {Kenneth Ooi and Zhen-Ting Ong and Karn N. Watcharasupat and Bhan Lam and Joo Young Hong and Woon-Seng Gan},
  doi      = {10.1109/TAFFC.2023.3247914},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {105-120},
  title    = {ARAUS: A large-scale dataset and baseline models of affective responses to augmented urban soundscapes},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Facial expression recognition in classrooms: Ethical
considerations and proposed guidelines for affect detection in
educational settings. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(1), 93–104. (<a
href="https://doi.org/10.1109/TAFFC.2023.3275624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent technological and educational shifts have made it possible to capture students’ facial expressions during learning with the goal of detecting learners’ emotional states. Those interested in affect detection argue these tools will support automated emotions-based learning interventions, providing educational professionals with the opportunity to develop individualized, emotionally responsive instructional offerings at scale. Despite these proposed use-cases, few have considered the inherent ethical concerns related to detecting and reporting on learners’ emotions specifically within applied educational contexts. As such, this article utilizes a Reflexive Principlism approach to establish a typology of proactive reflexive ethical implications in tracking students’ emotions through changes in their facial expressions. Through this approach the authors differentiate between use in research and applied education contexts, arguing that the latter should be curtailed until the ethics of affective computing in educational settings is better established.},
  archive  = {J},
  author   = {Allison Macey Banzon and Jonathan Beever and Michelle Taub},
  doi      = {10.1109/TAFFC.2023.3275624},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {93-104},
  title    = {Facial expression recognition in classrooms: Ethical considerations and proposed guidelines for affect detection in educational settings},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The ethics of AI in games. <em>IEEE Transactions on
Affective Computing</em>, <em>15</em>(1), 79–92. (<a
href="https://doi.org/10.1109/TAFFC.2023.3276425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Video games are one of the richest and most popular forms of human-computer interaction and, hence, their role is critical for our understanding of human behaviour and affect at a large scale. As artificial intelligence (AI) tools are gradually adopted by the game industry a series of ethical concerns arise. Such concerns, however, have so far not been extensively discussed in a video game context. Motivated by the lack of a comprehensive review on the ethics of AI as applied to games, we survey the current state of the art in this area and discuss ethical considerations of these systems from the holistic perspective of the affective loop . Through the components of this loop, we study the ethical challenges that AI faces in video game development. Elicitation highlights the ethical boundaries of artificially induced emotions; sensing showcases the trade-off between privacy and safe gaming spaces; and detection , as utilised during in-game adaptation , poses challenges to transparency and ownership. This paper calls for an open dialogue and action for the games of today and the virtual spaces of the future. By setting an appropriate framework we aim to protect users and to guide developers towards safer and better experiences for their customers.},
  archive  = {J},
  author   = {David Melhart and Julian Togelius and Benedikte Mikkelsen and Christoffer Holmgård and Georgios N. Yannakakis},
  doi      = {10.1109/TAFFC.2023.3276425},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {79-92},
  title    = {The ethics of AI in games},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Measuring and fostering diversity in affective computing
research. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(1), 63–78. (<a
href="https://doi.org/10.1109/TAFFC.2023.3244041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This work presents a longitudinal study of diversity among the Affective Computing research community members. We explore several dimensions of diversity, including gender, geography, institutional types of affiliations and selected combinations of dimensions. We cover the last 10 years of the IEEE Transactions on Affective Computing (TAFFC) journal and the International Conference on Affective Computing and Intelligent Interaction (ACII), the primary sources of publications in Affective Computing. We also present an analysis of diversity among the members of the Association for the Advancement of Affective Computing (AAAC). Our findings reveal a “leaky pipeline” in the field, with a low –albeit slowly increasing over the years– representation of women. They also show that academic institutions clearly dominate publications, ahead of industry and governmental centres. In terms of geography, most publications come from the USA, contributions from Latin America or Africa being almost non-existent. Lastly, we find that diversity in the characteristics of researchers (gender and geographic location) influences diversity in the topics. To conclude, we analyse initiatives that have been undertaken in other AI-related research communities to foster diversity, and recommend a set of initiatives that could be applied to the Affective Computing field to increase diversity in its different facets. The diversity data collected in this work are publicly available, ensuring strict personal data protection and governance rules.},
  archive  = {J},
  author   = {Isabelle Hupont and Songül Tolan and Pedro Frau and Lorenzo Porcaro and Emilia Gómez},
  doi      = {10.1109/TAFFC.2023.3244041},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {63-78},
  title    = {Measuring and fostering diversity in affective computing research},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ethical considerations and checklist for affective research
with wearables. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(1), 50–62. (<a
href="https://doi.org/10.1109/TAFFC.2022.3222524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As the popularity of wearables increases, so does their utility for studying emotions. Using new technologies points to several ethical challenges to be considered to improve research designs. There are several ethical recommendations for utilizing wearables to study human emotions, but they focus on emotion recognition systems applications rather than research design and implementation. To address this gap, we have developed a perspective on wearables, especially in daily life, adapting the ReCODE Health - Digital Health Framework and companion checklist. Therefore, our framework consists of four domains: (1) participation experience, (2) privacy, (3) data management, and (4) access and usability. We identified 33 primary risks of using wearables to study emotions, including research-related negative emotions, collecting, processing, storing, sharing personal and biological information, commercial technology validity and reliability, and exclusivity issues. We also proposed possible strategies for minimizing risks. We consulted the new ethical guidelines with members of ethics committees and relevant researchers. The judges ( N = 26) positively rated our solutions and provided useful feedback that helped us refine our guidance. Finally, we summarized our proposals with a checklist for researchers’ convenience. Our guidelines contribute to future research by providing improved protection of participants’ and scientists’ interests.},
  archive  = {J},
  author   = {Maciej Behnke and Stanislaw Saganowski and Dominika Kunc and Przemysław Kazienko},
  doi      = {10.1109/TAFFC.2022.3222524},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {50-62},
  title    = {Ethical considerations and checklist for affective research with wearables},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatio-temporal graph analytics on secondary affect data for
improving trustworthy emotional AI. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(1), 30–49. (<a
href="https://doi.org/10.1109/TAFFC.2023.3296695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Ethical affective computing (AC) requires maximizing the benefits to users while minimizing its harm to obtain trust from users. This requires responsible development and deployment to ensure fairness, bias mitigation, privacy preservation, and accountability. To obtain this, we require methodologies that can quantify, visualize, analyze, and mine insights from affect data. Hence, in this article, we propose a spatio-temporal model for representing secondary affect data from network sciences’ perspective. We propose a network science-based model to represent spatio-temporal data, e.g., action units’ sequences, and continuous affect reports. In particular, the proposed model captures the spatial and temporal strength of the relationship among essential variables in the data. The proposed model allows to analyze data as a whole system. We also demonstrated the use case of the model for graph analytics on secondary affect data that can assist to measure and quantify several issues that can be originated from the study setup, data recording devices, and the influences/biases that can originate from the perspective of the affect reporters. We also demonstrated the use cases of the proposed method on ethical trustworthy emotional AI via measuring biases from de-identified data and how it contributes towards ethics, transparency, value alignment, and governance.},
  archive  = {J},
  author   = {Md Taufeeq Uddin and Lijun Yin and Shaun Canavan},
  doi      = {10.1109/TAFFC.2023.3296695},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {30-49},
  title    = {Spatio-temporal graph analytics on secondary affect data for improving trustworthy emotional AI},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An (e)affective bind: Situated affectivity and the prospect
of affect recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>15</em>(1), 18–29. (<a
href="https://doi.org/10.1109/TAFFC.2023.3281069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Several prominent criticisms have recently challenged the possibility of algorithmically determining or recognising human affect. This paper ethically evaluates one underexplored avenue for overcoming such deficiencies in categorical affect recognition technologies (ARTs). Specifically, the emerging literature on “situated affectivity” offers valuable guidance on three fronts. First, it conceptually substantiates existing criticism by stressing the contextual dimensions of human affect. Second, in highlighting the breadth of information required for more accurate ARTs, it suggest that they may necessitate penetrating forms of surveillance the ethical literature has already provided good reason to resist. Third, it corroborates postphenomenological insights concerning the socially co-constituting character of technology; and thereby grounds the need for a “situated ethics” sensitive to the ways that ARTs might reconstruct human affectivity (and reinforce existing power hierarchies). The deep worry is that this concern is likely to persist even if concerns over accuracy and surveillance are mitigated. A detailed consideration of this novel ethical concern suggests that the ethically acceptable use of ARTs may only be possible in highly contained settings. Yet doubts over such containment places advocates of ARTs in a “double bind” they need to address.},
  archive  = {J},
  author   = {Jason Branford},
  doi      = {10.1109/TAFFC.2023.3281069},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {18-29},
  title    = {An (E)Affective bind: Situated affectivity and the prospect of affect recognition},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Opacity, transparency, and the ethics of affective
computing. <em>IEEE Transactions on Affective Computing</em>,
<em>15</em>(1), 4–17. (<a
href="https://doi.org/10.1109/TAFFC.2023.3278230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Human opacity is the intrinsic quality of unknowability of human beings with respect to machines. The descriptive relationship between humans and machines, which captures how much information one can gather about the other, can be explicated using an opacity-transparency relationship. This relationship allows us to describe and normatively evaluate a spectrum of opacity where humans and machines may be either opaque or transparent. In this paper, we argue that the advent of Affective Computing (AC) has begun to shift the ideal position of humans on this spectrum towards greater transparency, while much of this technology is shifting towards opacity. We explore the implications of this shift with regard to the affective information of humans and how the threat to human opacity by AC systems has various adverse repercussions, such as infringement of one&#39;s autonomy, deception, manipulation, and increased anxiety. There are also distributive consequences that expose vulnerable groups to unjustified burdens and reduce them to mere profiles. We further provide an assessment of current AC technology, which follows the descriptive relationship between humans and machines from the lens of opacity and transparency. Finally, we foresee and address three possible objections to our claims. These are the beneficence of AC systems, their relation to privacy, and their restrictive capacity to capture human affects. Through these arguments, the paper aims to bring attention to the ontological relationship between humans and machines from the perspective of opacity and transparency while emphasizing on the gravity of the ethical concerns raised by their threat to human opacity.},
  archive  = {J},
  author   = {Manohar Kumar and Aisha Aijaz and Omkar Chattar and Jainendra Shukla and Raghava Mutharaju},
  doi      = {10.1109/TAFFC.2023.3278230},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {4-17},
  title    = {Opacity, transparency, and the ethics of affective computing},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guest editorial: Ethics in affective computing. <em>IEEE
Transactions on Affective Computing</em>, <em>15</em>(1), 1–3. (<a
href="https://doi.org/10.1109/TAFFC.2023.3322918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Stunning advances in machine learning are heralding a new era in sensing, interpreting, simulating and stimulating human emotion. In the human sciences, research is increasingly highlighting the explanatory power of emotions, feelings, and other affective processes to predict how we think and behave. This is beginning to translate into an explosion of applications that can improve human wellbeing including methods to reduce stress and improve emotion regulation skills, techniques to support healthier social media use, pain monitoring in neonates, and decision-support tools that recognize emotional bias.},
  archive  = {J},
  author   = {Jonathan Gratch and Gretchen Greene and Rosalind Picard and Lachlan Urquhart and Michel Valstar},
  doi      = {10.1109/TAFFC.2023.3322918},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {1-3},
  title    = {Guest editorial: Ethics in affective computing},
  volume   = {15},
  year     = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
