<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TCC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tcc---105">TCC - 105</h2>
<ul>
<li><details>
<summary>
(2024). COCSN: A multi-tiered cascaded optical circuit switching
network for data center. <em>TCC</em>, <em>12</em>(4), 1463–1475. (<a
href="https://doi.org/10.1109/TCC.2024.3488275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A cascaded network represents a classic scaling-out model in traditional electrical switching networks. Recent proposals have integrated optical circuit switching at specific tiers of these networks to reduce power consumption and enhance topological flexibility. Utilizing a multi-tiered cascaded optical circuit switching network is expected to extend the advantages of optical circuit switching further. The main challenges fall into two categories. First, an architecture with sufficient connectivity is required to support varying workloads. Second, the network reconfiguration is more complex and necessitates a low-complexity scheduling algorithm. In this work, we propose COCSN, a multi-tiered cascaded optical circuit switching network architecture for data center. COCSN employs wavelength-selective switches that integrate multiple wavelengths to enhance network connectivity. We formulate a mathematical model covering lightpath establishment, network reconfiguration, and reconfiguration goals, and propose theorems to optimize the model. Based on the theorems, we introduce an over-subscription-supported wavelength-by-wavelength scheduling algorithm, facilitating agile establishment of lightpaths in COCSN tailored to communication demand. This algorithm effectively addresses scheduling complexities and mitigates the issue of lengthy WSS configuration times. Simulation studies investigate the impact of flow length, WSS reconfiguration time, and communication domain on COCSN, verifying its significantly lower complexity and superior performance over classical cascaded networks.},
  archive      = {J_TCC},
  author       = {Shuo Li and Huaxi Gu and Xiaoshan Yu and Hua Huang and Songyan Wang and Zeshan Chang},
  doi          = {10.1109/TCC.2024.3488275},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1463-1475},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {COCSN: A multi-tiered cascaded optical circuit switching network for data center},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Aggregate monitoring for geo-distributed kubernetes cluster
federations. <em>TCC</em>, <em>12</em>(4), 1449–1462. (<a
href="https://doi.org/10.1109/TCC.2024.3482574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed monitoring is an essential functionality to allow large cluster federations to efficiently schedule applications on a set of available geo-distributed resources. However, periodically reporting the precise status of each available server is both unnecessary to allow accurate scheduling and unscalable when the number of servers grows. This paper proposes Acala, an aggregate monitoring framework for geo-distributed Kubernetes cluster federations which aims to provide the management cluster with aggregated information about the entire cluster instead of individual servers. Based on actual deployment under a controlled environment in the geo-distributed Grid’5000 testbed, our evaluations show that Acala reduces the cross-cluster network traffic by up to 97% and the scrape duration by up to 55% in the single member cluster experiment. Our solution also decreases cross-cluster network traffic by 95% and memory resource consumption by 83% in multiple member cluster scenarios. A comparison of scheduling efficiency with and without data aggregation shows that aggregation has minimal effects on the system’s scheduling function. These results indicate that our approach is superior to the existing solution and is suitable to handle large-scale geo-distributed Kubernetes cluster federation environments.},
  archive      = {J_TCC},
  author       = {Chih-Kai Huang and Guillaume Pierre},
  doi          = {10.1109/TCC.2024.3482574},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1449-1462},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Aggregate monitoring for geo-distributed kubernetes cluster federations},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Group formation and sampling in group-based hierarchical
federated learning. <em>TCC</em>, <em>12</em>(4), 1433–1448. (<a
href="https://doi.org/10.1109/TCC.2024.3482865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical federated learning has emerged as a pragmatic approach to addressing scalability, robustness, and privacy concerns within distributed machine learning, particularly in the context of edge computing. This hierarchical method involves grouping clients at the edge, where the constitution of client groups significantly impacts overall learning performance, influenced by both the benefits obtained and costs incurred during group operations (such as group formation and group training). This is especially true for edge and mobile devices, which are more sensitive to computation and communication overheads. The formation of groups is critical for group-based hierarchical federated learning but often neglected by researchers, especially in the realm of edge systems. In this paper, we present a comprehensive exploration of a group-based federated edge learning framework utilizing the hierarchical cloud-edge-client architecture and employing probabilistic group sampling. Our theoretical analysis of its convergence rate, considering the characteristics of client groups, reveals the pivotal role played by group heterogeneity in achieving convergence. Building on this insight, we introduce new methods for group formation and group sampling, aiming to mitigate data heterogeneity within groups and enhance the convergence and overall performance of federated learning. Our proposed methods are validated through extensive experiments, demonstrating their superiority over current algorithms in terms of prediction accuracy and training cost.},
  archive      = {J_TCC},
  author       = {Jiyao Liu and Xuanzhang Liu and Xinliang Wei and Hongchang Gao and Yu Wang},
  doi          = {10.1109/TCC.2024.3482865},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1433-1448},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Group formation and sampling in group-based hierarchical federated learning},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HEXO: Offloading long-running compute- and memory-intensive
workloads on low-cost, low-power embedded systems. <em>TCC</em>,
<em>12</em>(4), 1415–1432. (<a
href="https://doi.org/10.1109/TCC.2024.3482178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {OS-capable embedded systems exhibiting a very low power consumption are available at an extremely low price point. It makes them highly compelling in a datacenter context. We show that sharing long-running, compute-intensive datacenter workloads between a server machine and one or a few connected embedded boards of negligible cost and power consumption can yield significant performance and energy benefits. Our approach, named Heterogeneous EXecution Offloading (HEXO), selectively offloads Virtual Machines (VMs) from server-class machines to embedded boards. Our design tackles several challenges. We address the Instruction Set Architecture (ISA) difference between typical servers (x86) and embedded systems (ARM) through hypervisor and guest OS-level support for heterogeneous-ISA runtime VM migration. We cope with the low amount of resources in embedded systems by using lightweight VMs – unikernels – and by using the server&#39;s free RAM as remote memory for embedded boards through a transparent lightweight memory disaggregation mechanism for heterogeneous server-embedded clusters, called Netswap. VMs are offloaded based on an estimation of the slowdown expected from running on a given board. We build a prototype of HEXO and demonstrate significant increases in throughput (up to 67%) and energy efficiency (up to 56%) using benchmarks representative of compute-intensive long-running workloads.},
  archive      = {J_TCC},
  author       = {Pierre Olivier and A K M Fazla Mehrab and Sandeep Errabelly and Stefan Lankes and Mohamed Lamine Karaoui and Robert Lyerly and Sang-Hoon Kim and Antonio Barbalace and Binoy Ravindran},
  doi          = {10.1109/TCC.2024.3482178},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1415-1432},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {HEXO: Offloading long-running compute- and memory-intensive workloads on low-cost, low-power embedded systems},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint offloading and resource allocation for collaborative
cloud computing with dependent subtask scheduling on multi-core server.
<em>TCC</em>, <em>12</em>(4), 1401–1414. (<a
href="https://doi.org/10.1109/TCC.2024.3481039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative cloud computing (CCC) has emerged as a promising paradigm to support computation-intensive and delay-sensitive applications by leveraging MEC and MCC technologies. However, the coupling between multiple variables and subtask dependencies within an application poses significant challenges to the computation offloading mechanism. To address this, we investigate the computation offloading problem for CCC by jointly optimizing offloading decisions, resource allocation, and subtask scheduling across a multi-core edge server. First, we exploit latency to design a subtask dependency model within the application. Next, we formulate a System Energy-Time Cost ( $SETC$ ) minimization problem that considers the trade-off between time and energy consumption while satisfying subtask dependencies. Due to the complexity of directly solving the formulated problem, we decompose it and propose two offloading algorithms, namely Maximum Local Searching Offloading (MLSO) and Sequential Searching Offloading (SSO), to jointly optimize offloading decisions and resource allocation. We then model dependent subtask scheduling across the multi-core edge server as a Job-Shop Scheduling Problem (JSSP) and propose a Genetic-based Task Scheduling (GTS) algorithm to achieve optimal dependent subtask scheduling on the multi-core edge server. Finally, our simulation results demonstrate the effectiveness of the proposed MLSO, SSO, and GTS algorithms under different parameter settings.},
  archive      = {J_TCC},
  author       = {Zihan Gao and Peixiao Zheng and Wanming Hao and Shouyi Yang},
  doi          = {10.1109/TCC.2024.3481039},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1401-1414},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Joint offloading and resource allocation for collaborative cloud computing with dependent subtask scheduling on multi-core server},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RAM: A resource-aware DDoS attack mitigation framework in
clouds. <em>TCC</em>, <em>12</em>(4), 1387–1400. (<a
href="https://doi.org/10.1109/TCC.2024.3480194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed Denial of Service (DDoS) attacks threaten cloud servers by flooding redundant requests, leading to system resource exhaustion and legitimate service shutdown. Existing DDoS attack mitigation mechanisms mainly rely on resource expansion, which may result in unexpected resource over-provisioning and accordingly increase cloud system costs. To effectively mitigate DDoS attacks without consuming extra resources, the main challenges lie in the compromisesbetween incoming requests and available cloud resources. This paper proposes a resource-aware DDoS attack mitigation framework named RAM, where the mechanism of feedback in control theory is employed to adaptively adjust the interaction between incoming requests and available cloud resources. Specifically, two indicators including request confidence level and maximum cloud workload are designed. In terms of these two indicators, the incoming requests will be classified using proportional-integral-derivative (PID) feedback control-based classification scheme with request determination adaptation. The incoming requests can be subsequently processed according to their confidence levels as well as the workload and available resources of cloud servers, which achieves an effective resource-aware mitigation of DDoS attacks. Extensive experiments have been conducted to verify the effectiveness of RAM, which demonstrate that the proposed RAM can improve the request classification performance and guarantee the quality of service.},
  archive      = {J_TCC},
  author       = {Fangyuan Xing and Fei Tong and Jialong Yang and Guang Cheng and Shibo He},
  doi          = {10.1109/TCC.2024.3480194},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1387-1400},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {RAM: A resource-aware DDoS attack mitigation framework in clouds},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Minimizing response delay in UAV-assisted mobile edge
computing by joint UAV deployment and computation offloading.
<em>TCC</em>, <em>12</em>(4), 1372–1386. (<a
href="https://doi.org/10.1109/TCC.2024.3478172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a promising technique for offloading computation tasks from mobile devices, Unmanned Aerial Vehicle (UAV)-assisted Mobile Edge Computing (MEC) utilizes UAVs as computational resources. A popular method for enhancing the quality of service (QoS) of UAV-assisted MEC systems is to jointly optimize UAV deployment and computation task offloading. This imposes the challenge of dynamically adjusting UAV deployment and computation offloading to accommodate the changing positions and computational requirements of mobile devices. Due to the real-time requirements of MEC computation tasks, finding an efficient joint optimization approach is imperative. This paper proposes an algorithm aimed at minimizing the average response delay in a UAV-assisted MEC system. The approach revolves around the joint optimization of UAV deployment and computation offloading through convex optimization. We break down the problem into three sub-problems: UAV deployment, Ground Device (GD) access, and computation tasks offloading, which we address using the block coordinate descent algorithm. Observing the $NP$ -hardness nature of the original problem, we present near-optimal solutions to the decomposed sub-problems. Simulation results demonstrate that our approach can generate a joint optimization solution within seconds and diminish the average response delay compared to state-of-the-art algorithms and other advanced algorithms, with improvements ranging from 4.70% to 42.94%.},
  archive      = {J_TCC},
  author       = {Jianshan Zhang and Haibo Luo and Xing Chen and Hong Shen and Longkun Guo},
  doi          = {10.1109/TCC.2024.3478172},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1372-1386},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Minimizing response delay in UAV-assisted mobile edge computing by joint UAV deployment and computation offloading},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CloudBrain-ReconAI: A cloud computing platform for MRI
reconstruction and radiologists’ image quality evaluation. <em>TCC</em>,
<em>12</em>(4), 1359–1371. (<a
href="https://doi.org/10.1109/TCC.2024.3476418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient collaboration between engineers and radiologists is important for image reconstruction algorithm development and image quality evaluation in magnetic resonance imaging (MRI). Here, we develop CloudBrain-ReconAI, an online cloud computing platform, for algorithm deployment, fast and blind reader study. This platform supports online image reconstruction using state-of-the-art artificial intelligence and compressed sensing algorithms with applications for fast imaging (Cartesian and non-Cartesian sampling) and high-resolution diffusion imaging. Through visiting the website, radiologists can easily score and mark images. Then, automatic statistical analysis will be provided.},
  archive      = {J_TCC},
  author       = {Yirong Zhou and Chen Qian and Jiayu Li and Zi Wang and Yu Hu and Biao Qu and Liuhong Zhu and Jianjun Zhou and Taishan Kang and Jianzhong Lin and Qing Hong and Jiyang Dong and Di Guo and Xiaobo Qu},
  doi          = {10.1109/TCC.2024.3476418},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1359-1371},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {CloudBrain-ReconAI: A cloud computing platform for MRI reconstruction and radiologists’ image quality evaluation},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). D-STACK: High throughput DNN inference by effective
multiplexing and spatio-temporal scheduling of GPUs. <em>TCC</em>,
<em>12</em>(4), 1344–1358. (<a
href="https://doi.org/10.1109/TCC.2024.3476210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hardware accelerators such as GPUs are required for real-time, low latency inference with Deep Neural Networks (DNN). Providing inference services in the cloud can be resource intensive, and effectively utilizing accelerators in the cloud is important. Spatial multiplexing of the GPU, while limiting the GPU resources (GPU%) to each DNN to the right amount, leads to higher GPU utilization and higher inference throughput. Right-sizing the GPU for each DNN the optimal batching of requests to balance throughput and service level objectives (SLOs), and maximizing throughput by appropriately scheduling DNNs are still significant challenges.This article introduces a dynamic and fair spatio-temporal scheduler (D-STACK) for multiple DNNs to run in the GPU concurrently. We develop and validate a model that estimates the parallelism each DNN can utilize and a lightweight optimization formulation to find an efficient batch size for each DNN. Our holistic inference framework provides high throughput while meeting application SLOs. We compare D-STACK with other GPU multiplexing and scheduling methods (e.g., NVIDIA Triton, Clipper, Nexus), using popular DNN models. Our controlled experiments with multiplexing several popular DNN models achieve up to $1.6\times$ improvement in GPU utilization and up to $4\times$ improvement in inference throughput.},
  archive      = {J_TCC},
  author       = {Aditya Dhakal and Sameer G. Kulkarni and K. K. Ramakrishnan},
  doi          = {10.1109/TCC.2024.3476210},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1344-1358},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {D-STACK: High throughput DNN inference by effective multiplexing and spatio-temporal scheduling of GPUs},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FaaSCtrl: A comprehensive-latency controller for serverless
platforms. <em>TCC</em>, <em>12</em>(4), 1328–1343. (<a
href="https://doi.org/10.1109/TCC.2024.3473015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serverless computing systems have become very popular because of their natural advantages with respect to auto-scaling, load balancing and fast distributed processing. As of today, almost all serverless systems define two QoS classes: best-effort ( $BE$ ) and latency-sensitive ( $LS$ ). Systems typically do not offer any latency or QoS guarantees for $BE$ jobs and run them on a best-effort basis. In contrast, systems strive to minimize the processing time for $LS$ jobs. This work proposes a precise definition for these job classes and argues that we need to consider a bouquet of performance metrics for serverless applications, not just a single one. We thus propose the comprehensive latency ( $CL$ ) that comprises the mean, tail latency, median and standard deviation of a series of invocations for a given serverless function. Next, we design a system FaaSCtrl , whose main objective is to ensure that every component of the $CL$ is within a prespecified limit for an LS application, and for BE applications, these components are minimized on a best-effort basis. Given the sheer complexity of the scheduling problem in a large multi-application setup, we use the method of surrogate functions in optimization theory to design a simpler optimization problem that relies on performance and fairness. We rigorously establish the relevance of these metrics through characterization studies. Instead of using standard approaches based on optimization theory, we use a much faster reinforcement learning (RL) based approach to tune the knobs that govern process scheduling in Linux, namely the real-time priority and the assigned number of cores. RL works well in this scenario because the benefit of a given optimization is probabilistic in nature, owing to the inherent complexity of the system. We show using rigorous experiments on a set of real-world workloads that FaaSCtrl achieves its objectives for both LS and BE applications and outperforms the state-of-the-art by 36.9% (for tail response latency) and 44.6% (for response latency&#39;s std. dev.) for LS applications.},
  archive      = {J_TCC},
  author       = {Abhisek Panda and Smruti R. Sarangi},
  doi          = {10.1109/TCC.2024.3473015},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1328-1343},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {FaaSCtrl: A comprehensive-latency controller for serverless platforms},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). QoS-aware, cost-efficient scheduling for data-intensive DAGs
in multi-tier computing environment. <em>TCC</em>, <em>12</em>(4),
1314–1327. (<a href="https://doi.org/10.1109/TCC.2024.3468913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s scientific landscape, Directed Acyclic Graphs (DAGs) are pivotal for representing task dependencies in data-intensive applications. Traditionally, two dominant bottom-up DAG scheduling approaches exist: one overlooks communication contention and the other fails to exploit parallelization for improving latency. This study distinguishes itself by advocating a top-down approach prioritizing latency or cost optimization in multi-tier environments to fulfill QoS and SLA requirements. Our strategy effectively mitigates bandwidth contention and facilitates parallel executions, leading to substantial completion time reductions. Our findings suggest that myopic knowledge-based scheduling, emphasizing latency or cost minimization, can yield benefits comparable to its look-ahead counterparts. Through latency-efficient and cost-efficient topological sorting, our wDAGSplit strategy introduces a two-stage partitioning and scheduling approach. Its simplicity and adaptability extend its usability to DAGs of any scale. Evaluated on over 100,000 real-world DAG applications, wDAGSplit demonstrates latency enhancements of up to 80x compared to Edge-only scenarios, 15x to Near-Edge-only, and 6x to Cloud-only. In terms of cost, our approach achieves enhancements of up to 60x compared to Edge-only scenarios, 250x to NE-only, and 70x to Cloud-only. Moreover, for DAGs with 50 tasks, we achieve 5x reduced latency compared to previous approaches, along with a complexity reduction of up to 24 times.},
  archive      = {J_TCC},
  author       = {Paridhika Kayal and Alberto Leon-Garcia},
  doi          = {10.1109/TCC.2024.3468913},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1314-1327},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {QoS-aware, cost-efficient scheduling for data-intensive DAGs in multi-tier computing environment},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anomaly transformer ensemble model for cloud data anomaly
detection. <em>TCC</em>, <em>12</em>(4), 1305–1313. (<a
href="https://doi.org/10.1109/TCC.2024.3466174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stability and user trust in cloud services depends on prompt detection and response to diverse anomalies. This study focuses on an Ensemble-based anomaly detection methodology that integrates log data with computing resource metrics, aiming to overcome the limitations of traditional single-data models. To process the unstructured nature of log data, we use the Drain Parser to transform it into a structured format, and Doc2Vec embeds it. The study adheres to a reconstruction-based approach for anomaly detection, specifically building upon the Anomaly Transformer model. The proposed model leverages the concept of an Anomaly Transformer based on the Attention mechanism. It integrates preprocessed metric data with log data for effective anomaly detection. Experiments were conducted using metric and log data collected from real-world cloud environments. The model’s performance was evaluated based on accuracy, recall, precision, f1 score, and AUROC. The results demonstrate that our proposed Ensemble-based model outperforms traditional models such as LSTM, VAR, and deeplog.},
  archive      = {J_TCC},
  author       = {Won Sakong and Jongyeop Kwon and Kyungha Min and Suyeon Wang and Wooju Kim},
  doi          = {10.1109/TCC.2024.3466174},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1305-1313},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Anomaly transformer ensemble model for cloud data anomaly detection},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WorkloadDiff: Conditional denoising diffusion probabilistic
models for cloud workload prediction. <em>TCC</em>, <em>12</em>(4),
1291–1304. (<a href="https://doi.org/10.1109/TCC.2024.3461649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate workload forecasting plays a crucial role in optimizing resource allocation, enhancing performance, and reducing energy consumption in cloud data centers. Deep learning-based methods have emerged as the dominant approach in this field, exhibiting exceptional performance. However, most existing methods lack the ability to quantify confidence, limiting their practical decision-making utility. To address this limitation, we propose a novel denoising diffusion probabilistic model (DDPM)-based method, termed WorkloadDiff, for multivariate probabilistic workload prediction. WorkloadDiff leverages both original and noisy signals from input conditions using a two-path neural network. Additionally, we introduce a multi-scale feature extraction method and an adaptive fusion approach to capture diverse temporal patterns within the workload. To enhance consistency between conditions and predicted values, we incorporate a resampling strategy into the inference of WorkloadDiff. Extensive experiments conducted on four public datasets demonstrate the superior performance of WorkloadDiff over all baseline models, establishing it as a robust tool for resource management in cloud data centers.},
  archive      = {J_TCC},
  author       = {Weiping Zheng and Zongxiao Chen and Kaiyuan Zheng and Weijian Zheng and Yiqi Chen and Xiaomao Fan},
  doi          = {10.1109/TCC.2024.3461649},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1291-1304},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {WorkloadDiff: Conditional denoising diffusion probabilistic models for cloud workload prediction},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lightweight privacy-preserving ciphertext retrieval scheme
based on edge computing. <em>TCC</em>, <em>12</em>(4), 1273–1290. (<a
href="https://doi.org/10.1109/TCC.2024.3461732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of cloud computing and Internet of Things (IoT) technologies, large amounts of data collected from IoT devices are encrypted and outsourced to cloud servers for storage and sharing. However, traditional ciphertext retrieval schemes impose high computation and storage overhead on end users. Meanwhile, IoT devices with limited resources are difficult to adapt to large amounts of data computation and transmission, which leads to transmission delay and poor user experience. In this article, we propose a lightweight privacy-preserving ciphertext retrieval scheme based on edge computing (LPCR) by extending searchable encryption (SE) and ciphertext policy attribute-based encryption (CP-ABE) techniques. First, to avoid network delay and paralysis, we introduce edge servers into LPCR and design a collaboration mechanism between the user side and the edge servers. The user side only needs to accomplish lightweight computation and storage tasks, which greatly reduces their resource consumption. Second, we extend the basic ciphertext policy attribute-based keyword search (CP-ABKS) technique and design the Linear Secret Sharing Scheme (LSSS) access control algorithm with attribute values to hide access policies and attributes. In addition, to improve the retrieval accuracy, the document indexes and query trapdoors are set up by conjunctive keywords to help the cloud server locate exactly the data that the user wishes to query. Formal security analysis verifies that LPCR can achieve the security of chosen plaintext attack (CPA) and chosen keyword attack (CKA), and resist collusion attack. Simulation experiments prove that LPCR is lightweight and feasible.},
  archive      = {J_TCC},
  author       = {Na Wang and Wen Zhou and Qingyun Han and Jianwei Liu and Weilue Liao and Junsong Fu},
  doi          = {10.1109/TCC.2024.3461732},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1273-1290},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A lightweight privacy-preserving ciphertext retrieval scheme based on edge computing},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generative adversarial privacy for multimedia analytics
across the IoT-edge continuum. <em>TCC</em>, <em>12</em>(4), 1260–1272.
(<a href="https://doi.org/10.1109/TCC.2024.3459789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of multimedia-enabled IoT devices and edge computing enables a new class of data-intensive applications. However, analyzing the massive volumes of multimedia data presents significant privacy challenges. We propose a novel framework called generative adversarial privacy (GAP) that leverages generative adversarial networks (GANs) to synthesize privacy-preserving surrogate data for multimedia analytics across the IoT-Edge continuum. GAP carefully perturbs the GAN&#39;s training process to provide rigorous differential privacy guarantees without compromising utility. Moreover, we present optimization strategies, including dynamic privacy budget allocation, adaptive gradient clipping, and weight clustering to improve convergence and data quality under a constrained privacy budget. Theoretical analysis proves that GAP provides rigorous privacy protections while enabling high-fidelity analytics. Extensive experiments on real-world multimedia datasets demonstrate that GAP outperforms existing methods, producing high-quality synthetic data for privacy-preserving multimedia processing in diverse IoT-Edge applications.},
  archive      = {J_TCC},
  author       = {Xin Wang and Jianhui Lv and Byung-Gyu Kim and Carsten Maple and B. D. Parameshachari and Adam Slowik and Keqin Li},
  doi          = {10.1109/TCC.2024.3459789},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1260-1272},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Generative adversarial privacy for multimedia analytics across the IoT-edge continuum},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FedPAW: Federated learning with personalized aggregation
weights for urban vehicle speed prediction. <em>TCC</em>,
<em>12</em>(4), 1248–1259. (<a
href="https://doi.org/10.1109/TCC.2024.3452696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle speed prediction is crucial for intelligent transportation systems, promoting more reliable autonomous driving by accurately predicting future vehicle conditions. Due to variations in drivers’ driving styles and vehicle types, speed predictions for different target vehicles may significantly differ. Existing methods may not realize personalized vehicle speed prediction while protecting drivers’ data privacy. We propose a Federated learning framework with Personalized Aggregation Weights (FedPAW) to overcome these challenges. This method captures client-specific information by measuring the weighted mean squared error between the parameters of local models and global models. The server sends tailored aggregated models to clients instead of a single global model, without incurring additional computational and communication overhead for clients. To evaluate the effectiveness of FedPAW, we collected driving data in urban scenarios using the autonomous driving simulator CARLA, employing an LSTM-based Seq2Seq model with a multi-head attention mechanism to predict the future speed of target vehicles. The results demonstrate that our proposed FedPAW ranks lowest in prediction error within the time horizon of 10 seconds, with a 0.8% reduction in test MAE, compared to eleven representative benchmark baselines.},
  archive      = {J_TCC},
  author       = {Yuepeng He and Pengzhan Zhou and Yijun Zhai and Fang Qu and Zhida Qin and Mingyan Li and Songtao Guo},
  doi          = {10.1109/TCC.2024.3452696},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1248-1259},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {FedPAW: Federated learning with personalized aggregation weights for urban vehicle speed prediction},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attribute-based hierarchical keyword auditing with batch
fault localization assisted by smart contracts. <em>TCC</em>,
<em>12</em>(4), 1232–1247. (<a
href="https://doi.org/10.1109/TCC.2024.3452324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Keyword-based auditing (KA) provides a means for users to verify the integrity of only the outsourced data they are interested in. Existing KA schemes employ relation authentication labels to conduct targeted audits with keywords, which significantly improves the cost-effectiveness. However, such schemes typically support only a single-challenge scenario, which may not always be practical. To overcome this constraint, we introduce a hierarchical challenge mechanism grounded in user attributes. This mechanism leverages inequality and affiliation relationships to comply with a predefined tree structure for access policies. Incorporated during the challenge-response phase of the auditing model, it permits users to initiate cross-challenges. Expanding upon this hierarchical mechanism, we propose an attribute-based hierarchical keyword auditing scheme, abbreviated as $\mathcal{AHKA}$ . $\mathcal{AHKA}$ combines searchable encryption to conduct cross-targeted audits and benefits from the hash collision mapping of Bloom filters to safeguard against keyword guessing attacks. Moreover, we design a fault localization algorithm based on a variant of the binary search technique. It locates in batch the faulty cloud servers and damaged data blocks after an audit failure. As an integral part of $\mathcal{AHKA}$ , the algorithm significantly enhances our scheme&#39;s practicability. Security analyses indicate that $\mathcal{AHKA}$ can effectively withstand both forgery and replace attacks on audit proofs. The smart contract component ensures that our scheme&#39;s processes can be monitored and regulated. Experimental data corroborate that deploying $\mathcal{AHKA}$ on the client side and on the blockchain is both efficient and feasible.},
  archive      = {J_TCC},
  author       = {Jingting Xue and Shuqin Luo and Fagen Li and Wenzheng Zhang and Liang Liu and Yu Zhou and Xiaojun Zhang},
  doi          = {10.1109/TCC.2024.3452324},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1232-1247},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Attribute-based hierarchical keyword auditing with batch fault localization assisted by smart contracts},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large-scale measurements and optimizations on latency in
edge clouds. <em>TCC</em>, <em>12</em>(4), 1218–1231. (<a
href="https://doi.org/10.1109/TCC.2024.3452094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of next-generation latency-critical applications places strict requirements on network latency and stability. Edge cloud, an instantiated paradigm for edge computing, is gaining more and more attention due to its benefits of low latency. In this work, we make an in-depth investigation into the network QoS, especially end-to-end latency, at both spatial and temporal dimensions on a nationwide edge computing platform. Through the measurements, we collect a multi-variable large-scale real-world dataset on latency. We then quantify how the spatial-temporal factors affect the end-to-end latency, and verify the predictability of end-to-end latency. The results reveal the limitation of centralized clouds and illustrate how could edge clouds provide low and stable latency. Our results also point out that existing edge clouds merely increase the density of servers and ignore spatial-temporal factors, so they still suffer from high latency and fluctuations. Based on a quantified latency impact factor, we have proposed several optimization strategies for edge cloud latency and validated their effectiveness. We also propose a robust prototype edge cloud model based on lessons we learn from the measurement and evaluate its performance in the production environment. Evaluation result shows that edge clouds achieve 84.1% latency reduction with 0.5 ms latency fluctuation and 73.3% QoS improvement compared with the centralized clouds.},
  archive      = {J_TCC},
  author       = {Heng Zhang and Shaoyuan Huang and Mengwei Xu and Deke Guo and Xiaofei Wang and Xin Wang and Victor C. M. Leung and Wenyu Wang},
  doi          = {10.1109/TCC.2024.3452094},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1218-1231},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Large-scale measurements and optimizations on latency in edge clouds},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transfer learning based multi-objective evolutionary
algorithm for dynamic workflow scheduling in the cloud. <em>TCC</em>,
<em>12</em>(4), 1200–1217. (<a
href="https://doi.org/10.1109/TCC.2024.3450858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Managing scientific applications in the Cloud poses many challenges in terms of workflow scheduling, especially in handling multi-objective workflow scheduling under quality of service (QoS) constraints. However, most studies address the workflow scheduling problem on the premise of the unchanged environment, without considering the high dynamics of the Cloud. In this paper, we model the constrained workflow scheduling in a dynamic Cloud environment as a dynamic multi-objective optimization problem with preferences, and propose a transfer learning based multi-objective evolutionary algorithm (TL-MOEA) to tackle the workflow scheduling problem of dynamic nature. Specifically, an elite-led transfer learning strategy is proposed to explore effective parameter adaptation for the MOEA by transferring helpful knowledge from elite solutions in the past environment to accelerate the optimization process. In addition, a multi-space diversity learning strategy is developed to maintain the diversity of the population. To satisfy various QoS constraints of workflow scheduling, a preference-based selection strategy is further designed to enable promising solutions for each iteration. Extensive experiments on five well-known scientific workflows demonstrate that TL-MOEA can achieve highly competitive performance compared to several state-of-art algorithms, and can obtain triple win solutions with optimization objectives of minimizing makespan, cost and energy consumption for dynamic workflow scheduling with user-defined constraints.},
  archive      = {J_TCC},
  author       = {Huamao Xie and Ding Ding and Lihong Zhao and Kaixuan Kang},
  doi          = {10.1109/TCC.2024.3450858},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1200-1217},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Transfer learning based multi-objective evolutionary algorithm for dynamic workflow scheduling in the cloud},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online pricing and resource scheduling for profit
maximization of cloud storage providers. <em>TCC</em>, <em>12</em>(4),
1186–1199. (<a href="https://doi.org/10.1109/TCC.2024.3450876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is increasing competition among cloud object storage service (COSS) providers as the demand for COSSs grows. However, existing pricing models offered by commercial COSS providers fail to effectively adapt to changing client demand and resource supply. Consequently, many COSS providers are still grappling with operational challenges in maximizing their profits, such as pricing policy, load balancing, server scheduling, and energy management. In this paper, we propose a novel approach called time-dependent pricing and scheduling ( TD-PnS ), which is based on the Lyapunov-drift-minus-profit technique. To maximize the profits of COSS providers, TD-PnS enables joint and dynamic decision-making across several key factors that have been dealt with separately so far: (i) service pricing, (ii) CPU clock scaling and encoding scheduling, (iii) network scheduling, and (iv) energy storage management. We propose an enhanced version of TD-PnS , called TD-PnS-Adv , further to improve other aspects, such as system stabilization. Finally, through trace-driven simulations utilizing a real dataset, we demonstrate the superior performance of the proposed algorithms compared to existing algorithms and pricing models in terms of profit maximization.},
  archive      = {J_TCC},
  author       = {Kyungtae Lee and Yeongjin Kim},
  doi          = {10.1109/TCC.2024.3450876},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1186-1199},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Online pricing and resource scheduling for profit maximization of cloud storage providers},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FUSIONIZE++: Improving serverless application performance
using dynamic task inlining and infrastructure optimization.
<em>TCC</em>, <em>12</em>(4), 1172–1185. (<a
href="https://doi.org/10.1109/TCC.2024.3451108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Function-as-a-Service (FaaS) execution model increases developer productivity by removing operational concerns such as managing hardware or software runtimes. Developers, however, still need to partition their applications into FaaS functions, which is error-prone and complex: Encapsulating only the smallest logical unit of an application as a FaaS function maximizes flexibility and reusability. Yet, it also leads to invocation overheads, additional cold starts, and may increase cost due to double billing during synchronous invocations. Conversely, deploying an entire application as a single FaaS function avoids these overheads but decreases flexibility. In this paper we present Fusionize , a framework that automates optimizing for this trade-off by automatically fusing application code into an optimized multi-function composition. Developers only need to write fine-grained application code following the serverless model, while Fusionize automatically fuses different parts of the application into FaaS functions, manages their interactions, and configures the underlying infrastructure. At runtime, it monitors application performance and adapts it to minimize request-response latency and costs. Real-world use cases show that Fusionize can improve the deployment artifacts of the application, reducing both median request-response latency and cost of an example IoT application by more than 35%.},
  archive      = {J_TCC},
  author       = {Trever Schirmer and Joel Scheuner and Tobias Pfandzelter and David Bermbach},
  doi          = {10.1109/TCC.2024.3451108},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1172-1185},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {FUSIONIZE++: Improving serverless application performance using dynamic task inlining and infrastructure optimization},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Burst load frequency prediction based on google cloud
platform server. <em>TCC</em>, <em>12</em>(4), 1158–1171. (<a
href="https://doi.org/10.1109/TCC.2024.3449884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread use of cloud computing platforms has increased server load pressure. Especially the frequent occurrence of burst load problems caused resource waste, data damage and loss, and security loopholes, which have posed a severe threat to the service capabilities and stability of the cloud platform. To reduce or avoid the harm caused by burst load problems, this article conducts in-depth research on the frequency of burst loads. Based on Google cluster tracking data, this paper proposes a new burst load frequency calculation model called the ”Two-step Judgment” and a burst load frequency prediction model called the ”Combined-LSTM. ” The Two-step Judgment model uses data attributes for rough judgment and then uses the random forest algorithm for precise judgment to ensure accurate calculation of the frequency of burst loads. The Combined-LSTM model is a multi-input single-output prediction model constructed using a multi-model ensemble method. This model combines the advantages of the 1-Dimensional Convolutional Neural Network(1D-CNN), Gated Recurrent Unit(GRU), and Long Short-Term Memory(LSTM) and uses parallel computing methods to achieve accurate prediction of burst load frequency. According to the model evaluation, the Two-step Judgment model and the Combined-LSTM model showed significant advantages over other prediction models in accuracy, generalization ability, and time complexity.},
  archive      = {J_TCC},
  author       = {Hui Wang},
  doi          = {10.1109/TCC.2024.3449884},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1158-1171},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Burst load frequency prediction based on google cloud platform server},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel scheduling approach for spark workflow tasks with
deadline and uncertain performance in multi-cloud networks.
<em>TCC</em>, <em>12</em>(4), 1145–1157. (<a
href="https://doi.org/10.1109/TCC.2024.3449771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {These days, the usage of cloud computing services for different applications has been growing progressively. The applications, including business, commerce, healthcare, and others, require additional computation capabilities for their executions. To fulfil their expanding computational demands, cloud computing offers a pay-as-you-go billing model to run these applications cost-effectively. However, due to the complex requirements of these applications, more than one cloud system is required because single-cloud solutions are often limited by resource constraints, such as inadequate storage and computing power, as well as single-point failures that can compromise the integrity of the entire application. Consequently, multi-cloud strategies, which provide more scalable storage and computing resources, are becoming increasingly popular. However, the multi-cloud landscape consists of many cloud providers, and effectively managing workflow scheduling presents a significant hurdle in this dynamic environment. This paper focuses on scheduling Spark workflow tasks in multi-cloud networks. It addresses the challenges posed by different pricing models, dynamic resource provisioning, inter- and intra-transmission time, and the instability of resource performance. To solve these challenges, we propose a novel heuristic-based approach that considers different constraints such as VM instances heterogeneity, priority constraints, transmission times, and the impact of performance uncertainty. The goal is to schedule all tasks on virtual machines (VMs) with rental costs as low as possible while meeting workflow deadlines. The simulation results show that the proposed method effectively schedules Spark workflow tasks in multi-cloud networks, improving the scheduling performance by 50% compared to existing approaches.},
  archive      = {J_TCC},
  author       = {Kamran Yaseen Rajput and Xiaoping Li and Jinquan Zhang and Abdullah Lakhan},
  doi          = {10.1109/TCC.2024.3449771},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1145-1157},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A novel scheduling approach for spark workflow tasks with deadline and uncertain performance in multi-cloud networks},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enabling authorized fine-grained data retrieval over
aggregated encrypted medical data in cloud-assisted e-health systems.
<em>TCC</em>, <em>12</em>(4), 1131–1144. (<a
href="https://doi.org/10.1109/TCC.2024.3445430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Encrypted medical data outsourced to cloud servers can be used for personal health certification, health monitoring, and medical research. These data are essential to support the development of the medical industry. However, the traditional peer-to-peer data-sharing paradigm can lead to data abuse by malicious data analysis centers. Moreover, the encryption used to protect users’ outsourced privacy restricts the flexibility of data retrieval. Based on the modified double trapdoor cryptosystem, we propose an authorized data retrieval scheme over aggregated encrypted medical data (ADR-AED) in cloud-assisted e-healthcare systems. In ADR-AED, patients can access and decrypt personal data and authorize the data analysis center (DAC) to retrieve corresponding data. Specifically, we design an authorized retrieval-test mechanism for a group of patients to DAC. This allows DAC to extract valuable information from a threshold number of authorized users. Additionally, each patient can flexibly retrieve fine-grained medical data in different periods and submit them to a doctor for diagnostic analysis. The security analysis and performance evaluation demonstrate the feasibility of ADR-AED in the deployment of cloud-assisted e-healthcare systems.},
  archive      = {J_TCC},
  author       = {Wei Tang and Xiaojun Zhang and Dawu Gu and Chao Huang and Jingting Xue and Xiangyu Liang},
  doi          = {10.1109/TCC.2024.3445430},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1131-1144},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Enabling authorized fine-grained data retrieval over aggregated encrypted medical data in cloud-assisted E-health systems},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient secure CNN inference: A multi-server framework
based on conditional separable and homomorphic encryption. <em>TCC</em>,
<em>12</em>(4), 1116–1130. (<a
href="https://doi.org/10.1109/TCC.2024.3443405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning inference has become a fundamental component of cloud service providers, while privacy issues during services have received significant attention. Although many privacy-preserving schemes have been proposed, they require further improvement. In this article, we propose Serpens , an efficient convolutional neural network (CNN) secure inference framework to protect users’ uploaded data. We introduce a pair of novel concepts, namely separable and conditional separable, to determine whether a layer in CNNs can be computed over multiple servers or not. We demonstrate that linear layers are separable and construct factor-functions to reduce their overhead to nearly zero. For the two nonlinear layers, i.e., ReLU and max pooling, we design four secure protocols based on homomorphic encryption and random masks for two- and n-server settings. These protocols are essentially different from existing schemes, which are primarily based on garbled circuits. In addition, we extensively propose a method to split the image securely. The experimental results demonstrate that Serpens is $60\times -197\times$ faster than the previous scheme in the two-server setting. The superiority of Serpens is even more significant in the n-server setting, only less than an order of magnitude slower than performing plaintext inference over clouds.},
  archive      = {J_TCC},
  author       = {Longlong Sun and Hui Li and Yanguo Peng and Jiangtao Cui},
  doi          = {10.1109/TCC.2024.3443405},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1116-1130},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Efficient secure CNN inference: A multi-server framework based on conditional separable and homomorphic encryption},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enabling privacy-preserving parallel computation of linear
regression in edge computing networks. <em>TCC</em>, <em>12</em>(4),
1103–1115. (<a href="https://doi.org/10.1109/TCC.2024.3440656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear regression is a classical statistical model with a wide range of applications. The function of linear regression is to predict the value of a dependent variable (the output) given an independent variable (the input). The training of a linear regression model is to find a linear relationship between the input and the output based on data samples. IoT applications usually require real-time data processing. Nonetheless, the existing schemes about privacy-preserving outsourcing of linear regression cannot fully meet the rapid response requirement for computation. To address this issue, we consider employing multiple edge servers to accomplish privacy-preserving parallel computation of linear regression. We propose two novel solutions based on edge servers in edge computing networks and construct two efficient schemes for linear regression. In the first scheme, we present a new blinding technique for data privacy protection. Two edge servers are employed to execute the encrypted linear regression task in parallel. To further enhance the efficiency, we design an adaptive parallel algorithm, which is adopted in the second scheme. Multiple edge servers are employed in the second scheme to achieve higher efficiency. We analyze the correctness, privacy, and verifiability of the proposed schemes. Finally, we assess the computational overhead of the proposed schemes and conduct experiments to validate the performance advantages of the proposed schemes.},
  archive      = {J_TCC},
  author       = {Wenjing Gao and Jia Yu and Huaqun Wang},
  doi          = {10.1109/TCC.2024.3440656},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1103-1115},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Enabling privacy-preserving parallel computation of linear regression in edge computing networks},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BaaSLess: Backend-as-a-service (BaaS)-enabled workflows in
federated serverless infrastructures. <em>TCC</em>, <em>12</em>(4),
1088–1102. (<a href="https://doi.org/10.1109/TCC.2024.3439268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serverless is a popular paradigm for expressing compute-intensive applications as serverless workflows. In practice, a significant portion of the computing is typically offloaded to various Backend-as-a-Service (BaaS) cloud services. The recent rise of federated serverless and Sky computing offers cost and performance advantages for these BaaS-enabled serverless workflows. However, due to vendor lock-in and lack of service interoperability, many challenges remain that impact the development, deployment, and scheduling of BaaS-enabled serverless workflows in federated serverless infrastructures. This paper introduces BaaSLess – a novel platform that delivers global and dynamic federated BaaS to serverless workflows. BaaSLess provides: i) a novel SDK for uniform and dynamic access to federated BaaS services, reducing the complexity associated with the development of BaaS-enabled serverless workflows, ii) a novel globally-federated serverless BaaS framework that delivers a suite of BaaS-less ML services, including text-to-speech, speech-to-text, translation, and OCR, together with a globally-federated storage infrastructure, comprising AWS and Google cloud providers, and iii) a novel model and an algorithm for scheduling BaaS-enabled serverless workflows to improve their performance. Experimental results using three complementary BaaS-enabled serverless workflows show that BaaSLess improves workflow execution time by up to $2.95\times$ compared to the state-of-the-art serverless schedulers, often at a lower cost.},
  archive      = {J_TCC},
  author       = {Thomas Larcher and Philipp Gritsch and Stefan Nastic and Sashko Ristov},
  doi          = {10.1109/TCC.2024.3439268},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1088-1102},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {BaaSLess: Backend-as-a-service (BaaS)-enabled workflows in federated serverless infrastructures},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A scalable and write-optimized disaggregated b+-tree with
adaptive cache assistance. <em>TCC</em>, <em>12</em>(4), 1074–1087. (<a
href="https://doi.org/10.1109/TCC.2024.3437472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disaggregated memory (DM) architecture separates CPU and DRAM into computing/memory resource pools and interconnects them with high-speed networks. Storage systems on DM locate data by distributed index. However, existing distributed indexes either suffer from prohibitive synchronization overhead of write operation or sacrifice the performance of read operation, resulting in low throughput, high tail latency, and challenging trade-off. In this paper, we present Marlin+, a scalable and optimized B+-tree on DM. Marlin+ provides atomic granularity synchronization between write operations via three strategies: 1) a concurrent algorithm that is friendly to IDU operations (Insert, Delete, and Update), enabling different clients to concurrently operate on the same leaf node, 2) shared-exclusive leaf node lock, effectively preventing conflicts between index structure modification operation (SMO) and IDU operations, and 3) critical path compression of write to reduce latency of write operation. Moreover, Marlin+ proposes an adaptive remote address cache to accelerate the access of hot data. Compared to the state-of-the-art schemes based on DM, Marlin achieves 2.21× higher throughput and 83.4% lower P99 latency under YCSB hybrid workloads. Compared to Marlin, Marlin+ improves the throughput by up to 1.58× and reduces the P50 latency by up to 50.5% under YCSB read-intensive workloads.},
  archive      = {J_TCC},
  author       = {Hang An and Fang Wang and Dan Feng and Xiaomin Zou and Zefeng Liu and Jianshun Zhang},
  doi          = {10.1109/TCC.2024.3437472},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1074-1087},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A scalable and write-optimized disaggregated b+-tree with adaptive cache assistance},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparkle: Deep learning driven autotuning for taming
high-dimensionality of spark deployments. <em>TCC</em>, <em>12</em>(4),
1058–1073. (<a href="https://doi.org/10.1109/TCC.2024.3437484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential growth of data in the Cloud has highlighted the need for more efficient data processing. In-Memory Computing frameworks (e.g., Spark) offer improved efficiency for large-scale data analytics, however, they also provide a plethora of configuration parameters that affect the resource consumption and performance of applications. Manually optimizing these parameters is a time-consuming process, due to i) the high-dimensional configuration space, ii) the complex inter-relationship between different parameters, iii) the diverse nature of workloads and iv) the inherent data heterogeneity. We introduce Sparkle , an end-to-end deep learning-based framework for automating the performance modeling and tuning of Spark applications. We introduce a modular DNN architecture that expands to the entire Spark parameter configuration space and provides a universal performance modeling approach, completely eliminating the need for human or statistical reasoning. By employing a genetic optimization process, Sparkle quickly traverses the design space and identifies highly optimized Spark configurations. Our experiments on the HiBench benchmark suite show that Sparkle delivers an average prediction accuracy of 93%, with high generalization capabilities, i.e., $\approx 80\%$ accuracy for unseen workloads, dataset sizes and configurations, outperforming state-of-art. Regarding end-to-end optimization, Sparkle efficiently explores Spark&#39;s high-dimensional parameter space, delivering new dominant Spark configurations, which correspond to 65% Pareto coverage w.r.t its Spark native optimization counterpart.},
  archive      = {J_TCC},
  author       = {Dimosthenis Masouros and George Retsinas and Sotirios Xydis and Dimitrios Soudris},
  doi          = {10.1109/TCC.2024.3437484},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1058-1073},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Sparkle: Deep learning driven autotuning for taming high-dimensionality of spark deployments},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MFSSE: Multi-keyword fuzzy ranked symmetric searchable
encryption with pattern hidden in mobile cloud computing. <em>TCC</em>,
<em>12</em>(4), 1042–1057. (<a
href="https://doi.org/10.1109/TCC.2024.3430237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel Multi-keyword Fuzzy Symmetric Searchable Encryption (SSE) with patterns hidden, namely MFSSE. In MFSSE, the search trapdoor can be modified differently each time even if the keywords are the same when performing multi-keyword search to prevent the leakage of search patterns. Moreover, MFSSE modifies the search trapdoor by introducing random false negative and false positive errors to resist access pattern leakage. Furthermore, MFSSE utilizes efficient cryptographic algorithms (e.g., Locality-Sensitive Hashing) and lightweight operations (such as, integer addition, matrix multiplication, etc.) to minimize computational and communication, and storage overheads on mobile devices while meeting security and functional requirements. Specifically, its query process requires only a single round of communication, in which, the communication cost is linearly related to the number of the documents in the database, and is independent of the total number of keywords and the number of queried keywords; its computational complexity for matching a document is $O(1)$ ; and it requires only a small amount of fixed local storage (i.e., secret key) to be suitable for mobile scenarios. The experimental results demonstrate that MFSSE can prevent the leakage of access patterns and search patterns, while keeping a low communication and computation overheads.},
  archive      = {J_TCC},
  author       = {Dajiang Chen and Zeyu Liao and Zhidong Xie and Ruidong Chen and Zhen Qin and Mingsheng Cao and Hong-Ning Dai and Kuan Zhang},
  doi          = {10.1109/TCC.2024.3430237},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1042-1057},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {MFSSE: Multi-keyword fuzzy ranked symmetric searchable encryption with pattern hidden in mobile cloud computing},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Secure and flexible coded distributed matrix multiplication
based on edge computing for industrial metaverse. <em>TCC</em>,
<em>12</em>(4), 1026–1041. (<a
href="https://doi.org/10.1109/TCC.2024.3415165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Industrial Metaverse is driving a new revolution wave for smart manufacturing domain by reproducing the real industrial environment in a virtual space. Real-time synchronization and rendering of all industrial factors result in numerous time-sensitive and computation-intensive tasks, especially matrix multiplication. Distributed edge computing (DEC) can be exploited to handle these tasks due to its low-latency and powerful computing. In this paper, we propose an efficient and reliable coded DEC framework to compute large-scale matrix multiplication tasks. However, an existence of stragglers causes high computation latency that seriously limits the application of DEC in the Industrial Metaverse. To mitigate the impact of stragglers, we design a secure and flexible PolyDot (SFPD) code, which enables information theoretic security (ITS) protection. Several improvements can be achieved with the proposed SFPD. First, it can achieve a smaller recovery threshold than that of the existing codes in almost all settings. And compared with the original PolyDot codes, our SFPD code considers the extra workers required to add ITS protection. It also provides a flexible tradeoff between recovery threshold and communication &amp; computation loads by simply adjusting two given storage parameters $p$ and $t$ . Furthermore, as an important application scenario, the SFPD code is employed to secure model training in machine learning, which can alleviate the straggler effects and protect ITS of raw data. The experiments demonstrate that the SFPD code can significantly speed up the training process while providing ITS of data. Finally, we provide comprehensive performance analysis which shows the superiority of the SFPD code.},
  archive      = {J_TCC},
  author       = {Houming Qiu and Kun Zhu and Dusit Niyato},
  doi          = {10.1109/TCC.2024.3415165},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1026-1041},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Secure and flexible coded distributed matrix multiplication based on edge computing for industrial metaverse},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non-clairvoyant scheduling of distributed machine learning
with inter-job and intra-job parallelism on heterogeneous GPUs.
<em>TCC</em>, <em>12</em>(4), 1011–1025. (<a
href="https://doi.org/10.1109/TCC.2024.3414440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed machine learning (DML) has shown great promise in accelerating model training on multiple GPUs. To increase GPU utilization, a common practice is to let multiple learning jobs share GPU clusters, where the most fundamental and critical challenge is how to efficiently schedule these jobs on GPUs. However, existing works about DML job scheduling are constrained to settings with homogeneous GPUs. GPU heterogeneity is common in practice, but its influence on multiple DML job scheduling has been seldom studied. Moreover, DML jobs have internal structures that contain great parallelism potentials, which have not yet been fully exploited in the heterogeneous computing environment. In this paper, we propose Hare , a DML job scheduler that exploits both inter-job and intra-job parallelism in a heterogeneous GPU cluster. Hare adopts a relaxed fixed-scale synchronization scheme that allows independent tasks to be flexibly scheduled within a training round. Given full knowledge of job arrival time and sizes, we propose a fast heuristic algorithm to minimize the average job completion time and derive its theoretical bound is derived. Without prior knowledge of jobs, we propose an online algorithm based on the Heterogeneity-aware Least-Attained Service (HLAS) policy. We evaluate Hare using a small-scale testbed and a trace-driven simulator. The results show that it can outperform the state-of-the-art, achieving a performance improvement of about 2.94×.},
  archive      = {J_TCC},
  author       = {Fahao Chen and Peng Li and Celimuge Wu and Song Guo},
  doi          = {10.1109/TCC.2024.3414440},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {1011-1025},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Non-clairvoyant scheduling of distributed machine learning with inter-job and intra-job parallelism on heterogeneous GPUs},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive cloud resource quota scheme based on dynamic
portraits and task-resource matching. <em>TCC</em>, <em>12</em>(4),
996–1010. (<a href="https://doi.org/10.1109/TCC.2024.3410390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the unrestricted location of cloud resources, an increasing number of users are opting to apply for them. However, determining the appropriate resource quota has always been a challenge for applicants. Excessive quotas can result in resource wastage, while insufficient quotas can pose stability risks. Therefore, it&#39;s necessary to propose an adaptive quota scheme for cloud resource. Most existing researches have designed fixed quota schemes for all users, without considering the differences among users. To solve this, we propose an adaptive cloud quota scheme through dynamic portraits and task-resource optimal matching. Specifically, we first aggregate information from text, statistical, and fractal three dimensions to establish dynamic portraits. On this basis, the bidirectional mixture of experts (Bi-MoE) model is designed to match the most suitable resource combinations for tasks. Moreover, we define the time-varying rewards and utilize portrait-based reinforcement learning (PRL) to obtain the optimal quotas, which ensures stability and reduces waste. Extensive simulation results demonstrate that the proposed scheme achieves a memory utilization rate of around 70%. Additionally, it shows improvements in task execution stability, throughput, and the percentage of effective execution time.},
  archive      = {J_TCC},
  author       = {Zuodong Jin and Dan Tao and Peng Qi and Ruipeng Gao},
  doi          = {10.1109/TCC.2024.3410390},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {996-1010},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {An adaptive cloud resource quota scheme based on dynamic portraits and task-resource matching},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-data center tie-line power smoothing method based on
demand response. <em>TCC</em>, <em>12</em>(4), 983–995. (<a
href="https://doi.org/10.1109/TCC.2024.3410377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geographically distributed data centers (DCs) have emerged as significant energy consumers, which has led to the integration of renewable energy sources (RES) into DC power provisioning systems. However, the intermittent nature of RES and the randomness of user requests can cause significant fluctuations in DC operating power. It can be detrimental to the operation of IT equipment and lead to instability in the power grid. In this paper, aiming for tightly coupled interconnection scenarios with multi-data centers in varying regions, a multi-data center tie-line power smoothing method based on demand response is proposed. By modulating the power load of server clusters with workload scheduling, we establish a control model combined with intra-DC temporal task migration and inter-DC spatial task migration to deal with high-frequency power fluctuations. The uninterruptible power supply (UPS) battery control model is established to tackle low-frequency fluctuations. Furthermore, we design the two-stage heuristic power regulation algorithm to achieve the best practice of smoothing effect by real-time tracking of power targets after two-layer filtering. Finally, this paper performs a detailed performance simulation evaluation based on tracking data from a real DC and wind and photovoltaic (PV) new energy generation data, using four interconnected DC parks of different sizes across different regions as examples. The simulation results demonstrate that the proposed method effectively smoothing the multi-data center&#39;s tie-line power. Additionally, inter-DC temporal task migration serves as a viable solution to overcome the limitations of task migration response within a single DC, reducing the frequency of UPS battery bank charges and discharges, which in turn prolongs their service life. This approach facilitates the utilization of RES while maintaining power quality, and it also aids in reducing the escalating operation and maintenance expenses of DCs.},
  archive      = {J_TCC},
  author       = {Ting Yang and Yuxing Hou and Shaotang Cai and Jie Yu and Haibo Pen},
  doi          = {10.1109/TCC.2024.3410377},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {983-995},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Multi-data center tie-line power smoothing method based on demand response},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient user-centric privacy-friendly and flexible
wearable data aggregation and sharing. <em>TCC</em>, <em>12</em>(4),
967–982. (<a href="https://doi.org/10.1109/TCC.2024.3375801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wearable devices can offer services to individuals and the public. However, wearable data collected by cloud providers may pose privacy risks. To reduce these risks while maintaining full functionality, healthcare systems require solutions for privacy-friendly data processing and sharing that can accommodate three main use cases: (i) data owners requesting processing of their own data, and multiple data requesters requesting data processing of (ii) a single or (iii) multiple data owners. Existing work lacks data owner access control and does not efficiently support these cases, making them unsuitable for wearable devices. To address these limitations, we propose a novel, efficient, user-centric, privacy-friendly, and flexible data aggregation and sharing scheme, named SAMA. SAMA uses a multi-key partial homomorphic encryption scheme to allow flexibility in accommodating the aggregation of data originating from a single or multiple data owners while preserving privacy during the processing. It also uses ciphertext-policy attribute-based encryption scheme to support fine-grain sharing with multiple data requesters based on user-centric access control. Formal security analysis shows that SAMA supports data confidentiality and authorisation. SAMA has also been analysed in terms of computational and communication overheads. Our experimental results demonstrate that SAMA supports privacy-preserving flexible data aggregation more efficiently than the relevant state-of-the-art solutions.},
  archive      = {J_TCC},
  author       = {Khlood Jastaniah and Ning Zhang and Mustafa A. Mustafa},
  doi          = {10.1109/TCC.2024.3375801},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {10-12},
  number       = {4},
  pages        = {967-982},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Efficient user-centric privacy-friendly and flexible wearable data aggregation and sharing},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Corrections to “DNN surgery: Accelerating DNN inference on
the edge through layer partitioning.” <em>TCC</em>, <em>12</em>(3), 966.
(<a href="https://doi.org/10.1109/TCC.2024.3404548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we reference the previous conference version and complete the grant number mentioned in the acknowledgments of the conference version.},
  archive      = {J_TCC},
  author       = {Huanghuang Liang and Qianlong Sang and Chuang Hu and Dazhao Cheng and Xiaobo Zhou and Dan Wang and Wei Bao and Yu Wang},
  doi          = {10.1109/TCC.2024.3404548},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {966},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Corrections to “DNN surgery: Accelerating DNN inference on the edge through layer partitioning”},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Security, reliability, cost, and energy-aware scheduling of
real-time workflows in compute-continuum environments. <em>TCC</em>,
<em>12</em>(3), 954–965. (<a
href="https://doi.org/10.1109/TCC.2024.3426282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging computing paradigms like mist, edge, and fog computing address challenges in the real-time processing of vast Internet of Things (IoT) applications. Alongside, cloud computing offers a suitable platform for executing services. Together, they form a multi-tier computing environment known as compute-continuum to efficiently enhance data management and task execution of real-time tasks. The primary considerations for compute-continuum include variations in resource configuration and network architecture, rental cost model, application security needs, energy consumption, transmission latency, and system reliability. To address these problems, we propose two scheduling algorithms (RCSECH and RSECH) for real-time multi-workflow scheduling frameworks. Both algorithms optimize for rental cost, energy consumption, and task reliability when scheduling real-time workflows while considering deadlines and security requirements as constraints. RCSECH also factors in reliability alongside these constraints. The environment under investigation consists of a compute-continuum architecture consisting of mist, edge, fog, and cloud layers, each potentially composed of heterogeneous resources. The framework undergoes evaluation via simulation experiments, revealing promising results. Specifically, the framework exhibits the capability to enhance reliability by up to 7%, reduce energy consumption by 8%, surpass reliability constraints by more than 25%, and generate cost savings by at least 15%.},
  archive      = {J_TCC},
  author       = {Ahmad Taghinezhad-Niar and Javid Taheri},
  doi          = {10.1109/TCC.2024.3426282},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {954-965},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Security, reliability, cost, and energy-aware scheduling of real-time workflows in compute-continuum environments},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). <span class="math inline"><em>ε</em></span>&lt;Mml:math
xmlns:mml=“http://www.w3.org/1998/math/MathML”&gt;&lt;mml:mi&gt;ɛ&lt;/mml:mi&gt;&lt;/mml:math&gt;-LAP:
A lightweight and adaptive cache partitioning scheme with prudent
resizing decisions for content delivery networks. <em>TCC</em>,
<em>12</em>(3), 942–953. (<a
href="https://doi.org/10.1109/TCC.2024.3420454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As dependence on Content Delivery Networks (CDNs) increases, there is a growing need for innovative solutions to optimize cache performance amid increasing traffic and complicated cache-sharing workloads. Allocating exclusive resources to applications in CDNs boosts the overall cache hit ratio (OHR), enhancing efficiency. However, the traditional method of creating the miss ratio curve (MRC) is unsuitable for CDNs due to the diverse sizes of items and the vast number of applications, leading to high computational overhead and performance inconsistency. To tackle this issue, we propose a l ightweight and a daptive cache p artitioning scheme called $\varepsilon$ -LAP. This scheme uses a corresponding shadow cache for each partition and sorts them based on the average hit numbers on the granularity unit in the shadow caches. During partition resizing, $\varepsilon$ -LAP transfers storage capacity, measured in units of granularity, from the $(N-k+1)$ -th ( $k\leq \frac{N}{2}$ ) partition to the $k$ -th partition. A learning threshold parameter, i.e., $\varepsilon$ , is also introduced to prudently determine when to resize partitions, improving caching efficiency. This can eliminate about 96.8% of unnecessary partition resizing without compromising performance. $\varepsilon$ -LAP, when deployed in PicCloud at Tencent , improved OHR by 9.34% and reduced the average user access latency by 12.5 ms. Experimental results show that $\varepsilon$ -LAP outperforms other cache partitioning schemes in terms of both OHR and access latency, and it effectively adapts to workload variations.},
  archive      = {J_TCC},
  author       = {Peng Wang and Yu Liu and Ziqi Liu and Zhelong Zhao and Ke Liu and Ke Zhou and Zhihai Huang},
  doi          = {10.1109/TCC.2024.3420454},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {942-953},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {$\varepsilon$&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mml:mi&gt;ɛ&lt;/mml:mi&gt;&lt;/mml:math&gt;-LAP: a lightweight and adaptive cache partitioning scheme with prudent resizing decisions for content delivery networks},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-dimensional flat indexing for encrypted data.
<em>TCC</em>, <em>12</em>(3), 928–941. (<a
href="https://doi.org/10.1109/TCC.2024.3408905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of indexing encrypted data outsourced to an external cloud server to support server-side execution of multi-attribute queries. Our approach partitions the dataset in groups with the same number of tuples, and associates all tuples in a group with the same combination of index values, so to guarantee protection against static inferences. Our indexing approach does not require any modifications to the server-side software stack, and requires limited storage at the client for query support. The experimental evaluation considers, for the storage of the encrypted and indexed dataset, both a relational database (PostgreSQL) and a key-value database (Redis). We carried out extensive experiments evaluating client-storage requirements and query performance. The experimental results confirm the efficiency of our solution. The proposal is supported by an open source implementation.},
  archive      = {J_TCC},
  author       = {Sabrina De Capitani di Vimercati and Dario Facchinetti and Sara Foresti and Gianluca Oldani and Stefano Paraboschi and Matthew Rossi and Pierangela Samarati},
  doi          = {10.1109/TCC.2024.3408905},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {928-941},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Multi-dimensional flat indexing for encrypted data},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How to securely and efficiently solve the large-scale
modular system of linear equations on the cloud. <em>TCC</em>,
<em>12</em>(3), 913–927. (<a
href="https://doi.org/10.1109/TCC.2024.3408240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud-assisted computation empowers resource-constrained clients to efficiently tackle computationally intensive tasks by outsourcing them to resource-rich cloud servers. In the current era of Big Data, the widespread need to solve large-scale modular linear systems of equations ( $\mathcal {LMLSE}$ ) of the form $\mathbf {A}\mathbf {x}\equiv \mathbf {b}\;{\rm mod}\;{q}$ poses a significant challenge, particularly for lightweight devices. This paper delves into the secure outsourcing of $\mathcal {LMLSE}$ under a malicious single-server model and, to the best of our knowledge, introduces the inaugural protocol tailored to this specific context. The cornerstone of our protocol lies in the innovation of a novel matrix encryption method based on sparse unimodular matrix transformations. This novel technique bestows our protocol with several key advantages. First and foremost, it ensures robust privacy for all computation inputs, encompassing $\mathbf {A},\mathbf {b}, q$ , and the output $\mathbf {x}$ , as validated by thorough theoretical analysis. Second, the protocol delivers optimal verifiability, enabling clients to detect cloud server misbehavior with an unparalleled probability of 1. Furthermore, it boasts high efficiency, requiring only a single interaction between the client and the cloud server, significantly reducing local-client time costs. For an $m$ -by- $n$ matrix $\mathbf {A}$ , a given parameter $\lambda =\omega (\log q)$ , and $\rho =2.371552$ , the time complexity is diminished from $O(\max \lbrace m n^{\rho -1}, m^{\rho -2} n^{2}\rbrace \cdot (\log q)^{2})$ to $O((mn+m^{2})\lambda \log q+mn(\log q)^{2})$ . The comprehensive results of our experimental performance evaluations substantiate the protocol&#39;s practical efficiency and effectiveness.},
  archive      = {J_TCC},
  author       = {Chengliang Tian and Jia Yu and Panpan Meng and Guoyan Zhang and Weizhong Tian and Yan Zhang},
  doi          = {10.1109/TCC.2024.3408240},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {913-927},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {How to securely and efficiently solve the large-scale modular system of linear equations on the cloud},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to optimize workflow scheduling for an edge–cloud
computing environment. <em>TCC</em>, <em>12</em>(3), 897–912. (<a
href="https://doi.org/10.1109/TCC.2024.3408006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread deployment of intelligent Internet of Things (IoT) devices brings tighter latency demands on complex workload patterns such as workflows. In such applications, tremendous dataflows are generated and processed in accordance with specific service chains. Edge computing has proven its feasibility in reducing the traffic in the core network and relieving cloud datacenters of fragmented computational demands. However, the efficient scheduling of workflows in hybrid edge–cloud networks is still challenging for the intelligent IoT paradigm. Existing works make dispatching decisions prior to real execution, making it difficult to cope with the dynamicity of the environment. Consequently, the schedulers are affected both by the scheduling strategy and by the mutual impact of dynamic workloads. We design an intelligent workflow scheduler for use in an edge–cloud network where workloads are generated with continuous steady arrivals. We develop new graph neural network (GNN)-based representations for task embedding and we design a proximal policy optimization (PPO)-based online learning scheduler. We further introduce an intrinsic reward to obtain an instantaneous evaluation of the dispatching decision and correct the scheduling policy on-the-fly. Numerical results validate the feasibility of our proposal as it outperforms existing works with an improved quality of service (QoS) level.},
  archive      = {J_TCC},
  author       = {Kaige Zhu and Zhenjiang Zhang and Sherali Zeadally and Feng Sun},
  doi          = {10.1109/TCC.2024.3408006},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {897-912},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Learning to optimize workflow scheduling for an Edge–Cloud computing environment},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Delay-sensitive task offloading optimization by geometric
programming. <em>TCC</em>, <em>12</em>(3), 889–896. (<a
href="https://doi.org/10.1109/TCC.2024.3406384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile cloud computing is an emerging technology to address the resource limitation of mobile terminals. These terminals need to satisfy the performance requirements of emerging resource-consuming applications. Among these applications, delay-sensitive applications are becoming popular with the requirements of low execution times. Satisfying the delay requirements of these applications is the main objective in the task offloading of mobile cloud computing. In this paper, considering a network of wireless and wired infrastructures, a resource allocation problem in the form of a non-convex problem is formulated to provide a fair delay for offloaded tasks by delay-sensitive applications. Both transmission and computation delays are included in the formulation of the offloading delay. To tackle the problem&#39;s complexity, the assignment of mobile terminals to radio access networks and cloud servers is done by proposing greedy assignment solutions. The derived problem which is a geometric programming problem is then solved using convex programming. The performance of the proposed solution is evaluated versus the number of mobile terminals with different values of bandwidth resources at the radio network, workloads, and demand CPU cycles at mobile terminals. Numerical results demonstrate the effectiveness of the proposed solution to decrease the offloading delay in comparison with similar schemes.},
  archive      = {J_TCC},
  author       = {Mohammad Fathi and Mohammad Saroughi and Azarhedi Zareie},
  doi          = {10.1109/TCC.2024.3406384},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {889-896},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Delay-sensitive task offloading optimization by geometric programming},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving data locality of tasks by executor allocation in
spark computing environment. <em>TCC</em>, <em>12</em>(3), 876–888. (<a
href="https://doi.org/10.1109/TCC.2024.3406041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of data locality is crucial for distributed systems (e.g., Spark and Hadoop) to process Big Data. Most of the existing research optimized the data locality from the aspect of task scheduling. However, as the execution container of Spark&#39;s tasks, the executor launched on different nodes can directly affect the data locality achieved by the tasks. This article tries to improve the data locality of tasks by executor allocation in Spark framework. First, because of different communication modes at stages, we separately model the communication cost of tasks for transferring input data to the executors. Then formalize an optimal executor allocation problem to minimize the total communication cost of transferring all input data. This problem is proven to be NP-hard. Finally, we present a greed dropping heuristic algorithm to provide solution to the executor allocation problem. Our proposals are implemented in Spark-3.4.0 and its performance is evaluated through representative micro-benchmarks (i.e., WordCount , Join , Sort ) and macro-benchmarks (i.e., PageRank and LDA ). Extensive experiments show that the proposed executor allocation strategy can decrease the network traffic and data access time by improving the data locality during the task scheduling. Its performance benefits are particularly significant for iterative applications.},
  archive      = {J_TCC},
  author       = {Zhongming Fu and Mengsi He and Yang Yi and Zhuo Tang},
  doi          = {10.1109/TCC.2024.3406041},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {876-888},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Improving data locality of tasks by executor allocation in spark computing environment},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A group-vehicles oriented reputation assessment scheme for
edge VANETs. <em>TCC</em>, <em>12</em>(3), 859–875. (<a
href="https://doi.org/10.1109/TCC.2024.3406509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of the smart traffic, the traditional vehicular Ad hoc Networks (VANETs) and Traffic Estimation and Prediction System (TrEPS) do not satisfy the growing safety requirement, due to the network delay, transmit price and privacy security. In this paper, we propose a group-vehicles oriented reputation assessment scheme for edge VANETs. Firstly, based on edge computing, we build a reputation assessment framework for Group-Vehicles, to validate the correctness of message for other vehicles rapidly. Secondly, through filtering the malicious feedback and faulty message, our scheme can effectively defend against the Bad-mouth attack and Zigzag attack to assure the security of VANETs. Thirdly, the message isolation is implemented by the group-vehicles management, to enhance the privacy security of scheme. In the end, we validate the effectiveness of our scheme through experiments. In other words, even though the proportion of Bad-mouth attack vehicles is about 40%, the precision is 92.12%, and the recall is 88.25%. Also, the proportion of Zigzag attack vehicles is about 40%, the precision is 88.52%, and the recall is 86.75%.},
  archive      = {J_TCC},
  author       = {Changbo Ke and Fu Xiao and Yan Cao and Zhiqiu Huang},
  doi          = {10.1109/TCC.2024.3406509},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {859-875},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A group-vehicles oriented reputation assessment scheme for edge VANETs},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyperion: Hardware-based high-performance and secure system
for container networks. <em>TCC</em>, <em>12</em>(3), 844–858. (<a
href="https://doi.org/10.1109/TCC.2024.3403175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Containers have become the predominant virtualization technique for deploying microservices in cloud environments. However, container networking, critical for microservice functionality, often introduces significant overhead and resource consumption, potentially degrading the performance of microservices. This challenge arises from the complexity of the software-based network data plane, responsible for network virtualization and access control within container traffic. To tackle this challenge, we propose Hyperion , a novel hardware-based container networking system that prioritizes high performance and security. Leveraging smartNICs, commonly found in cloud environments, Hyperion implements a fully-functional container network data plane, encompassing network virtualization and access control. It also has the capability to dynamically optimize its data plane for agile responses to frequent changes in container environments, ensuring up-to-date data plane operation. This hardware-based design empowers Hyperion to significantly improve the overall container networking performance without relying on the host system resources. Notably, Hyperion seamlessly integrates with existing containerized applications without necessitating modifications. Our evaluation shows that compared to state-of-the-art solutions, Hyperion achieves significant improvements in HTTP container communication latency and throughput by up to 2.25x and 4.3x, respectively. Furthermore, it reduces CPU utilization associated with container networking by up to 4x.},
  archive      = {J_TCC},
  author       = {Myoungsung You and Minjae Seo and Jaehan Kim and Seungwon Shin and Jaehyun Nam},
  doi          = {10.1109/TCC.2024.3403175},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {844-858},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Hyperion: Hardware-based high-performance and secure system for container networks},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BPS: Batching, pipelining, surgeon of continuous deep
inference on collaborative edge intelligence. <em>TCC</em>,
<em>12</em>(3), 830–843. (<a
href="https://doi.org/10.1109/TCC.2024.3399616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Users on edge generate deep inference requests continuously over time. Mobile/edge devices located near users can undertake the computation of inference locally for users, e.g., the embedded edge device on an autonomous vehicle. Due to limited computing resources on one mobile/edge device, it may be challenging to process the inference requests from users with high throughput. An attractive solution is to (partially) offload the computation to a remote device in the network. In this paper, we examine the existing inference execution solutions across local and remote devices and propose an adaptive scheduler, a BPS scheduler, for continuous deep inference on collaborative edge intelligence. By leveraging data parallel, neurosurgeon, reinforcement learning techniques, BPS can boost the overall inference performance by up to $8.2 \times$ over the baseline schedulers. A lightweight compressor, FF, specialized in compressing intermediate output data for neurosurgeon, is proposed and integrated into the BPS scheduler. FF exploits the operating character of convolutional layers and utilizes efficient approximation algorithms. Compared to existing compression methods, FF achieves up to 86.9% lower accuracy loss and up to 83.6% lower latency overhead.},
  archive      = {J_TCC},
  author       = {Xueyu Hou and Yongjie Guan and Nakjung Choi and Tao Han},
  doi          = {10.1109/TCC.2024.3399616},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {830-843},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {BPS: Batching, pipelining, surgeon of continuous deep inference on collaborative edge intelligence},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Are ARM cloud servers ready for database workloads? An
experimental study. <em>TCC</em>, <em>12</em>(3), 818–829. (<a
href="https://doi.org/10.1109/TCC.2024.3393895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Almost all major cloud providers offer virtual machines running on servers with 64-bit ARM CPUs. For example, Amazon Web Services (AWS) designed custom ARM-based CPUs named Graviton2 and Graviton3. Other cloud providers, such as Microsoft Azure and Google Cloud Platform (GCP), employ servers with Ampere Altra CPUs. In this context, we conduct a comprehensive experimental study covering in-memory key-value stores, relational databases, enterprise blockchains, and Machine Learning inference. We cover all the available types of ARM cloud processors, including Graviton2 (AWS), Graviton3 (AWS), Ampere Altra (Azure and GCP), Yitian 710 (Alibaba Cloud), and Kunpeng 920 (Huawei Cloud). Our analysis shows that Yitian and Graviton3 are serious competitors for servers with Intel Xeon CPUs, achieving similar or better results with in-memory workloads. However, the performance of OLAP, ML inference, and blockchain on ARM-based servers is below that of Xeon. The reasons are mainly threefold 1) un-optimized software, 2) lower clock frequency, and 3) lower performance at core level. Surprisingly, ARM servers spend 2X more time in Linux kernel system calls compared to Xeon servers. Nonetheless, ARM-based servers show great potential. Given their lower cloud computing price, ARM servers could be the ideal choice when the performance is not critical.},
  archive      = {J_TCC},
  author       = {Dumitrel Loghin},
  doi          = {10.1109/TCC.2024.3393895},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {818-829},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Are ARM cloud servers ready for database workloads? an experimental study},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comments on “privacy aware data deduplication for side
channel in cloud storage.” <em>TCC</em>, <em>12</em>(2), 814–817. (<a
href="https://doi.org/10.1109/TCC.2024.3376996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TCC},
  author       = {Xin Tang and Yudan Zhu and Mingjun Fu},
  doi          = {10.1109/TCC.2024.3376996},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {814-817},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Comments on “Privacy aware data deduplication for side channel in cloud storage”},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Service recovery in NFV-enabled networks: Algorithm design
and analysis. <em>TCC</em>, <em>12</em>(2), 800–813. (<a
href="https://doi.org/10.1109/TCC.2024.3402185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network function virtualization (NFV), a novel network architecture, promises to offer a lot of convenience in network design, deployment, and management. This paradigm, although flexible, suffers from many risks engendering interruption of services, such as node and link failures. Thus, resiliency is one of the requirements in NFV-enabled network design for recovering network services once occurring failures. Therefore, in addition to a primary chain of virtual network functions (VNFs) for a service, one typically allocates the corresponding backup VNFs to satisfy the resiliency requirement. Nevertheless, this approach consumes network resources that can be inherently employed to deploy more services. Moreover, one can hardly recover all interrupted services due to the limitation of network backup resources. In this context, the importance of the services is one of the factors employed to judge the recovery priority. In this article, we first assign each service a weight expressing its importance, then seek to retrieve interrupted services such that the total weight of the recovered services is maximum. Hence, we also call this issue the VNF restoration for recovering weighted services (VRRWS) problem. We next demonstrate the difficulty of the VRRWS problem is NP-hard and propose an effective technique, termed online recovery algorithm (ORA), to address the problem without necessitating the backup resources. Eventually, we conduct extensive simulations to evaluate the performance of the proposed algorithm as well as the factors affecting the recovery. The experiment shows that the available VNFs should be migrated to appropriate nodes during the recovery process to achieve better results.},
  archive      = {J_TCC},
  author       = {Dung H. P. Nguyen and Chih-Chieh Lin and Tu N. Nguyen and Shao-I Chu and Bing-Hong Liu},
  doi          = {10.1109/TCC.2024.3402185},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {800-813},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Service recovery in NFV-enabled networks: Algorithm design and analysis},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing long-term cloud workload forecasting framework:
Anomaly handling and ensemble learning in multivariate time series.
<em>TCC</em>, <em>12</em>(2), 789–799. (<a
href="https://doi.org/10.1109/TCC.2024.3400859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting workloads and responding promptly with resource scaling and migration is critical to optimizing operations and enhancing resource management in cloud environments. However, the diverse and dynamic nature of devices within cloud environments complicates workload forecasting. These challenges often lead to service level agreement violations or inefficient resource usage. Hence, this paper proposes an Enhanced Long-Term Cloud Workload Forecasting (E-LCWF) framework designed specifically for efficient resource management in these heterogeneous and dynamic environments. The E-LCWF framework processes individual resource workloads as multivariate time series and enhances model performance through anomaly detection and handling. Additionally, the E-LCWF framework employs an error-based ensemble approach, using transformer-based models and Long-Term Time Series Forecasting (LTSF) linear models, each of which has demonstrated exceptional performance in LTSF. Experimental results obtained using virtual machine data from real-world management information systems and manufacturing execution systems show that the E-LCWF framework outperforms state-of-the-art models in forecasting accuracy.},
  archive      = {J_TCC},
  author       = {Yeong-Min Kim and Seunghwan Song and Byoung-Mo Koo and Jeena Son and Yeseul Lee and Jun-Geol Baek},
  doi          = {10.1109/TCC.2024.3400859},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {789-799},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Enhancing long-term cloud workload forecasting framework: Anomaly handling and ensemble learning in multivariate time series},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Achieving privacy-preserving online multi-layer perceptron
model in smart grid. <em>TCC</em>, <em>12</em>(2), 777–788. (<a
href="https://doi.org/10.1109/TCC.2024.3399771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of Big Data technology, the power industry has also entered the data-driven intelligence era. Cloud computing-based smart grids give the power industry stronger capabilities in data analytics. Electricity load forecasting in the cloud helps smart grids allocate resources appropriately. However, the users’ privacy is easily compromised in the load forecasting process with cloud computing. The electricity usage data collected by the system may contain sensitive information about the users, which could lead to serious privacy leakage. In order to solve the issues, we propose a novel privacy-preserving cloud-aided load forecasting scheme for the cloud computing-based smart grid. It contains a secure online training algorithm and an efficient real-time forecasting algorithm. Meanwhile, the two-party interaction security scheme is more suitable for real-world applications. Before being sent to the cloud server, the control center of the smart grids encrypts the data using homomorphic encryption. During the process of model training and forecasting, the data remains securely encrypted at all times to avoid the risk of data privacy breaches. Finally, security and experimental analyses show that our scheme effectively avoids privacy leakage while reducing resource consumption.},
  archive      = {J_TCC},
  author       = {Chunqiang Hu and Huijun Zhuang and Jiajun Chen and Pengfei Hu and Tao Xiang and Jiguo Yu},
  doi          = {10.1109/TCC.2024.3399771},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {777-788},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Achieving privacy-preserving online multi-layer perceptron model in smart grid},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An open API architecture to discover the trustworthy
explanation of cloud AI services. <em>TCC</em>, <em>12</em>(2), 762–776.
(<a href="https://doi.org/10.1109/TCC.2024.3398609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents the design of an open-API-based explainable AI (XAI) service to provide feature contribution explanations for cloud AI services. Cloud AI services are widely used to develop domain-specific applications with precise learning metrics. However, the underlying cloud AI services remain opaque on how the model produces the prediction. We argue that XAI operations are accessible as open APIs to enable the consolidation of the XAI operations into the cloud AI services assessment. We propose a design using a microservice architecture that offers feature contribution explanations for cloud AI services without unfolding the network structure of the cloud models. We can also utilize this architecture to evaluate the model performance and XAI consistency metrics showing cloud AI services’ trustworthiness. We collect provenance data from operational pipelines to enable reproducibility within the XAI service. Furthermore, we present the discovery scenarios for the experimental tests regarding model performance and XAI consistency metrics for the leading cloud vision AI services. The results confirm that the architecture, based on open APIs, is cloud-agnostic. Additionally, data augmentations result in measurable improvements in XAI consistency metrics for cloud AI services.},
  archive      = {J_TCC},
  author       = {Zerui Wang and Yan Liu and Jun Huang},
  doi          = {10.1109/TCC.2024.3398609},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {762-776},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {An open API architecture to discover the trustworthy explanation of cloud AI services},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low-carbon operation of data centers with joint workload
sharing and carbon allowance trading. <em>TCC</em>, <em>12</em>(2),
750–761. (<a href="https://doi.org/10.1109/TCC.2024.3396476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data centers (DCs) have witnessed rapid growth due to the proliferation of cloud computing and internet services. The huge electricity demand and the associated carbon emissions of DCs have great impacts on power system reliability and environmental sustainability. This paper proposes a bilevel model for low-carbon operation of DCs via carbon-integrated locational marginal prices (CLMPs). In the upper level, the power system operator sequentially solves the optimal power flow and the carbon emission flow problems to determine the CLMPs. In the lower level, a joint workload sharing and carbon trading model for DCs is developed to minimize their overall operation cost while keeping each DC&#39;s carbon footprint within its carbon allowance. To solve the bilevel model and preserve the privacy of DCs, we propose a bisection-embedded iterative method. It can tackle the issue of oscillation, thereby ensuring convergence. In addition, a filtering mechanism-based distributed algorithm is proposed to solve the lower-level DC problem in a distributed manner with much reduced communication overhead. Case studies on both small-scale and large-scale systems demonstrate the effectiveness and benefits of the proposed method.},
  archive      = {J_TCC},
  author       = {Dongxiang Yan and Mo-Yuen Chow and Yue Chen},
  doi          = {10.1109/TCC.2024.3396476},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {750-761},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Low-carbon operation of data centers with joint workload sharing and carbon allowance trading},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FLAIR: A fast and low-redundancy failure recovery framework
for inter data center network. <em>TCC</em>, <em>12</em>(2), 737–749.
(<a href="https://doi.org/10.1109/TCC.2024.3393735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the fast developments of 5G and IoT technologies, Inter-Datacenter (Inter-DC) networks are facing unprecedented pressure to duplicate large volumes of geographically distributed user data in a real-time manner. Meanwhile, with the expansion of Inter-DC networks scale, link/node failures also become increasingly frequent, negatively affecting the data transmission efficiency. Therefore, link failure recovery methods become of utmost importance. Many works investigated fast failure recovery, yet none of them consider the deployment overhead of such recovery schemes. While in this article, we found that the side-effect of deploying recovery strategies and the future availability of the recovered transmissions are also crucial for fast recovery. So we propose a fast and low-redundancy failure recovery framework, FLAIR, which consists of a fast recovery strategy FRAVaR and a redundancy removal algorithm ROSE. FRAVaR takes full consideration of deployment overhead by minimizing shuffle traffic. On its base, ROSE regularly eliminates the cumulative rerouting redundancy by removing unnecessary routing updates. The experiment results on 4 realistic network topologies show that FLAIR successfully reduces up to 48.2% deployment overhead compared with the state-of-the-art solutions, and thus reduces up to 70.2% recovery speed and improves up to 36% network utilization.},
  archive      = {J_TCC},
  author       = {Yuchao Zhang and Haoqiang Huang and Ahmed M. Abdelmoniem and Gaoxiong Zeng and Chenyue Zheng and Xirong Que and Wendong Wang and Ke Xu},
  doi          = {10.1109/TCC.2024.3393735},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {737-749},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {FLAIR: A fast and low-redundancy failure recovery framework for inter data center network},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralized funding of public goods in blockchain system:
Leveraging expert advice. <em>TCC</em>, <em>12</em>(2), 725–736. (<a
href="https://doi.org/10.1109/TCC.2024.3394973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Public goods projects, such as open-source technology, are essential for the blockchain ecosystem&#39;s growth. However, funding these projects effectively remains a critical issue within the ecosystem. Currently, the funding protocols for blockchain public goods lack professionalism and fail to learn from past experiences. To address this challenge, our research introduces a human oracle protocol involving public goods projects, experts, and funders. In our approach, funders contribute investments to a funding pool, while experts offer investment advice based on their expertise in public goods projects. The oracle&#39;s decisions on funding support are influenced by the reputations of the experts. Experts earn or lose reputation based on how well their project implementations align with their advice, with successful investments leading to higher reputations. Our oracle is designed to adapt to changing circumstances, such as experts exiting or entering the decision-making process. We also introduce a regret bound to gauge the oracle&#39;s effectiveness. Theoretically, we establish an upper regret bound for both static and dynamic models and demonstrate its closeness to an asymptotically equal lower bound. Empirically, we implement our protocol on a test chain and show that our oracle&#39;s investment decisions closely mirror optimal investments in hindsight.},
  archive      = {J_TCC},
  author       = {Jichen Li and Yukun Cheng and Wenhan Huang and Mengqian Zhang and Jiarui Fan and Xiaotie Deng and Jan Xie and Jie Zhang},
  doi          = {10.1109/TCC.2024.3394973},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {725-736},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Decentralized funding of public goods in blockchain system: Leveraging expert advice},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and evaluation of a hierarchical characterization and
adaptive prediction model for cloud workloads. <em>TCC</em>,
<em>12</em>(2), 712–724. (<a
href="https://doi.org/10.1109/TCC.2024.3393114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Workload characterization and subsequent prediction are significant steps in maintaining the elasticity and scalability of resources in Cloud Data Centers. Due to the high variance in cloud workloads, designing a prediction algorithm that models the variations in the workload is a non-trivial task. If the workload predictor is unable to handle the dynamism in the workloads, then the result of the predictor may lead to over-provisioning or under-provisioning of cloud resources. To address this problem, we have created a Super Markov Prediction Model (SMPM) whose behaviour changes as per the change in the workload patterns. As the time progresses, based on the workload pattern SMPM uses different sequence models to predict the future workload. To evaluate the proposed model, we have experimented with Alibaba trace 2018, Google Cluster Trace (GCT), Alibaba trace 2020 and TPC-W workload trace. We have compared SMPM&#39;s prediction results with existing state-of-the-art prediction models and empirically verified that the proposed prediction model achieves a better accuracy as quantified using Root Mean Square Error (RMSE) and Mean Absolute Error (MAE).},
  archive      = {J_TCC},
  author       = {Karthick Seshadri and Korrapati Sindhu and S. Nagesh Bhattu and Chidambaran Kollengode},
  doi          = {10.1109/TCC.2024.3393114},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {712-724},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Design and evaluation of a hierarchical characterization and adaptive prediction model for cloud workloads},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). P4Hauler: An accelerator-aware in-network load balancer for
applications performance boosting. <em>TCC</em>, <em>12</em>(2),
697–711. (<a href="https://doi.org/10.1109/TCC.2024.3389658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TCC},
  author       = {Hesam Tajbakhsh and Ricardo Parizotto and Alberto Schaeffer-Filho and Israat Haque},
  doi          = {10.1109/TCC.2024.3389658},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {697-711},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {P4Hauler: An accelerator-aware in-network load balancer for applications performance boosting},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SVD: A scalable virtual machine disk format. <em>TCC</em>,
<em>12</em>(2), 684–696. (<a
href="https://doi.org/10.1109/TCC.2024.3391390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TCC},
  author       = {Kevin Nguetchouang and Stella Bitchebe and Theophile Dubuc and Mar Callau-Zori and Christophe Hubert and Pierre Olivier and Alain Tchana},
  doi          = {10.1109/TCC.2024.3391390},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {684-696},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {SVD: A scalable virtual machine disk format},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A blockchain-based framework to resolve the oligopoly issue
in cloud computing. <em>TCC</em>, <em>12</em>(2), 671–683. (<a
href="https://doi.org/10.1109/TCC.2024.3390933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing is one of the foundation technologies of Industry 4.0. Cloud 2.0 is the upcoming cloud technology that addresses several bottlenecks of Cloud 1.0. For instance, the presence of small service providers is threatened by the dominance of a few giant service providers in today’s cloud market in Cloud 1.0. Under this circumstance, the small service providers must work together to compete with the giant competitors to survive in the market. For that, small service providers require a transparent, fair, cost-effective, fault-tolerant, and easily scalable platform that can provide reliable and quality services to customers. This work introduces a blockchain-based framework to provide such a platform for cloud service providers and their customers. Here, a new consensus mechanism is proposed to maintain the system’s fairness, decentralization, and consistency. A consensus-based service monitoring concept is also introduced to assess the service quality. If a service provider does not deliver the committed quality of service (QoS), a penalty is imposed on the service provider. This framework is designed so that the service providers are always bound to provide committed QoS to the customers. Finally, we performed several experiments, and the experimental results corroborate our claims regarding the proposed framework.},
  archive      = {J_TCC},
  author       = {Amit Biswas and Gaurav Baranwal and Abhinav Kumar},
  doi          = {10.1109/TCC.2024.3390933},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {671-683},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A blockchain-based framework to resolve the oligopoly issue in cloud computing},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchal bilateral access control with constant size
ciphertexts for mobile cloud computing. <em>TCC</em>, <em>12</em>(2),
659–670. (<a href="https://doi.org/10.1109/TCC.2024.3386126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile cloud computing (MCC) integrates the advantages of mobile networks and cloud computing, enabling users to enjoy personalized services without constraints and restrictions of time and place. While this brings convenience, it also comes with risks such as privacy breaches and unauthorized access to outsourced data. Bilateral access control is a promising technique for addressing these issues. However, the current bilateral access control schemes cannot solve problems such as single point failure. To further enhance and enrich the existing schemes, we propose hierarchical bilateral access control. In the proposed scheme, the permission of generating encryption keys and decryption keys can be delegated to its child nodes, which alleviates the computation and communication overheads of the parent nodes and weaken the potential risks of single-point failure. Additionally, the ciphertext size remains constant, reducing the costs of transmitting and storing ciphertext and relieving resource limitations on devices. We then prove the privacy and authenticity of the scheme in the random oracle model. Finally, the comprehensive performance comparison and analysis demonstrate the efficiency of the proposed scheme.},
  archive      = {J_TCC},
  author       = {Axin Wu and Yinghui Zhang and Jianhao Zhu and Qiuxia Zhao and Yu Zhang},
  doi          = {10.1109/TCC.2024.3386126},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {659-670},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Hierarchal bilateral access control with constant size ciphertexts for mobile cloud computing},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weighted scheduling of time-sensitive coflows. <em>TCC</em>,
<em>12</em>(2), 644–658. (<a
href="https://doi.org/10.1109/TCC.2024.3384514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TCC},
  author       = {Olivier Brun and Rachid El-Azouzi and Quang-Trung Luu and Francesco De Pellegrini and Balakrishna J. Prabhu and Cédric Richier},
  doi          = {10.1109/TCC.2024.3384514},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {644-658},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Weighted scheduling of time-sensitive coflows},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Space-hard obfuscation against shared cache attacks and its
application in securing ECDSA for cloud-based blockchains. <em>TCC</em>,
<em>12</em>(2), 625–643. (<a
href="https://doi.org/10.1109/TCC.2024.3383661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud computing environments, virtual machines (VMs) running on cloud servers are vulnerable to shared cache attacks, such as Spectre and Foreshadow. By exploiting memory sharing among VMs, these attacks can compromise cryptographic keys in software modules. Program obfuscation serves as a promising countermeasure against key compromises by transforming a program into an unintelligent form while preserving its functionality. Unfortunately, for certain cryptographic algorithms such as the digital signature schemes, it is extremely difficult to construct provably secure obfuscators using traditional obfuscation approaches. To address such a challenge, this study proposes a novel approach to construct obfuscators for cryptographic algorithms named space-hard obfuscation, which can mitigate the threats from adversaries with the capability of acquiring a limited size of memory in shared cache attacks. Considering the extensive use of the Elliptic Curve Digital Signature Algorithm (ECDSA) in cloud-based Blockchain-as-a-Service (BaaS) and its potential vulnerability to shared cache attacks, we construct an exemplary scheme with provable security using space-hard obfuscation for ECDSA. Experimental results have demonstrated the scheme&#39;s high efficiency on cloud servers, as well as its successful integration with Hyperledger Fabric and Ethereum, two widely used blockchain systems.},
  archive      = {J_TCC},
  author       = {Yang Shi and Yimin Li and Tianyuan Luo and Xiong Jiang and Bowen Du and Hongfei Fan},
  doi          = {10.1109/TCC.2024.3383661},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {625-643},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Space-hard obfuscation against shared cache attacks and its application in securing ECDSA for cloud-based blockchains},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Makespan and security-aware workflow scheduling for cloud
service cost minimization. <em>TCC</em>, <em>12</em>(2), 609–624. (<a
href="https://doi.org/10.1109/TCC.2024.3382351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The market penetration of Infrastructure-as-a-Service (IaaS) in cloud computing is increasing benefiting from its flexibility and scalability. One of the most important issues for IaaS cloud service providers is to minimize the monetary cost while meeting cloud user experience requirements such as makespan and security. Prior works on cloud service cost minimization ignore either security or makespan which is very important for user experience. In this article, we propose a two-stage algorithm to solve the cloud service cost minimization problem at the premise of satisfying the security and makespan requirements of cloud users. Specifically, in the first stage, we propose a novel security service selection scheme to ensure system security by judiciously selecting security services with low cost for tasks under the constraints of time and security. In the second stage, to further reduce the cloud service cost, we design a workflow scheduling method based on an improved firefly algorithm (IFA). The IFA-based method schedules cloud service workflows to virtual machines of small cost at the premise of guaranteeing security and makespan. It can quickly find the workflow scheduling solution with minimized cost using our designed updating scheme and mapping operator. Extensive simulations are conducted on real-world workflows to verify the efficacy of the proposed two-stage method. Simulation results show that the proposed two-stage method outperforms the baseline and two benchmarking methods in terms of cost minimization without violating security and time constraints. Compared to benchmarking methods, the cloud service cost can be reduced by up to 57.6% by using our proposed approach.},
  archive      = {J_TCC},
  author       = {Liying Li and Chengliang Zhou and Peijin Cong and Yufan Shen and Junlong Zhou and Tongquan Wei},
  doi          = {10.1109/TCC.2024.3382351},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {609-624},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Makespan and security-aware workflow scheduling for cloud service cost minimization},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic task offloading in edge computing based on
dependency-aware reinforcement learning. <em>TCC</em>, <em>12</em>(2),
594–608. (<a href="https://doi.org/10.1109/TCC.2024.3381646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative edge computing (CEC) is an emerging computing paradigm in which edge nodes collaborate to perform tasks from end devices. Task offloading decides when and at which edge node tasks are executed. Most existing studies assume task profiles and network conditions are known in advance, which can hardly adapt to dynamic real-world computation environments. Some learning-based methods use online task offloading without considering task dependency and network flow scheduling, leading to underutilized resources and flow congestion. We study Online Dependent Task Offloading (ODTO) in CEC, jointly optimizing network flow scheduling to optimize quality of service by reducing task completion time and energy consumption. The challenge of ODTO lies in how to offload dependent tasks and schedule network flows in dynamic networks. We model ODTO as the Markov Decision Process (MDP) and propose an Asynchronous Deep Progressive Reinforcement Learning (ADPRL) approach that optimize offloading and bandwidth decisions. We design a novel dependency-aware reward mechanism to address task dependency and dynamic network. Extensive experiments on the Alibaba cluster trace dataset and synthetic dataset indicate that our algorithm outperforms heuristic and learning-based methods in average task completion time and energy consumption.},
  archive      = {J_TCC},
  author       = {Xiangchun Chen and Jiannong Cao and Yuvraj Sahni and Shan Jiang and Zhixuan Liang},
  doi          = {10.1109/TCC.2024.3381646},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {594-608},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Dynamic task offloading in edge computing based on dependency-aware reinforcement learning},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep reinforcement learning based dynamic flowlet switching
for DCN. <em>TCC</em>, <em>12</em>(2), 580–593. (<a
href="https://doi.org/10.1109/TCC.2024.3382132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flowlet switching has been proven to be an effective technology for fine-grained load balancing in data center networks. However, flowlet detection based on static flowlet timeout values, lacks accuracy and effectiveness in complex network environments. In this article, we propose a new deep reinforcement learning approach, called DRLet, to dynamically detect flowlets. DRLet offers two advantages: first, it provides dynamic flowlet timeout values to detect bursts into fine-grained flowlets; second, flowlet timeout values are automatically configured by the deep reinforcement learning agent, which only requires simple and measurable network states, instead of any prior knowledge, to achieve the pre-defined goal. With our approach, the flowlet timeout value dynamically matches the network load scenario, ensuring the accuracy and effectiveness of flowlet detection while suppressing packet reordering. Our results show that DRLet achieves superior performance compared to existing schemes based on static flowlet timeout values in both baseline and asymmetric topologies.},
  archive      = {J_TCC},
  author       = {Xinglong Diao and Huaxi Gu and Wenting Wei and Guoyong Jiang and Baochun Li},
  doi          = {10.1109/TCC.2024.3382132},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {580-593},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Deep reinforcement learning based dynamic flowlet switching for DCN},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Live migration of virtual machines based on dirty page
similarity. <em>TCC</em>, <em>12</em>(2), 563–579. (<a
href="https://doi.org/10.1109/TCC.2024.3379494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-copy-based Virtual Machine (VM) live migration seamlessly migrates the running VM to the target physical server by pre-copying memory pages and realizing updates through loop iterations. This method, which has high reliability and robustness, can effectively achieve load balancing and reduce energy consumption. It is widely used in the industry to manage server cluster resources. However, it also involves many problems, such as many dirty memory pages resulting from repeated transmission and convergence failure of iterative transmission. Hence, pre-copy live migration cannot efficiently allocate server cluster resources. To resolve these problems, a VM pre-copy live migration technology based on the similarity of dirty memory pages is proposed in this paper. The access priority of historical dirty memory pages was determined by calculating the similarity weight based on the Hamming distance. A priority-based delay transmission scheme for high dirty pages and low dirty pages was used to decrease the frequent transmission of high dirty memory pages, increase the convergence speed of the live-migration iterative copy process, and reduce the overall migration time of VMs. A comparative analysis of experimental results based on six dimensions showed that the proposed method achieved better migration efficiency than the conventional live migration strategy.},
  archive      = {J_TCC},
  author       = {Yucong Chen and Shuaixin Xu and Hubin Yang and Rui Zhou and Deke Guo and Qingguo Zhou},
  doi          = {10.1109/TCC.2024.3379494},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {563-579},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Live migration of virtual machines based on dirty page similarity},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anole: Scheduling flows for fast datacenter networks with
packet re-prioritization. <em>TCC</em>, <em>12</em>(2), 550–562. (<a
href="https://doi.org/10.1109/TCC.2024.3376716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many existing datacenter transports perform one-shot packet priority tagging at end-hosts and leave them fixed during the packet&#39;s transmission. In this article, we experimentally show that: 1) such fixed packet priority is not sufficient for FCT (flow completion time) minimization, and 2) adjusting packet transmission priority in the network requires effective coordination among switches. Building on these insights, we present Anole, a new datacenter transport that advocates packet re-prioritization in near-bottleneck switches to minimize FCT. To this end, Anole integrates three simple-yet-effective techniques. First, it employs an in-network telemetry (INT) based approach to dynamically detect the bottleneck for each flow. Second, it adopts an on-off rate control mechanism for each sender to pause heavily congested flows but send lightly- and non-congested ones. Last, it leverages an altruistic scheduling policy at each switch to let the flows whose next hops are bottleneck switches give way to others. We implement an Anole prototype based on DPDK and show, through both testbed experiments and simulations, that Anole delivers significant performance advantages. For example, compared to EPN, Homa, and Aeolus, it shortens the average FCT of all (small) flows by up to 61.6% (89.1%).},
  archive      = {J_TCC},
  author       = {Song Zhang and Lide Suo and Wenxin Li and Yuan Liu and Yulong Li and Keqiu Li},
  doi          = {10.1109/TCC.2024.3376716},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {550-562},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Anole: Scheduling flows for fast datacenter networks with packet re-prioritization},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Game-based low complexity and near optimal task offloading
for mobile blockchain systems. <em>TCC</em>, <em>12</em>(2), 539–549.
(<a href="https://doi.org/10.1109/TCC.2024.3376394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) finds applications across diverse fields but grapples with privacy and security concerns. Blockchain offers a remedy by instilling trust among IoT devices. The development of blockchain in IoT encounters hurdles due to its resource-intensive computation processing, notably in PoW-based systems. Cloud and edge computing can facilitate the application of blockchain in this environment, and the IoT users who want to mine in blockchain need to pay the computation resource rent to the Cloud Computing Service Provider (CCSP) for offloading the mining workload. In this scenario, these IoT miners can form groups to trade with CCSP to maximize their utility. In this paper, a mixed model of the Stackelberg game and coalition formation game is embraced to address the grouping and pricing issues between IoT miners and CCSP. In particular, the Stackelberg game is utilized to handle the pricing problem, and the coalition formation game is employed to tackle the best group partition problem. Moreover, a coalition formation algorithm is proposed to obtain a near-optimal solution with very low complexity. Simulation results show that our proposed algorithm can obtain a performance that is very near to the exhaustive search method, outperforms other existing schemes, and requires only a small computation overhead.},
  archive      = {J_TCC},
  author       = {Junfei Wang and Jing Li and Zhen Gao and Zhu Han and Chao Qiu and Xiaofei Wang},
  doi          = {10.1109/TCC.2024.3376394},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {539-549},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Game-based low complexity and near optimal task offloading for mobile blockchain systems},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluation of application layer DDoS attack effect in cloud
native applications. <em>TCC</em>, <em>12</em>(2), 522–538. (<a
href="https://doi.org/10.1109/TCC.2024.3374798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud native application is especially susceptible to application layer DDoS attack. This attributes to the internal service calls, by which microservices cooperate and communicate with each other, amplifying the effect of application layer DDoS attack. Since different services have varying degrees of sensitivity to an attack, a sophisticated attacker can take advantage of those especially expensive API calls to produce serious damage to the availability of services and applications with ease. To better analyze the severity of and mitigate application layer DDoS attacks in cloud native applications, we propose a novel method to evaluate the effect of application layer DDoS attack, that is able to quantitatively characterize the amplifying effect introduced by the complex structure of application system. We first present the descriptive model of the scenario. Then, Riemannian manifolds are constructed as the state spaces of the attack scenarios, in which attacks are described as homeomorphisms. Finally, we apply differential geometry principles to quantitatively calculate the attack effect, which is derived from the action of an attack and the movement it produces in the state spaces. The proposed method is validated in various application scenarios. We show that our approach provides accurate evaluation results, and outperforms existing solutions.},
  archive      = {J_TCC},
  author       = {Kewei Wang and Changzhen Hu and Chun Shan},
  doi          = {10.1109/TCC.2024.3374798},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {522-538},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Evaluation of application layer DDoS attack effect in cloud native applications},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight and privacy-preserving dual incentives for
mobile crowdsensing. <em>TCC</em>, <em>12</em>(2), 504–521. (<a
href="https://doi.org/10.1109/TCC.2024.3372598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incentive plays an important role in mobile crowdsensing (MCS), as it impels mobile users to participate in sensing tasks and provide high-quality sensing data. However, considering the privacy (including identity privacy, sensing data privacy, and reputation value privacy) and practicality (including reliability, quality awareness, and efficiency) issues in practice, it is a challenge to design such an effective incentive scheme for MCS applications. Existing studies either fail to provide adequate privacy-preserving capabilities or have low practicality. To address these issues, we propose a scheme called BRRV in MCS which relies on two rounds of range reliability assessment to guarantee the reliability of data while achieving privacy preservation. In addition, we also present a lightweight scheme called LRRV in MCS which relies on a single round of range reliability assessment to guarantee the reliability of data while achieving lightweight and privacy preservation. Moreover, to fairly stimulate participants, constrain participants’ malicious behavior, and improve the probability of high-quality data, we design a quality-aware reputation-based reward and penalty strategy to achieve dual incentives (including money incentives and reputation incentives) for participants. Furthermore, comprehensive theoretical analysis and experimental evaluation demonstrate that our proposed schemes are significantly superior to the existing schemes in several aspects.},
  archive      = {J_TCC},
  author       = {Lin Wan and Zhiquan Liu and Yong Ma and Yudan Cheng and Yongdong Wu and Runchuan Li and Jianfeng Ma},
  doi          = {10.1109/TCC.2024.3372598},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {504-521},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Lightweight and privacy-preserving dual incentives for mobile crowdsensing},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Context-aware consensus algorithm for blockchain-empowered
federated learning. <em>TCC</em>, <em>12</em>(2), 491–503. (<a
href="https://doi.org/10.1109/TCC.2024.3372814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TCC},
  author       = {Yao Zhao and Youyang Qu and Yong Xiang and Feifei Chen and Longxiang Gao},
  doi          = {10.1109/TCC.2024.3372814},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {491-503},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Context-aware consensus algorithm for blockchain-empowered federated learning},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trustless collaborative cloud federation. <em>TCC</em>,
<em>12</em>(2), 476–490. (<a
href="https://doi.org/10.1109/TCC.2024.3372370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TCC},
  author       = {Bishakh Chandra Ghosh and Sandip Chakraborty},
  doi          = {10.1109/TCC.2024.3372370},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {476-490},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Trustless collaborative cloud federation},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stackelberg-game-based multi-user multi-task offloading in
mobile edge computing. <em>TCC</em>, <em>12</em>(2), 459–475. (<a
href="https://doi.org/10.1109/TCC.2024.3370909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) brings abundant computing resources to the edge networks, which supports users in offloading their tasks to the edge instead of the cloud, thereby reducing service delay and improving users’ quality of experience. In this article, we consider a three-tier multi-user multi-task offloading model, which contains multiple users with each user possessing multiple tasks, multiple base stations (BSs) with edge servers and a remote cloud. Taking into account the selfishness of individuals in the MEC system, we respectively formulate optimization problems for users, BSs and the cloud. Users aim to make their offloading strategies to minimize their respective costs, while BSs and the cloud aim to make their computation resource allocation decisions to minimize their respective task completion delays. We model the interaction among these selfish individuals based on Stackelberg game, where users act as leaders and BSs and the cloud act as followers. By using backward induction, we prove the existence of Stackelberg Equilibrium (SE). We further propose a distributed algorithm that enables the system to reach the SE, which includes three user selection strategies for the BSs. The numerical results demonstrate the superiority of the proposed scheme compared with several approaches.},
  archive      = {J_TCC},
  author       = {Xinglin Zhang and Zhongling Wang and Fengsen Tian and Zheng Yang},
  doi          = {10.1109/TCC.2024.3370909},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {459-475},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Stackelberg-game-based multi-user multi-task offloading in mobile edge computing},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Polaris: Accelerating asynchronous federated learning with
client selection. <em>TCC</em>, <em>12</em>(2), 446–458. (<a
href="https://doi.org/10.1109/TCC.2024.3370688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TCC},
  author       = {Yufei Kang and Baochun Li},
  doi          = {10.1109/TCC.2024.3370688},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {446-458},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Polaris: Accelerating asynchronous federated learning with client selection},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Partial decode and compare: An efficient verification scheme
for coded edge computing. <em>TCC</em>, <em>12</em>(2), 431–445. (<a
href="https://doi.org/10.1109/TCC.2024.3370834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TCC},
  author       = {Jin Wang and Wei Jiang and Jingya Zhou and Zhaobo Lu and Kejie Lu and Jianping Wang},
  doi          = {10.1109/TCC.2024.3370834},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {431-445},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Partial decode and compare: An efficient verification scheme for coded edge computing},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cyclic matrix coding to mitigate ACK blocking of MPTCP in
data center networks. <em>TCC</em>, <em>12</em>(2), 419–430. (<a
href="https://doi.org/10.1109/TCC.2024.3366534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TCC},
  author       = {Zhaoyi Li and Jiawei Huang and Shiqi Wang and Wenjun Lyu and Jianxin Wang},
  doi          = {10.1109/TCC.2024.3366534},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {419-430},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cyclic matrix coding to mitigate ACK blocking of MPTCP in data center networks},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generic construction: Cryptographic reverse firewalls for
public key encryption with keyword search in cloud storage.
<em>TCC</em>, <em>12</em>(2), 405–418. (<a
href="https://doi.org/10.1109/TCC.2024.3366435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Snowden incident illustrates that an adversary may launch an algorithm substitution attack (ASA) by tampering with the algorithms of protocol participants to obtain users’ secret information. A measure against ASA is to equip the protocol participants with cryptographic reverse firewalls (CRF). Public key encryption with keyword search (PEKS) as a cryptographic primitive allows users to search encrypted file in cloud servers while ensuring the security of the original file. The existing CRF constructions for PEKS does not consider the trust level of CRFs, leaving honest-but-curious CRF to deal with trapdoors that should be sent in the secure channels, which brings new security risks. This article first introduces the notion of malleable designated tester public key encryption with keyword search (M-DPEKS). Based on M-DPEKS, we propose the generic construction of public key encryption with keyword search with cryptographic reverse firewalls to overcome the privacy leakage issue in cloud storage. Security proof indicates the generic construction is secure against ASA. Lastly, we instantiate the generic construction with a concrete M-DPEKS scheme and analyze the computation cost and communication overhead to evaluate the efficiency.},
  archive      = {J_TCC},
  author       = {Yang Ming and Hang Liu and Chenhao Wang and Yi Zhao},
  doi          = {10.1109/TCC.2024.3366435},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {405-418},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Generic construction: Cryptographic reverse firewalls for public key encryption with keyword search in cloud storage},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A cloud-edge collaboration framework for generating process
digital twin. <em>TCC</em>, <em>12</em>(2), 388–404. (<a
href="https://doi.org/10.1109/TCC.2024.3362989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking the process of remote task execution is critical to timely process analysis by collecting the evidence of correct execution or failure, which generates a process digital twin (DT) for remote supervision. Generally, it will encounter the challenge of constrained communication, high overhead, and high traceability demand, leading to the efficient remote process tracking issue. Existing approaches can address the issue by monitoring or simulating remote task execution. Nevertheless, they do not provide a cost-effective solution, especially when unexpected situation occurs. Thus, we proposed a new cloud-edge collaboration framework for process DT generation. It addresses the efficient remote process tracking issue with a real-virtual collaborative process tracking (RVCPT) approach. The approach contains three patterns of real-virtual collaboration for tracking the entire process of task execution with a coevolution pattern, identifying unexpected situations with a discrimination pattern, and generating a process DT with a real-virtual fusion pattern. This approach can minimize tracking overhead, and meanwhile maintains high traceability, which maximizes the overall cost-effectiveness. With prototype development, case study and experimental evaluation show the applicability and performance advantage of the new cloud-edge collaboration framework in remote supervision.},
  archive      = {J_TCC},
  author       = {Bingqing Shen and Han Yu and Pan Hu and Hongming Cai and Jingzhi Guo and Boyi Xu and Lihong Jiang},
  doi          = {10.1109/TCC.2024.3362989},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {388-404},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A cloud-edge collaboration framework for generating process digital twin},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Locality-aware and fault-tolerant batching for machine
learning on distributed datasets. <em>TCC</em>, <em>12</em>(2), 370–387.
(<a href="https://doi.org/10.1109/TCC.2024.3351716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of distributed ML training is largely determined by workers that generate gradients in the slowest pace, i.e., stragglers. The state-of-the-art load balancing approaches consider that each worker stores a complete dataset locally and the data fetching time can be ignored. They only consider the computation capacity of workers in equalizing the gradient computation time. However, we find that in scenarios of ML on distributed datasets, whether in edge computing or distributed data cache systems, the data fetching time is non-negligible and often becomes the primary cause of stragglers. In this paper, we present LOFT, an adaptive load balancing approach for ML upon distributed datasets at the edge. It aims to balance the time to generate gradients at each worker while ensuring the model accuracy. Specifically, LOFT features a locality-aware batching. It builds performance and optimization models upon data fetching and gradient computation time. Leveraging the models, it develops an adaptive scheme based on grid search. Furthermore, it offers Byzantine gradient aggregation upon Ring All-Reduce, which makes itself fault-tolerant under Byzantine gradients brought by a small batch size. Experiments with twelve public DNN models and four open datasets show that LOFT reduces the training time by up to 46%, while reducing the training loss by up to 67% compared to LB-BSP.},
  archive      = {J_TCC},
  author       = {Liu Liu and Zhijun Ding and Dazhao Cheng and Xiaobo Zhou},
  doi          = {10.1109/TCC.2024.3351716},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {370-387},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Locality-aware and fault-tolerant batching for machine learning on distributed datasets},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PsvCNN: A zero-knowledge CNN prediction integrity
verification strategy. <em>TCC</em>, <em>12</em>(2), 359–369. (<a
href="https://doi.org/10.1109/TCC.2024.3350233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model prediction based on machine learning is provided as a service in cloud environments, but how to verify that the model prediction service is entirely conducted becomes a critical challenge. Although zero-knowledge proof techniques potentially solve the integrity verification problem, when applied to the prediction integrity of massive privacy-preserving Convolutional Neural Networks (CNNs), the significant proof burden results in low practicality. In this research, we present psvCNN (parallel splitting zero-knowledge technique for integrity verification). The psvCNN scheme effectively improves the utilization of computational resources in CNN prediction integrity proving by an independent splitting design. Through a convolutional kernel-based model splitting design and an underlying zero-knowledge succinct non-interactive knowledge argument, our psvCNN develops parallelizable zero-knowledge proof circuits for CNN prediction. Furthermore, psvCNN presents an updated Freivalds algorithm for a faster integrity verification process. In terms of proof time and storage, experiments show that psvCNN is practical and efficient. psvCNN generates a prediction integrity proof with a proof size of 1.2MB in 7.65s for the structurally complicated CNN model VGG16. psvCNN is 3765 times quicker than the latest zk-SNARK-based non-interactive method vCNN and 12 times faster than the latest sumcheck-based interactive technique zkCNN in terms of proving time.},
  archive      = {J_TCC},
  author       = {Yongkai Fan and Binyuan Xu and Linlin Zhang and Gang Tan and Shui Yu and Kuan-Ching Li and Albert Zomaya},
  doi          = {10.1109/TCC.2024.3350233},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4},
  number       = {2},
  pages        = {359-369},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {PsvCNN: A zero-knowledge CNN prediction integrity verification strategy},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Personalized and differential privacy-aware video stream
offloading in mobile edge computing. <em>TCC</em>, <em>12</em>(1),
347–358. (<a href="https://doi.org/10.1109/TCC.2024.3362355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Mobile Edge Computing (MEC), the collaboration between end devices and servers guarantees the low-latency and high-accuracy video stream analysis. However, such paradigm of video stream offloading poses a serious threat to the location privacy and the usage pattern privacy of end devices. The existing works offer strict privacy guarantee for users, but they do not take the features of video stream into consideration, thus leading to the relatively higher computation cost. To tackle this issue, we propose a personalized and differential privacy-aware video stream offloading scheme that supports users personalized and time-varying privacy requirements, provides corresponding differential privacy preservation, and generates minimal latency and energy cost. Specifically, we formulate an NP-hard optimization that jointly optimizes the video frame rate, frame resolution and offloading ratio to maximize the analysis accuracy of video stream and minimize the energy cost and the latency subject to the channel bandwidth, computing resources, and personalized and time-varying privacy requirements. Then, we design a online learning-based and personalized privacy-aware video stream offloading algorithm for the optimization problem and thereby obtain the optimal video stream offloading scheme. Last, the extensive experimental results validate the superior performance of the proposed scheme, compared to the three latest existing works.},
  archive      = {J_TCC},
  author       = {Ping Zhao and Ziyi Yang and Guanglin Zhang},
  doi          = {10.1109/TCC.2024.3362355},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {347-358},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Personalized and differential privacy-aware video stream offloading in mobile edge computing},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Achieving low latency for multipath transmission in RDMA
based data center network. <em>TCC</em>, <em>12</em>(1), 337–346. (<a
href="https://doi.org/10.1109/TCC.2024.3365075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote Direct Memory Access (RDMA) achieves ultra-low latency, high throughput and low CPU overhead in data center by implementing the transport logic in hardware network interface card (NIC). However, RDMA faces new challenges in the heterogeneous multipath environment as it is very sensitive to packet reordering. When some packets are blocked in slow paths, the other packets delivered through fast paths have to be buffered at the receiver&#39;s NIC, consuming the limited on-chip memory resources. In this paper, we propose a new RDMA-based multipath transmission scheme with advanced fast retransmission called as AFR-MPRDMA. Specifically, once detecting congestion at the slow path, the sender will retransmit the blocked packets on other fast paths to speed up the transmission of blocked packets. Moreover, the receiver dynamically adjusts the buffer size for the out-of-order packets to avoid either unnecessary retransmission or long latency. The results of large-scale tests show that AFR-MPRDMA effectively mitigates packets blocking issue and reduces average flow completion time (AFCT) by up to 61% compared with the state-of-the-art RDMA-based schemes.},
  archive      = {J_TCC},
  author       = {Zhaoyi Li and Jiawei Huang and Shiqi Wang and Jianxin Wang},
  doi          = {10.1109/TCC.2024.3365075},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {337-346},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Achieving low latency for multipath transmission in RDMA based data center network},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enabling multi-layer threat analysis in dynamic cloud
environments. <em>TCC</em>, <em>12</em>(1), 319–336. (<a
href="https://doi.org/10.1109/TCC.2024.3365736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most Threat Analysis (TA) techniques analyze threats to targeted assets (e.g., components, services) by considering static interconnections among them. However, in dynamic environments, e.g., the Cloud, resources can instantiate, migrate across physical hosts, or decommission to provide rapid resource elasticity to its users. Existing TA techniques are not capable of addressing such requirements. Moreover, complex multi-layer/multi-asset attacks on Cloud systems are increasing, e.g., the Equifax data breach; thus, TA approaches must be able to analyze them. This article proposes ThreatPro, which supports dynamic interconnections and analysis of multi-layer attacks in the Cloud. ThreatPro facilitates threat analysis by developing a technology-agnostic information flow model, representing the Cloud&#39;s functionality through conditional transitions. The model establishes the basis to capture the multi-layer and dynamic interconnections during the life cycle of a Virtual Machine. ThreatPro contributes to (1) enabling the exploration of a threat&#39;s behavior and its propagation across the Cloud, and (2) assessing the security of the Cloud by analyzing the impact of multiple threats across various operational layers/assets. Using public information on threats from the National Vulnerability Database, we validate ThreatPro&#39;s capabilities, i.e., identify and trace actual Cloud attacks and speculatively postulate alternate potential attack paths.},
  archive      = {J_TCC},
  author       = {Salman Manzoor and Antonios Gouglidis and Matthew Bradbury and Neeraj Suri},
  doi          = {10.1109/TCC.2024.3365736},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {319-336},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Enabling multi-layer threat analysis in dynamic cloud environments},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Edge-cloud collaborative UAV object detection: Edge-embedded
lightweight algorithm design and task offloading using fuzzy neural
network. <em>TCC</em>, <em>12</em>(1), 306–318. (<a
href="https://doi.org/10.1109/TCC.2024.3361858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of artificial intelligence and Unmanned Aerial Vehicle (UAV) technology, AI-based UAVs are increasingly utilized in various industrial and civilian applications. This paper presents a distributed Edge-Cloud collaborative framework for UAV object detection, aiming to achieve real-time and accurate detection of ground moving targets. The framework incorporates an Edge-Embedded Lightweight ( ${{\text{E}}^{2}}\text{L}$ ) object algorithm with an attention mechanism, enabling real-time object detection on edge-side embedded devices while maintaining high accuracy. Additionally, a decision-making mechanism based on fuzzy neural network facilitates adaptive task allocation between the edge-side and cloud-side. Experimental results demonstrate the improved running rate of the proposed algorithm compared to YOLOv4 on the edge-side NVIDIA Jetson Xavier NX, and the superior performance of the distributed Edge-Cloud collaborative framework over traditional edge computing or cloud computing algorithms in terms of speed and accuracy},
  archive      = {J_TCC},
  author       = {Yazhou Yuan and Shicong Gao and Ziteng Zhang and Wenye Wang and Zhezhuang Xu and Zhixin Liu},
  doi          = {10.1109/TCC.2024.3361858},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {306-318},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Edge-cloud collaborative UAV object detection: Edge-embedded lightweight algorithm design and task offloading using fuzzy neural network},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cloud-assisted laconic private set intersection cardinality.
<em>TCC</em>, <em>12</em>(1), 295–305. (<a
href="https://doi.org/10.1109/TCC.2024.3361882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Laconic Private Set Intersection (LPSI) is a type of PSI protocols characterized by the requirement of only two-round interactions and by having a reused message in the first round that is independent of the set size. Recently, Aranha et al. (CCS’2022) proposed a LPSI protocol that utilizes the pairing-based accumulator. However, this protocol heavily relies on time-consuming bilinear pairing operations, which can potentially cause a bottleneck. Furthermore, in certain scenarios like contact tracing, it is sufficient to only reveal the intersection cardinality. To tackle this problem and expand on its functionalities, we introduce a cloud-assisted two-party LPSI cardinality (TLPSI-CA) that inherits the properties of LPSI. Interestingly, the cloud-assisted TLPSI-CA eliminates the direct interaction between the sender and receiver, enabling the sender&#39;s message to be reused across any number of protocol executions. Besides, we further extend it to the multi-party scenario, which also possesses laconic properties. Then, we prove the two protocols’ security in achieving the defined ideal functionalities. Finally, we evaluate the performance of both protocols and find that TLPSI-CA successfully reduces the local computation costs for participants. Additionally, the multi-party protocol performs similarly to TLPSI-CA, with the exception of the higher communication costs incurred by the receiver.},
  archive      = {J_TCC},
  author       = {Axin Wu and Xiangjun Xin and Jianhao Zhu and Wei Liu and Chang Song and Guoteng Li},
  doi          = {10.1109/TCC.2024.3361882},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {295-305},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cloud-assisted laconic private set intersection cardinality},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrating bayesian optimization and machine learning for
the optimal configuration of cloud systems. <em>TCC</em>,
<em>12</em>(1), 277–294. (<a
href="https://doi.org/10.1109/TCC.2024.3361070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian Optimization (BO) is an efficient method for finding optimal cloud configurations for several types of applications. On the other hand, Machine Learning (ML) can provide helpful knowledge about the application at hand thanks to its predicting capabilities. This work proposes a general approach based on BO, which integrates elements from ML techniques in multiple ways, to find an optimal configuration of recurring jobs running in public and private cloud environments, possibly subject to black-box constraints, e.g., application execution time or accuracy. We test our approach by considering several use cases, including edge computing, scientific computing, and Big Data applications. Results show that our solution outperforms other state-of-the-art black-box techniques, including classical autotuning and BO- and ML-based algorithms, reducing the number of unfeasible executions and corresponding costs up to 2–4 times.},
  archive      = {J_TCC},
  author       = {Bruno Guindani and Danilo Ardagna and Alessandra Guglielmi and Roberto Rocco and Gianluca Palermo},
  doi          = {10.1109/TCC.2024.3361070},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {277-294},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Integrating bayesian optimization and machine learning for the optimal configuration of cloud systems},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DRL-based contract incentive for wireless-powered and
UAV-assisted backscattering MEC system. <em>TCC</em>, <em>12</em>(1),
264–276. (<a href="https://doi.org/10.1109/TCC.2024.3360443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) is viewed as a promising technology to address the challenges of intensive computing demands in hotspots (HSs). In this article, we consider a unmanned aerial vehicle (UAV)-assisted backscattering MEC system. The UAVs can fly from parking aprons to HSs, providing energy to HSs via RF beamforming and collecting data from wireless users in HSs through backscattering. We aim to maximize the long-term utility of all HSs, subject to the stability of the HSs’ energy queues. This problem is a joint optimization of the data offloading decision and contract design that should be adaptive to the users’ random task demands and the time-varying wireless channel conditions. A deep reinforcement learning based contract incentive (DRLCI) strategy is proposed to solve this problem in two steps. First, we use deep Q-network (DQN) algorithm to update the HSs’ offloading decisions according to the changing network environment. Second, to motivate the UAVs to participate in resource sharing, a contract specific to each type of UAVs has been designed, utilizing Lagrangian multiplier method to approach the optimal contract. Simulation results show the feasibility and efficiency of the proposed strategy, demonstrating a better performance than the natural DQN and Double-DQN algorithms.},
  archive      = {J_TCC},
  author       = {Che Chen and Shimin Gong and Wenjie Zhang and Yifeng Zheng and Yeo Chai Kiat},
  doi          = {10.1109/TCC.2024.3360443},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {264-276},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {DRL-based contract incentive for wireless-powered and UAV-assisted backscattering MEC system},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient verifiable cloud-assisted PSI cardinality for
privacy-preserving contact tracing. <em>TCC</em>, <em>12</em>(1),
251–263. (<a href="https://doi.org/10.1109/TCC.2024.3360098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Private set intersection cardinality (PSI-CA) allows two parties to learn the size of the intersection between two private sets without revealing other additional information, which is a promising technique to solve privacy concerns in contact tracing. Efficient PSI protocols typically use oblivious transfer, involving multiple rounds of interaction and leading to heavy local computation overheads and protocol delays, especially when interacting with many receivers. Cloud-assisted PSI-CA is a better solution as it relieves participants’ burdens of computation and communication. However, cloud servers may return incorrect or incomplete results for some reason, leading to an incorrectness issue. At present, to our knowledge, existing cloud-assisted PSI-CA protocols cannot address such a concern. To address this, we propose two specific verifiable cloud-assisted PSI-CA protocols: one based on a two-server protocol and the other on a single-server protocol. Further, we employ Cuckoo hashing to optimize these two protocols, enabling the receiver&#39;s computational costs independent of the size of the sender&#39;s set. We also prove the security of the protocols and implement them. Finally, we analyze and discuss their performance demonstrating that the single-server verifiable PSI-CA protocol does not introduce significant computation or communication costs while adding functionalities.},
  archive      = {J_TCC},
  author       = {Yafeng Chen and Axin Wu and Yuer Yang and Xiangjun Xin and Chang Song},
  doi          = {10.1109/TCC.2024.3360098},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {251-263},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Efficient verifiable cloud-assisted PSI cardinality for privacy-preserving contact tracing},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Root cause analysis for cloud-native applications.
<em>TCC</em>, <em>12</em>(1), 232–250. (<a
href="https://doi.org/10.1109/TCC.2024.3358823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Root cause analysis (RCA) is a critical component in maintaining the reliability and performance of modern cloud applications. However, due to the inherent complexity of cloud environments, traditional RCA techniques become insufficient in supporting system administrators in daily incident response routines. This article presents an RCA solution specifically designed for cloud applications, capable of pinpointing failure root causes and recreating complete fault trajectories from the root cause to the effect. The novelty of our approach lies in approximating causal symptom dependencies by synergizing several symptom correlation methods that assess symptoms in terms of structural, semantic, and temporal aspects. The solution integrates statistical methods with system structure and behavior mining, offering a more comprehensive analysis than existing techniques. Based on these concepts, in this work, we provide definitions and construction algorithms for RCA model structures used in the inference, propose a symptom correlation framework encompassing essential elements of symptom data analysis, and provide a detailed description of the elaborated root cause identification process. Functional evaluation on a live microservice-based system demonstrates the effectiveness of our approach in identifying root causes of complex failures across multiple cloud layers.},
  archive      = {J_TCC},
  author       = {Bartosz Żurkowski and Krzysztof Zieliński},
  doi          = {10.1109/TCC.2024.3358823},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {232-250},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Root cause analysis for cloud-native applications},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Alleviating congestion via switch design for fair buffer
allocation in datacenters. <em>TCC</em>, <em>12</em>(1), 219–231. (<a
href="https://doi.org/10.1109/TCC.2024.3357595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In data-centers, the composite origin and bursty nature of traffic, the small bandwidth-delay product and the tiny switch buffers lead to unusual congestion patterns that are not handled well by traditional end-to-end congestion control mechanisms such as those deployed in TCP. Existing works address the problem by modifying TCP to adapt it to the idiosyncrasies of data-centers. While this is feasible in private environments, it remains almost impossible to achieve practically in public multi-tenant clouds where a multitude of operating systems and thus congestion control protocols co-exist. In this work, we design a simple switch-based active queue management scheme to deal with such congestion issues adequately. Our approach requires no modification to TCP which enables seamless deployment in public data-centers via switch firmware updates. We present a simple analysis to show the stability and effectiveness of our approach, then discuss the real implementations in software and hardware on the NetFPGA platform. Numerical results from ns-2 simulation and experimental results from a small testbed cluster demonstrate the effectiveness of our approach in achieving high overall throughput, good fairness, smaller flow completion times (FCT) for short-lived flows, and a significant reduction in the tail of the FCT distribution.},
  archive      = {J_TCC},
  author       = {Ahmed M. Abdelmoniem and Brahim Bensaou},
  doi          = {10.1109/TCC.2024.3357595},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {219-231},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Alleviating congestion via switch design for fair buffer allocation in datacenters},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fault tolerance oriented SFC optimization in SDN/NFV-enabled
cloud environment based on deep reinforcement learning. <em>TCC</em>,
<em>12</em>(1), 200–218. (<a
href="https://doi.org/10.1109/TCC.2024.3357061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In software defined network/network function virtualization (SDN/NFV)-enabled cloud environment, cloud services can be implemented as service function chains (SFCs), which consist of a series of ordered virtual network functions. However, due to fluctuations of cloud traffic and without knowledge of cloud computing network configuration, designing SFC optimization approach to obtain flexible cloud services in dynamic cloud environment is a pivotal challenge. In this paper, we propose a fault tolerance oriented SFC optimization approach based on deep reinforcement learning. We model fault tolerance oriented SFC elastic optimization problem as a Markov decision process, in which the reward is modeled as a weighted function, including minimizing energy consumption and migration cost, maximizing revenue benefit and load balancing. Then, taking binary integer programming model as constraints of quality of cloud services, we design optimization approaches for single-agent double deep Q-network (SADDQN) and multi-agent DDQN (MADDQN). Among them, MADDQN decentralizes training tasks from control plane to data plane to reduce the probability of single point of failure for the centralized controller. Experimental results show that the designed approaches have better performance. MADDQN can almost reach the upper bound of theoretical solution obtained by assuming a prior knowledge of the dynamics of cloud traffic.},
  archive      = {J_TCC},
  author       = {Jing Chen and Jia Chen and Kuo Guo and Renkun Hu and Tao Zou and Jun Zhu and Hongke Zhang and Jingjing Liu},
  doi          = {10.1109/TCC.2024.3357061},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {200-218},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Fault tolerance oriented SFC optimization in SDN/NFV-enabled cloud environment based on deep reinforcement learning},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reversible data hiding in shared images with separate cover
image reconstruction and secret extraction. <em>TCC</em>,
<em>12</em>(1), 186–199. (<a
href="https://doi.org/10.1109/TCC.2024.3351143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reversible data hiding is widely utilized for secure communication and copyright protection. Recently, to improve embedding capacity and visual quality of stego-images, some Partial Reversible Data Hiding (PRDH) schemes are proposed. But these schemes are over the plaintext domain. To protect the privacy of the cover image, Reversible Data Hiding in Encrypted Images (RDHEI) techniques are preferred. In addition, the full separability of cover image reconstruction and data restoration is also an important characteristic that cannot be achieved by most RDHEI schemes. To solve the issues, a partial and a complete Reversible Data Hiding in Shared Images with Separate Cover Image Reconstruction and Secret Extraction (RDHSI-SRE) are proposed in this paper. In the proposed schemes, the secret data is divided by Secret Sharing (SS). Then, the marked shared images are generated based on the proposed modify-and-recalculate strategy. The receiver can extract embedded data and reconstruct the image separably using k -out-of- n marked shared images. In the embedding phase of partial RDHSI-SRE (PRDHSI-SRE), the pixel values are modified according to the proposed Minimizing-Square-Errors Strategy to achieve high visual quality, and the complete RDHSI-SRE (CRDHSI-SRE) embeds data by modifying random coefficients to achieve reversibility. The experimental results and theoretical analyses demonstrate that the proposed schemes have a high embedding performance. Most importantly, the proposed schemes are fault-tolerant and completely separable.},
  archive      = {J_TCC},
  author       = {Lizhi Xiong and Xiao Han and Ching-Nung Yang and Yun-Qing Shi},
  doi          = {10.1109/TCC.2024.3351143},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {186-199},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Reversible data hiding in shared images with separate cover image reconstruction and secret extraction},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BatOpt: Optimizing GPU-based deep learning inference using
dynamic batch processing. <em>TCC</em>, <em>12</em>(1), 174–185. (<a
href="https://doi.org/10.1109/TCC.2024.3350561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) has been applied in billions of mobile devices due to its astonishing performance in image, text, and audio processing. However, limited by the computing capability of mobile devices, a large amount of DL inference tasks need to be offloaded to edge or cloud servers, which makes powerful GPU servers are struggling to ensure the quality of service(QoS). To better utilize the highly parallel computing architecture of GPU to improve the QoS, we propose BatOpt, a framework that uses dynamic batch processing to strike a good balance between service latency and GPU memory usage in DL inference services. Specifically, BatOpt innovatively models the DL inference service as a $M/G(a,b)/1/N$ queue, with the consideration of stochastic task arrivals, which enables it to predict the service latency accurately in different system states. Furthermore, we propose an optimization algorithm to trade off the service latency and GPU memory usage in different system states by analyzing the queueing model. We have implemented BatOpt on Pytorch and evaluated it on an RTX 2080 GPU using real DL models. BatOpt brings up to 31x and 4.3x times performance boost in terms of service latency, compared to single-input and fixed-batch-size strategies, respectively. And BatOpt&#39;s maximum GPU memory usage is only 0.3x that of greedy-dynamic-batch-size strategy on the premise of the same service latency.},
  archive      = {J_TCC},
  author       = {Deyu Zhang and Yunzhen Luo and Yaobo Wang and Xiaoyan Kui and Ju Ren},
  doi          = {10.1109/TCC.2024.3350561},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {174-185},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {BatOpt: Optimizing GPU-based deep learning inference using dynamic batch processing},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-mode instance-intensive workflow task batch scheduling
in containerized hybrid cloud. <em>TCC</em>, <em>12</em>(1), 159–173.
(<a href="https://doi.org/10.1109/TCC.2023.3344194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The migration of containerized microservices from virtual machines (VMs) to cloud data centers has become the most advanced deployment technique for large software applications in the cloud. This study investigates the scheduling of instance-intensive workflow (IWF) tasks to be executed in containers on a hybrid cloud when computational resources are limited. The process of scheduling these IWF tasks becomes complicated when considering the deployment time of containers, inter-task communication time, and their dependencies simultaneously, particularly when the task can choose multi-mode executions due to the flexible computational resource allocation of the container. We propose a batch scheduling strategy (BSS) for the IWF task scheduling problem. The BSS prioritizes the execution of IWF tasks with high repetition rates with a certain probability and records the virtual machines and modes selected for task execution, which can reduce the data transfer time and the randomness of computation. Based on this, we use an improved hybrid algorithm combined with BSS to solve the multi-mode IWF task scheduling problem. The experimental results demonstrate that employing the BSS can reduce the scheduling time by 6% when the number of workflows increases to 80. Additionally, we tested the effectiveness of all operators in the algorithm, and the results show that each step of the algorithm yields good performance. Compared to similar algorithms in related studies, the overall algorithm can achieve a maximum reduction of approximately 18% in the target value.},
  archive      = {J_TCC},
  author       = {An Liu and Ming Gao and Jiafu Tang},
  doi          = {10.1109/TCC.2023.3344194},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {159-173},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Multi-mode instance-intensive workflow task batch scheduling in containerized hybrid cloud},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VSA-SD: A service discovery method based on vector symbol
architecture for low-cost IoT system development. <em>TCC</em>,
<em>12</em>(1), 145–158. (<a
href="https://doi.org/10.1109/TCC.2023.3344512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, with the widening applications of the Internet of Things (IoT), more and more perception services (e.g., air quality indicator services, road traffic congestion monitoring services, etc) with different arguments (e.g., data type, source location, creator, etc) will be deployed by dedicated IT infrastructure service providers for constructing customized IoT systems with low cost by subscription. So it is an indispensable step to check whether the required perception services with specified arguments have been available for the constructing IoT through discovery method to reduce the redundancy of service deployment. However, it is a challenging problem to design efficient (i.e., achieving high accuracy and low response delay with low overhead), highly robust, and trustworthy mechanisms for discovering perception services on resource-constrained IoT devices. To solve this problem, we proposed a distributed service discovery method, named VSA-SD, based on the Vector Symbolic Architecture (VSA). This method employs hyperdimensional vectors to describe services in a distributed manner, and measures the degree of service matching by calculating the Hamming distance, thereby achieving service discovery. We implemented VSA-SD in NBUFlow, which is an IoT task construction and offloading test platform, and evaluated its performance through comprehensive experiments. Results show that VSA-SD outperforms the centralized, hybrid, and other distributed service discovery mechanisms in terms of accuracy, response delay, overhead, robustness, trustability, interoperability, and mobility.},
  archive      = {J_TCC},
  author       = {Haiming Chen and Lei Wang and Wei Qin and Xinyan Zhou and Li Cui},
  doi          = {10.1109/TCC.2023.3344512},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {145-158},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {VSA-SD: A service discovery method based on vector symbol architecture for low-cost IoT system development},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pay-per-proof: Decentralized outsourced multi-user PoR for
cloud storage payment using blockchain. <em>TCC</em>, <em>12</em>(1),
130–144. (<a href="https://doi.org/10.1109/TCC.2023.3343710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing has been widely applied in data storage, but cloud computing is not armed with an efficient integrity check mechanism for users to learn whether their large volumes of data have been kept intact by the cloud. The concept of proofs of retrievability (PoR) was introduced to address such an issue by enabling users to check the integrity of their data stored by the cloud. But PoR requires users to regularly send queries to the cloud, and its integrity check method cannot be extended to share the verification responsibility in the multi-user setting where different users store the same data to the cloud. With such concerns in mind, we put forth a notion called outsourced multi-user proofs of retrievability ( $\mathtt {OMTPoR}$ ) which allows users with the same data stored by the cloud to share the information for the integrity check, and a third party is required to regularly check data integrity on behalf of users using the shared information. We give a concrete construction of $\mathtt {OMTPoR}$ based on the homomorphic property of an existing property and analyze its security. To enforce honest integrity checks, we build the concrete $\mathtt {OMTPoR}$ construction over the blockchain using smart contracts to guarantee the honesty of participants, yielding a decentralized outsourced multi-user PoR solution that utilizes the blockchain miners as the third parties. Furthermore, our solution enables the cloud server to obtain payment for the storage service if the PoR is verified by the miners. We fully implement the $\mathtt {OMTPoR}$ scheme over the blockchain to evaluate its performance, which demonstrates obvious superiority over traditional PoR schemes without the detection of data duplication.},
  archive      = {J_TCC},
  author       = {Hui Cui and Zhiguo Wan and Tianyu Zhaolu and Huaqun Wang and Atsuko Miyaji},
  doi          = {10.1109/TCC.2023.3343710},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {130-144},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Pay-per-proof: Decentralized outsourced multi-user PoR for cloud storage payment using blockchain},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient approximation algorithms for scheduling coflows
with total weighted completion time in identical parallel networks.
<em>TCC</em>, <em>12</em>(1), 116–129. (<a
href="https://doi.org/10.1109/TCC.2023.3340729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the scheduling problem of coflows in identical parallel networks, a well-known $\mathcal {NP}$ -hard problem. We consider both flow-level scheduling and coflow-level scheduling problems. In the flow-level scheduling problem, flows within a coflow can be transmitted through different network cores, while in the coflow-level scheduling problem, flows within a coflow must be transmitted through the same network core. The key difference between these two problems lies in their scheduling granularity. Previous approaches relied on linear programming to solve the scheduling order. In this article, we enhance the efficiency of solving by utilizing the primal-dual method. For the flow-level scheduling problem, we propose an approximation algorithm that achieves approximation ratios of $6-\frac{2}{m}$ and $5-\frac{2}{m}$ for arbitrary and zero release times, respectively, where $m$ represents the number of network cores. Additionally, for the coflow-level scheduling problem, we introduce an approximation algorithm that achieves approximation ratios of $4m+1$ and $\text{4}m$ for arbitrary and zero release times, respectively. The algorithm presented in this article has practical applications in data centers, such as those operated by Google or Facebook. The simulated results demonstrate the superior performance of our algorithms compared to previous approach, emphasizing their practical utility.},
  archive      = {J_TCC},
  author       = {Chi-Yeh Chen},
  doi          = {10.1109/TCC.2023.3340729},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {116-129},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Efficient approximation algorithms for scheduling coflows with total weighted completion time in identical parallel networks},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrated computation offloading, UAV trajectory control,
edge-cloud and radio resource allocation in SAGIN. <em>TCC</em>,
<em>12</em>(1), 100–115. (<a
href="https://doi.org/10.1109/TCC.2023.3339394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the computation offloading problem in hybrid edge-cloud based space-air-ground integrated networks (SAGIN), where joint optimization of partial computation offloading, unmanned aerial vehicle (UAV) trajectory control, user scheduling, edge-cloud computation, radio resource allocation, and admission control is performed. Specifically, the considered SAGIN employs multiple UAV-mounted edge servers with controllable UAV trajectory and a cloud sever which can be reached by ground users (GUs) via multi-hop low-earth-orbit (LEO) satellite communications. This design aims to minimize the weighted energy consumption of the GUs and UAVs while satisfying the maximum delay constraints of underlying computation tasks. To tackle the underlying non-convex mixed integer non-linear optimization problem, we use the alternating optimization approach where we iteratively solve four sub-problems, namely user scheduling, partial offloading control and bit allocation over time slots, computation resource and bandwidth allocation, and multi-UAV trajectory control until convergence. Moreover, feasibility verification and admission control strategies are proposed to handle overloaded network scenarios. Furthermore, the successive convex approximation (SCA) method is employed to convexify and solve the non-convex computation resource and bandwidth allocation and UAV trajectory control sub-problems. Via extensive numerical studies, we illustrate the effectiveness of our proposed design compared to baselines.},
  archive      = {J_TCC},
  author       = {Minh Dat Nguyen and Long Bao Le and André Girard},
  doi          = {10.1109/TCC.2023.3339394},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {100-115},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Integrated computation offloading, UAV trajectory control, edge-cloud and radio resource allocation in SAGIN},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing cloud data lake queries with a balanced coverage
plan. <em>TCC</em>, <em>12</em>(1), 84–99. (<a
href="https://doi.org/10.1109/TCC.2023.3339208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud data lakes emerge as an inexpensive solution for storing very large amounts of data. The main idea is the separation of compute and storage layers. Thus, cheap cloud storage is used for storing the data, while compute engines are used for running analytics on this data in “on-demand” mode. However, to perform any computation on the data in this architecture, the data should be moved from the storage layer to the compute layer over the network for each calculation. Obviously, that hurts calculation performance and requires huge network bandwidth. In this paper, we study different approaches to improve query performance in a data lake architecture. We define an optimization problem that can provably speed up data lake queries. We prove that the problem is NP-hard and suggest heuristic approaches. Then, we demonstrate through the experiments that our approach is feasible and efficient (up to ×30 query execution time improvement based on the TPC-H benchmark).},
  archive      = {J_TCC},
  author       = {Grisha Weintraub and Ehud Gudes and Shlomi Dolev and Jeffrey D. Ullman},
  doi          = {10.1109/TCC.2023.3339208},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {84-99},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Optimizing cloud data lake queries with a balanced coverage plan},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A publicly verifiable outsourcing matrix computation scheme
based on smart contracts. <em>TCC</em>, <em>12</em>(1), 70–83. (<a
href="https://doi.org/10.1109/TCC.2023.3337848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix computation is a crucial mathematical tool in scientific fields such as Artificial Intelligence and Cryptographic computation. However, it is difficult for resource-limited devices to execute large-scale matrix computations independently. Outsourcing matrix computation (OMC) is a promising solution that engages a cloud server to process complicated matrix computations for resource-limited devices. However, existing OMC schemes lack public verifiability, and thus resource-limited devices cannot verdict the correctness of the computing results. In this paper, for the first time, we propose a smart contract-based OMC scheme that publicly verifies the outsourcing matrix computation results. In our scheme, a smart contract running over the blockchain serves as a decentralized trusted third party to ensure the correctness of the matrix computation results. To overcome the Verifier&#39;s Dilemma in the blockchain, we present a blockchain-compatible matrix verification method that decreases the time complexity from $O(n^{3})$ to $O(n^{2})$ by utilizing a blinding method with the check digit and padding matrices. We make the verification become the form of comparing whether two results are identical rather than naive re-computing. Finally, we perform experiments on Ethereum and ARM Cortex-M4 and give in-depth analysis and performance evaluation, demonstrating our scheme&#39;s practicability and effectiveness.},
  archive      = {J_TCC},
  author       = {Hao Wang and Chunpeng Ge and Lu Zhou and Zhe Liu and Dongwan Lan and Xiaozhen Lu and Danni Jiang},
  doi          = {10.1109/TCC.2023.3337848},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {70-83},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A publicly verifiable outsourcing matrix computation scheme based on smart contracts},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A stochastic approach for scheduling AI training jobs in
GPU-based systems. <em>TCC</em>, <em>12</em>(1), 53–69. (<a
href="https://doi.org/10.1109/TCC.2023.3336540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we optimize the scheduling of Deep Learning (DL) training jobs from the perspective of a Cloud Service Provider running a data center, which efficiently selects resources for the execution of each job to minimize the average energy consumption while satisfying time constraints. To model the problem, we first develop a Mixed-Integer Non-Linear Programming formulation. Unfortunately, the computation of an optimal solution is prohibitively expensive, and to overcome this difficulty, we design a heuristic STochastic Scheduler (STS). Exploiting the probability distribution of early termination, STS determines how to adapt the resource assignment during the execution of the jobs to minimize the expected energy cost while meeting the job due dates. The results of an extensive experimental evaluation show that STS guarantees significantly better results than other methods in the literature, effectively avoiding due date violations and yielding a percentage total cost reduction between 32% and 80% on average. We also prove the applicability of our method in real-world scenarios, as obtaining optimal schedules for systems of up to 100 nodes and 400 concurrent jobs requires less than 5 seconds. Finally, we evaluated the effectiveness of GPU sharing, i.e., running multiple jobs in a single GPU. The obtained results demonstrate that depending on the workload and GPU memory, this further reduces the energy cost by 17–29% on average.},
  archive      = {J_TCC},
  author       = {Federica Filippini and Jonatha Anselmi and Danilo Ardagna and Bruno Gaujal},
  doi          = {10.1109/TCC.2023.3336540},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {53-69},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A stochastic approach for scheduling AI training jobs in GPU-based systems},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing the micro-architectural performance of the
current and emerging edge infrastructure. <em>TCC</em>, <em>12</em>(1),
40–52. (<a href="https://doi.org/10.1109/TCC.2023.3333813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Network Function Virtualization (NFV) is the essential technology proposed to tackle the next-generation mobile system’s various flexibility features. In this article, we implement a thorough micro-architectural performance investigation on the NFV-enabled edge virtual Radio Access Network (vRAN) and the emerging 5G new-radio (nr) platform to unveil the main micro-architectural bottlenecks of the next-generation network’s vRAN system. Based on our experimental results, we find that the high core bound hinders the processing speed of the vRAN and 5G nr platforms. Several solutions alleviating the vRAN’s core bound are proposed to accelerate the vRAN system’s processing speed. Besides, we observe that the current co-location strategy cannot maximize the COTS servers’ CPU utilization and meanwhile eliminate the system hang-up caused by CPU resource contention. We fill this gap by proposing an optimized co-location strategy based on our observed vRAN co-location characterization. Finally, we detect that on the modern hyper-threading-enabled COTS servers, the current pin core policy of 5G nr will cause L3 cache contention, which will lead to severe system hang-up. A novel threads management mechanism is proposed to eliminate this system hang-up on the hyper-threading-enabled COTS servers.},
  archive      = {J_TCC},
  author       = {Jianda Wang and Zhen Wang and Weili Wu and Yang Hu},
  doi          = {10.1109/TCC.2023.3333813},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {40-52},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Optimizing the micro-architectural performance of the current and emerging edge infrastructure},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leakage-suppressed encrypted keyword queries over multiple
cloud servers. <em>TCC</em>, <em>12</em>(1), 26–39. (<a
href="https://doi.org/10.1109/TCC.2023.3333223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searchable encryption is a technique that can support operations on encrypted data directly. However, searchable encryption is still vulnerable to attacks that exploit the leakages from encrypted query results. This article presents an effective multi-server searchable encryption scheme to prevent volume and access pattern leakages. To hide the volume leakage of a keyword, a new index construction is proposed to compress multiple results into one index. To prevent the attacker from observing the access pattern of injected records, the update and search phases are executed in batches, such that the server can only retrieve multiple numbers of fixed volumes. To reduce the co-occurrence leakage, we propose our index distribution algorithm. Both records and queries are dispatched among cloud servers such that the attacker cannot recover the trapdoor values by only observing one cloud server. We use the minimum $s-t$ cut algorithm to find the optimal assignment strategy that can diminish the query response time and the information disclosure at the same time. We formally analyze the security strengths and conduct evaluations. The experimental results indicate that our designs can strike a good balance between security and efficiency.},
  archive      = {J_TCC},
  author       = {Yi Dou and Henry C. B. Chan},
  doi          = {10.1109/TCC.2023.3333223},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {26-39},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Leakage-suppressed encrypted keyword queries over multiple cloud servers},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning-based dynamic memory allocation schemes for apache
spark data processing. <em>TCC</em>, <em>12</em>(1), 13–25. (<a
href="https://doi.org/10.1109/TCC.2023.3329129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Apache Spark is an in-memory analytic framework that has been adopted in the industry and research fields. Two memory managers, Static and Unified, are available in Spark to allocate memory for caching Resilient Distributed Datasets (RDDs) and executing tasks. However, we find that the static memory manager (SMM) lacks flexibility, while the unified memory manager (UMM) puts heavy pressure on the garbage collection of the JVM on which Spark resides. To address these issues, we design a learning-based bidirectional usage-bounded memory allocation scheme to support dynamic memory allocation with the consideration of both memory demands and latency introduced by garbage collection. We first develop an auto-tuning memory manager (ATuMm) that adopts an intuitive feedback-based learning solution. However, ATuMm is a slow learner that can only alter the states of Java Virtual Memory (JVM) Heap in a limited range. That is, ATuMm decides to increase or decrease the boundary between the execution and storage memory pools by a fixed portion of JVM Heap size. To overcome this shortcoming, we further develop a new reinforcement learning-based memory manager (Q-ATuMm) that uses a Q-learning intelligent agent to dynamically learn and tune the partition of JVM Heap. We implement our new memory managers in Spark 2.4.0 and evaluate them by conducting experiments in a real Spark cluster. Our experimental results show that our memory manager can reduce the total garbage collection time and thus further improve Spark applications’ performance (i.e., reduced latency) compared to the existing Spark memory management solutions. By integrating our machine learning-driven memory manager into Spark, we can further obtain around 1.3x times reduction in the latency.},
  archive      = {J_TCC},
  author       = {Danlin Jia and Li Wang and Natalia Valencia and Janki Bhimani and Bo Sheng and Ningfang Mi},
  doi          = {10.1109/TCC.2023.3329129},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {13-25},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Learning-based dynamic memory allocation schemes for apache spark data processing},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Slim and fast: Low-overhead container overlay network with
fast connection setup. <em>TCC</em>, <em>12</em>(1), 1–12. (<a
href="https://doi.org/10.1109/TCC.2023.3282238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale cloud applications today are often deployed using multiple containers, and a container overlay network is the de facto method to provide connectivity among these containers. However, the existing tunneling-based overlay network incurs significant performance overhead due to the need of transformation for every packet. Recent work Slim , through manipulating connection-level meta-data, allows containers to use host OS sockets directly thus they can achieve good performance without extra packet tunneling. Nevertheless, the connection setup is significantly slowed down, which requires an extra round-trip communication between both sides to pass the mapping information of the host OS socket and the container socket. This greatly hurts the performance of many cloud applications that must process short connections at high speed. We propose SlimFast , a low-overhead container overlay network which provides a fast connection setup. SlimFast directly uses the host OS socket for container communication as Slim . However, SlimFast needs no extra communication during connection setup. We reserve a dedicated host port for the container network and use socket mapping table to locally find the right container socket during connection setup. We implement SlimFast which is compatible with existing container applications. Experiments show that, SlimFast can improve the connection setup time by about 2.1x compared with Slim , meanwhile maintaining low-overhead during data transmission as Slim . This brings significant performance improvement to real applications. Particularly, testbed results show that SlimFast improves the throughput of Nginx proxy and Memcached by about 0.9x and 2.2x, respectively.},
  archive      = {J_TCC},
  author       = {Fusheng Lin and Xin Zhang and Guo Chen and Li Chen and Kenli Li and Hongbo Jiang},
  doi          = {10.1109/TCC.2023.3282238},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Slim and fast: Low-overhead container overlay network with fast connection setup},
  volume       = {12},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
