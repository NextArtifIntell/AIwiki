<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>THMS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="thms---74">THMS - 74</h2>
<ul>
<li><details>
<summary>
(2024). Object-goal navigation of home care robot based on human
activity inference and cognitive memory. <em>THMS</em>, <em>54</em>(6),
808–817. (<a href="https://doi.org/10.1109/THMS.2024.3467150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As older adults&#39; memory and cognitive ability deteriorate, designing a cognitive robot system to find the desired objects for users becomes more critical. Cognitive abilities, such as detecting and memorizing the environment and human activities are crucial in implementing effective human–robot interaction and navigation. In addition, robots must possess language understanding capabilities to comprehend human speech and respond promptly. This research aims to develop a mobile robot system for home care that incorporates human activity inference and cognitive memory to reason about the target object&#39;s location and navigate to find it. The method comprises three modules: 1) an object-goal navigation module for mapping the environment, detecting surrounding objects, and navigating to find the target object, 2) a cognitive memory module for recognizing human activity and storing encoded information, and 3) an interaction module to interact with humans and infer the target object&#39;s position. By leveraging Big Data, human cues, and a commonsense knowledge graph, the system can efficiently and robustly search for target objects. The effectiveness of the system is validated through both simulated and real-world scenarios.},
  archive      = {J_THMS},
  author       = {Chien-Ting Chen and Shen Jie Koh and Fu-Hao Chang and Yi-Shiang Huang and Li-Chen Fu},
  doi          = {10.1109/THMS.2024.3467150},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {808-817},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Object-goal navigation of home care robot based on human activity inference and cognitive memory},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design of a robotic system featured with high operation
transparency for quantifying arm impedance during ultrasound scanning.
<em>THMS</em>, <em>54</em>(6), 798–807. (<a
href="https://doi.org/10.1109/THMS.2024.3442537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Experienced sonographers can adjust their arm impedance in real-time to obtain high-quality ultrasound (US) images during US scanning. These operational skills can be captured through robot systems with multimodal data collection capabilities (position, force, and impedance). However, low operational transparency between the system (generally, a serial robot with admittance control) and its users will result in significant delays and errors, interfering with the skill acquisition process. The paper proposes a new system that adopts the parallel mechanism (Omega.7) to improve the transparency of the operation. The scanning probe and a 6-axis force sensor are attached to the end of Omega.7. When operating the probe, a zero-force drag effect can be realized through gravity and torque compensations. The arm impedance during the scanning can be measured through the force disturbance method by analyzing external forces on the device. Ultrasonic scans were conducted on phantoms of different hardness, and arm impedance was measured. Statistical analysis reveals that when scanning softer phantoms, arms exhibit higher stiffness. The transparency analysis results show that the equipment designed in this paper has a higher level of transparency than the scheme of serial robot with admittance control. The high operation transparency of the system makes it an ideal skill-acquisition device with broad applications.},
  archive      = {J_THMS},
  author       = {Baoshan Niu and Dapeng Yang and Yangjunjian Zhou and Le Zhang and Qi Huang and Yikun Gu},
  doi          = {10.1109/THMS.2024.3442537},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {798-807},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Design of a robotic system featured with high operation transparency for quantifying arm impedance during ultrasound scanning},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fusion of temporal transformer and spatial graph
convolutional network for 3-d skeleton-parts-based human motion
prediction. <em>THMS</em>, <em>54</em>(6), 788–797. (<a
href="https://doi.org/10.1109/THMS.2024.3452133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of human motion prediction has gained prominence, finding applications in various domains such as intelligent surveillance and human–robot interaction. However, predicting full-body human motion poses challenges in capturing joint interactions, handling diverse movement patterns, managing occlusions, and ensuring real-time performance. To address these challenges, the proposed model adopts a skeleton-parted strategy to dissect the skeleton structure, enhancing coordination and fusion between body parts. This novel method combines transformer-enabled graph convolutional networks for predicting human motion in 3-D skeleton data. It integrates a temporal transformer (T-Transformer) for comprehensive temporal feature extraction and a spatial graph convolutional network (S-GCN) for capturing spatial characteristics of human motion. The model&#39;s performance is evaluated on two comprehensive human motion datasets, Human3.6M and CMU motion capture (CMU Mocap), containing numerous videos encompassing short and long human motion sequences. Results indicate that the proposed model outperforms state-of-the-art methods on both datasets, significantly improving the average mean per joint positional error (avg-MPJPE) by 3.50% and 11.45% for short-term and long-term motion prediction, respectively. Similarly, on the CMU Mocap dataset, it achieves avg-MPJPE improvements of 2.69% and 1.05% for short-term and long-term motion prediction, respectively, demonstrating its superior accuracy in predicting human motion over extended periods. The study also investigates the impact of different numbers of T-Transformers and S-GCNs and explores the specific roles and contributions of the T-Transformer, S-GCN, and cross-part components.},
  archive      = {J_THMS},
  author       = {Mayank Lovanshi and Vivek Tiwari and Rajesh Ingle and Swati Jain},
  doi          = {10.1109/THMS.2024.3452133},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {788-797},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Fusion of temporal transformer and spatial graph convolutional network for 3-D skeleton-parts-based human motion prediction},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A modified dynamic movement primitive algorithm for adaptive
gait control of a lower limb exoskeleton. <em>THMS</em>, <em>54</em>(6),
778–787. (<a href="https://doi.org/10.1109/THMS.2024.3458905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A major challenge in the lower limb exoskeleton for walking assistance is the adaptive gait control. In this article, a modified dynamic movement primitive (DMP) (MDMP) control is proposed to achieve gait adjustment with different assistance levels. This is achieved by inclusion of interaction forces in the formulation of DMP, which enables learning from physical human–robot interaction. A threshold force is introduced accounting for different levels of walking assistance from the exoskeleton. The MDMP is, thus, capable of generating adjustable gait and reshaping trajectories with data from the interaction force sensors. The experiments on five subjects show that the average differences between the human body and the exoskeleton are 4.13° and 1.92° on the hip and knee, respectively, with average interaction forces of 42.54 N and 26.36 N exerted on the subjects&#39; thigh and shank. The results demonstrated that the MDMP method can effectively provide adjustable gait for walking assistance.},
  archive      = {J_THMS},
  author       = {Lingzhou Yu and Shaoping Bai},
  doi          = {10.1109/THMS.2024.3458905},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {778-787},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A modified dynamic movement primitive algorithm for adaptive gait control of a lower limb exoskeleton},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The augmented intelligence perspective on human-in-the-loop
reinforcement learning: Review, concept designs, and future directions.
<em>THMS</em>, <em>54</em>(6), 762–777. (<a
href="https://doi.org/10.1109/THMS.2024.3467370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented intelligence (AuI) is a concept that combines human intelligence (HI) and artificial intelligence (AI) to leverage their respective strengths. While AI typically aims to replace humans, AuI integrates humans into machines, recognizing their irreplaceable role. Meanwhile, human-in-the-loop reinforcement learning (HITL-RL) is a semisupervised algorithm that integrates humans into the traditional reinforcement learning (RL) algorithm, enabling autonomous agents to gather inputs from both humans and environments, learn, and select optimal actions across various environments. Both AuI and HITL-RL are still in their infancy. Based on AuI, we propose and investigate three separate concept designs for HITL-RL: HI-AI , AI-HI , and parallel-HI-and-AI approaches, each differing in the order of HI and AI involvement in decision making. The literature on AuI and HITL-RL offers insights into integrating HI into existing concept designs. A preliminary study in an Atari game offers insights for future research directions. Simulation results show that human involvement maintains RL convergence and improves system stability, while achieving approximately similar average scores to traditional $Q$ -learning in the game. Future research directions are proposed to encourage further investigation in this area.},
  archive      = {J_THMS},
  author       = {Kok-Lim Alvin Yau and Yasir Saleem and Yung-Wey Chong and Xiumei Fan and Jer Min Eyu and David Chieng},
  doi          = {10.1109/THMS.2024.3467370},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {762-777},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {The augmented intelligence perspective on human-in-the-loop reinforcement learning: Review, concept designs, and future directions},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning for human–machine systems with advanced
persistent threats. <em>THMS</em>, <em>54</em>(6), 753–761. (<a
href="https://doi.org/10.1109/THMS.2024.3439625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article conducts a thorough exploration of the implications of machine learning (ML) in conjunction with human–machine systems within the military domain. It scrutinizes the strategic development efforts of ML by pertinent institutions, particularly in the context of military applications and the domain of advanced persistent threats. Prominent nations have delineated a technical trajectory for the integration of ML into their military frameworks. To bolster the structure and efficacy of their various military branches and units, there has been a concentrated deployment of numerous ML research endeavors. These initiatives encompass the study of sophisticated ML algorithms and the acceleration of artificial intelligence technology adaptation for intelligence processing, autonomous platforms, command and control infrastructures, and weapons systems. Forces across the globe are actively embedding ML technologies into a range of platforms-terrestrial, naval, aerial, space-faring, and cybernetic. This integration spans weaponry, networks, cognitive operations, and additional systems. Furthermore, this article reviews the incorporation within the sphere of military human–machine interaction in the Russia–Ukraine conflict. In this war, cyber human–machine interaction has become a pivotal arena of contention between Russia and Ukraine, with key levers that influence the conflict&#39;s course. In addition, the article examines the adoption of ML in prospective military functions such as, operations, intelligence gathering, networking, logistics, identification protocols, healthcare, data analysis trends, and other critical areas marked by current developments and trajectories. It also proffers a series of recommendations for the future integration of ML to inform strategic direction and research.},
  archive      = {J_THMS},
  author       = {Long Chen and Wei Zhang and Yanqing Song and Jianguo Chen},
  doi          = {10.1109/THMS.2024.3439625},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {753-761},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Machine learning for Human–Machine systems with advanced persistent threats},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-model cross-stream learning for self-supervised human
action recognition. <em>THMS</em>, <em>54</em>(6), 743–752. (<a
href="https://doi.org/10.1109/THMS.2024.3467334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering the instance-level discriminative ability, contrastive learning methods, including MoCo and SimCLR, have been adapted from the original image representation learning task to solve the self-supervised skeleton-based action recognition task. These methods usually use multiple data streams (i.e., joint, motion, and bone) for ensemble learning, meanwhile, how to construct a discriminative feature space within a single stream and effectively aggregate the information from multiple streams remains an open problem. To this end, this article first applies a new contrastive learning method called bootstrap your own latent (BYOL) to learn from skeleton data, and then formulate SkeletonBYOL as a simple yet effective baseline for self-supervised skeleton-based action recognition. Inspired by SkeletonBYOL, this article further presents a cross-model and cross-stream (CMCS) framework. This framework combines cross-model adversarial learning (CMAL) and cross-stream collaborative learning (CSCL). Specifically, CMAL learns single-stream representation by cross-model adversarial loss to obtain more discriminative features. To aggregate and interact with multistream information, CSCL is designed by generating similarity pseudolabel of ensemble learning as supervision and guiding feature generation for individual streams. Extensive experiments on three datasets verify the complementary properties between CMAL and CSCL and also verify that the proposed method can achieve better results than state-of-the-art methods using various evaluation protocols.},
  archive      = {J_THMS},
  author       = {Mengyuan Liu and Hong Liu and Tianyu Guo},
  doi          = {10.1109/THMS.2024.3467334},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {743-752},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Cross-model cross-stream learning for self-supervised human action recognition},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Speech-driven gesture generation using transformer-based
denoising diffusion probabilistic models. <em>THMS</em>, <em>54</em>(6),
733–742. (<a href="https://doi.org/10.1109/THMS.2024.3456085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While it is crucial for human-like avatars to perform co-speech gestures, existing approaches struggle to generate natural and realistic movements. In the present study, a novel transformer-based denoising diffusion model is proposed to generate co-speech gestures. Moreover, we introduce a practical sampling trick for diffusion models to maintain the continuity between the generated motion segments while improving the within-segment motion likelihood and naturalness. Our model can be used for online generation since it generates gestures for a short segment of speech, e.g., 2 s. We evaluate our model on two large-scale speech-gesture datasets with finger movements using objective measurements and a user study, showing that our model outperforms all other baselines. Our user study is based on the Metahuman platform in the Unreal Engine, a popular tool for creating human-like avatars and motions.},
  archive      = {J_THMS},
  author       = {Bowen Wu and Chaoran Liu and Carlos Toshinori Ishi and Hiroshi Ishiguro},
  doi          = {10.1109/THMS.2024.3456085},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {733-742},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Speech-driven gesture generation using transformer-based denoising diffusion probabilistic models},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting human postures for manual material handling tasks
using a conditional diffusion model. <em>THMS</em>, <em>54</em>(6),
723–732. (<a href="https://doi.org/10.1109/THMS.2024.3472548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting workers&#39; body postures is crucial for effective ergonomic interventions to reduce musculoskeletal disorders (MSDs). In this study, we employ a novel generative approach to predict human postures during manual material handling tasks. Specifically, we implement two distinct network architectures, U-Net and multilayer perceptron (MLP), to build the diffusion model. The model training and testing utilizes a dataset featuring 35 full-body anatomical landmarks collected from 25 participants engaged in a variety of lifting tasks. In addition, we compare our models with two conventional generative networks (conditional generative adversarial network and conditional variational autoencoder) for comprehensive analysis. Our results show that the U-Net model performs well in predicting posture similarity [root-mean-square error (RMSE) of key-point coordinates = 5.86 cm; and RMSE of joint angle coordinates = 13.67 $^{\circ }$ ], while the MLP model leads to higher posture variability (e.g., standard deviation of joint angles = 4.49 $^{\circ }$ /4.18 $^{\circ }$ for upper arm flexion/extension joints). Moreover, both generative models demonstrate reasonable prediction validity (RMSE of segment lengths are within 4.83 cm). Overall, our proposed diffusion models demonstrate good similarity and validity in predicting lifting postures, while also providing insights into the inherent variability of constrained lifting postures. This novel use of diffusion models shows potential for tailored posture prediction in common occupational environments, representing an advancement in motion synthesis and contributing to workplace design and MSD risk mitigation.},
  archive      = {J_THMS},
  author       = {Liwei Qing and Bingyi Su and Sehee Jung and Lu Lu and Hanwen Wang and Xu Xu},
  doi          = {10.1109/THMS.2024.3472548},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {723-732},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Predicting human postures for manual material handling tasks using a conditional diffusion model},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reconstructing visual stimulus representation from EEG
signals based on deep visual representation model. <em>THMS</em>,
<em>54</em>(6), 711–722. (<a
href="https://doi.org/10.1109/THMS.2024.3407875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing visual stimulus representation is a significant task in neural decoding. Until now, most studies have considered functional magnetic resonance imaging (fMRI) as the signal source. However, fMRI-based image reconstruction methods are challenging to apply widely due to the complexity and high cost of acquisition equipment. Taking into account the advantages of the low cost and easy portability of electroencephalogram (EEG) acquisition equipment, we propose a novel image reconstruction method based on EEG signals in this article. First, to meet the high recognizability of visual stimulus images in a fast-switching manner, we construct a visual stimuli image dataset and obtain the corresponding EEG dataset through EEG signals collection experiment. Second, we introduce the deep visual representation model (DVRM), comprising a primary encoder and a subordinate decoder, to reconstruct visual stimuli representation. The encoder is designed based on residual-in-residual dense blocks to learn the distribution characteristics between EEG signals and visual stimulus images. Meanwhile, the decoder is designed using a deep neural network to reconstruct the visual stimulus representation from the learned deep visual representation. The DVRM can accommodate the deep and multiview visual features of the human natural state, resulting in more precise reconstructed images. Finally, we evaluate the DVRM based on the quality of the generated images using our EEG dataset. The results demonstrate that the DVRM exhibits an excellent performance in learning deep visual representation from EEG signals, generating reconstructed representation of images that are realistic and highly resemble the original images.},
  archive      = {J_THMS},
  author       = {Hongguang Pan and Zhuoyi Li and Yunpeng Fu and Xuebin Qin and Jianchen Hu},
  doi          = {10.1109/THMS.2024.3407875},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {711-722},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Reconstructing visual stimulus representation from EEG signals based on deep visual representation model},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analyzing surgeon–robot cooperative performance in
robot-assisted intravascular catheterization. <em>THMS</em>,
<em>54</em>(6), 698–710. (<a
href="https://doi.org/10.1109/THMS.2024.3452975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robot-assisted catheterization offers a promising technique for cardiovascular interventions, addressing the limitations of manual interventional surgery, where precise tool manipulation is critical. In remote-control robotic systems, the lack of force feedback and imprecise navigation challenge cooperation between the surgeon and robot. This study proposes a manipulation-based evaluation framework to assess the cooperative performance between different operators and robot using kinesthetic, kinematic, and haptic data from multi-sensor technologies. The proposed evaluation framework achieves a recognition accuracy of 99.99% in assessing the cooperation between operator and robot. Additionally, the study investigates the impact of delay factors, considering no delay, constant delay, and variable delay, on cooperation characteristics. The findings suggest that variable delay contributes to improved cooperation performance between operator and robot in a primary-secondary isomorphic robotic system, compared to a constant delay factor. Furthermore, operators with experience in manual percutaneous coronary interventions exhibit significantly better cooperative manipulate on with the robot system than those without such experience, with respective synergy ratios of 89.66%, 90.28%, and 91.12% based on the three aspects of delay consideration. Moreover, the study explores interaction information, including distal force of tools-tissue and contact force of hand-control-ring, to understand how operators with different technical skills adjust their control strategy to prevent damage to the vascular vessel caused by excessive force while ensuring enough tension to navigate complex paths. The findings highlight the potential of variable delay to enhance cooperative control strategies in robotic catheterization systems, providing a basis for optimizing surgeon-robot collaboration in cardiovascular interventions.},
  archive      = {J_THMS},
  author       = {Wenjing Du and Guanlin Yi and Olatunji Mumini Omisore and Wenke Duan and Toluwanimi Oluwadra Akinyemi and Xingyu Chen and Jiang Liu and Boon-Giin Lee and Lei Wang},
  doi          = {10.1109/THMS.2024.3452975},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {698-710},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Analyzing Surgeon–Robot cooperative performance in robot-assisted intravascular catheterization},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bioinspired virtual reality toolkit for robot-assisted
medical application: BioVRbot. <em>THMS</em>, <em>54</em>(6), 688–697.
(<a href="https://doi.org/10.1109/THMS.2024.3462416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasingly pervasive usage of robotic surgery not only calls for advances in clinical application but also implies high availability for preliminary medical education using virtual reality. Virtual reality is currently upgrading medical education by presenting complicated medical information in an immersive and interactive way. A system that allows multiple users to observe and operate via simulated surgical platforms using wearable devices has become an efficient solution for teaching where a real surgical platform is not available. This article developed a bioinspired virtual reality toolkit (BioVRbot) for education and training in robot-assisted minimally invasive surgery. It allows multiple users to manipulate the robots working on cooperative virtual surgery using bioinspired control. The virtual reality scenario is implemented using unity and can be observed with independent virtual reality headsets. A MATLAB server is designed to manage robot motion planning of incremental teleoperation compliance with the remote center of motion constraints. Wearable sensorized gloves are adopted for continuous control of the tooltip and the gripper. Finally, the practical use of the developed surgical virtual system is demonstrated with cooperative operation tasks. It could be further spread into the classroom for preliminary education of robot-assisted surgery for early-stage medical students.},
  archive      = {J_THMS},
  author       = {Hang Su and Francesco Jamal Sheiban and Wen Qi and Salih Ertug Ovur and Samer Alfayad},
  doi          = {10.1109/THMS.2024.3462416},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {688-697},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A bioinspired virtual reality toolkit for robot-assisted medical application: BioVRbot},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of a MR training system for living donor liver
transplantation using simulated liver phantom and ICP tracking
technology. <em>THMS</em>, <em>54</em>(6), 678–687. (<a
href="https://doi.org/10.1109/THMS.2024.3450689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Living donor liver transplantation (LT) is a curative treatment for decompensation liver cirrhosis, some metabolic diseases, and acute liver failure. For specific conditions of hepatocellular carcinoma, LT provides a better prognosis than other known treatments do. During living donor LT, recognition and preservation of the middle hepatic vein (MHV) and its main branch are extremely important and closely related to the outcomes for the donor and recipient. Currently, preoperative computed tomography (CT) scans and intraoperative ultrasound are used to evaluate the location of the MHV; however, the information from CT scans and ultrasound is two-dimensional and lacks specific perception data. To achieve better MHV tracking during surgery, this work presents a mixed-reality (MR) training system for open liver LT surgery, which uses a simulated elastic liver phantom and iterative closest point (ICP) tracking technology. We created a three-dimensional (3-D) liver reconstruction model based on CT images from 20 patients and produced a series of equal-sized elastic liver phantoms with soft vessels inside. The ICP algorithm was used to track the liver phantom with the MR system, and the 3-D reconstruction model was superimposed on the phantom. The experimental results revealed that the registration error was &lt;4 mm. The feedback from ten novice surgeons who practiced with the proposed system was positive. It is expected that the proposed system for LT could enhance the overall effectiveness of surgeon training and serve as a reference for other applications in the future.},
  archive      = {J_THMS},
  author       = {Tsung-Han Yang and Yi-Chun Du and Cheng-Bin Xu and Wei-Siang Ciou},
  doi          = {10.1109/THMS.2024.3450689},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {678-687},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Development of a MR training system for living donor liver transplantation using simulated liver phantom and ICP tracking technology},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optical see-through head-mounted display with mitigated
parallax-related registration errors: A user study validation.
<em>THMS</em>, <em>54</em>(6), 668–677. (<a
href="https://doi.org/10.1109/THMS.2024.3468019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For an optical see-through (OST) augmented reality (AR) head-mounted display (HMD) to assist in performing high-precision activities in the peripersonal space, a fundamental requirement is the correct spatial registration between the virtual information and the real environment. This registration can be achieved through a calibration procedure involving the parameterization of the virtual rendering camera via an eye-replacement camera that observes a calibration pattern rendered onto the OST display. In a previous feasibility study, we demonstrated and proved, with the same eye-replacement camera used for the calibration, that, in the case of an OST display with a focal plane close to the user&#39;s working distance, there is no need for prior-to-use viewpoint-specific calibration refinements obtained through eye-tracking cameras or additional alignment-based calibration steps. The viewpoint parallax-related AR registration error is indeed submillimetric within a reasonable range of depths around the display focal plane. This article confirms, through a user study based on a monocular virtual-to-real alignment task, that this finding is accurate and usable. In addition, we found that by performing the alignment-free calibration procedure via a high-resolution camera, the AR registration accuracy is substantially improved compared with that of other state-of-the-art approaches, with an error lower than 1mm over a notable range of distances. These results demonstrate the safe usability of OST HMDs for high-precision task guidance in the peripersonal space.},
  archive      = {J_THMS},
  author       = {Nadia Cattari and Fabrizio Cutolo and Vincenzo Ferrari},
  doi          = {10.1109/THMS.2024.3468019},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {668-677},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Optical see-through head-mounted display with mitigated parallax-related registration errors: A user study validation},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Building contextualized trust profiles in conditionally
automated driving. <em>THMS</em>, <em>54</em>(6), 658–667. (<a
href="https://doi.org/10.1109/THMS.2024.3452411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trust is crucial for ensuring the safety, security, and widespread adoption of automated vehicles (AVs), and if trust is lacking, drivers and the general public may hesitate to embrace this technology. This research seeks to investigate contextualized trust profiles in order to create personalized experiences for drivers in AVs with varying levels of reliability. A driving simulator experiment involving 70 participants revealed three distinct contextualized trust profiles (i.e., confident copilots , myopic pragmatists , and reluctant automators ) identified through K-means clustering, and analyzed in relation to drivers&#39; dynamic trust, dispositional trust, initial learned trust, personality traits, and emotions. The experiment encompassed eight scenarios where participants were requested to take over control from the AV in three conditions: a control condition, a false alarm condition, and a miss condition. To validate the models, a multinomial logistic regression model was constructed using the shapley additive explanations explainer to determine the most influential features in predicting contextualized trust profiles, achieving an F1-score of 0.90 and an accuracy of 0.89. In addition, an examination of how individual factors impact contextualized trust profiles provided valuable insights into trust dynamics from a user-centric perspective. The outcomes of this research hold significant implications for the development of personalized in-vehicle trust monitoring and calibration systems to modulate drivers&#39; trust levels, thereby enhancing safety and user experience in automated driving.},
  archive      = {J_THMS},
  author       = {Lilit Avetisyan and Jackie Ayoub and X. Jessie Yang and Feng Zhou},
  doi          = {10.1109/THMS.2024.3452411},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {658-667},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Building contextualized trust profiles in conditionally automated driving},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring factors related to drivers’ mental model of and
trust in advanced driver assistance systems using an ABN-based mixed
approach. <em>THMS</em>, <em>54</em>(6), 646–657. (<a
href="https://doi.org/10.1109/THMS.2024.3436876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drivers’ appropriate mental models of and trust in advanced driver assistance systems (ADAS) are essential to driving safety in vehicles with ADAS. Although several previous studies evaluated drivers’ ADAS mental models of and trust in adaptive cruise control and lane-keeping assist systems, research gaps still exist. Specifically, recent developments in ADAS have made more advanced functions available but they have been under-investigated. Furthermore, the widely adopted proportional correctness-based scores may not differentiate drivers’ objective ADAS mental model and subjective bias toward the ADAS. Finally, most previous studies adopted only regression models to explore the influential factors and thus may have ignored the underlying association among the factors. Therefore, our study aimed to explore drivers’ mental models of and trust in emerging ADAS by using the sensitivity (i.e., d’ ) and response bias (i.e., c ) measures from the signal detection theory. We modeled the data from 287 drivers using additive Bayesian network (ABN) and further interpreted the graph model using regression analysis. We found that different factors might be associated with drivers’ objective knowledge of ADAS and subjective bias toward the existence of functions/limitations. Furthermore, drivers’ subjective bias was more associated with their trust in ADAS compared to objective knowledge. The findings from our study provide new insights into the influential factors on drivers’ mental models of ADAS and better reveal how mental models can affect trust in ADAS. It also provides a case study on how the mixed approach with ABN and regression analysis can model observational data.},
  archive      = {J_THMS},
  author       = {Chunxi Huang and Jiyao Wang and Song Yan and Dengbo He},
  doi          = {10.1109/THMS.2024.3436876},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {646-657},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Exploring factors related to drivers’ mental model of and trust in advanced driver assistance systems using an ABN-based mixed approach},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reliability and models of subjective motion incongruence
ratings in urban driving simulations. <em>THMS</em>, <em>54</em>(6),
634–645. (<a href="https://doi.org/10.1109/THMS.2024.3450831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In moving-base driving simulators, the sensation of the inertial car motion provided by the motion system is controlled by the motion cueing algorithm (MCA). Due to the difficulty of reproducing the inertial motion in urban simulations, accurate prediction tools for subjective evaluation of the simulator&#39;s inertial motion are required. In this article, an open-loop driving experiment in an urban scenario is discussed, in which 60 participants evaluated the motion cueing through an overall rating and a continuous rating method. Three MCAs were tested that represent different levels of motion cueing quality. It is investigated under which conditions the continuous rating method provides reliable data in urban scenarios through the estimation of Cronbach&#39;s alpha and McDonald&#39;s omega. Results show that the better the motion cueing is rated, the lower the reliability of that rating data is, and the less the continuous rating and overall rating correlate. This suggests that subjective ratings for motion quality are dominated by (moments of) incongruent motion, while congruent motion is less important. Furthermore, through a forward regression approach, it is shown that participants&#39; rating behavior can be described by a first-order low-pass filtered response to the lateral specific force mismatch (66.0%), as well as a similar response to the longitudinal specific force mismatch (34.0%). By this better understanding of the acquired ratings in urban driving simulations, including their reliability and predictability, incongruences can be more accurately targeted and reduced.},
  archive      = {J_THMS},
  author       = {Maurice Kolff and Joost Venrooij and Markus Schwienbacher and Daan M. Pool and Max Mulder},
  doi          = {10.1109/THMS.2024.3450831},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {634-645},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Reliability and models of subjective motion incongruence ratings in urban driving simulations},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Emotion recognition of playing musicians from EEG, ECG, and
acoustic signals. <em>THMS</em>, <em>54</em>(5), 619–629. (<a
href="https://doi.org/10.1109/THMS.2024.3430327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigated the automatic recognition of felt and musically communicated emotions using electroencephalogram (EEG), electrocardiogram (ECG), and acoustic signals, which were recorded from eleven musicians instructed to perform music in order to communicate happiness, sadness, relaxation, and anger. Musicians&#39; self-reports indicated that the emotions they musically expressed were highly consistent with those they actually felt. Results showed that the best classification performances, in a subject-dependent classification using a KNN classifier were achieved by using features derived from both the EEG and ECG (with an accuracy of 98.11%). Which was significantly more accurate than using ECG features alone, but was not significantly more accurate than using EEG features alone. The use of acoustic features alone or in combination with EEG and/or ECG features did not lead to better performances than those achieved with EEG plus ECG or EEG alone. Our results suggest that emotion detection of playing musicians, both felt and musically communicated, when coherent, can be classified in a more reliable way using physiological features than involving acoustic features. The reported machine learning results are a step toward the development of affective brain–computer interfaces capable of automatically inferring the emotions of a playing musician in real-time.},
  archive      = {J_THMS},
  author       = {Luca Turchet and Barry O&#39;Sullivan and Rupert Ortner and Christoph Guger},
  doi          = {10.1109/THMS.2024.3430327},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {619-629},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Emotion recognition of playing musicians from EEG, ECG, and acoustic signals},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Layered modeling of affective, perception, and visual
properties: Optimizing structure with genetic algorithm. <em>THMS</em>,
<em>54</em>(5), 609–618. (<a
href="https://doi.org/10.1109/THMS.2024.3434573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To design the “Kansei value” aspect of a product, it is useful to design multilayered relationships of perceptual and affective responses via the physical or psychophysical properties of the product. However, because they are qualitative and ambiguous, designing a model is time-consuming. Moreover, the design was conducted by hypothesis and trial-and-error by the experimenter. In this article, we developed a method to automatically construct several semioptimal structures by applying a genetic algorithm to model design based on structural equation modeling, using the results of image measurement and subjective evaluation experiments on various material samples. Under set convergence conditions, the method constructed statistically optimized structures that represent the relationships among adjectives describing perception and affective, and the properties. A semantic validation was performed to determine the final model. As a result, the proposed method could be used to construct a model that can be interpreted as semantically and statistically superior compared to methods in related studies. A unique feature of this article was the use of the physical and psychophysical properties obtained by measurements in the construction of a multilayer model. Also, the advantage of this method is that it can be used to construct important structures that may be overlooked.},
  archive      = {J_THMS},
  author       = {Shuhei Watanabe and Takahiko Horiuchi},
  doi          = {10.1109/THMS.2024.3434573},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {609-618},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Layered modeling of affective, perception, and visual properties: Optimizing structure with genetic algorithm},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supporting human–robot interaction by projected augmented
reality and a brain interface. <em>THMS</em>, <em>54</em>(5), 599–608.
(<a href="https://doi.org/10.1109/THMS.2024.3414208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a brain–computer interface (BCI) coupled with an augmented reality (AR) system to support human–robot interaction in controlling a robotic arm for pick-and-place tasks. BCIs can process steady-state visual evoked potentials (SSVEPs), which are signals generated through visual stimuli. The visual stimuli may be conveyed to the user with AR systems, expanding the range of possible applications. The proposed approach leverages the capabilities of the NextMind BCI to enable users to select objects in the range of the robotic arm. By displaying a visual anchor associated with each object in the scene with projected AR, the NextMind device can detect when users focus their eyesight on one of them, thus triggering the pick-up action of the robotic arm. The proposed system has been designed considering the needs and limitations of mobility-impaired people to support them when controlling a robotic arm for pick-and-place tasks. Two different approaches for positioning the visual anchors are proposed and analyzed. Experimental tests involving users show that both approaches are highly appreciated. The system performances are extremely robust, thus allowing the users to select objects in an easy, fast, and reliable way.},
  archive      = {J_THMS},
  author       = {Francesco De Pace and Federico Manuri and Matteo Bosco and Andrea Sanna and Hannes Kaufmann},
  doi          = {10.1109/THMS.2024.3414208},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {599-608},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Supporting Human–Robot interaction by projected augmented reality and a brain interface},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ARMedicalSketch: Exploring 3D sketching for medical image
using true 2D-3D interlinked visualization and interaction.
<em>THMS</em>, <em>54</em>(5), 589–598. (<a
href="https://doi.org/10.1109/THMS.2024.3432735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In traditional clinical practice, doctors often have to deal with 3D information based on 2D-displayed medical images. There is a considerable mismatch between the 2D and 3D dimensions in image interaction during clinical diagnosis, making image manipulation challenging and time-consuming. In this study, we explored 3D sketching for medical images using true 2D-3D interlinked visualization and interaction, presenting a novel AR environment named ARMedicalSketch. It supports image display enhancement preprocessing and 3D interaction tasks for original 3D medical images. Our interaction interface, based on 3D autostereoscopic display technology, provides both floating 3D display and 2D tablet display while enabling glasses-free visualization. We presented a method of 2D-3D interlinked visualization and interaction, employing synchronized projection visualization and a virtual synchronized interactive plane to establish an integrated relationship between 2D and 3D displays. Additionally, we utilized gesture sensors and a 2D touch tablet to capture the user&#39;s hand information for convenient interaction. We constructed the prototype and conducted a user study involving 23 students and 2 clinical experts. The controlled study compared our proposed system with a 2D display prototype, showing enhanced efficiency in interacting with medical images while maintaining 2D interaction accuracy, particularly in tasks involving strong 3D spatial correlation. In the future, we aim to further enhance the interaction precision and application scenarios of ARMedicalSketch.},
  archive      = {J_THMS},
  author       = {Nan Zhang and Tianqi Huang and Hongen Liao},
  doi          = {10.1109/THMS.2024.3432735},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {589-598},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {ARMedicalSketch: Exploring 3D sketching for medical image using true 2D-3D interlinked visualization and interaction},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Responses to external repeated perturbations vary with time
intervals. <em>THMS</em>, <em>54</em>(5), 582–588. (<a
href="https://doi.org/10.1109/THMS.2024.3426302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is generally accepted that stimuli with different time intervals lead to different contributions of relevant learning substrates. We investigated postural responses in men to trains of perturbations with different time intervals. A total of 12 male volunteers with no neurological deficits (age: 33.33±3.12 S.D.) experienced a sequence of perturbations. Two sequences of perturbations by a translational plate to still standing participants in the anterior-posterior direction were designed and administered: the first sequence consisted of 24 repeated perturbations with an interval of 5 s, while the second sequence consisted of ones with an interval of 2.5 s. A perturbation of a smaller magnitude was inserted into each sequence as a catch trial. We found that while a shorter interval and a longer interval both led to the learning effect (P &lt; 0.05), a shorter interval results in a stiff strategy, presumably through muscle cocontraction, minimizing the degree of body sway. The learned motor response continued after experiencing the catch trial (P &lt; 0.05). The results imply that stimulus intervals could lead to a different adaptation mechanism in the neuromotor system in the way to regain postural stability. Also, our results suggest that training for male individuals with repetition of postural perturbations with a shorter time interval leads to a stiff strategy with a greater degree. A stiff strategy could lower the ability to cope with unexpected postural threats, possibly leading to falls.},
  archive      = {J_THMS},
  author       = {Dongwon Kim and Jong-Moon Hwang},
  doi          = {10.1109/THMS.2024.3426302},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {582-588},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Responses to external repeated perturbations vary with time intervals},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Resilience in operators, technologies, and systems.
<em>THMS</em>, <em>54</em>(5), 565–581. (<a
href="https://doi.org/10.1109/THMS.2024.3408804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Changes in technology, particularly for example in commercial air operations, have led to incremental increases in the number and variety of associated safety procedures and checklists. This work addresses concerns about how people and systems respond to unanticipated events, as applicable to such commercial air operations, and to examine whether these “ if–then ” approaches prove sufficient to respond to prospective operational uncertainties. The current work draws from the literature on systems resilience, cognitive flexibility, and adaptation to consider how response strategies that are integrated into human–machine training at all levels of operation can aid in effective resolution to such unanticipated events. A number of scientific insights, methods, and domains are identified as being able to be employed to avoid catastrophic failure in current and prospective operational environments. While heuristics for advisement do provide an initial level of defensive protection, evolving airspace operations need to be adaptive to, and resilient in respect of, emerging and even unanticipated challenges. Prospective response strategies need to encompass both the demands that can be evidently foreseen and those that remain at present, indeterminate. Resilience in responding appears to be a primary dimension of success in relation to these challenges. The information herein distilled can increase operator performance and aviation systems’ response to nonproceduralized and unanticipated events as well as being applied to a vast array of other safety-critical operations beyond this one realm.},
  archive      = {J_THMS},
  author       = {P. A. Hancock and Jessica Cruit},
  doi          = {10.1109/THMS.2024.3408804},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {565-581},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Resilience in operators, technologies, and systems},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). User characteristics and their impact on the perceived
usable security of physical authentication devices. <em>THMS</em>,
<em>54</em>(5), 554–564. (<a
href="https://doi.org/10.1109/THMS.2024.3421538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical authentication devices (PADs) offer a higher level of security than other authentication technologies commonly used in multifactor authentication (MFA) schemes because they are much less vulnerable to attack. However, PAD uptake remains significantly lower than that for SMS and app-based approaches, accounting for only 10% of all authentication technologies currently being utilized in MFA. Prior studies indicate that the primary reason for this low adoption rate is due to negative users&#39; perceptions and attitudes toward the usability of PADs; many of these studies often skew toward a particular set of users (e.g., young university students, etc.), often creating a bias toward what usable security entails. To address this limitation, we have formulated an original research methodology that segments users into specific groups based on their user characteristics (i.e., age, education, and experience) and examines how each group defines usability and ranks their preferences regarding certain security features. Based on a survey of 410 participants, our results indicate that there are indeed different usable security preferences for each user group, and we, therefore, provide recommendations on how existing PADs might be enhanced to support usability and improve adoption rates.},
  archive      = {J_THMS},
  author       = {Jongkil Jay Jeong and Syed Wajid Ali Shah and Ashish Nanda and Robin Doss and Mohammad Nosouhi and Jeb Webb},
  doi          = {10.1109/THMS.2024.3421538},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {554-564},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {User characteristics and their impact on the perceived usable security of physical authentication devices},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time posture identification system for wheelchair users
preventing the generation of pressure ulcers. <em>THMS</em>,
<em>54</em>(5), 546–553. (<a
href="https://doi.org/10.1109/THMS.2024.3422267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prevention is key to avoid pressure ulcer generation in people with mobility restrictions. In recent years, preventive medicine has focused on posture control by considering people who frequently have the same position for too long, such as wheelchair users. Optical fiber sensors have gained recognition for their applications in biomedical engineering; however, approaches to assistive devices, such as wheelchairs, have been relatively unexplored. This study proposes a polymeric-optical-fiber (POF) sensing system based on machine learning (ML) for human posture recognition in an electrical wheelchair-based human machine interface (HMI). The ML-based model was used to classify time- and frequency-domain features obtained from a matrix of POF-based pressure sensors and 24 photodetectors during the execution of eight body postures. In an offline stage, multiclassification was conducted using k-nearest neighbors (KNN), decision tree, extra tree classifier (ETC), and random forest, where the best performance, in terms of accuracy (ACC), was obtained through the use of ETC (94%). Hence, this classifier was implemented in real-time, where the wheelchair-based HMI achieved a CPU time of approximately 117 ms, and an ACC higher than 96%, outperforming the metrics previously reported in the literature. We believe that this study contributes to the development of smart assistive systems that integrate ML and soft sensors to recognize body postures in an HMI, which is a promising approach for preventing the generation of pressure ulcers in wheelchair users.},
  archive      = {J_THMS},
  author       = {Aura Ximena Gonzalez-Cely and Cristian Felipe Blanco-Diaz and Teodiano Bastos-Filho and Camilo Arturo Rodriguez-Diaz},
  doi          = {10.1109/THMS.2024.3422267},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {546-553},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Real-time posture identification system for wheelchair users preventing the generation of pressure ulcers},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HmOS: An extensible platform for task-oriented human–machine
computing. <em>THMS</em>, <em>54</em>(5), 536–545. (<a
href="https://doi.org/10.1109/THMS.2024.3414432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With rapid advancements in artificial intelligence (AI) technologies, AI-powered machines are increasingly capable of collaborating with humans to enhance decision-making in various human–machine collaboration scenarios, e.g., medical diagnosis, criminal justice, and autonomous driving. As a result, human–machine computing (HMC) has emerged as a promising computing paradigm that integrates the expertise of humans with the reliable data processing capabilities of machines. Using HMC to facilitate the processing of domain-specific tasks has a lot of potential, but is limited in system-level scalability, i.e., there is no one common easy-to-use interface. In this article, we present human-machine operating system (hmOS) , an open extensible platform for researchers to experiment with HMC for investigating system-centric human–machine collaboration problems. hmOS supports flexible human–machine collaboration on the strength of the quality-aware task decomposition and allocation. To achieve that, the underlying system architecture and runtime environment are first developed to build a foundational abstraction for the kernel of hmOS . Second, hmOS facilitates flexible human–machine collaboration through a suitability-based task allocation mechanism, quality estimation guided by fuzzy rules, and iterative feedback on result tuning. We implement the newly proposed hmOS in a prototype featuring interactive interfaces. Finally, we conduct extensive and realistic experiments to validate the effectiveness of our platform across diverse tasks, showcasing the broad feasibility of hmOS .},
  archive      = {J_THMS},
  author       = {Hui Wang and Zhiwen Yu and Yao Zhang and Yanfei Wang and Fan Yang and Liang Wang and Jiaqi Liu and Bin Guo},
  doi          = {10.1109/THMS.2024.3414432},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {536-545},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {HmOS: An extensible platform for task-oriented Human–Machine computing},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic review on custom data gloves. <em>THMS</em>,
<em>54</em>(5), 520–535. (<a
href="https://doi.org/10.1109/THMS.2024.3394674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hands are a fundamental tool humans use to interact with the environment and objects. Through hand motions, we can obtain information about the shape and materials of the surfaces we touch, modify our surroundings by interacting with objects, manipulate objects and tools, or communicate with other people by leveraging the power of gestures. For these reasons, sensorized gloves, which can collect information about hand motions and interactions, have been of interest since the 1980s in various fields, such as human–machine interaction and the analysis and control of human motions. Over the last 40 years, research in this field explored different technological approaches and contributed to the popularity of wearable custom and commercial products targeting hand sensorization. Despite a positive research trend, these instruments are not widespread yet outside research environments and devices aimed at research are often ad hoc solutions with a low chance of being reused. This article aims to provide a systematic literature review for custom gloves to analyze their main characteristics and critical issues, from the type and number of sensors to the limitations due to device encumbrance. The collection of this information lays the foundation for a standardization process necessary for future breakthroughs in this research field.},
  archive      = {J_THMS},
  author       = {Valerio Belcamino and Alessandro Carfì and Fulvio Mastrogiovanni},
  doi          = {10.1109/THMS.2024.3394674},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {520-535},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A systematic review on custom data gloves},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). To err is automation: Can trust be repaired by the automated
driving system after its failure? <em>THMS</em>, <em>54</em>(5),
508–519. (<a href="https://doi.org/10.1109/THMS.2024.3434680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Failures of the automated driving system (ADS) in automated vehicles (AVs) can damage driver–ADS cooperation (e.g., causing trust damage) and traffic safety. Researchers suggest infusing a human-like ability, active trust repair, into automated systems, to mitigate broken trust and other negative impacts resulting from their failures. Trust repair is regarded as a key ergonomic design in automated systems. Trust repair strategies (e.g., apology) are examined and supported by some evidence in controlled environments, however, rarely subjected to empirical evaluations in more naturalistic environments. To fill this gap, we conducted a test track study, invited participants ( N = 257) to experience an ADS failure, and tested the influence of the ADS’ trust repair on trust and other psychological responses. Half of participants ( n = 128) received the ADS’ verbal message (consisting of apology, explanation, and promise) by a human voice ( n = 63) or by Apple&#39;s Siri ( n = 65) after its failure. We measured seven psychological responses to AVs and ADS [e.g., trust and behavioral intention (BI)]. We found that both strategies cannot repair damaged trust. The human-voice-repair strategy can to some degree mitigate other detrimental influences (e.g., reductions in BI) resulting from the ADS failure, but this effect is only notable among participants without substantial driving experience. It points to the importance of conducting ecologically valid and field studies for validating human-like trust repair strategies in human–automation interaction and of developing trust repair strategies specific to safety-critical situations.},
  archive      = {J_THMS},
  author       = {Peng Liu and Yueying Chu and Guanqun Wang and Zhigang Xu},
  doi          = {10.1109/THMS.2024.3434680},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {508-519},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {To err is automation: Can trust be repaired by the automated driving system after its failure?},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating perceived mental workload from eye-tracking data
based on benign anisocoria. <em>THMS</em>, <em>54</em>(5), 499–507. (<a
href="https://doi.org/10.1109/THMS.2024.3432864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From the initial phases of human–computer interaction, where the computer was unaware of the users&#39; mental states, we are now progressing toward cognition-aware user interfaces. One crucial cognitive state considered by research on cognition-aware user interfaces is the cognitive load. Eye-tracking has been suggested as one particularly unobtrusive method for estimating cognitive load. Although the accuracy of cognitive load detection has improved in recent work, it is still insufficient for cognition-aware user interfaces, which require high accuracy for getting accepted by the user. This article introduces two new eye-tracking metrics for estimating perceived cognitive load based on benign anisocoria (BA). Unlike previous pupil-based metrics, our metrics are based on pupil size asymmetry between the left and right eye. As a case study, we illustrate the effectiveness of the proposed metrics on a recently published eye-tracking dataset recorded under laboratory conditions. The results show that our proposed features based on BA can improve the performance of classifiers for detecting the perceived mental workload associated with an $N$ -back test. The best classification accuracy was 84.24% while the classification accuracy in the absence of the proposed features was 81.91% for the light gradient boosting classifier.},
  archive      = {J_THMS},
  author       = {Suvodip Chakraborty and Peter Kiefer and Martin Raubal},
  doi          = {10.1109/THMS.2024.3432864},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {499-507},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Estimating perceived mental workload from eye-tracking data based on benign anisocoria},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust object selection in spontaneous gaze-controlled
application using exponential moving average and hidden markov model.
<em>THMS</em>, <em>54</em>(5), 485–498. (<a
href="https://doi.org/10.1109/THMS.2024.3413781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human gaze is a promising input modality for interactive applications due to its advantages: giving benefits to motion-impaired people while providing faster, intuitive, and easy interaction. The most common form of gaze interaction is object selection. During the last decade, gaze gestures and smooth pursuit-based interaction have been emerging techniques for spontaneous object selection in various gaze-controlled applications. Unfortunately, the challenge of spontaneous interaction demands no prior gaze-to-screen calibration, which leads to inaccurate object selection. To overcome the accuracy issue, this article proposes a novel method for spontaneous gaze interaction based on Pearson product-moment correlation as a measure of similarity, an exponential moving average filter for signal denoising, and a hidden Markov model to perform eye movement classification. Based on experimental results, our approach yielded the best object selection accuracy and success time of $\text{89.60}\pm \text{10.59}\%$ and $\text{4364}\pm \text{235.86}$ ms, respectively. Our results imply that spontaneous interaction for gaze-controlled applications is possible with careful consideration of the underlying techniques to handle noisy data generated by the eye tracker. Furthermore, the proposed method is promising for future development of interactive touchless display systems that comply with the health protocols of the World Health Organization during the COVID-19 pandemic.},
  archive      = {J_THMS},
  author       = {Suatmi Murnani and Noor Akhmad Setiawan and Sunu Wibirama},
  doi          = {10.1109/THMS.2024.3413781},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {485-498},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Robust object selection in spontaneous gaze-controlled application using exponential moving average and hidden markov model},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Utilizing gramian angular fields and convolution neural
networks in flex sensors glove for human–computer interaction.
<em>THMS</em>, <em>54</em>(4), 475–483. (<a
href="https://doi.org/10.1109/THMS.2024.3404101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current sensor systems using the human–computer interface to develop a hand gesture recognition system remain challenging. This research presents the development of hand gesture recognition with 16-DoF glove sensors combined with a convolution neural network. The flex sensors are attached to 16 pivot joints of the human hand on the glove so that each knuckle flex can be measured while holding the object. The 16-DoF point sensors collecting circuit and adjustable buffer circuit were developed in this research to work with the Arduino Nano microcontroller to record each sensor&#39;s signal. This article investigates the time-series data of the flex sensor signal into 2-D colored images, concatenating the signals into one bigger image with a Gramian angular field and then recognition through a deep convolutional neural network (DCNN). The 16-DoF glove sensors were proposed for testing with three experiments using 8 models of DCNN recognition. These were conducted on 20 hand gesture recognition, 12 hand sign recognition, and object manipulation according to shape. The experimental results indicated that the best performance for the hand grasp experiment is 99.49% with Resnet 101, the hand sign experiment is 100% with Alexnet, and the object attribute experiment is 99.77% with InceptionNet V3.},
  archive      = {J_THMS},
  author       = {Chana Chansri and Jakkree Srinonchat},
  doi          = {10.1109/THMS.2024.3404101},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {475-483},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Utilizing gramian angular fields and convolution neural networks in flex sensors glove for Human–Computer interaction},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BERT-based semantic-aware heterogeneous graph embedding
method for enhancing app usage prediction accuracy. <em>THMS</em>,
<em>54</em>(4), 465–474. (<a
href="https://doi.org/10.1109/THMS.2024.3412273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread adoption of smartphones and mobile Internet, understanding user behavior and improving user experience are critical. This article introduces semantic-aware (SA)-BERT, a novel model that integrates spatio-temporal and semantic information to represent App usage effectively. Leveraging BERT, SA-BERT captures rich contextual information. By introducing a specific objective function to represent the cooccurrence of App-time-location paths, SA-BERT can effectively model complex App usage structures. Based on this method, we adopt the learned embedding vectors in App usage prediction tasks. We evaluate the performance of SA-BERT using a large-scale real-world dataset. As demonstrated in the numerous experimental results, our model outperformed other strategies evidently. In terms of the prediction accuracy, we achieve a performance gain of 34.9% compared with widely used the SA representation learning via graph convolutional network (SA-GCN), and 134.4% than the context-aware App usage prediction with heterogeneous graph embedding. In addition, we reduced 79.27% training time compared with SA-GCN.},
  archive      = {J_THMS},
  author       = {Xi Fang and Hui Yang and Liu Shi and Yilong Wang and Li Li},
  doi          = {10.1109/THMS.2024.3412273},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {465-474},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {BERT-based semantic-aware heterogeneous graph embedding method for enhancing app usage prediction accuracy},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated classification of cognitive visual objects using
multivariate swarm sparse decomposition from multichannel EEG-MEG
signals. <em>THMS</em>, <em>54</em>(4), 455–464. (<a
href="https://doi.org/10.1109/THMS.2024.3395153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In visual object decoding, magnetoencephalogram (MEG) and electroencephalogram (EEG) activation patterns demonstrate the utmost discriminative cognitive analysis due to their multivariate oscillatory nature. However, high noise in the recorded EEG-MEG signals and subject-specific variability make it extremely difficult to classify subject&#39;s cognitive responses to different visual stimuli. The proposed method is a multivariate extension of the swarm sparse decomposition method (MSSDM) for multivariate pattern analysis of EEG-MEG-based visual activation signals. In comparison, it is an advanced technique for decomposing nonstationary multicomponent signals into a finite number of channel-aligned oscillatory components that significantly enhance visual activation-related sub-bands. The MSSDM method adopts multivariate swarm filtering and sparse spectrum to automatically deliver optimal frequency bands in channel-specific sparse spectrums, resulting in improved filter banks. By combining the advantages of the multivariate SSDM and Riemann&#39;s correlation-assisted fusion feature (RCFF), the MSSDM-RCFF algorithm is investigated to improve the visual object recognition ability of EEG-MEG signals. We have also proposed time–frequency representation based on MSSDM to analyze discriminative cognitive patterns of different visual object classes from multichannel EEG-MEG signals. A proposed MSSDM is evaluated on multivariate synthetic signals and multivariate EEG-MEG signals using five classifiers. The proposed fusion feature and linear discriminant analysis classifier-based framework outperformed all existing state-of-the-art methods used for visual object detection and achieved the highest accuracy of 86.42% using tenfold cross-validation on EEG-MEG multichannel signals.},
  archive      = {J_THMS},
  author       = {Shailesh Vitthalrao Bhalerao and Ram Bilas Pachori},
  doi          = {10.1109/THMS.2024.3395153},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {455-464},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Automated classification of cognitive visual objects using multivariate swarm sparse decomposition from multichannel EEG-MEG signals},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling brake perception response time in on-road and
roadside hazards using an integrated cognitive architecture.
<em>THMS</em>, <em>54</em>(4), 441–454. (<a
href="https://doi.org/10.1109/THMS.2024.3408841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we used a computational cognitive architecture called queuing network–adaptive control of thought rational–situation awareness (QN–ACTR–SA) to model and simulate the brake perception response time (BPRT) to visual roadway hazards. The model incorporates an integrated driver model to simulate human driving behavior and uses a dynamic visual sampling model to simulate how drivers allocate their attention. We validated the model by comparing its results to empirical data from human participants who encountered on-road and roadside hazards in a simulated driving environment. The results showed that BPRT was shorter for on-road hazards compared to roadside hazards and that the overall model fitness had a mean absolute percentage error of 9.4% and a root mean squared error of 0.13 s. The modeling results demonstrated that QN–ACTR–SA could effectively simulate BPRT to both on-road and roadside hazards and capture the difference between the two contrasting conditions.},
  archive      = {J_THMS},
  author       = {Umair Rehman and Shi Cao and Carolyn G. Macgregor},
  doi          = {10.1109/THMS.2024.3408841},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {441-454},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Modeling brake perception response time in on-road and roadside hazards using an integrated cognitive architecture},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LANDER: Visual analysis of activity and uncertainty in
surveillance video. <em>THMS</em>, <em>54</em>(4), 427–440. (<a
href="https://doi.org/10.1109/THMS.2024.3409722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision algorithms face challenges of limited visual presentation and unreliability in pedestrian activity assessment. In this article, we introduce LANDER, an interactive analysis system for visual exploration of pedestrian activity and uncertainty in surveillance videos. This visual analytics system focuses on three common categories of uncertainties in object tracking and action recognition. LANDER offers an overview visualization of activity and uncertainty, along with spatio-temporal exploration views closely associated with the scene. Expert evaluation and user study indicate that LANDER outperforms traditional video exploration in data presentation and analysis workflow. Specifically, compared to the baseline method, it excels in reducing retrieval time ( $p&amp;lt; $ 0.01), enhancing uncertainty identification ( $p&amp;lt; $ 0.05), and improving the user experience ( $p&amp;lt; $ 0.05).},
  archive      = {J_THMS},
  author       = {Tong Li and Guodao Sun and Baofeng Chang and Yunchao Wang and Qi Jiang and Yuanzhong Ying and Li Jiang and Haixia Wang and Ronghua Liang},
  doi          = {10.1109/THMS.2024.3409722},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {427-440},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {LANDER: Visual analysis of activity and uncertainty in surveillance video},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed formation control for a class of
human-in-the-loop multiagent systems. <em>THMS</em>, <em>54</em>(4),
416–426. (<a href="https://doi.org/10.1109/THMS.2024.3398631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the distributed formation control problem for a class of human-in-the-loop (HiTL) multiagent systems (MASs) is studied. A hidden Markov jump MAS is employed to model the HiTL MAS, which integrates the human models, the MAS model, and their interactions. The HiTL MAS investigated in this article is composed of two parts: a leader without human in the control loop and a group of followers in which each follower is simultaneously controlled by a human operator and an automation. For each follower, a hidden Markov model is used for modeling the human behaviors in consideration of the random nature of human internal state (HIS) reasoning and the uncertainty from HIS observation. By means of a stochastic Lyapunov function, a necessary and sufficient condition is first developed in terms of the linear matrix inequalities (LMIs) to ensure the formation of the HiTL MAS in the mean-square sense. Then, an LMI approach to the human-assistance control design is proposed for the automations in the followers to guarantee the mean-square formation of the HiTL MAS. Finally, simulation results are presented to verify the effectiveness of the proposed methods.},
  archive      = {J_THMS},
  author       = {Xiao-Xiao Zhang and Huai-Ning Wu and Jin-Liang Wang},
  doi          = {10.1109/THMS.2024.3398631},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {416-426},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Distributed formation control for a class of human-in-the-loop multiagent systems},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Personalized trajectory-based risk prediction on curved
roads with consideration of driver turning behavior and workload.
<em>THMS</em>, <em>54</em>(4), 406–415. (<a
href="https://doi.org/10.1109/THMS.2024.3407333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and robust risk prediction on curved roads can significantly reduce lane departure accidents and improve traffic safety. However, limited study has considered dynamic driver-related factors in risk prediction, resulting in poor algorithm adaptiveness to individual differences. This article presents a novel personalized risk prediction method with consideration of driver turning behavior and workload by using the predicted vehicle trajectory.First, driving simulation experiments are conducted to collect synchronized trajectory data, vehicle dynamic data, and eye movement data. The drivers are distracted by answering questions via a Bluetooth headset, leading to an increased cognitive workload. Secondly, the k -means clustering algorithm is utilized to extract two turning behaviors: driving toward the inner and outer side of a curved road. The turning behavior of each trajectory is then recognized using the trajectory data. In addition, the driver workload is recognized using the vehicle dynamic features and eye movement features. Thirdly, an extra personalization index is introduced to a long short-term memory encoder–decoder trajectory prediction network. This index integrates the driver turning behavior and workload information. After introducing the personalization index, the root-mean-square errors of the proposed network are reduced by 15.6%, 23.5%, and 29.1% with prediction horizons of 2, 3, and 4 s, respectively. Fourthly, the risk potential field theory is employed for risk prediction using the predicted trajectory data. This approach implicitly incorporates the driver&#39;s personalized information into risk prediction.},
  archive      = {J_THMS},
  author       = {Yahui Liu and Jingyuan Li and Yingbo Sun and Xuewu Ji and Chen Lv},
  doi          = {10.1109/THMS.2024.3407333},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {406-415},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Personalized trajectory-based risk prediction on curved roads with consideration of driver turning behavior and workload},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fast and efficient approach for human action recovery from
corrupted 3-d motion capture data using QR decomposition-based
approximate SVD. <em>THMS</em>, <em>54</em>(4), 395–405. (<a
href="https://doi.org/10.1109/THMS.2024.3400290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a robust algorithm for the fast recovery of human actions from corrupted 3-D motion capture (mocap) sequences. The proposed algorithm can deal with misrepresentations and incomplete representations in mocap data simultaneously. Fast convergence of the proposed algorithm is ensured by minimizing the overhead associated with time and resource utilization. To this end, we have used an approximate singular value decomposition (SVD) based on QR decomposition and $\ell _{2,1}$ norm minimization as a replacement for the conventional nuclear norm-based SVD. In addition, the proposed method is braced by incorporating the spatio-temporal properties of human action in the optimization problem. For this, we have introduced pair-wise hierarchical constraint and the trajectory movement constraint in the problem formulation. Finally, the proposed method is void of the requirement of a sizeable database for training the model. The algorithm can easily be adapted to work on any form of corrupted mocap sequences. The proposed algorithm is faster by 30% on average compared with the counterparts employing similar kinds of constraints with improved performance in recovery.},
  archive      = {J_THMS},
  author       = {M. S. Subodh Raj and Sudhish N. George},
  doi          = {10.1109/THMS.2024.3400290},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {395-405},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A fast and efficient approach for human action recovery from corrupted 3-D motion capture data using QR decomposition-based approximate SVD},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Singularity-free finite-time adaptive optimal control for
constrained coordinated uncertain robots. <em>THMS</em>, <em>54</em>(4),
385–394. (<a href="https://doi.org/10.1109/THMS.2024.3397351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the singularity-free finite-time adaptive optimal control problem for coordinated robots, where the position and velocity are constrained within the asymmetric yet time-varying ranges. Different from the existing results concerning constrained control, the imposed feasibility conditions are relaxed by skillfully integrating a nonlinear state-dependent function into the backstepping design procedure. Therein, the typical feature of the designed finite-time controller lies in the application of the modified smooth switching function, rendering the designed controller powerful enough to eliminate singularity problem. Notably, with the aid of the constructed optimal cost function and neural network-based critic architecture, the optimal control law is established under the backstepping design framework. It is theoretically verified that the designed controller is of satisfied optimization and finite-time tracking ability, and desired constrained objective in the meanwhile. The validity of the resulting control algorithm is eventually substantiated via two robotic manipulators.},
  archive      = {J_THMS},
  author       = {Shenquan Wang and Wen Yang and Yulian Jiang and Mohammed Chadli and Yanzheng Zhu},
  doi          = {10.1109/THMS.2024.3397351},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {385-394},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Singularity-free finite-time adaptive optimal control for constrained coordinated uncertain robots},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A physics-based virtual reality haptic system design and
evaluation by simulating human-robot collaboration. <em>THMS</em>,
<em>54</em>(4), 375–384. (<a
href="https://doi.org/10.1109/THMS.2024.3407109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in virtual reality (VR) technology facilitate tracking real-world objects and users&#39; movements in the virtual environment (VE) and inspire researchers to develop a physics-based haptic system (i.e., real object haptics) instead of computer-generated haptic feedback. However, there is limited research on the efficacy of such VR systems in enhancing operators’ sensorimotor learning for tasks that require high motor and physical demands. Therefore, this study aimed to design and evaluate the efficacy of a physics-based VR system that provides users with realistic cutaneous and kinesthetic haptic feedback. We designed a physics-based VR system, named PhyVirtual, and simulated human–robot collaborative (HRC) sequential pick-and-place lifting tasks in the VE. Participants performed the same tasks in the real environment (RE) with human–human collaboration instead of human–robot collaboration. We used a custom-designed questionnaire, the NASA-TLX, and electromyography activities from biceps, middle, and anterior deltoid muscles to determine user experience, workload, and neuromuscular dynamics, respectively. Overall, the majority of responses (&gt;65%) demonstrated that the system is easy-to-use, easy-to-learn, and effective in improving motor skill performance. While compared to tasks performed in the RE, no significant difference was observed in the overall workload for the PhyVirtual system. The electromyography data exhibited similar trends ( p &gt; 0.05; r &gt; 0.89) for both environments. These results show that the PhyVirtual system is an effective tool to simulate safe human–robot collaboration commonly seen in many modern warehousing settings. Moreover, it can be used as a viable replacement for live sensorimotor training in a wide range of fields.},
  archive      = {J_THMS},
  author       = {Syed T. Mubarrat and Antonio Fernandes and Suman K. Chowdhury},
  doi          = {10.1109/THMS.2024.3407109},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {375-384},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A physics-based virtual reality haptic system design and evaluation by simulating human-robot collaboration},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bilateral teleoperation strategy augmented by EMGP-VH for
live-line maintenance robot. <em>THMS</em>, <em>54</em>(4), 362–374. (<a
href="https://doi.org/10.1109/THMS.2024.3412910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In robot-assisted live-line maintenance, bilateral teleoperation is still a popular and effective approach in assisting operators to accomplish hazards tasks. Particularly, teleoperation under overhead power lines attach greater expectation on safe operation and telepresence. In this article, we propose a visual-haptic bilateral teleoperation strategy, i.e., EMGP-VH , based on visual guidance, haptic constraint and mixed reality (MR) augmentation. To the best of our knowledge, electromagnetic field is first applied to serve the path planning of teleoperation in live-line maintenance. In visual guidance, EMG-potential fields are integrated into RRT* to calculate a low-energy path. At the same time, real-time haptic constraint is calculated based on a tube virtual fixture. MR augmentation also works as an indispensable part in both the platform construction and visual guidance. Our proposal has been extensively compared using seven objective performances and three subjective questionnaires both in simulation and real-world experiment with five different scenes and two approaches state-of-the-art, respectively. Functionality of EMGP-RRT* and effectiveness of haptic constraint are further analyzed. Results show that EMGP-RRT* has significant improvements both in searching efficiency and safety performances; and the proposed system ( EMGP-VH ) significantly contributes to improving telepresence and ensuring safe operations during live-line maintenance, resulting in a 30% reduction in operation time and a 60% decrease in trajectory offset.},
  archive      = {J_THMS},
  author       = {Shaodong Li and Peiyuan Gao and Yongzheng Chen},
  doi          = {10.1109/THMS.2024.3412910},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {362-374},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A bilateral teleoperation strategy augmented by EMGP-VH for live-line maintenance robot},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review of robot-assisted hand spasticity assessment.
<em>THMS</em>, <em>54</em>(4), 349–361. (<a
href="https://doi.org/10.1109/THMS.2024.3393014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spasticity is a common neuromuscular abnormality following upper motor neuron lesions. Conventionally, spasticity is assessed through manual clinical scales, which have limitations due to the subjectivity involved. The development of rehabilitation robotics introduced new solutions to this problem, producing novel robot-assisted spasticity assessment approaches. In this article, we present the current state and challenges of robot-assisted hand spasticity assessment (RAHSA) based on a review of instrumented clinical scales, biomechanical and neurophysiological measures, and medical imaging methods for upper extremity spasticity assessment between January 2000 and February 2023. The characteristics of hand anatomy and spasticity symptoms make it challenging to develop RAHSA approaches and corresponding robotic systems. Although the combination of hand robots and instrumented assessment methods has evoked studies on RAHSA, more research is needed on the new assessment approaches fusing neurological and nonneurological measures and novel robotic systems specifically designed for hand spasticity assessment.},
  archive      = {J_THMS},
  author       = {Hao Yu and Alyson Nelson and Mustafa Suphi Erden},
  doi          = {10.1109/THMS.2024.3393014},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {349-361},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A review of robot-assisted hand spasticity assessment},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gesture-mmWAVE: Compact and accurate millimeter-wave
radar-based dynamic gesture recognition for embedded devices.
<em>THMS</em>, <em>54</em>(3), 337–347. (<a
href="https://doi.org/10.1109/THMS.2024.3385124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic gesture recognition using millimeter-wave radar is a promising contactless mode of human–computer interaction with wide-ranging applications in various fields, such as intelligent homes, automatic driving, and sign language translation. However, the existing models have too many parameters and are unsuitable for embedded devices. To address this issue, we propose a dynamic gesture recognition method (named “Gesture-mmWAVE”) using millimeter-wave radar based on the multilevel feature fusion (MLFF) and transformer model. We first arrange each frame of the original echo collected by the frequency-modulated continuously modulated millimeter-wave radar in the Chirps × Samples format. Then, we use a 2-D fast Fourier transform to obtain the range-time map and Doppler-time map of gestures while improving the echo signal-to-noise ratio by coherent accumulation. Furthermore, we build an MLFF-transformer network for dynamic gesture recognition. The MLFF-transformer network comprises an MLFF module and a transformer module. The MLFF module employs the residual strategies to fuse the shallow, middle, and deep features and reduce the parameter size of the model using depthwise-separable convolution. The transformer module captures the global features of dynamic gestures and focuses on essential features using the multihead attention mechanism. The experimental results demonstrate that our proposed model achieves an average recognition accuracy of 99.11% on a dataset with 10% random interference. The scale of the proposed model is only 0.42M, which is 25% of that of the MobileNet V3-samll model. Thus, this method has excellent potential for application in embedded devices due to its small parameter size and high recognition accuracy.},
  archive      = {J_THMS},
  author       = {Biao Jin and Xiao Ma and Bojun Hu and Zhenkai Zhang and Zhuxian Lian and Biao Wang},
  doi          = {10.1109/THMS.2024.3385124},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {337-347},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Gesture-mmWAVE: Compact and accurate millimeter-wave radar-based dynamic gesture recognition for embedded devices},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AI-SenseVision: A low-cost artificial-intelligence-based
robust and real-time assistance for visually impaired people.
<em>THMS</em>, <em>54</em>(3), 325–336. (<a
href="https://doi.org/10.1109/THMS.2024.3375655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visually impaired people (VIPs) encounter various challenges in their daily lives, and there is a need for portable, user-friendly device for real-time assistance to give them guidance regarding their surroundings. This article presents an artificial-intelligence-based innovative wearable assistive device—artificial intelligence (AI)-SenseVision—to analyze visual and sensory information about the objects and obstacles present in the scene to perceive the surrounding environment. The device is a complete amalgamation of sensor and computer-vision-based technologies that generate auditory information with the name of identified objects or audio warnings for detected obstacles. The performance of the trained deep-learning model is rigorously tested in complex and real-life scenarios using various statistical parameters for experimental validation. Moreover, the trained deep-learning models have been integrated into a low-cost single-board processor to make a standalone cost-effective device. All data processing is done within an optimized single hardware setup, and the user can easily access different modes, such as indoor and outdoor mode, while also enabling object counting in observed scenes. The proposed system has low-cost sensors, multiple operational modes, easy integration, and small volume, making this assistive device helpful for VIPs for independent navigation and collision prevention.},
  archive      = {J_THMS},
  author       = {Rakesh Chandra Joshi and Nitin Singh and Anuj Kumar Sharma and Radim Burget and Malay Kishore Dutta},
  doi          = {10.1109/THMS.2024.3375655},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {325-336},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {AI-SenseVision: A low-cost artificial-intelligence-based robust and real-time assistance for visually impaired people},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SonoMyoNet: A convolutional neural network for predicting
isometric force from highly sparse ultrasound images. <em>THMS</em>,
<em>54</em>(3), 317–324. (<a
href="https://doi.org/10.1109/THMS.2024.3389690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultrasound imaging or sonomyography has been found to be a robust modality for measuring muscle activity due to its ability to image deep-seated muscles directly while providing superior spatiotemporal specificity compared with surface electromyography-based techniques. Quantifying the morphological changes during muscle activity involves computationally expensive approaches for tracking muscle anatomical structures or extracting features from brightness-mode (B-mode) images and amplitude-mode signals. This article uses an offline regression convolutional neural network called SonoMyoNet to estimate continuous isometric force from sparse ultrasound scanlines. SonoMyoNet learns features from a few equispaced scanlines selected from B-mode images and utilizes the learned features to estimate continuous isometric force accurately. The performance of SonoMyoNet was evaluated by varying the number of scanlines to simulate the placement of multiple single-element ultrasound transducers in a wearable system. Results showed that SonoMyoNet could accurately predict isometric force with just four scanlines and is immune to speckle noise and shifts in the scanline location. Thus, the proposed network reduces the computational load involved in feature tracking algorithms and estimates muscle force from the global features of sparse ultrasound images.},
  archive      = {J_THMS},
  author       = {Anne Tryphosa Kamatham and Meena Alzamani and Allison Dockum and Siddhartha Sikdar and Biswarup Mukherjee},
  doi          = {10.1109/THMS.2024.3389690},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {317-324},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {SonoMyoNet: A convolutional neural network for predicting isometric force from highly sparse ultrasound images},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human vital signs estimation using resonance sparse spectrum
decomposition. <em>THMS</em>, <em>54</em>(3), 304–316. (<a
href="https://doi.org/10.1109/THMS.2024.3381074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The noncontact measurement and monitoring of human vital signs has evolved into a valuable tool for efficient health management. Because of the greater penetration capability through material and clothes, which is less affected by environmental conditions such as illumination, temperature, and humidity, mmWave radar has been extensively researched for human vital sign measurement in the past years. However, interference due to unwanted clutter, random body movement, and respiration harmonics make accurate retrieval of the heart rate (HR) difficult. This article proposes a resonance sparse spectrum decomposition (RSSD) algorithm and harmonics used algorithm (HUA) for accurate HR extraction. RSSD addresses the clutter and random body movement effects from phase signals, while HUA uses harmonics to extract HR accurately. A set of controlled experiments was conducted under different scenarios, and the proposed method is validated against ground truth HR/RR data collected by a smart vest. Our results show an accuracy of up to 98%–100% for distances up to 2 m. The method substantially improves HR estimation accuracy by effectively mitigating the effects of noise in the phase signal, even under heavy clutter and moderate body movement. Our results demonstrate that the proposed method effectively counters harmonic interference for accurate estimation of HR comparable to RR estimation up to a distance of 4 m from the radar sensor.},
  archive      = {J_THMS},
  author       = {Anuradha Singh and Saeed Ur Rehman and Sira Yongchareon and Peter Han Joo Chong},
  doi          = {10.1109/THMS.2024.3381074},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {304-316},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Human vital signs estimation using resonance sparse spectrum decomposition},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Noncontact respiratory anomaly detection using infrared
light-wave sensing. <em>THMS</em>, <em>54</em>(3), 292–303. (<a
href="https://doi.org/10.1109/THMS.2024.3381574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human respiratory rate and its pattern convey essential information about the physical and psychological states of the subject. Abnormal breathing can indicate fatal health issues leading to further diagnosis and treatment. Wireless light-wave sensing (LWS) using incoherent infrared light shows promise in safe, discreet, efficient, and noninvasive human breathing monitoring without raising privacy concerns. The respiration monitoring system needs to be trained on different types of breathing patterns to identify breathing anomalies. The system must also validate the collected data as a breathing waveform, discarding any faulty data caused by external interruption, user movement, or system malfunction. To address these needs, this study simulated normal and different types of abnormal respiration using a robot that mimics human breathing patterns. Then, time-series respiration data were collected using infrared light-wave sensing technology. Three machine learning algorithms, decision tree, random forest and XGBoost, were applied to detect breathing anomalies and faulty data. Model performances were evaluated through cross-validation, assessing classification accuracy, precision, and recall scores. The random forest model achieved the highest classification accuracy of 96.75% with data collected at a 0.5 m distance. In general, ensemble models like random forest and XGBoost performed better than a single model in classifying the data collected at multiple distances from the LWS setup.},
  archive      = {J_THMS},
  author       = {Md Zobaer Islam and Brenden Martin and Carly Gotcher and Tyler Martinez and John F. O&#39;Hara and Sabit Ekin},
  doi          = {10.1109/THMS.2024.3381574},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {292-303},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Noncontact respiratory anomaly detection using infrared light-wave sensing},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CST framework: A robust and portable finger motion tracking
framework. <em>THMS</em>, <em>54</em>(3), 282–291. (<a
href="https://doi.org/10.1109/THMS.2024.3385105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finger motion tracking is a significant challenge in the field of motion capture. However, existing technology for finger motion tracking often requires the wearing of a heavy device and a laborious calibration process to track the bending angle of each joint; this can be challenging, particularly because the motion of each finger has a high coupling characteristic. To address this issue, in this work, we have proposed a compressed sensing-based tracking (CST) framework that enables the estimation of the bending angle of all hand joints using sensors smaller than the number of hand joints. Our framework also integrates a real-time calibration function, which significantly simplifies the calibration process. We developed a glove with multiple liquid metal sensors and an inertial measurement unit to evaluate the effectiveness of our CST framework. The experimental results show that our CST framework can achieve high-speed and accurate hand arbitrary motion capture with only 12 sensors. The motion-tracking gloves developed on this basis are user-friendly and particularly suitable for human–computer interaction applications in robot control, the metaverse and other fields.},
  archive      = {J_THMS},
  author       = {Yong Ding and Mingchen Zou and Yueyang Teng and Yue Zhao and Xingyu Jiang and Xiaoyu Cui},
  doi          = {10.1109/THMS.2024.3385105},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {282-291},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {CST framework: A robust and portable finger motion tracking framework},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human-centered evaluation of EMG-based upper-limb prosthetic
control modes. <em>THMS</em>, <em>54</em>(3), 271–281. (<a
href="https://doi.org/10.1109/THMS.2024.3381094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this study was to experimentally test the effects of different electromyographic-based prosthetic control modes on user task performance, cognitive workload, and perceived usability to inform further human-centered design and application of these prosthetic control interfaces. We recruited 30 able-bodied participants for a between-subjects comparison of three control modes: direct control (DC), pattern recognition (PR), and continuous control (CC). Multiple human-centered evaluations were used, including task performance, cognitive workload, and usability assessments. To ensure that the results were not task-dependent, this study used two different test tasks, including the clothespin relocation task and Southampton hand assessment procedure-door handle task. Results revealed performance with each control mode to vary among tasks. When the task had high-angle adjustment accuracy requirements, the PR control outperformed DC. For cognitive workload, the CC mode was superior to DC in reducing user load across tasks. Both CC and PR control appear to be effective alternatives to DC in terms of task performance and cognitive load. Furthermore, we observed that, when comparing control modes, multitask testing and multifaceted evaluations are critical to avoid task-induced or method-induced evaluation bias. Hence, future studies with larger samples and different designs will be needed to expand the understanding of prosthetic device features and workload relationships.},
  archive      = {J_THMS},
  author       = {Yunmei Liu and Joseph Berman and Albert Dodson and Junho Park and Maryam Zahabi and He Huang and Jaime Ruiz and David B. Kaber},
  doi          = {10.1109/THMS.2024.3381094},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {271-281},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Human-centered evaluation of EMG-based upper-limb prosthetic control modes},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Thresholds for perceiving changes in friction when combined
with linear system dynamics. <em>THMS</em>, <em>54</em>(3), 260–270. (<a
href="https://doi.org/10.1109/THMS.2024.3368358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding human perception of haptic feedback is critical when designing and regulating these interfaces. In recent years, experiments have been conducted to determine the just-noticeable difference (JND) in mass–spring–damper dynamics, using a hydraulic admittance display in the form of a side-stick. These experiments have resulted in a model of JNDs when interacting with linear second-order dynamics. In real-world applications, however, control force dynamics also commonly include nonlinearities, such as friction. This research extends the current understanding of JNDs in linear systems by including the nonlinear case, where friction is also present. Experiments were conducted to determine JNDs in friction when combined with second-order system dynamics. Results indicate that friction JND can be independent of linear system dynamics as long as its value compared to the linear system&#39;s impedance is sufficiently large. As a consequence, friction JND follows Weber&#39;s law, also when it is combined with mass–spring–damper dynamics, unless the level of friction approaches the detection threshold, which in turn can be influenced by the linear system dynamics. Based on the findings presented, it is possible to conduct targeted experiments to confirm and add to these initial results.},
  archive      = {J_THMS},
  author       = {Robbin Veldhuis and Max Mulder and M. M. van Paassen},
  doi          = {10.1109/THMS.2024.3368358},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {260-270},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Thresholds for perceiving changes in friction when combined with linear system dynamics},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trust in range estimation system in battery electric
vehicles–a mixed approach. <em>THMS</em>, <em>54</em>(3), 250–259. (<a
href="https://doi.org/10.1109/THMS.2024.3381116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The electrification of vehicle power systems has become a dominant trend worldwide. However, with current technologies, range anxiety is still a major obstacle to the popularization of battery electric vehicles (BEVs). Previous research has found that users’ trust in the BEVs’ range estimation system (RES) is associated with their range anxiety. However, influential factors of trust in RES have not yet been explored. Thus, a questionnaire was designed to model the factors that are directly (i.e., implicit factors) and indirectly (i.e., explicit factors) associated with BEV users’ trust in RES. Following the three-layer automation trust framework (i.e., dispositional trust, situational trust, and learned trust), a questionnaire was designed and administrated online. In total, 367 valid samples were collected from BEV users in mainland China. A mixed approach combining Bayesian network (BN) and regression analyses (i.e., BN–regression mixed approach) was proposed to explore the potential topological relationships among factors. Four implicit factors (i.e., sensitivity to BEV brand, knowledge of RES, users’ emotional stability, and trust in the battery estimation system of their phones) have been found to be directly associated with BEV users’ trust in RES. Furthermore, four explicit factors (i.e., users’ highest education, regional charging infrastructure development, BEV brand, and household income) were found to be indirectly associated with users’ trust in RES. This study further demonstrates the effectiveness of using a BN–regression mixed approach to explore topological relationships among social–psychological factors. Future strategies aiming to modulate trust in RES can target toward factors in different levels of the topological structure.},
  archive      = {J_THMS},
  author       = {Jiyao Wang and Ran Tu and Ange Wang and Dengbo He},
  doi          = {10.1109/THMS.2024.3381116},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {250-259},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Trust in range estimation system in battery electric Vehicles–A mixed approach},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hand segmentation with dense dilated u-net and structurally
incoherent nonnegative matrix factorization-based gesture recognition.
<em>THMS</em>, <em>54</em>(3), 238–249. (<a
href="https://doi.org/10.1109/THMS.2024.3390415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust segmentation of hands in a cluttered environment for hand gesture recognition has remained a challenge in computer vision. In this work, a two-stage gesture recognition framework is proposed. In the first stage, we segment hands using the proposed deep learning algorithm, and in the second stage, we use these segmented hands to classify gestures using a novel structurally incoherent nonnegative matrix factorization approach. We propose a new deep learning framework for hand segmentation called densely dilated U-Net. We exploit recently proposed dense blocks and dilated convolution layers in our work. To cope with the scarcity of labeled datasets we extend our densely dilated U-Net for semisupervised hand segmentation using hand bounding boxes as cues. We provide quantitative and qualitative evaluation of proposed hand segmentation model on several public hand segmentation datasets including EgoHands, GTEA, EYTH, EDSH, and HOF. Semisupervised segmentation results are also obtained on two hand detection datasets including VIVA and CVRR. As an extension of our work, we show semisupervised segmentation and gesture recognition results using segmented hands on NUS-II cluttered hand gesture dataset. To validate the efficiency of our semisupervised algorithm we evaluate it on OUHands dataset with real ground truth labels. For gesture classification, we propose a novel structurally incoherent nonnegative matrix factorization algorithm. We propose to use CNN features extracted from segmented images for nonnegative matrix factorization. Experimental results on NUS-II and OUHands datasets demonstrate that our two-stage approach for gesture recognition yields superior results.},
  archive      = {J_THMS},
  author       = {Kankana Roy and Rajiv R. Sahay},
  doi          = {10.1109/THMS.2024.3390415},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {238-249},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Hand segmentation with dense dilated U-net and structurally incoherent nonnegative matrix factorization-based gesture recognition},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A synergistic formal-statistical model for recognizing
complex human activities. <em>THMS</em>, <em>54</em>(3), 229–237. (<a
href="https://doi.org/10.1109/THMS.2024.3382468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a view-independent synergistic model (formal and statistical) for efficiently recognizing complex human activities from video frames. To reduce the computational cost, the number of video frames is subsampled from 30 to 3 frames/s. SKD, a collaborative set of formal languages ( S OMA, K INISIS, and D RASIS), models simple and complex body actions and activities. SOMA language is a frame-based formal language representing body states (poses) extracted from frames. KINISIS is a formal language that uses the body poses extracted from SOMA to determine the consecutive poses (motion) that compose an activity. DRASIS language, finally, a convolution neural net, is used to classify simple activities, and an long short-term memory is used to recognize changes in activity. Experimental results using the SKD model on MSR Daily Activity three-dimensional (3-D) and UTKinect-Action3D datasets have shown that our method is among the top ones.},
  archive      = {J_THMS},
  author       = {Nikolaos Bourbakis and Anargyros Angeleas},
  doi          = {10.1109/THMS.2024.3382468},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {229-237},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A synergistic formal-statistical model for recognizing complex human activities},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RTSformer: A robust toroidal transformer with spatiotemporal
features for visual tracking. <em>THMS</em>, <em>54</em>(2), 214–225.
(<a href="https://doi.org/10.1109/THMS.2024.3370582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In complex environments, trackers are extremely susceptible to some interference factors, such as fast motions, occlusion, and scale changes, which result in poor tracking performance. The reason is that trackers cannot sufficiently utilize the target feature information in these cases. Therefore, it has become a particularly critical issue in the field of visual tracking to utilize the target feature information efficiently. In this article, a composite transformer involving spatiotemporal features is proposed to achieve robust visual tracking. Our method develops a novel toroidal transformer to fully integrate features while designing a template refresh mechanism to provide temporal features efficiently. Combined with the hybrid attention mechanism, the composite of temporal and spatial feature information is more conducive to mining feature associations between the template and search region than a single feature. To further correlate the global information, the proposed method adopts a closed-loop structure of the toroidal transformer formed by the cross-feature fusion head to integrate features. Moreover, the designed score head is used as a basis for judging whether the template is refreshed. Ultimately, the proposed tracker can achieve the tracking task only through a simple network framework, which especially simplifies the existing tracking architectures. Experiments show that the proposed tracker outperforms extensive state-of-the-art methods on seven benchmarks at a real-time speed of 56.5 fps.},
  archive      = {J_THMS},
  author       = {Fengwei Gu and Jun Lu and Chengtao Cai and Qidan Zhu and Zhaojie Ju},
  doi          = {10.1109/THMS.2024.3370582},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {214-225},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {RTSformer: A robust toroidal transformer with spatiotemporal features for visual tracking},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Head-pose estimation based on lateral canthus localizations
in 2-d images. <em>THMS</em>, <em>54</em>(2), 202–213. (<a
href="https://doi.org/10.1109/THMS.2024.3351138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Head-pose estimation plays an important role in computer vision. The head-pose estimation aims to determine the orientation of a human head by representing the yaw, pitch, and roll angles. Implementations can be achieved by different techniques depending on the type of input and training data. This article presents a simple three-dimensional (3-D) face model for estimating head poses. The personalized 3-D face model is constructed by 2-D face photographs. A frontal face photograph determines the plane coordinates of facial features. By knowing the yaw angles in the other averted face photograph, the depth coordinates can be determined. The yaw angle of the averted face is evaluated by the canthus positions. Once the 3-D face model is constructed, we can find the matching angles for a target head pose in a query 2-D photograph. The personalized 3-D face model rotates itself about the x -, y -, and z -axes and then projects its facial features onto plane coordinates. If the rotation angles are correct, the disparities between the 2-D facial features and those in the query face photograph are supposed to be at their minimum. The personalized 3-D face model is validated with the University of South Florida human-identification database. The performance of the proposed head-pose estimation is evaluated on the Biwi Kinect head-pose database and Pointing’04 head-pose image database. The results show that the proposed method outperforms state-of-the-art technologies on both benchmark databases.},
  archive      = {J_THMS},
  author       = {Shu-Nung Yao and Chang-Wei Huang},
  doi          = {10.1109/THMS.2024.3351138},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {202-213},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Head-pose estimation based on lateral canthus localizations in 2-D images},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging high-density EMG to investigate bipolar electrode
placement for gait prediction models. <em>THMS</em>, <em>54</em>(2),
192–201. (<a href="https://doi.org/10.1109/THMS.2024.3371099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To control wearable robotic systems, it is critical to obtain a prediction of the user&#39;s motion intent with high accuracy. Surface electromyography (sEMG) recordings have often been used as inputs for these devices, however bipolar sEMG electrodes are highly sensitive to their location. Positional shifts of electrodes after training gait prediction models can therefore result in severe performance degradation. This study uses high-density sEMG (HD-sEMG) electrodes to simulate various bipolar electrode signals from four leg muscles during steady-state walking. The bipolar signals were ranked based on the consistency of the corresponding sEMG envelope&#39;s activity and timing across gait cycles. The locations were then compared by evaluating the performance of an offline temporal convolutional network (TCN) that mapped sEMG signals to knee angles. The results showed that electrode locations with consistent sEMG envelopes resulted in greater prediction accuracy compared to hand-aligned placements ( p &lt; 0.01). However, performance gains through this process were limited, and did not resolve the position shift issue. Instead of training a model for a single location, we showed that randomly sampling bipolar combinations across the HD-sEMG grid during training mitigated this effect. Models trained with this method generalized over all positions, and achieved 70% less prediction error than location specific models over the entire area of the grid. Therefore, the use of HD-sEMG grids to build training datasets could enable the development of models robust to spatial variations, and reduce the impact of muscle-specific electrode placement on accuracy.},
  archive      = {J_THMS},
  author       = {Balint K. Hodossy and Annika S. Guez and Shibo Jing and Weiguang Huo and Ravi Vaidyanathan and Dario Farina},
  doi          = {10.1109/THMS.2024.3371099},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {192-201},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Leveraging high-density EMG to investigate bipolar electrode placement for gait prediction models},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extracting human levels of trust in human–swarm interaction
using EEG signals. <em>THMS</em>, <em>54</em>(2), 182–191. (<a
href="https://doi.org/10.1109/THMS.2024.3356421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trust is an essential building block of human civilization. However, when it relates to artificial systems, it has been a barrier to intelligent technology adoption in general. This article addresses the gap in determining levels of trust in scenarios that include humans interacting with a swarm of robots. Electroencephalography (EEG) recordings of the human observers of the different swarms allow for extracting specific EEG features related to different trust levels. Feature selection and machine learning methods comprise a classification system that would allow recognition of different levels of human trust in those human–swarm interaction scenarios. The results of this study suggest that EEG correlates of swarm trust exist and are distinguishable in machine learning feature classification with very high accuracy. Moreover, comparing common EEG features across all human subjects used in this study allows for the generalization of the classification method, providing solid evidence of specific areas and features of the human brain where activations are related to levels of human–swarm trust. This work has direct implications for effective human–machine teaming with applications to many fields, such as exploration, search and rescue operations, surveillance, environmental monitoring, and defense. In these applications, quantifying levels of human trust in the deployed swarm is of utmost importance because it can lead to swarm controllers that adapt their output based on the human&#39;s perceived trust level.},
  archive      = {J_THMS},
  author       = {Jesus A. Orozco and Panagiotis Artemiadis},
  doi          = {10.1109/THMS.2024.3356421},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {182-191},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Extracting human levels of trust in Human–Swarm interaction using EEG signals},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Behavioral, peripheral, and central neural correlates of
augmented reality guidance of manual tasks. <em>THMS</em>,
<em>54</em>(2), 172–181. (<a
href="https://doi.org/10.1109/THMS.2024.3354413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objective: The use of commercially available optical-see-through (OST) head-mounted displays (HMDs) in their own peripersonal space leads the user to experience two perception conflicts that deteriorate their performance in precision manual tasks: the vergence-accommodation conflict (VAC) and the focus rivalry. In this work, we aim characterizing for the first time the psychophysiological response associated with user&#39;s incorrect focus cues during the execution of an augmented reality (AR)-guided manual task with the Microsoft HoloLens OST-HMD. Methods: 21 subjects underwent to a “connecting-the-dots” experiment with and without the use of AR, and in both binocular and monocular conditions. For each condition, we quantified the changes in autonomic nervous system (ANS) activity of subjects by analyzing the electrodermal activity (EDA) and heart rate variability. Moreover, we analyzed the neural central correlates by means of power measures of brain activity and multivariate autoregressive measures of brain connectivity extracted from the electroencephalogram (EEG). Results: No statistically significant differences of ANS correlates were observed among tasks, although all EDA-related features varied between rest and task conditions. Conversely, significant differences among conditions were present in terms of EEG-power variations in the $\mu$ (8–13) Hz and $\beta$ (13–30) Hz bands. In addition, significant changes in the causal interactions of a brain network involved in motor movement and eye-hand coordination comprising the precentral gyrus, the precuneus, and the fusiform gyrus were observed. Conclusion: The physiological plausibility of our results suggest promising future applicability to investigate more complex scenarios, such as AR-guided surgery.},
  archive      = {J_THMS},
  author       = {Alejandro L. Callara and Gianluca Rho and Sara Condino and Vincenzo Ferrari and Enzo Pasquale Scilingo and Alberto Greco},
  doi          = {10.1109/THMS.2024.3354413},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {172-181},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Behavioral, peripheral, and central neural correlates of augmented reality guidance of manual tasks},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and investigation of a suspended backpack with
wide-range variable stiffness suspension for reducing energetic cost.
<em>THMS</em>, <em>54</em>(2), 162–171. (<a
href="https://doi.org/10.1109/THMS.2024.3355474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Suspended backpacks have been acknowledged for their advantages in load carriage, leading to the development of various designs aimed at enhancing their performance. However, current suspended backpacks typically possess fixed stiffness or limited adjustability, thereby limiting their adaptability to different load carriage tasks, such as varying walking speeds and load masses. This article introduced a suspended backpack design capable of modulating its stiffness over a wide range while maintaining a lightweight profile. The variable stiffness suspension (VSS) was integrated into the load frame of the suspended backpack and utilized a motor to adjust the stiffness by generating spring-like force based on the relative displacement between the load and the body. Experimental validation was conducted to assess the stiffness modulation of the suspended backpack. The VSS enabled the stiffness modulation of the suspended backpack ranging from 424 to 2182 N/m, which corresponded to the desired stiffness range for a 10–25 kg load at walking speeds for 3.5–6 km/h. Moreover, the mechanics of the carriers were analyzed to evaluate the impact of the suspended backpack on the individuals. Results showed that the designed VSS suspended backpack could reduce peak push-off force by 20.71% under the high working condition and energetic cost by 30.39% under the midworking condition. However, a tradeoff exists between minimizing the peak accelerative load force and energetic cost. The proposed design holds the potential for enhancing performance across various load carriage tasks, including human-in-the-loop energetic optimization.},
  archive      = {J_THMS},
  author       = {Xin Lin and Shucong Yin and Hao Du and Yuquan Leng and Chenglong Fu},
  doi          = {10.1109/THMS.2024.3355474},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {162-171},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Design and investigation of a suspended backpack with wide-range variable stiffness suspension for reducing energetic cost},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human-like trajectory planning based on postural synergistic
kernelized movement primitives for robot-assisted rehabilitation.
<em>THMS</em>, <em>54</em>(2), 152–161. (<a
href="https://doi.org/10.1109/THMS.2024.3360111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The motor synergy pattern is an intrinsic characteristic found in natural human movements, particularly in the upper limb. It is essential to improve the multijoint coordination ability for stroke patients by integrating the synergy pattern into rehabilitation tasks and trajectory design. However, current robot-assisted rehabilitation systems tend to overlook the incorporation of a multijoint synergy model. This article proposes postural synergistic kernelized movement primitives (PSKMP) method for the human-like trajectory planning of robot-assisted upper limb rehabilitation. First, the demonstrated trajectory obtained from the motion capture system is subject to principal component analysis to extract postural synergies. Then, the PSKMP is proposed by kernelizing the postural synergistic subspaces with the kernel treatment to preserve human natural movement characteristics. Finally, the rehabilitation trajectory accord with human motion habits can be generated based on generalized postural synergistic subspaces. This approach has undergone practical validation on an upper limb rehabilitation robot, and the experimental results show that the proposed method enables the generation of human-like trajectories adapted to new task points, in accordance with the natural movement style of human. This method holds great significance in promoting the recovery of coordination ability of stroke patients.},
  archive      = {J_THMS},
  author       = {Zemin Liu and Qingsong Ai and Haojie Liu and Wei Meng and Quan Liu},
  doi          = {10.1109/THMS.2024.3360111},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {152-161},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Human-like trajectory planning based on postural synergistic kernelized movement primitives for robot-assisted rehabilitation},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Working conditions of industrial robot operators–an overview
of technology dissemination, job characteristics, and health indicators
in modern production workplaces. <em>THMS</em>, <em>54</em>(2), 145–151.
(<a href="https://doi.org/10.1109/THMS.2024.3368525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flexible robotic systems change not only the production workflow as a whole but also the individual working conditions of their operators. The aim of this analysis of our study with more than 5900 participants was to get an overview of demographics, job characteristics, and health indicators of robot operators in comparison to nonrobotic machine operators and employees in Germany. We collected data by telephone interviews measuring technology use, stressors, and resources at work as well as health indicators. Results indicate systematic differences in working stressors and resources for robot users compared to other machine users as well as employees in general. In particular, the scope for decision-making at work was smaller for robot users, especially regarding the amount of work or the speed of work. Only isolated links could be found regarding the health indicators. The results therefore imply constant consideration of human factors to ensure productive as well as healthy working conditions with robots in modern industry.},
  archive      = {J_THMS},
  author       = {Matthias Hartwig and Patricia Rosen and Sascha Wischniewski},
  doi          = {10.1109/THMS.2024.3368525},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {145-151},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Working conditions of industrial robot Operators–An overview of technology dissemination, job characteristics, and health indicators in modern production workplaces},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multidataset characterization of window-based
hyperparameters for deep CNN-driven sEMG pattern recognition.
<em>THMS</em>, <em>54</em>(1), 131–142. (<a
href="https://doi.org/10.1109/THMS.2023.3329536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The control performance of myoelectric prostheses would not only depend on the feature extraction and classification algorithms but also on interactions of dynamic window-based hyperparameters (WBHP) used to construct input signals. However, the relationship between these hyperparameters and how they influence the performance of the convolutional neural networks (CNNs) during motor intent decoding has not been studied. Therefore, we investigated the impact of various combinations of WBHP (window length and overlap) employed for the construction of raw two-dimensional (2-D) surface electromyogram (sEMG) signals on the performance of CNNs when used for motion intent decoding. Moreover, we examined the relationship between the window length of the 2-D sEMG and three commonly used CNN kernel sizes. To ensure high confidence in the findings, we implemented three CNNs, which are variants of the existing models, and a newly proposed CNN model. Experimental analysis was conducted using three distinct benchmark databases, two from upper limb amputees and one from able-bodied subjects. The results demonstrate that the performance of the CNNs improved as the overlap between consecutively generated 2-D signals increased, with 75% overlap yielding the optimal improvement by 12.62% accuracy and 39.60% F1-score compared to no overlap. Moreover, the CNNs performance was better for kernel size of seven than three and five across the databases. For the first time, we have established with multiple evidence that WBHP would substantially impact the decoding outcome and computational complexity of deep neural networks, and we anticipate that this may spur positive advancement in myoelectric control and related fields.},
  archive      = {J_THMS},
  author       = {Frank Kulwa and Haoshi Zhang and Oluwarotimi Williams Samuel and Mojisola Grace Asogbon and Erik Scheme and Rami Khushaba and Alistair A. McEwan and Guanglin Li},
  doi          = {10.1109/THMS.2023.3329536},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {131-142},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A multidataset characterization of window-based hyperparameters for deep CNN-driven sEMG pattern recognition},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight whole-body human pose estimation with two-stage
refinement training strategy. <em>THMS</em>, <em>54</em>(1), 121–130.
(<a href="https://doi.org/10.1109/THMS.2024.3349652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human whole-body pose estimation is a challenging task since the model needs to learn more keypoints than the body-only case. To meet the needs of real-time performance while maintaining accuracy is also a hard issue in whole-body pose estimation due to the learning capability of lightweight networks. In order to solve the above problems to a large extent, we propose a light whole-body pose estimation method with an optimized training strategy. The model is designed based on bottom-up architecture as a base network followed by a refinement network. We propose a two-stage training process, which learns rough features in the first stage and then improves estimation precision in the second stage. An online data augmentation procedure is proposed in the second stage to improve refinement performance. We also introduce a separate learning refinement structure that fine-tunes for body, foot, and hand part independently. Experimental results show that our method improves over 8%–10% average precision compared with other lightweight state-of-the-art approaches in the whole-body pose estimation task, with nearly a quarter (25%) size of model parameters saved.},
  archive      = {J_THMS},
  author       = {Zhewei Zhang and Mingen Liu and Junyu Shen and Yujun Cheng and Shengjin Wang},
  doi          = {10.1109/THMS.2024.3349652},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {121-130},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Lightweight whole-body human pose estimation with two-stage refinement training strategy},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Speech enhancement—a review of modern methods.
<em>THMS</em>, <em>54</em>(1), 110–120. (<a
href="https://doi.org/10.1109/THMS.2023.3339663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A review of techniques to improve distorted speech is presented, noting the strengths and weaknesses of common methods. Speech signals are discussed from the point of view of which features should be preserved to retain both naturalness and intelligibility. Enhancement methods range from classical spectral subtraction and Wiener filtering to recent deep neural network approaches. The difficulty of finding objective acoustic measures that approximate perceptual speech quality is discussed. Suggestions to improve these methods are made.},
  archive      = {J_THMS},
  author       = {Douglas O&#39;Shaughnessy},
  doi          = {10.1109/THMS.2023.3339663},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {110-120},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Speech Enhancement—A review of modern methods},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Situated interpretation and data: Explainability to convey
machine misalignment. <em>THMS</em>, <em>54</em>(1), 100–109. (<a
href="https://doi.org/10.1109/THMS.2023.3334988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainable AI must simultaneously help people understand the world, the AI, and when the AI is misaligned to the world. We propose situated interpretation and data (SID) as a design technique to satisfy these requirements. We trained two machine learning algorithms, one transparent and one opaque, to predict future patient events that would require an emergency response team (ERT) mobilization. An SID display combined the outputs of the two algorithms with patient data and custom annotations to implicitly convey the alignment of the transparent algorithm to the underlying data. SID displays were shown to 30 nurses with 10 actual patient cases. Nurses reported their concern level (1–10) and intended response (1–4) for each patient. For all cases where the algorithms predicted no ERT (correctly or incorrectly), nurses correctly differentiated ERT from non-ERT in both concern and response. For all cases where the algorithms predicted an ERT, nurses differentiated ERT from non-ERT in response, but not concern. Results also suggest that nurses’ reported urgency was unduly influenced by misleading algorithm guidance in cases where the algorithm overpredicted and underpredicted the future ERT. However, nurses reported concern that was as or more appropriate than the predictions in 8 of 10 cases and differentiated ERT from non-ERT cases better than both algorithms, even the more accurate opaque algorithm, when the two predictions conflicted. Therefore, SID appears a promising design technique to reduce, but not eliminate, the negative impacts of misleading opaque and transparent algorithms.},
  archive      = {J_THMS},
  author       = {Dane Anthony Morey and Michael F. Rayo},
  doi          = {10.1109/THMS.2023.3334988},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {100-109},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Situated interpretation and data: Explainability to convey machine misalignment},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Classifying human manual control behavior using LSTM
recurrent neural networks. <em>THMS</em>, <em>54</em>(1), 89–99. (<a
href="https://doi.org/10.1109/THMS.2023.3327145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article discusses a long short-term memory (LSTM) recurrent neural network that uses raw time-domain data obtained in compensatory tracking tasks as input features for classifying (the adaptation of) human manual control with single- and double-integrator controlled element dynamics. Data from two different experiments were used to train and validate the LSTM classifier, including investigating effects of several key data preprocessing settings. The model correctly classifies human control behavior (cross-experiment validation accuracy 96%) using short 1.6-s data windows. To achieve this accuracy, it is found crucial to scale/standardize the input feature data and use a combination of input signals that includes the tracking error and human control output. A possible online application of the classifier was tested on data from a third experiment with time-varying and slightly different controlled element dynamics. The results show that the LSTM classification is still successful, which makes it a promising online technique to rapidly detect adaptations in human control behavior.},
  archive      = {J_THMS},
  author       = {Rogier Versteeg and Daan M. Pool and Max Mulder},
  doi          = {10.1109/THMS.2023.3327145},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {89-99},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Classifying human manual control behavior using LSTM recurrent neural networks},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using <span class="math inline"><em>B</em></span>-spline
model on depth camera data to predict physical activity energy
expenditure of different levels of human exercise. <em>THMS</em>,
<em>54</em>(1), 79–88. (<a
href="https://doi.org/10.1109/THMS.2023.3349030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy expenditure (EE) is often used to quantify physical activity. Currently, EE is estimated with data collected by inertial measurement units or depth cameras and verified by oxygen consumption data. Due to the different data collection time spans in this system, raw data were split into minute-by-minute windows, and summary statistics for each window were computed. However, using summary statistics to aggregate data might be influenced by redundant noise or result in the loss of valuable information. This article presents a modeling method using functional analysis to characterize the trajectory of the collected skeletal data, thus enabling the effective use of the complete data. Next, the fitted values of the skeletal data can be aligned to the overall EE data and used to predict the overall EE as well as the task-based EE. The study results revealed for metabolic equivalent of task prediction that the root-mean-square error (RMSE) derived for the proposed method was $&amp;lt; $ 0.5 and that the mean absolute error (MAE) was approximately 0.3. Models for estimating task-based EE, including EE related to standing and walking task, also exhibited low RMSE and MAE values. Accordingly, the proposed modeling approach is superior to summary statistics for estimating EE in depth camera systems.},
  archive      = {J_THMS},
  author       = {Yi-Ting Hwang and Ya-Ru Hsu and Bor-Shing Lin},
  doi          = {10.1109/THMS.2023.3349030},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {79-88},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Using $B$-spline model on depth camera data to predict physical activity energy expenditure of different levels of human exercise},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep learning based lightweight human activity recognition
system using reconstructed WiFi CSI. <em>THMS</em>, <em>54</em>(1),
68–78. (<a href="https://doi.org/10.1109/THMS.2023.3348694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity recognition (HAR) is a key technology in the field of human–computer interaction. Unlike systems using sensors or special devices, the WiFi channel state information (CSI)-based HAR systems are noncontact and low cost, but they are limited by high computational complexity and poor cross-domain generalization performance. In order to address the above problems, a reconstructed WiFi CSI tensor and deep learning based lightweight HAR system (Wisor-DL) is proposed, which firstly reconstructs WiFi CSI signals with a sparse signal representation algorithm, and a CSI tensor construction and decomposition algorithm. Then, gated temporal convolutional network with residual connections is designed to enhance and fuse the features of the reconstructed WiFi CSI signals. Finally, dendrite network makes the final decision of activity instead of the traditional dense layer. Experimental results show that Wisor-DL is a lightweight HAR system with high recognition accuracy and satisfactory cross-domain generalization ability.},
  archive      = {J_THMS},
  author       = {Xingcan Chen and Yi Zou and Chenglin Li and Wendong Xiao},
  doi          = {10.1109/THMS.2023.3348694},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {68-78},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A deep learning based lightweight human activity recognition system using reconstructed WiFi CSI},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling awareness requirements in groupware: From cards to
diagrams. <em>THMS</em>, <em>54</em>(1), 56–67. (<a
href="https://doi.org/10.1109/THMS.2023.3332592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Up to now, groupware has enjoyed a certain stability in terms of the users’ technical requirements, being the awareness dimension one of its key services to provide usability and improve collaboration. Nonetheless, currently, groupware technologies are being stressed: on the one hand, the pandemic of COVID-19 has greatly driven the massive use of groupware tools to overcome physical distancing; on the other hand, the new digital worlds (with disruptive devices, changing paradigms, and growing productive needs) are introducing new collaboration settings. This, and the fact that software engineering methods are not paying enough attention to the awareness, makes us concentrate on facilitating its design. Thus, we have created a visual modeling technique, based on a conceptual framework, to be used by the developers of groupware systems to describe awareness requirements. This visual language, called the awareness description diagrams, has been validated in some experimental activities. The results obtained show that this is a valid technique in order to model the awareness support, that it is useful and understandable for groupware engineers, and that the visual representation is preferred to a more textual one in terms of expressiveness.},
  archive      = {J_THMS},
  author       = {Crescencio Bravo and Rafael Duque and Ana I. Molina and Jesús Gallardo},
  doi          = {10.1109/THMS.2023.3332592},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {56-67},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Modeling awareness requirements in groupware: From cards to diagrams},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EEG-based familiar and unfamiliar face classification using
filter-bank differential entropy features. <em>THMS</em>,
<em>54</em>(1), 44–55. (<a
href="https://doi.org/10.1109/THMS.2023.3332209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The face recognition of familiar and unfamiliar people is an essential part of our daily lives. However, its neural mechanism and relevant electroencephalography (EEG) features are still unclear. In this study, a new EEG-based familiar and unfamiliar faces classification method is proposed. We record the multichannel EEG with three different face-recall paradigms, and these EEG signals are temporally segmented and filtered using a well-designed filter-bank strategy. The filter-bank differential entropy is employed to extract discriminative features. Finally, the support vector machine (SVM) with Gaussian kernels serves as the robust classifier for EEG-based face recognition. In addition, the F-score is employed for feature ranking and selection, which helps to visualize the brain activation in time, frequency, and spatial domains, and contributes to revealing the neural mechanism of face recognition. With feature selection, the highest mean accuracy of 74.10% can be yielded in face-recall paradigms over ten subjects. Meanwhile, the analysis of results indicates that the EEG-based classification performance of face recognition will be significantly affected when subjects lie. The time–frequency topographical maps generated according to feature importance suggest that the delta band in the prefrontal region correlates to the face recognition task, and the brain response pattern varies from person to person. The present work demonstrates the feasibility of developing an efficient and interpretable brain–computer interface for EEG-based face recognition.},
  archive      = {J_THMS},
  author       = {Guoyang Liu and Yiming Wen and Janet H. Hsiao and Di Zhang and Lan Tian and Weidong Zhou},
  doi          = {10.1109/THMS.2023.3332209},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {44-55},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {EEG-based familiar and unfamiliar face classification using filter-bank differential entropy features},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EMG-based detection of minimum effective load with
robotic-resistance leg extensor training. <em>THMS</em>, <em>54</em>(1),
34–43. (<a href="https://doi.org/10.1109/THMS.2023.3347404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To promote rapid recovery and quality of life after a musculoskeletal disorder, rehabilitation exercises that are suitable for each individual&#39;s physical condition are important. In cases of disuse muscle atrophy of the quadriceps, inappropriate training can cause injury. Although resistance-training robotic systems have been developed and could adjust resistance load, a systematic detection method with appropriate force strength for automatic adjustment for each individual has not yet been established. In the current study, we developed an electromyogram (EMG) based method that determines the minimum effective resistance load for muscle growth. Using an integrated EMG (IEMG) model of incremental resistance load focused, we constructed a method to determine the minimum effective resistance load with logarithmic functions. The feasibility of our method was tested with a slow training protocol using a wire-driven leg extension training robot to measure the relationship between IEMG and resistance load by applying the incremental resistance load. The proposed model was found to be suitable for six young and four elderly subjects with different levels of muscle mass, and the load derived for each person was shown to induce effectively acute thigh circumference expansion, which is a factor leading to future muscle hypertrophy.},
  archive      = {J_THMS},
  author       = {Tamon Miyake and Hiromasa Ito and Naomi Okamura and Yo Kobayashi and Masakatsu G. Fujie and Shigeki Sugano},
  doi          = {10.1109/THMS.2023.3347404},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {34-43},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {EMG-based detection of minimum effective load with robotic-resistance leg extensor training},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). What challenges does the full-touch HMI mode bring to
driver’s lateral control ability? A comparative study based on real
roads. <em>THMS</em>, <em>54</em>(1), 21–33. (<a
href="https://doi.org/10.1109/THMS.2023.3342045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the full-touch human–machine interface (HMI) mode has been widely used in vehicles built by Tesla. This interaction mode replaces the conventional physical interaction modality with a screen, and it has a good sense of technological experience. However, it is unclear whether this mode will make the driver&#39;s lateral control more challenging than the conventional mode (CM). To investigate this issue, two most common secondary tasks were designed: dialing and navigation entry tasks and real-world road experiments were conducted using two instrumented vehicles. The vehicle operating parameters and the driver manual data were collected in different modes, respectively. Interestingly, the opposite results were found regarding the effect of the full-touch mode (FTM) on the driver&#39;s lateral control ability in different secondary tasks. Compared with the CM, the lateral control ability decreased less during the dialing task relative to the baseline driving in the FTM, while the lateral control ability decreased more in the FTM during the navigation entry task. In addition, drivers’ lateral control decreased further as task difficulty and driving speed increased regardless of mode. This study provides a theoretical basis for the development of laws and regulations regarding full-touch HMI mode.},
  archive      = {J_THMS},
  author       = {Xia Zhao and Zhao Li and Rui Fu and Chang Wang and Yingshi Guo},
  doi          = {10.1109/THMS.2023.3342045},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {21-33},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {What challenges does the full-touch HMI mode bring to driver&#39;s lateral control ability? a comparative study based on real roads},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Haptic shared control for dissipating phantom traffic jams.
<em>THMS</em>, <em>54</em>(1), 11–20. (<a
href="https://doi.org/10.1109/THMS.2023.3315519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic jams occurring on highways cause increased travel time as well as increased fuel consumption and collisions. So-called phantom traffic jams are traffic jams that do not have a clear cause, such as a merging on-ramp or an accident. Phantom traffic jams make up 50% of all traffic jams and result from instabilities in the traffic flow that are caused by human driving behavior. Automating the longitudinal vehicle motion of only 5% of all cars in the flow can dissipate phantom traffic jams. However, driving automation introduces safety issues when human drivers need to take over the control from the automation. We investigated whether phantom traffic jams can be dissolved using haptic shared control. This keeps humans in the loop and thus bypasses the problem of humans&#39; limited capacity to take over control, while benefiting from most advantages of automation. In an experiment with 24 participants in a driving simulator, we tested the effect of haptic shared control on the dynamics of traffic flow and compared it with manual control and full automation. We also investigated the effect of two control types on participants&#39; behavior during simulated silent automation failures. Results show that haptic shared control can help dissipating phantom traffic jams better than fully manual control but worse than full automation. We also found that haptic shared control reduces the occurrence of unsafe situations caused by silent automation failures compared to full automation. Our results suggest that haptic shared control can dissipate phantom traffic jams while preventing safety risks associated with full automation.},
  archive      = {J_THMS},
  author       = {Klaas O. Koerten and David. A. Abbink and Arkady Zgonnikov},
  doi          = {10.1109/THMS.2023.3315519},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {11-20},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Haptic shared control for dissipating phantom traffic jams},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel measure of human safety perception in response to
flight characteristics of collocated UAVs in virtual reality.
<em>THMS</em>, <em>54</em>(1), 1–10. (<a
href="https://doi.org/10.1109/THMS.2023.3336294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article examines how people respond to the presence of a flying robot under various operating conditions using traditional human physiological measures and a novel head movement measurement. A central issue to the integration of flying robotic systems into human-populated environments is how to improve the level of comfort and safety for people around them. Traditional motion control algorithms in robotics tend to focus on the actual safety of collision avoidance. However, people&#39;s perceived safety is not necessarily equivalent to the actual safety of the vehicle. Therefore flight control systems must account for people&#39;s perception of safety beyond the actual safety of the aerial vehicles in order to allow for successful interaction between humans and the unmanned aerial vehicles (UAVs). Across three experiments participants passively observed quadrotor trajectories in a simulated virtual reality environment. Quadrotor flight characteristics were manipulated in terms of speed, altitude, and audibility to examine their effect on physiological arousal and head motion kinematics. Physiological arousal was greater when the quadrotor was flying with the audio on than off, and at eye-height than overhead, and decreased over repeated exposure. In addition, head acceleration away from the UAVs indicating defensive behavior was stronger for faster speed and audible UAVs. These data suggest head acceleration can serve as a new index specific for measuring perceived safety. Applications intended for human comfort need to consider constraints from specific measures of perceived safety in addition to traditional measures of general physiological arousal.},
  archive      = {J_THMS},
  author       = {Christopher Widdowson and Hyung-Jin Yoon and Naira Hovakimyan and Ranxiao Frances Wang},
  doi          = {10.1109/THMS.2023.3336294},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A novel measure of human safety perception in response to flight characteristics of collocated UAVs in virtual reality},
  volume       = {54},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
